<doc id="41564" url="http://en.wikipedia.org/wiki?curid=41564" title="Polarization (waves)">
Polarization (waves)

Polarization (also polarisation) is a property of waves that can oscillate with more than one orientation. Electromagnetic waves such as light exhibit polarization, as do some other types of wave, such as gravitational waves. Sound waves in a gas or liquid do not exhibit polarization, since the oscillation is always in the direction the wave travels.
In an electromagnetic wave, both the electric field and magnetic field are oscillating but in different directions; by convention the "polarization" of light refers to the polarization of the electric field. Light which can be approximated as a plane wave in free space or in an isotropic medium propagates as a transverse wave—both the electric and magnetic fields are perpendicular to the wave's direction of travel. The oscillation of these fields may be in a single direction (linear polarization), or the field may rotate at the optical frequency (circular or elliptical polarization). In that case the direction of the fields' rotation, and thus the specified polarization, may be either clockwise or counter clockwise; this is referred to as the wave's chirality or "handedness".
The most common optical materials (such as glass) are isotropic and simply preserve the polarization of a wave but do not differentiate between polarization states. However there are important classes of materials classified as birefringent or optically active in which this is not the case and a wave's polarization will generally be modified or will affect propagation through it. A polarizer is an optical filter that transmits only one polarization.
Polarization is an important parameter in areas of science dealing with transverse wave propagation, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.
Introduction.
Wave propagation and polarization.
Most sources of light are classified as incoherent and unpolarized (or only "partially polarized") because they consist of a random mixture of waves having different spatial characteristics, frequencies (wavelengths), phases, and polarization states. However for understanding electromagnetic waves and polarization in particular, it is easiest to just consider coherent plane waves; these are sinusoidal waves of one particular direction (or wavevector), frequency, phase, and polarization state. Characterizing an optical system in relation to a plane wave with those given parameters can then be used to predict its response to a more general case, since a wave with any specified spatial structure can be decomposed into a combination of plane waves (its so-called angular spectrum). And incoherent states can be modeled stochastically as a weighted combination of such uncorrelated waves with some distribution of frequencies (its "spectrum"), phases, and polarizations.
Transverse electromagnetic waves.
Electromagnetic waves (such as light), traveling in free space or another homogeneous isotropic non-attenuating medium, are properly described as transverse waves, meaning that a plane wave's electric field vector E and magnetic field H are in directions perpendicular to (or "transverse" to) the direction of wave propagation; E and H are also perpendicular to each other. Considering a monochromatic plane wave of optical frequency "f" (light of vacuum wavelength λ has a frequency of "f = c/λ" where "c" is the speed of light), let us take the direction of propagation as the "z" axis. Being a transverse wave the E and H fields must then contain components only in the "x" and "y" directions whereas "Ez=Hz=0". Using complex (or phasor) notation, we understand the instantaneous physical electric and magnetic fields to be given by the real parts of their corresponding complex quantities used in the following equations. As a function of time "t" and spatial position "z" (since for a plane wave in the +"z" direction the fields have no dependence on "x" or "y") these complex fields can be written as:
and
where λ/"n" is the wavelength "in the medium" (whose refractive index is "n") and "T" = 1/"f" is the period of the wave. In the second more compact form, as these equations are customarily expressed, these factors are described using the wavenumber formula_3 and angular frequency (or "radian frequency") formula_4. In a more general formulation with propagation "not" restricted to the "+z" direction, then the spatial dependence "kz" is replaced by formula_5 where formula_6 is called the wave vector, the magnitude of which is the wavenumber.
Thus the leading vectors e and h each contain up to two nonzero (complex) components describing the amplitude and phase of the wave's "x" and "y" polarization components (again, there can be no "z" polarization component for a transverse wave in the +"z" direction). For a given medium with a characteristic impedance formula_7, h is related to e by:
and
Note that given that relationship, the dot product of E and H must be zero:
indicating that these vectors are orthogonal (at right angles to each other), as expected.
So knowing the propagation direction (+"z" in this case) and η, one can just as well specify the wave in terms of just "ex" and "ey" describing the electric field. The vector containing "ex" and "ey" (but without the "z" component which is necessarily zero for a transverse wave) is known as a Jones vector. In addition to specifying the polarization state of the wave, a general Jones vector also specifies the overall magnitude and phase of that wave. Specifically, the intensity of the light wave is proportional to the sum of the squared magnitudes of the two electric field components:
however the wave's "state of polarization" is only dependent on the (complex) "ratio" of "ey" to "ex". So let us just consider waves whose "|ex|2 + |ey|2 = 1"; this happens to correspond to an intensity of about .00133 watts per square meter in free space (where formula_13 formula_14). And since the absolute phase of a wave is unimportant in discussing its polarization state, let us stipulate that the phase of "ex" is zero, in other words "ex" is a real number while "ey" may be complex. Under these restrictions, "ex" and "ey" can be represented as follows:
where the polarization state is now totally parameterized by the value of "Q" (such that -1 < "Q" < 1) and the relative phase formula_17. By convention when one speaks of a wave's "polarization," if not otherwise specified, reference is being made to the polarization of the electric field. The polarization of the magnetic field always follows that of the electric field but with a 90 degree rotation, as detailed above.
Non-transverse polarization.
In addition to transverse waves, there are many wave motions where the oscillation is not limited to directions perpendicular to the direction of propagation. These cases are beyond the scope of the current article which concentrates on transverse waves (such as most electromagnetic waves in bulk media), however one should be aware of cases where the polarization of a coherent wave cannot be described simply using a Jones vector, as we have just done.
Just considering electromagnetic waves, we note that the preceding discussion strictly applies to plane waves in a homogeneous isotropic non-attenuating medium, whereas in an anisotropic medium (such as birefringent crystals as discussed below) the electric or magnetic field may have longitudinal as well as transverse components. In those cases the electric displacement "D" and magnetic flux density "B" still obey the above geometry but due to anisotropy in the electric susceptibility (or in the magnetic permeability), now given by a tensor, the direction of "E" (or "H") may differ from that of "D" (or "B"). Even in isotropic media, so-called inhomogeneous waves can be launched into a medium whose refractive index has a significant imaginary part (or "extinction coefficient") such as metals; these fields are also not strictly transverse.:179–184:51–52 Surface waves or waves propagating in a waveguide (such as an optical fiber) are generally "not" transverse waves, but might be described as an electric or magnetic transverse mode, or a hybrid mode.
Even in free space, longitudinal field components can be generated in focal regions, where the plane wave approximation breaks down. An extreme example is radially or tangentially polarized light, at the focus of which the electric or magnetic field respectively is "entirely" longitudinal (along the direction of propagation).
For longitudinal waves such as sound waves in fluids, the direction of oscillation is by definition along the direction of travel, so the issue of polarization is not normally even mentioned. On the other hand, sound waves in a bulk solid can be transverse as well as longitudinal, for a total of three polarization components. In this case, the transverse polarization is associated with the direction of the shear stress and displacement in directions perpendicular to the propagation direction, while the longitudinal polarization describes compression of the solid and vibration along the direction of propagation. The differential propagation of transverse and longitudinal polarizations is important in seismology.
Polarization state.
Polarization is best understood by initially considering only pure polarization states, and only a coherent sinusoidal wave at some optical frequency. The vector on the right might describe the oscillation of the electric field emitted by a single-mode laser (whose oscillation frequency would be typically 1015 times faster!). The field oscillates in the "x-y" plane, along the page, with the wave propagating in the "z" direction, perpendicular to the page.
The first two diagrams below trace the electric field vector over a complete cycle for linear polarization at two different orientations; these are each considered a distinct "State Of Polarization" (SOP). Note that the linear polarization at 45° can also be viewed as the addition of a horizontally linearly polarized wave (as in the leftmost figure) and a vertically polarized wave of the same amplitude "in the same phase".
Now if one were to introduce a phase shift in between those horizontal and vertical polarization components, one would generally obtain elliptical polarization as is shown in the third figure. When the phase shift is exactly ±90°, then "circular polarization" is produced (fourth and fifth figures). Thus is circular polarization created in practice, starting with linearly polarized light and employing a quarter-wave plate to introduce such a phase shift. The result of two such phase-shifted components in causing a rotating electric field vector is depicted in the animation on the right. Note that circular or elliptical polarization can involve either a clockwise or counterclockwise rotation of the field. These correspond to distinct polarization states, such as the two circular polarizations shown above.
Of course the orientation of the "x" and "y" axes used in this description is arbitrary. The choice of such a coordinate system and viewing the polarization ellipse in terms of the "x" and "y" polarization components, corresponds to the definition of the Jones vector (below) in terms of those basis polarizations. One would typically choose axes to suit a particular problem such as "x" being in the plane of incidence. Since there are separate reflection coefficients for the linear polarizations in and orthogonal to the plane of incidence ("p" and "s" polarizations, see below), that choice greatly simplifies the calculation of a wave's reflection from a surface.
Moreover, one can use as basis functions "any" pair of orthogonal polarization states, not just linear polarizations. For instance, choosing right and left circular polarizations as basis functions simplifies the solution of problems involving circular birefringence (optical activity) or circular dichroism.
Polarization ellipse.
Consider a purely polarized monochromatic wave. If one were to plot the electric field vector over one cycle of oscillation, an ellipse would generally be obtained, as is shown in the figure, corresponding to a particular state of elliptical polarization. Note that linear polarization and circular polarization can be seen as special cases of elliptical polarization.
A polarization state can then be described in relation to the geometrical parameters of the ellipse, and its "handedness", that is, whether the rotation around the ellipse is clockwise or counter clockwise. One parameterization of the elliptical figure specifies the orientation angle ψ, defined as the angle between the major axis of the ellipse and the "x"-axis along with the ellipticity ε=a/b, the ratio of the ellipse's major to minor axis. (also known as the axial ratio). The ellipticity parameter is an alternative parameterization of an ellipse's eccentricity formula_18, or the ellipticity angle, χ = arctan b/a= arctan 1/ε as is shown in the figure. The angle χ is also significant in that the latitude (angle from the equator) of the polarization state as represented on the Poincaré sphere (see below) is equal to ±2χ. The special cases of linear and circular polarization correspond to an ellipticity ε of infinity and unity (or χ of zero and 45°) respectively.
Jones vector.
Full information on a completely polarized state is also provided by the amplitude and phase of oscillations in two components of the electric field vector in the plane of polarization. This representation was used above to show how different states of polarization are possible. The amplitude and phase information can be conveniently represented as a two-dimensional complex vector (the Jones vector):
Here formula_20 and formula_21 denote the amplitude of the wave in the two components of the electric field vector, while formula_22 and formula_23 represent the phases. The product of a Jones vector with a complex number of unit modulus gives a different Jones vector representing the same ellipse, and thus the same state of polarization. The physical electric field, as the real part of the Jones vector, would be altered but the polarization state itself is independent of absolute phase. The basis vectors used to represent the Jones vector need not represent linear polarization states (i.e. be real). In general any two orthogonal states can be used, where an orthogonal vector pair is formally defined as one having a zero inner product. A common choice is left and right circular polarizations, for example to model the different propagation of waves in two such components in circularly birefringent media (see below) or signal paths of coherent detectors sensitive to circular polarization.
Coordinate frame.
Regardless of whether polarization state is represented using geometric parameters or Jones vectors, implicit in the parameterization is the orientation of the coordinate frame. This permits a degree of freedom, namely rotation about the propagation direction. When considering light that is propagating parallel to the surface of the Earth, the terms "horizontal" and "vertical" polarization are often used, with the former being associated with the first component of the Jones vector, or zero azimuth angle. On the other hand, in astronomy the equatorial coordinate system is generally used instead, with the zero azimuth (or position angle, as it is more commonly called in astronomy to avoid confusion with the horizontal coordinate system) corresponding to due north.
"s" and "p" designations.
Another coordinate system frequently used relates to the plane made by the propagation direction and a vector perpendicular to the plane of a reflecting surface. This is known as the "plane of incidence". The component of the electric field parallel to this plane is termed "p-like" (parallel) and the component perpendicular to this plane is termed "s-like" (from "senkrecht", German for perpendicular). Polarized light with its electric field along the plane of incidence is thus denoted "p-polarized", while light whose electric field is normal to the plane of incidence is called "s-polarized". "P" polarization is commonly referred to as "transverse-magnetic" (TM), and has also been termed "pi-polarized" or "tangential plane polarized". "S" polarization is also called "transverse-electric" (TE), as well as "sigma-polarized" or "sagittal plane polarized".
Unpolarized and partially polarized light.
Definition.
Most common sources of visible light, including thermal (black body) radiation and fluorescence (but "not" lasers), produce light described as "incoherent". Radiation is produced independently by a large number of atoms or molecules whose emissions are uncorrelated and generally of random polarizations. In this case the light is said to be "unpolarized". This term is somewhat inexact, since at any instant of time at one location there is a definite direction to the electric and magnetic fields, however it implies that the polarization changes so quickly in time that it will not be measured or relevant to the outcome of an experiment. A so-called depolarizer acts on a polarized beam to create one which is actually "fully" polarized at every point, but in which the polarization varies so rapidly across the beam that it may be ignored in the intended applications.
Light is said to be "partially polarized" when there is more power in one polarization mode than another. At any particular wavelength, partially polarized light can be statistically described as the superposition of a completely unpolarized component, and a completely polarized one.:330 One may then describe the light in terms of the degree of polarization, and the parameters of the polarized component. That polarized component can be described in terms of a Jones vector or polarization ellipse, as is detailed above. However in order to also describe the degree of polarization, one normally employs Stokes parameters (see below) to specify a state of partial polarization.:351,374–375
Motivation.
The transmission of plane waves through a homogeneous medium are fully described in terms of Jones vectors and 2×2 Jones matrices. However in practice there are cases in which all of the light cannot be viewed in such a simple manner due to spatial inhomogeneities or the presence of mutually incoherent waves. So-called depolarization, for instance, cannot be described using Jones matrices. For these cases it is usual instead to use a 4×4 matrix that acts upon the Stokes 4-vector. Such matrices were first used by Paul Soleillet in 1929, although they have come to be known as Mueller matrices. While every Jones matrix has a Mueller matrix, the reverse is not true. Mueller matrices are then used to describe the observed polarization effects of the scattering of waves from complex surfaces or ensembles of particles, as shall now be presented.:377–379
Coherency matrix.
The Jones vector perfectly describes the state of polarization "and phase" of a single monochromatic wave, representing a pure state of polarization as described above. However any mixture of waves of different polarizations (or even of different frequencies) do "not" correspond to a Jones vector. In so-called partially polarized radiation the fields are stochastic, and the variations and correlations between components of the electric field can only be described statistically. One such representation is the coherency matrix::137–142
where angular brackets denote averaging over many wave cycles. Several variants of the coherency matrix have been proposed: the Wiener coherency matrix and the spectral coherency matrix of Richard Barakat measure the coherence of a spectral decomposition of the signal, while the Wolf coherency matrix averages over all time/frequencies.
The coherency matrix contains all second order statistical information about the polarization. This matrix can be decomposed into the sum of two idempotent matrices, corresponding to the eigenvectors of the coherency matrix, each representing a polarization state that is orthogonal to the other. An alternative decomposition is into completely polarized (zero determinant) and unpolarized (scaled identity matrix) components. In either case, the operation of summing the components corresponds to the incoherent superposition of waves from the two components. The latter case gives rise to the concept of the "degree of polarization"; i.e., the fraction of the total intensity contributed by the completely polarized component.
Stokes parameters.
The coherency matrix is not easy to visualize, and it is therefore common to describe incoherent or partially polarized radiation in terms of its total intensity ("I"), (fractional) degree of polarization ("p"), and the shape parameters of the polarization ellipse. An alternative and mathematically convenient description is given by the Stokes parameters, introduced by George Gabriel Stokes in 1852. The relationship of the Stokes parameters to intensity and polarization ellipse parameters is shown in the equations and figure below.
Here "Ip", 2ψ and 2χ are the spherical coordinates of the polarization state in the three-dimensional space of the last three Stokes parameters. Note the factors of two before ψ and χ corresponding respectively to the facts that any polarization ellipse is indistinguishable from one rotated by 180°, or one with the semi-axis lengths swapped accompanied by a 90° rotation. The Stokes parameters are sometimes denoted "I", "Q", "U" and "V".
Poincaré sphere.
Neglecting the first Stokes parameter "S"0 (or "I"), the three other Stokes parameters can be plotted directly in three dimensional Cartesian coordinates. For a given power in the polarized component given by:
the set of all polarization states are then mapped to points on the surface of the so-called "Poincaré sphere" (but of radius "P"), as shown in the accompanying diagram.
Often the total beam power is not of interest, in which case a normalized Stokes vector is used by dividing the Stokes vector by the total intensity "S"0: 
The normalized Stokes vector formula_33 then has unity power (formula_34) and the three significant Stokes parameters plotted in three dimensions will lie on the unity-radius Poincaré sphere for pure polarization states (where formula_35). Partially polarized states will lie "inside" the Poincaré sphere at a distance of formula_36 from the origin. When the non-polarized component is not of interest, the Stokes vector can be further normalized to obtain 
When plotted, that point will lie on the surface of the unity-radius Poincaré sphere and indicate the state of polarization of the polarized component.
Any two antipodal points on the Poincaré sphere refer to orthogonal polarization states. The overlap between any two polarization states is dependent solely on the distance between their locations along the sphere. This property, which can only be true when pure polarization states are mapped onto a sphere, is the motivation for the invention of the Poincaré sphere and the use of Stokes parameters which are thus plotted on (or beneath) it.
Implications for reflection and propagation.
Polarization in wave propagation.
In a vacuum, the components of the electric field propagate at the speed of light, so that the phase of the wave varies in space and time while the polarization state does not. That is, the electric field vector e of a plane wave in the +"z" direction follows:
where "k" is the wavenumber. As noted above, the instantaneous electric field is the real part of the product of the Jones vector times the phase factor formula_39. When an electromagnetic wave interacts with matter, its propagation is altered according to the material's (complex) index of refraction. When the real or imaginary part of that refractive index is dependent on the polarization state of a wave, properties known as birefringence and polarization dichroism (or diattenuation) respectively, then the polarization state of a wave will generally be altered.
In such media, an electromagnetic wave with any given state of polarization may be decomposed into two orthogonally polarized components that encounter different propagation constants. The effect of propagation over a given path on those two components is most easily characterized in the form of a complex 2×2 transformation matrix J known as a Jones matrix:
The Jones matrix due to passage through a transparent material is dependent on the propagation distance as well as the birefringence. The birefringence (as well as the average refractive index) will generally be dispersive, that is, it will vary as a function of optical frequency (wavelength). In the case of non-birefringent materials, however, the 2×2 Jones matrix is the identity matrix (multiplied by a scalar phase factor and attenuation factor), implying no change in polarization during propagation.
For propagation effects in two orthogonal modes, the Jones matrix can be written as
where "g"1 and "g"2 are complex numbers
describing the phase delay and possibly the amplitude attenuation due to propagation in each of the two polarization eigenmodes. T is a unitary matrix representing a change of basis from these propagation modes to the linear system used for the Jones vectors; in the case of linear birefringence or diattenuation the modes are themselves linear polarization states so T and T−1 can be omitted if the coordinate axes have been chosen appropriately.
Birefringence.
In media termed birefringent, in which the amplitudes are unchanged but a differential phase delay occurs, the Jones matrix is a unitary matrix: |"g"1| = |"g"2| = 1. Media termed diattenuating (or "dichroic" in the sense of polarization), in which only the amplitudes of the two polarizations are affected differentially, may be described using a Hermitian matrix (generally multiplied by a common phase factor). In fact, since "any" matrix may be written as the product of unitary and positive Hermitian matrices, light propagation through any sequence of polarization-dependent optical components can be written as the product of these two basic types of transformations.
In birefringent media there is no attenuation but two modes accrue a differential phase delay. Well known manifestations of linear birefringence (that is, in which the basis polarizations are orthogonal linear polarizations) appear in optical wave plates/retarders and many crystals. If linearly polarized light passes through a birefringent material, its state of polarization will generally change "unless" its polarization direction is identical to one of those basis polarizations. Since the phase shift, and thus the change in polarization state, is usually wavelength dependent, such objects viewed under white light in between two polarizers may give rise to colorful effects, as seen in the accompanying photograph.
Circular birefringence is also termed optical activity especially in chiral fluids, or Faraday rotation when due to the presence of a magnetic field along the direction of propagation. When linearly polarized light is passed through such an object, it will exit still linearly polarized but with the axis of polarization rotated. A combination of linear and circular birefringence will have as basis polarizations two orthogonal elliptical polarizations; the term "elliptical birefringence" however is rarely used.
One can visualize the case of linear birefringence (with two orthogonal linear propagation modes) with an incoming wave linearly polarized at a 45° angle to those modes. As a differential phase starts to accrue, the polarization becomes elliptical, eventually changing to purely circular polarization (90° phase difference), then to elliptical and eventually linear polarization (180° phase) perpendicular to the original polarization, then through circular again (270° phase), then elliptical with the original azimuth angle, and finally back to the original linearly polarized state (360° phase) where the cycle begins anew. In general the situation is more complicated and can be characterized as a rotation in the Poincaré sphere about the axis defined by the propagation modes. Examples for linear (blue), circular (red), and elliptical (yellow) birefringence are shown in the figure on the left. The total intensity and degree of polarization are unaffected. If the path length in the birefringent medium is sufficient, the two polarization components of a collimated beam (or ray) can exit the material with a positional offset, even though their final propagation directions will be the same (assuming the entrance face and exit face are parallel). This is commonly viewed using calcite crystals, which present the viewer with two slightly offset images, in opposite polarizations, of an object behind the crystal. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669.
Dichroism.
Media in which transmission of one polarization mode is preferentially reduced are called "dichroic" or "diattenuating". Like birefringence, diattenuation can be with respect to linear polarization modes (in a crystal) or circular polarization modes (usually in a liquid).
Devices that block nearly all of the radiation in one mode are known as "polarizing filters" or simply "polarizers". This corresponds to "g"2=0 in the above representation of the Jones matrix. The output of an ideal polarizer is a specific polarization state (usually linear polarization) with an amplitude equal to the input wave's original amplitude in that polarization mode. Power in the other polarization mode is eliminated. Thus if unpolarized light is passed through an ideal polarizer (where "g"1=1 and "g"2=0) exactly half of its initial power is retained. Practical polarizers, especially inexpensive sheet polarizers, have additional loss so that
"g"1 < 1. However in many instances the more relevant figure of merit is the polarizer's degree of polarization or extinction ratio, which involve a comparison of "g"1 to "g"2. Since Jone's vectors refer to waves' amplitudes (rather than intensity), when illuminated by unpolarized light the remaining power in the unwanted polarization will be ("g"2/"g"1)2 of the power in the intended polarization.
Specular reflection.
In addition to birefringence and dichroism in extended media, polarization effects describable using Jones matrices can also occur at (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected; for a given material those proportions (and also the phase of reflection) are dependent on the angle of incidence and are different for the "s" and "p" polarizations. Therefore the polarization state of reflected light (even if initially unpolarized) is generally changed.
Any light striking a surface at a special angle of incidence known as Brewster's angle, where the reflection coefficient for "p" polarization is zero, will be reflected with only the "s"-polarization remaining. This principle is employed in the so-called "pile of plates polarizer" (see figure) in which part of the "s" polarization is removed by reflection at each Brewster angle surface, leaving only the "p" polarization after transmission through many such surfaces. The generally smaller reflection coefficient of the "p" polarization is also the basis of polarized sunglasses; by blocking the "s" (horizontal) polarization, most of the glare due to reflection from a wet street, for instance, is removed.:348–350
In the important special case of reflection at normal incidence (not involving anisotropic materials) there is no particular "s" or "p" polarization. Both the "x" and "y" polarization components are reflected identically, and therefore the polarization of the reflected wave is identical to that of the incident wave. However in the case of circular (or elliptical) polarization, the handedness of the polarization state is thereby reversed, since by convention this is specified relative to the direction of propagation. The circular rotation of the electric field around the "x-y" axes called "right-handed" for a wave in the "+z" direction is "left-handed" for a wave in the "-z" direction. But in the general case of reflection at a nonzero angle of incidence, no such generalization can be made. For instance, right-circularly polarized light reflected from a dielectric surface at a grazing angle, will still be right-handed (but elliptically) polarized. Linear polarized light reflected from a metal at non-normal incidence will generally become elliptically polarized. These cases are handled using Jones vectors acted upon by the different Fresnel coefficients for the "s" and "p" polarization components.
Measurement techniques involving polarization.
Some optical measurement techniques are based on polarization. In many other optical techniques polarization is crucial or at least must be taken into account and controlled; such examples are too numerous to mention.
Measurement of stress.
In engineering, the phenomenon of stress induced birefringence allows for stresses in transparent materials to be readily observed. As noted above and seen in the accompanying photograph, the chromaticity of birefringence typically creates colored patterns when viewed in between two polarizers. As external forces are applied, internal stress induced in the material is thereby observed. Additionally, birefringence is frequently observed due to stresses "frozen in" at the time of manufacture. This is famously observed in cellophane tape whose birefringence is due to the stretching of the material during the manufacturing process.
Ellipsometry.
Ellipsometry is a powerful technique for the measurement of the optical properties of a uniform surface. It involves measuring the polarization state of light following specular reflection from such a surface. This is typically done as a function of incidence angle or wavelength (or both). Since ellipsometry relies on reflection, it is not required for the sample to be transparent to light or for its back side to be accessible.
Ellipsometry can be used to model the (complex) refractive index of a surface of a bulk material. It is also very useful in determining parameters of one or more thin film layers deposited on a substrate. Due to their reflection properties, not only are the predicted magnitude of the "p" and "s" polarization components, but their relative phase shifts upon reflection, compared to measurements using an ellipsometer. A normal ellipsometer does not measure the actual reflection coefficient (which requires careful photometric calibration of the illuminating beam) but the ratio of the "p" and "s" reflections, as well as change of polarization ellipticity (hence the name) induced upon reflection by the surface being studied. In addition to use in science and research, ellipsometers are used in situ to control production processes for instance.:585ff:632
Geology.
The property of (linear) birefringence is widespread in crystalline minerals, and indeed was pivotal in the initial discovery of polarization. In mineralogy, this property is frequently exploited using polarization microscopes, for the purpose of identifying minerals. See optical mineralogy for more details.:163–164
Sound waves in solid materials exhibit polarization. Differential propagation of the three polarizations through the earth is a crucial in the field of seismology. Horizontally and vertically polarized seismic waves (shear waves)are termed SH and SV, while waves with longitudinal polarization (compressional waves) are termed P-waves.:48–50:56–57
Chemistry.
We have seen (above) that the birefringence of a type of crystal is useful in identifying it, and thus detection of linear birefringence is especially useful in geology and mineralogy. Linearly polarized light generally has its polarization state altered upon transmission through such a crystal, making it stand out when viewed in between two crossed polarizers, as seen in the photograph, above. Likewise, in chemistry, rotation of polarization axes in a liquid solution can be a useful measurement. In a liquid, linear birefringence is impossible, however there may be circular birefringence when a chiral molecule is in solution. When the right and left handed enantiomers of such a molecule are present in equal numbers (a so-called racemic mixture) then their effects cancel out. However when there is only one (or a preponderance of one), as is more often the case for organic molecules, a net circular birefringence (or "optical activity") is observed, revealing the magnitude of that imbalance (or the concentration of the molecule itself, when it can be assumed that only one enantiomer is present). This is measured using a polarimeter in which polarized light is passed through a tube of the liquid, at the end of which is another polarizer which is rotated in order to null the transmission of light through it.:360–365:169–172
Astronomy.
In many areas of astronomy, the study of polarized electromagnetic radiation from outer space is of great importance. Although not usually a factor in the thermal radiation of stars, polarization is also present in radiation from coherent astronomical sources (e.g. hydroxyl or methanol masers), and incoherent sources such as the large radio lobes in active galaxies, and pulsar radio radiation (which may, it is speculated, sometimes be coherent), and is also imposed upon starlight by scattering from interstellar dust. Apart from providing information on sources of radiation and scattering, polarization also probes the interstellar magnetic field via Faraday rotation.:119,124:336–337 The polarization of the cosmic microwave background is being used to study the physics of the very early universe. Synchrotron radiation is inherently polarised. It has been suggested that astronomical sources caused the chirality of biological molecules on Earth.
Applications and examples.
Polarized sunglasses.
Unpolarized light, after reflection at a specular (shiny) surface, generally obtains a degree of polarization. This phenomenon was observed in 1808 by the mathematician Étienne-Louis Malus after whom Malus' law is named. Polarizing sunglasses exploit this effect to reduce glare from reflections by horizontal surfaces, notably the road ahead viewed at a grazing angle.
Wearers of polarized sunglasses will occasionally observe inadvertent polarization effects such as color-dependent birefringent effects, for example in toughened glass (e.g., car windows) or items made from transparent plastics, in conjunction with natural polarization by reflection or scattering. The polarized light from LCD monitors (see below) is very conspicuous when these are worn.
Sky polarization and photography.
Polarization is observed in the light of the sky, as this is due to sunlight scattered by aerosols as it passes through the earth's atmosphere. The scattered light produces the brightness and color in clear skies. This partial polarization of scattered light can be used to darken the sky in photographs, increasing the contrast. This effect is most strongly observed at points on the sky making a 90° angle to the sun. Polarizing filters use these effects to optimize the results of photographing scenes in which reflection or scattering by the sky is involved.:346–347:495–499
Sky polarization has been used for orientation in navigation. The "sky compass", was used in the 1950s when navigating near the poles of the Earth's magnetic field when neither the sun nor stars were visible (e.g., under daytime cloud or twilight). It has been suggested, controversially, that the Vikings exploited a similar device (the "sunstone") in their extensive expeditions across the North Atlantic in the 9th–11th centuries, before the arrival of the magnetic compass in Europe in the 12th century. Related to the sky compass is the "polar clock", invented by Charles Wheatstone in the late 19th century.:67–69
Display technologies.
The principle of liquid-crystal display (LCD) technology relies on the rotation of the axis of linear polarization by the liquid crystal array. Light from the backlight (or the back reflective layer, in devices not including or requiring a backlight) first passes through a linear polarizing sheet. That polarized light passes through the actual liquid crystal layer which may be organized in pixels (for a TV or computer monitor) or in another format such as a seven-segment display or one with custom symbols for a particular product. The liquid crystal layer is produced with a consistent right (or left) handed chirality, essentially consisting of tiny helices. This causes circular birefringence, and is engineered so that there is a 90 degree rotation of the linear polarization state. However when a voltage is applied across a cell, the molecules straighten out, lessening or totally losing the circular birefringence. On the viewing side of the display is another linear polarizing sheet, usually oriented at 90 degrees from the one behind the active layer. Therefore when the circular birefringence is removed by the application of a sufficient voltage, the polarization of the transmitted light remains at right angles to the front polarizer, and the pixel appears dark. With no voltage, however, the 90 degree rotation of the polarization causes it to exactly match the axis of the front polarizer, allowing the light through. Intermediate voltages create intermediate rotation of the polarization axis and the pixel has an intermediate intensity. Displays based on this principle are widespread, and now are used in the vast majority of televisions, computer monitors and video projectors, rendering the previous CRT technology essentially obsolete. The use of polarization in the operation of LCD displays is immediately apparent to someone wearing polarized sunglasses, often making the display unreadable.
In a totally different sense, polarization encoding has become the leading (but not sole) method for delivering separate images to the left and right eye in stereoscopic displays used for 3D movies. This involves separate images intended for each eye either projected from two different projectors with orthogonally oriented polarizing filters or, more typically, from a single projector with time multiplexed polarization (a fast alternating polarization device for successive frames). Polarized 3D glasses with suitable polarizing filters ensure that each eye receives only the intended image. Historically such systems used linear polarization encoding because it was inexpensive and offered good separation. However circular polarization makes separation of the two images insensitive to tilting of the head, and is widely used in 3-D movie exhibition today, such as the system from RealD. Projecting such images requires screens that maintain the polarization of the projected light when viewed in reflection (such as silver screens); a normal diffuse white projection screen causes depolarization of the projected images, making it unsuitable for this application.
Although now obsolete, CRT computer displays suffered from reflection by the glass envelope, causing glare from room lights and consequently poor contrast. Several anti-reflection solutions were employed to ameliorate this problem. One solution utilized the principle of reflection of circularly polarized light. A circular polarizing filter in front of the screen allows for the transmission of (say) only right circularly polarized room light. Now, right circularly polarized light (depending on the convention used) has its electric (and magnetic) field direction rotating clockwise while propagating in the +z direction. Upon reflection, the field still has the same direction of rotation, but now propagation is in the −z direction making the reflected wave "left" circularly polarized. With the right circular polarization filter placed in front of the reflecting glass, the unwanted light reflected from the glass will thus be in very polarization state that is "blocked" by that filter, eliminating the reflection problem. The reversal of circular polarization on reflection and elimination of reflections in this manner can be easily observed by looking in a mirror while wearing 3-D movie glasses which employ left and right handed circular polarization in the two lenses. Closing one eye, the other eye will see a reflection in which it cannot see itself; that lens appears black! However the other lens (of the closed eye) will have the correct circular polarization allowing the closed eye to be easily seen by the open one.
Radio transmission.
All radio (and microwave) antennas used for transmitting or receiving are intrinsically polarized. They transmit in (or receive signals from) a particular polarization, being totally insensitive to the opposite polarization; in certain cases that polarization is a function of direction. As is the convention in optics, the "polarization" of a radio wave is understood to refer to the polarization of its electric field, with the magnetic field being at a 90 degree rotation with respect to it for a linearly polarized wave.
The vast majority of antennas are linearly polarized. In fact it can be shown from considerations of symmetry that an antenna that lies entirely in a plane which also includes the observer, can "only" have its polarization in the direction of that plane. This applies to many cases, allowing one to easily infer such an antenna's polarization at an intended direction of propagation. So a typical rooftop Yagi or log-periodic antenna with horizontal conductors, as viewed from a second station toward the horizon, is necessarily horizontally polarized. But a vertical "whip antenna" or AM broadcast tower used as an antenna element (again, for observers horizontally displaced from it) will transmit in the vertical polarization. A turnstile antenna with its four arms in the horizontal plane, likewise transmits horizontally polarized radiation toward the horizon. However when that same turnstile antenna is used in the "axial mode" (upwards, for the same horizontally-oriented structure) its radiation is circularly polarized. At intermediate elevations it is elliptically polarized.
Polarization is important in radio communications because, for instance, if one attempts to use a horizontally polarized antenna to receive a vertically polarized transmission, the signal strength will be substantially reduced (or under very controlled conditions, reduced to nothing). This principle is used in satellite television in order to double the channel capacity over a fixed frequency band. The same frequency channel can be used for two signals broadcast in opposite polarizations. By adjusting the receiving antenna for one or the other polarization, either signal can be selected without interference from the other.
Especially due to the presence of the ground, there are some differences in propagation (and also in reflections responsible for TV ghosting) between horizontal and vertical polarizations. AM and FM broadcast radio usually use vertical polarization, while television uses horizontal polarization. At low frequencies especially, horizontal polarization is avoided. That is because the phase of a horizontally polarized wave is reversed upon reflection by the ground. A distant station in the horizontal direction will receive both the direct and reflected wave, which thus tend to cancel each other. This problem is avoided with vertical polarization. Polarization is also important in the transmission of radar pulses and reception of radar reflections by the same or a different antenna. For instance, back scattering of radar pulses by rain drops can be avoided by using circular polarization. Just as specular reflection of circularly polarized light reverses the handedness of the polarization, as discussed above, the same principle applies to scattering by objects much smaller than a wavelength such as rain drops. On the other hand, reflection of that wave by an irregular metal object (such as an airplane) will typically introduce a change in polarization and (partial) reception of the return wave by the same antenna.
The effect of free electrons in the ionosphere, in conjunction with the earth's magnetic field, causes Faraday rotation, a sort of circular birefringence. This is the same mechanism which can rotate the axis of linear polarization by electrons in interstellar space as mentioned below. The magnitude of Faraday rotation caused by such a plasma is greatly exaggerated at lower frequencies, so at the higher microwave frequencies used by satellites the effect is minimal. However medium or short wave transmissions received following refraction by the ionosphere are strongly affected. Since a wave's path through the ionosphere and the earth's magnetic field vector along such a path are rather unpredictable, a wave transmitted with vertical (or horizontal) polarization will generally have a resulting polarization in an arbitrary orientation at the receiver.
Polarization and vision.
Many animals are capable of perceiving some of the components of the polarization of light, e.g., linear horizontally polarized light. This is generally used for navigational purposes, since the linear polarization of sky light is always perpendicular to the direction of the sun. This ability is very common among the insects, including bees, which use this information to orient their communicative dances.:102–103 Polarization sensitivity has also been observed in species of octopus, squid, cuttlefish, and mantis shrimp.:111–112 In the latter case, one species measures all six orthogonal components of polarization, and is believed to have optimal polarization vision. The rapidly changing, vividly colored skin patterns of cuttlefish, used for communication, also incorporate polarization patterns, and mantis shrimp are known to have polarization selective reflective tissue. Sky polarization was thought to be perceived by pigeons, which was assumed to be one of their aids in homing, but research indicates this is a popular myth.
The naked human eye is weakly sensitive to polarization, without the need for intervening filters. Polarized light creates a very faint pattern near the center of the visual field, called Haidinger's brush. This pattern is very difficult to see, but with practice one can learn to detect polarized light with the naked eye.:118
Angular momentum using circular polarization.
It is well known that electromagnetic radiation carries a certain linear momentum in the direction of propagation. In addition, however, light carries a certain angular momentum if it is circularly polarized (or partially so). In comparison with lower frequencies such as microwaves, the amount of angular momentum in light, even of pure circular polarization, compared to the same wave's linear momentum (or radiation pressure) is very small and difficult to even measure. However it was utilized in a remarkable experiment to achieve an incredibly high rotation speed.
The University of St Andrews team caused a microscopic bead of calcium carbonate 4 micrometres in diameter to rotate at speeds of up to 600 million revolutions per minute. The bead was suspended by a laser beam in a location using the principle of optical tweezers. However that beam was also circularly polarized. The calcium carbonate (calcite) bead, being birefringent, caused light transmitted through it to slightly change its polarization into one that was not fully circularly polarized, and which therefore had less angular momentum. The difference in the angular momentum between the incident beam and light transmitted through the bead was imparted to the bead itself. Suspended in a near-vacuum and facing little friction, the rotation rate of the bead could be increased to rates as high as 10 million revolutions per second. This rotation rate corresponded to a centrifugal acceleration some one billion times that of gravity on the Earth surface, but which surprisingly did not lead to the bead's disintegration.
Notes and references.
</dl>

</doc>
<doc id="41565" url="http://en.wikipedia.org/wiki?curid=41565" title="Polarization-maintaining optical fiber">
Polarization-maintaining optical fiber

In fiber optics, polarization-maintaining optical fiber (PMF or PM fiber) is a single-mode optical fiber in which linearly polarized light, if properly launched into the fiber, maintains a linear polarization during propagation, exiting the fiber in a specific linear polarization state; there is little or no cross-coupling of optical power between the two polarization modes. Such fiber is used in special applications where preserving polarization is essential.
Polarization crosstalk.
In an ordinary (non-polarization-maintaining) fiber, two polarization modes (say vertical and horizontal polarization) have the same nominal phase velocity due to the fiber's circular symmetry. However tiny amounts of random birefringence in such a fiber, or bending in the fiber, will cause a tiny amount of crosstalk from the vertical to the horizontal polarization mode. And since even a short portion of fiber, over which a tiny coupling coefficient may apply, is many thousands of wavelengths long, even that small coupling between the two polarization modes, applied coherently, can lead to a large power transfer to the horizontal mode, completely changing the wave's net state of polarization. Since that coupling coefficient was unintended and a result of arbitrary stress or bending applied to fiber, the output state of polarization will itself be random, and will vary as those stresses or bends vary; it will also vary with wavelength. 
Principle of operation.
Polarization-maintaining fibers work by "intentionally" introducing a systematic linear birefringence in the fiber, so that there are two well defined polarization modes which propagate along the fiber with very distinct phase velocities. The beat length Lb of such a fiber (for a particular wavelength) is the distance (typically a few millimeters) over which the wave in one mode will experience an additional delay of one wavelength compared to the other polarization mode. Thus a length Lb /2 of such fiber is equivalent to a half-wave plate. Now consider that there might be a random coupling between the two polarization states over a significant length of such fiber. At point 0 along the fiber, the wave in polarization mode 1 induces an amplitude into mode 2 at some phase. However at point 1/2 Lb along the fiber, the same coupling coefficient between the polarization modes induces an amplitude into mode 2 which is now 180 degrees "out of phase" with the wave coupled at point zero, leading to cancellation. At point Lb along the fiber the coupling is again in the original phase, but at 3/2 Lb it is again out of phase and so on. The possibility of coherent addition of wave amplitudes through crosstalk over distances much larger than Lb is thus eliminated. Most of the wave's power remains in the original polarization mode, and exits the fiber in that mode's polarization as it is oriented at the fiber end. Optical fiber connectors used for PM fibers are specially keyed so that the two polarization modes are aligned and exit in a specific orientation.
Note that a polarization-maintaining fiber does not polarize light as a polarizer does. Rather, PM fiber maintains the linear polarization of linearly polarized light provided that it is launched into the fiber aligned with one of the fiber's polarization modes. Launching linearly polarized light into the fiber at a different angle will excite both polarization modes, conducting the same wave at a slightly different phase velocities. At most points along the fiber the net polarization will be an elliptically polarized state, with a return to the original polarization state after an integer number of beat lengths. Consequently, if visible laser light is launched into the fiber exciting both polarization modes, scattering of propagating light viewed from the side, is observed with a light and dark pattern periodic over each beat length, since scattering is preferentially perpendicular to the polarization direction.
Designs.
Several different designs are used to create birefringence in a fiber. The fiber may be geometrically asymmetric or have a refractive index profile which is asymmetric such as the design using an elliptical cladding as shown in the diagram. Alternatively, stress permanently induced in the fiber will produce stress birefringence; this may be accomplished using rods of another material included within the cladding. Several different shapes of rod are used, and the resulting fiber is sold under brand names such as "Panda" and "Bow-tie".
It is possible to create a circularly birefringent optical fiber just using an ordinary (circularly symmetric) single-mode fiber and twisting it, thus creating internal torsional stress. That causes the phase velocity of right and left hand circular polarizations to significantly differ. Thus the two circular polarizations propagate with little crosstalk in between them
Applications.
Polarization-maintaining optical fibers are used in special applications, such as in fiber optic sensing, interferometry and quantum key distribution. They are also commonly used in telecommunications for the connection between a source laser and a modulator, since the modulator requires polarized light as input. They are rarely used for long-distance transmission, because PM fiber is expensive and has higher attenuation than singlemode fiber. 
The output of a PM fiber is typically characterized by its polarization extinction ratio (PER)—the ratio of correctly to incorrectly polarized light, expressed in decibels. The quality of PM patchcords and pigtails can be characterized with a PER meter.

</doc>
<doc id="41566" url="http://en.wikipedia.org/wiki?curid=41566" title="Polling, Mühldorf">
Polling, Mühldorf

Polling is a municipality in the district of Mühldorf in Bavaria in Germany.

</doc>
<doc id="41567" url="http://en.wikipedia.org/wiki?curid=41567" title="Power budget">
Power budget

In telecommunication, a power budget (or system budget) is the allocation, within a system, of available transmitter power output to achieve the desired effective radiated power, among the various functions that need to be performed. 
An example of a power budget in a communications satellite is the allocation of available power among various functions, such as maintaining satellite orientation, maintaining orbital control, performing signal reception, and performing signal transmission. 

</doc>
<doc id="41568" url="http://en.wikipedia.org/wiki?curid=41568" title="Power factor">
Power factor

In electrical engineering, the power factor of an AC electrical power system is defined as the ratio of the real power flowing to the load, to the apparent power in the circuit, and is a dimensionless number in the closed interval of -1 to 1, meaning that the voltage & current waveforms are not in phase, reducing the instantaneous product of the two waveforms (V x I). Real power is the capacity of the circuit for performing work in a particular time. Apparent power is the product of the current and voltage of the circuit. Due to energy stored in the load and returned to the source, or due to a non-linear load that distorts the wave shape of the current drawn from the source, the apparent power will be greater than the real power. A negative power factor occurs when the device (which is normally the load) generates power, which then flows back towards the source, which is normally considered the generator.
In an electric power system, a load with a low power factor draws more current than a load with a high power factor for the same amount of useful power transferred. The higher currents increase the energy lost in the distribution system, and require larger wires and other equipment. Because of the costs of larger equipment and wasted energy, electrical utilities will usually charge a higher cost to industrial or commercial customers where there is a low power factor.
Linear loads with low power factor (such as induction motors) can be corrected with a passive network of capacitors or inductors. Non-linear loads, such as rectifiers, distort the current drawn from the system. In such cases, active or passive power factor correction may be used to counteract the distortion and raise the power factor. The devices for correction of the power factor may be at a central substation, spread out over a distribution system, or built into power-consuming equipment.
Linear circuits.
In a purely resistive AC circuit, voltage and current waveforms are in step (or in phase), changing polarity at the same instant in each cycle. All the power entering the load is consumed (or dissipated). Where reactive loads are present, such as with capacitors or inductors, energy storage in the loads results in a phase difference between the current and voltage waveforms. During each cycle of the AC voltage, extra energy, in addition to any energy consumed in the load, is temporarily stored in the load in electric or magnetic fields, and then returned to the power grid a fraction of the period later. The "ebb and flow" of this nonproductive power increases the current in the line. Thus, a circuit with a low power factor will use higher currents to transfer a given quantity of real power than a circuit with a high power factor. A linear load does not change the shape of the waveform of the current, but may change the relative timing (phase) between voltage and current.
Circuits containing purely resistive heating elements (filament lamps, cooking stoves, etc.) have a power factor of 1. Circuits containing inductive or capacitive elements (electric motors, solenoid valves, lamp ballasts, and others) often have a power factor below 1.
Definition and calculation.
AC power flow has three components: real power (also known as active power) ("P"), expressed in watts (W); apparent power ("S"), customarily expressed in volt-amperes (VA); and reactive power ("Q"), customarily expressed in reactive volt-amperes (var). The VA and var are non-SI units mathematically identical to the watt, but are used in engineering practice instead of the watt in order to state what quantity is being expressed. The SI explicitly disallows using units for this purpose, or as the only source of information about a physical quantity as used.
The power factor is defined as:
In the case of a perfectly sinusoidal waveform, P, Q and S can be expressed as vectors that form a vector triangle such that: 
If formula_3 is the phase angle between the current and voltage, then the power factor is equal to the cosine of the angle, formula_4, and:
Since the units are consistent, the power factor is by definition a dimensionless number between −1 and 1. When power factor is equal to 0, the energy flow is entirely reactive, and stored energy in the load returns to the source on each cycle. When the power factor is 1, all the energy supplied by the source is consumed by the load. Power factors are usually stated as "leading" or "lagging" to show the sign of the phase angle. Capacitive loads are leading (current leads voltage), and inductive loads are lagging (current lags voltage).
If a purely resistive load is connected to a power supply, current and voltage will change polarity in step, the power factor will be unity (1), and the electrical energy flows in a single direction across the network in each cycle. Inductive loads such as transformers and motors (any type of wound coil) consume reactive power with current waveform lagging the voltage. Capacitive loads such as capacitor banks or buried cable generate reactive power with current phase leading the voltage. Both types of loads will absorb energy during part of the AC cycle, which is stored in the device's magnetic or electric field, only to return this energy back to the source during the rest of the cycle.
For example, to get 1 kW of real power, if the power factor is unity, 1 kVA of apparent power needs to be transferred (1 kW ÷ 1 = 1 kVA). At low values of power factor, more apparent power needs to be transferred to get the same real power. To get 1 kW of real power at 0.2 power factor, 5 kVA of apparent power needs to be transferred (1 kW ÷ 0.2 = 5 kVA). This apparent power must be produced and transmitted to the load in the conventional fashion, and is subject to the usual distributed losses in the production and transmission processes.
Electrical loads consuming alternating current power consume both real power and reactive power. The vector sum of real and reactive power is the apparent power. The presence of reactive power causes the real power to be less than the apparent power, and so, the electric load has a power factor of less than 1.
A negative power factor (0 to -1) can result from returning power to the source, such as in the case of a building fitted with solar panels when their power is not being fully utilised within the building and the surplus is fed back into the supply.
Power factor correction of linear loads.
A high power factor is generally desirable in a transmission system to reduce transmission losses and improve voltage regulation at the load. It is often desirable to adjust the power factor of a system to near 1.0. When reactive elements supply or absorb reactive power near the load, the apparent power is reduced. Power factor correction may be applied by an electric power transmission utility to improve the stability and efficiency of the transmission network. Individual electrical customers who are charged by their utility for low power factor may install correction equipment to reduce those costs.
Power factor correction brings the power factor of an AC power circuit closer to 1 by supplying reactive power of opposite sign, adding capacitors or inductors that act to cancel the inductive or capacitive effects of the load, respectively. For example, the inductive effect of motor loads may be offset by locally connected capacitors. If a load had a capacitive value, inductors (also known as "reactors" in this context) are connected to correct the power factor. In the electricity industry, inductors are said to "consume" reactive power and capacitors are said to "supply" it, even though the energy is just moving back and forth on each AC cycle.
The reactive elements can create voltage fluctuations and harmonic noise when switched on or off. They will supply or sink reactive power regardless of whether there is a corresponding load operating nearby, increasing the system's no-load losses. In the worst case, reactive elements can interact with the system and with each other to create resonant conditions, resulting in system instability and severe overvoltage fluctuations. As such, reactive elements cannot simply be applied without engineering analysis.
An automatic power factor correction unit consists of a number of capacitors that are switched by means of contactors. These contactors are controlled by a regulator that measures power factor in an electrical network. Depending on the load and power factor of the network, the power factor controller will switch the necessary blocks of capacitors in steps to make sure the power factor stays above a selected value.
Instead of using a set of switched capacitors, an unloaded synchronous motor can supply reactive power. The reactive power drawn by the synchronous motor is a function of its field excitation. This is referred to as a "synchronous condenser". It is started and connected to the electrical network. It operates at a leading power factor and puts vars onto the network as required to support a system's voltage or to maintain the system power factor at a specified level.
The condenser's installation and operation are identical to large electric motors. Its principal advantage is the ease with which the amount of correction can be adjusted; it behaves like an electrically variable capacitor. Unlike capacitors, the amount of reactive power supplied is proportional to voltage, not the square of voltage; this improves voltage stability on large networks. Synchronous condensers are often used in connection with high-voltage direct-current transmission projects or in large industrial plants such as steel mills.
For power factor correction of high-voltage power systems or large, fluctuating industrial loads, power electronic devices such as the Static VAR compensator or STATCOM are increasingly used. These systems are able to compensate sudden changes of power factor much more rapidly than contactor-switched capacitor banks, and being solid-state require less maintenance than synchronous condensers.
Non-linear loads.
Examples of non-linear loads on a power system are rectifiers (such as used in a power supply), and arc discharge devices such as fluorescent lamps, electric welding machines, or arc furnaces. Because current in these systems is interrupted by a switching action, the current contains frequency components that are multiples of the power system frequency. Distortion power factor is a measure of how much the harmonic distortion of a load current decreases the average power transferred to the load.
Non-sinusoidal components.
Non-linear loads change the shape of the current waveform from a sine wave to some other form. Non-linear loads create harmonic currents in addition to the original (fundamental frequency) AC current. Filters consisting of linear capacitors and inductors can prevent harmonic currents from entering the supplying system.
In linear circuits having only sinusoidal currents and voltages of one frequency, the power factor arises only from the difference in phase between the current and voltage. This is "displacement power factor". The concept can be generalized to a total, distortion, or true power factor where the apparent power includes all harmonic components. This is of importance in practical power systems that contain non-linear loads such as rectifiers, some forms of electric lighting, electric arc furnaces, welding equipment, switched-mode power supplies and other devices.
A typical multimeter will give incorrect results when attempting to measure the AC current drawn by a non-sinusoidal load; the instruments sense the average value of a rectified waveform. The average response is then calibrated to the effective, RMS value. An RMS sensing multimeter must be used to measure the actual RMS currents and voltages (and therefore apparent power). To measure the real power or reactive power, a watt meter designed to work properly with non-sinusoidal currents must be used.
Distortion power factor.
The "distortion power factor" describes how the harmonic distortion of a load current decreases the average power transferred to the load. 
formula_7 is the total harmonic distortion of the load current. formula_8 is the fundamental component of the current and formula_9 is the total current – both are root mean square-values (distortion power factor can also be used to describe individual order harmonics, using the corresponding current in place of total current). This definition with respect to total harmonic distortion assumes that the voltage stays undistorted (sinusoidal, without harmonics). This simplification is often a good approximation for stiff voltage sources (not being affected by changes in load downstream in the distribution network). Total harmonic distortion of typical generators from current distortion in the network is on the order of 1–2%, which can have larger scale implications but can be ignored in common practice.
The result when multiplied with the displacement power factor (DPF) is the overall, true power factor or just power factor (PF):
Distortion in three-phase networks.
In practice, the local effects of distortion current on devices in a three-phase distribution network rely on the magnitude of certain order harmonics rather than the total harmonic distortion.
For example, the triplen, or zero-sequence, harmonics (3rd, 9th, 15th, etc.) have the property of being in-phase when compared line-to-line. In a delta-wye transformer, these harmonics can result in circulating currents in the delta windings and result in greater resistive heating. In a wye-configuration of a transformer, triplen harmonics will not create these currents, but they will result in a non-zero current in the neutral wire. This could overload the neutral wire in some cases 
and create error in kilowatt-hour metering systems and billing revenue. The presence of current harmonics in a transformer also result in larger eddy currents in the magnetic core of the transformer. Eddy current losses generally increase as the square of the frequency, lowering the transformer's efficiency, dissipating additional heat, and reducing its service life.
Negative-sequence harmonics (5th, 11th, 17th, etc.) combine 120 degrees out of phase, similarly to the fundamental harmonic but in a reversed sequence. In generators and motors, these currents produce magnetic fields which oppose the rotation of the shaft and sometimes result in damaging mechanical vibrations.
Switched-mode power supplies.
A particularly important class of non-linear loads is the millions of personal computers that typically incorporate switched-mode power supplies (SMPS) with rated output power ranging from a few watts to more than 1 kW. Historically, these very-low-cost power supplies incorporated a simple full-wave rectifier that conducted only when the mains instantaneous voltage exceeded the voltage on the input capacitors. This leads to very high ratios of peak-to-average input current, which also lead to a low distortion power factor and potentially serious phase and neutral loading concerns.
A typical switched-mode power supply first makes a DC bus, using a bridge rectifier or similar circuit. The output voltage is then derived from this DC bus. The problem with this is that the rectifier is a non-linear device, so the input current is highly non-linear. That means that the input current has energy at harmonics of the frequency of the voltage.
This presents a particular problem for the power companies, because they cannot compensate for the harmonic current by adding simple capacitors or inductors, as they could for the reactive power drawn by a linear load. Many jurisdictions are beginning to legally require power factor correction for all power supplies above a certain power level.
Regulatory agencies such as the EU have set harmonic limits as a method of improving power factor. Declining component cost has hastened implementation of two different methods. To comply with current EU standard EN61000-3-2, all switched-mode power supplies with output power more than 75 W must include passive power factor correction, at least. 80 Plus power supply certification requires a power factor of 0.9 or more.
Power factor correction (PFC) in non-linear loads.
Passive PFC.
The simplest way to control the harmonic current is to use a filter that passes current only at line frequency (50 or 60 Hz). The filter consists of capacitors or inductors, and makes a non-linear device looks more like a linear load. An example of passive PFC is a valley-fill circuit.
A disadvantage of passive PFC is that it requires larger inductors or capacitors than an equivalent power active PFC circuit. Also, in practice, passive PFC is often less effective at improving the power factor.
Active PFC.
Active PFC is the use of power electronics to change the waveform of current drawn by a load to improve the power factor. Some types of the active PFC are boost, buck, buck-boost and synchronous condenser. Active power factor correction can be single-stage or multi-stage.
In the case of a switched-mode power supply, a boost converter is inserted between the bridge rectifier and the main input capacitors. The boost converter attempts to maintain a constant DC bus voltage on its output while drawing a current that is always in phase with and at the same frequency as the line voltage. Another switched-mode converter inside the power supply produces the desired output voltage from the DC bus. This approach requires additional semiconductor switches and control electronics, but permits cheaper and smaller passive components. It is frequently used in practice. For example, SMPS with passive PFC can achieve power factor of about 0.7–0.75, SMPS with active PFC, up to 0.99 power factor, while a SMPS without any power factor correction has a power factor of only about 0.55–0.65.
Due to their very wide input voltage range, many power supplies with active PFC can automatically adjust to operate on AC power from about 100 V (Japan) to 230 V (Europe). That feature is particularly welcome in power supplies for laptops.
Dynamic PFC.
Dynamic power factor correction (DPFC), sometimes referred to as "real-time power factor correction," is used for electrical stabilization in cases of rapid load changes (e.g. at large manufacturing sites). DPFC is useful when standard power factor correction would cause over or under correction. DPFC uses semiconductor switches, typically thyristors, to quickly connect and disconnect capacitors or inductors from the network in order to improve power factor.
Importance of power factor in distribution systems.
Power factors below 1.0 require a utility to generate more than the minimum volt-amperes necessary to supply the real power (watts). This increases generation and transmission costs. For example, if the load power factor were as low as 0.7, the apparent power would be 1.4 times the real power used by the load. Line current in the circuit would also be 1.4 times the current required at 1.0 power factor, so the losses in the circuit would be doubled (since they are proportional to the square of the current). Alternatively all components of the system such as generators, conductors, transformers, and switchgear would be increased in size (and cost) to carry the extra current.
Utilities typically charge additional costs to commercial customers who have a power factor below some limit, which is typically 0.9 to 0.95. Engineers are often interested in the power factor of a load as one of the factors that affect the efficiency of power transmission.
With the rising cost of energy and concerns over the efficient delivery of power, active PFC has become more common in consumer electronics. Current Energy Star guidelines for computers call for a power factor of ≥ 0.9 at 100% of rated output in the PC's power supply. According to a white paper authored by Intel and the U.S. Environmental Protection Agency, PCs with internal power supplies will require the use of active power factor correction to meet the ENERGY STAR 5.0 Program Requirements for Computers.
In Europe, EN 61000-3-2 requires power factor correction be incorporated into consumer products.
Measuring the power factor.
The power factor in a single-phase circuit (or balanced three-phase circuit) can be measured with the wattmeter-ammeter-voltmeter method, where the power in watts is divided by the product of measured voltage and current. The power factor of a balanced polyphase circuit is the same as that of any phase. The power factor of an unbalanced poly phase circuit is not uniquely defined.
A direct reading power factor meter can be made with a moving coil meter of the electrodynamic type, carrying two perpendicular coils on the moving part of the instrument. The field of the instrument is energized by the circuit current flow. The two moving coils, A and B, are connected in parallel with the circuit load. One coil, A, will be connected through a resistor and the second coil, B, through an inductor, so that the current in coil B is delayed with respect to current in A. At unity power factor, the current in A is in phase with the circuit current, and coil A provides maximum torque, driving the instrument pointer toward the 1.0 mark on the scale. At zero power factor, the current in coil B is in phase with circuit current, and coil B provides torque to drive the pointer towards 0. At intermediate values of power factor, the torques provided by the two coils add and the pointer takes up intermediate positions.
Another electromechanical instrument is the polarized-vane type. In this instrument a stationary field coil produces a rotating magnetic field, just like a polyphase motor. The field coils are connected either directly to polyphase voltage sources or to a phase-shifting reactor if a single-phase application. A second stationary field coil, perpendicular to the voltage coils, carries a current proportional to current in one phase of the circuit. The moving system of the instrument consists of two vanes that are magnetized by the current coil. In operation the moving vanes take up a physical angle equivalent to the electrical angle between the voltage source and the current source. This type of instrument can be made to register for currents in both directions, giving a four-quadrant display of power factor or phase angle.
Digital instruments can be made that either directly measure the time lag between voltage and current waveforms and so calculate the power factor, or that measure both true and apparent power in the circuit and calculate the quotient. The first method is only accurate if voltage and current are sinusoidal; loads such as rectifiers distort the waveforms from the sinusoidal shape.
Mnemonics.
English-language power engineering students are advised to remember: 
"ELI the ICE man" or "ELI on ICE" – the voltage E leads the current I in an inductor L; the current leads the voltage in a capacitor C. 
Another common mnemonic is CIVIL – in a capacitor (C) the current (I) leads voltage (V), voltage (V) leads current (I) in an inductor (L).

</doc>
<doc id="41569" url="http://en.wikipedia.org/wiki?curid=41569" title="Power failure transfer">
Power failure transfer

In telecommunication, the term power failure transfer has the following meanings:
Power-failure transfer is an emergency mode of operation in which one and only one instrument may be powered from each trunk line from the subscriber location to the central office.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41570" url="http://en.wikipedia.org/wiki?curid=41570" title="Power-law index profile">
Power-law index profile

For optical fibers, a power-law index profile is an index of refraction profile characterized by 
where
formula_2
and formula_3 is the nominal refractive index as a function of distance from the fiber axis, formula_4 is the nominal refractive index on axis, formula_5 is the refractive index of the cladding, which is taken to be homogeneous (formula_6), formula_7 is the core radius, and formula_8 is a parameter that defines the shape of the profile. formula_7 is often used in place of formula_8. Hence, this is sometimes called an alpha profile. 
For this class of profiles, multimode distortion is smallest when formula_8 takes a particular value depending on the material used. For most materials, this optimum value is approximately 2. In the limit of infinite formula_8, the profile becomes a step-index profile.

</doc>
<doc id="41571" url="http://en.wikipedia.org/wiki?curid=41571" title="Power margin">
Power margin

In telecommunication, the power margin is the difference between available signal power and the minimum signal power needed to overcome system losses and still satisfy the minimum input requirements of the receiver for a given performance level. 
System power margin reflects the excess signal level, present at the input of the receiver, that is available to compensate for (a) the effects of component aging in the transmitter, receiver, or physical transmission medium, and (b) a deterioration in propagation conditions. "Synonym" system power margin.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41572" url="http://en.wikipedia.org/wiki?curid=41572" title="Precision">
Precision


</doc>
<doc id="41574" url="http://en.wikipedia.org/wiki?curid=41574" title="Preemphasis improvement">
Preemphasis improvement

In FM broadcasting, preemphasis improvement is the improvement in the signal-to-noise ratio of the high-frequency portion of the baseband, "i.e.," modulating, signal, which improvement results from passing the modulating signal through a preemphasis network. 
Preemphasis increases the magnitude of the higher signal frequencies, thereby improving the signal-to-noise ratio. At the output of the discriminator in the FM receiver, a deemphasis network restores the original signal power distribution.
"FM improvement factor" is the quotient obtained by dividing the signal-to-noise ratio (SNR) at the output of an FM receiver by the carrier-to-noise ratio (CNR) at the input of the receiver. When the FM improvement factor is greater than unity, the improvement in the SNR is always obtained at the expense of an increased bandwidth in the receiver and the transmission path. 
"FM improvement threshold" is the point in an FM (frequency modulation) receiver at which the peaks in the RF signal equal the peaks of the thermal noise generated in the receiver. A baseband signal-to-noise ratio of about 30 dB is typical at the improvement threshold, and this ratio improves 1 dB for each decibel of increase in the signal above the threshold.

</doc>
<doc id="41576" url="http://en.wikipedia.org/wiki?curid=41576" title="Preventive maintenance">
Preventive maintenance

Preventive maintenance (PM) has the following meanings:
The primary goal of maintenance is to avoid or mitigate the consequences of failure of equipment. This may be by preventing the failure before it actually occurs which Planned Maintenance and Condition Based Maintenance help to achieve. It is designed to preserve and restore equipment reliability by replacing worn components before they actually fail. Preventive maintenance activities include partial or complete overhauls at specified periods, oil changes, lubrication and so on. In addition, workers can record equipment deterioration so they know to replace or repair worn parts before they cause system failure. The ideal preventive maintenance program would prevent all equipment failure before it occurs.
There is a controversy of sorts regarding the propriety of the usage “preventative.”
References.
</dl>

</doc>
<doc id="41577" url="http://en.wikipedia.org/wiki?curid=41577" title="Primary channel">
Primary channel

In telecommunication, the term primary channel has the following meanings: 
A primary channel may support the transfer of information in one direction only, either direction alternately, or both directions simultaneously.

</doc>
<doc id="41578" url="http://en.wikipedia.org/wiki?curid=41578" title="Primary Rate Interface">
Primary Rate Interface

The Primary Rate Interface (PRI) is a telecommunications interface standard used on an Integrated Services Digital Network (ISDN) for carrying multiple DS0 voice and data transmissions between the network and a user.
PRI is the standard for providing telecommunication services to offices. It is based on the T-carrier (T1) line in the US and Canada, and the E-carrier (E1) line in Europe. The T1 line consists of 24 channels, while an E1 has 32.
PRI provides a varying number of channels depending on the standards in the country of implementation. In North America and Japan it consists of 23xB (B channels (bearer channels)) and 1xD (D channel (delta channel)) (23 64-kbit/s digital channels + 1 64-kbit/s signaling/control channel) on a T1 (1.544 Mbit/s). In Europe and Australia it is 30xB + 2xD on an E1 2.048 Mbit/s. One timeslot on the E1 is used for synchronization purposes and is not considered to be a B or D channel.
Fractional T1.
Fewer active B channels (also called bearer channels) can be used for a fractional T1. Bearer channels may also be known as user channels. More channels can be used with more T1s, within certain design limits.
PRI and BRI.
The Integrated Services Digital Network (ISDN) prescribes two levels of service:
Each B-channel carries data, voice, and other services. The D-channel carries control and signaling information. Larger connections are possible using PRI pairing. A dual T1-PRI could have 24 + 23 = 47 B-channels and 1 D-channel (often called "47B + D"), but more commonly has 46 B-channels and 2 D-channels thus providing a backup signaling channel. The concept applies to E1s as well and both can include more than 2 PRIs. When configuring multiple T1's as ISDN-PRI's, it's possible to use NFAS (non-facility associated signalling) to enable one or two D-channels to support additional B-channels on separate T1 circuits.
Application.
The Primary Rate Interface channels are typically used by medium to large enterprises with digital PBXs to provide them digital access to the Public Switched Telephone Network (PSTN). The 23 (or 30) B-channels can be used flexibly and reassigned when necessary to meet special needs such as video conferences. The Primary Rate user is hooked up directly to the telephone company central office.
PRI channels and direct inward dialing are also common as a means of delivering inbound calls to voice over IP gateways from the PSTN.
ISDN PRI variants (switch types).
 primary-ni2 National ISDN-2 Switch type for the U.S. 
 primary-4ess Lucent 4ESS switch type for the U.S.
 primary-5ess Lucent 5ESS switch type for the U.S.
 primary-dms100 Northern Telecom DMS-100 switch type for the U.S.
 primary-dpnss DPNSS switch type for Europe
 primary-net5 NET5 switch type for UK, Europe, Asia and Australia
 primary-ni National ISDN-1 Switch type for the U.S.
 primary-ni2c The Cisco NAS-SC switchtype based on NI2C
 primary-ntt Japanese ISDN PRI switches
 primary-qsig QSIG switch type
 primary-ts014 TS014 switch type for Australia (obsolete)
 primary 32 Channels
^^(The above is a bastardized list, mostly taken from a cisco router, the "primary-" is mainly found on their equipment and is not truly part of the protocol name. The ni2, while being a true protocol option, is not found amongst the configuration options on the cisco, primary-ni is the setting on the cisco which supports the NI2 protocol. dms100 is in actuality NI1)

</doc>
<doc id="41579" url="http://en.wikipedia.org/wiki?curid=41579" title="Primary station">
Primary station

In a data communication network, the primary station is the station responsible for unbalanced control of a data link. 
The primary station generates commands and interprets responses, and is responsible for initialization of data and control information interchange, organization and control of data flow, retransmission control, and all recovery functions at the link level.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41580" url="http://en.wikipedia.org/wiki?curid=41580" title="Primary time standard">
Primary time standard

In telecommunications, a primary time standard is a time standard that does not require calibration against another time standard. 
Examples of primary time, ("i.e.", frequency standards) are caesium standards and hydrogen masers. 
The international second is based on the microwave frequency (9,192,631,770 Hz) associated with the atomic resonance of the hyperfine ground state levels of the caesium-133 atom in a magnetically neutral environment. Realizable caesium frequency standards use a strong electromagnet to deliberately introduce a magnetic field which overwhelms that of the Earth. The presence of this strong magnetic field introduces a slight, but known, increase in the atomic resonance frequency. However, very small variations in the calibration of the electric current in the electromagnet introduce minuscule frequency variations among different caesium oscillators.

</doc>
<doc id="41581" url="http://en.wikipedia.org/wiki?curid=41581" title="Principal clock">
Principal clock

In telecommunications, the principal clock of a set of redundant clocks, is the clock that is selected for normal use. The principal clock may be selected because of a property, "e.g." superior accuracy, that makes it a unique member of the set. 
The term "principal clock" should not be confused with, or used as a synonym for, the term "primary frequency standard."

</doc>
<doc id="41582" url="http://en.wikipedia.org/wiki?curid=41582" title="Priority">
Priority

Priority may refer to:

</doc>
<doc id="41583" url="http://en.wikipedia.org/wiki?curid=41583" title="Priority level">
Priority level

Priority level or priority, in the Telecommunications Service Priority system, is the level that may be assigned to an NS/EP telecommunications service, which level specifies the order in which provisioning or restoration of the service is to occur relative to other NS/EP or non-NS/EP telecommunication services. 
Priority levels authorized are designated (highest to lowest) "E," "1," "2," "3," "4," and "5" for provisioning and "1," "2," "3," "4," and "5" for restoration. 

</doc>
<doc id="41584" url="http://en.wikipedia.org/wiki?curid=41584" title="Private line">
Private line

In wired telephony, a private line or tie line is a service that involves dedicated circuits, private switching arrangements, and/or predefined transmission paths, whether virtual or physical, which provide communications between specific locations. Most private lines connect only two locations though they may be switched at either end or both. Some have multiple drop points. 
"Note:" Among subscribers to the public switched telephone network(s), the term "private line" is often used to mean a one-party switched telephone line, as opposed to a party line.
CTCSS.
In radio or wireless telephony, "Private Line" is a term trademarked by Motorola to describe their implementation of a Continuous Tone Coded Squelch System (CTCSS), a method of using low frequency subaudible tones to share a single radio channel among multiple users. Each user group would use a different low frequency tone. Motorola's trade name (especially the abbreviation "PL") has become a genericized trademark for the method. General Electric used the term 'Channel Guard' to describe the same system and other manufacturers used other terms. A later digital version of Private Line is called Digital Private Line, or DPL.

</doc>
<doc id="41585" url="http://en.wikipedia.org/wiki?curid=41585" title="Proceed-to-select">
Proceed-to-select

In telecommunications, proceed-to-select is a signal or event in the call-access phase of a data call, which signal or event confirms the reception of a call-request signal and advises the calling data terminal equipment to proceed with the transmission of the selection signals. 
Examples of proceed-to-select pertain to a dial tone in a telephone system.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41586" url="http://en.wikipedia.org/wiki?curid=41586" title="Propagation constant">
Propagation constant

The propagation constant of an electromagnetic wave is a measure of the change undergone by the amplitude of the wave as it propagates in a given direction. The quantity being measured can be the voltage or current in a circuit or a field vector such as electric field strength or flux density. The propagation constant itself measures change per metre but is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.
The propagation constant is expressed logarithmically, almost universally to the base e, rather than the more usual base 10 used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.
Alternative names.
The term propagation constant is somewhat of a misnomer as it usually varies strongly with "ω". It is probably the most widely used term but there are a large variety of alternative names used by various authors for this quantity. These include, transmission parameter, transmission function, propagation parameter, propagation coefficient and transmission constant. In plural, it is usually implied that "α" and "β" are being referenced separately but collectively as in transmission parameters, propagation parameters, propagation coefficients, transmission constants and secondary coefficients. This last occurs in transmission line theory, the term "secondary" being used to contrast to the "primary line coefficients". The primary coefficients being the physical properties of the line; R,C,L and G, from which the secondary coefficients may be derived using the telegrapher's equation. Note that, at least in the field of transmission lines, the term transmission coefficient has a different meaning despite the similarity of name. Here it is the corollary of reflection coefficient.
Definition.
The propagation constant, symbol γ, for a given system is defined by the ratio of the amplitude at the source of the wave to the amplitude at some distance "x", such that,
Since the propagation constant is a complex quantity we can write:
where
That β does indeed represent phase can be seen from Euler's formula;
which is a sinusoid which varies in phase as θ varies but does not vary in amplitude because;
The reason for the use of base e is also now made clear. The imaginary phase constant, iβ, can be added directly to the attenuation constant, α, to form a single complex number that can be handled in one mathematical operation provided they are to the same base. Angles measured in radians require base e, so the attenuation is likewise in base e.
The propagation constant for copper (or any other conductor) lines can be calculated from the primary line coefficients by means of the relationship;
where;
Attenuation constant.
In telecommunications, the term attenuation constant, also called attenuation parameter or attenuation coefficient, is the attenuation of an electromagnetic wave propagating through a medium per unit distance from the source. It is the real part of the propagation constant and is measured in nepers per metre. A neper is approximately 8.7dB. Attenuation constant can be defined by the amplitude ratio;
The propagation constant per unit length is defined as the natural logarithmic of ratio of the sending end current or voltage to the receiving end current or voltage.
Copper lines.
The attenuation constant for copper lines (or ones made of any other conductor) can be calculated from the primary line coefficients as shown above. For a line meeting the distortionless condition, with a conductance G in the insulator, the attenuation constant is given by;
however, a real line is unlikely to meet this condition without the addition of loading coils and, furthermore, there are some frequency dependent effects operating on the primary "constants" which cause a frequency dependence of the loss. There are two main components to these losses, the metal loss and the dielectric loss.
The loss of most transmission lines are dominated by the metal loss, which causes a frequency dependency due to finite conductivity of metals, and the skin effect inside a conductor. The skin effect causes R along the conductor to be approximately dependent on frequency according to;
Losses in the dielectric depend on the loss tangent (formula_11) of the material, which depends inversely on the wavelength of the signal and is directly proportional to the frequency.
Optical fibre.
The attenuation constant for a particular propagation mode in an optical fiber, the real part of the axial propagation constant.
Phase constant.
In electromagnetic theory, the phase constant, also called phase change constant, parameter or coefficient is the imaginary component of the propagation constant for a plane wave. It represents the change in phase per metre along the path travelled by the wave at any instant and is equal to real part of the angular wavenumber of the wave. It is represented by the symbol β and is measured in units of radians per metre.
From the definition of (angular) wavenumber;
For a transmission line, the Heaviside condition of the telegrapher's equation tells us that the wavenumber must be proportional to frequency for the transmission of the wave to be undistorted in the time domain. This includes, but is not limited to, the ideal case of a lossless line. The reason for this condition can be seen by considering that a useful signal is composed of many different wavelengths in the frequency domain. For there to be no distortion of the waveform, all these waves must travel at the same velocity so that they arrive at the far end of the line at the same time as a group. Since wave phase velocity is given by;
it is proved that β is required to be proportional to ω. In terms of primary coefficients of the line, this yields from the telegrapher's equation for a distortionless line the condition;
However, practical lines can only be expected to approximately meet this condition over a limited frequency band.
Filters and two-port networks.
The term propagation constant or propagation function is applied to filters and other two-port networks used for signal processing. In these cases, however, the attenuation and phase coefficients are expressed in terms of nepers and radians per network section rather than per metre. Some authors make a distinction between per metre measures (for which "constant" is used) and per section measures (for which "function" is used).
The propagation constant is a useful concept in filter design which invariably uses a cascaded section topology. In a cascaded topology, the propagation constant, attenuation constant and phase constant of individual sections may be simply added to find the total propagation constant etc.
Cascaded networks.
The ratio of output to input voltage for each network is given by,
formula_16
formula_17
formula_18
The terms formula_19 are impedance scaling terms and their use is explained in the image impedance article.
The overall voltage ratio is given by,
formula_20
Thus for "n" cascaded sections all having matching impedances facing each other, the overall propagation constant is given by,
formula_21
See also.
The concept of penetration depth is one of many ways to describe the absorption of electromagnetic waves. For the others, and their interrelationships, see the article: Mathematical descriptions of opacity.
References.
</dl>

</doc>
<doc id="41587" url="http://en.wikipedia.org/wiki?curid=41587" title="Propagation path obstruction">
Propagation path obstruction

In telecommunication, a propagation path obstruction is a man-made or natural physical feature that lies near enough to a radio path to cause a measurable effect on path loss, exclusive of reflection effects. An obstruction may lie to the side, above, or below the path. Ridges, bridges, cliffs, buildings, and trees are examples of obstructions. If the clearance from the nearest anticipated path position, over the expected range of Earth radius k-factor, exceeds 0.6 of the first Fresnel zone radius, the feature is not normally considered an obstruction.

</doc>
<doc id="41589" url="http://en.wikipedia.org/wiki?curid=41589" title="Protective distribution system">
Protective distribution system

A protective distribution system (PDS), also called "protected distribution system", is a US government term for wireline or fiber-optics telecommunication system that includes terminals and adequate acoustical, electrical, electromagnetic, and physical safeguards to permit its use for the unencrypted transmission of classified information. At one time these systems were called "approved circuits".
A complete protected distribution system includes the subscriber and terminal equipment and the interconnecting lines.
Description.
The purpose of a PDS is to deter, detect and/or make difficult physical access to the communication lines carrying national security information.
A specification called the National Security Telecommunications and Information Systems Security Instruction (NSTISSI) 7003 was issued in December 1996 by the Committee on National Security Systems.
Approval authority, standards, and guidance for the design, installation, and maintenance for PDS are provided by NSTISSI 7003 to U.S. government departments and agencies and their contractors and vendors. This instruction describes the requirements for all PDS installations within the U.S. and for low and medium threat locations outside the U.S.
PDS is commonly used to protect SIPRNet and JWICS networks.
The document superseded one numbered NASCI 4009 on Protected Distribution Systems, dated December 30, 1981, and part of a document called NACSEM 5203, that covered guidelines for facility design, using the designations "red" and "black".
There are two types of PDS: hardened distribution systems and simple distribution systems. distribution fast as compare to transmission systems due to more load consuming.
Hardened distribution.
Hardened distribution PDSs provide significant physical protection and can be implemented in three forms: hardened carrier PDSs, alarmed carrier PDSs and continuously viewed carrier PDSs.
Hardened carrier.
In a hardened carrier PDS, the data cables are installed in a carrier constructed of electrical metallic tubing (EMT), ferrous conduit or pipe, or ridged sheet steel ducting. All of the connections in a Hardened Carrier System are permanently sealed completely around all surfaces with welds, epoxy or other such sealants. If the hardened carrier is buried under ground, to secure cables running between buildings for example, the carrier containing the cables is encased in concrete.
With a hardened carrier system, detection is accomplished via human inspections that are required to be performed periodically. Therefore, hardened carriers are installed below ceilings or above flooring so they can be visually inspected to ensure that no intrusions have occurred. These periodic visual inspections (PVIs) occur at a frequency dependent upon the level of threat to the environment, the security classification of the data, and the access control to the area.
Alarmed carrier.
As an alternative to conducting human visual inspections, an alarmed carrier PDS may be constructed to automate the inspection process through electronic monitoring with an alarm system. In an Alarmed Carrier PDS, the carrier system is “alarmed” with specialized optical fibers deployed within the conduit for the purpose of sensing acoustic vibrations that usually occur when an intrusion is being attempted on the conduit in order to gain access to the cables.
Alarmed carrier PDS offers several advantages over hardened carrier PDS:
Legacy alarmed carrier Systems monitor the carrier containing the cables being protected. More advanced systems monitor the fibers within, or intrinsic to, the cables being protected to turn those cables into sensors, which detect intrusion attempts.
Depending on the government organization, utilizing an alarmed carrier PDS in conjunction with interlocking armored cable may, in some cases, allow for the elimination of the carrier systems altogether. In these instances, the cables being protected can be installed in existing conveyance (wire basket, ladder rack) or suspended cabling (on D-rings, J-Hooks, etc.).
Continuously viewed carrier.
A Continuously Viewed Carrier PDS is one that is under continuous observation, 24 hours per day (including when operational). Such circuits may be grouped together, but should be separated from all non-continuously viewed circuits ensuring an open field of view. Standing orders should include the requirement to investigate any attempt to disturb the PDS. Appropriate security personnel should investigate the area of attempted penetration within 15 minutes of discovery. This type of hardened carrier is not used for Top Secret or special category information for non-U.S. UAA.
UAA is an Uncontrolled Access Area (UAA). Like definitions include Controlled Access Area (CAA) and Restricted Access Area (RAA). A Secure Room (SR) offers the highest degree of protection.
Therefore from the least protected (least secure) to the most protected is as follows:
UAA
RAA
CAA
SR
Simple distribution.
Simple distribution PDSs are afforded a reduced level of physical security protection as compared to a hardened distribution PDS. They use a simple carrier system and the following means are acceptable under NSTISSI 7003: 

</doc>
<doc id="41590" url="http://en.wikipedia.org/wiki?curid=41590" title="Protocol-control information">
Protocol-control information

In telecommunication, the term protocol-control information or PCI has the following meanings: 
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41591" url="http://en.wikipedia.org/wiki?curid=41591" title="Protocol data unit">
Protocol data unit

In telecommunications, the term protocol data unit (PDU) has the following meanings: 
OSI model.
PDUs are relevant in relation to each of the first 4 layers of the OSI model as follows:
Given a context pertaining to a specific OSI layer, PDU is sometimes used as a synonym for its representation at that layer.
Media access control protocol data unit.
Media access control protocol data unit or MPDU is a message (Protocol data unit) exchanged between media access control (MAC) entities in a communication system based on the layered Open Systems Interconnection model.
In systems where the MPDU may be larger than the MAC service data unit (MSDU), the MPDU may include multiple MSDUs as a result of packet aggregation. In systems where the MPDU is smaller than the MSDU, then one MSDU may generate multiple MPDUs as a result of packet segmentation.
Packet-switched data networks.
In the context of packet-switched data networks, a protocol data unit (PDU) is best understood in relation to a service data unit (SDU).
The features or services of the network are implemented in distinct "layers". For example, sending ones and zeros across a wire, fiber, etc. is done by the physical layer, organizing the ones and zeros into chunks of data and getting them safely to the right place on the wire is done by the data link layer, passing data chunks over multiple connected networks is done by the network layer and delivery of the data to the right software application at the destination is done by the transport layer.
Between the layers (and between the application and the top-most layer), the layers pass service data units across the interfaces. The application or higher layer understands the structure of the data in the SDU, but the lower layer at the interface does not; it treats it as payload, undertaking to get it to the same interface at the destination. In order to do this, the protocol layer will add to the SDU certain data it needs to perform its function. For example, it might add a port number to identify the application, a network address to help with routing, a code to identify the type of data in the packet and error-checking information. All this additional information, plus the original service data unit from the higher layer, constitutes the "protocol data unit" at this layer. The significance of this is that the PDU is the structured information that is passed to a matching protocol layer further along on the data's journey that allows the layer to deliver its intended function or service. The matching layer, or "peer", decodes the data to extract the original service data unit, decide if it is error-free and where to send it next, etc. Unless we have already arrived at the lowest (physical) layer, the PDU is passed to the peer using services of the next lower layer in the protocol "stack". When the PDU passes over the interface from the layer that constructed it to the layer that merely delivers it (and therefore does not understand its internal structure), it becomes a service data unit to that layer. The addition of addressing and control information (which is called encapsulation) to an SDU to form a PDU and the passing of that PDU to the next lower layer as an SDU repeats until the lowest layer is reached and the data passes over some medium as a physical signal.
The above process can be likened to the mail system in which a letter (SDU) is placed in an envelope on which is written an address (addressing and control information) making it a PDU. The sending post office might look only at the post code and place the letter in a mail bag so that the address on the envelope can no longer be seen, making it now an SDU. The mail bag is labelled with the destination post code and so becomes a PDU, until it is combined with other bags in a crate, when it is now an SDU, and the crate is labelled with the region to which all the bags are to be sent, making the crate a PDU. When the crate reaches the destination matching its label, it is opened and the bags (SDUs) removed only to become PDUs when someone reads the code of the destination post office. The letters themselves are SDUs when the bags are opened but become PDUs when the address is read for final delivery. When the addressee finally opens the envelope, the top-level SDU, the letter itself, emerges.

</doc>
<doc id="41592" url="http://en.wikipedia.org/wiki?curid=41592" title="Provisioning">
Provisioning

In telecommunication, provisioning involves the process of preparing and equipping a network to allow it to provide (new) services to its users. In National Security/Emergency Preparedness telecommunications services, "provisioning" equates to "initiation" and includes altering the state of an existing priority service or capability.
The concept of network provisioning or service mediation, mostly used in the telecommunication industry, refers to the provisioning of the customer's services to the network elements. It requires the existence of networking equipment and depends on network planning and design.
In a modern signal infrastructure employing information technology (IT) at all levels, there is no possible distinction between telecommunications services and "higher level" infrastructure. Accordingly, provisioning configures any required systems, provides users with access to data and technology resources, and refers to all enterprise-level information-resource management involved.
Organizationally, a CIO typically manages provisioning, necessarily involving human resources and IT departments cooperating to:
As its core, the provisioning process monitors access rights and privileges to ensure the security of an enterprise's resources and user privacy. As a secondary responsibility, it ensures compliance and minimizes the vulnerability of systems to penetration and abuse. As a tertiary responsibility, it tries to reduce the amount of custom configuration using boot image control and other methods that radically reduce the number of different configurations involved.
Discussion of provisioning often appears in the context of virtualization, orchestration, utility computing, cloud computing, and open-configuration concepts and projects. For instance, the OASIS Provisioning Services Technical Committee (PSTC) defines an XML-based framework for exchanging user, resource, and service-provisioning information - SPML (Service Provisioning Markup Language) for "managing the provisioning and allocation of identity information and system resources within and between organizations".
Once provisioning has taken place, the process of SysOpping ensures the maintenance of services to the expected standards. Provisioning thus refers only to the setup or startup part of the service operation, and SysOpping to the ongoing support.
Network provisioning.
The services which are assigned to the customer in the customer relationship management (CRM) have to be provisioned on the network element which is enabling the service and allows the customer to actually use the service. The relation between a service configured in the CRM and a service on the network elements is not necessarily a 1:1 relation. Some services can be enabled by more than one network element, e.g. the Microsoft Media Server (mms://) service.
During the provisioning, the service mediation device translates the service and the corresponding parameters of the service to one or more services/parameters on the network elements involved.
The algorithm used to translate a system service into network services is called provisioning logic.
Electronic invoice feeds from your carriers can be automatically downloaded directly into the core of the telecom expense management (TEM) software and it will immediately conduct an audit of each single line item charge all the way down to the User Support and Operations Center (USOC) level. The provisioning software will capture each circuit number provided by all of your carriers and if bills outside of the contracted rate an exception rule will trigger a red flag and notify the pre-established staff member to review the billing error.
Server provisioning.
Server provisioning is a set of actions to prepare a server with appropriate systems, data and software, and make it ready for network operation. Typical tasks when provisioning a server are: select a server from a pool of available servers, load the appropriate software (operating system, device drivers, middleware, and applications), appropriately customize and configure the system and the software to create or change a boot image for this server, and then change its parameters, such as IP address, IP Gateway to find associated network and storage resources (sometimes separated as "resource provisioning") to audit the system. By auditing the system, you ensure OVAL compliance with limit vulnerability, ensure compliance, or install patches. After these actions, you restart the system and load the new software. This makes the system ready for operation. Typically an internet service provider (ISP) or Network Operations Center will perform these tasks to a well-defined set of parameters, for example, a boot image that the organization has approved and which uses software it has license to. Many instances of such a boot image create a virtual dedicated host.
There are many software products available to automate the provisioning of servers, services and end-user devices. Examples: BMC Bladelogic Server Automation, HP Server Automation, IBM Tivoli Provisioning Manager, Redhat Kickstart, xCAT, HP Insight CMU, etc. Middleware and applications can be installed either when the operating system is installed or afterwards by using an Application Service Automation tool. Further questions are addressed in academia such as when provisioning should be issued and how many servers are needed in multi-tier, or multi-service applications.
In cloud computing servers may be provisioned via a web user interface or an application programming interface. One of the unique things about cloud computing is how rapidly and easily this can be done. Monitoring software can be used to trigger automatic provisioning when existing resources become too heavily stressed.
In short, server provisioning configures servers based on resource requirements. The use of a hardware or software component (e.g. single/dual processor, RAM, HDD, RAID controller, a number of LAN cards, applications, OS, etc.) depends on the functionality of the server, such as ISP, virtualization, NOS, or voice processing. Server redundancy depends on the availability of servers in the organization. Critical applications have less downtime when using cluster servers, RAID, or a mirroring system.
Service used by most larger scale centers in part to avoid this. Additional resource provisioning may be done per service.
User provisioning.
User provisioning refers to the creation, maintenance and deactivation of user objects and user attributes, as they exist in one or more systems, directories or applications, in response to automated or interactive business processes. User provisioning software may include one or more of the following processes: change propagation, self-service workflow, consolidated user administration, delegated user administration, and federated change control. User objects may represent employees, contractors, vendors, partners, customers or other recipients of a service. Services may include electronic mail, inclusion in a published user directory, access to a database, access to a network or mainframe, etc. User provisioning is a type of identity management software, particularly useful within organizations, where users may be represented by multiple objects on multiple systems.
Self-service provisioning for cloud computing services.
On-demand self-service is described by the National Institute of Standards and Technology (NIST) as an essential characteristic of Cloud computing. The self-service nature of cloud computing lets end users obtain and remove cloud services―including applications, the infrastructure supporting the applications, and configuration― themselves without requiring the assistance of an IT staff member. The automatic self-servicing may target different application goals and constraints (e.g. deadlines and cost), as well as handling different application architectures (e.g., bags-of-tasks and workflows). Cloud users can obtain cloud services through a cloud service catalog or a self-service portal. Because business users can obtain and configure cloud services themselves, this means IT staff can be more productive and gives them more time to manage cloud infrastructures.
However one problem of cloud service provisioning is that it is not instantaneous. A cloud VM can be acquired at any time by the user, but it may take up to several minutes for the acquired VM to be ready to use. The VM startup time is dependent on factors, such as image size, VM type, data center location, number of VMs, etc. Cloud providers have different VM startup performance.
Mobile subscriber provisioning.
Mobile subscriber provisioning refers to the setting up of new services, such as GPRS, MMS and Instant Messaging for an existing subscriber of a mobile phone network, and any gateways to standard Internet chat or mail services. The network operator typically sends these settings to the subscriber's handset using SMS text services or HTML, and less commonly WAP, depending on what the mobile operating systems can accept.
A general example of provisioning is with data services. A mobile user who is using his or her device for voice calling may wish to switch to data services in order to read emails or browse the Internet. The mobile device's services are "provisioned" and thus the user is able to stay connected through push emails and other features of smartphone services.
Device management systems can benefit end-users by incorporating plug and play data services, supporting whatever device the end-user is using.. Such a platform can automatically detect devices in the network, sending them settings for immediate and continued usability. The process is fully automated, keeping the history of used devices and sending settings only to subscriber devices which were not previously set. One method of managing mobile updates is to filter IMEI/IMSI pairs. Some operators report activity of 50 over-the-air settings update files per second.
Mobile content provisioning.
This refers to delivering mobile content, such as mobile internet to a mobile phone, agnostic of the features of said device. These may include operating system type and versions, Java version, browser version, screen form factors, audio capabilities, language settings and a plethora of other characteristics. As of April 2006, an estimated 5000 permutations are relevant. Mobile content provisioning facilitates a common user experience, though delivered on widely different handsets.
Internet access provisioning.
When getting a user / customer online, beyond user provisioning and network provisioning, the client system must be configured. This process may include many steps, depending on the connection technology in question (DSL, Cable, Fibre, etc.). The possible steps are:
There are four approaches to provisioning an internet access:
External links.
 at DMOZ

</doc>
<doc id="41593" url="http://en.wikipedia.org/wiki?curid=41593" title="Pseudorandom noise">
Pseudorandom noise

In cryptography, pseudo random noise (PRN) is a signal similar to noise which satisfies one or more of the standard tests for statistical randomness. 
Although it seems to lack any definite pattern, pseudo random noise consists of a deterministic sequence of pulses that will repeat itself after its period.
In cryptographic devices, the pseudo random noise pattern is determined by a key and the repetition period can be very long, even millions of digits.
Pseudo random noise is used in some electronic musical instruments, either by itself or as an input to subtractive synthesis, and in many white noise machines.
In spread-spectrum systems, the receiver correlates a locally generated signal with the received signal.
Such spread-spectrum systems require a set of one or more "codes" or "sequences" such that
In a direct-sequence spread spectrum system, each bit in the pseudorandom binary sequence is known as a "chip" and the "inverse" of its period as "chip rate". "Compare bit rate and baud."
In a frequency-hopping spread spectrum sequence, each value in the pseudo random sequence is known as a "channel number" and the "inverse" of its period as the "hop rate". FCC Part 15 mandates at least 50 different channels and at least a 2.5 Hz hop rate for narrow band frequency-hopping systems.
GPS satellites broadcast data at a rate of 50 data bits per second – each satellite modulates its data with one PN bit stream at 1.023 million chips per second and the same data with another PN bit stream at 10.23 million chips per second.
GPS receivers correlate the received PN bit stream with a local reference to measure distance. GPS is a receive-only system that uses relative timing measurements from several satellites (and the known positions of the satellites) to determine receiver position.
Other range-finding applications involve two-way transmissions.
A local station generates a pseudo random bit sequence and transmits it to the remote location (using any modulation technique).
Some object at the remote location echoes this PN signal back to the location station – either passively, as in some kinds of radar and sonar systems, or using an active transponder at the remote location, as in the Apollo Unified S-band system.
By correlating a (delayed version of) the transmitted signal with the received signal, a precise round trip time to the remote location can be determined and thus the distance.
PN code.
A pseudo noise code (PN code) or pseudo random noise code (PRN code) is one that has a spectrum similar to a random sequence of bits but is deterministically generated. The most commonly used sequences in direct-sequence spread spectrum systems are maximal length sequences, Gold codes, Kasami codes, and Barker codes.

</doc>
<doc id="41595" url="http://en.wikipedia.org/wiki?curid=41595" title="Psophometer">
Psophometer

In telecommunications, a psophometer is an instrument that provides a visual indication of the audible effects of disturbing voltages of various frequencies. 
A psophometer usually incorporates a weighting network. The characteristics of the weighting network depend on the type of circuit under investigation, such as whether the circuit is used for high-fidelity music or for normal speech.

</doc>
<doc id="41596" url="http://en.wikipedia.org/wiki?curid=41596" title="Psophometric voltage">
Psophometric voltage

Psophometric voltage is a circuit noise voltage measured with a psophometer that includes a CCIF-1951 weighting network. 
"Psophometric voltage" should not be confused with ""psophometric emf," i.e.", the emf in a generator or line with 600 Ω internal resistance. For practical purposes, the psophometric emf is twice the corresponding psophometric voltage. 
Psophometric voltage readings, "V", in millivolts, are commonly converted to dBm(psoph) by dBm(psoph) = 20 log10"V" – 57.78.

</doc>
<doc id="41597" url="http://en.wikipedia.org/wiki?curid=41597" title="Public data transmission service">
Public data transmission service

In telecommunication, a public data transmission service is a data transmission service that is established and operated by a telecommunication administration, or a recognized private operating agency, and uses a public data network. 
A public data transmission service may include Circuit Switched Data packet-switched, and leased line data transmission.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41598" url="http://en.wikipedia.org/wiki?curid=41598" title="Public land mobile network">
Public land mobile network

A public land mobile network (PLMN), as defined in telecommunications regulation, is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public.
A PLMN is identified by the Mobile Country Code (MCC) and the Mobile Network Code (MNC). Each operator providing mobile services has its own PLMN. PLMNs interconnect with other PLMNs and Public switched telephone networks (PSTN) for telephone communications or with internet service providers for data and internet access of which links are defined as interconnect links between providers. These links mostly incorporate SDH digital transmission networks via fiber optic on land and digital microwave links.
Access to PLMN services is achieved by means of an "air interface" involving radio communications between mobile phones or other wireless enabled user equipment and land-based radio transmitters or radio base stations or even fiber optic distributed SDH network between mobile base stations and central stations via SDH equipment (ADMs) with integrated IP network services.
Public switched telephone network.
Public Land Mobile Networks need to connect to the Public Switched Telephone Network (PSTN) in order to route calls.
The PSTN is the world's collection of interconnected voice-oriented public telephone networks, in much the same way that the Internet is the concatenation of the world's public IP-based packet-switched networks. It is both commercially- and government-owned. This aggregation of circuit-switching telephone networks has evolved greatly from the days of Alexander Graham Bell, and in the late 20th century became almost entirely digital in nature — except for the final link from the central (local) telephone office to the user (the local loop). It also extends into mobile as well as fixed telephones.
The PSTN also furnishes much of the Internet's long-distance infrastructure and, for the majority of users, the access network as well. Because Internet Service Providers (ISPs) pay the long-distance carriers for access to their infrastructure, and share the circuits among many users through packet switching, the end Internet user avoids having to pay usage tolls to anyone other than their ISP.
Many observers believe that the long-term future of the PSTN is to be just one application of the Internet — however, the Internet has some way to go before this transition can be made. For example, the Quality of Service (QoS) guarantee is one aspect that needs to be improved for Voice over IP (VOIP) technology.
The PSTN is largely governed by technical standards created by the ITU-T, and uses E.163/E.164 addresses (usually called telephone numbers) for addressing. A number of large private telephone networks are not connected to the PSTN, and are used for military purposes (such as the Defense Switched Network). There are also private networks run by large companies that are linked to the PSTN, but only through controlled gateways such as private branch exchanges.
Specifications.
A GSM PLMN may be described by a limited set of access interfaces and a limited set of GSM PLMN connection types to support the telecommunication services described in the GSM 02-series of specifications.
PLMN is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public. A PLMN may be considered as an extension of a fixed network, e.g., the Public Switched Telephone Network (PSTN) or as an integral part of the PSTN. This is just one view-point on PLMN. PLMN mostly refers to the whole system of networking hardware and software that enables wireless communication, irrespective of the service area or service provider (cf. Internet backbone). Sometimes a separate PLMN is defined for each country or for each service provider. This systematic ambiguity (of terminological scope) also affects the "PSTN" term. Sometimes it refers to the whole circuit-switched system, while other times it is specific to each country.
PLMN is not a term specific to GSM. In fact, GSM can be treated as an example of a PLMN system. These days (as of January, 2006) many discussions are going on to form the structure of UMTS PLMN for the third-generation systems. Access to PLMN services is achieved by means of an air interface involving radio communications between mobile phones or other wireless-enabled user equipment and land-based radio transmitters or radio base stations. PLMNs interconnect with other PLMNs and PSTNs for telephone communications or with Internet service providers for data and internet access.
A public land mobile network may be defined as a number of mobile services switching center areas within a common numbering plan and a common routing plan. With respect to their functions, the PLMNs may be regarded as independent communications entities, even though different PLMNs may be interconnected through the PSTN/ISDN for the forwarding of calls or network information. The MSCs of a PLMN can be interconnected similarly to allow interaction. A PLMN may have several interfaces with the fixed network (e.g., one for each MSC). Inter-working between two PLMNs may be performed via an international switching center. The PLMN is connected via an NCP to the PSTN/ISDN. If there are two mobile service suppliers in the same country, they can be connected through the same PSTN/ISDN.
Objectives of a GSM PLMN.
The general objective of a PLMN is to facilitate wireless communication and to interlink the wireless network with the fixed wired network. The PLMN was specified by the European Telecommunications Standard Institute (ETSI) following up with their GSM specification. Even as times changed, the GSM PLMN objectives conceptually remained the same.
Architecture.
GSM architecture is basically the PLMN architecture itself as the subject is GSM PLMN. Various interfaces between the GSM subsystems are to be considered, along with the signaling system and the various components (both hardware and software).
Subsystems.
The GSM PLMN is divided into signaling network and mobile network. Each of these has various subsystems, which are grouped under three major systems: the Network and Switching Subsystem (NSS), the Base Station Subsystem (BSS), and the operation and support system (OSS).
Operation and Support System (OSS).
The operations and maintenance center (OMC) is connected to all equipment in the switching system and to the BSC. The implementation of OMC is called the operation and support system (OSS). The OSS is the functional entity from which the network operator monitors and controls the system. The purpose of OSS is to offer the customer cost-effective support for centralized, regional, and local operational and maintenance activities that are required for a GSM network. An important function of OSS is to provide a network overview and support the maintenance activities of different operation and maintenance organizations.
Additional functional elements.
Other functional elements shown are as follows:
There are three viewpoints of interoperability between PLMN and PSTN:
Conclusion.
A PLMN is essential for the effective working of any wireless network, just like the need for PSTN in wireline networks. PLMN facilitates interoperation with its own subsystems in order to perform operation of the GSM system in particular and any wired network in general.

</doc>
<doc id="41599" url="http://en.wikipedia.org/wiki?curid=41599" title="Pulsating direct current">
Pulsating direct current

A pulsating direct current is a type of Direct Current (DC) that changes in value over short periods of time. 
A pulsating direct current may change in value, "i.e.," be always present but at different levels, or it may be interrupted completely. The changes may be irregular or at regular intervals (at a specific frequency), but the current never changes direction.
Pulsating currents are commonly the consequence of using diode rectifiers, or DC sources of lower amplitude connected in series with AC sources. It can be smoother, if a large value capacitor is used in parallel with the rectified source.
Pulsating direct current is used on PWM controllers.
Difference from AC.
Pulsating direct current has an average value equal to a constant (DC) along with a time-dependent pulsating component added to it, while the average value of alternating current is zero in steady state (or a constant if it has a DC offset, value of which will then be equal to that offset). Devices and circuits may respond differently to pulsating DC than they would to non-pulsating DC, such as a battery or regulated power supply and should be evaluated.

</doc>
<doc id="41600" url="http://en.wikipedia.org/wiki?curid=41600" title="Pulse">
Pulse

In medicine, one's pulse represents the tactile arterial palpation of the heartbeat by trained fingertips. The pulse may be palpated in any place that allows an artery to be compressed against a bone, such as at the neck (carotid artery), on the inside of the elbow (brachial artery), at the wrist (radial artery), at the groin (femoral artery), behind the knee (popliteal artery), near the ankle joint (posterior tibial artery), and on foot (dorsalis pedis artery). Pulse (or the count of arterial pulse per minute) is equivalent to measuring the heart rate. The heart rate can also be measured by listening to the heart beat directly (auscultation), traditionally using a stethoscope and counting it for a minute. The study of the pulse is known as sphygmology.
Physiology.
The pulse is a decidedly low tech and high yield and antiquated term still useful at the bedside in an age of computational analysis of cardiac performance. Claudius Galen was perhaps the first physiologist to describe the pulse. The pulse is an expedient tactile method of determination of systolic blood pressure to a trained observer. Diastolic blood pressure is non-palpable and unobservable by tactile methods, occurring between heartbeats.
Pressure waves generated by the heart in systole moves the arterial walls. Forward movement of blood occurs when the boundaries are pliable and compliant. These properties form enough to create a palpable pressure wave.
The heart rate may be greater or lesser than the pulse rate depending upon physiologic demand. In this case, the heart rate is determined by auscultation or audible sounds at the heart apex, in which case it is not the pulse. The "pulse deficit" (difference between heart beats and pulsations at the periphery) is determined by simultaneous palpation at the radial artery and auscultation at the heart apex. It may be present in case of premature beats or atrial fibrillation.
Pulse velocity, pulse deficits and much more physiologic data are readily and simplistically visualized by the use of one or more arterial catheters connected to a transducer and oscilloscope. This invasive technique has been commonly used in intensive care since the 1970s.
The rate of the pulse is observed and measured by tactile or visual means on the outside of an artery and is recorded as beats per minute or BPM.
The pulse may be further indirectly observed under light absorbances of varying wavelengths with assigned and inexpensively reproduced mathematical ratios. Applied capture of variances of light signal from the blood component hemoglobin under oxygenated vs. deoxygenated conditions allows the technology of pulse oximetry.
Characteristics of pulse.
Rate.
Normal pulse rates at rest, in beats per minute (BPM):
The pulse rate can be used to check overall heart health and fitness level. Generally lower is better, but bradycardias can be dangerous. Symptoms of a dangerously slow heartbeat include weakness, loss of energy and fainting.
Rhythm.
A normal pulse is regular in rhythm and force. An irregular pulse may be due to sinus arrhythmia, premature beats, ectopic beats, atrial fibrillation, Paroxysmal Atrial Tachycardia, atrial flutter, partial heart block etc. Intermittent dropping out of beats at pulse is called "intermittent pulse". Examples of "regular" intermittent (regularly irregular) pulse include pulsus bigeminus, pulsus trigeminus. An example of "irregular" intermittent (irregularly irregular) pulse is delirium cordis.
Volume.
The degree of expansion displayed by artery during diastolic and systolic state is called volume. It also known as amplitude, expansion or size of pulse.
Hypokinetic pulse.
A weak pulse signifies narrow pulse pressure. It may be due to low cardiac output (as seen in shock, congestive cardiac failure), hypovolemia, valvular heart disease (such as aortic outflow tract obstruction, mitral stenosis, aortic arch syndrome) etc.
Hyperkinetic pulse.
A bounding pulse signifies high pulse pressure. It may be due to low peripheral resistance (as seen in fever, anemia, thyrotoxicosis, hyperkinetic heart syndrome, A-V fistula, Paget's disease, beriberi, liver cirrhosis), increased cardiac output, increased stroke volume (as seen in anxiety, exercise, complete heart block, aortic regurgitation), decreased distensibility of arterial system (as seen in atherosclerosis, hypertension and coarctation of aorta).
The strength of the pulse can also be reported:
Force.
Also known as compressibility of pulse. It is a rough measure of systolic blood pressure.
Tension.
It corresponds to diastolic blood pressure. A low tension pulse (pulsus mollis), the vessel is soft or impalpable between beats. In high tension pulse (pulsus durus), vessels feels rigid even between pulse beats.
Form.
A form or contour of a pulse is palpatiory estimation of arteriogram. A quickly rising and quickly falling pulse (pulsus celer) is seen in aortic regurgitation. A slow rising and slowly falling pulse (pulsus tardus) is seen in aortic stenosis.
Equality.
Comparing pulses and different places gives valuable clinical information.
A discrepant or unequal pulse between left and right radial artery is observed in anomalous or aberrant course of artery, coarctation of aorta, aortitis, dissecting aneurysm, peripheral embolism etc. An unequal pulse between upper and lower extremities is seen in coarctation to aorta, aortitis, block at bifurcation of aorta, dissection of aorta, iatrogenic trauma and arteriosclerotic obstruction.
Condition of arterial wall.
A normal artery is not palpable after flattening by digital pressure. A thick radial artery which is palpable 7.5-10 cm up the forearm is suggestive of arteriosclerosis.
Radio-femoral delay.
In coarctation of aorta, femoral pulse may be significantly delayed as compared to radial pulse (unless there is coexisting aortic regurgitation). The delay can also be observed in supravalvar aortic stenosis.
Patterns.
Several pulse patterns can be of clinical significance. These include:
Common palpable sites.
Upper limb.
Chinese medicine has focused on the pulse in the upper limbs for several centuries. The concept of pulse diagnosis is essentially a treatise based upon palpation and observations of the radial and ulnar volar pulses at the readily accessible wrist.

</doc>
<doc id="41601" url="http://en.wikipedia.org/wiki?curid=41601" title="Pulse-address multiple access">
Pulse-address multiple access

In telecommunications, pulse-address multiple access (PAMA) is a channel access method that enables the ability of a communication satellite to receive signals from several Earth terminals simultaneously and to amplify, translate, and relay the signals back to Earth, based on the addressing of each station by an assignment of a unique combination of time and frequency slots. 
This ability may be restricted by allowing only some of the terminals access to the satellite at any given time.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41605" url="http://en.wikipedia.org/wiki?curid=41605" title="Pulse duration">
Pulse duration

In signal processing and telecommunication, pulse duration is the interval between the time, during the first transition, that the amplitude of the pulse reaches a specified fraction (level) of its final amplitude, and the time the pulse amplitude drops, on the last transition, to the same level. 
The interval between the 50% points of the final amplitude is usually used to determine or define pulse duration, and this is understood to be the case unless otherwise specified. Other fractions of the final amplitude, e.g., 90% or 1/e, may also be used, as may the root mean square (rms) value of the pulse amplitude. 
In radar, the pulse duration is the time the radar's transmitter is energized during each cycle.

</doc>
<doc id="41606" url="http://en.wikipedia.org/wiki?curid=41606" title="Pulse link repeater">
Pulse link repeater

In telecommunications, a pulse link repeater (PLR) is a device that interfaces concatenated E and M signaling paths. 
A PLR converts a ground, received from the E lead of one signal path, to −48 Vdc, which is applied to the M lead of the concatenated signal path. 
In many commercial carrier systems, the channel bank cards or modules have a "PLR" option that permits the direct concatenation of E&M signaling paths, without the need for separate PLR equipment.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41607" url="http://en.wikipedia.org/wiki?curid=41607" title="Pulsing">
Pulsing

Pulsing may refer to:

</doc>
<doc id="41608" url="http://en.wikipedia.org/wiki?curid=41608" title="Pumping">
Pumping

Pumping can refer to:

</doc>
<doc id="41610" url="http://en.wikipedia.org/wiki?curid=41610" title="Push-to-type operation">
Push-to-type operation

Push-to-type operation: In telegraph or data transmission systems, that method of communication in which the operator at a station must keep a switch operated in order to send messages. 
Push-to-type operation is used in radio systems where the same frequency is employed for transmission and reception. 
Push-to-type operation is a derivative form of transmission and may be used in simplex, half-duplex, or duplex operation. "Synonym" press-to-type operation.
This is similar to Push-to-talk operation for radio phone communications.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41611" url="http://en.wikipedia.org/wiki?curid=41611" title="Quadrature">
Quadrature

Quadrature may refer to:
In signal processing:
In mathematics:
In physics:
In astronomy:
Quadrature may also refer to:

</doc>
<doc id="41612" url="http://en.wikipedia.org/wiki?curid=41612" title="Quadruply clad fiber">
Quadruply clad fiber

In fiber optics, a quadruply clad fiber is a single-mode optical fiber that has four claddings. Each cladding has a refractive index lower than that of the core. With respect to one another, their relative refractive indices are, in order of distance from the core: lowest, highest, lower, higher. 
A quadruply clad fiber has the advantage of very low macrobending losses. It also has two zero-dispersion points, and moderately low dispersion over a wider wavelength range than a singly clad fiber or a doubly clad fiber.

</doc>
<doc id="41613" url="http://en.wikipedia.org/wiki?curid=41613" title="Quality control">
Quality control

Quality control, or QC for short, is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as "A part of quality management focused on fulfilling quality requirements".
This approach places an emphasis on three aspects:
Controls include product inspection, where every product is examined visually, and often using a stereo microscope for fine detail before the product is sold into the external market. Inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example.
The quality of the outputs is at risk if any of these three aspects is deficient in any way.
Quality control emphasizes testing of products to uncover defects and reporting to management who make the decision to allow or deny product release, whereas quality assurance attempts to improve and stabilize production (and associated processes) to avoid, or at least minimize, issues which led to the defect(s) in the first place. For contract work, particularly work awarded by government agencies, quality control issues are among the top reasons for not renewing a contract.
Notable approaches to quality control.
There is a tendency for individual consultants and organizations to name their own unique approaches to quality control—a few of these have ended up in widespread use:
Quality control in project management.
In project management, quality control requires the project manager and the project team to inspect the accomplished work to ensure its alignment with the project scope. In practice, projects typically have a dedicated quality control team which focuses on this area.

</doc>
<doc id="41615" url="http://en.wikipedia.org/wiki?curid=41615" title="Quasi-analog signal">
Quasi-analog signal

In telecommunication, a quasi-analog signal is a digital signal that has been converted to a form suitable for transmission over a specified analog channel. 
The specification of the analog channel should include frequency range, bandwidth, signal-to-noise ratio, and envelope delay distortion. When quasi-analog form of signaling is used to convey message traffic over dial-up telephone systems, it is often referred to as voice-data. A modem may be used for the conversion process.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41616" url="http://en.wikipedia.org/wiki?curid=41616" title="Queuing delay">
Queuing delay

In telecommunication and computer engineering, the queuing delay (or queueing delay) is the time a job waits in a queue until it can be executed. It is a key component of network delay. In a switched network, the time between the completion of signaling by the call originator and the arrival of a ringing signal at the call receiver. Queues may be caused by delays at the originating switch, intermediate switches, or the call receiver servicing switch. In a data network, the sum of the delays between the request for service and the establishment of a circuit to the called data terminal equipment (DTE). In a packet-switched network, the sum of the delays encountered by a packet between the time of insertion into the network and the time of delivery to the addressee. 
This term is most often used in reference to routers. When packets arrive at a router, they have to be processed and transmitted. A router can only process one packet at a time. If packets arrive faster than the router can process them (such as in a burst transmission) the router puts them into the queue (also called the buffer) until it can get around to transmitting them. Delay can also vary from packet to packet so averages and statistics are usually generated when measuring and evaluating queuing delay. 
As a queue begins to fill up due to traffic arriving faster than it can be processed, the amount of delay a packet experiences going through the queue increases. The speed at which the contents of a queue can be processed is a function of the transmission rate of the facility. This leads to the classic delay curve. The average delay any given packet is likely to experience is given by the formula 1/(μ-λ) where μ is the number of packets per second the facility can sustain and λ is the average rate at which packets are arriving to be serviced. This formula can be used when no packets are dropped from the queue. 
The maximum queuing delay is proportional to buffer size. The longer the line of packets waiting to be transmitted, the longer the average waiting time is. The router queue of packets waiting to be sent also introduces a potential cause of packet loss. Since the router has a finite amount of buffer memory to hold the queue, a router which receives packets at too high a rate may experience a full queue. In this case, the router has no other option than to simply discard excess packets.
When the transmission protocol uses the dropped-packets symptom of filled buffers to regulate its transmit rate, as the Internet's TCP does, bandwidth is fairly shared at near theoretical capacity with minimal network congestion delays. Absent this feedback mechanism the delays become both unpredictable and rise sharply, a symptom also seen as freeways approach capacity; metered onramps are the most effective solution there, just as TCP's self-regulation is the most effective solution when the traffic is packets instead of cars). This result is both hard to model mathematically and quite counterintuitive to people who lack experience with mathematics or real networks. Failing to drop packets, choosing instead to buffer an ever-increasing number of them, produces bufferbloat.
In Kendall's notation, the M/M/1/K queuing model, where K is the size of the buffer, may be used to analyze the queuing delay in a specific system. Kendall's notation should be used to calculate the queuing delay when packets are dropped from the queue. The M/M/1/K queuing model is the most basic and important queuing model for network analysis.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41618" url="http://en.wikipedia.org/wiki?curid=41618" title="Radiation angle">
Radiation angle

In fiber optics, the radiation angle is half the vertex angle of the cone of light emitted at the exit face of an optical fiber. 
The cone boundary is usually defined (a) by the angle at which the far-field irradiance has decreased to a specified fraction of its maximum value or (b) as the cone within which there is a specified fraction of the total radiated power at any point in the far field.

</doc>
<doc id="41619" url="http://en.wikipedia.org/wiki?curid=41619" title="Radiation mode">
Radiation mode

For an optical fiber or waveguide, a radiation mode or unbound mode is a mode which is not confined by the fiber core. Such a mode has fields that are transversely oscillatory everywhere external to the waveguide, and exists even at the limit of zero wavelength.
Specifically, a radiation mode is one for which 
where "β" is the imaginary part of the axial propagation constant, integer "l" is the azimuthal index of the mode, "n"("r") is the refractive index at radius "r", "a" is the core radius, and "k" is the free-space wave number, "k" = 2π/"λ", where "λ" is the wavelength. Radiation modes correspond to refracted rays in the terminology of geometric optics.

</doc>
<doc id="41620" url="http://en.wikipedia.org/wiki?curid=41620" title="Radiation pattern">
Radiation pattern

In the field of antenna design the term radiation pattern (or antenna pattern or far-field pattern) refers to the "directional" (angular) dependence of the strength of the radio waves from the antenna or other source.
Particularly in the fields of fiber optics, lasers, and integrated optics, the term radiation pattern may also be used as a synonym for the near-field pattern or Fresnel pattern. This refers to the "positional" dependence of the electromagnetic field in the near-field, or Fresnel region of the source. The near-field pattern is most commonly defined over a plane placed in front of the source, or over a cylindrical or spherical surface enclosing it.
The far-field pattern of an antenna may be determined experimentally at an antenna range, or alternatively, the near-field pattern may be found using a near-field scanner, and the radiation pattern deduced from it by computation. The far-field radiation pattern can also be calculated from the antenna shape by computer programs such as NEC. Other software, like HFSS can also compute the near field. 
The far field radiation pattern may be represented graphically as a plot of one of a number of related variables, including; the field strength at a constant (large) radius (an amplitude pattern or field pattern), the power per unit solid angle (power pattern) and the directive gain. Very often, only the relative amplitude is plotted, normalized either to the amplitude on the antenna boresight, or to the total radiated power. The plotted quantity may be shown on a linear scale, or in dB. The plot is typically represented as a three-dimensional graph (as at right), or as separate graphs in the vertical plane and horizontal plane. This is often known as a polar diagram.
Reciprocity.
It is a fundamental property of antennas that the receiving pattern (sensitivity as a function of direction) of an antenna when used for receiving is identical to the far-field radiation pattern of the antenna when used for transmitting. This is a consequence of the reciprocity theorem of electromagnetics and is proved below. Therefore in discussions of radiation patterns the antenna can be viewed as either transmitting or receiving, whichever is more convenient.
Typical patterns.
Since electromagnetic radiation is dipole radiation, it is not possible to build an antenna that radiates equally in all directions, although such a hypothetical isotropic antenna is used as a reference to calculate antenna gain. The simplest antennas, monopole and dipole antennas, consist of one or two straight metal rods along a common axis. 
These axially symmetric antennas have radiation patterns with a similar symmetry, called omnidirectional patterns; they radiate equal power in all directions perpendicular to the antenna, with the power varying only with the angle to the axis, dropping off to zero on the antenna's axis. This illustrates the general principle that if the shape of an antenna is symmetrical, its radiation pattern will have the same symmetry.
In most antennas, the radiation from the different parts of the antenna interferes at some angles. This results in zero radiation at certain angles where the radio waves from the different parts arrive out of phase, and local maxima of radiation at other angles where the radio waves arrive in phase. Therefore the radiation plot of most antennas shows a pattern of maxima called "lobes" at various angles, separated by ""nulls" at which the radiation goes to zero. 
The larger the antenna is compared to a wavelength, the more lobes there will be. In a directive antenna in which the objective is to direct the radio waves in one particular direction, the lobe in that direction is larger than the others; this is called the "main lobe". The axis of maximum radiation, passing through the center of the main lobe, is called the "beam axis"" or "boresight axis". In some antennas, such as split-beam antennas, there may exist more than one major lobe. A minor lobe is any lobe except a major lobe.
The other lobes, representing unwanted radiation in other directions, are called "side lobes". The side lobe in the opposite direction (180°) from the main lobe is called the "back lobe"". Usually it refers to a minor lobe that
occupies the hemisphere in a direction opposite to that of the major (main) lobe.
Minor lobes usually represent radiation in undesired directions, and they should be minimized. Side lobes are normally the largest of the minor lobes. The level of minor lobes is usually expressed as a ratio of the power density in the lobe in question to that of the major lobe. This ratio is often termed the side lobe ratio or side lobe level. Side lobe levels of −20 dB or smaller are usually not desirable in many applications. Attainment of a side lobe level smaller than −30 dB usually requires very careful design and construction. In most radar systems, for example, low side lobe ratios are very important to minimize false target indications through the side lobes.
Proof of reciprocity.
For a complete proof, see the reciprocity (electromagnetism) article. Here, we present a common simple proof limited to the approximation of two antennas separated by a large distance compared to the size of the antenna, in a homogeneous medium. The first antenna is the test antenna whose patterns are to be investigated; this antenna is free to point in any direction. The second antenna is a reference antenna, which points rigidly at the first antenna.
Each antenna is alternately connected to a transmitter having a particular source impedance, and a receiver having the same input impedance (the impedance may differ between the two antennas).
It will be assumed that the two antennas are sufficiently far apart that the properties of the transmitting antenna are not affected by the load placed upon it by the receiving antenna. Consequently, the amount of power transferred from the transmitter to the receiver can be expressed as the product of two independent factors; one depending on the directional properties of the transmitting antenna, and the other depending on the directional properties of the receiving antenna.
For the transmitting antenna, by the definition of gain, formula_1, the radiation power density at a distance formula_2 from the antenna (i.e. the power passing through unit area) is
Here, the arguments formula_4 and formula_5 indicate a dependence on direction from the antenna, and formula_6 stands for the power the transmitter would deliver into a matched load. The gain formula_1 may be broken down into three factors; the antenna gain (the directional redistribution of the power), the radiation efficiency (accounting for ohmic losses in the antenna), and lastly the loss due to mismatch between the antenna and transmitter. Strictly, to include the mismatch, it should be called the realized gain, but this is not common usage.
For the receiving antenna, the power delivered to the receiver is 
Here formula_9 is the power density of the incident radiation, and formula_10 is the antenna aperture or effective area of the antenna (the area the antenna would need to occupy in order to intercept the observed captured power). The directional arguments are now relative to the receiving antenna, and again formula_10 is taken to include ohmic and mismatch losses.
Putting these expressions together, the power transferred from transmitter to receiver is
where formula_1 and formula_10 are directionally dependent properties of the transmitting and receiving antennas respectively. For transmission from the reference
antenna (2), to the test antenna (1), that is
and for transmission in the opposite direction
Here, the gain formula_17 and effective area formula_18 of antenna 2 are fixed, because the orientation of this antenna is fixed with respect to the first.
Now for a given disposition of the antennas, the reciprocity theorem requires that the power transfer is equally effective in each direction, i.e.
whence
But the right hand side of this equation is fixed (because the orientation of antenna 2 is fixed), and so
i.e. the directional dependence of the (receiving) effective aperture and the (transmitting) gain are identical (QED). Furthermore, the constant of proportionality is the same irrespective of the nature of the antenna, and so must be the same for all antennas. Analysis of a particular antenna (such as a Hertzian dipole), shows that this constant is formula_22, where formula_23 is the free-space wavelength. Hence, for any antenna the gain and the effective aperture are related by
Even for a receiving antenna, it is more usual to state the gain than to specify the effective aperture. The power delivered to the receiver is therefore more usually written as
(see link budget). The effective aperture is however of interest for comparison with the actual physical size of the antenna.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41622" url="http://en.wikipedia.org/wiki?curid=41622" title="Radio equipment">
Radio equipment

Radio equipment, as defined in "Federal Information Management Regulations", is any equipment or interconnected system or subsystem of equipment (both transmission and reception) that is used to communicate over a distance by modulating and radiating electromagnetic waves in space without artificial guide. This does not include such items as microwave, satellite, or cellular telephone equipment.

</doc>
<doc id="41623" url="http://en.wikipedia.org/wiki?curid=41623" title="Radio fix">
Radio fix

In telecommunication and position fixing, the term radio fix has the following meanings: 
Compare triangulation.
Obtaining a radio fix.
A single transmitter can be used to give a line of position (LOP) of the craft. The (true) bearing to the station from the craft, TB or QUJ, is composed of the true heading, TH, plus the relative bearing, RB, of the station. The bearing from the station (QTE) is found by adding 180° to the QUJ figure.
The line of position is then the line of bearing QUJ (i.e. from the station to the receiver) passing through the station.
For the diagram on the right, we have:
A radio fix on two stations can be found in exactly the same way. The intersection of the two position lines gives the position of the receiver. For the diagram on the right, the LOPs are found as before:
Remembering that the LOPs pass through their respective stations, it is now simple to find the location of the craft.
Remember too, that bearings and direction are given/recorded with respect to True North and to Magnetic North. Values used by mobile stations usually need to be converted from Magnetic to True. (Fixed stations are expected to use True).

</doc>
<doc id="41625" url="http://en.wikipedia.org/wiki?curid=41625" title="Radiometry">
Radiometry

Radiometry is a set of techniques for measuring electromagnetic radiation, including visible light. Radiometric techniques in optics characterize the distribution of the radiation's power in space, as opposed to photometric techniques, which characterize the light's interaction with the human eye. Radiometry is distinct from quantum techniques such as photon counting.
The use of radiometers to determine the temperature of objects and gasses by measuring radiation flux is called pyrometry. Handheld pyrometer devices are often marketed as infrared thermometers.
Radiometry is important in astronomy, especially radio astronomy, and plays a significant role in Earth remote sensing. The measurement techniques categorized as "radiometry" in optics are called "photometry" in some astronomical applications, contrary to the optics usage of the term.
Spectroradiometry is the measurement of absolute radiometric quantities in narrow bands of wavelength.
Integral and spectral radiometric quantities.
Integral quantities (like radiant flux) describe the total effect of radiation of all wavelengths or frequencies, while spectral quantities (like spectral power) describe the effect of radiation of a single wavelength "λ" or frequency "ν". To each integral quantity there are corresponding spectral quantities, for example the radiant flux Φe corresponds to the spectral power Φe,"λ" and Φe,"ν".
Getting an integral quantity's spectral counterpart requires a limit transition. This comes from the idea that the precisely requested wavelength photon existence probability is zero. Let us show the relation between them using the radiant flux as an example:
Integral flux, whose unit is W:
Spectral flux by wavelength, whose unit is W/m:
where formula_3 is the radiant flux of the radiation in a small wavelength interval ["λ", "λ" + d"λ"].
The area under a plot with wavelength horizontal axis equals to the total radiant flux.
Spectral flux by frequency, whose unit is W/Hz:
where formula_3 is the radiant flux of the radiation in a small frequency interval ["ν", "ν" + d"ν"].
The area under a plot with frequency horizontal axis equals to the total radiant flux.
Spectral flux multiplied by wavelength or frequency, whose unit is W, i.e. the same as the integral quantity:
The area under a plot with logarithmic wavelength or frequency horizontal axis equals to the total radiant flux.
The spectral quantities by wavelength "λ" and frequency "ν" are related by equations featuring the speed of light "c":
The integral quantity can be obtained by the spectral quantity's integration:

</doc>
<doc id="41627" url="http://en.wikipedia.org/wiki?curid=41627" title="Random number">
Random number

Random number may refer to:

</doc>
<doc id="41628" url="http://en.wikipedia.org/wiki?curid=41628" title="Receive-after-transmit time delay">
Receive-after-transmit time delay

In telecommunication, receive-after-transmit time delay is the time interval between (a) the instant of keying off the local transmitter to stop transmitting and (b) the instant the local receiver output has increased to 90% of its steady-state value in response to an rf signal from a distant transmitter. 
The rf signal from the distant transmitter must exist at the local receiver input prior to, or at the time of, keying off the local transmitter. 
Receive-after-transmit time delay applies only to half-duplex operation.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41629" url="http://en.wikipedia.org/wiki?curid=41629" title="Received noise power">
Received noise power

In telecommunications, received noise power is a measure of noise in a receiver. For example, the received noise power might be:
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41630" url="http://en.wikipedia.org/wiki?curid=41630" title="Attack-time delay">
Attack-time delay

In telecommunications, attack-time delay is the time needed for a receiver or transmitter to respond to an incoming signal.
For a receiver, the attack-time delay is defined as the time interval from the instant a step radio-frequency (RF) signal, at a level equal to the receiver's threshold of sensitivity, is applied to the receiver input, to the instant when the receiver's output amplitude reaches 90% of its steady-state value. If a squelch circuit is operating, the receiver attack-time delay includes the time for the receiver to break squelch. 
For a transmitter, the attack-time delay is defined as the interval from the instant the transmitter is keyed-on to the instant the transmitted RF signal amplitude has increased to a specified level, usually 90% of its key-on steady-state value. The transmitter attack-time delay excludes the time required for automatic antenna tuning.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41633" url="http://en.wikipedia.org/wiki?curid=41633" title="Recorder warning tone">
Recorder warning tone

In telecommunication, a recorder warning tone is a tone transmitted over a telephone line to indicate to the called party that the calling party is recording the conversation.
In the US, the recorder warning tone is a half-second burst of 1400 Hz applied every 15 seconds. The recorder warning tone is required by law to be generated as an integral part of any recording device used for the purpose and is required to be not under the control of the calling party. The tone is recorded together with the conversation.

</doc>
<doc id="41635" url="http://en.wikipedia.org/wiki?curid=41635" title="Recovery procedure">
Recovery procedure

In telecommunication, a recovery procedure is a process that attempts to bring a system back to a normal operating state. Examples:

</doc>
<doc id="41636" url="http://en.wikipedia.org/wiki?curid=41636" title="Reference circuit">
Reference circuit

A reference circuit is a hypothetical electric circuit of specified equivalent length and configuration, and having a defined transmission characteristic or characteristics, used primarily as a reference for measuring the performance of other, "i.e.," real, circuits or as a guide for planning and engineering of circuits and networks. 
Normally, several types of reference circuits are defined, with different configurations, because communications are required over a wide range of distances. Another type of reference circuit shows how to configure integrated circuits into function blocks, which Analog Devices provides for electrical design engineers. Analog Devices' Circuits from the Lab reference circuits are fully tested and come with the schematics, evaluation boards, and device drivers necessary for system integration. A group of related reference circuits is also called a "reference system".

</doc>
<doc id="41637" url="http://en.wikipedia.org/wiki?curid=41637" title="Reference clock">
Reference clock

A reference clock may refer to the following:

</doc>
<doc id="41638" url="http://en.wikipedia.org/wiki?curid=41638" title="Cycloid">
Cycloid

A cycloid is the curve traced by a point on the rim of a circular wheel as the wheel rolls along a straight line without slippage.
It is an example of a roulette, a curve generated by a curve rolling on another curve.
The inverted cycloid (a cycloid rotated through 180°) is the solution to the brachistochrone problem (i.e., it is the curve of fastest descent under gravity) and the related tautochrone problem (i.e., the period of an object in descent without friction inside this curve does not depend on the object's starting position).
History.
It was in the left hand try-pot of the Pequod, with the soapstone diligently circling round me, that I was first indirectly struck by the remarkable fact, that in geometry all bodies gliding along the cycloid, my soapstone for example, will descend from any point in precisely the same time.
Moby Dick by Herman Melville, 1851
The cycloid has been called "The Helen of Geometers" as it caused frequent quarrels among 17th-century mathematicians.
Historians of mathematics have proposed several candidates for the discoverer of the cycloid. Mathematical historian Paul Tannery cited similar work by the Syrian philosopher Iamblichus as evidence that the curve was likely known in antiquity. English mathematician John Wallis writing in 1679 attributed the discovery to Nicholas of Cusa, but subsequent scholarship indicates Wallis was either mistaken or the evidence used by Wallis is now lost. Galileo Galilei's name was put forward at the end of the 19th century and at least one author reports credit being given to Marin Mersenne. Beginning with the work of Moritz Cantor and Siegmund Günther, scholars now assign priority to French mathematician Charles de Bovelles based on his description of the cycloid in his "Introductio in geometriam", published in 1503. In this work, Bovelles mistakes the arch traced by a rolling wheel as part of a larger circle with a radius 120% larger than the smaller wheel.
Galileo originated the term "cycloid" and was the first to make a serious study of the curve. According to his student Evangelista Torricelli, in 1599 Galileo attempted the quadrature of the cycloid (constructing a square with area equal to the area under the cycloid) with an unusually empirical approach that involved tracing both the generating circle and the resulting cycloid on sheet metal, cutting them out and weighing them. He discovered the ratio was roughly 3:1 but incorrectly concluded the ratio was an irrational fraction, which would have made quadrature impossible. Around 1628, Gilles Persone de Roberval likely learned of the quadrature problem from Père Marin Mersenne and effected the quadrature in 1634 by using Cavalieri's Theorem. However, this work was not published until 1693 (in his "Traité des Indivisibles").
Constructing the tangent of the cycloid dates to August 1638 when Mersenne received unique methods from Roberval, Pierre de Fermat and René Descartes. Mersenne passed these results along to Galileo, who gave them to his students Torricelli and Viviana, who were able to produce a quadrature. This result and others were published by Torricelli in 1644, which is also the first printed work on the cycloid. This led to Roberval charging Torricelli with plagiarism, with the controversy cut short by Torricelli's early death in 1647.
In 1658, Blaise Pascal had given up mathematics for theology but, while suffering from a toothache, began considering several problems concerning the cycloid. His toothache disappeared, and he took this as a heavenly sign to proceed with his research. Eight days later he had completed his essay and, to publicize the results, proposed a contest. Pascal proposed three questions relating to the center of gravity, area and volume of the cycloid, with the winner or winners to receive prizes of 20 and 40 Spanish doubloons. Pascal, Roberval and Senator Carcavy were the judges, and neither of the two submissions (by John Wallis and Antoine Lalouvère) were judged to be adequate.:198 While the contest was ongoing, Christopher Wren sent Pascal a proposal for a proof of the rectification of the cycloid; Roberval claimed promptly that he had known of the proof for years. Wallis published Wren's proof (crediting Wren) in Wallis's "Tractus Duo", giving Wren priority for the first published proof.
Fifteen years later, Christiaan Huygens had deployed the cycloidal pendulum to improve chronometers and had discovered that a particle would traverse an inverted cycloidal arch in the same amount of time, regardless of its starting point. In 1686, Gottfried Wilhelm Leibniz used analytic geometry to describe the curve with a single equation. In 1696, Johann Bernoulli posed the brachistochrone problem, the solution of which is a cycloid.
Equations.
The cycloid through the origin, generated by a circle of radius "r", consists of the points ("x", "y"), with
where "t" is a real parameter, corresponding to the angle through which the rolling circle has rotated, measured in radians. For given "t", the circle's centre lies at "x" = "rt", "y" = "r".
Solving for "t" and replacing, the Cartesian equation is found to be:
An expression of the equation in the form "y" = "f"("x") is not possible using standard functions.
The first arch of the cycloid consists of points such that
When "y" is viewed as a function of "x", the cycloid is differentiable everywhere except at the cusps where it hits the "x"-axis, with the derivative tending toward formula_4 or formula_5 as one approaches a cusp. The map from "t" to ("x", "y") is a differentiable curve or parametric curve of class "C"∞ and the singularity where the derivative is 0 is an ordinary cusp.
The cycloid satisfies the differential equation:
Evolute.
The evolute of the cycloid has the property of being exactly the same cycloid it originates from. This can otherwise be seen from the tip of a wire initially lying on a half arc of cycloid describing a cycloid arc equal to the one it was lying on once unwrapped (see also cycloidal pendulum and arc length).
Demonstration.
There are several demonstrations of the assertion. The one presented here uses the physical definition of cycloid and the kinematic property that the instantaneous velocity of a point is tangent to its trajectory.
Referring to the picture on the right, formula_7 and formula_8 are two tangent points belonging to two rolling circles. The two circles start to roll with same speed and same direction without skidding. formula_7 and formula_8 start to draw two cycloid arcs as in the picture. Considering the line connecting formula_7 and formula_8 at an arbitrary instant (red line), it is possible to prove that "the line is anytime tangent in P2 to the lower arc and orthogonal to the tangent in P1 of the upper arc". One sees that:
Area.
One arch of a cycloid generated by a circle of radius "r" can be parameterized by
with 
formula_24
Since
the area under the arch is
Arc length.
The arc length "S" of one arch is given by
Another immediate way to calculate the length of the cycloid given the properties of the Evolute is to notice that when a wire describing an evolute has been completely unwrapped it extends itself along two diameters, a length of 4r. Because the wire does not change length during the unwrapping it follows that the length of half an arc of cycloid is 4r and a complete arc is 8r.
Cycloidal pendulum.
If a simple pendulum is suspended from the cusp of an inverted cycloid, such that the "string" is constrained between the adjacent arcs of the cycloid, and the pendulum's length is equal to that of half the arc length of the cycloid (i.e., twice the diameter of the generating circle), the bob of the pendulum also traces a cycloid path. Such a cycloidal pendulum is isochronous, regardless of amplitude. The equation of motion is given by:
The 17th-century Dutch mathematician Christiaan Huygens discovered and proved these properties of the cycloid while searching for more accurate pendulum clock designs to be used in navigation.
Related curves.
Several curves are related to the cycloid.
All these curves are roulettes with a circle rolled along a uniform curvature. The cycloid, epicycloids, and hypocycloids have the property that each is similar to its evolute. If "q" is the product of that curvature with the circle's radius, signed positive for epi- and negative for hypo-, then the curve:evolute similitude ratio is 1 + 2"q".
The classic Spirograph toy traces out hypotrochoid and epitrochoid curves.
Use in architecture.
The cycloidal arch was used by architect Louis Kahn in his design for the Kimbell Art Museum in Fort Worth, Texas. It was also used in the design of the Hopkins Center in Hanover, New Hampshire.
Use in violin plate arching.
Early research indicated that some transverse arching curves of the plates of golden age violins are closely modeled by curtate cycloid curves. Later work indicates that curtate cycloids do not serve as general models for these curves, which vary considerably.

</doc>
<doc id="41639" url="http://en.wikipedia.org/wiki?curid=41639" title="Reference noise">
Reference noise

In telecommunication, reference noise is the magnitude of circuit noise chosen as a reference for measurement. 
Many different levels with a number of different weightings are in current use, and care must be taken to ensure that the proper parameters are stated. 
Specific ones include: dBa, dBa(F1A), dBa(HA1), dBa0, dBm, dBm(psoph), dBm0, dBrn, dBrnC, dBrnC0, dBrn(f1-f2), dBrn(144-line), dBx.

</doc>
<doc id="41640" url="http://en.wikipedia.org/wiki?curid=41640" title="Reference surface">
Reference surface

In fiber optic technology, a reference surface is that surface of an optical fiber that is used to contact the transverse-alignment elements of a component such as a connector or mechanical splice. For telecommunications-grade fibers, the reference surface is the outer surface of the cladding. For plastic-clad silica (PCS) fibers, which have a strippable polymer cladding (not to be confused with the polymer overcoat of an all-silica fiber), the reference surface may be the core.

</doc>
<doc id="41641" url="http://en.wikipedia.org/wiki?curid=41641" title="Reflection coefficient">
Reflection coefficient

In physics and electrical engineering the reflection coefficient is a parameter that describes how much of an electromagnetic wave is reflected by an impedance discontinuity in the transmission medium. It is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors. For example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the radio wave is reflected by an impedance. The reflection coefficient is closely related to the "transmission coefficient". The reflectance of a system is also sometimes called a "reflection coefficient".
Different specialties have different applications for the term.
Telecommunications.
In telecommunications, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave. In particular, at a discontinuity in a transmission line, it is the complex ratio of the electric field strength of the reflected wave (formula_1) to that of the incident wave (formula_2). This is typically represented with a formula_3 (capital gamma) and can be written as:
The reflection coefficient may also be established using other field or circuit quantities. 
The reflection coefficient of a load is determined by its impedance formula_5 and the impedance toward the source formula_6
formula_7
Notice that a negative reflection coefficient means that the reflected wave receives a 180°, or formula_8, phase shift.
The magnitude (designated by vertical bars) of the reflection coefficient can be calculated from the standing wave ratio, formula_9:
The reflection coefficient is displayed graphically using a Smith chart.
Seismology.
Reflection coefficient is used in feeder testing for reliability of medium.
Optics and microwaves.
In optics and electromagnetics in general, "reflection coefficient" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. Typically, the reflectance is represented by a capital "R", while the amplitude reflection coefficient is represented by a lower-case "r".
Semipermeable membranes.
The reflection coefficient in semipermeable membranes relates to how such a membrane can reflect solute particles from passing through. A value of zero results in all particles passing through. A value of one is such that no particle can pass. It is used in the Starling equation.

</doc>
<doc id="41642" url="http://en.wikipedia.org/wiki?curid=41642" title="Reflection loss">
Reflection loss

In telecommunications, reflection loss occurs on a line which results in part of the energy being reflected back to the source. This can occur:

</doc>
<doc id="41643" url="http://en.wikipedia.org/wiki?curid=41643" title="Reflective array antenna">
Reflective array antenna

In telecommunication and radar, a reflective array antenna is a class of directive antennas in which multiple driven elements are mounted in front of a flat surface designed to reflect the radio waves in a desired direction. They are often used in the VHF frequency band, and these versions often resemble a highway billboard, so they are sometimes called "billboard antennas". The curtain array is a larger version used by shortwave radio stations.
Reflective array antennas usually have a number of identical driven elements, fed in phase, in front of a flat, electrically large reflecting surface to produce a unidirectional beam, increasing antenna gain and reducing radiation in unwanted directions. The individual elements are most commonly half wave dipoles, although they sometimes contain parasitic elements as well as driven elements. The reflector may be a metal sheet or more commonly a wire screen. A metal screen reflects radio waves as well as a solid metal sheet as long as the holes in the screen are smaller than about one-tenth of a wavelength, so screens are often used to reduce weight and wind loads on the antenna. They usually consist of a grill of parallel wires or rods, oriented parallel to the axis of the dipole elements.
The driven elements are fed by a network of transmission lines, which divide the power from the RF source equally between the elements. This often has the circuit geometry of a tree structure.
Radiation pattern and beam steering.
When driven in phase, the radiation pattern of the reflective array is a single main lobe perpendicular to the plane of the antenna, plus several sidelobes at equal angles to either side. The more elements used, the narrower the main lobe and the less power is radiated in the sidelobes. The main lobe of the antenna can be steered electronically within a limited angle by phase shifting the drive signals applied to the individual elements. Each antenna element is fed through a phase shifter which can be controlled digitally, delaying each signal by a successive amount. This causes the wavefronts created by the superposition of the individual elements to be at an angle to the plane of the antenna. Antennas that use this technique are called phased arrays and are being intensively developed, particularly for use in radar systems. 
Another option for steering the beam is mounting the entire array structure on a rotating bearing and rotating it mechanically.
Gain limits.
The more driven elements that are used, the larger the antenna is compared to a wavelength, and the higher the gain, and the narrower the beamwidth of the antenna's main lobe. However, as the number of driven elements increases, the complexity of the required feed network increases. Ultimately, the rising inherent losses in the feed network become greater than the additional gain achieved with more elements, limiting the maximum gain that can be achieved. The gain of practical array antennas is limited to about 25 - 30 dB. "Active" array antennas, in which groups of elements are driven by separate RF amplifiers, can have much higher gain, but are prohibitively expensive.
Since the 1980s, versions for use at microwave frequencies have been made with patch antenna elements mounted in front of a metal surface.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41644" url="http://en.wikipedia.org/wiki?curid=41644" title="Reflectance">
Reflectance

Reflectance of the surface of a material is its effectiveness in reflecting radiant energy. It is the fraction of incident electromagnetic power that is reflected at an interface. The reflectance spectrum or spectral reflectance curve is the plot of the reflectance as a function of wavelength.
Mathematical definitions.
Hemispherical reflectance.
Hemispherical reflectance of a surface, denoted "R", is defined as
where
Spectral hemispherical reflectance.
Spectral hemispherical reflectance in frequency and spectral hemispherical reflectance in wavelength of a surface, denoted "R"ν and "R"λ respectively, are defined as
where
Directional reflectance.
Directional reflectance of a surface, denoted "R"Ω, is defined as
where
Spectral directional reflectance.
Spectral directional reflectance in frequency and spectral directional reflectance in wavelength of a surface, denoted "R"ν,Ω and "R"λ,Ω respectively, are defined as
where
Reflectivity.
For homogeneous and semi-infinite (see halfspace) materials, reflectivity is the same as reflectance. 
Reflectivity is the square of the magnitude of the Fresnel reflection coefficient,
which is the ratio of the reflected to incident electric field; 
as such the reflection coefficient can be expressed as a complex number as determined by the Fresnel equations for a single layer, whereas the reflectance is always a positive real number.
For layered and finite media, according to the CIE, reflectivity is distinguished from reflectance by the fact that reflectivity is a value that applies to "thick" reflecting objects. When reflection occurs from thin layers of material, internal reflection effects can cause the reflectance to vary with surface thickness. Reflectivity is the limit value of reflectance as the sample becomes thick; it is the intrinsic reflectance of the surface, hence irrespective of other parameters such as the reflectance of the rear surface. Another way to interpret this is that the reflectance is the fraction of electromagnetic power reflected from a specific sample, while reflectivity is a property of the material itself, which would be measured on a perfect machine if the material filled half of all space.
Surface type.
Going back to the fact that reflectance is a directional property, most surfaces can be divided into those that give specular reflection and those that give diffuse reflection:
Most real objects have some mixture of diffuse and specular reflective properties.
Water reflectance.
Reflection occurs when light moves from a medium with one index of refraction into a second medium with a different index of refraction.
Specular reflection from a body of water is calculated by the Fresnel equations. Fresnel reflection is directional and therefore does not contribute significantly to albedo which is primarily diffuse reflection.
A real water surface may be wavy. Reflectance assuming a flat surface as given by the Fresnel equations can be adjusted to account for waviness.
Grating efficiency.
The generalization of reflectance to a diffraction grating, which disperses light by wavelength, is called diffraction efficiency.
Applications.
Reflectance is an important concept in the fields of optics, solar thermal energy, telecommunication and radar.

</doc>
<doc id="41646" url="http://en.wikipedia.org/wiki?curid=41646" title="Refractive index contrast">
Refractive index contrast

Refractive index contrast, in an optical fiber, is a measure of the relative difference in refractive index of the core and cladding. Refractive index contrast, Δ, is given by Δ = ("n"12- "n"22)/(2"n"12), where "n"1 is the maximum refractive index in the core and "n"2 is the refractive index of the homogeneous cladding. Normal optical fibers have very low refractive index contrast(Δ«1)hence are weakly guided medium. The weak guiding will cause more of the Electrical field to "leak" and travel through the cladding(as evanescent waves) as compared to the strongly guided waveguides. 

</doc>
<doc id="41647" url="http://en.wikipedia.org/wiki?curid=41647" title="Reframing time">
Reframing time

In telecommunication, the reframing time (or frame-alignment recovery time) is the time interval between the instant at which a valid frame-alignment signal is available at the receiving data terminal equipment and the instant at which frame alignment is established. 
The reframing time includes the time required for replicated verification of the validity of the frame-alignment signal.

</doc>
<doc id="41648" url="http://en.wikipedia.org/wiki?curid=41648" title="Regeneration">
Regeneration

Regeneration is renewal through the internal processes of a body or system. Something which embodies this action can be described as regenerative. Many titles of cultural work and cultural and scientific concepts use the term, and may refer to:

</doc>
<doc id="41649" url="http://en.wikipedia.org/wiki?curid=41649" title="Relative transmission level">
Relative transmission level

In telecommunication, relative transmission level is the ratio of the signal power, at a given point in a transmission system, to a reference signal power. 
The ratio is usually determined by applying a standard test tone at zero transmission level point (or applying adjusted test tone power at any other point) and measuring the gain or loss to the location of interest. A distinction should be made between the standard test tone power and the expected median power of the actual signal required as the basis for the design of transmission systems.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41650" url="http://en.wikipedia.org/wiki?curid=41650" title="Release time (telecommunication)">
Release time (telecommunication)

In telecommunication, release time is the time interval for a circuit to respond when an enabling signal is discontinued, for example:

</doc>
<doc id="41651" url="http://en.wikipedia.org/wiki?curid=41651" title="Reliability">
Reliability

Reliability may refer to:

</doc>
<doc id="41652" url="http://en.wikipedia.org/wiki?curid=41652" title="Remote access">
Remote access

Remote access may refer to:

</doc>
<doc id="41653" url="http://en.wikipedia.org/wiki?curid=41653" title="Remote call forwarding">
Remote call forwarding

In telecommunication, a remote call forwarding is a service feature that allows calls coming to a remote call forwarding number to be automatically forwarded to any answering location designated by the call receiver.
Customers may have a remote-forwarding telephone number in a central switching office without having any other local telephone service in that office.
One common purpose for this service is to enable customers to retain their telephone number when they move to a location serviced by a different telephone exchange. The service is useful for business customers with widely-advertised numbers which appear on headed paper, vehicles and various marketing literature. When customers ring, their calls are seamlessly forwarded to the new location.
Remote call forwarding is also a means for a suburban business to obtain a city-centre local number (with its full large-city coverage area) for inbound calls; while cheaper than a foreign exchange line, this can reduce long-distance telephony costs in markets where local calls are flat-rated but trunk calls are expensive.
One alternative to RCF is Caller Redirect whereby callers simply hear an intercept message notifying them that the number has changed. Another alternative is to port the existing number to a voice over IP carrier, which is not tied to a single physical location as the subscriber may be anywhere on broadband Internet.
Remote Access to Call Forwarding.
Remote Call Forwarding (RCF) requires neither a physical telephone set nor physical input by customer to get calls forwarded.
In this respect, it differs from the (similarly named) Remote Access to Call Forwarding in that the number is attached to a physical line where it rings normally until a call is made to a remote number to enable redirection.
To activate Remote Access to Call Forwarding, a subscriber calls a provider-supplied Remote Access Directory Number, enters the telephone number of the line to be redirected along with a Personal Identification Number (PIN), a vertical service code (such as 72# or *73) and the number to which the calls are to be forwarded.
Remote Access to Call Forwarding allows incoming calls to be diverted and answered elsewhere if a subscriber cannot use their telephone normally (for instance, the number is assigned to a lost or stolen wireless handset or to a landline in need of repair service). In some cases, a business which subscribes to standard call forwarding (*72) may be able to request temporary redirection of inbound calls when calling a telco repair service number (such as 6-1-1) to report a line outage.

</doc>
<doc id="41654" url="http://en.wikipedia.org/wiki?curid=41654" title="Remote Operations Service Element protocol">
Remote Operations Service Element protocol

The Remote Operations Service Element (ROSE) is the OSI service interface, specified in , that (a) provides remote operation capabilities, (b) allows interaction between entities in a distributed application, and (c) upon receiving a remote operations service request, allows the receiving entity to attempt the operation and report the results of the attempt to the requesting entity. 
OSI application protocols such as X.400 and X.500 utilize the services provided by ROSE. The ROSE protocol itself is defined using the notation of ASN.1.

</doc>
<doc id="41655" url="http://en.wikipedia.org/wiki?curid=41655" title="Repeater">
Repeater

In telecommunications, a repeater is an electronic device that receives a signal and retransmits it at a higher level or higher power, or onto the other side of an obstruction, so that the signal can cover longer distances. It is a generic term that refers to several different types of devices; a "telephone repeater" is an amplifier in a telephone line, an "optical repeater" is an optoelectronic circuit that amplifies the light beam in a optical fiber cable; and a "radio repeater" is a radio receiver and transmitter that retransmits a radio signal. 
A broadcast relay station performs an analogous role in broadcast radio and television.
Overview.
When an information-bearing signal passes through a communication channel, it is progressively degraded due to loss of power. For example, when a telephone call passes through a wire telephone line, some of the power in the alternating electric current which represents the audio signal is dissipated as heat in the resistance of the copper wire. The longer the wire is, the more power is lost, and the smaller the amplitude of the signal at the far end. So with a long enough wire the call will not be audible at the other end. Similarly, the farther from a radio station a receiver is, the weaker the radio signal, and the poorer the reception. A repeater is an electronic device in a communication channel that increases the power of a signal and retransmits it, allowing it to travel further. Since it amplifies the signal, it requires a source of electric power.
The term "repeater" originated with telegraphy in the 19th century, and referred to an electromechanical device (a relay) used to regenerate telegraph signals. Use of the term has continued in telephony and data communications.
In computer networking, because repeaters work with the actual physical signal, and do not attempt to interpret the data being transmitted, they operate on the physical layer, the first layer of the OSI model.
Types.
Repeaters can be divided into two types depending on the type of data they handle:
Talk around channel.
In a system using radio repeaters, a designated channel may be provided that allows direct mobile-to-mobile communication. This "talk-around channel" can be used if the repeater is out of order, busy, or if the mobiles are operating out of the coverage area of the repeater. 
Telephone repeater.
Before the invention of electronic amplifiers, mechanically coupled carbon microphones were used as amplifiers in telephone repeaters. After the turn of the century it was found that negative resistance mercury lamps could amplify, and they were used. The invention of audion tube repeaters around 1916 made transcontinental telephony practical. In the 1930s vacuum tube repeaters using hybrid coils became commonplace, allowing the use of thinner wires. In the 1950s negative impedance gain devices were more popular, and a transistorized version called the E6 repeater was the final major type used in the Bell System before the low cost of digital transmission made all voiceband repeaters obsolete. Frequency frogging repeaters were commonplace in frequency-division multiplexing systems from the middle to late 20th century...

</doc>
<doc id="41656" url="http://en.wikipedia.org/wiki?curid=41656" title="Repeating coil">
Repeating coil

In telecommunications, a repeating coil is a voice-frequency transformer characterized by a closed magnetic core, a pair of identical balanced primary (line) windings, a pair of identical but not necessarily balanced secondary (drop) windings, and low transmission loss at voice frequencies. It permits transfer of voice currents from one winding to another by magnetic induction, matches line and drop impedances, and prevents direct conduction between the line and the drop.
It is a special application of an isolation transformer, and is often used to prevent ground loops or earth loops, which cause humming or buzzing in audio circuits. It also prevents low direct current voltages from passing.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41657" url="http://en.wikipedia.org/wiki?curid=41657" title="Reproduction speed">
Reproduction speed

In telecommunication, the term reproduction speed has the following meanings: 
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41658" url="http://en.wikipedia.org/wiki?curid=41658" title="Reradiation">
Reradiation

In telecommunication, the term reradiation has the following meanings:
Near-field effects of an AM antenna may extend out two miles (3 km) or more. Cellular and microwave towers within this radius can reflect the AM signal out at a different frequency. This process results in interfering frequencies called reradiation.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41659" url="http://en.wikipedia.org/wiki?curid=41659" title="Resolution">
Resolution

Resolution may refer to:

</doc>
<doc id="41660" url="http://en.wikipedia.org/wiki?curid=41660" title="Resonance">
Resonance

In physics, resonance is a phenomenon that consists of a given system being driven by another vibrating system or by external forces to oscillate with greater amplitude at some preferential frequencies. Frequencies at which the response amplitude is a relative maximum are known as the system's resonant frequencies, or resonance frequencies. At resonant frequencies, small periodic driving forces have the ability to produce large amplitude oscillations. This is because the system stores vibrational energy.
Resonance occurs when a system is able to store and easily transfer energy between two or more different storage modes (such as kinetic energy and potential energy in the case of a pendulum). However, there are some losses from cycle to cycle, called damping. When damping is small, the resonant frequency is approximately equal to the natural frequency of the system, which is a frequency of unforced vibrations. Some systems have multiple, distinct, resonant frequencies.
Resonance phenomena occur with all types of vibrations or waves: there is mechanical resonance, acoustic resonance, electromagnetic resonance, nuclear magnetic resonance (NMR), electron spin resonance (ESR) and resonance of quantum wave functions. Resonant systems can be used to generate vibrations of a specific frequency (e.g., musical instruments), or pick out specific frequencies from a complex vibration containing many frequencies (e.g., filters).
The term Resonance (from Latin resonantia ‘echo,’ from resonare ‘resound’) originates from the field of acoustics, particularly observed in musical instruments, e.g. when strings started to vibrate and to produce sound without direct excitation by the player. 
Examples.
One familiar example is a playground swing, which acts as a pendulum. Pushing a person in a swing in time with the natural interval of the swing (its resonant frequency) will make the swing go higher and higher (maximum amplitude), while attempts to push the swing at a faster or slower tempo will result in smaller arcs. This is because the energy the swing absorbs is maximized when the pushes are "in phase" with the swing's natural oscillations, while some of the swing's energy is actually extracted by the opposing force of the pushes when they are not.
Resonance occurs widely in nature, and is exploited in many manmade devices. It is the mechanism by which virtually all sinusoidal waves and vibrations are generated. Many sounds we hear, such as when hard objects of metal, glass, or wood are struck, are caused by brief resonant vibrations in the object. Light and other short wavelength electromagnetic radiation is produced by resonance on an atomic scale, such as electrons in atoms. Other examples are:
Theory.
The exact response of a resonance, especially for frequencies far from the resonant frequency, depends on the details of the physical system, and is usually not exactly symmetric about the resonant frequency, as illustrated for the simple harmonic oscillator above.
For a lightly damped linear oscillator with a resonance frequency Ω, the "intensity" of oscillations "I" when the system is driven with a driving frequency ω is typically approximated by a formula that is symmetric about the resonance frequency:
The intensity is defined as the square of the amplitude of the oscillations. This is a Lorentzian function, and this response is found in many physical situations involving resonant systems. Γ is a parameter dependent on the damping of the oscillator, and is known as the "linewidth" of the resonance. Heavily damped oscillators tend to have broad linewidths, and respond to a wider range of driving frequencies around the resonant frequency. The linewidth is inversely proportional to the Q factor, which is a measure of the sharpness of the resonance.
In electrical engineering, this approximate symmetric response is known as the "universal resonance curve", a concept introduced by Frederick E. Terman in 1932 to simplify the approximate analysis of radio circuits with a range of center frequencies and Q values.
Resonators.
A physical system can have as many resonant frequencies as it has degrees of freedom; each degree of freedom can vibrate as a harmonic oscillator. Systems with one degree of freedom, such as a mass on a spring, pendulums, balance wheels, and LC tuned circuits have one resonant frequency. Systems with two degrees of freedom, such as coupled pendulums and resonant transformers can have two resonant frequencies. As the number of coupled harmonic oscillators grows, the time it takes to transfer energy from one to the next becomes significant. The vibrations in them begin to travel through the coupled harmonic oscillators in waves, from one oscillator to the next.
Extended objects that can experience resonance due to vibrations inside them are called resonators, such as organ pipes, vibrating strings, quartz crystals, microwave cavities, and laser rods. Since these can be viewed as being made of millions of coupled moving parts (such as atoms), they can have millions of resonant frequencies. The vibrations inside them travel as waves, at an approximately constant velocity, bouncing back and forth between the sides of the resonator. If the distance between the sides is formula_2, the length of a roundtrip is formula_3. In order to cause resonance, the phase of a sinusoidal wave after a roundtrip has to be equal to the initial phase, so the waves will reinforce. So the condition for resonance in a resonator is that the roundtrip distance, formula_3, be equal to an integer number of wavelengths formula_5 of the wave:
If the velocity of a wave is formula_7, the frequency is formula_8 so the resonant frequencies are:
So the resonant frequencies of resonators, called normal modes, are equally spaced multiples of a lowest frequency called the fundamental frequency. The multiples are often called overtones. There may be several such series of resonant frequencies, corresponding to different modes of vibration.
Q factor.
The Q factor or "quality factor" is a dimensionless parameter that describes how under-damped an oscillator or resonator is, or equivalently, characterizes a resonator's bandwidth relative to its center frequency.
Higher "Q" indicates a lower rate of energy loss relative to the stored energy of the oscillator, i.e., the oscillations die out more slowly. A pendulum suspended from a high-quality bearing, oscillating in air, has a high "Q", while a pendulum immersed in oil has a low "Q". In order to sustain a system in resonance in constant amplitude by providing power externally, the energy that has to be provided within each cycle is less than the energy stored in the system (i.e., the sum of the potential and kinetic) by a factor of formula_10. Oscillators with high-quality factors have low damping which tends to make them ring longer.
Sinusoidally driven resonators having higher Q factors resonate with greater amplitudes (at the resonant frequency) but have a smaller range of frequencies around the frequency at which they resonate. The range of frequencies at which the oscillator resonates is called the bandwidth. Thus, a high Q tuned circuit in a radio receiver would be more difficult to tune, but would have greater selectivity, it would do a better job of filtering out signals from other stations that lie nearby on the spectrum. High Q oscillators operate over a smaller range of frequencies and are more stable. (See oscillator phase noise.)
The quality factor of oscillators varies substantially from system to system. Systems for which damping is important (such as dampers keeping a door from slamming shut) have "Q" = 1⁄2. Clocks, lasers, and other systems that need either strong resonance or high frequency stability need high-quality factors. Tuning forks have quality factors around "Q" = 1000. The quality factor of atomic clocks and some high-Q lasers can reach as high as 1011 and higher.
There are many alternate quantities used by physicists and engineers to describe how damped an oscillator is that are closely related to its quality factor. Important examples include: the damping ratio, relative bandwidth, linewidth, and bandwidth measured in octaves.
Types of resonance.
Mechanical and acoustic resonance.
Mechanical resonance is the tendency of a mechanical system to absorb more energy when the frequency of its oscillations matches the system's natural frequency of vibration than it does at other frequencies. It may cause violent swaying motions and even catastrophic failure in improperly constructed structures including bridges, buildings, trains, and aircraft. When designing objects, engineers must ensure the mechanical resonance frequencies of the component parts do not match driving vibrational frequencies of motors or other oscillating parts, a phenomenon known as resonance disaster.
Avoiding resonance disasters is a major concern in every building, tower, and bridge construction project. As a countermeasure, shock mounts can be installed to absorb resonant frequencies and thus dissipate the absorbed energy. The Taipei 101 building relies on a 660 t—a tuned mass damper—to cancel resonance. Furthermore, the structure is designed to resonate at a frequency which does not typically occur. Buildings in seismic zones are often constructed to take into account the oscillating frequencies of expected ground motion. In addition, engineers designing objects having engines must ensure that the mechanical resonant frequencies of the component parts do not match driving vibrational frequencies of the motors or other strongly oscillating parts.
Clocks keep time by mechanical resonance in a balance wheel, pendulum, or quartz crystal.
The cadence of runners has been hypothesized to be energetically favorable due to resonance between the elastic energy stored in the lower limb and the mass of the runner.
Acoustic resonance is a branch of mechanical resonance that is concerned with the mechanical vibrations across the frequency range of human hearing, in other words sound. For humans, hearing is normally limited to frequencies between about 20 Hz and 20,000 Hz (20 kHz),
Acoustic resonance is an important consideration for instrument builders, as most acoustic instruments use resonators, such as the strings and body of a violin, the length of tube in a flute, and the shape of, and tension on, a drum membrane.
Like mechanical resonance, acoustic resonance can result in catastrophic failure of the object at resonance. The classic example of this is breaking a wine glass with sound at the precise resonant frequency of the glass, although this is difficult in practice.
Electrical resonance.
Electrical resonance occurs in an electric circuit at a particular "resonant frequency" when the impedance of the circuit is at a minimum in a series circuit or at maximum in a parallel circuit (or when the transfer function is at a maximum).
Optical resonance.
An optical cavity, also called an "optical resonator", is an arrangement of mirrors that forms a standing wave cavity resonator for light waves. Optical cavities are a major component of lasers, surrounding the gain medium and providing feedback of the laser light. They are also used in optical parametric oscillators and some interferometers. Light confined in the cavity reflects multiple times producing standing waves for certain resonant frequencies. The standing wave patterns produced are called "modes". Longitudinal modes differ only in frequency while transverse modes differ for different frequencies and have different intensity patterns across the cross-section of the beam. Ring resonators and whispering galleries are examples of optical resonators that do not form standing waves.
Different resonator types are distinguished by the focal lengths of the two mirrors and the distance between them; flat mirrors are not often used because of the difficulty of aligning them precisely. The geometry (resonator type) must be chosen so the beam remains stable, i.e., the beam size does not continue to grow with each reflection. Resonator types are also designed to meet other criteria such as minimum beam waist or having no focal point (and therefore intense light at that point) inside the cavity.
Optical cavities are designed to have a very large Q factor; a beam will reflect a very large number of times with little attenuation. Therefore the frequency line width of the beam is very small compared to the frequency of the laser.
Additional optical resonances are guided-mode resonances and surface plasmon resonance, which result in anomalous reflection and high evanescent fields at resonance. In this case, the resonant modes are guided modes of a waveguide or surface plasmon modes of a dielectric-metallic interface. These modes are usually excited by a subwavelength grating.
Orbital resonance.
In celestial mechanics, an orbital resonance occurs when two orbiting bodies exert a regular, periodic gravitational influence on each other, usually due to their orbital periods being related by a ratio of two small integers. Orbital resonances greatly enhance the mutual gravitational influence of the bodies. In most cases, this results in an "unstable" interaction, in which the bodies exchange momentum and shift orbits until the resonance no longer exists. Under some circumstances, a resonant system can be stable and self-correcting, so that the bodies remain in resonance. Examples are the 1:2:4 resonance of Jupiter's moons Ganymede, Europa, and Io, and the 2:3 resonance between Pluto and Neptune. Unstable resonances with Saturn's inner moons give rise to gaps in the rings of Saturn. The special case of 1:1 resonance (between bodies with similar orbital radii) causes large Solar System bodies to clear the neighborhood around their orbits by ejecting nearly everything else around them; this effect is used in the current definition of a planet.
Atomic, particle, and molecular resonance.
Nuclear magnetic resonance (NMR) is the name given to a physical resonance phenomenon involving the observation of specific quantum mechanical magnetic properties of an atomic nucleus in the presence of an applied, external magnetic field. Many scientific techniques exploit NMR phenomena to study molecular physics, crystals, and non-crystalline materials through NMR spectroscopy. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).
All nuclei containing odd numbers of nucleons have an intrinsic magnetic moment and angular momentum. A key feature of NMR is that the resonant frequency of a particular substance is directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonant frequencies of the sample's nuclei depend on where in the field they are located. Therefore, the particle can be located quite precisely by its resonant frequency.
Electron paramagnetic resonance, otherwise known as "Electron Spin Resonance" (ESR) is a spectroscopic technique similar to NMR, but uses unpaired electrons instead. Materials for which this can be applied are much more limited since the material needs to both have an unpaired spin and be paramagnetic.
The Mössbauer effect is the resonant and recoil-free emission and absorption of gamma ray photons by atoms bound in a solid form.
Resonance in particle physics appears in similar circumstances to classical physics at the level of quantum mechanics and quantum field theory. However, they can also be thought of as unstable particles, with the formula above valid if the formula_11 is the decay rate and formula_12 replaced by the particle's mass M. In that case, the formula comes from the particle's propagator, with its mass replaced by the complex number formula_13. The formula is further related to the particle's decay rate by the optical theorem.
Failure of the original Tacoma Narrows Bridge.
The dramatically visible, rhythmic twisting that resulted in the 1940 collapse of "Galloping Gertie", the original Tacoma Narrows Bridge, is misleadingly characterized as an example of resonance phenomenon in certain textbooks. The catastrophic vibrations that destroyed the bridge were not due to simple mechanical resonance, but to a more complicated interaction between the bridge and the winds passing through it—a phenomenon known as aeroelastic flutter, which is a kind of "self-sustaining vibration" as referred to in the nonlinear theory of vibrations. Robert H. Scanlan, father of bridge aerodynamics, has written an article about this misunderstanding.
Resonance causing a vibration on the International Space Station.
The rocket engines for the International Space Station (ISS) are controlled by autopilot. Ordinarily the uploaded parameters for controlling the engine control system for the Zvezda module will cause the rocket engines to boost the International Space Station to a higher orbit. The rocket engines are hinge-mounted, and ordinarily the operation is not noticed by the crew. But on January 14, 2009, the uploaded parameters caused the autopilot to swing the rocket engines in larger and larger oscillations, at a frequency of 0.5 Hz. These oscillations were captured on video, and lasted for 142 seconds.

</doc>
<doc id="41661" url="http://en.wikipedia.org/wiki?curid=41661" title="Response">
Response

Response may refer to:

</doc>
<doc id="41662" url="http://en.wikipedia.org/wiki?curid=41662" title="Response time (technology)">
Response time (technology)

In technology, response time is the time a system or functional unit takes to react to a given input.
Computing.
Response time is the total amount of time it takes to respond to a request for service. That service can be anything from a memory fetch, to a disk IO, to a complex database query, or loading a full web page. Ignoring transmission time for a moment, the response time is the sum of the service time and wait time. The service time is the time it takes to do the work you requested. For a given request the service time varies little as the workload increases – to do X amount of work it always takes X amount of time. The wait time is how long the request had to wait in a queue before being serviced and it varies from zero, when no waiting is required, to a large multiple of the service time, as many requests are already in the queue and have to be serviced first.
With basic queueing theory math you can calculate how the average wait time increases as the device providing the service goes from 0-100% busy. As the device becomes busier, the average wait time increases in a non-linear fashion. The busier the device is, the more dramatic the response time increases will seem as you approach 100% busy; all of that increase is caused by increases in wait time, which is the result of all the requests waiting in queue that have to run first.
Transmission time gets added to response time when your request and the resulting response has to travel over a network and it can be very significant. Transmission time can include propagation delays due to distance (the speed of light is finite), delays due to transmission errors, and data communication bandwidth limits (especially at the last mile) slowing the transmission speed of the request or the reply.
Real-time Systems.
In real-time systems the response time of a task or thread is defined as the time elapsed between the dispatch (time when task is ready to execute) to the time when it finishes its job (one dispatch). Response time is different from WCET which is the maximum time the task would take if it were to execute without interference. It is also different from deadline which is the length of time during which the task's output would be valid in the context of the specific system.
Display technologies.
Response time is the amount of time a pixel in a display takes to change. It is measured in Milliseconds(ms). Lower numbers mean faster transitions and therefore fewer visible image artifacts. Older monitors with long response times would create Display motion blur around moving objects, making them unacceptable for rapidly moving images. Typically response times are "usually" measured from grey-to-grey transitions, but there is no industry standard.

</doc>
<doc id="41663" url="http://en.wikipedia.org/wiki?curid=41663" title="Responsivity">
Responsivity

Responsivity measures the input–output gain of a detector system. In the specific case of a photodetector, responsivity measures the electrical output per optical input. 
The responsivity of a photodetector is usually expressed in units of either amperes or volts per watt of incident radiant power. For a system that responds linearly to its input, there is a unique responsivity. For nonlinear systems, the responsivity is the local slope (derivative). Many common photodetectors respond linearly as a function of the incident power. 
Responsivity is a function of the wavelength of the incident radiation and of the sensor properties, such as the bandgap of the material of which the photodetector is made. One simple expression for the responsivity "R" of a photodetector in which an optical signal is converted into an electrical current (known as a photocurrent) is 
formula_1
where formula_2 is the quantum efficiency (the conversion efficiency of photons to electrons) of the detector for a given wavelength, formula_3 is the electron charge, formula_4 is the frequency of the optical signal, and formula_5 is Planck's constant. This expression is also given in terms of formula_6, the wavelength of the optical signal, and has units of amperes per watt (A/W).
The term responsivity is also used to summarize input–output relationship in non-electrical systems. For example, a neuroscientist may measure how neurons in the visual pathway respond to light. In this case, responsivity summarizes the change in the neural response per unit signal strength. The responsivity in these applications can have a variety of units. The signal strength typically is controlled by varying either intensity (intensity-response function) or contrast (contrast-response function). The neural response measure depends on the part of the nervous system under study. For example, at the level of the retinal cones, the response might be in photocurrent. In the central nervous system the response is usually spikes per second. In functional neuroimaging, the response measure is usually BOLD contrast. The responsivity units reflect the relevant stimulus and physiological units. 
When describing an amplifier, the more common term is gain. 
"Deprecated synonym" sensitivity. A system's sensitivity is the inverse of the stimulus level required to produce a threshold response, with the threshold typically chosen just above the noise level.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41664" url="http://en.wikipedia.org/wiki?curid=41664" title="Restoration">
Restoration

Restoration may refer to:

</doc>
<doc id="41665" url="http://en.wikipedia.org/wiki?curid=41665" title="Return loss">
Return loss

In telecommunications, return loss is the loss of power in the signal returned/reflected by a discontinuity in a transmission line or optical fiber. This discontinuity can be a mismatch with the terminating load or with a device inserted in the line. It is usually expressed as a ratio in decibels (dB);
Return loss is related to both standing wave ratio (SWR) and reflection coefficient (Γ). Increasing return loss corresponds to lower SWR. Return loss is a measure of how well devices or lines are matched. A match is good if the return loss is high. A high return loss is desirable and results in a lower insertion loss.
Return loss is used in modern practice in preference to SWR because it has better resolution for small values of reflected wave.
Sign.
Properly, loss quantities, when expressed in decibels, should be positive numbers. However, return loss has historically been expressed as a negative number, and this convention is still widely found in the literature.
The correct definition of return loss is the difference in dB between the incident power sent towards the Device Under Test (DUT) and the power reflected, resulting in a positive sign:
However taking the ratio of reflected to incident power results in a negative sign for return loss;
Return loss with a positive sign is identical to the magnitude of Γ when expressed in decibels but of opposite sign. That is, return loss with a negative sign is more properly called reflection coefficient. The S-parameter "S"11 from two-port network theory is frequently also called return loss, but is actually equal to Γ.
Caution is required when discussing increasing or decreasing return loss since these terms strictly have the opposite meaning when return loss is defined as a negative quantity.
Electrical.
In metallic conductor systems, reflections of a signal traveling down a conductor can occur at a discontinuity or impedance mismatch. The ratio of the amplitude of the reflected wave "Vr" to the amplitude of the incident wave "Vi" is known as the reflection coefficient formula_4.
When the source and load impedances are known values, the reflection coefficient is given by
where "Z"S is the impedance toward the source and "Z"L is the impedance toward the load.
Return loss is the negative of the magnitude of the reflection coefficient in dB. Since power is proportional to the square of the voltage, return loss is given by,
where the vertical bars indicate magnitude. Thus, a large positive return loss indicates the reflected power is small relative to the incident power, which indicates good impedance match from source to load.
When the actual transmitted (incident) power and the reflected power are known (i.e., through measurements and/or calculations), then the return loss in dB can be calculated as the difference between the incident power "P"i (in dBm) and the reflected power "P"r (in dBm),
Optical.
In optics (particularly in fiberoptics) a loss that takes place at discontinuities of refractive index, especially at an air-glass interface such as a fiber endface. At those interfaces, a fraction of the optical signal is reflected back toward the source. This reflection phenomenon is also called "Fresnel reflection loss," or simply "Fresnel loss."
Fiber optic transmission systems use lasers to transmit signals over optical fiber, and a high optical return loss (ORL) can cause the laser to stop transmitting correctly. The measurement of ORL is becoming more important in the characterization of optical networks as the use of wavelength-division multiplexing increases. These systems use lasers that have a lower tolerance for ORL, and introduce elements into the network that are located in close proximity to the laser.
where formula_10 is the reflected power and formula_11 is the incident, or input, power.
References.
</dl>

</doc>
<doc id="41666" url="http://en.wikipedia.org/wiki?curid=41666" title="RF power margin">
RF power margin

In telecommunication, the term RF power margin has the following meanings: 

</doc>
<doc id="41667" url="http://en.wikipedia.org/wiki?curid=41667" title="Ringaround">
Ringaround

In telecommunication, the term ringaround has the following meanings:

</doc>
<doc id="41669" url="http://en.wikipedia.org/wiki?curid=41669" title="Ringdown">
Ringdown

In telephony, ringdown is a method of signaling an operator in which telephone ringing current is sent over the line to operate a lamp or cause the operation of a self-locking relay known as a "drop".
Ringdown is used in manual operation, as distinguished from automatic signaling by dialing a number. The signal consists of a continuous or pulsed alternating current (AC) signal transmitted over the line. It may be used with or without a telephone switchboard. The term originated in magneto telephone signaling in which cranking the magneto generator, either integrated into the telephone set or housed in a connected ringer box, would not only "ring" its bell but also cause a drop to fall "down" at the telephone exchange switchboard, marked with the number of the line to which the magneto telephone instrument was connected. The last ringdown telephone exchange in the United States was located at Bryant Pond, Maine, had 400+ subscribers, and converted to dial service in October 1983.
Ringdown operator.
In telephone systems where calls from distant automated exchanges arrive for manual subscribers or non-dialable points, there often would be a ringdown operator (reachable from the distant operator console by dialling NPA+181) who would manually ring the desired subscriber on a party line or toll station. On some systems, this function was carried out by the inward operator (NPA+121). In both cases, this is a telephone operator at the destination who provides assistance solely to other operators on inbound toll calls; the ringdown operator nominally cannot be dialled directly by the subscriber.
Non-operator use.
In an application "not" involving a telephone operator, a two-point automatic ringdown circuit, or ringdown, has a telephone at each end. When the telephone at one end goes off-hook, the phone at the other end instantly rings. No dialing is involved and therefor telephone sets without dials are sometimes used.
Many ringdown circuits work in both directions. In some cases a circuit is designed to work in one direction only. That is, going off-hook at one end (end A) rings the other (end B). Going off-hook at end B has no effect at end A.
Ringdown features are often part of a key telephone system. In the wire spring relay key service units of the Bell System 1A2, a model 216 automatic ringdown was used to operate the circuit. In the 400-series units, a number of different KTUs operate (supervise) a ringdown, including the model 415. In other situations, the ringdown is powered and operated by equipment inside the telephone exchange.
In the case of enterprises with a private branch exchange (PBX) switch, the ringdown can be operated by the PBX key. The switch is programmed to ring a specific extension (the called phone) when a defined extension (the calling phone) goes off-hook. The PBX does not offer dial tone to the calling extension: it only detects on-hook or off-hook status.
Voice over IP adapters can be networked and configured to provide automatic ringdown by selecting a dial plan which replaces the empty string with a predefined number or SIP address, dialed immediately. (Some Cisco VoIP phones and analog adapters treat a dial plan of (S0 <:1234567890>) as a hotline configuration which dials 1-234-567890 zero seconds after the telephone is taken off-hook, for instance).
These circuits are used:
In some cases, automatic ringdown circuits have one-to-many configurations. When one phone goes off-hook, a group of phones is made to ring simultaneously.
In cases where one or both ends of the circuit terminate in a key telephone system, a well-designed system will have no hold feature on the ringdown circuit unless supervision provides a Calling Party Control (CPC) signal.
PLAR.
PLAR stands for Private Line Automatic Ringdown. It is a type of analog signalling often used with telephone based intercom systems. When a device is taken off-hook, it applies a ringing voltage to the circuit. Other devices on the same pair will ring. Then, when another device is answered, a call will be maintained over the circuit at normal voltage. The telephone company switch is not involved in the process, making this a private line.

</doc>
<doc id="41670" url="http://en.wikipedia.org/wiki?curid=41670" title="Ringer equivalence number">
Ringer equivalence number

The ringer equivalence number (REN) is a telecommunications measure that represents the electrical loading effect of a telephone ringer on a telephone line. In the United States, the REN was first defined by U.S. Code of Regulations, Title 47, Part 68, based on the load that a standard Bell System model 500 telephone represented, and was later determined in accordance with ANSI/TIA-968-B (August 2009).
Although the REN was developed in the United States, analogous measurement systems exist internationally. In some countries, particularly in Commonwealth nations, the REN is also known as the ringer approximated loading number (RAL).
Definition.
A ringer equivalency number of 1 represents the loading effect of a single traditional telephone ringing circuit, such as that within the Western Electric model 500 telephone. The REN of modern telephone equipment may be significantly lower than 1. For example, externally powered digital-ring telephones may have a REN as low as 0.1, while modern analog-ring telephones, in which the ringer is powered from the telephone line, typically have a REN of approximately 0.8.
In the United States, the FCC Part 68 specification defined 1 REN as equivalent to a 6930 Ω resistor in series with an 8 µF (microfarad) capacitor. The modern ANSI/TIA-968-B specification (August 2009) defines 1 REN as an impedance of at (type A ringer), or from 15 Hz to 68 Hz (type B ringer).
Maximum REN loading.
The total REN load on a subscriber line is the sum of the REN loads of all devices connected to the line; this number expresses the overall loading effect of the subscriber equipment on the central office ringing current source. Subscriber telephone lines are usually limited to support a load of 5 REN or less.
If the total allowable ringer load is exceeded, the phone circuit may fail to ring or otherwise malfunction. For example, Call waiting, caller ID and ADSL services are often affected by high ringer load. 20th century equipment tends to contribute to a larger REN than new equipment.
Some analog telephone adapters for Internet telephony require analog telephones with low REN, for example, the AT&T 210 is a basic phone which does not require an external electrical connection and has a REN of 0.9B.
International usage.
In the United Kingdom and Commonwealth nations it may be known also as the ringer approximated loading (RAL) and a maximum of 4 is allowed on any British Telecom (BT) line.
In Australia it is also called ringer equivalence number and a maximum of 3 is allowed on any Telstra or Optus Line.
In Canada it is called a Load Number (LN); which must not exceed 100. The LN of each device represents the percentage of total load allowed.
In Europe 1 REN used to be equivalent to an 1800 Ω resistor in series with a 1 µF capacitor. The latest ETSI specification (2003–09) calls for 1 REN to be greater than 16 kΩ at 25 Hz and 50 Hz.

</doc>
<doc id="41671" url="http://en.wikipedia.org/wiki?curid=41671" title="Ring latency">
Ring latency

In a ring network, such as Token Ring, ring latency is the time required for a signal to propagate once around the ring. Ring latency may be measured in seconds or in bits at the data transmission rate. Ring latency includes signal propagation delays in (a) the ring medium, (b) the drop cables, and (c) the data stations connected to the ring network. 

</doc>
<doc id="41672" url="http://en.wikipedia.org/wiki?curid=41672" title="Round-trip delay time">
Round-trip delay time

In telecommunications, the round-trip delay time (RTD) or round-trip time (RTT) is the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received. This time delay therefore consists of the propagation times between the two points of a signal.
Computer networks.
In the context of computer networks, the signal is generally a data packet, and the RTT is also known as the ping time. An internet user can determine the RTT by using the ping command.
Space technology.
In space technology, the "round-trip delay time" or "round trip light time" is the time light (and hence any signal) takes to go to a space probe and return.
Protocol design.
Network links with both a high bandwidth and a high RTT can have a very large amount of data (the bandwidth-delay product) "in flight" at any given time. Such "long fat pipes" require a special protocol design. One example is the TCP window scale option.
The RTT was originally estimated in TCP by:
Where α is constant weighting factor(0 ≤ α < 1). Choosing a value α close to 1 makes the weighted average immune to changes that last a short time (e.g., a single segment that encounters long delay). Choosing a value for α close to 0 makes the weighted average respond to changes in delay very quickly.
This was improved by the Jacobson/Karels algorithm, which takes standard deviation into account as well.
Once a new RTT is calculated, it is entered into the equation above to obtain an average RTT for that connection, and the procedure continues for every new calculation.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).
External links.
]'s Institute for Telecommunication Sciences in Boulder, Colorado]

</doc>
<doc id="41673" url="http://en.wikipedia.org/wiki?curid=41673" title="Routing indicator">
Routing indicator

In telecommunication, the term routing indicator (RI) has the following meanings: 
A Routing Indicator is a group of letters assigned to identify a station within a tape relay network to facilitate routing of traffic. It indicates the status of the station and may indicate its geographical area. The following factors are reflected in routing indicator assignment:
Routing indicators consist of not less than four, and not more than seven
letters, including suffixes. The intent of allocated letters and of letter position is as follows:

</doc>
<doc id="41674" url="http://en.wikipedia.org/wiki?curid=41674" title="Rubidium standard">
Rubidium standard

A rubidium standard or rubidium atomic clock is a frequency standard in which a specified hyperfine transition of electrons in rubidium-87 atoms is used to control the output frequency. It is the most inexpensive, compact, and widely used type of atomic clock, used to control the frequency of television stations, cell phone base stations, in test equipment, and global navigation satellite systems like GPS. Commercial rubidium clocks are less accurate than cesium atomic clocks, which serve as primary frequency standards, so the rubidium clock is a secondary frequency standard. However, rubidium fountains are currently being developed that are even more stable than caesium fountain clocks.
All commercial rubidium frequency standards operate by disciplining a crystal oscillator to the rubidium hyperfine transition of . The intensity of light from a rubidium discharge lamp that reaches a photodetector through a resonance cell will drop by about 0.1% when the rubidium vapor in the resonance cell is exposed to microwave power near the transition frequency. The crystal oscillator is stabilized to the rubidium transition by detecting the light dip while sweeping an RF synthesizer (referenced to the crystal) through the transition frequency.

</doc>
<doc id="41675" url="http://en.wikipedia.org/wiki?curid=41675" title="Rural radio service">
Rural radio service

Rural Radiotelephone Service (RRTS) provides basic, analog communications service between locations deemed so remote that traditional wireline service or service by other means is not feasible. RRTS uses channelized radio to provide radiotelephone services such as Basic Exchange Telephone Radio Service between a fixed subscriber location and a remote central office, private line service between a two fixed locations or interconnection between two or more central offices. RRTS does not enable mobile communications.
Licensing.
In the United States, the Federal Communications Commission issues initial Rural Radiotelephone Service licenses on a site-by-site basis. Once a license is issued, the licensee can sell or lease the license to another party.
The FCC service rules for Rural Radiotelephone are located in 47 C.F.R. Part 22 Subpart F.
Technical information.
In the United States, the ULS radio service code and description for Rural Radiotelephone licenses is CR – Rural Radiotelephone. The licensed spectrum is divided in 44 channels of 20 kHz each.

</doc>
<doc id="41676" url="http://en.wikipedia.org/wiki?curid=41676" title="Saturation">
Saturation

Saturation, saturated, unsaturation or unsaturated may refer to:

</doc>
<doc id="41677" url="http://en.wikipedia.org/wiki?curid=41677" title="Scan">
Scan

Scan may refer to:
Acronyms:
Businesses:
Electronics or computer related:
Medical:
Other uses:

</doc>
<doc id="41679" url="http://en.wikipedia.org/wiki?curid=41679" title="Schematic">
Schematic

A schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures. A schematic usually omits all details that are not relevant to the information the schematic is intended to convey, and may add unrealistic elements that aid comprehension. For example, a subway map intended for riders may represent a subway station with a dot; the dot doesn't resemble the actual station at all but gives the viewer information without unnecessary visual clutter. A schematic diagram of a chemical process uses symbols to represent the vessels, piping, valves, pumps, and other equipment of the system, emphasizing their interconnection paths and suppressing physical details. In an electronic circuit diagram, the layout of the symbols may not resemble the layout in the physical circuit. In the schematic diagram, the symbolic elements are arranged to be more easily interpreted by the viewer.
Schematics and other types of diagrams, e.g.,
A semi-schematic diagram combines some of the abstraction of a purely schematic diagram with other elements displayed as realistically as possible, for various reasons. It is a compromise between a purely abstract diagram (e.g. the schematic of the Washington Metro) and an exclusively realistic representation (e.g. the corresponding aerial view of Washington).
Electrical and electronic industry.
In electrical and electronic industry, a schematic diagram is often used to describe the design of equipment. Schematic diagrams are often used for the maintenance and repair of electronic and electromechanical systems. Original schematics were done by hand, using standardized templates or pre-printed adhesive symbols, but today Electrical CAD software is often used.
In electronic design automation, until the 1980s schematics were virtually the only formal representation for circuits. More recently, with the progress of computer technology, other representations were introduced and specialized computer languages were developed, since with the explosive growth of the complexity of electronic circuits, traditional schematics are becoming less practical. For example, hardware description languages are indispensable for modern digital circuit design.
Schematics for electronic circuits are prepared by designers using EDA (Electronic Design Automation) tools called schematic capture tools or schematic entry tools. These tools go beyond simple drawing of devices and connections. Usually they are integrated into the whole IC design flow and linked to other EDA tools for verification and simulation of the circuit under design.
In electric power systems design, a schematic drawing called a one-line diagram is frequently used to represent substations, distribution systems or even whole electrical power grids. These diagrams simplify and compress the details that would be repeated on each phase of a three-phase system, showing only one element instead of three. Electrical diagrams for switchgear often have common device functions designate by standard function numbers.
Schematics in repair manuals.
Schematic diagrams are used extensively in repair manuals to help users understand the interconnections of parts, and to provide graphical instruction to assist in dismantling and rebuilding mechanical assemblies. Many automotive and motorcycle repair manuals devote a significant number of pages to schematic diagrams.

</doc>
<doc id="41680" url="http://en.wikipedia.org/wiki?curid=41680" title="Scrambler">
Scrambler

In telecommunications, a scrambler is a device that transposes or inverts signals or otherwise encodes a message at the transmitter to make the message unintelligible at a receiver not equipped with an appropriately set descrambling device. Whereas encryption usually refers to operations carried out in the digital domain, scrambling usually refers to operations carried out in the analog domain. Scrambling is accomplished by the addition of components to the original signal or the changing of some important component of the original signal in order to make extraction of the original signal difficult. Examples of the latter might include removing or changing vertical or horizontal sync pulses in television signals; televisions will not be able to display a picture from such a signal. Some modern scramblers are actually encryption devices, the name remaining due to the similarities in use, as opposed to internal operation.
In telecommunications and recording, a "scrambler" (also referred to as a "randomizer") is a device that manipulates a data stream before transmitting. The manipulations are reversed by a "descrambler" at the receiving side. Scrambling is widely used in satellite, radio relay communications and PSTN modems. A scrambler can be placed just before a FEC coder, or it can be placed after the FEC, just before the modulation or line code. A scrambler in this context has nothing to do with encrypting, as the intent is not to render the message unintelligible, but to give the transmitted data useful engineering properties.
A scrambler replaces sequences (referred to as "whitening sequences") into other sequences without removing undesirable sequences, and as a result it changes the probability of occurrence of vexatious sequences. Clearly it is not foolproof as there are input sequences that yield all-zeros, all-ones, or other undesirable periodic output sequences. A scrambler is therefore not a good substitute for a line code, which, through a coding step, removes unwanted sequences.
Purposes of scrambling.
A scrambler (or randomizer) can be either:
There are two main reasons why scrambling is used:
Scramblers are essential components of physical layer system standards besides interleaved coding and modulation. They are usually defined based on linear feedback shift registers (LFSRs) due to their good statistical properties and ease of implementation in hardware.
It is common for physical layer standards bodies to refer to lower-layer (physical layer and link layer) encryption as scrambling as well. This may well be because (traditional) mechanisms employed are based on feedback shift registers as well.
Some standards for digital television, such as DVB-CA and MPE, refer to encryption at the link layer as scrambling.
Types of scramblers.
Additive (synchronous) scramblers.
"Additive scramblers" (they are also referred to as "synchronous") transform the input data stream by applying a pseudo-random binary sequence (PRBS) (by modulo-two addition). Sometimes a pre-calculated PRBS stored in the Read-only memory is used, but more often it is generated by a linear feedback shift register (LFSR).
In order to assure a synchronous operation of the transmitting and receiving LFSR (that is, "scrambler" and "descrambler"), a "sync-word" must be used.
A sync-word is a pattern that is placed in the data stream through equal intervals (that is, in each frame). A receiver searches for a few sync-words in adjacent frames and hence determines the place when its LFSR must be reloaded with a pre-defined "initial state".
The "additive descrambler" is just the same device as the additive scrambler.
Additive scrambler/descrambler is defined by the polynomial of its LFSR (for the scrambler on the picture above, it is formula_1) and its "initial state".
Multiplicative (self-synchronizing) scramblers.
"Multiplicative scramblers" (also known as "feed-through") are called so because they perform a "multiplication" of the input signal by the scrambler's transfer function in Z-space. They are discrete linear time-invariant systems.
A multiplicative scrambler is recursive and a multiplicative descrambler is non-recursive. Unlike additive scramblers, multiplicative scramblers do not need the frame synchronization, that is why they are also called "self-synchronizing". Multiplicative scrambler/descrambler is defined similarly by a polynomial (for the scrambler on the picture it is formula_2), which is also a "transfer function" of the descrambler.
Comparison of scramblers.
Scramblers have certain drawbacks: 
Noise.
The first voice scramblers were invented at Bell Labs in the period just before World War II. These sets consisted of electronics that could mix two signals, or alternately "subtract" one signal back out again. The two signals were provided by telephones for one, and a record player for the other. Sets of matching pairs of records were produced containing recordings of noise, which would then be played into the telephone and the mixed signal sent over the wires. The noise would then be subtracted back out at the far end using the matching record, leaving the original voice signal intact. Eavesdroppers would hear only the noisy signal, unable to understand the voice inside.
One of those, used (among other duties) for telephone conversations between Winston Churchill and Franklin D. Roosevelt was intercepted and unscrambled by the Germans. At least one German engineer had worked at Bell Labs before the war and came up with a way to break them. Later versions were sufficiently different that the German team was unable to unscramble them. Early versions were known as "A-3" (from AT&T Corporation). An unrelated device called SIGSALY was used for higher-level voice communications.
The noise was provided on large shellac phonograph records made in pairs, shipped as needed, and destroyed after use. This worked, but was enormously awkward. Just achieving synchronization of the two records proved difficult. Post-war electronics made such systems much easier to work with by creating pseudo-random noise based on a short input tone. In use, the caller would play a tone into the phone, and both scrambler units would then listen to the signal and synchronize to it. This provided limited security, however, as any listener with a basic knowledge of the electronic circuitry could often produce a machine of similar-enough settings to break into the communications.
Cryptographic.
It was the need to synchronize the scramblers that suggested to James H. Ellis the idea for non-secret encryption which ultimately led to the invention of both the RSA encryption algorithm and Diffie–Hellman key exchange well before either was reinvented publicly by Rivest, Shamir, and Adleman, or by Diffie and Hellman.
The latest scramblers are not scramblers in the truest sense of the word, but rather digitizers combined with encryption machines. In these systems the original signal is first converted into digital form, and then the digital data is encrypted and sent. Using modern public-key systems, these "scramblers" are much more secure than their earlier analog counterparts. Only these types of systems are considered secure enough for sensitive data.
Voice inversion scrambling can be as simple as inverting the frequency bands around a static point to various complex methods of changing the inversion point randomly and in real time and using multiple bands.
The "scramblers" used in cable television are designed to prevent casual signal theft - not to provide any real security. Early versions of these devices simply "inverted" one important component of the TV signal, re-inverting it at the client end for display. Later devices were only slightly more complex, filtering out that component entirely and then adding it by examining other portions of the signal. In both cases the circuitry could be easily built by any reasonably knowledgeable hobbyist. "See Television encryption"
Electronic kits for scrambling and descrambling are available from hobbyist suppliers. Scanner enthusiasts often use them to listen in to scrambled communications at car races and some public service transmissions. It is also common in FRS radios. This is an easy way to learn about scrambling.
The term "scrambling" is sometimes incorrectly used when jamming is meant.

</doc>
<doc id="41681" url="http://en.wikipedia.org/wiki?curid=41681" title="Screen">
Screen

Screen or Screens may refer to:

</doc>
<doc id="41682" url="http://en.wikipedia.org/wiki?curid=41682" title="Secondary frequency standard">
Secondary frequency standard

In telecommunications, a secondary frequency standard is a frequency standard that does not have inherent accuracy, and therefore must be calibrated against a primary frequency standard. 
Secondary standards include crystal oscillators and rubidium standards. A crystal oscillator depends for its frequency on its physical dimensions, which vary with fabrication and environmental conditions. A rubidium standard is a secondary standard even though it uses atomic transitions, because it takes the form of a gas cell through which an optical signal is passed. The gas cell has inherent inaccuracies because of gas pressure variations, including those induced by temperature variations. There are also variations in the concentrations of the required buffer gases, which variations cause frequency deviations.

</doc>
<doc id="41684" url="http://en.wikipedia.org/wiki?curid=41684" title="Security">
Security

Security is the degree of resistance to, or protection from, harm. It applies to any vulnerable and valuable asset, such as a person, dwelling, community, nation, or organization.
As noted by the Institute for Security and Open Methodologies (ISECOM) in the OSSTMM 3, security provides "a form of protection where a separation is created between the assets and the threat." These separations are generically called "controls," and sometimes include changes to the asset or the threat.
Perceived security compared to real security.
Perception of security may be poorly mapped to measureable objective security. For example, the fear of earthquakes has been reported to be more common than the fear of slipping on the bathroom floor although the latter kills many more people than the former. Similarly, the perceived effectiveness of security measures is sometimes different from the actual security provided by those measures. The presence of security protections may even be taken for security itself. For example, two computer security programs could be interfering with each other and even cancelling each other's effect, while the owner believes s/he is getting double the protection.
Security theater is a critical term for deployment of measures primarily aimed at raising subjective security without a genuine or commensurate concern for the effects of that measure on objective security. For example, some consider the screening of airline passengers based on static databases to have been Security Theater and Computer Assisted Passenger Prescreening System to have created a "decrease" in objective security.
Perception of security can increase objective security when it affects or deters malicious behavior, as with visual signs of security protections, such as video surveillance, alarm systems in a home, or an anti-theft system in a car such as a vehicle tracking system or warning sign. Since some intruders will decide not to attempt to break into such areas or vehicles, there can actually be less damage to windows in addition to protection of valuable objects inside. Without such advertisement, an intruder might, for example, approach a car, break the window, and then flee in response to an alarm being triggered. Either way, perhaps the car itself and the objects inside aren't stolen, but with "perceived security" even the windows of the car have a lower chance of being damaged.
Categorizing security.
There is an immense literature on the analysis and categorization of security. Part of the reason for this is that, in most security systems, the "weakest link in the chain" is the most important. The situation is asymmetric since the 'defender' must cover all points of attack while the attacker need only identify a single weak point upon which to concentrate.
Security concepts.
Certain concepts recur throughout different fields of security:
Home security.
Home security is something applicable to all of us and involves the hardware in place on a property, and personal security practices. The hardware would be the doors, locks, alarm systems, lighting that is installed on your property. Personal security practices would be ensuring doors are locked, alarms activated, windows closed and many other routine tasks which act to prevent a burglary.
Computer security.
Computer security, also known as cybersecurity or IT security, is security applied to computing devices such as computers and smartphones, as well as computer networks such as private and public networks, including the whole Internet. The field includes all the processes and mechanisms by which digital equipment, information and services are protected from unintended or unauthorized access, change or destruction, and is of growing importance due to the increasing reliance of computer systems in most societies. It includes physical security to prevent theft of equipment and information security to protect the data on that equipment. Those terms generally do not refer to physical security, but a common belief among computer security experts is that a physical security breach is one of the worst kinds of security breaches as it generally allows full access to both data and equipment.
Security management in organizations.
In the corporate world, various aspects of security are historically addressed separately - notably by distinct and often noncommunicating departments for IT security, physical security, and fraud prevention. Today there is a greater recognition of the interconnected nature of security requirements, an approach variously known as holistic security, "all hazards" management, and other terms.
Inciting factors in the convergence of security disciplines include the development of digital video surveillance technologies (see Professional video over IP) and the digitization and networking of physical control systems (see SCADA). Greater interdisciplinary cooperation is further evidenced by the February 2005 creation of the Alliance for Enterprise Security Risk Management, a joint venture including leading associations in security (ASIS), information security (ISSA, the Information Systems Security Association), and IT audit (ISACA, the Information Systems Audit and Control Association).
In 2007 the International Organisation for Standardization (ISO) released ISO 28000 - Security Management Systems for the supply chain. Although the title supply chain is included, this Standard specifies the requirements for a security management system, including those aspects critical to security assurance for any organisation or enterprise wishing to manage the security of the organisation and its activities.
ISO 28000 is the foremost risk based security system and is suitable for managing both public and private regulatory security, customs and industry based security schemes and requirements.

</doc>
<doc id="41685" url="http://en.wikipedia.org/wiki?curid=41685" title="Security kernel">
Security kernel

In telecommunication, the term security kernel has the following meanings: 

</doc>
<doc id="41686" url="http://en.wikipedia.org/wiki?curid=41686" title="Security management">
Security management

Security management is the identification of an organization's assets (including information assets), followed by the development, documentation, and implementation of policies and procedures for protecting these assets.
An organisation uses such security management procedures as information classification, risk assessment, and risk analysis to identify threats, categorise assets, and rate system vulnerabilities so that they can implement effective controls.
Loss prevention.
Loss prevention focuses on what your critical assets are and how you are going to protect them. A key component to loss prevention is assessing the potential threats to the successful achievement of the goal. This must include the potential opportunities that further the object (why take the risk unless there's an upside?) Balance probability and impact determine and implement measures to minimize or eliminate those threats.
Security risk management.
Management of security risks applies the principles of risk management to the management of security threats. It consists of identifying threats (or risk causes), assessing the effectiveness of existing controls to face those threats, determining the risks' consequence(s), prioritizing the risks by rating the likelihood and impact, classifying the type of risk and selecting and appropriate risk option or risk response.
Risk options.
Risk avoidance.
The first choice to be considered. The possibility of eliminating the existence of criminal opportunity or avoiding the creation of such an opportunity is always the best solution, when additional considerations or factors are not created as a result of this action that would create a greater risk. As an example, removing all the cash from a retail outlet would eliminate the opportunity for stealing the cash–but it would also eliminate the ability to conduct business.
Risk reduction.
When avoiding or eliminating the criminal opportunity conflicts with the ability to conduct business, the next step is the reduction of the opportunity and potential loss to the lowest level consistent with the function of the business. In the example above, the application of risk reduction might result in the business keeping only enough cash on hand for one day’s operation.
Risk spreading.
Assets that remain exposed after the application of reduction and avoidance are the subjects of risk spreading. This is the concept that limits loss or potential losses by exposing the perpetrator to the probability of detection and apprehension prior to the consummation of the crime through the application of perimeter lighting, barred windows and intrusion detection systems. The idea here is to reduce the time available to steal assets and escape without apprehension.
Risk transfer.
Transferring risks to other alternatives when those risks have not been reduced to acceptable levels. The two primary methods of accomplishing risk transfer are to insure the assets or raise prices to cover the loss in the event of a criminal act. Generally speaking, when the first three steps have been properly applied, the cost of transferring risks is much lower.
Risk acceptance.
All remaining risks must simply be assumed by the business as a risk of doing business. Included with these accepted losses are deductibles which have been made as part of the insurance coverage.

</doc>
<doc id="41687" url="http://en.wikipedia.org/wiki?curid=41687" title="Self-synchronizing code">
Self-synchronizing code

In coding theory, especially in telecommunications, a self-synchronizing code is a uniquely decodable code in which the symbol stream formed by a portion of one code word, or by the overlapped portion of any two adjacent code words, is not a valid code word. Put another way, a set of strings (called "code words") over an alphabet is called a self-synchronizing code if for each string obtained by concatenating two code words, the substring starting at the second symbol and ending at the second-last symbol does not contain any code word as substring. Every self-synchronizing code is a prefix code, but not all prefix codes are self-synchronizing.
Other terms for self-synchronizing code are synchronized code or, ambiguously, comma-free code. A self-synchronizing code permits the proper framing of transmitted code words provided that no uncorrected errors occur in the symbol stream; external synchronization is not required. Self-synchronizing codes also allow recovery from uncorrected errors in the stream; with most prefix codes, an uncorrected error in a single bit may propagate errors further in the stream and make the subsequent data corrupted.
Importance of self-synchronizing codes is not limited to data transmission. Self-synchronization also facilitates some cases of data recovery, for example of a digitally encoded text.
Synchronizing word.
A code X over an alphabet A has a synchronizing word w in "A"+ if 
A prefix code is synchronized if and only if it has a synchronizing word.

</doc>
<doc id="41688" url="http://en.wikipedia.org/wiki?curid=41688" title="Semiautomatic switching system">
Semiautomatic switching system

In telecommunication, the term semiautomatic switching system has the following meanings: 

</doc>
<doc id="41690" url="http://en.wikipedia.org/wiki?curid=41690" title="Sensitivity">
Sensitivity

Sensitivity may refer to:

</doc>
<doc id="41691" url="http://en.wikipedia.org/wiki?curid=41691" title="Separate-channel signaling">
Separate-channel signaling

Separate-channel signaling is a form of signaling in which the whole or a part of one or more channels in a multichannel system is used to provide for supervisory and control signals for the message traffic channels. 
The same channels, such as frequency bands or time slots, that are used for signaling are not used for message traffic. 

</doc>
<doc id="41694" url="http://en.wikipedia.org/wiki?curid=41694" title="Service termination point">
Service termination point

In telecommunication, service termination point is the last point of service rendered by a commercial carrier under applicable tariffs.
Usually, the service termination point is on the customer premises and corresponds to the demarcation point. The customer is responsible for equipment and operation from the service termination point to user end instruments.

</doc>
<doc id="41695" url="http://en.wikipedia.org/wiki?curid=41695" title="Shadow loss">
Shadow loss

In telecommunication, the term shadow loss has the following meanings: 

</doc>
<doc id="41696" url="http://en.wikipedia.org/wiki?curid=41696" title="Shannon's law">
Shannon's law

Shannon's law may refer to:

</doc>
<doc id="41697" url="http://en.wikipedia.org/wiki?curid=41697" title="Sheath">
Sheath

Sheath pronounced as /ʃiːθ/ and /ʃeθ/ in Old English may refer to:

</doc>
<doc id="41698" url="http://en.wikipedia.org/wiki?curid=41698" title="Shield">
Shield

A shield is a type of personal armor, meant to intercept attacks, either by stopping projectiles such as arrows or redirecting a hit from a sword, mace, battle axe or similar weapon to the side of the shield-bearer.
Shields vary greatly in size, ranging from large panels that protect the user's entire body to small models (such as the buckler) that were intended for hand-to-hand-combat use. Shields also vary a great deal in thickness; whereas some shields were made of relatively deep, absorbent, wooden planking to protect soldiers from the impact of spears and crossbow bolts, others were thinner and lighter and designed mainly for deflecting blade strikes.
In prehistory and during the era of the earliest civilizations, shields were made of wood, animal hide, woven reeds or wicker. In classical antiquity, the Migration Period and the Middle Ages, they were normally constructed of poplar, lime or another split-resistant timber, covered in some instances with a material such as leather or rawhide and often reinforced with a metal boss, rim or banding. They were carried by foot soldiers, knights and cavalry.
Depending on time and place, shields could be round, oval, square, rectangular, triangular, bilobal or scalloped. Sometimes they took on the form of kites or flatirons, or had rounded tops on a rectangular base with perhaps an eyehole. The shield was held by a central grip or by straps which went over or around the user's arm.
Often shields were decorated with a painted pattern or an animal representation and these designs developed into systematized heraldic devices during the High Middle Ages for purposes of battlefield identification. Even after the introduction of gunpowder and firearms to the battlefield, shields continued to be used by certain groups. In the 18th century, for example, Scottish Highland fighters liked to wield small shields (known as a targe), and as late as the 19th century, some non-industrialized peoples (such as Zulu warriors) employed them when waging war.
In the 20th and 21st century, shields have been used by military and police units that specialize in anti-terrorist actions, hostage rescue, riot control and siege-breaking. The modern term usually refers to a device that is held in the hand or attached to the arm, as opposed to an armored suit or a bullet-proof vest. Shields are also sometimes mounted on vehicle-mounted weapons to protect the operator.
Prehistory and antiquity.
The oldest form of shield was a protection device designed to block attacks by hand weapons, such as swords, axes and maces, or ranged weapons like sling-stones and arrows. Shields have varied greatly in construction over time and place. Sometimes shields were made of metal, but wood or animal hide construction was much more common; wicker and even turtle shells have been used. Many surviving examples of metal shields are generally felt to be ceremonial rather than practical, for example the Yetholm-type shields of the Bronze Age or the Iron Age Battersea shield. The shield was used to make the Greek Phalanx formation.
Size and weight varied greatly. Lightly armored warriors relying on speed and surprise would generally carry light shields ("pelte") that were either small or thin. Heavy troops might be equipped with robust shields that could cover most of the body. Many had a strap called a guige that allowed it to be slung over the user's back when not in use or on horseback. During the 14th-13th century BC, the Sards or Shardana, working as mercenaries for the Egyptian pharaoh Ramses II, utilized either large or small round shields against the Hittites. The Mycenaean Greeks used two types of shields: the "figure-of-eight" shield and a rectangular "tower" shield. These shields were made primarily from a wicker frame and then reinforced with leather. Covering the body from head to foot, the figure-of-eight and tower shield offered most of the warriors body a good deal of protection in man to man combat. The Ancient Greek hoplites used a round, bowl-shaped wooden shield that was reinforced with bronze and called an aspis. Another name for this type of shield is a hoplon. The hoplon shield inspired the name for hoplite soldiers. The hoplon was also the longest lasting and most famous and influential of all of the ancient Greek shields. The Spartans used the aspis to create the Greek Phalanx formation. Their shields offered protection not only for themselves but for their comrades to their left and right. Examples of Germanic wooden shields c350 BC - 500 AD survive from weapons sacrifices in Danish bogs.
The heavily armored Roman legionaries carried large shields ("scuta") that could provide far more protection, but made swift movement a little more difficult. The "scutum" originally had an oval shape, but gradually the curved tops and sides were cut to produce the familiar rectangular shape most commonly seen in the early Imperial legions. Famously, the Romans used their shields to create a tortoise-like formation called a "testudo" in which entire groups of soldiers would be enclosed in an armoured box to provide protection against missiles. Many ancient shield designs featured incuts of one sort or another. This was done to accommodate the shaft of a spear, thus facilitating tactics requiring the soldiers to stand close together forming a wall of shields.
Middle Ages.
In the early European Middle Ages, round shields with light, non-splitting wood like linden, fir, alder or poplar, usually reinforced with leathercover on one or both sides and occasionally metal rims, encircling a metal shield boss were typical. These light shields suited a fighting style where each incoming blow is intercepted with the boss in order to deflect it.
The Normans introduced the kite shield around the 10th century, which was rounded at the top and tapered at the bottom. This gave some protection to the user's legs, without adding too much to the total weight of the shield. 
The kite shield predominantly features enarmes, leather straps used to grip the shield tight to the arm. Used by foot and mounted troops alike, it gradually came to replace the round shield as the common choice until the end of the 12th century, when more efficient limb armour allowed the shields to grow shorter, and be entirely replaced by the 14th century.
As body armour improved, knight's shields became smaller, leading to the familiar heater shield style. Both kite and heater style shields were made of several layers of laminated wood, with a gentle curve in cross section. The heater style inspired the shape of the symbolic heraldic shield that is still used today. Eventually, specialised shapes were developed such as the "bouche", which had a lance rest cut into the upper corner of the lance side, to help guide it in combat or tournament. Free standing shields called pavises, which were propped up on stands, were used by medieval crossbowmen who needed protection while reloading.
In time, some armoured foot knights gave up shields entirely in favour of mobility and two-handed weapons. Other knights and common soldiers adopted the buckler (origin of the term "swashbuckler"). The buckler is a small round shield, typically between 8 and 16 inches (20–40 centimeters) in diameter. The buckler was one of very few types of shield that were usually made of metal. Small and light, the buckler was easily carried by being hung from a belt; it gave little protection from missiles and was reserved for hand-to-hand combat where it served both for protection and offence. The buckler's use began in the Middle Ages and continued well into the 16th Century.
In Italy, the targa, parma and rotella were utilized by common people, fencers and even knights. The development of plate armour made shields less and less common as plate armour eliminated the need for a shield. Lightly armoured troops continued to use shields after men-at-arms and knights ceased to use them. Shields continued in use even after gunpowder powered weapons made them essentially obsolete on the battlefield. In the 18th Century, the Scottish clans used a small, round shield called a targe that was partially effective against the firearms of the time, although it was arguably more often used against British infantry bayonets and cavalry swords in close-in fighting.
During the 19th Century, non-industrial cultures with little access to guns were still using war shields. Zulu warriors carried large lightweight shields made from a single ox hide supported by a wooden spine, these were called Ishlangu. This was used in combination with a short spear (assegai) and/or club.
Although the size of shield would vary due to personal preference and role, most were thin compared to common belief (a misconception aided by the depiction of heavy shields in films). When used in fighting, shields were most effective when used to deflect glancing blows. By deflecting a sword blow to the side, rather than blocking it head on, the attacker could be rendered open to a counterattack. This technique allowed the shield to be made lighter and more easily wielded, while reducing the amount of energy and risk of injury posed to the shield-bearer.
Modern era.
Law enforcement shields.
Shields for protection from armed attack are still used by many police forces around the world. These modern shields are usually intended for two broadly distinct purposes. The first type, riot shields, are used for riot control and can be made from metal or polymers such as polycarbonate Lexan or Makrolon or boPET Mylar. These typically offer protection from relatively large and low velocity projectiles, such as rocks and bottles, as well as blows from fists or clubs. Synthetic riot shields are normally transparent, allowing full use of the shield without obstructing vision. Similarly, metal riot shields often have a small window at eye level for this purpose. These riot shields are most commonly used to block and push back crowds when the users stand in a "wall" to block protesters, and to protect against shrapnel, projectiles, molotov cocktails, and during hand-to-hand combat.
The second type of modern police shield is the bullet-resistant tactical shield. These shields are typically manufactured from advanced synthetics such as Kevlar and are designed to be bulletproof, or at least bullet resistant. Two types of shields are available:
Tactical shields often have a firing port so that the officer holding the shield can fire a weapon while being protected by the shield, and they often have a bulletproof glass viewing port. They are typically employed by specialist police, such as SWAT teams in high risk entry and siege scenarios, such as hostage rescue and breaching gang compounds, as well as in antiterrorism operations. Tactical shields often have a large signs stating "POLICE" (or the name of a force, such as "US MARSHALS") to indicate that the user is a law enforcement officer.
Gun shields.
With the widespread use of machine guns in World War I and in subsequent conflicts, battlegrounds were swept with automatic weapons fire. While soldiers who are in foxholes or trenches are protected from this fire, soldiers who are manning mounted machine guns were vulnerable to being hit. Since WW I, there have been a variety of attempts to install armored gun shields on tripod-mounted machine guns or vehicle-mounted weapons to protect machine gunners. Transportation devices with mounted guns that may have gun shields to protect the gunner include jeeps, Humvees, armored cars, and boats.
Non-martial applications.
Many non-martial devices also employ shielding of a kind—not usually a single device worn on an arm but various protective plates or other insulation positioned where needed. Space craft have heat shields to ensure a safe re-entry. Electronics uses shielding to reduce electrical noise and crosstalk between signals. Better-quality patch cables used in audio and electronic music have shielding to reduce interference and noise. People and systems that must work in the presence of ionizing radiation (X-rays) such as dentists, hospital technicians, and patients undergoing X-rays are protected with lead shielding clothing.
Emblems that resemble heraldic shields are also called shields. Movie studio Warner Bros. uses a shield emblazoned with WB as its logo. The Looney Tunes cartoons, released through Warner Bros., open with the WB shield zooming through concentric circles.

</doc>
<doc id="41699" url="http://en.wikipedia.org/wiki?curid=41699" title="Shift register">
Shift register

In digital circuits, a shift register is a cascade of flip flops, sharing the same clock, in which the output of each flip-flop is connected to the "data" input of the next flip-flop in the chain, resulting in a circuit that shifts by one position the "bit array" stored in it, "shifting in" the data present at its input and "shifting out" the last bit in the array, at each transition of the clock input.
More generally, a shift register may be multidimensional, such that its "data in" and stage outputs are themselves bit arrays: this is implemented simply by running several shift registers of the same bit-length in parallel.
Shift registers can have both parallel and serial inputs and outputs. These are often configured as 'serial-in, parallel-out' (SIPO) or as 'parallel-in, serial-out' (PISO). There are also types that have both serial and parallel input and types with serial and parallel output. There are also 'bidirectional' shift registers which allow shifting in both directions: L→R or R→L. The serial input and last output of a shift register can also be connected to create a 'circular shift register'.
Serial-in and Serial-out (SISO).
Destructive readout.
These are the simplest kind of shift registers. The data string is presented at 'Data In', and is shifted right one stage each time 'Data Advance' is brought high. At each advance, the bit on the far left (i.e. 'Data In') is shifted into the first flip-flop's output. The bit on the far right (i.e. 'Data Out') is shifted out and lost. 
The data are stored after each flip-flop on the 'Q' output, so there are four storage 'slots' available in this arrangement, hence it is a 4-bit Register. To give an idea of the shifting pattern, imagine that the register holds 0000 (so all storage slots are empty). As 'Data In' presents 1,0,1,1,0,0,0,0 (in that order, with a pulse at 'Data Advance' each time—this is called clocking or strobing) to the register, this is the result. The left hand column corresponds to the left-most flip-flop's output pin, and so on.
So the serial output of the entire register is 10110000. It can be seen that if data were to be continued to input, it would get exactly what was put in, but offset by four 'Data Advance' cycles. This arrangement is the hardware equivalent of a queue. Also, at any time, the whole register can be set to zero by bringing the reset (R) pins high.
This arrangement performs "destructive readout" - each datum is lost once it has been shifted out of the right-most bit.
Serial-in, parallel-out (SIPO).
This configuration allows conversion from serial to parallel format. Data is input serially, as described in the SISO section above. Once the data has been clocked in, it may be either read off at each output simultaneously, or it can be shifted out
In this configuration, each flip-flop is edge triggered. The initial flip-flop operates at the given clock frequency. Each subsequent flip-flop halves the frequency of its predecessor, which doubles its duty cycle. As a result, it takes twice as long for the rising/falling edge to trigger each subsequent flip-flop; this staggers the serial input in the time domain, leading to parallel output.
In cases where the parallel outputs should not change during the serial loading process, it is desirable to use a latched or buffered output. In a latched shift register (such as the 74595) the serial data is first loaded into an internal buffer register, then upon receipt of a load signal the state of the buffer register is copied into a set of output registers. In general, the practical application of the serial-in/parallel-out shift register is to convert data from serial format on a single wire to parallel format on multiple wires.
Parallel-in, Serial-out (PISO).
This configuration has the data input on lines D1 through D4 in parallel format, being D1 the MSB. To write the data to the register, the Write/Shift control line must be held HIGH. To shift the data, the W/S control line is brought LOW and the registers are clocked. The arrangement now acts as a SISO shift register, with D1 as the Data Input. However, as long as the number of clock cycles is not more than the length of the data-string, the Data Output, Q, will be the parallel data read off in order. The animation below shows the write/shift sequence, including the internal state of the shift register. 
Uses.
One of the most common uses of a shift register is to convert between serial and parallel interfaces. This is useful as many circuits work on groups of bits in parallel, but serial interfaces are simpler to construct. Shift registers can be used as simple delay circuits. Several bidirectional shift registers could also be connected in parallel for a hardware implementation of a stack.
SIPO registers are commonly attached to the output of microprocessors when more General Purpose Input/Output pins are required than are available. This allows several binary devices to be controlled using only two or three pins, but slower than parallel I/O - the devices in question are attached to the parallel outputs of the shift register, then the desired state of all those devices can be sent out of the microprocessor using a single serial connection. Similarly, PISO configurations are commonly used to add more binary inputs to a microprocessor than are available - each binary input (i.e. a button or more complicated circuitry) is attached to a parallel input of the shift register, then the data is sent back via serial to the microprocessor using several fewer lines than originally required.
Shift registers can also be used as pulse extenders. Compared to monostable multivibrators, the timing has no dependency on component values, however it requires external clock and the timing accuracy is limited by a granularity of this clock. Example: Ronja Twister, where five 74164 shift registers create the core of the timing logic this way ().
In early computers, shift registers were used to handle data processing: two numbers to be added were stored in two shift registers and clocked out into an arithmetic and logic unit (ALU) with the result being fed back to the input of one of the shift registers (the accumulator) which was one bit longer since binary addition can only result in an answer that is the same size or one bit longer. 
Many computer languages include instructions to 'shift right' and 'shift left' the data in a register, effectively dividing by two or multiplying by two for each place shifted.
Very large serial-in serial-out shift registers (thousands of bits in size) were used in a similar manner to the earlier delay line memory in some devices built in the early 1970s. Such memories were sometimes called "circulating memory". For example, the Datapoint 3300 terminal stored its display of 25 rows of 72 columns of upper-case characters using fifty-four 200-bit shift registers, arranged in six tracks of nine packs each, providing storage for 1800 six-bit characters. The shift register design meant that scrolling the terminal display could be accomplished by simply pausing the display output to skip one line of characters.
History.
One of the first known examples of a shift register was in the Colossus, a code-breaking machine of the 1940s. It was a five-stage device built of vacuum tubes and thyratrons. A shift register was also used in the IAS machine, built by John von Neumann and others at the Institute for Advanced Study in the late 1940s.

</doc>
<doc id="41700" url="http://en.wikipedia.org/wiki?curid=41700" title="Shot noise">
Shot noise

Shot noise or Poisson noise is a type of electronic noise which can be modeled by a Poisson process.
In electronics shot noise originates from the discrete nature of electric charge. Shot noise also occurs in photon counting in optical devices, where shot noise is associated with the particle nature of light.
Origin.
It is known that in a statistical experiment such as tossing a fair coin and counting the occurrences of heads and tails, the numbers of heads and tails after a great many throws will differ by only a tiny percentage, while after only a few throws outcomes with a significant excess of heads over tails or vice versa are common; if an experiment with a few throws is repeated over and over, the outcomes will fluctuate a lot. (It can be proven that the relative fluctuations reduce as the reciprocal square root of the number of throws, a result valid for all statistical fluctuations, including shot noise.)
Shot noise exists because phenomena such as light and electric current consist of the movement of discrete (also called "quantized") 'packets'. Consider light—a stream of discrete photons—coming out of a laser pointer and hitting a wall to create a visible spot. The fundamental physical processes that govern light emission are such that these photons are emitted from the laser at random times; but the many billions of photons needed to create a spot are so many that the brightness, the number of photons per unit time, varies only infinitesimally with time. However, if the laser brightness is reduced until only a handful of photons hit the wall every second, the relative fluctuations in number of photons, i.e., brightness, will be significant, just as when tossing a coin a few times. These fluctuations are shot noise.
The concept of shot noise was first introduced in 1918 by Walter Schottky who studied fluctuations of current in vacuum tubes.
Shot noise may be dominant when the finite number of particles that carry energy (such as electrons in an electronic circuit or photons in an optical device) is sufficiently small so that uncertainties due to the Poisson distribution, which describes the occurrence of independent random events, are of significance. It is important in electronics, telecommunications, optical detection, and fundamental physics.
The term can also be used to describe any noise source, even if solely mathematical, of similar origin. For instance, particle simulations may produce a certain amount of "noise", where due to the small number of particles simulated, the simulation exhibits undue statistical fluctuations which don't reflect the real-world system. The magnitude of shot noise increases according to the square root of the expected number of events, such as the electrical current or intensity of light. But since the strength of the signal itself increases more rapidly, the "relative" proportion of shot noise decreases and the signal to noise ratio (considering only shot noise) increases anyway. Thus shot noise is most frequently observed with small currents or low light intensities that have been amplified.
For large numbers, the Poisson distribution approaches a normal distribution about its mean, and the elementary events (photons, electrons, etc.) are no longer individually observed, typically making shot noise in actual observations indistinguishable from true Gaussian noise. Since the standard deviation of shot noise is equal to the square root of the average number of events "N", the signal-to-noise ratio (SNR) is given by:
Thus when "N" is very large, the signal-to-noise ratio is very large as well, and any "relative" fluctuations in "N" due to other sources are more likely to dominate over shot noise. However when the other noise source is at a fixed level, such as thermal noise, or grows slower than formula_2, increasing "N" (the DC current or light level, etc.) can lead to dominance of shot noise.
Properties.
Electronic devices.
Shot noise in electronic circuits consists of random fluctuations of the electric current in a DC current which originate due to fact that current actually consists of a flow of discrete charges (electrons). Because the electron has such a tiny charge, however, shot noise is of relative insignificance in many (but not all) cases of electrical conduction. For instance 1 ampere of current consists of about electrons per second; even though this number will randomly vary by several billion in any given second, such a fluctuation is minuscule compared to the current itself. In addition, shot noise is often less significant as compared with two other noise sources in electronic circuits, flicker noise and Johnson–Nyquist noise. However, shot noise is temperature and frequency independent, in contrast to Johnson–Nyquist noise, which is proportional to temperature, and flicker noise, with the spectral density decreasing with the frequency. Therefore at high frequencies and low temperatures shot noise may become the dominant source of noise.
With very small currents and considering shorter time scales (thus wider bandwidths) shot noise can be significant. For instance, a microwave circuit operates on time scales of less than a nanosecond and if we were to have a current of 16 nanoamperes that would amount to only 100 electrons passing every nanosecond. According to Poisson statistics the "actual" number of electrons in any nanosecond would vary by 10 electrons rms, so that one sixth of the time less than 90 electrons would pass a point and one sixth of the time more than 110 electrons would be counted in a nanosecond. Now with this small current viewed on this time scale, the shot noise amounts to 1/10 of the DC current itself.
The result by Schottky, based on the assumption that the statistics of electrons passage is Poissonian, reads for the spectral noise density at the frequency formula_3,
where formula_5 is the electron charge, and formula_6 is the average current created by the electron stream. The noise spectral power is frequency independent, which means the noise is white. This is the classical result in the sense that it does not take into account that electrons obey Fermi–Dirac statistics. This can be combined with the Landauer formula, which relates the average current with the transmission eigenvalues formula_7 of the contact through which the current is measured (formula_8 labels transport channels). In the simplest case these transmission eigenvalues can be taken energy independent, the Landauer formula is 
where formula_10 is the applied voltage. This provides for 
commonly referred to as the Poisson value of shot noise, formula_12. The correct result takes into account the quantum statistics of electrons and reads (at zero temperature)
It was obtained in the 1990s by Khlus, Lesovik (independently the single-channel case), and Büttiker (multi-channel case). This noise is white and is always suppressed with respect to the Poisson value. The degree of suppression, formula_14, is known as the Fano factor. Noises produced by different transport channels are independent. Fully open (formula_15) and fully closed (formula_16) channels produce no noise, since there are no irregularities in the electron stream.
At finite temperature, a closed expression for noise can be written as well. It interpolates between shot noise (zero temperature) and Nyquist-Johnson noise (high temperature).
Effects of interactions.
While this is the result when the electrons contributing to the current occur completely randomly, unaffected by each other, there are important cases in which these natural fluctuations are largely suppressed due to a charge build up. Take the previous example in which an average of 100 electrons go from point A to point B every nanosecond. During the first half of a nanosecond we would expect 50 electrons to arrive at point B on the average, but in a particular half nanosecond there might well be 60 electrons which arrive there. This will create a more negative electric charge at point B than average, and that extra charge will tend to "repel" the further flow of electrons from leaving point A during the remaining half nanosecond. Thus the net current integrated over a nanosecond will tend more to stay near its average value of 100 electrons rather than exhibiting the expected fluctuations (10 electrons rms) we calculated. This is the case in ordinary metallic wires and in metal film resistors, where shot noise is almost completely cancelled due to this anti-correlation between the motion of individual electrons, acting on each other through the coulomb force. 
However this reduction in shot noise does not apply when the current results from random events at a potential barrier which all the electrons must overcome due to a random excitation, such as by thermal activation. This is the situation in p-n junctions, for instance. A semiconductor diode is thus commonly used as a noise source by passing a particular DC current through it.
Shot noise is distinct from voltage and current fluctuations expected in thermal equilibrium; this occurs without any applied DC voltage or current flowing. These fluctuations are known as Johnson–Nyquist noise or thermal noise and increase in proportion to the Kelvin temperature of any resistive component. However both are instances of white noise and thus cannot be distinguished simply by observing them even though their origins are quite dissimilar.
Since shot noise is a Poisson process due to the finite charge of an electron, one can compute the root mean square current fluctuations as being of a magnitude
where "q" is the elementary charge of an electron, Δ"f" is the bandwidth in hertz over which the noise is considered, and "I" is the DC current flowing.
For a current of 100 mA, measuring the current noise over a bandwidth of 1 Hz, we obtain
If this noise current is fed through a resistor a noise voltage of
would be generated. Coupling this noise through a capacitor, one could supply a noise power of
to a matched load.
Optics.
In optics, shot noise describes the fluctuations of the number of photons detected (or simply counted in the abstract) due to their occurrence independent of each other. This is therefore another consequence of discretization, in this case of the energy in the electromagnetic field in terms of photons. In the case of photon "detection", the relevant process is the random conversion of photons into photo-electrons for instance, thus leading to a larger effective shot noise level when using a detector with a quantum efficiency below unity. Only in an exotic squeezed coherent state can the number of photons measured per unit time have fluctuations smaller than the square root of the expected number of photons counted in that period of time. Of course there are other mechanisms of noise in optical signals which often dwarf the contribution of shot noise. When these are absent, however, optical detection is said to be "photon noise limited" as only the shot noise (also known as "quantum noise" or "photon noise" in this context) remains.
Shot noise is easily observable in the case of photomultipliers and avalanche photodiodes used in the Geiger mode, where individual photon detections are observed. However the same noise source is present with higher light intensities measured by any photo detector, and is directly measurable when it dominates the noise of the subsequent electronic amplifier. Just as with other forms of shot noise, the fluctuations in a photo-current due to shot noise scale as the square-root of the average intensity: 
The shot noise of a coherent optical beam (having no other noise sources) is a fundamental physical phenomenon, reflecting quantum fluctuations in the electromagnetic field (due to the so-called zero-point energy). This sets a lower bound on the noise introduced by quantum amplifiers which preserve the phase of an optical signal.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41701" url="http://en.wikipedia.org/wiki?curid=41701" title="Sideband">
Sideband

In radio communications, a sideband is a band of frequencies higher than or lower than the carrier frequency, containing power as a result of the modulation process. The sidebands consist of all the Fourier components of the modulated signal except the carrier. All forms of modulation produce sidebands.
Amplitude modulation of a carrier wave normally results in two mirror-image sidebands. The signal components above the carrier frequency constitute the upper sideband (USB), and those below the carrier frequency constitute the lower sideband (LSB). In conventional AM transmission, the carrier and both sidebands are present, sometimes called double sideband amplitude modulation (DSB-AM).
In some forms of AM, the carrier may be removed, producing double sideband with suppressed carrier (DSB-SC). An example is the stereophonic difference (L-R) information transmitted in stereo FM broadcasting on a 38 kHz subcarrier. The receiver locally regenerates the subcarrier by doubling a special 19 kHz pilot tone, but in other DSB-SC systems, the carrier may be regenerated directly from the sidebands by a Costas loop or squaring loop. This is common in digital transmission systems such as BPSK where the signal is continually present.
If part of one sideband and all of the other remain, it is called vestigial sideband, used mostly with television broadcasting, which would otherwise take up an unacceptable amount of bandwidth. Transmission in which only one sideband is transmitted is called single-sideband transmission or SSB. SSB is the predominant voice mode on shortwave radio other than shortwave broadcasting. Since the sidebands are mirror images, which sideband is used is a matter of convention. In amateur radio, LSB is traditionally used below 10 MHz and USB is used above 10 MHz. 
In SSB, the carrier is suppressed, significantly reducing the electrical power (by up to 12 dB) without affecting the information in the sideband. This makes for more efficient use of transmitter power and RF bandwidth, but a beat frequency oscillator must be used at the receiver to reconstitute the carrier. Another way to look at an SSB receiver is as an RF-to-audio frequency transposer: in USB mode, the dial frequency is subtracted from each radio frequency component to produce a corresponding audio component, while in LSB mode each incoming radio frequency component is subtracted from the dial frequency. 
Sidebands can also interfere with adjacent channels. The part of the sideband that would overlap the neighboring channel must be suppressed by filters, before or after modulation (often both). In Broadcast band frequency modulation (FM), subcarriers above 75 kHz are limited to a small percentage of modulation and are prohibited above 99 kHz altogether to protect the ±75 kHz normal deviation and ±100 kHz channel boundaries. Amateur radio and public service FM transmitters generally utilize ±5 kHz deviation.

</doc>
<doc id="41702" url="http://en.wikipedia.org/wiki?curid=41702" title="Signal compression">
Signal compression

Signal compression may refer to:

</doc>
<doc id="41703" url="http://en.wikipedia.org/wiki?curid=41703" title="Signaling (telecommunications)">
Signaling (telecommunications)

In telecommunication, signaling (signalling in British English) has the following meanings: 
Signaling systems may be classified based on several principal characteristics.
In-band and out-of-band signaling.
In the public switched telephone network (PSTN), in-band signaling is the exchange of call control information within the same channel that the telephone call itself is using. An example is dual-tone multi-frequency signaling (DTMF), which is used on most telephone lines to customer premises.
Out-of-band signaling is telecommunication signaling on a dedicated channel separate from that used for the telephone call. Out-of-band signaling has been used since Signaling System No. 6 (SS6) was introduced in the 1970s, and also in Signalling System No. 7 (SS7) in 1980 which became the standard for signaling among exchanges ever since.
Line versus register signaling.
Line signaling is concerned with conveying information on the state of the line or channel, such as on-hook, off-hook (answer supervision and disconnect supervision, together referred to as "supervision"), ringing current (alerting), and recall. In the middle 20th century, supervision signals on long-distance trunks in North America were usually inband, for example at 2600 Hz, necessitating a notch filter to prevent interference. Late in the century, all supervisory signals were out of band. With the advent of digital trunks, supervision signals are carried by robbed bits or other bits in the E1-carrier dedicated to signaling.
Register signaling is concerned with conveying addressing information, such as the calling and/or called telephone number. In the early days of telephony, with operator handling calls, the addressing formation is by voice as "Operator, connect me to Mr. Smith please". In the first half of the 20th century, addressing formation is by using a rotary dial, which rapidly breaks the line current into pulses, with the number of pulses conveying the address. Finally, starting in the second half of the century, address signaling is by DTMF.
Channel-associated versus common-channel signaling.
Channel-associated signaling (CAS) employs a signaling channel which is dedicated to a specific bearer channel.
Common-channel signaling (CCS) employs a signaling channel which conveys signaling information relating to multiple bearer channels. These bearer channels therefore have their signaling channel in common.
Compelled signaling.
Compelled signaling refers to signaling where receipt of each signal from an originating register needs to be explicitly acknowledged before the next signal is able to be sent.
Most forms of R2 register signaling are compelled (see R2 signaling), while R1 multi-frequency signaling is not.
The term is only relevant in the case of signaling systems that use discrete signals (e.g. a combination of tones to denote one digit), as opposed to signaling systems which are message-oriented (such as SS7 and ISDN Q.931) where each message is able to convey multiple items of formation (e.g. multiple digits of the called telephone number).
Subscriber versus trunk signaling.
Subscriber signaling refers to signaling between the telephone and the telephone exchange. Trunk signaling is signaling between exchanges.
Classification.
Every signaling system can be characterized along each of the above axes of classification. A few examples:
Whereas common-channel signaling systems are out-of-band by definition, and in-band signaling systems are also necessarily channel-associated, the above metering pulse example demonstrates that there exist channel-associated signaling systems which are out-of-band.

</doc>
<doc id="41705" url="http://en.wikipedia.org/wiki?curid=41705" title="Signal-to-crosstalk ratio">
Signal-to-crosstalk ratio

The signal-to-crosstalk ratio at a specified point in a circuit is the ratio of the power of the wanted signal to the power of the unwanted signal from another channel. 
The signals are adjusted in each channel so that they are of equal power at the zero transmission level point in their respective channels. 
The signal-to-crosstalk ratio is usually expressed in dB. 

</doc>
<doc id="41706" url="http://en.wikipedia.org/wiki?curid=41706" title="Signal-to-noise ratio">
Signal-to-noise ratio

Signal-to-noise ratio (abbreviated SNR) is a measure used in science and engineering that compares the level of a desired signal to the level of background noise. It is defined as the ratio of signal power to the noise power, often expressed in decibels. A ratio higher than 1:1 (greater than 0 dB) indicates more signal than noise. While SNR is commonly quoted for electrical signals, it can be applied to any form of signal (such as isotope levels in an ice core or biochemical signaling between cells).
The signal-to-noise ratio, the bandwidth, and the channel capacity of a communication channel are connected by the Shannon–Hartley theorem.
Signal-to-noise ratio is sometimes used informally to refer to the ratio of useful information to false or irrelevant data in a conversation or exchange. For example, in online discussion forums and other online communities, off-topic posts and spam are regarded as "noise" that interferes with the "signal" of appropriate discussion.
Definition.
Signal-to-noise ratio is defined as the ratio of the power of a signal (meaningful information) and the power of background noise (unwanted signal):
where "P" is average power. Both signal and noise power must be measured at the same or equivalent points in a system, and within the same system bandwidth.
If the variance of the signal and noise are known, and the signal is zero-mean:
If the signal and the noise are measured across the same impedance, then the SNR can be obtained by calculating the square of the amplitude ratio:
where "A" is root mean square (RMS) amplitude (for example, RMS voltage). 
Decibels.
Because many signals have a very wide dynamic range, signals are often expressed using the logarithmic decibel scale. Based upon the definition of decibel, signal and noise may be expressed in decibels (dB) as
and
In a similar manner, SNR may be expressed in decibels as
Using the definition of SNR 
Using the quotient rule for logarithms
Substituting the definitions of SNR, signal, and noise in decibels into the above equation results in an important formula for calculating the signal to noise ratio in decibels, when the signal and noise are also in decibels:
In the above formula, P is measured in units of power, such as Watts or miliWatts, and signal-to-noise ratio is a pure number. 
However, when the signal and noise are measured in Volts or Amperes, which are measures of amplitudes, they must be squared to be proportionate to power as shown below:
Dynamic range.
The concepts of signal-to-noise ratio and dynamic range are closely related. Dynamic range measures the ratio between the strongest un-distorted signal on a channel and the minimum discernible signal, which for most purposes is the noise level. SNR measures the ratio between an arbitrary signal level (not necessarily the most powerful signal possible) and noise. Measuring signal-to-noise ratios requires the selection of a representative or "reference" signal. In audio engineering, the reference signal is usually a sine wave at a standardized nominal or alignment level, such as 1 kHz at +4 dBu (1.228 VRMS).
SNR is usually taken to indicate an "average" signal-to-noise ratio, as it is possible that (near) instantaneous signal-to-noise ratios will be considerably different. The concept can be understood as normalizing the noise level to 1 (0 dB) and measuring how far the signal 'stands out'.
Difference from conventional power.
In physics, the average power of an AC signal is defined as the average value of voltage times current; for resistive (non-reactive) circuits, where voltage and current are in phase, this is equivalent to the product of the rms voltage and current:
But in signal processing and communication, one usually assumes that formula_13 so that factor is usually not included while measuring power or energy of a signal. This may cause some confusion among readers, but the resistance factor is not significant for typical operations performed in signal processing, or for computing power ratios. For most cases, the power of a signal would be considered to be simply
where 'A' is the amplitude of the AC signal.
Alternative definition.
An alternative definition of SNR is as the reciprocal of the coefficient of variation, i.e., the ratio of mean to standard deviation of a signal or measurement:
where formula_16 is the signal mean or expected value and formula_17 is the standard deviation of the noise, or an estimate thereof. Notice that such an alternative definition is only useful for variables that are always non-negative (such as photon counts and luminance). Thus it is commonly used in image processing, where the SNR of an image is usually calculated as the ratio of the mean pixel value to the standard deviation of the pixel values over a given neighborhood. Sometimes SNR is defined as the square of the alternative definition above.
The "Rose criterion" (named after Albert Rose) states that an SNR of at least 5 is needed to be able to distinguish image features at 100% certainty. An SNR less than 5 means less than 100% certainty in identifying image details.
Yet another alternative, very specific and distinct definition of SNR is employed to characterize sensitivity of imaging systems; see Signal-to-noise ratio (imaging).
Related measures are the "contrast ratio" and the "contrast-to-noise ratio".
SNR for various modulation systems.
Amplitude modulation.
Channel signal-to-noise ratio is given by
where W is the bandwidth and formula_19 is modulation index
Output signal-to-noise ratio (of AM receiver) is given by
Frequency modulation.
Channel signal-to-noise ratio is given by
Output signal-to-noise ratio is given by
Improving SNR in practice.
All real measurements are disturbed by noise. This includes electronic noise, but can also include external events that affect the measured phenomenon — wind, vibrations, gravitational attraction of the moon, variations of temperature, variations of humidity, etc., depending on what is measured and of the sensitivity of the device. It is often possible to reduce the noise by controlling the environment. Otherwise, when the characteristics of the noise are known and are different from the signals, it is possible to filter it or to process the signal.
For example, it is sometimes possible to use a lock-in amplifier to modulate and confine the signal within a very narrow bandwidth and then filter the detected signal to the narrow band where it resides, thereby eliminating most of the broadband noise. When the signal is constant or periodic and the noise is random, it is possible to enhance the SNR by averaging the measurement. In this case the noise goes down as the square root of the number of averaged samples.
Digital signals.
When a measurement is digitized, the number of bits used to represent the measurement determines the maximum possible signal-to-noise ratio. This is because the minimum possible noise level is the error caused by the quantization of the signal, sometimes called Quantization noise. This noise level is non-linear and signal-dependent; different calculations exist for different signal models. Quantization noise is modeled as an analog error signal summed with the signal before quantization ("additive noise").
This theoretical maximum SNR assumes a perfect input signal. If the input signal is already noisy (as is usually the case), the signal's noise may be larger than the quantization noise. Real analog-to-digital converters also have other sources of noise that further decrease the SNR compared to the theoretical maximum from the idealized quantization noise, including the intentional addition of dither.
Although noise levels in a digital system can be expressed using SNR, it is more common to use Eb/No, the energy per bit per noise power spectral density.
The modulation error ratio (MER) is a measure of the SNR in a digitally modulated signal.
Fixed point.
For "n"-bit integers with equal distance between quantization levels (uniform quantization) the dynamic range (DR) is also determined.
Assuming a uniform distribution of input signal values, the quantization noise is a uniformly distributed random signal with a peak-to-peak amplitude of one quantization level, making the amplitude ratio 2"n"/1. The formula is then:
This relationship is the origin of statements like "16-bit audio has a dynamic range of 96 dB". Each extra quantization bit increases the dynamic range by roughly 6 dB.
Assuming a full-scale sine wave signal (that is, the quantizer is designed such that it has the same minimum and maximum values as the input signal), the quantization noise approximates a sawtooth wave with peak-to-peak amplitude of one quantization level and uniform distribution. In this case, the SNR is approximately
Floating point.
Floating-point numbers provide a way to trade off signal-to-noise ratio for an increase in dynamic range. For n bit floating-point numbers, with n-m bits in the mantissa and m bits in the exponent:
Note that the dynamic range is much larger than fixed-point, but at a cost of a worse signal-to-noise ratio. This makes floating-point preferable in situations where the dynamic range is large or unpredictable. Fixed-point's simpler implementations can be used with no signal quality disadvantage in systems where dynamic range is less than 6.02m. The very large dynamic range of floating-point can be a disadvantage, since it requires more forethought in designing algorithms.
Optical SNR.
Optical signals have a carrier frequency that is much higher than the modulation frequency (about 200 THz and more). This way the noise covers a bandwidth that is much wider than the signal itself. The resulting signal influence relies mainly on the filtering of the noise. To describe the signal quality without taking the receiver into account, the optical SNR (OSNR) is used. The OSNR is the ratio between the signal power and the noise power in a given bandwidth. Most commonly a reference bandwidth of 0.1 nm is used. This bandwidth is independent of the modulation format, the frequency and the receiver. For instance an OSNR of 20dB/0.1 nm could be given, even the signal of 40 GBit DPSK would not fit in this bandwidth. OSNR is measured with an optical spectrum analyzer.
Types and abbreviations.
Signal to noise ratio may be abbreviated as SNR and less commonly as S/N. PSNR stands for Peak signal-to-noise ratio. GSNR stands for Geometric Signal-to-Noise Ratio. SINR is the Signal-to-noise-plus-interference ratio.

</doc>
<doc id="41707" url="http://en.wikipedia.org/wiki?curid=41707" title="Signal transition">
Signal transition

Signal transition: In the modulation of a carrier, a change from one significant condition to another. 
Examples of signal transitions are a change from one electrical current, voltage, or power level to another; a change from one optical power level to another; a phase shift; or a change from one frequency or wavelength to another. 
Signal transitions are used to create signals that represent information, such as "0" and "1" or "mark" and "space".

</doc>
<doc id="41710" url="http://en.wikipedia.org/wiki?curid=41710" title="Simple Network Management Protocol">
Simple Network Management Protocol

Simple Network Management Protocol (SNMP) is an "Internet-standard protocol for managing devices on IP networks". Devices that typically support SNMP include routers, switches, servers, workstations, printers, modem racks and more. SNMP is widely used in network management systems to monitor network-attached devices for conditions that warrant administrative attention. SNMP is a component of the Internet Protocol Suite as defined by the Internet Engineering Task Force (IETF). It consists of a set of standards for network management, including an application layer protocol, a database schema, and a set of data objects.
SNMP exposes management data in the form of variables on the managed systems, which describe the system configuration. These variables can then be queried (and sometimes set) by managing applications.
Overview and basic concepts.
In typical uses of SNMP one or more administrative computers, called "managers", have the task of monitoring or managing a group of hosts or devices on a computer network. Each managed system executes, at all times, a software component called an "agent" which reports information via SNMP to the manager.
SNMP agents expose management data on the managed systems as variables. The protocol also permits active management tasks, such as modifying and applying a new configuration through remote modification of these variables. The variables accessible via SNMP are organized in hierarchies. These hierarchies, and other metadata (such as type and description of the variable), are described by Management Information Bases (MIBs).
An SNMP-managed network consists of three key components:
A "managed device" is a network node that implements an SNMP interface that allows unidirectional (read-only) or bidirectional (read and write) access to node-specific information. Managed devices exchange node-specific information with the NMSs. Sometimes called network elements, the managed devices can be any type of device, including, but not limited to, routers, access servers, switches, bridges, hubs, IP telephones, IP video cameras, computer hosts, and printers.
An "agent" is a network-management software module that resides on a managed device. An agent has local knowledge of management information and translates that information to or from an SNMP-specific form.
A "network management station" (NMS) executes applications that monitor and control managed devices. NMSs provide the bulk of the processing and memory resources required for network management. One or more NMSs may exist on any managed network.
Management information base (MIB).
SNMP itself does not define which information (which variables) a managed system should offer. Rather, SNMP uses an extensible design, where the available information is defined by management information bases (MIBs). MIBs describe the structure of the management data of a device subsystem; they use a hierarchical namespace containing object identifiers (OID). Each OID identifies a variable that can be read or set via SNMP. MIBs use the notation defined by Structure of Management Information Version 2.0 (SMIv2, RFC 2578), a subset of ASN.1.
Protocol details.
SNMP operates in the Application Layer of the Internet Protocol Suite (Layer 7 of the OSI model). The SNMP agent receives requests on UDP port 161. The manager may send requests from any available source port to port 161 in the agent. The agent response will be sent back to the source port on the manager. The manager receives notifications ("Traps" and "InformRequests") on port 162. The agent may generate notifications from any available port. When used with Transport Layer Security or Datagram Transport Layer Security requests are received on port 10161 and traps are sent to port 10162.
SNMPv1 specifies five core protocol data units (PDUs). Two other PDUs, "GetBulkRequest" and "InformRequest" were added in SNMPv2 and the "Report" PDU was added in SNMPv3.
All SNMP PDUs are constructed as follows:
The seven SNMP protocol data units (PDUs) are as follows:
Development and usage.
Version 1.
SNMP version 1 (SNMPv1) is the initial implementation of the SNMP protocol. SNMPv1 operates over protocols such as User Datagram Protocol (UDP), Internet Protocol (IP), OSI Connectionless Network Service (CLNS), AppleTalk Datagram-Delivery Protocol (DDP), and Novell Internet Packet Exchange (IPX). SNMPv1 is widely used and is the de facto network-management protocol in the Internet community.
The first RFCs for SNMP, now known as SNMPv1, appeared in 1988:
These protocols were obsoleted by:
After a short time, RFC 1156 (MIB-1) was replaced by the more often used:
Version 1 has been criticized for its poor security. Authentication of clients is performed only by a "community string", in effect a type of password, which is transmitted in cleartext. The '80s design of SNMP V1 was done by a group of collaborators who viewed the officially sponsored OSI/IETF/NSF (National Science Foundation) effort (HEMS/CMIS/CMIP) as both unimplementable in the computing platforms of the time as well as potentially unworkable. SNMP was approved based on a belief that it was an interim protocol needed for taking steps towards large scale deployment of the Internet and its commercialization. In that time period Internet-standard authentication/security was both a dream and discouraged by focused protocol design groups.
Version 2.
SNMPv2 (RFC 1441–RFC 1452), revises version 1 and includes improvements in the areas of performance, security, confidentiality, and manager-to-manager communications. It introduced "GetBulkRequest", an alternative to iterative GetNextRequests for retrieving large amounts of management data in a single request. However, the new party-based security system in SNMPv2, viewed by many as overly complex, was not widely accepted. This version of SNMP reached the Proposed Standard level of maturity, but was deemed obsoleted by later versions.
"Community-Based Simple Network Management Protocol version 2", or "SNMPv2c", is defined in RFC 1901–RFC 1908. SNMPv2c comprises SNMPv2 "without" the controversial new SNMP v2 security model, using instead the simple community-based security scheme of SNMPv1. This version is one of relatively few standards to meet the IETF's Draft Standard maturity level, and was widely considered the "de facto" SNMPv2 standard. It too was later obsoleted, by SNMPv3.
"User-Based Simple Network Management Protocol version 2", or "SNMPv2u", is defined in RFC 1909–RFC 1910. This is a compromise that attempts to offer greater security than SNMPv1, but without incurring the high complexity of SNMPv2. A variant of this was commercialized as "SNMP v2*", and the mechanism was eventually adopted as one of two security frameworks in SNMP v3.
SNMPv1 & SNMPv2c interoperability.
As presently specified, SNMPv2c is incompatible with SNMPv1 in two key areas: message formats and protocol operations. SNMPv2c messages use different header and protocol data unit (PDU) formats from SNMPv1 messages. SNMPv2c also uses two protocol operations that are not specified in SNMPv1. Furthermore, RFC 2576 defines two possible SNMPv1/v2c coexistence strategies: proxy agents and bilingual network-management systems.
Proxy agents.
An SNMPv2 agent can act as a proxy agent on behalf of SNMPv1 managed devices, as follows:
The proxy agent maps SNMPv1 trap messages to SNMPv2 trap messages and then forwards them to the NMS.
Bilingual network-management system.
Bilingual SNMPv2 network-management systems support both SNMPv1 and SNMPv2. To support this dual-management environment, a management application in the bilingual NMS must contact an agent. The NMS then examines information stored in a local database to determine whether the agent supports SNMPv1 or SNMPv2. Based on the information in the database, the NMS communicates with the agent using the appropriate version of SNMP.
Version 3.
Although SNMPv3 makes no changes to the protocol aside from the addition of cryptographic security, it looks much different due to new textual conventions, concepts, and terminology.
SNMPv3 primarily added security and remote configuration enhancements to SNMP.
Due to lack of security with the use of SNMP, network administrators were using other means, such as telnet for configuration, accounting, and fault management.
SNMPv3 addresses issues related to the large-scale deployment of SNMP, accounting, and fault management. Currently, SNMP is predominantly used for monitoring and performance management.
SNMPv3 defines a secure version of SNMP and also facilitates remote configuration of the SNMP entities.
SNMPv3 provides a secure environment for the management of systems covering the following:
SNMPv3 focuses on two main aspects, namely security and administration. The security aspect is addressed by offering both strong authentication and data encryption for privacy. The administration aspect is focused on two parts, namely notification originators and proxy forwarders.
SNMPv3 defines a number of security-related capabilities. The initial specifications defined the USM and VACM, which were later followed by a transport security model that provided support for SNMPv3 over SSH and SNMPv3 over TLS and DTLS.
Security has been the biggest weakness of SNMP since the beginning. Authentication in SNMP Versions 1 and 2 amounts to nothing more than a password (community string) sent in clear text between a manager and agent.
Each SNMPv3 message contains security parameters which are encoded as an octet string. The meaning of these security parameters depends on the security model being used.
SNMPv3 provides important security features:
s of 2004[ [update]] the IETF recognizes "Simple Network Management Protocol version 3" as defined by RFC 3411–RFC 3418 (also known as STD0062) as the current standard version of SNMP. The IETF has designated SNMPv3 a full Internet standard, the highest maturity level for an RFC. It considers earlier versions to be obsolete (designating them variously "Historic" or "Obsolete").
In practice, SNMP implementations often support multiple versions: typically SNMPv1, SNMPv2c, and SNMPv3.
Implementation issues.
SNMP implementations vary across platform vendors. In some cases, SNMP is an added feature, and is not taken seriously enough to be an element of the core design. Some major equipment vendors tend to over-extend their proprietary command line interface (CLI) centric configuration and control systems.
SNMP's seemingly simple tree structure and linear indexing may not always be understood well enough within the internal data structures that are elements of a platform's basic design. Consequently, processing SNMP queries on certain data sets may result in higher CPU utilization than necessary. One example of this would be large routing tables, such as BGP or IGP.
Some SNMP values (especially tabular values) require specific knowledge of table indexing schemes, and these index values are not necessarily consistent across platforms. This can cause correlation issues when fetching information from multiple devices that may not employ the same table indexing scheme (for example fetching disk utilization metrics, where a specific disk identifier is different across platforms.)
Resource indexing.
Modular devices may dynamically increase or decrease their SNMP indices (a.k.a. instances) whenever slotted hardware is added or removed. Although this is most common with hardware, virtual interfaces have the same effect. Index values are typically assigned at boot time and remain fixed until the next reboot. Hardware or virtual entities added while the device is 'live' may have their indices assigned at the end of the existing range and possibly reassigned at the next reboot. Network inventory and monitoring tools need to have the device update capability by properly reacting to the cold start trap from the device reboot in order to avoid corruption and mismatch of polled data.
Index assignments for an SNMP device instance may change from poll to poll mostly as a result of changes initiated by the system administrator. If information is needed for a particular interface, it is imperative to determine the SNMP index before retrieving the data needed. Generally, a description table like ifDescr will map a user friendly name like Serial 0/1 (Blade 0, port 1) to an SNMP index. However, this is not necessarily the case for a specific SNMP value, and can be arbitrary for an SNMP implementation.
Security implications.
A person who is unfamiliar with the SNMP design rationale and/or cryptography, may ask why a challenge-response handshake was not used to improve security. The reasons are:
Autodiscovery.
SNMP by itself is simply a protocol for collecting and organizing information about managed devices (network and device monitoring), and modifying that information on these devices, causing change in their behavior (network management). Most toolsets implementing SNMP offer some form of discovery mechanism, a standardized collection of data common to most platforms and devices, to get a new user or implementor started. One of these features is often a form of automatic discovery, where new devices discovered in the network are polled automatically. For SNMPv1 and SNMPv2c, this presents a security risk, in that your SNMP read communities will be broadcast in cleartext to the target device. SNMPv3 mitigates this risk, however it does not protect against traffic analysis and potential network topology discovery by the adversary. While security requirements and risk profiles vary from organization to organization, care should be taken when using a feature like this, with special regard to common environments such as mixed-tenant datacenters, server hosting and colocation facilities, and similar environments.

</doc>
<doc id="41712" url="http://en.wikipedia.org/wiki?curid=41712" title="Simplex signaling">
Simplex signaling

Simplex signaling (SX) is signaling in which two conductors are used for a single channel, and a center-tapped coil, or its equivalent, is used to split the signaling current equally between the two conductors. The return path for the current is through ground. 
It is distinct from a phantom circuit in which the return current path for power or signaling is provided through different signal conductors.
SX signaling may be one-way, for intra-central-office use, or the simplex legs may be connected to form full duplex signaling circuits that function like composite (CX) signaling circuits with E&M lead control.
Simplex is also used to describe a powering method where one or more signal conductors carries direct current to power a remote device, which sends its output signal back on the same conductor. Phantom powering as used in audio is a form of simplex powering, as the return current flows through the ground or shield conductor.

</doc>
<doc id="41714" url="http://en.wikipedia.org/wiki?curid=41714" title="SINAD">
SINAD

Signal-to-noise and distortion ratio (SINAD) is a measure of the quality of a signal from a communications device, often defined as:
where formula_2 is the average power of the signal, noise and distortion components. SINAD is usually expressed in dB and is quoted alongside the receiver RF sensitivity, to give a quantitative evaluation of the receiver sensitivity. Note that with this definition, unlike SNR, a SINAD reading can never be less than 1 (i.e. it is always positive when quoted in dB).
When calculating the distortion, it is common to exclude the DC components.
Due to widespread use, SINAD has collected several different definitions. SINAD is commonly defined as:
Regardless of the exact definition, it is always true that a lower SINAD value means worse performance of the system. As the received RF signal becomes weaker it becomes progressively lost in the noise and distortion generated by receiver, demodulation and audio output drive circuits. By convention, the minimum acceptable SINAD level that will not swamp intelligible speech is 12dB for a narrow band FM voice radio system.
Commercial Radio Specifications.
A typical example, quoted from a commercial hand held VHF or UHF radio, might be:
which is stating that the receiver will produce intelligible speech with a signal at its input as low as 0.25μV. Radio receiver designers will test the product in a laboratory following a procedure. A typical example procedure is as follows:
According to the radio designer, intelligible speech can be detected 12dB above the receiver's noise floor (noise and distortion). Regardless on how accurate this output power is regarding intelligible speech, having a standard output SINAD allows for easy comparison between radio receiver input sensitivities. This value is typical for VHF commercial radio while 0.35μV is probably more typical for UHF. In the real world lower SINAD values (more noise) can still result in intelligible speech but it is tiresome work to listen to a voice in that much noise.
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41715" url="http://en.wikipedia.org/wiki?curid=41715" title="Single-frequency signaling">
Single-frequency signaling

Single-frequency signaling (SF) is line signaling (in telephony) in which dial pulses or supervisory signals are conveyed by a single voice-frequency tone in each direction. SF and similar systems were used in 20th-century carrier systems.
An SF signaling unit converts DC signaling (usually, at least in long distance circuits, E&M signaling) to a format (characterized by the presence or absence of a single voice-frequency tone), which is suitable for transmission over an AC path, "e.g.", a carrier system. The SF tone is present in the on-hook or idle state and absent during the seized state. In the seized state, dial pulses are conveyed by bursts of SF tone, corresponding to the interruptions in dc continuity created by a rotary dial or other DC dialing mechanism. 
The SF tone may occupy a small portion of the user data channel spectrum, "e.g.," 1600 Hz or 2600 Hz (SF "in-band signaling)". There may be a notch filter at the precise SF frequency, either filtering the circuit at all times or only when the circuit is off-hook, to prevent the user from inadvertently disconnecting a call if the users voice has a sufficiently strong spectral content at the SF frequency, a falsing condition known as "talk-off". Notoriously, this property was exploited by blue boxers and other toll fraudsters. The SF tone may also be just outside the user voice band, "e.g.," 3600 Hz. 
The Defense Data Network (DDN) transmitted DC line signaling pulses or supervisory signals, or both, over carrier channels or cable pairs on a four wire circuit basis using a 2600 Hz signal tone. The conversion into tones, or vice versa, is done by SF signal units.
SF was developed in the early 20th century and standardized in middle century. It declined in the 1970s due to the adoption of T-carrier, and was largely abandoned late in the century in favor of common-channel signaling.

</doc>
<doc id="41716" url="http://en.wikipedia.org/wiki?curid=41716" title="Single-mode optical fiber">
Single-mode optical fiber

In fiber-optic communication, a single-mode optical fiber (SMF) is an optical fiber designed to carry light only directly down the fibre - the transverse mode. Modes are the possible solutions of the Helmholtz equation for waves, which is obtained by combining Maxwell's equations and the boundary conditions. These modes define the way the wave travels through space, i.e. how the wave is distributed in space. Waves can have the same mode but have different frequencies. This is the case in single-mode fibers, where we can have waves with different frequencies, but of the same mode, which means that they are distributed in space in the same way, and that gives us a single ray of light. Although the ray travels parallel to the length of the fiber, it is often called transverse mode since its electromagnetic vibrations occur perpendicular (transverse) to the length of the fiber. The 2009 Nobel Prize in Physics was awarded to Charles K. Kao for his theoretical work on the single-mode optical fiber.
History.
Professor Huang Hongjia of the Chinese Academy of Sciences, developed coupling wave theory in the field of microwave theory. He led a research team that successfully developed Single-mode optical fiber in 1980.
Characteristics.
Like multi-mode optical fibers, single mode fibers do exhibit modal dispersion resulting from multiple spatial modes but with narrower modal dispersion. Single mode fibers are therefore better at retaining the fidelity of each light pulse over longer distances than multi-mode fibers. For these reasons, single-mode fibers can have a higher bandwidth than multi-mode fibers. Equipment for single mode fiber is more expensive than equipment for multi-mode optical fiber, but the single mode fiber itself is usually cheaper in bulk. 
A typical single mode optical fiber has a core diameter between 8 and 10.5 µm and a cladding diameter of 125 µm. There are a number of special types of single-mode optical fiber which have been chemically or physically altered to give special properties, such as dispersion-shifted fiber and nonzero dispersion-shifted fiber. Data rates are limited by polarization mode dispersion and chromatic dispersion. , data rates of up to 10 gigabits per second were possible at distances of over 80 km with commercially available transceivers (Xenpak). By using optical amplifiers and dispersion-compensating devices, state-of-the-art DWDM optical systems can span thousands of kilometers at 10 Gbit/s, and several hundred kilometers at 40 Gbit/s.
The lowest-order bounds mode is ascertained for the wavelength of interest by solving Maxwell's equations for the boundary conditions imposed by the fiber, which are determined by the core diameter and the refractive indices of the core and cladding. The solution of Maxwell's equations for the lowest order bound mode will permit a pair of orthogonally polarized fields in the fiber, and this is the usual case in a communication fiber. 
In step-index guides, single-mode operation occurs when the normalized frequency, "V", is less than or equal to 2.405. For power-law profiles, single-mode operation occurs for a normalized frequency, "V", less than approximately 
where "g" is the profile parameter. 
In practice, the orthogonal polarizations may not be associated with degenerate modes.
OS1 and OS2 are standard single-mode optical fiber used with wavelengths 1310 nm and 1550 nm (size 9/125 µm) with a maximum attenuation of 1 dB/km (OS1) and .4 dB/km (OS2). OS1 is defined in ISO/IEC 11801, and OS2 is defined in ISO/IEC 24702.
Connectors.
Optical fiber connectors are used to join optical fibers where a connect/disconnect capability is required. The basic connector unit is a connector assembly. A connector assembly consists of an adapter and two connector plugs. 
Due to the sophisticated polishing and tuning procedures that may be incorporated into optical connector manufacturing, connectors are generally assembled onto optical fiber in a supplier’s manufacturing facility. However, the assembly and
polishing operations involved can be performed in the field, for example to make cross-connect jumpers to size.
Optical fiber connectors are used in telephone company central offices, at installations on customer premises, and in outside plant applications. Their uses include:
Outside plant applications may involve locating connectors underground in subsurface enclosures that may be subject to flooding, on outdoor walls, or on utility poles. The closures that enclose them may be hermetic, or may be “free-breathing.” Hermetic closures will prevent subjection of the connectors within to temperature swings unless they are breached. Free-breathing enclosures will subject them to temperature and humidity swings, and possibly to condensation and biological action from airborne bacteria, insects, etc. Connectors in the underground plant may be subjected to groundwater immersion if the closures containing them are breached or improperly assembled.
The latest industry requirements for optical fiber connectors are in Telcordia "Generic Requirements for Singlemode Optical Connectors and Jumper Assemblies".
A "multi-fiber" optical connector is designed to simultaneously join multiple optical fibers together, with each optical fiber being joined to only one other optical fiber. 
The last part of the definition is included so as not to confuse multi-fiber connectors with a branching component, such as a coupler. The latter joins one optical fiber to two or more other optical fibers.
Multi-fiber optical connectors are designed to be used wherever quick and/or repetitive connects and disconnects of a group of fibers are needed. Applications include telecommunications companies’ Central Offices (COs), installations on customer premises, and Outside Plant (OSP) applications.
The multi-fiber optical connector can be used in the creation of a low-cost switch for use in fiber optical testing. Another application is in cables delivered to a user with pre-terminated multi-fiber jumpers. This would reduce the need for field splicing, which could greatly reduce the amount of hours necessary for placing an optical fiber cable in a telecommunications network. This, in turn, would result in savings for the installer of such cable.
Industry requirements for multi-fiber optical connectors are covered in "Generic Requirements for Multi-Fiber Optical Connectors".
Fiber Optic Switches.
An optical switch is a component with two or more ports that selectively transmits,
redirects, or blocks an optical signal in a transmission medium. According to Telcordia , an optical switch must be actuated to select or change between states. The actuating signal (also referred to as the control signal) is usually electrical, but in principle, could be optical or mechanical. (The control signal format may be Boolean and may be a separate signal; or, in the case of optical actuation, the control signal may be encoded in the input data signal. Switch performance is generally intended to be independent of wavelength within the component passband.) See Optical switch for more detailed information.

</doc>
<doc id="41717" url="http://en.wikipedia.org/wiki?curid=41717" title="S interface">
S interface

The S interface or S reference point, also known as S0, is a User–network interface reference point for basic rate access in an Integrated Services Digital Network (ISDN) environment, that
The S interface is electrically equivalent to the T interface, and the two are jointly referred to as the S/T interface.

</doc>
<doc id="41718" url="http://en.wikipedia.org/wiki?curid=41718" title="Skew">
Skew

Skew may refer to:
In mathematics:
In statistics:
In chemistry
In optics:
In engineering:
In finance:
In telecommunications:
In computers:
In aviation:
In fantasy baseball:
In carpentry or construction:
See also.
SKU refers to a Stock-keeping unit, a unique identifier for each distinct product and service that can be purchased in business.

</doc>
<doc id="41719" url="http://en.wikipedia.org/wiki?curid=41719" title="Skip zone">
Skip zone

A skip zone, also called a silent zone or zone of silence, is a region where a radio transmission can not be received located between regions both closer and farther from the transmitter where reception is possible.
When using medium to high frequency radio telecommunication, there are radio waves which travel both parallel to the ground, and towards the ionosphere, referred to as a ground wave and sky wave, respectively. A skip zone is an annular region between the farthest points at which the ground wave can be received and the nearest point at which the refracted sky waves can be received. Within this region, no signal can be received because, due to the actual conditions of the local ionosphere, the relevant sky waves are not reflected but penetrate the ionosphere. 
The skip zone is a natural phenomenon that cannot be influenced by technical means. Its width depends on the height and shape of the ionosphere and, particularly, on the local ionospheric maximum electron density characterized by critical frequency foF2. It varies mainly with this parameter, being larger for low foF2. With a fixed working frequency it is large by night and may even disappear by day. Transmitting at night is most effective for long distance communication but the skip zone becomes significantly larger. 
Very high frequency waves and higher normally travel through the ionosphere wherefore communication via skywave is exceptional. A highly ionized Es-Layer that occasionally may appear in Summer may produce such an example.
Another method of decreasing the skip zone is by decreasing the frequency of the radio waves. Decreasing the frequency is akin to increasing the ionospheric width. A point is eventually reached when decreasing the frequency results in a zero distance skip zone. In other words, a frequency exists for which vertically incident radio waves will always be refracted back to the Earth. This frequency is equivalent to the ionospheric plasma frequency and is also known as the ionospheric critical frequency, or foF2.
Skip zone is the subject of a film 'SKIPZONE' made in 1992 by UK artist, Peter Lee-Jones. It refers to areas in Scottish Highlands where it is difficult to obtain radio and TV reception.

</doc>
<doc id="41720" url="http://en.wikipedia.org/wiki?curid=41720" title="Slant range">
Slant range

In radio electronics, especially radar terminology, slant range is the line-of-sight distance between two points which are not at the same level relative to a specific datum.
An example of slant range is the distance to an aircraft flying at high altitude with respect to that of the radar antenna. The slant range (1) is the hypotenuse of the triangle represented by the altitude of the aircraft and the distance between the radar antenna and the aircraft's ground track (point (3) on the earth directly below the aircraft). In the absence of altitude information, for example from a height finder, the aircraft location would be plotted farther (2) from the antenna than its actual ground track.

</doc>
<doc id="41721" url="http://en.wikipedia.org/wiki?curid=41721" title="Slave clock">
Slave clock

In telecommunication, a slave clock is a clock that is coordinated with a master clock. Slave clock coordination is usually achieved by phase-locking the slave clock signal to a signal received from the master clock. To adjust for the transit time of the signal from the master clock to the slave clock, the phase of the slave clock may be adjusted with respect to the signal from the master clock so that both clocks are in phase. Thus, the time markers of both clocks, at the output of the clocks, occur simultaneously.
In other areas, the term refers to satellite electrical clocks that operate remotely from an electrical pulse issued by a master clock. In the 19th and early 20th centuries, slave clocks were widely used throughout public buildings and business offices, and their remote operation was regulated by electrical signals sent by a centralized master clock.
These older styles of slave clocks either keep time by themselves, and are corrected by the master clock, or require impulses from the master clock to advance. Many slave clocks of these types remain in operation, most commonly in schools.
IBM/Simplex Synchronous Correction cycle.
Synchronous Systems.
This type of system was developed by the International Time Recording Company (ITR) in the early 1900s. This company was later renamed International Business Machines (IBM), which sold their time division to the Simplex Time Recorder Company in 1958. The methods & clock mechanisms have remained basically unchanged, other than appearance and style over the years. Today, this same protocol is used by several manufacturers, including Lathem, Dukane, Cincinnati and Standard Electric Time. Digital master clocks have replaced older mechanical types.
All consist of a master clock and a number of secondary clocks in remote locations. These secondary clocks appear to run like an ordinary plug-in electric clock and run on their own synchronous electric motor and will maintain correct time unless there is an interruption in electric service, depending on the master clock only for the periodic synchronizing impulse. These clocks differ from the ‘impulse’ type, which are electrically advanced each minute by the master clock. Impulse type clocks are usually called ‘slave’ clocks since they cannot function at all without the master clock.
There are two basic variations in synchronous systems, ’wired’ and ‘electronic’.
In a wired system, the secondary clocks are ‘hard-wired’ to the master clock with their own network of wiring. This dedicated wiring consists of 3 wires and requires that all secondary clocks be connected to this network in order to receive corrections from the master clock. These systems are usually 110 volts but some are 24 volts. The master clock energizes the third wire to apply voltage to the correction solenoids
(called ‘clutch magnets’ by the manufacturer) in the secondary clocks.
In an electronic system, the correction impulse is applied to the normal building wiring, in the form of a high-frequency audio signal (typically 3510 Hz) applied to the normal 60 Hz frequency of the electrical system. Special receivers in the secondary clocks receive this impulse & apply voltage to the correction solenoid. In this type system, secondary clocks need no special wiring & may be plugged into any 110 volt outlet in the building.
The cycle can also be initiated manually for testing & adjustment as follows:<br>
With clock running, rotate 90 degrees to the right, so that the ‘9’ is at the top. Hold in this position approximately 8 seconds (you may hear a clicking sound during this time), then return the clock to its normal upright position. Within 60 seconds, the hands will slowly advance to the next hour as described below in the third paragraph under ‘Hourly Correction’.
The actual correction that follows is the same in both types of systems. The only difference (described above) is the method by which the correction solenoid is energized.
Hourly Correction.
The correction impulse is issued by the system master clock at 57 minutes 54 seconds after each hour and lasts for 8 seconds (until 58 minutes 02 seconds). The correction solenoids in the secondary clocks are energized by this pulse. This causes the attached armature and cam to engage a gear and rotate upwards to lift a latch allowing the correction cycle to begin. This takes 6 seconds and causes the secondary clocks to begin their correction cycle at precisely 58 minutes 0 seconds.
The correction cycle lasts precisely 60 seconds, ending at 59 minutes 0 seconds at which time, all clocks in the system will display the same time as the master clock.
The cycle causes an internal clock-wise sweep, beginning at the 59 minute position and takes 60 seconds, rotating around the dial, ending at the 59 minute position. During this sweep, if the minute hand is not in proper position, when the sweep rotation reaches it, the minute hand is slowly rotated at 1 rpm until it reaches the 59 minute mark. At this point it is now in sync with the master clock.
During the cycle, the red second hand continues its normal 1 rpm motion but is held in place when it reaches the 60 second mark. It is released when the cycle is complete (minute hand reaches 59 minutes) and now minutes and seconds are both in sync with the master clock.
12 Hour Correction.
This impulse is issued by the system master clock at 5:57:54 AM and PM and is a longer version of the hourly correction impulse. Instead of lasting 8 seconds, this impulse is of 14 seconds duration, lasting until 58 minutes, 08 seconds. The cams in the secondary clocks are rotated further and lock the correction cycle latch open, causing the cycle to continue until the clocks reach 5:59:00. Depending on how many hours the secondary clocks were slow, they will now be several minutes slower than the master. This error is then corrected by the next normal hourly correction at 6:57:54.
Pictures.
Mechanical slave clocks from the 1950s and 1960s era.

</doc>
<doc id="41722" url="http://en.wikipedia.org/wiki?curid=41722" title="Slave station">
Slave station

Slave station may refer to:

</doc>
<doc id="41724" url="http://en.wikipedia.org/wiki?curid=41724" title="Slip">
Slip

Slip, SLIP, or slippery may refer to:

</doc>
<doc id="41725" url="http://en.wikipedia.org/wiki?curid=41725" title="Spatial application">
Spatial application

A spatial application is a technological application (such as video) requiring high spatial resolution, possibly at the expense of reduced temporal positioning accuracy, "i.e.," increased jerkiness. 
Examples of spatial applications include the requirement to display small characters and to resolve fine detail in still video, or in motion video that contains very limited motion.

</doc>
<doc id="41727" url="http://en.wikipedia.org/wiki?curid=41727" title="Specific detectivity">
Specific detectivity

Specific detectivity, or D*, for a photodetector is a figure of merit used to characterize performance, equal to the reciprocal of noise-equivalent power (NEP), normalized per square root of the sensor's area and frequency bandwidth (reciprocal of twice the integration time).
Specific detectivity is given by formula_1, where formula_2 is the area of the photosensitive region of the detector and formula_3 is the frequency bandwidth. It is commonly expressed in "Jones" units (formula_4)in honor of Robert Clark Jones who originally defined it.
Given that noise-equivalent power can be expressed as a function of the responsivity formula_5 (in units of formula_6 or formula_7) and the noise spectral density formula_8 (in units of formula_9 or formula_10) as formula_11, it's common to see the specific detectivity expressed as formula_12.
It is often useful to express the specific detectivity in terms of relative noise levels present in the device. A common expression is given below.
formula_13
With "q" as the electronic charge, formula_14 is the wavelength of interest, "h" is Planck's constant, "c" is the speed of light, "k" is Boltzmann's constant, "T" is the temperature of the detector, formula_15 is the zero-bias dynamic resistance area product (often measured experimentally, but also expressible in noise level assumptions), formula_16 is the quantum efficiency of the device, and formula_17 is the total flux of the source (often a blackbody) in photons/sec/cm².
Detectivity measurement.
Detectivity can be measured from a suitable optical setup using known parameters.
You will need a known light source with known irradiance at a given standoff distance. The incoming light source will be chopped at a certain frequency, and then each wavelet will be integrated over a given time constant over a given number of frames.
In detail, we compute the bandwidth formula_18directly from the integration time constant formula_19.
formula_20
Next, an rms signal and noise needs to be measured from a set of formula_21 frames. This is done either directly by the instrument, or done as post-processing.
formula_22
formula_23
Now, the computation of the radiance formula_24 in W/sr/cm² must be computed where cm² is the emitting area. Next, emitting area must be converted into a projected area and the solid angle; this product is often called the etendue. This step can be obviated by the use of a calibrated source, where the exact number of photons/s/cm² is known at the detector. If this is unknown, it can be estimated using the black-body radiation equation, detector active area formula_25 and the etendue. This ultimately converts the outgoing radiance of the black body in W/sr/cm² of emitting area into one of W observed on the detector.
The broad-band responsivity, is then just the signal weighted by this wattage.
formula_26
Where,
From this metric noise-equivalent power can be computed by taking the noise level over the responsivity.
formula_32
Similarly, noise-equivalent irradiance can be computed using the responsivity in units of photons/s/W instead of in units of the signal.
Now, the detectivity is simply the noise-equivalent power normalized to the bandwidth and detector area.
formula_33
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41728" url="http://en.wikipedia.org/wiki?curid=41728" title="Speckle pattern">
Speckle pattern

A speckle pattern is an intensity pattern produced by the mutual interference of a set of wavefronts. This phenomenon has been investigated by scientists since the time of Newton, but speckles have come into prominence since the invention of the laser and have now found a variety of applications.
Speckle patterns typically occur in diffuse reflections of monochromatic light such as laser light. Such reflections may occur on materials such as paper, white paint, rough surfaces, or in media with a large number of scattering particles in space, such as airborne dust or in cloudy liquids.
Explanation.
The speckle effect is a result of the interference of many waves of the same frequency, having different phases and amplitudes, which add together to give a resultant wave whose amplitude, and therefore intensity, varies randomly. If each wave is modelled by a vector, then it can be seen that if a number of vectors with random angles are added together, the length of the resulting vector can be anything from zero to the sum of the individual vector lengths—a 2-dimensional random walk, sometimes known as a drunkard's walk. In the limit of many interfering waves the distribution of intensities (which go as the square of the vector's length) becomes exponential formula_1, where formula_2 is the mean intensity.
When a surface is illuminated by a light wave, according to diffraction theory, each point on an illuminated surface acts as a source of secondary spherical waves. The light at any point in the scattered light field is made up of waves which have been scattered from each point on the illuminated surface. If the surface is rough enough to create path-length differences exceeding one wavelength, giving rise to phase changes greater than 2π, the amplitude, and hence the intensity, of the resultant light varies randomly.
If light of low coherence (i.e., made up of many wavelengths) is used, a speckle pattern will not normally be observed, because the speckle patterns produced by individual wavelengths have different dimensions and will normally average one another out. However, speckle patterns can be observed in polychromatic light in some conditions.
Subjective speckles.
When an image is formed of a rough surface which is illuminated by a coherent light (e.g. a laser beam), a speckle pattern is observed in the image plane; this is called a “subjective speckle pattern” – see image above. It is called "subjective" because the detailed structure of the speckle pattern depends on the viewing system parameters; for instance, if the size of the lens aperture changes, the size of the speckles change. If the position of the imaging system is altered, the pattern will gradually change and will eventually be unrelated to the original speckle pattern.
This can be explained as follows. Each point in the image can be considered to be illuminated by a finite area in the object. The size of this area is determined by the diffraction-limited resolution of the lens which is given by the Airy disk whose diameter is 2.4λu/D, where λ is the wavelength of the light, u is the distance between the object and the lens, and D is the diameter of the lens aperture. (This is a simplified model of diffraction-limited imaging).
The light at neighbouring points in the image has been scattered from areas which have many points in common and the intensity of two such points will not differ much. However, two points in the image which are illuminated by areas in the object which are separated by the diameter of the Airy disk, have light intensities which are unrelated. This corresponds to a distance in the image of 2.4λv/D where v is the distance between the lens and the image. Thus, the ‘size’ of the speckles in the image is of this order.
The change in speckle size with lens aperture can be observed by looking at a laser spot on a wall directly, and then through a very small hole. The speckles will be seen to increase significantly in size.
Objective speckles.
 When laser light which has been scattered off a rough surface falls on another surface, it forms an “objective speckle pattern”. If a photographic plate or another 2-D optical sensor is located within the scattered light field without a lens, a speckle pattern is obtained whose characteristics depend on the geometry of the system and the wavelength of the laser. The speckle pattern in the figure was obtained by pointing a laser beam at the surface of a mobile phone so that the scattered light fell onto an adjacent wall. A photograph was then taken of the speckle pattern formed on the wall (strictly speaking, this also has a second subjective speckle pattern but its dimensions are much smaller than the objective pattern so it is not seen in the image)
The light at a given point in the speckle pattern is made up of contributions from the whole of the scattering surface. The relative phases of these waves vary across the surface, so that the sum of the individual waves varies randomly. The pattern is the same regardless of how it is imaged, just as if it were a painted pattern.
The "size" of the speckles is a function of the wavelength of the light, the size of the laser beam which illuminates the first surface, and the distance between this surface and the surface where the speckle pattern is formed. This is the case because when the angle of scattering changes such that the relative path difference between light scattered from the centre of the illuminated area compared with light scattered from the edge of the illuminated changes by λ, the intensity becomes uncorrelated. Dainty derives an expression for the mean speckle size as λz/L where L is the width of the illuminated area and z is the distance between the object and the location of the speckle pattern.
Near-field speckles.
Objective speckles are usually obtained in the far field (also called Fraunhofer region, that
is the zone where Fraunhofer diffraction happens). This means that they are generated "far" from 
the object that emits or scatters light. Speckles can be observed also close to the
scattering object, in the near field (also called Fresnel region, that is, the region where Fresnel diffraction happens). This kind of speckles are called Near Field Speckles. 
See near and far field for a more rigorous definition of "near" and "far".
The statistical properties of a far-field speckle pattern (i.e., the speckle form and dimension)
depend on the form and dimension of the region hit by laser light.
By contrast, a very interesting feature of near field speckles is that
their statistical properties are closely related to the form and structure of the scattering object:
objects that scatter at high angles generate small near field speckles, and "vice versa".
Under Rayleigh-Gans condition, in particular, speckle dimension mirrors the average dimension
of the scattering objects, while, in general, the statistical properties of near field
speckles generated by a sample
depend on the light scattering distribution.
Actually, the condition under which the near field speckles appear has been
described as more strict than the usual Fresnel condition.
Applications.
When lasers were first invented, the speckle effect was considered to be a severe drawback in using lasers to illuminate objects, particularly in holographic imaging because of the grainy image produced. It was later realized that speckle patterns could carry information about the object's surface deformations, and this effect is exploited in holographic interferometry and electronic speckle pattern interferometry. The speckle effect is also used in speckle imaging and in eye testing using speckle.
Speckle is the chief limitation of coherent lidar and coherent imaging in optical heterodyne detection.
In the case of near field speckles, the statistical properties depend on the light scattering
distribution of a given sample. This allows the use of near field speckle analysis to detect the scattering distribution; this is the so-called near-field scattering
technique.
When the speckle pattern changes in time, due to changes in the illuminated surface, the phenomenon is known as dynamic speckle, and it can be used to measure activity, by means of, for example,an optical flow sensor (optical computer mouse). In biological materials, the phenomenon is known as biospeckle.
Reduction.
Speckle is considered to be a problem in laser based display systems like the Laser TV. Speckle is usually quantified by the speckle contrast. Speckle contrast reduction is essentially the creation of many independent speckle patterns, so that they average out on the retina/detector. This can be achieved by,
Rotating diffusers—which destroys the spatial coherence of the laser light—can also be used to reduce the speckle. Moving/vibrating screens may also be solutions. The Mitsubishi Laser TV appears to use such a screen which requires special care according to their product manual. A more detailed discussion on laser speckle reduction can be found in 
Synthetic array heterodyne detection was developed to reduce speckle noise in coherent optical imaging and coherent DIAL lidar.
In scientific applications, a spatial filter can be used to reduce speckle.

</doc>
<doc id="41729" url="http://en.wikipedia.org/wiki?curid=41729" title="Spectral width">
Spectral width

In telecommunications, spectral width is the wavelength interval over which the magnitude of all spectral components is equal to or greater than a specified fraction of the magnitude of the component having the maximum value.
In optical communications applications, the usual method of specifying spectral width is the full width at half maximum. This is the same convention used in bandwidth, defined as the frequency range where power drops by less than half (at most −3 dB).
The FWHM method may be difficult to apply when the spectrum has a complex shape. Another method of specifying spectral width is a special case of root-mean-square deviation where the independent variable is wavelength, λ, and "f" (λ) is a suitable radiometric quantity.
The "relative spectral width", Δλ/λ, is frequently used where Δλ is obtained according to note 1, and λ is the center wavelength.

</doc>
<doc id="41730" url="http://en.wikipedia.org/wiki?curid=41730" title="Speed of service">
Speed of service

In telecommunication, speed of service is the time for a message to be received, for example:

</doc>
<doc id="41732" url="http://en.wikipedia.org/wiki?curid=41732" title="Spill-forward feature">
Spill-forward feature

In telecommunication, a spill-forward feature is a service feature, in the operation of an intermediate office, that, acting on incoming trunk service treatment indications, assumes routing control of the call from the originating office. This increases the chances of completion by offering the call to more trunk groups than are available in the originating office.

</doc>
<doc id="41734" url="http://en.wikipedia.org/wiki?curid=41734" title="Spread spectrum">
Spread spectrum

In telecommunication and radio communication, spread-spectrum techniques are methods by which a signal (e.g. an electrical, electromagnetic, or acoustic signal) generated with a particular bandwidth is deliberately spread in the frequency domain, resulting in a signal with a wider bandwidth. These techniques are used for a variety of reasons, including the establishment of secure communications, increasing resistance to natural interference, noise and jamming, to prevent detection, and to limit power flux density (e.g. in satellite downlinks).
Spread-spectrum telecommunications.
This is a technique in which a telecommunication signal is transmitted on a bandwidth considerably larger than the frequency content of the original information. Frequency hopping is a basic modulation technique used in spread spectrum signal transmission.
Spread-spectrum telecommunications is a signal structuring technique that employs direct sequence, frequency hopping, or a hybrid of these, which can be used for multiple access and/or multiple functions. This technique decreases the potential interference to other receivers while achieving privacy. Spread spectrum generally makes use of a sequential noise-like signal structure to spread the normally narrowband information signal over a relatively wideband (radio) band of frequencies. The receiver correlates the received signals to retrieve the original information signal. Originally there were two motivations: either to resist enemy efforts to jam the communications (anti-jam, or AJ), or to hide the fact that communication was even taking place, sometimes called low probability of intercept (LPI).
Frequency-hopping spread spectrum (FHSS), direct-sequence spread spectrum (DSSS), time-hopping spread spectrum (THSS), chirp spread spectrum (CSS), and combinations of these techniques are forms of spread spectrum. Each of these techniques employs pseudorandom number sequences — created using pseudorandom number generators — to determine "and" control the spreading pattern of the signal across the allocated bandwidth. Ultra-wideband (UWB) is another modulation technique that accomplishes the same purpose, based on transmitting short duration pulses. Wireless standard IEEE 802.11 uses either FHSS or DSSS in its radio interface.
Invention of frequency hopping.
On March 17, 1903, Nikola Tesla was granted a patent for a system of frequency hopping between two or more channels to prevent communications being blocked. In 1908 Jonathan Zenneck wrote Wireless Telegraphy, which expanded on this concept. Starting in 1915, Zenneck's system was used by Germany to secure battle field communications.
Avant garde composer George Antheil and Golden Age actress Hedy Lamarr were granted on August 11, 1942 for their "Secret Communication System" for use in radio guided torpedoes. Their approach was unique in that frequency coordination was done with paper player piano rolls - a novel approach which was never put in practice.
Spread-spectrum clock signal generation.
Spread-spectrum clock generation (SSCG) is used in some synchronous digital systems, especially those containing microprocessors, to reduce the spectral density of the electromagnetic interference (EMI) that these systems generate. A synchronous digital system is one that is driven by a clock signal and, because of its periodic nature, has an unavoidably narrow frequency spectrum. In fact, a perfect clock signal would have all its energy concentrated at a single frequency (the desired clock frequency) and its harmonics. Practical synchronous digital systems radiate electromagnetic energy on a number of narrow bands spread on the clock frequency and its harmonics, resulting in a frequency spectrum that, at certain frequencies, can exceed the regulatory limits for electromagnetic interference (e.g. those of the FCC in the United States, JEITA in Japan and the IEC in Europe).
Spread-spectrum clocking avoids this problem by using one of the methods previously described to reduce the peak radiated energy and, therefore, its electromagnetic emissions and so comply with electromagnetic compatibility (EMC) regulations.
It has become a popular technique to gain regulatory approval because it requires only simple equipment modification. It is even more popular in portable electronics devices because of faster clock speeds and increasing integration of high-resolution LCD displays into ever smaller devices. Since these devices are designed to be lightweight and inexpensive, traditional passive, electronic measures to reduce EMI, such as capacitors or metal shielding, are not viable. Active EMI reduction techniques such as spread-spectrum clocking are needed in these cases.
However, spread-spectrum clocking, like other kinds of dynamic frequency change, can also create challenges for designers. Principal among these is clock/data misalignment, or clock skew.
Note that this method does not reduce total radiated energy, and therefore systems are not necessarily less likely to cause interference. Spreading energy over a larger bandwidth effectively reduces electrical and magnetic readings within narrow bandwidths. Typical measuring receivers used by EMC testing laboratories divide the electromagnetic spectrum into frequency bands approximately 120 kHz wide. If the system under test were to radiate all its energy in a narrow bandwidth, it would register a large peak. Distributing this same energy into a larger bandwidth prevents systems from putting enough energy into any one narrowband to exceed the statutory limits. The usefulness of this method as a means to reduce real-life interference problems is often debated, since it is perceived that spread-spectrum clocking hides rather than resolves higher radiated energy issues by simple exploitation of loopholes in EMC legislation or certification procedures. This situation results in electronic equipment sensitive to narrow bandwidth(s) experiencing much less interference, while those with broadband sensitivity, or even operated at other frequencies (such as a radio receiver tuned to a different station), will experience more interference.
FCC certification testing is often completed with the spread-spectrum function enabled in order to reduce the measured emissions to within acceptable legal limits. However, the spread-spectrum functionality may be disabled by the user in some cases. As an example, in the area of personal computers, some BIOS writers include the ability to disable spread-spectrum clock generation as a user setting, thereby defeating the object of the EMI regulations. This might be considered a loophole, but is generally overlooked as long as spread-spectrum is enabled by default.
An ability to disable spread-spectrum clocking in computer systems is considered useful for overclocking, as spread spectrum can lower maximum clock speed achievable due to clock skew.

</doc>
<doc id="41735" url="http://en.wikipedia.org/wiki?curid=41735" title="Squelch">
Squelch

In telecommunications, squelch is a circuit function that acts to suppress the audio (or video) output of a receiver in the absence of a sufficiently strong desired input signal. Squelch is widely used in two-way radios to suppress the annoying sound of channel noise when the radio is not receiving a transmission.
Carrier squelch.
A carrier squelch or noise squelch is the most simple variant of all. It operates strictly on the signal strength, such as when a television mutes the audio or blanks the video on "empty" channels, or when a walkie talkie mutes the audio when no signal is present. In some designs, the squelch threshold is preset. For example, television squelch settings are usually preset. Receivers in base stations at remote mountain top sites are usually not adjustable remotely from the control point.
In devices such as two-way radios (also known as radiotelephones), the squelch can be adjusted with a knob, others have push buttons or a sequence of button presses. This setting adjusts the threshold at which signals will open (un-mute) the audio channel. Backing off the control will turn on the audio, and the operator will hear white noise (also called "static" or squelch noise) if there is no signal present. The usual operation is to adjust the control until the channel just shuts off - then only a small threshold signal is needed to turn on the speaker. However, if a weak signal is annoying, the operator can adjust the squelch to open only when stronger signals are received.
A typical FM two-way radio carrier squelch circuit is noise operated. It takes out the voice components of the receive audio by passing the detected audio through a high-pass filter. A typical filter might pass frequencies over 4,000 Hz (4 kHz). The squelch control adjusts the gain of an amplifier which varies the level of noise coming out of the filter. The audio output of the filter and amplifier is rectified and produces a DC voltage when noise is present. The presence of continuous noise on an idle channel creates a DC voltage which turns the receiver audio off. When a signal with little or no noise is received, the noise-derived voltage goes away and the receiver audio is unmuted. Some applications have the receiver tied to other equipment that uses the audio muting control voltage as a "signal present" indication.
Tone squelch and selective calling.
Tone squelch, or other forms of selective calling, is sometimes used to solve interference problems. Where more than one user is on the same channel ("co-channel" users), selective calling addresses a subset of all receivers. Instead of turning on the receive audio for any signal, the audio turns on only in the presence of the correct selective calling code. This is akin to the use of a lock on a door. A carrier squelch is unlocked and will let any signal in. Selective calling locks out all signals except ones with the correct code.
In non-critical uses, selective calling can also be used to hide the presence of interfering signals such as receiver-produced intermodulation. Receivers with poor specifications—such as scanners or low-cost mobile radios—cannot reject the strong signals present in urban environments. The interference will still be present. It will still degrade system performance but by using selective calling the user will not have to hear the noises produced by receiving the interference.
Four different techniques are commonly used. Selective calling can be regarded as a form of in-band signalling.
CTCSS.
CTCSS (Continuous Tone-Coded Squelch System) continuously superimposes any one of about 50 low-pitch audio tones on the transmitted signal, ranging from 67 to 254 Hz. The original tone set was 10, then 32 tones, and has been expanded even further over the years. CTCSS is often called "PL tone" (for "Private Line", a trademark of Motorola), or simply "tone squelch". General Electric's implementation of CTCSS is called "Channel Guard" (or "CG"). RCA Corporation used the name "Quiet Channel", or "QC". There are many other company-specific names used by radio vendors to describe compatible options. Any CTCSS system that has compatible tones is interchangeable. Old and new radios with CTCSS and radios across manufacturers are compatible.
SelCall.
Selcall (Selective Calling) transmits a burst of up to five inband audio tones at the beginning of each transmission. This feature (sometimes called "tone burst") is common in European systems. Early systems used one tone (commonly called "Tone Burst"). Several tones were used, the most common being 1,750 Hz, which is still used in European amateur radio repeater systems. The addressing scheme provided by one tone was not enough, so a two tone system was devised - one tone followed by a second tone (sometimes called a "1+1" system). Later on, Motorola marketed a system called "Quik-Call" that used two simultaneous tones followed by two more simultaneous tones (sometimes called a "2+2" system) that was heavily used by fire department dispatch systems in the USA. Later selective call systems used paging system technology that made use of five sequential tones. In the same way that a single CTCSS tone would be used on an entire group of radios, a single five-tone sequence is used in a group of radios.
DCS.
DCS (Digital-Coded Squelch), generically known as "CDCSS" (Continuous Digital-Coded Squelch System), was designed as the digital replacement for CTCSS. In the same way that a single CTCSS tone would be used on an entire group of radios, the same DCS code is used in a group of radios. DCS is also referred to as "Digital Private Line" (or "DPL"), another trademark of Motorola, and likewise, General Electric's implementation of DCS is referred to a "Digital Channel Guard" (or "DCG"). DCS is also called "DTCS" (Digital Tone Code Squelch) by Icom, other names by other manufacturers. Radios with DCS options are generally compatible provided the radio's encoder-decoder will use the same code as radios in the existing system.
DCS adds a 134.4 bps (sub-audible) bitstream to the transmitted audio. The code word is a 23-bit Golay (23,12) code which has the ability to detect and correct errors of 3 or fewer bits. The word consists of 12 data bits followed by 11 check bits. The last 3 data bits are a fixed '001', this leaves 9 code bits (512 possibilities) which are conventionally represented as a 3-digit octal number. Note that the first bit transmitted is the LSB, so the code is "backwards" from the transmitted bit order. Only 83 of the 512 possible codes are available, to prevent falsing due to alignment collisions.
XTCSS.
XTCSS is the newest signalling technique and it provides 99 codes with the added advantage of "silent operation". XTCSS fitted radios are purposed to enjoy more privacy and flexibility of operation. XTCSS is implemented as a combination of CTCSS and in-band signalling.
Uses.
Squelch was invented first and is still in wide use in two-way/three-way radio, especially in the amateur radio world. Squelch of any kind is used to indicate loss of signal, which is used to keep commercial and amateur radio repeaters from transmitting continually. Since a carrier squelch receiver cannot tell a valid carrier from a spurious signal (noise, etc.), CTCSS is often used as well, as it avoids false keyups. Use of CTCSS is especially helpful on bands prone to skip and during band openings.
It is a bad idea to use any coded squelch system to hide interference issues in systems with life-safety or public-safety uses such as police, fire, search and rescue or ambulance company dispatching. Adding tone or digital squelch to a radio system does not solve interference issues, it just covers them up. The presence of interfering signals should be corrected rather than masked. Interfering signals masked by tone squelch will produce apparently random missed messages. The intermittent nature of interfering signals will make the problem difficult to reproduce and troubleshoot. Users will not understand why they cannot hear a call, and will lose confidence in their radio system.
Professional wireless microphones use squelch to avoid reproducing noise when the receiver does not receive enough signal from the microphone. Most professional models have adjustable squelch, usually set with a screwdriver adjustment on the receiver.

</doc>
<doc id="41736" url="http://en.wikipedia.org/wiki?curid=41736" title="Standard telegraph level">
Standard telegraph level

In telecommunication, standard telegraph level ("STL") is the power per individual telegraph channel required to yield the standard composite data level. 
For example, for a composite data level of -13 dBm at 0-dBm transmission level point (0TLP), the "STL" would be approximately -25 dBm for a 16-channel VFCT terminal computed from "STL" = - (13+10log10" n" ), where "n" is the number of telegraph channels and the "STL" is in dBm. 

</doc>
<doc id="41737" url="http://en.wikipedia.org/wiki?curid=41737" title="Standard test signal">
Standard test signal

In telecommunication, a standard test signal is a single-frequency signal with standardized level used for testing the peak power transmission capability and for measuring the total harmonic distortion of circuits or parts of an electric circuit. 
Standardized test signal levels and frequencies are listed in MIL-STD-188-100 and in the Code of Federal Regulations Title 47, part 68.

</doc>
<doc id="41738" url="http://en.wikipedia.org/wiki?curid=41738" title="Standard test tone">
Standard test tone

In telecommunication, a standard test tone is a pure tone with a standardized level generally used for level alignment of single links and of links in tandem. 
For standardized test signal levels and frequencies, see MIL-STD-188-100 for United States Department of Defense (DOD) use, and the Code of Federal Regulations Title 47, part 68 for other Government agencies.

</doc>
<doc id="41739" url="http://en.wikipedia.org/wiki?curid=41739" title="Standard time and frequency signal service">
Standard time and frequency signal service

The Standard Time and Frequency Signal (STFS) is a time signal service available in the United States which provides standard time and frequency signals, broadcast on very precise carrier frequencies by the U.S. Naval Observatory and the National Institute of Standards and Technology (NIST), formerly the National Bureau of Standards (NBS).
A similar service is operated in the United Kingdom by the National Physical Laboratory, broadcasting from Anthorn radio station in north-west England, and by the BBC using the 198kHz carrier of the Radio 4 national radio station with a frequency accuracy of 1 part in 1011.
The National Physical Laboratory, India is the premier research laboratory in India in the field of physical sciences. NPLI continues to disseminate standard time and frequency signals (STFS) via geostationary satellite INSAT with an accuracy of 10 ms.

</doc>
<doc id="41740" url="http://en.wikipedia.org/wiki?curid=41740" title="Standby">
Standby

Standby may refer to:

</doc>
<doc id="41741" url="http://en.wikipedia.org/wiki?curid=41741" title="Standing wave">
Standing wave

In physics, a standing wave – also known as a stationary wave – is a wave in a medium in which each point on the axis of the wave has an associated constant amplitude. The locations at which the amplitude is minimum are called nodes, and the locations where the amplitude is maximum are called antinodes. 
This phenomenon can occur because the medium is moving in the opposite direction to the wave, or it can arise in a stationary medium as a result of interference between two waves traveling in opposite directions. The most common cause of standing waves is the phenomenon of resonance, in which standing waves occur inside a resonator due to interference between waves reflected back and forth at the resonator's resonant frequency. 
For waves of equal amplitude traveling in opposing directions, there is on average no net propagation of energy.
Moving medium.
As an example of the first type, under certain meteorological conditions standing waves form in the atmosphere in the lee of mountain ranges. Such waves are often exploited by glider pilots.
Standing waves and hydraulic jumps also form on fast flowing river rapids and tidal currents such as the Saltstraumen maelstrom. Many standing river waves are popular river surfing breaks.
Opposing waves.
As an example of the second type, a "standing wave" in a transmission line is a wave in which the distribution of current, voltage, or field strength is formed by the superposition of two waves of the same frequency propagating in opposite directions. The effect is a series of nodes (zero displacement) and anti-nodes (maximum displacement) at fixed points along the transmission line. Such a standing wave may be formed when a wave is transmitted into one end of a transmission line and is reflected from the other end by an impedance mismatch, "i.e.", discontinuity, such as an open circuit or a short. The failure of the line to transfer power at the standing wave frequency will usually result in attenuation distortion.
In practice, losses in the transmission line and other components mean that a perfect reflection and a pure standing wave are never achieved. The result is a "partial standing wave", which is a superposition of a standing wave and a traveling wave. The degree to which the wave resembles either a pure standing wave or a pure traveling wave is measured by the standing wave ratio (SWR).
Another example is standing waves in the open ocean formed by waves with the same wave period moving in opposite directions. These may form near storm centres, or from reflection of a swell at the shore, and are the source of microbaroms and microseisms.
Mathematical description.
In one dimension, two waves with the same frequency, wavelength and amplitude traveling in opposite directions will interfere and produce a standing wave or stationary wave. For example: a wave traveling to the right along a taut string and hitting the end will reflect back in the other direction along the string, and the two waves will superpose to produce a standing wave. The reflective wave has to have the same amplitude and frequency as the incoming wave.
If the string is held at both ends, forcing zero movement at the ends, the ends become zeroes or "nodes" of the wave. The length of the string then becomes a measure of which waves the string will entertain: the longest wavelength is called the "fundamental". Half a wavelength of the fundamental fits on the string. Shorter wavelengths also can be supported as long as multiples of half a wavelength fit on the string. The frequencies of these waves all are multiples of the fundamental, and are called "harmonics" or "overtones". For example, a guitar player can select an overtone by putting a finger on a string to force a node at the proper position between the ends of the string, suppressing all harmonics that do not share this node.
Harmonic waves travelling in opposite directions can be represented by the equations below:
and
where:
So the resultant wave "y" equation will be the sum of "y1" and "y2":
Using the trigonometric sum-to-product identity for 'sin("u") + sin("v")' to simplify:
This describes a wave that oscillates in time, but has a spatial dependence that is stationary: sin("kx"). At locations "x" = 0, "λ"/2, "λ", 3"λ"/2, ... called the nodes the amplitude is always zero, whereas at locations "x" = "λ"/4, 3"λ"/4, 5"λ"/4, ... called the anti-nodes, the amplitude is maximum. The distance between two conjugative nodes or anti-nodes is "λ"/2.
Standing waves can also occur in two- or three-dimensional resonators. With standing waves on two-dimensional membranes such as drumheads, illustrated in the animations above, the nodes become nodal lines, lines on the surface at which there is no movement, that separate regions vibrating with opposite phase. These nodal line patterns are called Chladni figures. In three-dimensional resonators, such as musical instrument sound boxes and microwave cavity resonators, there are nodal surfaces.
Examples.
One easy to understand example of standing waves is two people shaking either end of a jump rope. If they shake in sync the rope can form a regular pattern of waves oscillating up and down, with stationary points along the rope where the rope is almost still (nodes) and points where the arc of the rope is maximum (antinodes)
Sound waves.
Standing waves are also observed in physical media such as strings and columns of air. Any waves traveling along the medium will reflect back when they reach the end. This effect is most noticeable in musical instruments where, at various multiples of a vibrating string or air column's natural frequency, a standing wave is created, allowing harmonics to be identified. Nodes occur at fixed ends and anti-nodes at open ends. If fixed at only one end, only odd-numbered harmonics are available. At the open end of a pipe the anti-node will not be exactly at the end as it is altered by its contact with the air and so end correction is used to place it exactly. The density of a string will affect the frequency at which harmonics will be produced; the greater the density the lower the frequency needs to be to produce a standing wave of the same harmonic.
Light.
Standing waves are also observed in optical media such as optical wave guides, optical cavities, etc. Lasers use optical cavities in the form of a pair of facing mirrors. The gain medium in the cavity (such as a crystal) emits light coherently, exciting standing waves of light in the cavity. The wavelength of light is very short (in the range of nanometers, 10−9 m) so the standing waves are microscopic in size. One use for standing light waves is to measure small distances, using optical flats.
Mechanical waves.
Standing waves can be mechanically induced into solid medium using resonance. One easy to understand example is two people shaking either end of a jump rope. If they shake in sync, the rope will form a regular pattern with nodes and antinodes and appear to be stationary, hence the name standing wave. Similarly a cantilever beam can have a standing wave imposed on it by applying a base excitation. In this case the free end moves the greatest distance laterally compared to any location along the beam. Such a device can be used as a sensor to track changes in frequency or phase of the resonance of the fiber. One application is as a measurement device for dimensional metrology.
Seismic waves.
Standing surface waves on the Earth are observed as free oscillations of the Earth.
Faraday waves.
The Faraday wave is a non-linear standing wave at the air-liquid interface induced by hydrodynamic instability. It can be used as a liquid-based template to assemble microscale materials.

</doc>
<doc id="41742" url="http://en.wikipedia.org/wiki?curid=41742" title="Standing wave ratio">
Standing wave ratio

In radio engineering and telecommunications, standing wave ratio (SWR) is a measure of impedance matching of loads to the characteristic impedance of a transmission line or waveguide. Impedance mismatches result in standing waves along the transmission line, and SWR is defined as the ratio of the partial standing wave's amplitude at an antinode (maximum) to the amplitude at a node (minimum) along the line.
The SWR is usually thought of in terms of the maximum and minimum AC voltages along the transmission line, thus called the voltage standing wave ratio or VSWR (sometimes pronounced "viswar"). For example, the VSWR value 1.2:1 denotes an AC voltage due to standing waves along the transmission line reaching a peak value 1.2 times that of the minimum AC voltage along that line. The SWR can as well be defined as the ratio of the maximum amplitude to minimum amplitude of the transmission line's currents, electric field strength, or the magnetic field strength. Neglecting transmission line loss, these ratios are identical.
The power standing wave ratio (PSWR) is defined as the square of the VSWR, however this terminology has no physical relation to actual powers involved in transmission.
The SWR can be measured with an instrument called an SWR meter. Since SWR is defined relative to the transmission line's characteristic impedance, the SWR meter must be constructed for that impedance; in practice most transmission lines used in these applications are coaxial cables with an impedance of either 50 or 75 ohms. Checking the SWR is a standard procedure in a radio station, for instance, to verify impedance matching of the antenna to the transmission line (and transmitter). Unlike connecting an impedance analyzer (or "impedance bridge") directly to the antenna (or other load), the SWR does not measure the actual impedance of the load, but quantifies the magnitude of the impedance mismatch just performing a measurement on the transmitter side of the transmission line.
Impedance matching.
SWR is used as a measure of impedance matching of a load to the characteristic impedance of a transmission line carrying radio frequency (RF) signals. This especially applies to transmission lines connecting radio transmitters and receivers with their antennas, as well as similar uses of RF cables such as cable television connections to TV receivers and distribution amplifiers. Impedance matching is achieved when the source impedance is the complex conjugate of the load impedance. The easiest way of achieving this, and the way that minimizes losses along the transmission line, is for both the source and load to be real, that is, pure resistances, equal to the characteristic impedance of the transmission line. When there is a mismatch between the load impedance and the transmission line, part of the forward wave sent toward the load is reflected back along the transmission line towards the source. The source then sees a different impedance than it expects which can lead to lesser (or in some cases, more) power being supplied by it, the result being very sensitive to the electrical length of the transmission line.
Such a mismatch is usually undesired and results in standing waves along the transmission line which magnifies transmission line losses (significant at higher frequencies and for longer cables). The SWR is a measure of the depth of those standing waves and is therefore a measure of the matching of the load to the transmission line. A matched load would result in an SWR of 1:1 implying no reflected wave. An infinite SWR represents complete reflection by a load unable to absorb electrical power, with all the incident power reflected back towards the source.
It should be understood that the match of a load to the transmission line is different from the match of a "source" to the transmission line or the match of a source to the load "seen through" the transmission line. For instance, if there is a perfect match between the load impedance "Z"load and the source impedance "Z"source="Z"*load, that perfect match will remain if the source and load are connected through a transmission line with an electrical length of one half wavelength (or a multiple of one half wavelengths) using a transmission line of "any" characteristic impedance "Z"0. However the SWR will generally not be 1:1, depending only on "Z"load and "Z"0. With a different length of transmission line, the source will see a different impedance than "Z"load which may or may not be a good match to the source. Sometimes this is deliberate, as when a quarter-wave matching section is used to improve the match between an otherwise mismatched source and load.
However typical RF sources such as transmitters and signal generators are designed to look into a purely resistive load impedance such as 50Ω or 75Ω, corresponding to common transmission lines' characteristic impedances. In those cases, matching the load to the transmission line, "Z"load="Z"0, "always" insures that the source will see the same load impedance as if the transmission line weren't there. This is identical to a 1:1 SWR. This condition ( "Z"load="Z"0) also means that the load seen by the source is independent of the transmission line's electrical length. Since the electrical length of a physical segment of transmission line depends on the signal frequency, violation of this condition means that the impedance seen by the source through the transmission line becomes a function of frequency (especially if the line is long), even if "Z"load is frequency-independent. So in practice, a good SWR (near 1:1) implies a transmitter's output seeing the exact impedance it expects for optimum and safe operation.
Relationship to the reflection coefficient.
The voltage component of a standing wave in a uniform transmission line consists of the forward wave (with complex amplitude formula_1) superimposed on the reflected wave (with complex amplitude formula_2).
A wave is partly reflected when a transmission line is terminated with other than a pure resistance equal to its characteristic impedance. The reflection coefficient formula_3 is defined thus:
formula_3 is a complex number that describes both the magnitude and the phase shift of the reflection. The simplest cases with formula_3 "measured at the load" are:
The SWR directly corresponds to the magnitude of formula_3.
At some points along the line the forward and reflected waves interfere constructively, exactly in phase, with the resulting amplitude formula_11 given by the sum of their those waves' amplitudes:
At other points, the waves interfere 180° out of phase with the amplitudes partially cancelling:
The voltage standing wave ratio is then equal to:
Since the magnitude of formula_3 always falls in the range [0,1], the SWR is always greater than or equal to unity. Note that the "phase" of "V"f and "V"r vary along the transmission line in opposite directions to each other. Therefore the complex valued reflection coefficient formula_3 varies as well, but only in phase. With the SWR dependent "only" on the complex magnitude of formula_3, it can be seen that the SWR measured at "any" point along the transmission line (neglecting transmission line losses) obtains an identical reading.
Since the power of the forward and reflected waves are proportional to the square of the voltage components due to each wave, SWR can be expressed in terms of forward and reflected power as follows:
In fact, most SWR meters operate by measuring both the forward power and the reflected power. Normalizing the power readings according to the forward power, a reading of the reflected power is then directly read off a meter in terms of SWR.
In the special case of a load "RL" which is purely resistive but unequal to the characteristic impedance of the transmission line "Z0", the SWR is given simply by their ratio:
with the ±1 chosen to obtain a value greater than unity.
The standing wave pattern.
Using complex notation for the voltage amplitudes, for a signal at frequency ν, the actual (real) voltages Vactual as a function of time "t" are understood to relate to the complex voltages according to:
Thus taking the real part of the complex quantity inside the parenthesis, the actual voltage consists of a sine wave at frequency ν with a peak amplitude equal to the complex magnitude of V, and with a phase given by the phase of the complex V. Then with the position along a transmission line given by x, with the line ending in a load located at x0, the complex amplitudes of the forward and reverse waves would be written as:
for some complex amplitude A (corresponding to the forward wave at x0). Here "k" is the wavenumber due to the guided wavelength along the transmission line. Note that some treatments use phasors where the time dependence is according to formula_22 and spatial dependence (for a wave in the +x direction) of formula_23. Either convention obtains the same result for Vactual.
According to the superposition principle the net voltage present at any point x on the transmission line is equal to the sum of the voltages due to the forward and reflected waves:
Since we are interested in the variations of the "magnitude" of Vnet along the line (as a function of x), we shall solve instead for the squared magnitude of that quantity, which simplifies the mathematics. To obtain the squared magnitude we multiply the above quantity by its complex conjugate:
Depending on the phase of the third term, one can see that the maximum and minimum values of Vnet (the square root of the quantity in the equations) are (1 + |Γ|)|A| and (1 − |Γ|)|A| respectively, for a standing wave ratio of:
as we had earlier asserted. Along the line, the above expression for formula_27 is seen to oscillate sinusoidally between formula_28 and formula_29 with a period of 2π/2k. This is "half" of the guided wavelength λ = 2π/k for the frequency ν. That can be seen as due to interference between two waves of that frequency which are travelling in "opposite" directions.
For example, at a frequency ν=20 MHz (free space wavelength of 15 m) in a transmission line whose velocity factor is 2/3, the guided wavelength (distance between voltage peaks of the forward wave alone) would be λ = 10 m. At instances when the forward wave at x = 0 is at zero phase (peak voltage) then at x = 10 m it would also be at zero phase, but at x = 5 m it would be at 180° phase (peak "negative" voltage). On the other hand, the magnitude of the voltage due to a standing wave produced by its addition to a reflected wave, would have a wavelength between peaks of only λ/2 = 5 m. Depending on the location of the load and phase of reflection, there might be a peak in the magnitude of Vnet at x = 1.3 m. Then there would be another peak found where |Vnet|=Vmax at x = 6.3 m, whereas it would find minima of the standing wave |Vnet| = Vmin at x = 3.8 m, 8.8 m, etc.
Practical implications of SWR.
The most common case for measuring and examining SWR is when installing and tuning transmitting antennas. When a transmitter is connected to an antenna by a feed line, the driving point impedance of the antenna must be resistive and matching the characteristic impedance of the feed line in order for the transmitter to see the impedance it was designed for (the impedance of the feed line, usually 50 or 75 ohms).
The impedance of a particular antenna design can vary due to a number of factors that cannot always be clearly identified. This includes the transmitter frequency (as compared to the antenna's design or resonant frequency), the antenna's height above the ground and proximity to large metal structures, and variations in the exact size of the conductors used to construct the antenna.
When an antenna and feed line do not have matching impedances, the transmitter sees an unexpected impedance, where it might not be able to produce its full power, and can even damage the transmitter in some cases.
The reflected power in the transmission line increases the average current and therefore losses in the transmission line compared to power actually delivered to the load.
It is the interaction of these reflected waves with forward waves which causes standing wave patterns, with the negative repercussions we have noted.
Matching the impedance of the antenna to the impedance of the feed line can sometimes be accomplished through adjusting the antenna itself, but otherwise is possible using an antenna tuner, an impedance matching device. Installing the tuner between the feed line and the antenna allows for the feed line to see a load close to its characteristic impedance, while sending most of the transmitter's power (a small amount may be dissipated within the tuner) to be radiated by the antenna despite its otherwise unacceptable feed point impedance. Installing a tuner in between the transmitter and the feed line can also transform the impedance seen at the transmitter end of the feed line to one preferred by the transmitter. However in the latter case, the feed line still has a high SWR present, with the resulting increased feed line losses unmitigated.
The magnitude of those losses are dependent on the type of transmission line, and its length. They always increase with frequency. For example, a certain antenna used well away from its resonant frequency may have an SWR of 6:1. For a frequency of 3.5 MHz, with that antenna fed through 75 meters of RG-8A coax, the loss due to standing waves would be 2.2 dB. However the same 6:1 mismatch through 75 meters of RG-8A coax would incur 10.8 dB of loss at 146 MHz. Thus, a better match of the antenna to the feed line, that is, a lower SWR, becomes increasingly important with increasing frequency, even if the transmitter is able to accommodate the impedance seen (or an antenna tuner is used between the transmitter and feed line).
Power standing wave ratio.
The term "power standing wave ratio" (PSWR) is sometimes referred to, and defined as the square of the voltage standing wave ratio. The term is widely cited as "misleading." In the words of Gridley:
The expression "power standing-wave ratio", which may sometimes be encountered is even more misleading, for the power distribution along a loss-free line is constant...—J. H. Gridley
In other words, there are no actual powers being compared. Patently a misnomer, the term "power standing wave ratio" is not the ratio of any two physical quantities.
However it does correspond to one type of measurement of SWR using what was formerly a standard measuring instrument at microwave frequencies. A slotted line involves a waveguide (or air-filled coaxial line) in which a small sensing antenna measures the electric field along the transmission line "directly". The electric field strength is commonly measured using a crystal detector or Schottky barrier diode. These detectors have a square law output for low levels of input. Readings therefore corresponded to the square of the electric field along the slot, "E"2("x"), with maximum and minimum readings of "E"2max and "E"2min found as the probe is moved along the slot. The ratio of these yields the PSWR directly, the square root of which is the VSWR.
Implications of SWR on medical applications.
SWR can also have a detrimental impact upon the performance of microwave based medical applications. In microwave electrosurgery an antenna that is placed directly into tissue may not always have an optimal match with the feedline resulting in an SWR. The presence of SWR can affect monitoring components used to measure power levels impacting the reliability of such measurements.

</doc>
<doc id="41743" url="http://en.wikipedia.org/wiki?curid=41743" title="Star coupler">
Star coupler

A star coupler is a device that takes in an input signal and splits it into several output signals.
In fiber optics, and especially in telecommunications, a star coupler is a passive optical device, used in network applications. An optical signal introduced into any input port is distributed to all output ports. Because of the way a passive star coupler is constructed, the number of ports is usually a power of 2; "i.e.", two input ports and two output ports (a "two-port" coupler, customarily called a "directional coupler," or "splitter" ); four input ports and four output ports (a "four-port" coupler); eight input ports and eight output ports (an "eight-port" coupler), "etc".
DEC computers.
A star coupler was also the name of a device sold by Digital Equipment Corporation (now part of Hewlett-Packard) of Maynard, Massachusetts. In this case, the star coupler interconnected links to computers via coaxial cable rather than optical fibres, but the function was essentially the same. The signal that was distributed was 70 Mb/s computer interconnect (CI) data and the star coupler provided two redundant paths of either 8 or 16 ports each. Digital's star coupler was developed for use with the VAX- and later Alpha-based computers running Digital's VMS operating system, to provide a passive, highly reliable interconnect for Digital's cluster technology.

</doc>
<doc id="41744" url="http://en.wikipedia.org/wiki?curid=41744" title="Start signal">
Start signal

In telecommunication, a start signal is a signal that prepares a device to receive data or to perform a function. 
In asynchronous serial communication, start signals are used at the beginning of a character that prepares the receiving device for the reception of the code elements. 
A start signal is limited to one signal element usually having the duration of a unit interval. 

</doc>
<doc id="41746" url="http://en.wikipedia.org/wiki?curid=41746" title="Statement">
Statement

Statement may refer to:

</doc>
<doc id="41748" url="http://en.wikipedia.org/wiki?curid=41748" title="Step-index profile">
Step-index profile

For an optical fiber, a step-index profile is a refractive index profile characterized by a uniform refractive index within the core and a sharp decrease in refractive index at the core-cladding interface so that the cladding is of a lower refractive index. The step-index profile corresponds to a power-law index profile with the profile parameter approaching infinity. The step-index profile is used in most single-mode fibers and some multimode fibers.
A step-index fiber is characterized by the core and cladding refractive indices "n1" and "n2" and the core and cladding radii a and b. Examples of standard core and cladding diameters 2a/2b are 8/125, 50/125, 62.5/125, 85/125, or 100/140 (units of µm). The fractional refractive-index change formula_1. The value of n1 is typically between 1.44 and 1.46, and formula_2 is typically between 0.001 and 0.02.
Step-index optical fiber is generally made by doping high-purity fused silica glass (SiO2) with different concentrations of materials like titanium, germanium, or boron.
Pulse dispersion in a step index optical fiber is given by
formula_3
where
formula_4 is the difference in refractive indices of core and cladding.
formula_5 is the refractive index of core
formula_6 is the length of the optical fiber under observation
formula_7
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41749" url="http://en.wikipedia.org/wiki?curid=41749" title="Stopband">
Stopband

A stopband is a band of frequencies, between specified limits, through which a circuit, such as a filter or telephone circuit, does not allow signals to pass, or the attenuation is above the required stopband attenuation level. Depending on application, the required attenuation within the stopband may typically be a value between 20 and 120 dB higher than the nominal passband attenuation, which often is 0 dB.
The lower and upper "limiting frequencies", also denoted lower and upper stopband corner frequencies, are the frequencies where the stopband and the transition bands meet in a filter specification. The stopband of a low-pass filter is the frequencies from the stopband corner frequency (which is slightly higher than the passband 3 dB cut-off frequency) up to the infinite frequency. The stopband of a high-pass filter consists of the frequencies from 0 hertz to a stopband corner frequency (slightly lower than the passband cut-off frequency). 
A band-stop filter has one stopband, specified by two non-zero and non-infinite corner frequencies. The difference between the limits in the band-stop filter is the stopband bandwidth, which usually is expressed in hertz. 
A bandpass filter typically has two stopbands. The shape factor of a bandpass filter is the relationship between the 3dB bandwidth, and the difference between the stopband limits.

</doc>
<doc id="41750" url="http://en.wikipedia.org/wiki?curid=41750" title="Stop signal">
Stop signal

In telecommunication, a stop signal is a signal that marks the end of part of a transmission, for example:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41751" url="http://en.wikipedia.org/wiki?curid=41751" title="Store-and-forward switching center">
Store-and-forward switching center

In telecommunication, a store-and-forward switching center is a message switching center in which a message is accepted from the originating user, "i.e.," sender, when it is offered, held in a physical storage, and forwarded to the destination user, "i.e.," receiver, in accordance with the priority placed upon the message by the originating user and the availability of an outgoing channel.
Store and forward switching centers are usually implemented in mobile service stations where the messages that are sent from the sender is first sent to these centers. If the destination address isn't available then the center stores this message and tries sending it later. This improves the probability of the message to be delivered. In the other case, if the destination is available at that time, then the message is immediately sent.

</doc>
<doc id="41752" url="http://en.wikipedia.org/wiki?curid=41752" title="Stressed environment">
Stressed environment

Stressed environment: In radio communications, an environment that is under the influence of extrinsic factors that degrade communications integrity, such as when (a) the benign communications medium is disturbed by natural or man-made events (such as an intentional nuclear burst), (b) the received signal is degraded by natural or man-made interference (such as jamming signals or co-channel interference), (c) an interfering signal can reconfigure the network, and/or (d) an adversary threatens successful communications, in which case radio signals may be encrypted in order to deny the adversary an intelligible message, traffic flow information, network information, or automatic link establishment (ALE) control information.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41754" url="http://en.wikipedia.org/wiki?curid=41754" title="Sublayer">
Sublayer

In telecommunication, the term sublayer has the following meanings: 

</doc>
<doc id="41757" url="http://en.wikipedia.org/wiki?curid=41757" title="Substitution method">
Substitution method

In optical fiber technology, the substitution method is a method of measuring the transmission loss of a fiber. It consists of:
The substitution method has certain shortcomings with regard to its accuracy, but its simplicity makes it a popular field test method. It is conservative, in that if it were used to measure the individual losses of several long fibers, and the long fibers were concatenated, the total loss obtained (excluding splice losses) would be expected to be lower than the sum of the individual fiber losses. 
Some modern optical power meters have the capability to set to zero the reference level measured at the output of the reference fiber, so that the transmission loss of the fiber under test may be read out directly.

</doc>
<doc id="41760" url="http://en.wikipedia.org/wiki?curid=41760" title="Summation check">
Summation check

In telecommunication, the term summation check (sum check) has the following meanings: 

</doc>
<doc id="41761" url="http://en.wikipedia.org/wiki?curid=41761" title="Supervisory program">
Supervisory program

A supervisory program or supervisor is a computer program, usually part of an operating system, that controls the execution of other routines and regulates work scheduling, input/output operations, error actions, and similar functions and regulates the flow of work in a data processing system. 
It can also refer to a program that allocates computer component space and schedules computer events by task queuing and system interrupts. Control of the system is returned to the supervisory program frequently enough to ensure that demands on the system are met.
Historically, this term was essentially associated with IBM's line of mainframe operating systems starting with OS/360. In other operating systems, the supervisor is generally called the kernel.
In the 1970s, IBM further abstracted the supervisor state from the hardware, resulting in a hypervisor that enabled full virtualization, i.e. the capacity to run multiple operating systems on the same machine totally independently from each other. Hence the first such system was called "Virtual Machine" or "VM".

</doc>
<doc id="41762" url="http://en.wikipedia.org/wiki?curid=41762" title="Reduced-carrier transmission">
Reduced-carrier transmission

Reduced-carrier transmission is an amplitude modulation (AM) transmission in which the carrier wave level is reduced to reduce wasted electrical power. Suppressed-carrier transmission is a special case in which the carrier level is reduced below that required for demodulation by a normal receiver. 
Reduction of the carrier level permits higher power levels in the sidebands than would be possible with conventional AM transmission. Carrier power must be restored by the receiving station to permit demodulation, usually by means of a beat frequency oscillator (BFO). Failure of the BFO to match the original carrier frequency when receiving such a signal will cause a heterodyne.
Suppressed carriers are often used for single sideband (SSB) transmissions, such as for amateur radio on shortwave. That system is referred to in full as SSB suppressed carrier (SSBSC) or (SSB-SC). International broadcasters agreed in 1985 to also use SSBSC entirely by 2015, though IBOC and IBAC digital radio (namely Digital Radio Mondiale) seems likely to make this irrelevant.
FM stereo transmissions use a double-sideband suppressed carrier (DSBSC) signal from a stereo generator, together with a pilot tone of exactly half the original carrier frequency. This allows reconstitution of the original stereo carrier, and hence the stereo signal.

</doc>
<doc id="41763" url="http://en.wikipedia.org/wiki?curid=41763" title="Surface wave">
Surface wave

In physics, a surface wave is a mechanical wave that propagates along the interface between differing media, usually as a gravity wave between two fluids with different densities. A surface wave can also be an elastic (or seismic) wave, such as "Rayleigh wave" or "Love wave". It can also be an electromagnetic wave guided by a refractive index gradient. In radio transmission, a ground wave is a surface wave that propagates close to the surface of the Earth.
Mechanical waves.
In seismology, several types of surface waves are encountered. Surface waves, in this mechanical sense, are commonly known as either "Love waves" (L waves) or "Rayleigh waves". A seismic wave is a wave that "travels through the Earth, often as the result of an earthquake or explosion." Love waves have transverse motion (movement is perpendicular to the direction of travel, like light waves), whereas Rayleigh waves have both longitudinal (movement parallel to the direction of travel, like sound waves) and transverse motion. Seismic waves are studied by seismologists and measured by a seismograph or seismometer. Surface waves span a wide frequency range, and the period of waves that are most damaging is usually 10 seconds or longer. Surface waves can travel around the globe many times from the largest earthquakes. Surface waves are caused when P waves and S waves come to the surface.
The term "surface wave" can describe waves over an ocean, even when they are approximated by Airy functions and are more properly called creeping waves. Examples are the waves at the surface of water and air (ocean surface waves), or ripples in the sand at the interface with water or air. Another example is internal waves, which can be transmitted along the interface of two water masses of different densities.
Electromagnetic waves.
"Ground waves" refer to the propagation of radio waves parallel to and adjacent to the surface of the Earth, following the curvature of the Earth. These "surface waves" are also known loosely as the Norton surface wave, the Zenneck surface wave, Sommerfeld waves, and gliding waves. See also Dyakonov surface waves (DSW) propagating at the interface of transparent materials with different symmetry.
Radio propagation.
Lower frequencies, below 3 MHz, travel efficiently as ground waves. This is because they are more strongly diffracted around obstacles due to their long wavelengths, allowing them to follow the Earth's curvature. The Earth has one refractive index and the atmosphere has another, thus constituting an interface that supports the surface wave transmission. Ground waves propagate in vertical polarization, with their magnetic field horizontal and electric field (close to) vertical. At VLF, the Ionosphere and earth's surface act as a waveguide.
Conductivity of the surface affects the propagation of ground waves, with more conductive surfaces such as sea water providing better propagation. Increasing the conductivity in a surface results in less dissipation. The refractive indices are subject to spatial and temporal changes. Since the ground is not a perfect electrical conductor, ground waves are attenuated as they follow the earth’s surface. The wavefronts initially are vertical, but the ground, acting as a lossy dielectric, causes the wave to tilt forward as it travels. This directs some of the energy into the earth where it is dissipated, so that the signal decreases exponentially. 
Most long-distance low frequency (LF) "longwave" radio communication (between 30 kHz and 300 kHz) is a result of groundwave propagation. Mediumwave radio transmissions (frequencies between 300 kHz and 3000 kHz), including AM broadcast band, travel both as groundwaves and, for longer distances at night, as skywaves. Ground losses become lower at lower frequencies, greatly increasing the coverage of AM stations using the lower end of the band. The VLF and LF frequencies are mostly used for military communications, especially with ships and submarines. The lower the frequency the better the waves penetrate sea water. Even extremely low frequency waves (below 3 kHz) have been used to communicate with deeply submerged submarines. 
Surface waves have been used in over-the-horizon radar, which operates mainly at frequencies between 2 and 20 MHz over the sea, which has a sufficiently high conductivity to convey the surface waves to and from a reasonable distance (up to 100 km or more; over-horizon radar also uses skywave propagation at much greater distances). In the development of radio, surface waves were used extensively. Early commercial and professional radio services relied exclusively on long wave, low frequencies and ground-wave propagation. To prevent interference with these services, amateur and experimental transmitters were restricted to the higher (HF) frequencies, felt to be useless since their ground-wave range was limited. Upon discovery of the other propagation modes possible at medium wave and short wave frequencies, the advantages of HF for commercial and military purposes became apparent. Amateur experimentation was then confined only to authorized frequencies in the range.
Mediumwave and shortwave reflect off the ionosphere at night, which is known as skywave. During daylight hours, the lower "D" layer of the ionosphere forms and absorbs lower frequency energy. This prevents skywave propagation from being very effective on mediumwave frequencies in daylight hours. At night, when the "D" layer dissipates, mediumwave transmissions travel better by skywave. Ground waves "do not" include ionospheric and tropospheric waves.
Microwave field theory.
Within microwave field theory, the interface of a dielectric and conductor supports "surface wave transmission". Surface waves have been studied as part of transmission lines and some may be considered as single-wire transmission lines.
Characteristics and utilizations of the electrical surface wave phenomenon include:

</doc>
<doc id="41764" url="http://en.wikipedia.org/wiki?curid=41764" title="Survivability">
Survivability

Survivability is the ability to remain alive or continue to exist. The term has more specific meaning in certain contexts.
Ecological.
Following disruptive forces such as flood, fire, disease war, or climate change some species of flora, fauna and local life forms are likely to survive more successfully than others because of consequent changes to their surrounding biophysical conditions.
Engineering.
In engineering, survivability is the quantified ability of a system, subsystem, equipment, process, or procedure to continue to function during and after a natural or man-made disturbance; e.g. nuclear electromagnetic pulse from the detonation of a nuclear weapon.
For a given application, survivability must be qualified by specifying the range of conditions over which the entity will survive, the minimum acceptable level or post-disturbance functionality, and the maximum acceptable downtime.
Military.
In the military environment, survivability is defined as the ability to remain mission capable after a single engagement. Engineers working in survivability are often responsible for improving four main system elements: 
The European Survivability Workshop introduced the concept of "Mission Survivability" whilst retaining the three core areas above, either pertaining to the "survivability" of a platform through a complete mission, or the "survivability" of the mission itself (i.e. probability of mission success). Recent studies have also introduced the concept of "Force Survivability" which relates to the ability of a force rather than an individual platform to remain "mission capable".
There is no clear prioritisation of the three elements; this will depend on the characteristics and role of the platform. Some platform types, such as submarines and airplanes, minimise their susceptibility and may, to some extent, compromise in the other areas. Main Battle Tanks minimise vulnerability through the use of heavy armours. Present day surface warship designs tend to aim for a balanced combination of all three areas.
Naval.
Survivability denotes the ability of a ship and its on-board systems to remain functional and continue designated mission in a man-made hostile environment. The naval vessels are designed to operate in a man-made hostile environment, and therefore the survivability is a vital feature required from them. The naval vessel’s survivability is a complicated subject affecting the whole life cycle of the vessel, and should be considered from the initial design phase of every war ship.
The classical definition of naval survivability includes three main aspects, which are susceptibility, vulnerability, and recoverability; although, recoverability is often subsumed within vulnerability.
Susceptibility consists of all the factors that expose the ship to the weapons effects in a combat environment. These factors in general are the operating conditions, the threat, and the features of the ship itself. The operating conditions, such as sea state, weather and atmospheric conditions, vary considerably, and their influence is difficult to address (hence they are often not accounted for in survivability assessment). The threat is dependent on the weapons directed against the ship and weapon’s performance, such as the range. The features of the ship in this sense include platform signatures (radar, infrared, acoustic, magnetic), the defensive systems on board, such as surface-to-air missiles, EW and decoys, and also the tactics employed by the platform in countering the attack (aspects such as speed, maneuverability, chosen aspect presented to the threat).
Vulnerability refers to the ability of the vessel to withstand the short-term effects of the threat weapon. Vulnerability is an attribute typical to the vessel and therefore heavily
affected by the vessel’s basic characteristics such as size, subdivision, armouring, and other hardening features, and also the design of the ship's systems, in particular the location of equipment, degrees of redundancy and separation, and the presence within a system of single point failures. Recoverability refers to vessel’s ability to restore and maintain its functionality after sustaining damage. Thus, recoverability is dependent on the actions aimed to neutralize the effects of the damage. These actions include firefighting, limiting the extent of flooding, and dewatering. Besides the equipment, the crew also has a vital role in recoverability.
Combat vehicle crew.
The crews of military combat vehicles face numerous lethal hazards which are both diverse and constantly evolving. Improvised Explosive Devices (IEDs), mines, and enemy fire are examples of such persistent and variable threats. Historically, measures taken to mitigate these hazards were concerned with protecting the vehicle itself, but due to this achieving only limited protection, the focus has now shifted to safeguarding the crew within from an ever-broadening range of threats, including Radio Controlled IEDs (RCIEDs), blast, fragmentation, heat stress, and dehydration.
The expressed goal of "crew survivability" is to ensure vehicle occupants are best protected. It goes beyond simply ensuring crew have the appropriate protective equipment and has expanded to include measuring the overpressure and blunt impact forces experienced by a vehicle from real blast incidents in order to develop medical treatment and improve overall crew survivability. Sustainable crew survivability is dependent on the effective integration of knowledge, training, and equipment:
Prevention and training.
Threat intelligence identifying trends, emerging technologies, and attack tactics used by enemy forces enables crews to implement procedures that will reduce their exposure to unnecessary risks. Such intelligence also allows for more effective pre-deployment training programs where personnel can be taught the most up-to-date developments in IED concealment, for example, or undertake tailored training that will enable them to identify the likely attack strategy of enemy forces. In addition, with expert, current threat intelligence, the most effective equipment can be procured or rapidly developed in support of operations.
Network.
Definitions of network survivability.
"The capability of a system to fulfill its mission, in a timely manner, in the presence of such as attacks or large-scale natural disasters. Survivability is a subset of resilience."
“The capability of a system to fulfill its mission, in a timely manner, in the presence of attacks, failures, or accidents.”

</doc>
<doc id="41765" url="http://en.wikipedia.org/wiki?curid=41765" title="Switched loop">
Switched loop

In telephony, a Switched loop is a circuit that automatically releases a connection from an attendant console or switchboard, once the connection has been made to the appropriate terminal. 
Loop buttons or jacks are used to answer incoming listed directory number calls, dial "0" internal calls, transfer requests, and intercepted calls. The attendant can handle only one telephone call at a time. "Synonym": released loop.

</doc>
<doc id="41767" url="http://en.wikipedia.org/wiki?curid=41767" title="Synchronism">
Synchronism

Synchronism is deliberately achieved coincidence at a specified point of time. 
Telecommunication.
In telecommunication the term synchronism has the following meanings: 
1. The state of being synchronous.
2. For repetitive events with the same, multiple, or submultiple repetition rates, a relationship among the events such that a significant instant of one event bears a fixed time relationship to a corresponding instant in another event.
3. The simultaneous occurrence of two or more events at the same instant on the same coordinated time standard.
Art and entertainment.
The term may refer to:

</doc>
<doc id="41768" url="http://en.wikipedia.org/wiki?curid=41768" title="Synchronizing">
Synchronizing

In telecommunication, the term synchronizing has the following meanings: 
In the civilian community, the noun ""synchronization" " is preferred to ""synchronizing"."
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41769" url="http://en.wikipedia.org/wiki?curid=41769" title="Synchronous network">
Synchronous network

In telecommunications, a synchronous network is a network in which clocks are controlled to run, ideally, at identical rates, or at the same mean rate with a fixed relative phase displacement, within a specified limited range. 
Ideally, the clocks are synchronous, but they may be mesochronous in practice. By common usage, such mesochronous networks are frequently described as "synchronous".

</doc>
<doc id="41770" url="http://en.wikipedia.org/wiki?curid=41770" title="Synchronous orbit">
Synchronous orbit

A synchronous orbit is an orbit in which an orbiting body (usually a satellite) has a period equal to the average rotational period of the body being orbited (usually a planet), and in the same direction of rotation as that body.
Simplified meaning.
A synchronous orbit is where something (like a satellite) orbiting a body (like a planet) is orbiting an exact equal line around the object like, orbiting the equator thus orbiting the same speed the planet is spinning.
Properties.
A satellite in a synchronous orbit that is both equatorial and circular will appear to be suspended motionless above a point on the orbited planet's equator. For synchronous satellites orbiting Earth, this is also known as a geostationary orbit. However, a synchronous orbit need not be equatorial; nor circular. A body in a non-equatorial synchronous orbit will appear to oscillate north and south above a point on the planet's equator, whereas a body in an elliptical orbit will appear to oscillate eastward and westward. As seen from the orbited body the combination of these two motions produces a figure-8 pattern called an analemma.
Nomenclature.
There are many specialized terms for synchronous orbits depending on the body orbited. The following are some of the more common ones. A synchronous orbit around Earth that is circular and lies in the equatorial plane is called a geostationary orbit. The more general case, when the orbit is inclined to Earth's equator or is non-circular is called a geosynchronous orbit. The corresponding terms for synchronous orbits around Mars are areostationary and areosynchronous orbits.
Examples.
An astronomical example is Pluto's moon Charon.
Much more commonly, synchronous orbits are employed by artificial satellites used for communication, such as geostationary satellites.
For natural satellites, which can attain a synchronous orbit only by tidally locking their parent body, it always goes in hand with synchronous rotation of the satellite. This is because the smaller body becomes tidally locked faster, and by the time a synchronous orbit is achieved, it has had a locked synchronous rotation for a long time already.

</doc>
<doc id="41771" url="http://en.wikipedia.org/wiki?curid=41771" title="System integrity">
System integrity

In telecommunications, the term system integrity has the following meanings: 

</doc>
<doc id="41772" url="http://en.wikipedia.org/wiki?curid=41772" title="System lifecycle">
System lifecycle

The system lifecycle in systems engineering is a view of a system or proposed system that addresses all phases of its existence to include system conception, design and development, production and/or construction, distribution, operation, maintenance and support, retirement, phase-out and disposal.
Conceptual design.
The conceptual design stage is the stage where an identified need is examined, requirements for potential solutions are defined, potential solutions are evaluated and a system specification is developed. The system specification represents the technical requirements that will provide overall guidance for system design. Because this document governs all future development, the stage cannot be completed until a conceptual design review has determined that the system specification properly addresses the motivating need.
Key steps within the conceptual design stage include:
Preliminary system design.
During this stage of the system lifecycle, subsystems that perform the desired system functions are designed and specified in compliance with the system specification. Interfaces between subsystems are defined, as well as overall test and evaluation requirements. At the completion of this stage, a development specification is produced that is sufficient to perform detailed design and development.
Key steps within the preliminary design stage include:
Detail design and development.
This stage includes the development of detailed designs that brings initial design work into a completed with form of specifications. This work includes the specification of interfaces between the system and its intended environment and a comprehensive evaluation of the systems logistical, maintenance and support requirements. The detail design and development is responsible for producing the product, process and material specifications and may result in substantial changes to the development specification.
Key steps within the detail design and development stage include:
Production and construction.
During the production and/or construction stage the product is built or assembled in accordance with the requirements specified in the product, process and material specifications and is deployed and tested within the operational target environment. System assessments are conducted in order to correct deficiencies and adapt the system for continued improvement.
Key steps within the product construction stage include:
Utilization and support.
Once fully deployed, the system is used for its intended operational role and maintained within its operational environment.
Key steps within the utilization and support stage include:
Phase-out and disposal.
Effectiveness and efficiency of the system must be continuously evaluated to determine when the product has met its maximum effective lifecycle. Considerations include: Continued existence of operational need, matching between operational requirements and system performance, feasibility of system phase-out versus maintenance, and availability of alternative systems.

</doc>
<doc id="41773" url="http://en.wikipedia.org/wiki?curid=41773" title="Systems control">
Systems control

Systems control, in a communications system, is the control and implementation of a set of functions that:

</doc>
<doc id="41774" url="http://en.wikipedia.org/wiki?curid=41774" title="Systems design">
Systems design

Systems design is the process of defining the architecture, components, modules, interfaces, and data for a system to satisfy specified requirements. Systems design could be seen as the application of systems theory to product development. There is some overlap with the disciplines of systems analysis, systems architecture and systems engineering.
Overview.
If the broader topic of product development "blends the perspective of marketing, design, and manufacturing into a single approach to product development," then design is the act of taking the marketing information and creating the design of the product to be manufactured. Systems design is therefore the process of defining and developing systems to satisfy specified requirements of the user. 
Until the 1990s systems design had a crucial and respected role in the data processing industry. In the 1990s standardization of hardware and software resulted in the ability to build modular systems. The increasing importance of software running on generic platforms has enhanced the discipline of software engineering.
Object-oriented analysis and design methods are becoming the most widely used methods for computer systems design. The UML has become the standard language in object-oriented analysis and design. It is widely used for modeling software systems and is increasingly used for high designing non-software systems and organizations.
Architectural design.
The architectural design of a system emphasizes on the design of the systems architecture which describes the structure, behavior, and more views of that system and analysis.
Logical design.
The logical design of a system pertains to an abstract representation of the data flows, inputs and outputs of the system. This is often conducted via modelling, using an over-abstract (and sometimes graphical) model of the actual system. In the context of systems design are included. Logical design includes ER Diagrams i.e. Entity Relationship Diagrams.
Physical design.
The physical design relates to the actual input and output processes of the system. This is laid down in terms of how data is input into a system, how it is verified/authenticated, how it is processed, and how it is displayed as 
In Physical design, the following requirements about the system are decided.
Put another way, the physical portion of systems design can generally be broken down into three sub-tasks:
User Interface Design is concerned with how users add information to the system and with how the system presents information back to them. Data Design is concerned with how the data is represented and stored within the system. Finally, Process Design is concerned with how data moves through the system, and with how and where it is validated, secured and/or transformed as it flows into, through and out of the system. At the end of the systems design phase, documentation describing the three sub-tasks is produced and made available for use in the next phase.
Physical design, in this context, does not refer to the tangible physical design of an information system. To use an analogy, a personal computer's physical design involves input via a keyboard, processing within the CPU, and output via a monitor, printer, etc. It would not concern the actual layout of the tangible hardware, which for a PC would be a monitor, CPU, motherboard, hard drive, modems, video/graphics cards, USB slots, etc.
It involves a detailed design of a user and a product database structure processor and a control processor. The H/S personal specification is developed for the proposed system.
Alternative design methodologies.
Rapid application development (RAD).
Rapid application development (RAD) is a methodology in which a systems designer produces prototypes for an end-user. The end-user reviews the prototype, and offers feedback on its suitability. This process is repeated until the end-user is satisfied with the final system.
Joint application design (JAD).
Joint application design (JAD) is a methodology which evolved from RAD, in which a systems designer consults with a group consisting of the following parties:
JAD involves a number of stages, in which the group collectively develops an agreed pattern for the design and implementation of the system.

</doc>
<doc id="41775" url="http://en.wikipedia.org/wiki?curid=41775" title="Tactical communications">
Tactical communications

Tactical communications are military communications in which information of any kind, especially orders and military intelligence, are conveyed from one command, person, or place to another upon a battlefield, particularly during the conduct of combat. It includes any kind of delivery of information, whether verbal, written, visual or auditory, and can be sent in a variety of ways. In modern times, this is usually done by electronic means. Tactical communications do not include communications provided to tactical forces by the Defense Communications System to non-tactical military commands, to tactical forces by civil organizations, nor does it include strategic communication.
Early Means.
The earliest way of communicating with others in a battle was by the commander's voice or by human messenger. Someone would have to run from one commander to a subordinate to tell them what to do. Once the horse was domesticated messages could travel much faster. A very fast way to send information was to use either drums, trumpets or flags. Each sound or banner would have a pre-determined significance for the soldier who would respond accordingly. Auditory signals were only as effective, though, as the receiver's ability to hear them. The din of battle or long distances could make using noise less effective. They were also limited in the amount of information they could convey; the information must be simple, such as "attack" or "retreat".
Visual cues, such as flags or smoke signals required the receiver to have a clear line of sight to the signal, and know when and where to look for them. Intricate warning systems have though always been used such as scouting towers with fires to signal incoming threats - this could occur at the tactical as well as the strategic level. The armies of the 19th century used two flags in combinations that replicated the alphabet. This allowed commanders the ability to send any order they wanted as they needed to, but still relied on line-of-sight. During the Siege of Paris (1870-1871) the defending French effectively used carrier pigeons to relay information between tactical units.
The Wireless Revolution.
Although visual communication flew at the speed of light, it relied on a direct line of sight between the sender and the receiver. Telegraphs helped theater commanders to move large armies about, but one certainly could not count on using immobile telegraph lines on a changing battlefield. At the end of the 19th century the disparate units across any field were instantaneously joined to their commanders by the invention and mass production of the radio. At first the radio could only broadcast tones, so messages were send via Morse code. The first field radios used by the United States Army saw action in the Spanish–American War (1898) and the Philippine Insurrection (1899-1902). At the same time as radios were deployed the field telephone was developed and made commercially viable. This caused a new signal occupation specialty to be developed: lineman.
The Digital Battlefield.
Security was a problem. If you broadcast your plans over radio waves, anyone with a similar radio listening to the same frequency could hear your plans. Advances in electronics, particularly during World War II, allowed for electronic scrambling of radio broadcasts, which permitted messages to be encrypted with ciphers too complex for humans to crack without the assistance of a similar, high-tech machine, such as the German Enigma machine. Once computer science advanced, large amounts of data could be sent over the airwaves in quick bursts of signals and more complex encryption was allowed.
Communication between armies were of course much more difficult before the electronic age and could only be achieved with messengers on horseback or by foot and with time delays according to the distance the messenger needed to travel. Advances in long-range communications aided the commander on the battlefield, for then they could receive news of any outside force or factor that could impact the conduct of a battle.
Sources.
History of Communications. Bakersfield, CA: William Penn School, 2011. http://library.thinkquest.org/5729/ 
Raines, Rebecca Robbins. "Getting the Message Through: A Branch History of the U.S. Army Signal Corps" (Washington: Center of Military History, US Army, 1999).
Rienzi, Thomas Matthew. "Vietnam Studies: Communications-Electronics 1962-1970. (Washington: Deptartment of the Army, 1985.
"Signal Corps History." Augusta, GA: United States Army Signal Center, 2012. http://www.signal.army.mil/ocos/rdiv/histarch/schist.asp

</doc>
<doc id="41776" url="http://en.wikipedia.org/wiki?curid=41776" title="Tactical communications system">
Tactical communications system

In telecommunication, a tactical communications system is a communications system that (a) is used within, or in direct support of, tactical forces, (b) is designed to meet the requirements of changing tactical situations and varying environmental conditions, (c) provides securable communications, such as voice, data, and video, among mobile users to facilitate command and control within, and in support of, tactical forces, and (d) usually requires extremely short installation times, usually on the order of hours, in order to meet the requirements of frequent relocation. 

</doc>
<doc id="41777" url="http://en.wikipedia.org/wiki?curid=41777" title="Tactical data information link–A">
Tactical data information link–A

In telecommunication, a tactical data information link—A (TADIL—A) is a netted link in which one unit acts as a net control station and interrogates each unit by roll call. Once interrogated, that unit transmits its data to the net. This means that each unit receives all the information transmitted. This is a direct transfer of data and no relaying is involved.
External links.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41778" url="http://en.wikipedia.org/wiki?curid=41778" title="Tape relay">
Tape relay

A tape relay is a method of retransmitting teletypewriter traffic from one channel to another, in which messages arriving on an incoming channel are recorded in the form of perforated tape, this punched tape then being either fed directly and automatically into an outgoing channel, or manually transferred to an automatic transmitter for transmission on an outgoing channel. Tape relay, sometimes informally called "torn tape operation" was commonplace during much of the 20th century. 

</doc>
<doc id="41779" url="http://en.wikipedia.org/wiki?curid=41779" title="T-carrier">
T-carrier

T-carrier, sometimes abbreviated as T-CXR, refers to one of several digital transmission systems developed by Bell Labs. T-carriers are used in North America, South Korea, and Japan. 
The first of these was Transmission System 1 (T-1), which Bell Labs introduced in 1962. T-1 greatly increased the number of telephone calls the telephone network was capable of transmitting at one time.
Transmission System 1 (T-1).
T-1 is a hardware specification for telecommunications trunking. A "trunk" is a single transmission channel between two points on the network: each point is either a switching center or a node (such as a telephone).
Initially, T-1 trunks were used only to connect major telephone exchanges, via the same twisted pair copper wire that the analog trunks used. If the exchanges were too far apart, a repeater boosted the signal.
Before the digital T-1 system, carrier wave systems such as 12-channel carrier systems worked by frequency division multiplexing; each call was an analog signal. A T-1 trunk could transmit 24 telephone calls at a time, because it used a digital carrier signal called Digital Signal 1 (DS-1). DS-1 is a communications protocol for multiplexing the bitstreams of up to 24 telephone calls, along with two special bits: a "framing bit" (for frame synchronization) and a "maintenance-signalling bit". T-1's maximum data transmission rate is 1.544 megabits per second.
Throughout Europe and most of the rest of the world there is a comparable transmission system called E-carrier, which is not directly compatible with T-carrier.
Legacy.
Existing frequency-division multiplexing carrier systems worked well for connections between distant cities, but required expensive modulators, demodulators and filters for every voice channel. For connections within metropolitan areas, Bell Labs in the late 1950s sought cheaper terminal equipment. Pulse-code modulation allowed sharing a coder and decoder among several voice trunks, so this method was chosen for the T1 system introduced into local use in 1961. In later decades, the cost of digital electronics declined to the point that an individual codec per voice channel became commonplace, but by then the other advantages of digital transmission had become entrenched.
The most common legacy of this system is the line rate speeds. "T1" now means any data circuit that runs at the original 1.544 Mbit/s line rate. Originally the T1 format carried 24 pulse-code modulated, time-division multiplexed speech signals each encoded in 64 kbit/s streams, leaving 8 kbit/s of framing information which facilitates the synchronization and demultiplexing at the receiver. The T2 and T3 circuit channels carry multiple T1 channels multiplexed, resulting in transmission rates of 6.312 and 44.736 Mbit/s, respectively. A T3 line comprises 28 T1 lines, each operating at total signaling rate of 1.544 Mbit/s. It is possible to get a fractional T3 line, meaning a T3 line with some of the 28 lines turned off, resulting in a slower transfer rate but typically at reduced cost.
Supposedly, the 1.544 Mbit/s rate was chosen because tests done by AT&T Long Lines in Chicago were conducted underground. The test site was typical of Bell System outside plant of the time in that, to accommodate loading coils, cable vault manholes were physically 2000 m apart, which determined the repeater spacing. The optimum bit rate was chosen empirically—the capacity was increased until the failure rate was unacceptable, then reduced to leave a margin. Companding allowed acceptable audio performance with only seven bits per PCM sample in this original T1/D1 system. The later D3 and D4 channel banks had an extended frame format, allowing eight bits per sample, reduced to seven every sixth sample or frame when one bit was "robbed" for signaling the state of the channel. The standard does not allow an all zero sample which would produce a long string of binary zeros and cause the repeaters to lose bit sync. However, when carrying data (Switched 56) there could be long strings of zeros, so one bit per sample is set to "1" (jam bit 7) leaving 7 bits × 8,000 frames per second for data.
A more detailed understanding of how the rate of 1.544 Mbit/s was divided into channels is as follows. (This explanation glosses over T1 voice communications, and deals mainly with the numbers involved.) Given that the telephone system nominal voiceband (including guardband) is 4,000 Hz, the required digital sampling rate is 8,000 Hz (see Nyquist rate). Since each T1 frame contains 1 byte of voice data for each of the 24 channels, that system needs then 8,000 frames per second to maintain those 24 simultaneous voice channels. Because each frame of a T1 is 193 bits in length (24 channels × 8 bits per channel + 1 framing bit = 193 bits), 8,000 frames per second is multiplied by 193 bits to yield a transfer rate of 1.544 Mbit/s (8,000 × 193 = 1,544,000).
Initially, T1 used Alternate Mark Inversion (AMI) to reduce frequency bandwidth and eliminate the DC component of the signal. Later B8ZS became common practice. For AMI, each mark pulse had the opposite polarity of the previous one and each space was at a level of zero, resulting in a three level signal which however only carried binary data. Similar British 23 channel systems at 1.536 megabaud in the 1970s were equipped with ternary signal repeaters, in anticipation of using a 3B2T or 4B3T code to increase the number of voice channels in future, but in the 1980s the systems were merely replaced with European standard ones. American T-carriers could only work in AMI or B8ZS mode.
The AMI or B8ZS signal allowed a simple error rate measurement. The D bank in the central office could detect a bit with the wrong polarity, or "bipolarity violation" and sound an alarm. Later systems could count the number of violations and reframes and otherwise measure signal quality and allow a more sophisticated alarm indication signal system.
Historical note on the 193-bit T1 frame.
The decision to use a 193-bit frame was made in 1958. To allow for the identification of information bits within a frame, two alternatives were considered. Assign (a) just one extra bit, or (b) additional eight bits per frame. The 8-bit choice is cleaner, resulting in a 200-bit frame, twenty-five 8-bit channels, of which 24 are traffic and one 8-bit channel available for operations, administration, and maintenance (OA&M). AT&T chose the single bit per frame not to reduce the required bit rate (1.544 vs 1.6 Mbit/s), but because AT&T Marketing worried that "if 8 bits were chosen for OA&M function, someone would then try to sell this as a voice channel and you wind up with nothing."
Soon after commercial success of T1 in 1962, the T1 engineering team realized the mistake of having only one bit to serve the increasing demand for housekeeping functions. They petitioned AT&T management to change to 8-bit framing. This was flatly turned down because it would make installed systems obsolete.
Having this hindsight, some ten years later, CEPT chose eight bits for framing the European E1, although as feared the extra channel is sometimes appropriated for voice or data.
Higher T.
1970s Bell Labs developed higher rate systems. T-1C with a more sophisticated modulation scheme carried 3 Mbit/s, on those balanced pair cables that could support it. T-2 carried 6.312 Mbit/s, requiring a special low-capacitance cable with foam insulation. This was standard for Picturephone. T-4 and T-5 used coaxial cables, similar to the old L-carriers used by AT&T Long Lines. TD microwave radio relay systems were also fitted with high rate modems to allow them to carry a DS1 signal in a portion of their FM spectrum that had too poor quality for voice service. Later they carried DS3 and DS4 signals. During the 1980s companies such as RLH Industries, Inc. developed T1 over optical fiber. The industry soon developed and evolved with multiplexed T1 transmission schemes.
Digital signal crossconnect.
DS1 signals are interconnected typically at Central Office locations at a common metallic cross-connect point known as a 
DSX-1. When a DS1 is transported over metallic outside plant cable, the signal travels over conditioned cable pairs known as a T1 span. A T1 span can have up to +-130 Volts of DC power superimposed on the associated four wire cable pairs to line or "Span" power line repeaters, and T1 NIU's (T1 Smartjacks). T1 span repeaters are typically engineered up to 6000 ft apart, depending on cable gauge, and at no more than 36 dB of loss before requiring a repeated span. There can be no cable bridge taps or Load Coils across any pairs.
T1 copper spans are being replaced by optical transport systems, but if a copper (Metallic) span is used, the T1 is typically carried over an HDSL encoded copper line. Four wire HDSL does not require as many repeaters as conventional T1 spans. Newer two wire HDSL (HDSL-2) equipment transports a full 1.544 Mbit/s T1 over a single copper wire pair up to approximately twelve thousand (12,000) feet (3.5 km), if all 24 gauge cable is used. HDSL-2 does not employ multiple repeaters as does conventional four wire HDSL, or newer HDSL-4 systems.
One advantage of HDSL is its ability to operate with a limited number of bridge taps, with no tap being closer than 500 ft from any HDSL transceiver. Both two or four wire HDSL equipment transmits and receives over the same cable wire pair, as compared to conventional T1 service that utilizes individual cable pairs for transmit or receive.
DS3 signals are rare except within buildings, where they are used for interconnections and as an intermediate step before being muxed onto a SONET circuit. This is because a T3 circuit can only go about 600 ft between repeaters. A customer who orders a DS3 usually receives a SONET circuit run into the building and a multiplexer mounted in a utility box. The DS3 is delivered in its familiar form, two coax cables (1 for send and 1 for receive) with BNC connectors on the ends.
Sources:
Bit robbing.
Twelve DS1 frames make up a single T1 Superframe (T1 SF). Each T1 Superframe is composed of two signaling frames. All T1 DS0 channels that employ in-band signaling will have its eighth bit over written, or "robbed" from the full 64 kbit/s DS0 payload, by either a logical ZERO or ONE bit to signify a circuit signaling state or condition. Hence robbed bit signaling will restrict a DS0 channel to a rate of only 56 kbit/s during two of the twelve DS1 frames that make up a T1 SF framed circuit. T1 SF framed circuits yield two independent signaling channels (A&B) T1 ESF framed circuits four signaling frames in a twenty four frame extended frame format that yield four independent signaling channels (A, B, C, and D).
NOTE: 56 kbit/s DS0 channels are associated with digital data service (DDS) services typically do not utilize the eighth bit of the DS0 as voice circuits that employ A&B out of band signaling. One exception is Switched 56kbit/s DDS. In DDS, bit eight is used to identify DTE request to send (RTS) condition. With Switched 56 DDS, bit eight is pulsed (alternately set to logical ZERO and ONE) to transmit two state dial pulse signaling information between a SW56 DDS CSU/DSU, and a digital end office switch.
The use of robbed-bit signaling in North America has decreased significantly as a result of Signaling System No 7 (SS7) on inter-office dial trunks. With SS7, the full 64 kbit/s DS0 channel is available for use on a connection, and allows 64 kbit/s, and 128 kbit/s ISDN data calls to exist over a switched trunk network connection if the supporting T1 carrier entity is optioned B8ZS (Clear Channel Capable).
Sources:
Carrier pricing.
North America.
Carriers price DS1 lines in many different ways. However, most boil down to two simple components: local loop (the cost the local incumbent charges to transport the signal from the end user's central office, otherwise known as a CO, to the point of presence, otherwise known as a POP, of the carrier) and the port (the cost to access the telephone network or the Internet through the carrier's network). Typically, the port price is based upon access speed and yearly commitment level while the loop is based on geography. The farther the CO and POP, the more the loop costs.
The loop price has several components built into it, including the mileage calculation (performed in V/H coordinates, not standard GPS coordinates) and the telco piece. Each local Bell operating company—namely Verizon, AT&T Inc., and Qwest—charge T-carriers different price per mile rates. Therefore, the price calculation has two distance steps: geomapping and the determination of local price arrangements.
While most carriers utilize a geographic pricing model as described above, some Competitive Local Exchange Carriers (CLECs), such as TelePacific, Integra Telecom, tw telecom, Windstream, Level 3 Communications, and XO Communications offer national pricing.
Under this DS1 pricing model, a provider charges the same price in every geography it services. National pricing is an outgrowth of increased competition in the T-carrier market space and the commoditization of T-carrier products. Providers that have adopted a national pricing strategy may experience widely varying margins as their suppliers, the Bell operating companies (e.g., Verizon, AT&T Inc., and Qwest), maintain geographic pricing models, albeit at wholesale prices.
For voice DS1 lines, the calculation is mostly the same, except that the port (required for Internet access) is replaced by LDU (otherwise known as Long Distance Usage). Once the price of the loop is determined, only voice-related charges are added to the total. In short, the total price = loop + LDU x minutes used.
T-carrier and E-carrier systems comparison.
"Note 1:" The DS designations are used in connection with the North American hierarchy only. Strictly speaking, a DS1 is the data carried on a T1 circuit, and likewise for a DS3 and a T3, but in practice the terms are used interchangeably.
"Note 2:" There are other data rates in use, e.g., military systems that operate at six and eight times the DS1 rate. At least one manufacturer has a commercial system that operates at 90 Mbit/s, twice the DS3 rate. New systems, which take advantage of the high data rates offered by optical communications links, are also deployed or are under development. Higher data rates are now often achieved by using synchronous optical networking (SONET) or synchronous digital hierarchy (SDH).
"Note 3:" A DS3 is delivered native on a copper trunk. DS3 may be converted to an optical fiber run when needing longer distances between termination points. When a DS3 is delivered over fiber it is still an analog type trunk connection at the termination points. When delivering data over an OC3 or greater SONET is used. A DS3 transported over SONET is encapsulated in a STS-1 SONET channel. An OC-3 SONET link contains three STS-1s, and therefore may carry three DS3s. Likewise, OC-12, OC-48, and OC-192 may carry 12, 48, and 192 DS3s respectively.

</doc>
<doc id="41781" url="http://en.wikipedia.org/wiki?curid=41781" title="Technical control facility">
Technical control facility

In telecommunication, a technical control facility (TCF) is a telecommunications facility, or a designated and specially configured part thereof, that (a) contains the equipment necessary for ensuring fast, reliable, and secure exchange of information, (b) typically includes distribution frames and associated panels, jacks, and switches and monitoring, test, conditioning, and orderwire equipment, and (c) allows telecommunications systems control personnel to exercise operational control of communications paths and facilities, make quality analyses of communications and communications channels, monitor operations and maintenance functions, recognize and correct deteriorating conditions, restore disrupted communications, provide requested on-call circuits, and take or direct such actions as may be required and practical to provide effective telecommunications services. 

</doc>
<doc id="41782" url="http://en.wikipedia.org/wiki?curid=41782" title="Telecommunications service">
Telecommunications service

In telecommunication, a telecommunications service is a service provided by a telecommunications provider, or a specified set of user-information transfer capabilities provided to a group of users by a telecommunications system.
The telecommunications service user is responsible for the information content of the message. The telecommunications service provider has the responsibility for the acceptance, transmission, and delivery of the message.
For purposes of regulation by the Federal Communications Commission under the U.S. Communications Act of 1934 and Telecommunications Act of 1996, the definition of telecommunications service is "the offering of telecommunications for a fee directly to the public, or to such classes of users as to be effectively available directly to the public, regardless of the facilities used." "Telecommunications", in turn, is defined as "the transmission, between or among points specified by the user, of information of the user’s choosing, without change in the form or content of the information as sent and received." 

</doc>
<doc id="41783" url="http://en.wikipedia.org/wiki?curid=41783" title="Teleconference">
Teleconference

A teleconference or teleseminar is the live exchange and mass articulation of information among several persons and machines remote from one another but linked by a telecommunications system. Terms such as audio conferencing, telephone conferencing and phone conferencing are also sometimes used to refer to teleconferencing.
The telecommunications system may support the teleconference by providing one or more of the following: audio, video, and/or data services by one or more means, such as telephone, computer, telegraph, teletypewriter, radio, and television.
Internet teleconferencing.
Internet teleconferencing includes internet telephone conferencing, videoconferencing, web conferencing, and Augmented Reality conferencing.
Internet telephony involves conducting a teleconference over the Internet or a Wide Area Network. One key technology in this area is Voice over Internet Protocol (VOIP). Popular software for personal use includes Skype, Google Talk, Windows Live Messenger and Yahoo! Messenger.
A working example of an Augmented Reality conferencing was demonstrated at the Salone di Mobile in Milano by AR+RFID Lab. is another AR teleconferencing tool.
Software and service providers.
Notable vendors with articles:

</doc>
<doc id="41784" url="http://en.wikipedia.org/wiki?curid=41784" title="Teletraining">
Teletraining

Teletraining is training that 
"Synonyms"

</doc>
<doc id="41785" url="http://en.wikipedia.org/wiki?curid=41785" title="Terminal adapter">
Terminal adapter

ISDN.
In ISDN terminology, a terminal adapter or TA is a device that connects a "terminal" (computer) to the ISDN network.
The TA therefore fulfills a similar function to the ones a modem has on the POTS network, and is therefore sometimes called an ISDN modem. The latter term, however, is partially misleading as there is no modulation or demodulation performed.
There are devices on the market that combine the functions of an ISDN TA with those of a classical modem (with an ISDN line interface). These combined TA/modems permit connections from both ISDN and analog-line/modem counterparts. In addition, a TA may contain an interface and codec for one or more analog telephone lines (aka "a/b line"), allowing an existing POTS installation to be upgraded to ISDN without changing phones.
Terminal adapters typically connect to a basic rate interface (S0, sometimes also U0). On the "terminal" side, the most popular interfaces are RS-232 serial and USB; others like V.35 or RS-449 are only of historical interest.
Devices connecting ISDN to a network (e.g. Ethernet) commonly include routing functionality; while they technically include a TA function, they are referred to as (ISDN) routers.
Mobile Networks.
In Mobile networks, the "terminal adapter" is used by the Terminal equipment to access the Mobile termination, using AT commands (see Hayes command set).
In 2G (such as GSM or CDMA), the terminal adapter is a theoretically optional while in 3G (such as W-CDMA), the terminal adapter is mandatory and is part of the Mobile Termination.
Automation industry.
In the automation industry, a terminal adapter is a passive device that converts a connector like the 8P8C (RJ-45) modular connector or 9 pin D-Sub into a terminal block to facilitate wiring. It is often used when daisy-chain wiring is necessary on a multi-node serial communication network like RS-485 or RS-422.

</doc>
<doc id="41786" url="http://en.wikipedia.org/wiki?curid=41786" title="Terminal equipment">
Terminal equipment

In telecommunication, the term terminal equipment has the following meanings:
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41787" url="http://en.wikipedia.org/wiki?curid=41787" title="Ternary signal">
Ternary signal

In telecommunication, a ternary signal is a signal that can assume, at any given instant, one of three states or significant conditions, such as power level, phase position, pulse duration, or frequency.
Examples of ternary signals are (a) a pulse that can have a positive, zero, or negative voltage value at any given instant (PAM-3), (b) a sine wave that can assume phases of 0°, 120°, or 240° relative to a clock pulse (3-PSK), and (c) a carrier wave that can assume any one of three different frequencies depending on three different modulation signal significant conditions (3-FM).
Some examples of PAM-3 line codes that use ternary signals are:
3-PSK can be seen as falling between "binary phase-shift keying" (BPSK) which uses two phases, and "quadrature phase-shift keying" (QPSK) which uses four phases.

</doc>
<doc id="41789" url="http://en.wikipedia.org/wiki?curid=41789" title="Thermodynamic temperature">
Thermodynamic temperature

Thermodynamic temperature is the absolute measure of temperature and it is one of the principal parameters of thermodynamics.
Thermodynamic temperature is defined by the third law of thermodynamics in which the theoretically lowest temperature is the null or zero point. At this point, called absolute zero, the particle constituents of matter have minimal motion and can become no colder. In the quantum-mechanical description, matter at absolute zero is in its ground state, which is its state of lowest energy. Thermodynamic temperature is often also called absolute temperature, for two reasons: one, proposed by Kelvin, that it does not depend on the properties of a particular material; two that it refers to an absolute zero according to the properties of the ideal gas.
The International System of Units specifies a particular scale for thermodynamic temperature. It uses the Kelvin scale for measurement and selects the triple point of water at as the fundamental fixing point. Other scales have been in use historically. The Rankine scale, using the degree Fahrenheit as its unit interval, is still in use as part of the English Engineering Units in the United States in some engineering fields. ITS-90 gives a practical means of estimating the thermodynamic temperature to a very high degree of accuracy.
Roughly, the temperature of a body at rest is a measure of the mean of the energy of the translational, vibrational and rotational motions of matter's particle constituents, such as molecules, atoms, and subatomic particles. The full variety of these kinetic motions, along with potential energies of particles, and also occasionally certain other types of particle energy in equilibrium with these, make up the total internal energy of a substance. Internal energy is loosely called the heat energy or thermal energy in conditions when no work is done upon the substance by its surroundings, or by the substance upon the surroundings. Internal energy may be stored in a number of ways within a substance, each way constituting a "degree of freedom". At equilibrium, each degree of freedom will have on average the same energy: formula_1 where formula_2 is the Boltzmann constant, unless that degree of freedom is in the quantum regime. The internal degrees of freedom (rotation, vibration, etc.) may be in the quantum regime at room temperature, but the translational degrees of freedom will be in the classical regime except at extremely low temperatures (fractions of kelvins) and it may be said that, for most situations, the thermodynamic temperature is specified by the average translational kinetic energy of the particles.
Overview.
Temperature is a measure of the random submicroscopic motions and vibrations of the particle constituents of matter. These motions comprise the internal energy of a substance. More specifically, the thermodynamic temperature of any bulk quantity of matter is the measure of the average kinetic energy per classical (i.e., non-quantum) degree of freedom of its constituent particles. "Translational motions" are almost always in the classical regime. Translational motions are ordinary, whole-body movements in three-dimensional space in which particles move about and exchange energy in collisions. "Figure 1" below shows translational motion in gases; ' below shows translational motion in solids. Thermodynamic temperature's null point, absolute zero, is the temperature at which the particle constituents of matter are as close as possible to complete rest; that is, they have minimal motion, retaining only quantum mechanical motion. Zero kinetic energy remains in a substance at absolute zero (see "Thermal energy at absolute zero", below).
Throughout the scientific world where measurements are made in SI units, thermodynamic temperature is measured in kelvins (symbol: K). Many engineering fields in the U.S. however, measure thermodynamic temperature using the Rankine scale.
By , the unit "kelvin" and its scale are defined by two points: absolute zero, and the triple point of Vienna Standard Mean Ocean Water (water with a specified blend of hydrogen and oxygen isotopes). Absolute zero, the lowest possible temperature, is defined as being precisely 0 K "and" −273.15 °C. The triple point of water is defined as being precisely 273.16 K "and" 0.01 °C. This definition does three things:
Temperatures expressed in kelvins are converted to degrees Rankine simply by multiplying by 1.8 as follows: "T"°R = 1.8"T"K, where "T"K and "T"°R are temperatures in kelvin and degrees Rankine respectively. Temperatures expressed in degrees Rankine are converted to kelvins by "dividing" by 1.8 as follows: "T"K = "T"°R⁄1.8.
Practical realization.
Although the Kelvin and Celsius scales are defined using absolute zero (0 K) and the triple point of water (273.16 K and 0.01 °C), it is impractical to use this definition at temperatures that are very different from the triple point of water. ITS-90 is then designed to represent the thermodynamic temperature as closely as possible throughout its range. Many different thermometer designs are required to cover the entire range. These include helium vapor pressure thermometers, helium gas thermometers, standard platinum resistance thermometers (known as SPRTs, PRTs or Platinum RTDs) and monochromatic radiation thermometers.
For some types of thermometer the relationship between the property observed (e.g., length of a mercury column) and temperature, is close to linear, so for most purposes a linear scale is sufficient, without point-by-point calibration. For others a calibration curve or equation is required. The mercury thermometer, invented before the thermodynamic temperature was understood, originally "defined" the temperature scale; its linearity made readings correlate well with true temperature, i.e. the "mercury" temperature scale was a close fit to the true scale.
The relationship of temperature, motions, conduction, and thermal energy.
The nature of kinetic energy, translational motion, and temperature.
The thermodynamic temperature is a measure of the average energy of the translational, vibrational, and rotational motions of matter's particle constituents (molecules, atoms, and subatomic particles). The full variety of these kinetic motions, along with potential energies of particles, and also occasionally certain other types of particle energy in equilibrium with these, contribute the total internal energy (loosely, the thermal energy) of a substance. Thus, internal energy may be stored in a number of ways (degrees of freedom) within a substance. When the degrees of freedom are in the classical regime ("unfrozen") the temperature is very simply related to the average energy of those degrees of freedom at equilibrium. The three translational degrees of freedom are unfrozen except for the very lowest temperatures, and their kinetic energy is simply related to the thermodynamic temperature over the widest range. The heat capacity, which relates heat input and temperature change, is discussed below.
The relationship of kinetic energy, mass, and velocity is given by the formula "Ek" = 1⁄2"mv"2. Accordingly, particles with one unit of mass moving at one unit of velocity have precisely the same kinetic energy, and precisely the same temperature, as those with four times the mass but half the velocity.
Except in the quantum regime at extremely low temperatures, the thermodynamic temperature of any "bulk quantity" of a substance (a statistically significant quantity of particles) is directly proportional to the mean average kinetic energy of a specific kind of particle motion known as "translational motion." These simple movements in the three "x", "y", and "z"–axis dimensions of space means the particles move in the three spatial "degrees of freedom." The temperature derived from this translational kinetic energy is sometimes referred to as "kinetic temperature" and is equal to the thermodynamic temperature over a very wide range of temperatures. Since there are three translational degrees of freedom (e.g., motion along the x, y, and z axes), the translational kinetic energy is related to the kinetic temperature by:
where:
While the Boltzmann constant is useful for finding the mean kinetic energy of a particle, it's important to note that even when a substance is isolated and in thermodynamic equilibrium (all parts are at a uniform temperature and no heat is going into or out of it), the translational motions of individual atoms and molecules occur across a wide range of speeds (see animation in "Figure 1" above). At any one instant, the proportion of particles moving at a given speed within this range is determined by probability as described by the Maxwell–Boltzmann distribution. The graph shown here in "Fig. 2 " shows the speed distribution of 5500 K helium atoms. They have a "most probable" speed of 4.780 km/s. However, a certain proportion of atoms at any given instant are moving faster while others are moving relatively slowly; some are momentarily at a virtual standstill (off the "x"–axis to the right). This graph uses "inverse speed" for its "x"–axis so the shape of the curve can easily be compared to the curves in "" below. In both graphs, zero on the "x"–axis represents infinite temperature. Additionally, the "x" and "y"–axis on both graphs are scaled proportionally.
The high speeds of translational motion.
Although very specialized laboratory equipment is required to directly detect translational motions, the resultant collisions by atoms or molecules with small particles suspended in a fluid produces Brownian motion that can be seen with an ordinary microscope. The translational motions of elementary particles are "very" fast and temperatures close to absolute zero are required to directly observe them. For instance, when scientists at the NIST achieved a record-setting cold temperature of 700 nK (billionths of a kelvin) in 1994, they used optical lattice laser equipment to adiabatically cool caesium atoms. They then turned off the entrapment lasers and directly measured atom velocities of 7 mm per second in order to calculate their temperature.  Formulas for calculating the velocity and speed of translational motion are given in the following footnote.
The internal motions of molecules and specific heat.
There are other forms of internal energy besides the kinetic energy of translational motion. As can be seen in the animation at right, molecules are complex objects; they are a population of atoms and thermal agitation can strain their internal chemical bonds in three different ways: via rotation, bond length, and bond angle movements. These are all types of "internal degrees of freedom". This makes molecules distinct from "monatomic" substances (consisting of individual atoms) like the noble gases helium and argon, which have only the three translational degrees of freedom. Kinetic energy is stored in molecules' internal degrees of freedom, which gives them an "internal temperature". Even though these motions are called "internal", the external portions of molecules still move—rather like the jiggling of a stationary water balloon. This permits the two-way exchange of kinetic energy between internal motions and translational motions with each molecular collision. Accordingly, as energy is removed from molecules, both their kinetic temperature (the temperature derived from the kinetic energy of translational motion) and their internal temperature simultaneously diminish in equal proportions. This phenomenon is described by the equipartition theorem, which states that for any bulk quantity of a substance in equilibrium, the kinetic energy of particle motion is evenly distributed among all the active (i.e. unfrozen) degrees of freedom available to the particles. Since the internal temperature of molecules are usually equal to their kinetic temperature, the distinction is usually of interest only in the detailed study of non-local thermodynamic equilibrium (LTE) phenomena such as combustion, the sublimation of solids, and the diffusion of hot gases in a partial vacuum.
The kinetic energy stored internally in molecules causes substances to contain more internal energy at any given temperature and to absorb additional internal energy for a given temperature increase. This is because any kinetic energy that is, at a given instant, bound in internal motions is not at that same instant contributing to the molecules' translational motions. This extra thermal energy simply increases the amount of energy a substance absorbs for a given temperature rise. This property is known as a substance's specific heat capacity.
Different molecules absorb different amounts of thermal energy for each incremental increase in temperature; that is, they have different specific heat capacities. High specific heat capacity arises, in part, because certain substances' molecules possess more internal degrees of freedom than others do. For instance, nitrogen, which is a diatomic molecule, has "five" active degrees of freedom at room temperature: the three comprising translational motion plus two rotational degrees of freedom internally. Since the two internal degrees of freedom are essentially unfrozen, in accordance with the equipartition theorem, nitrogen has five-thirds the specific heat capacity per mole (a specific number of molecules) as do the monatomic gases. Another example is gasoline (see table showing its specific heat capacity). Gasoline can absorb a large amount of thermal energy per mole with only a modest temperature change because each molecule comprises an average of 21 atoms and therefore has many internal degrees of freedom. Even larger, more complex molecules can have dozens of internal degrees of freedom.
The diffusion of thermal energy: Entropy, phonons, and mobile conduction electrons.
"Heat conduction "is the diffusion of thermal energy from hot parts of a system to cold. A system can be either a single bulk entity or a plurality of discrete bulk entities. The term "bulk" in this context means a statistically significant quantity of particles (which can be a microscopic amount). Whenever thermal energy diffuses within an isolated system, temperature differences within the system decrease (and entropy increases).
One particular heat conduction mechanism occurs when translational motion, the particle motion underlying temperature, transfers momentum from particle to particle in collisions. In gases, these translational motions are of the nature shown above in "Fig. 1. "As can be seen in that animation, not only does momentum (heat) diffuse throughout the volume of the gas through serial collisions, but entire molecules or atoms can move forward into new territory, bringing their kinetic energy with them. Consequently, temperature differences equalize throughout gases very quickly—especially for light atoms or molecules; convection speeds this process even more.
Translational motion in "solids", however, takes the form of "phonons "(see "Fig. 4" at right). Phonons are constrained, quantized wave packets that travel at a given substance's speed of sound. The manner in which phonons interact within a solid determines a variety of its properties, including its thermal conductivity. In electrically insulating solids, phonon-based heat conduction is "usually" inefficient and such solids are considered "thermal insulators" (such as glass, plastic, rubber, ceramic, and rock). This is because in solids, atoms and molecules are locked into place relative to their neighbors and are not free to roam.
Metals however, are not restricted to only phonon-based heat conduction. Thermal energy conducts through metals extraordinarily quickly because instead of direct molecule-to-molecule collisions, the vast majority of thermal energy is mediated via very light, mobile "conduction electrons." This is why there is a near-perfect correlation between metals' thermal conductivity and their electrical conductivity. Conduction electrons imbue metals with their extraordinary conductivity because they are "delocalized" (i.e., not tied to a specific atom) and behave rather like a sort of quantum gas due to the effects of "zero-point energy" (for more on ZPE, see "Note 1" below). Furthermore, electrons are relatively light with a rest mass only 1⁄1836 that of a proton. This is about the same ratio as a .22 Short bullet (29 grains or 1.88 g) compared to the rifle that shoots it. As Isaac Newton wrote with his third law of motion,
 Law #3: All forces occur in pairs, and these two forces are equal in magnitude and opposite in direction.
However, a bullet accelerates faster than a rifle given an equal force. Since kinetic energy increases as the square of velocity, nearly all the kinetic energy goes into the bullet, not the rifle, even though both experience the same force from the expanding propellant gases. In the same manner, because they are much less massive, thermal energy is readily borne by mobile conduction electrons. Additionally, because they're delocalized and "very" fast, kinetic thermal energy conducts extremely quickly through metals with abundant conduction electrons.
The diffusion of thermal energy: Black-body radiation.
Thermal radiation is a byproduct of the collisions arising from various vibrational motions of atoms. These collisions cause the electrons of the atoms to emit thermal photons (known as black-body radiation). Photons are emitted anytime an electric charge is accelerated (as happens when electron clouds of two atoms collide). Even "individual molecules" with internal temperatures greater than absolute zero also emit black-body radiation from their atoms. In any bulk quantity of a substance at equilibrium, black-body photons are emitted across a range of wavelengths in a spectrum that has a bell curve-like shape called a Planck curve (see graph in "Fig. 5" at right). The top of a Planck curve (the peak emittance wavelength) is located in a particular part of the electromagnetic spectrum depending on the temperature of the black-body. Substances at extreme cryogenic temperatures emit at long radio wavelengths whereas extremely hot temperatures produce short gamma rays (see "Table of common temperatures").
Black-body radiation diffuses thermal energy throughout a substance as the photons are absorbed by neighboring atoms, transferring momentum in the process. Black-body photons also easily escape from a substance and can be absorbed by the ambient environment; kinetic energy is lost in the process.
As established by the Stefan–Boltzmann law, the intensity of black-body radiation increases as the fourth power of absolute temperature. Thus, a black-body at 824 K (just short of glowing dull red) emits "60 times" the radiant power as it does at 296 K (room temperature). This is why one can so easily feel the radiant heat from hot objects at a distance. At higher temperatures, such as those found in an incandescent lamp, black-body radiation can be the principal mechanism by which thermal energy escapes a system.
Table of thermodynamic temperatures.
The full range of the thermodynamic temperature scale, from absolute zero to absolute hot, and some notable points between them are shown in the table below.
A The 2500 K value is approximate.<br>
B For a true blackbody (which tungsten filaments are not). Tungsten filaments’ emissivity is greater at shorter wavelengths, which makes them appear whiter.<br>
C Effective photosphere temperature.<br>
D For a true blackbody (which the plasma was not). The Z machine’s dominant emission originated from 40 MK electrons (soft x–ray emissions) within the plasma.</small>
The heat of phase changes.
The kinetic energy of particle motion is just one contributor to the total thermal energy in a substance; another is "phase transitions", which are the potential energy of molecular bonds that can form in a substance as it cools (such as during condensing and freezing). The thermal energy required for a phase transition is called "latent heat." This phenomenon may more easily be grasped by considering it in the reverse direction: latent heat is the energy required to "break" chemical bonds (such as during evaporation and melting). Almost everyone is familiar with the effects of phase transitions; for instance, steam at 100 °C can cause severe burns much faster than the 100 °C air from a hair dryer. This occurs because a large amount of latent heat is liberated as steam condenses into liquid water on the skin.
Even though thermal energy is liberated or absorbed during phase transitions, pure chemical elements, compounds, and eutectic alloys "exhibit no temperature change whatsoever" while they undergo them (see "Fig. 7," below right). Consider one particular type of phase transition: melting. When a solid is melting, crystal lattice chemical bonds are being broken apart; the substance is transitioning from what is known as a "more ordered state" to a "less ordered state". In "Fig. 7, "the melting of ice is shown within the lower left box heading from blue to green.
At one specific thermodynamic point, the melting point (which is 0 °C across a wide pressure range in the case of water), all the atoms or molecules are, on average, at the maximum energy threshold their chemical bonds can withstand without breaking away from the lattice. Chemical bonds are all-or-nothing forces: they either hold fast, or break; there is no in-between state. Consequently, when a substance is at its melting point, every joule of added thermal energy only breaks the bonds of a specific quantity of its atoms or molecules, converting them into a liquid of precisely the same temperature; no kinetic energy is added to translational motion (which is what gives substances their temperature). The effect is rather like popcorn: at a certain temperature, additional thermal energy can't make the kernels any hotter until the transition (popping) is complete. If the process is reversed (as in the freezing of a liquid), thermal energy must be removed from a substance.
As stated above, the thermal energy required for a phase transition is called "latent heat." In the specific cases of melting and freezing, it's called "enthalpy of fusion" or "heat of fusion." If the molecular bonds in a crystal lattice are strong, the heat of fusion can be relatively great, typically in the range of 6 to 30 kJ per mole for water and most of the metallic elements. If the substance is one of the monatomic gases, (which have little tendency to form molecular bonds) the heat of fusion is more modest, ranging from 0.021 to 2.3 kJ per mole. Relatively speaking, phase transitions can be truly energetic events. To completely melt ice at 0 °C into water at 0 °C, one must add roughly 80 times the thermal energy as is required to increase the temperature of the same mass of liquid water by one degree Celsius. The metals' ratios are even greater, typically in the range of 400 to 1200 times. And the phase transition of boiling is much more energetic than freezing. For instance, the energy required to completely boil or vaporize water (what is known as "enthalpy of vaporization") is roughly "540 times" that required for a one-degree increase.
Water's sizable enthalpy of vaporization is why one's skin can be burned so quickly as steam condenses on it (heading from red to green in "Fig. 7 "above). In the opposite direction, this is why one's skin feels cool as liquid water on it evaporates (a process that occurs at a sub-ambient wet-bulb temperature that is dependent on relative humidity). Water's highly energetic enthalpy of vaporization is also an important factor underlying why "solar pool covers" (floating, insulated blankets that cover swimming pools when not in use) are so effective at reducing heating costs: they prevent evaporation. For instance, the evaporation of just 20 mm of water from a 1.29-meter-deep pool chills its water 8.4 degrees Celsius (15.1 °F).
Internal energy.
The total energy of all particle motion translational and internal, including that of conduction electrons, plus the potential energy of phase changes, plus zero-point energy comprise the "internal energy" of a substance.
Internal energy at absolute zero.
As a substance cools, different forms of internal energy and their related effects simultaneously decrease in magnitude: the latent heat of available phase transitions is liberated as a substance changes from a less ordered state to a more ordered state; the translational motions of atoms and molecules diminish (their kinetic temperature decreases); the internal motions of molecules diminish (their internal temperature decreases); conduction electrons (if the substance is an electrical conductor) travel "somewhat" slower; and black-body radiation's peak emittance wavelength increases (the photons' energy decreases). When the particles of a substance are as close as possible to complete rest and retain only ZPE-induced quantum mechanical motion, the substance is at the temperature of absolute zero ("T"=0).
Note that whereas absolute zero is the point of zero thermodynamic temperature and is also the point at which the particle constituents of matter have minimal motion, absolute zero is not necessarily the point at which a substance contains zero thermal energy; one must be very precise with what one means by "internal energy". Often, all the phase changes that "can" occur in a substance, "will" have occurred by the time it reaches absolute zero. However, this is not always the case. Notably, "T"=0 helium remains liquid at room pressure and must be under a pressure of at least 25 bar to crystallize. This is because helium's heat of fusion (the energy required to melt helium ice) is so low (only 21 joules per mole) that the motion-inducing effect of zero-point energy is sufficient to prevent it from freezing at lower pressures. Only if under at least 25 bar of pressure will this latent thermal energy be liberated as helium freezes while approaching absolute zero. A further complication is that many solids change their crystal structure to more compact arrangements at extremely high pressures (up to millions of bars, or hundreds of gigapascals). These are known as "solid-solid phase transitions" wherein latent heat is liberated as a crystal lattice changes to a more thermodynamically favorable, compact one.
The above complexities make for rather cumbersome blanket statements regarding the internal energy in "T"=0 substances. Regardless of pressure though, what "can" be said is that at absolute zero, all solids with a lowest-energy crystal lattice such those with a "closest-packed arrangement" (see "Fig. 8," above left) contain minimal internal energy, retaining only that due to the ever-present background of zero-point energy.  One can also say that for a given substance at constant pressure, absolute zero is the point of lowest "enthalpy" (a measure of work potential that takes internal energy, pressure, and volume into consideration). Lastly, it is always true to say that all "T"=0 substances contain zero kinetic thermal energy. 
Practical applications for thermodynamic temperature.
Thermodynamic temperature is useful not only for scientists, it can also be useful for lay-people in many disciplines involving gases. By expressing variables in absolute terms and applying Gay–Lussac's law of temperature/pressure proportionality, solutions to everyday problems are straightforward; for instance, calculating how a temperature change affects the pressure inside an automobile tire. If the tire has a relatively cold pressure of 200 kPa-gage , then in absolute terms (relative to a vacuum), its pressure is 300 kPa-absolute.   Room temperature ("cold" in tire terms) is 296 K. If the tire pressure is 20 °C hotter (20 kelvins), the solution is calculated as 316 K⁄296 K = 6.8% greater thermodynamic temperature "and" absolute pressure; that is, a pressure of 320 kPa-absolute, which is 220 kPa-gage.
Definition of thermodynamic temperature.
The thermodynamic temperature is defined by the second law of thermodynamics and its consequences. The thermodynamic temperature can be shown to have special properties, and in particular can be seen to be uniquely defined (up to some constant multiplicative factor) by considering the efficiency of idealized heat engines. Thus the "ratio" "T"2/"T"1 of two temperatures"T"1 and"T"2 is the same in all absolute scales.
Strictly speaking, the temperature of a system is well-defined only if it is at thermal equilibrium. From a microscopic viewpoint, a material is at thermal equilibrium if the quantity of heat between its individual particles cancel out. There are many possible scales of temperature, derived from a variety of observations of physical phenomena.
Loosely stated, temperature differences dictate the direction of heat between two systems such that their combined energy is maximally distributed among their lowest possible states. We call this distribution "entropy". To better understand the relationship between temperature and entropy, consider the relationship between heat, work and temperature illustrated in the Carnot heat engine. The engine converts heat into work by directing a temperature gradient between a higher temperature heat source, "T"H, and a lower temperature heat sync, "T"C, through a gas filled piston. The work done per cycle is equal to the difference between the heat supplied to the engine by "T"H, "q"H, and the heat supplied to "T"C by the engine, "q"C. The efficiency of the engine is the work divided by the heat put into the system or
where wcy is the work done per cycle. Thus the efficiency depends only on qC/qH.
Carnot's theorem states that all reversible engines operating between the same heat reservoirs are equally efficient.
Thus, any reversible heat engine operating between temperatures "T"1 and "T"2 must have the same efficiency, that is to say, the efficiency is the function of only temperatures
In addition, a reversible heat engine operating between temperatures "T"1 and "T"3 must have the same efficiency as one consisting of two cycles, one between "T"1 and another (intermediate) temperature "T"2, and the second between "T"2 and"T"3. If this were not the case, then energy (in the form of "Q") will be wasted or gained, resulting in different overall efficiencies every time a cycle is split into component cycles; clearly a cycle can be composed of any number of smaller cycles.
With this understanding of "Q"1, "Q"2 and "Q"3, we note also that mathematically,
But the first function is "NOT" a function of "T"2, therefore the product of the final two functions "MUST" result in the removal of "T"2 as a variable. The only way is therefore to define the function f as follows:
and
so that
i.e. The ratio of heat exchanged is a function of the respective temperatures at which they occur. We can choose any monotonic function for our formula_12; it is a matter of convenience and convention that we choose formula_13. Choosing then "one" fixed reference temperature (i.e. triple point of water), we establish the thermodynamic temperature scale.
It is to be noted that such a definition coincides with that of the ideal gas derivation; also it is this "definition" of the thermodynamic temperature that enables us to represent the Carnot efficiency in terms of "T"H and "T"C, and hence derive that the (complete) Carnot cycle is isentropic:
Substituting this back into our first formula for efficiency yields a relationship in terms of temperature:
Notice that for "T"C=0 the efficiency is 100% and that efficiency becomes greater than 100% for "T"C<0, which cases are unrealistic. Subtracting the right hand side of Equation 4 from the middle portion and rearranging gives
where the negative sign indicates heat ejected from the system. The generalization of this equation is Clausius theorem, which suggests the existence of a state function "S" (i.e., a function which depends only on the state of the system, not on how it reached that state) defined (up to an additive constant) by
where the subscript indicates heat transfer in a reversible process. The function "S" corresponds to the entropy of the system, mentioned previously, and the change of "S" around any cycle is zero (as is necessary for any state function). Equation 5 can be rearranged to get an alternative definition for temperature in terms of entropy and heat (to avoid logic loop, we should first define entropy through statistical mechanics):
For a system in which the entropy "S" is a function "S"("E") of its energy "E", the thermodynamic temperature "T" is therefore given by
so that the reciprocal of the thermodynamic temperature is the rate of increase of entropy with energy.

</doc>
<doc id="41790" url="http://en.wikipedia.org/wiki?curid=41790" title="Third-order intercept point">
Third-order intercept point

In telecommunications, a third-order intercept point (IP3 or TOI) is a measure for weakly nonlinear systems and devices, for example receivers, linear amplifiers and mixers. It is based on the idea that the device nonlinearity can be modeled using a low-order polynomial, derived by means of Taylor series expansion. The third-order intercept point relates nonlinear products caused by the third-order nonlinear term to the linearly amplified signal, in contrast to the second-order intercept point that uses second order terms.
The intercept point is a purely mathematical concept, and does not correspond to a practically occurring physical power level. In many cases, it lies far beyond the damage threshold of the device.
Definitions.
Two different definitions for intercept points are in use:
It is worth noticing that these definitions differ by 4.8 dB (10 log10 3), so care should be taken when using existing equations, models or measurement data.
The intercept point is obtained graphically by plotting the output power versus the input power both on logarithmic scales (e.g., decibels). Two curves are drawn; one for the linearly amplified signal at an input tone frequency, one for a nonlinear product.
On a logarithmic scale, the function "xn" translates into a straight line with slope of "n". Therefore, the linearly amplified signal will exhibit a slope of 1. A third-order nonlinear product will increase by 3 dB in power when the input power is raised by 1 dB.
Both curves are extended with straight lines of slope 1 and n (3 for a third-order intercept point). The point where the curves intersect is the intercept point. It can be read off from the input or output power axis, leading to input or output intercept point, respectively (IIP3/OIP3).
Input and output intercept point differ by the small-signal gain of the device.
Practical considerations.
The concept of intercept point is based on the assumption of a weakly nonlinear system, meaning that higher-order nonlinear terms are small enough to be negligible.
In practice, the weakly nonlinear assumption may not hold for the upper end of the input power range, be it during measurement or during use of the amplifier. As a consequence, measured or simulated data will deviate from the ideal slope of "n".
The intercept point according to its basic definition should be determined by drawing the straight lines with slope 1 and n through the measured data at the smallest possible power level (possibly limited towards lower power levels by instrument or device noise).
It is a frequent mistake to derive intercept points by either changing the slope of the straight lines, or fitting them to points measured at too high a power level. In certain situations such a measure can be useful, but it is not an intercept point according to definition. Its value depends on the measurement conditions that need to be documented, whereas the IP according to definition is mostly unambiguous; although there is some dependency on frequency and tone spacing, depending on the physics of the device-under-test.
One of the useful applications of third order intercept point is as a rule-of-thumb measure to estimate nonlinear products. It can be seen that the spacing between two straight lines with slopes of 3 and 1 closes with slope 2.
For example, assume a device with an input-referred third-order intercept point of 10 dBm is driven with a test signal of −5 dBm. This power is 15 dB below the intercept point, therefore nonlinear products will appear at approximately 2x15 dB below the test signal power at the device output (in other words, 3×15 dB below the output-referred third-order intercept point).
A rule-of-thumb that holds for many linear radio frequency amplifiers is that the 1 dB compression point falls approximately 10 dB below the third-order intercept point.
Theory.
The third-order intercept point (TOI) is a property of the device transfer function "O" (see diagram).
This transfer function relates the output signal voltage level to the input signal voltage level. We assume a “linear” device having a transfer function whose small signal form may be expressed in terms of a power series containing only odd terms, making the transfer function an odd function of input signal voltage, i.e., "O"[−"s"("t")] = −"O"["s"("t")]. Where the signals passing through the actual device are modulated sinusoidal voltage waveforms (e.g., RF amplifier), device nonlinearities can be expressed in terms of how they affect individual sinusoidal signal components. For example, say the input voltage signal is the sine wave
and the device transfer function produces an output of the form
where "G" is the amplifier gain and "D"3 is cubic distortion. We may substitute the first equation into the second and, using the trigonometric identity
we obtain the device output voltage waveform as
The output waveform contains the original waveform, cos("ωt"), plus a new harmonic term, cos(3"ωt"), the "third-order". The coefficient of the cos("ωt") harmonic has two terms, one that varies linearly with "V" and one that varies with the cube of "V". In fact, the coefficient of cos("ωt") has nearly the same form as the transfer function, except for the factor ¾ on the cubic term. In other words, as signal level "V" is increased, the level of the cos("ωt") term in the output eventually levels off, similar to how the transfer function levels off. Of course, the coefficients of the higher-order harmonics will increase (with increasing "V") as the coefficient of the cos("ωt") term levels off (the power has to go somewhere).
If we now restrict our attention to that portion of the cos("ωt") coefficient which varies linearly with "V", and then ask ourselves, at what input voltage level, "V", will the coefficients of the first and third order terms have equal magnitudes (i.e., where the magnitudes intersect), we find that this happens when
which is the Third-Order Intercept Point (TOI). So, we see that the TOI input power level is simply 4/3 times the ratio of the gain and the cubic distortion term in the device transfer function. The smaller the cubic term is in relation to the gain, the more linear the device is and the higher the TOI is, which clearly makes sense. The TOI, being related to the magnitude squared of the input voltage waveform, is a power quantity, typically measured in milliwatts (mW). The TOI is always beyond operational power levels because the output power saturates before reaching this level.
The TOI is closely related to the amplifier's "1 dB compression point," which is defined as that point at which the "total" coefficient of the cos("ωt") term is 1 dB below the "linear portion" of that coefficient. We can relate the 1 dB compression point to the TOI as follows. Since 1 dB = 20 log10 1.122, we may say, in a voltage sense, that the 1dB compression point occurs when
or
or
In a power sense ("V"2 is a power quantity), a factor of 0.10875 corresponds to −9.636 dB, so by this approximate analysis, the 1 dB compression point occurs roughly 9.6 dB below the TOI.
"Recall:" decibel figure = 10 dB × log10(power ratio) = 20 dB × log10(voltage ratio).

</doc>
<doc id="41791" url="http://en.wikipedia.org/wiki?curid=41791" title="Threshold">
Threshold

Threshold may refer to: 

</doc>
<doc id="41792" url="http://en.wikipedia.org/wiki?curid=41792" title="Time-assignment speech interpolation">
Time-assignment speech interpolation

In telecommunication, a time-assignment speech interpolation (TASI) was an analog technique used on certain long transmission links to increase voice-transmission capacity.
TASI takes advantage of the fact that in typical person-person conversation, speech in a single direction occurs for approximately 40% of the time, the remaining time being occupied with pauses and/or silence. Statistical analysis demonstrated that for an average voice channel usage of 40%, over 74 speech conversations could be handled using 37 full Duplex speech circuits thereby doubling potential revenue for a small capital outlay relative to a highly expensive cable. e.g. £12.5 million (£263 million as of 2014) cost of the TAT-1 cable on which TASI was implemented.
TASI worked by switching additional users onto any channel temporarily idled because an original user has stopped speaking. When the original user resumes speaking, that user would, in turn, be switched to any channel that happened to be idle. The speech detector function is called voice activity detection. Clipping or loss of speech would occur for all conversations that needed to be assigned to an available idle channel and in practice lasted at least 17ms whilst information required to re-connect both parties was signalled by the TASI control circuits. An additional freezeout period lasting between 0 and 500ms would depend on the instantaneous loading of voice circuits. In actual use, these delays presented few problems in typical conversations.
One of the issues with using this type of technology was that the users listening on an idled channel can sometimes hear the conversation that has been switched onto it. Generally the sound heard was of very low volume and individual words are not distinguishable. See also crosstalk for a similar phenomenon in telecommunications. Another potential issue was ensuring that non-voice type circuits were not routed via TASI speech channels since these could seriously degrade the level of service where callers would encounter frequent clipped speech and breaks in the conversation.
TASI was invented by Bell Labs in the early 1960s to increase the capacity of transatlantic telephone cables. It was one of their first applications requiring electronic switching of voice circuits.
Later Digital Circuit Multiplication Equipment included TASI as a feature, not as distinct hardware.

</doc>
<doc id="41793" url="http://en.wikipedia.org/wiki?curid=41793" title="Time code ambiguity">
Time code ambiguity

In telecommunication, time code ambiguity is the shortest interval between successive repetitions of the same time code value.
For example, in a time code in which year-of-century (the '72' in 10/04/72) is the most slowly changing field, the time code ambiguity would be 100 years; it is ambiguous whether this value refers to a date in 1872, 1972 or some other century. For a digital clock in which hours and minutes up to a maximum of 11:59 are displayed, the time code ambiguity would be 12 hours.
The Year 2000 problem is an example of the pitfalls of time code ambiguity. Very often dates are now recorded with 4 digit years (10/04/1972). Assuming that the use of a 4-digit year field would continue, even in the far future, this would change the time code ambiguity from 100 years to 10 000 years.

</doc>
<doc id="41795" url="http://en.wikipedia.org/wiki?curid=41795" title="Minimum spanning tree">
Minimum spanning tree

Given a connected, undirected graph, a spanning tree of that graph is a subgraph that is a tree and connects all the vertices together. A single graph can have many different spanning trees. We can also assign a "weight" to each edge, which is a number representing how unfavorable it is, and use this to assign a weight to a spanning tree by computing the sum of the weights of the edges in that spanning tree. A minimum spanning tree (MST) or minimum weight spanning tree is then a spanning tree with weight less than or equal to the weight of every other spanning tree. More generally, any undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of minimum spanning trees for its connected components.
One example would be a telecommunications company laying cable to a new neighborhood. If it is constrained to bury the cable only along certain paths (e.g. along roads), then there would be a graph representing which points are connected by those paths. Some of those paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. Currency is an acceptable unit for edge weight — there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. A "spanning tree" for that graph would be a subset of those paths that has no cycles but still connects to every house; there might be several spanning trees possible. A "minimum spanning tree" would be one with the lowest total cost, thus would represent the least expensive path for laying the cable.
Properties.
Possible multiplicity.
"There may be several minimum spanning trees of the same weight having a minimum number of edges"; in particular, if all the edge weights of a given graph are the same, then every spanning tree of that graph is minimum.
If there are "n" vertices in the graph, then each spanning tree has "n"-1 edges.
Uniqueness.
"If each edge has a distinct weight then there will be only one, unique minimum spanning tree". This is true in many realistic situations, such as the telecommunications company example above, where it's unlikely any two paths have "exactly" the same cost. This generalizes to spanning forests as well.
If the edge weights are not unique, only the (multi-)set of weights in minimum spanning trees is unique, that is the same for all minimum spanning trees.
Proof:
Minimum-cost subgraph.
If the weights are "positive", then a minimum spanning tree is in fact a minimum-cost subgraph connecting all vertices, since subgraphs containing cycles necessarily have more total weight.
Cycle property.
"For any cycle "C" in the graph, if the weight of an edge "e" of "C" is larger than the individual weights of all other edges of "C", then this edge cannot belong to a MST."
Proof: Assume the contrary, i.e. that "e" belongs to an MST T1. Then deleting "e" will break T1 into two subtrees with the two ends of "e" in different subtrees. The remainder of "C" reconnects the subtrees, hence there is an edge "f" of "C" with ends in different subtrees, i.e., it reconnects the subtrees into a tree T2 with weight less than that of T1, because the weight of "f" is less than the weight of "e".
Cut property.
"For any cut "C" in the graph, if the weight of an edge "e" of "C" is strictly smaller than the weights of all other edges of "C", then this edge belongs to all MSTs of the graph."
Proof: assume the contrary, i.e., in the figure at right, make edge BC (weight 6) part of the MST T instead of edge e (weight 4). Adding e to T will produce a cycle, while replacing BC with e would produce MST of smaller weight. Thus, a tree containing BC is not a MST, a contradiction that violates our assumption. By a similar argument, if more than one edge is of minimum weight across a cut, then each such edge is contained in a minimum spanning tree.
Minimum-cost edge.
"If the edge of a graph with the minimum cost "e" is unique, then this edge is included in any MST."
Proof: if "e" was not included in the MST, removing any of the (larger cost) edges in the cycle formed after adding "e" to the MST, would yield a spanning tree of smaller weight.
Contraction.
"If T is a tree of MST edges, then we can "contract" T into a single vertex while maintaining the invariant that the MSF of the contracted graph plus T gives the MST for the graph before contraction.
Algorithms.
In all of the algorithms below, "m" is the number of edges in the graph and "n" is the number of vertices.
Classic algorithms.
The first algorithm for finding a minimum spanning tree was developed by Czech scientist Otakar Borůvka in 1926 (see Borůvka's algorithm). Its purpose was an efficient electrical coverage of Moravia. The algorithm proceeds in a sequence of stages. In each stage, called "Boruvka step", it identifies a forest "F" consisting of the minimum-weight edge incident to each vertex in the graph "G", then forms the graph G1=G\F as the input to the next step. Here G\F denotes the graph derived from G by contracting edges in F (by the Cut property, these edges belong to the MST). Each Boruvka step takes linear time. Since the number of vertices is reduced by at least half in each step, Boruvka's algorithm takes O("m" log "n") time.
A second algorithm is Prim's algorithm, which was invented by Jarnik in 1930 and rediscovered by Prim in 1957 and Dijkstra in 1959. Basically, it grows the MST (T) one edge at a time. Initially, T contains an arbitrary vertex. In each step, T is augmented with the least-weight edge (x,y) such that x is in T and y is not yet in T. By the Cut property, all edges added to T are in the MST. Its run-time is either O("m" log "n") or O("m" + "n" log "n"), depending on the data-structures used.
A third algorithm commonly in use is the Kruskal's algorithm, which also takes O("m" log "n") time.
A fourth algorithm, not as commonly used, is the reverse-delete algorithm, which is the reverse of Kruskal's algorithm. Its runtime is O("m" log "n" (log log "n")3).
All these four are greedy algorithms. Since they run in polynomial time, the problem of finding such trees is in FP, and related decision problems such as determining whether a particular edge is in the MST or determining if the minimum total weight exceeds a certain value are in P.
Faster algorithms.
Several researchers have tried to find more computationally-efficient algorithms.
In a comparison model, in which the only allowed operations on edge weights are pairwise comparisons, found a linear time randomized algorithm based on a combination of Borůvka's algorithm and the reverse-delete algorithm.
The fastest non-randomized comparison-based algorithm with known complexity, by Bernard Chazelle, is based on the soft heap, an approximate priority queue. Its running time is "O"("m" α("m","n")), where α is the classical functional inverse of the Ackermann function. The function α grows extremely slowly, so that for all practical purposes it may be considered a constant no greater than 4; thus Chazelle's algorithm takes very close to linear time.
Linear-time algorithms in special cases.
Dense graphs.
If the graph is dense (i.e. "m"/"n" ≥ log log log "n"), then a deterministic algorithm by Fredman and Tarjan finds the MST in time O("m"). The algorithm executes a number of phases. Each phase executes Prim's algorithm many times, each for a limited number of steps. The run-time of each phase is O("m"+"n"). If the number of vertices before a phase is formula_2, the number of vertices remaining after a phase is at most formula_3. Hence, at most formula_4 phases are needed, which gives a linear run-time for dense graphs.
There are other algorithms that work in linear time on dense graphs.
Integer weights.
If the edge weights are integers represented in binary, then deterministic algorithms are known that solve the problem in "O"("m" + "n") integer operations.
Whether the problem can be solved "deterministically" for a "general graph" in "linear time" by a comparison-based algorithm remains an open question.
Decision trees.
Given graph "G" where the nodes and edges are fixed but the weights are unknown, it is possible to construct a binary decision tree (DT) for calculating the MST for any permutation of weights. Each internal node of the DT contains a comparison between two edges, e.g. "Is the weight of the edge between "x" and "y" larger than the weight of the edge between "w" and "z"?". The two children of the node correspond to the two possible answers "yes" or "no". In each leaf of the DT, there is a list of edges from "G" that correspond to an MST. The runtime complexity of a DT is the largest number of queries required to find the MST, which is just the depth of the DT. A DT for a graph "G" is called "optimal" if it has the smallest depth of all correct DTs for "G".
For every integer "r", it is possible to find optimal decision trees for all graphs on "r" vertices by brute-force search. This search proceeds in two steps.
A. Generating all potential DTs
B. Identifying the correct DTs
To check if a DT is correct, it should be checked on all possible permutations of the edge weights.
Hence, the total time required for finding an optimal DT for "all" graphs with "r" vertices is: formula_14, which is less than: formula_15.
Optimal algorithm.
Seth Pettie and Vijaya Ramachandran have found a provably optimal deterministic comparison-based minimum spanning tree algorithm. The following is a simplified description of the algorithm.
The runtime of all steps in the algorithm is O("m"), "except for the step of using the decision trees". We don't know the runtime of this step, but we know that it is optimal - no algorithm can do better than the optimal decision tree.
Thus, this algorithm has the peculiar property that it is "provably optimal" although its runtime complexity is "unknown".
Parallel and distributed algorithms.
Research has also considered parallel algorithms for the minimum spanning tree problem.
With a linear number of processors it is possible to solve the problem in formula_17 time.
 demonstrate an algorithm that can compute MSTs 5 times faster on 8 processors than an optimized sequential algorithm.
Other specialized algorithms have been designed for computing minimum spanning trees of a graph so large that most of it must be stored on disk at all times. These "external storage" algorithms, for example as described in "Engineering an External Memory Minimum Spanning Tree Algorithm" by Roman, Dementiev et al., can operate, by authors' claims, as little as 2 to 5 times slower than a traditional in-memory algorithm. They rely on efficient external storage sorting algorithms and on graph contraction techniques for reducing the graph's size efficiently.
The problem can also be approached in a distributed manner. If each node is considered a computer and no node knows anything except its own connected links, one can still calculate the distributed minimum spanning tree.
MST on complete graphs.
Alan M. Frieze showed that given a complete graph on "n" vertices, with edge weights that are independent identically distributed random variables with distribution function formula_18 satisfying formula_19, then as "n" approaches +∞ the expected weight of the MST approaches formula_20, where formula_21 is the Riemann zeta function. Frieze and Steele also proved convergence in probability. Svante Janson proved a central limit theorem for weight of the MST.
For uniform random weights in formula_22, the exact expected size of the minimum spanning tree has been computed for small complete graphs.
Applications.
Minimum spanning trees have direct applications in the design of networks, including computer networks, telecommunications networks, transportation networks, water supply networks, and electrical grids (which they were first invented for, as mentioned above). They are invoked as subroutines in algorithms for other problems, including the Christofides algorithm for approximating the traveling salesman problem, approximating the multi-terminal minimum cut problem (which is equivalent in the single-terminal case to the maximum flow problem),
and approximating the minimum-cost weighted perfect matching.
Other practical applications based on minimal spanning trees include:
In pedagogical contexts, minimum spanning tree algorithms serve as a common introductory example of both graph algorithms and greedy algorithms due to their simplicity.
Financial Markets.
Minimum spanning trees can also be used to describe financial markets. A correlation matrix can be created by calculating a coefficient of correlation between any two stocks. The element of the "i"th row and "j"th column of the correlation matrix is defined as
formula_23
where formula_24 and formula_25 are the averaged return values of two stocks over a period of time. This matrix can be represented topologically as a complex network and a minimum spanning tree can be constructed to visualize relationships.
Related problems.
The problem of finding the Steiner tree of a subset of the vertices, that is, minimum tree that spans the given subset, is known to be NP-Complete.
A related problem is the "k"-minimum spanning tree ("k"-MST), which is the tree that spans some subset of "k" vertices in the graph with minimum weight.
A set of "k-smallest spanning trees" is a subset of "k" spanning trees (out of all possible spanning trees) such that no spanning tree outside the subset has smaller weight. (Note that this problem is unrelated to the "k"-minimum spanning tree.)
The Euclidean minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the Euclidean distance between vertices which are points in the plane (or space).
The rectilinear minimum spanning tree is a spanning tree of a graph with edge weights corresponding to the rectilinear distance between vertices which are points in the plane (or space).
In the distributed model, where each node is considered a computer and no node knows anything except its own connected links, one can consider distributed minimum spanning tree. The mathematical definition of the problem is the same but there are different approaches for a solution.
The capacitated minimum spanning tree is a tree that has a marked node (origin, or root) and each of the subtrees attached to the node contains no more than a "c" nodes. "c" is called a tree capacity. Solving CMST optimally is NP-hard, but good heuristics such as Esau-Williams and Sharma produce solutions close to optimal in polynomial time.
The degree constrained minimum spanning tree is a minimum spanning tree in with each vertex is connected to no more than "d" other vertices, for some given number "d". The case "d" = 2 is a special case of the traveling salesman problem, so the degree constrained minimum spanning tree is NP-hard in general.
For directed graphs, the minimum spanning tree problem is called the Arborescence problem and can be solved in quadratic time using the Chu–Liu/Edmonds algorithm.
A maximum spanning tree is a spanning tree with weight greater than or equal to the weight of every other spanning tree.
Such a tree can be found with algorithms such as Prim's or Kruskal's after multiplying the edge weights by -1 and solving
the MST problem on the new graph. A path in the maximum spanning tree is the widest path in the graph between its two endpoints: among all possible paths, it maximizes the weight of the minimum-weight edge.
Maximum spanning trees find applications in parsing algorithms for natural languages
and in training algorithms for conditional random fields.
The dynamic MST problem concerns the update of a previously computed MST after an edge weight change in the original graph or the insertion/deletion of a vertex.
The minimum labeling spanning tree problem is to find a spanning tree with least types of labels if each edge in a graph is associated with a label from a finite label set instead of a weight.
Minimum bottleneck spanning tree.
A bottleneck edge is the highest weighted edge in a spanning tree. A spanning tree is a minimum bottleneck spanning tree (or MBST) if the graph does not contain a spanning tree with a smaller bottleneck edge weight. A MST is necessarily a MBST (provable by the cut property), but a MBST is not necessarily a MST.

</doc>
<doc id="41796" url="http://en.wikipedia.org/wiki?curid=41796" title="Time-division multiplexing">
Time-division multiplexing

Time-division multiplexing (TDM) is a method of transmitting and receiving independent signals over a common signal path by means of synchronized switches at each end of the transmission line so that each signal appears on the line only a fraction of time in an alternating pattern. This form of signal multiplexing was developed in telecommunications for telegraphy systems in the late 1800s, but found its most common application in digital telephony in the second half of the 20th century.
History.
Time-division multiplexing was first developed for applications in telegraphy to route multiple transmissions simultaneously over a single transmission line. In the 1870s, Émile Baudot developed a time-multiplexing system of multiple Hughes telegraph machines.
In 1953 a 24-channel TDM was placed in commercial operation by RCA Communications to send audio information between RCA's facility on Broad Street, New York, their transmitting station at Rocky Point and the receiving station at Riverhead, Long Island, New York. The communication was by a microwave system throughout Long Island. The experimental TDM system was developed by RCA Laboratories between 1950 and 1953.
In 1962, engineers from Bell Labs developed the first D1 Channel Banks, which combined 24 digitised voice calls over a 4-wire copper trunk between Bell central office analogue switches. A channel bank sliced a 1.544 Mbit/s digital signal into 8,000 separate frames, each composed of 24 contiguous bytes. Each byte represented a single telephone call encoded into a constant bit rate signal of 64 kbit/s. Channel banks used a byte's fixed position (temporal alignment) in the frame to determine which call it belonged to.
Technology.
Time-division multiplexing is used primarily for digital signals, but may be applied in analog multiplexing in which two or more signals or bit streams are transferred appearing simultaneously as sub-channels in one communication channel, but are physically taking turns on the channel. The time domain is divided into several recurrent "time slots" of fixed length, one for each sub-channel. A sample byte or data block of sub-channel 1 is transmitted during time slot 1, sub-channel 2 during time slot 2, etc. One TDM frame consists of one time slot per sub-channel plus a synchronization channel and sometimes error correction channel before the synchronization. After the last sub-channel, error correction, and synchronization, the cycle starts all over again with a new frame, starting with the second sample, byte or data block from sub-channel 1, etc.
Application examples.
TDM can be further extended into the time division multiple access (TDMA) scheme, where several stations connected to the same physical medium, for example sharing the same frequency channel, can communicate. Application examples include:
Multiplexed digital transmission.
In circuit-switched networks, such as the public switched telephone network (PSTN), it is desirable to transmit multiple subscriber calls over the same transmission medium to effectively utilize the bandwidth of the medium. TDM allows transmitting and receiving telephone switches to create channels ("tributaries") within a transmission stream. A standard DS0 voice signal has a data bit rate of 64 kbit/s. A TDM circuit runs at a much higher signal bandwidth, permitting the bandwidth to be divided into time frames (time slots) for each voice signal which is multiplexed onto the line by the transmitter. If the TDM frame consists of "n" voice frames, the line bandwidth is "n"*64 kbit/s.
Each voice time slot in the TDM frame is called a channel. In European systems, standard TDM frames contain 30 digital voice channels (E1), and in American systems (T1), they contain 24 channels. Both standards also contain extra bits (or bit time slots) for signaling and synchronization bits.
Multiplexing more than 24 or 30 digital voice channels is called "higher order multiplexing". Higher order multiplexing is accomplished by multiplexing the standard TDM frames. For example, a European 120 channel TDM frame is formed by multiplexing four standard 30 channel TDM frames. At each higher order multiplex, four TDM frames from the immediate lower order are combined, creating multiplexes with a bandwidth of "n"*64 kbit/s, where "n" = 120, 480, 1920, etc.
Telecommunications systems.
There are three types of synchronous TDM: T1, SONET/SDH, and ISDN.
Plesiochronous digital hierarchy (PDH) was developed as a standard for multiplexing higher order frames. PDH created larger numbers of channels by multiplexing the standard Europeans 30 channel TDM frames. This solution worked for a while; however PDH suffered from several inherent drawbacks which ultimately resulted in the development of the Synchronous Digital Hierarchy (SDH). The requirements which drove the development of SDH were these:
SDH has become the primary transmission protocol in most PSTN networks. It was developed to allow streams 1.544 Mbit/s and above to be multiplexed, in order to create larger SDH frames known as Synchronous Transport Modules (STM). The STM-1 frame consists of smaller streams that are multiplexed to create a 155.52 Mbit/s frame. SDH can also multiplex packet based frames e.g. Ethernet, PPP and ATM.
While SDH is considered to be a transmission protocol (Layer 1 in the OSI Reference Model), it also performs some switching functions, as stated in the third bullet point requirement listed above. The most common SDH Networking functions are these:
SDH network functions are connected using high-speed optic fibre. Optic fibre uses light pulses to transmit data and is therefore extremely fast. Modern optic fibre transmission makes use of wavelength-division multiplexing (WDM) where signals transmitted across the fibre are transmitted at different wavelengths, creating additional channels for transmission. This increases the speed and capacity of the link, which in turn reduces both unit and total costs.
Statistical time-division multiplexing.
Statistical time division multiplexing (STDM) is an advanced version of TDM in which both the address of the terminal and the data itself are transmitted together for better routing. Using STDM allows bandwidth to be split over one line. Many college and corporate campuses use this type of TDM to distribute bandwidth.
On a 10-Mbit line entering a network, STDM can be used to provide 178 terminals with a dedicated 56k connection (178 * 56k = 9.96Mb). A more common use however is to only grant the bandwidth when that much is needed. STDM does not reserve a time slot for each terminal, rather it assigns a slot when the terminal is requiring data to be sent or received.
In its primary form, TDM is used for circuit mode communication with a fixed number of channels and constant bandwidth per channel. Bandwidth reservation distinguishes time-division multiplexing from statistical multiplexing such as statistical time division multiplexing. In pure TDM, the time slots are recurrent in a fixed order and pre-allocated to the channels, rather than scheduled on a packet-by-packet basis.
In dynamic TDMA, a scheduling algorithm dynamically reserves a variable number of time slots in each frame to variable bit-rate data streams, based on the traffic demand of each data stream. Dynamic TDMA is used in:
Asynchronous time-division multiplexing (ATDM), is an alternative nomenclature in which STDM designates synchronous time-division multiplexing, the older method that uses fixed time slots.

</doc>
<doc id="41797" url="http://en.wikipedia.org/wiki?curid=41797" title="Time-domain reflectometer">
Time-domain reflectometer

A time-domain reflectometer (TDR) is an electronic instrument that uses time-domain reflectometry to characterize and locate faults in metallic cables (for example, twisted pair wire or coaxial cable). It can also be used to locate discontinuities in a connector, printed circuit board, or any other electrical path. The equivalent device for optical fiber is an optical time-domain reflectometer.
Description.
A TDR measures reflections along a conductor. In order to measure those reflections, the TDR will transmit an incident signal onto the conductor and listen for its reflections. If the conductor is of a uniform impedance and is properly terminated, then there will be no reflections and the remaining incident signal will be absorbed at the far-end by the termination. Instead, if there are impedance variations, then some of the incident signal will be reflected back to the source. A TDR is similar in principle to radar.
Reflection.
Generally, the reflections will have the same shape as the incident signal, but their sign and magnitude depend on the change in impedance level. If there is a step increase in the impedance, then the reflection will have the same sign as the incident signal; if there is a step decrease in impedance, the reflection will have the opposite sign. The magnitude of the reflection depends not only on the amount of the impedance change, but also upon the loss in the conductor.
The reflections are measured at the output/input to the TDR and displayed or plotted as a function of time. Alternatively, the display can be read as a function of cable length because the speed of signal propagation is almost constant for a given transmission medium.
Because of its sensitivity to impedance variations, a TDR may be used to verify cable impedance characteristics, splice and connector locations and associated losses, and estimate cable lengths.
Incident signal.
TDRs use different incident signals. Some TDRs transmit a pulse along the conductor; the resolution of such instruments is often the width of the pulse. Narrow pulses can offer good resolution, but they have high frequency signal components that are attenuated in long cables. The shape of the pulse is often a half cycle sinusoid. For longer cables, wider pulse widths are used.
Fast rise time steps are also used. Instead of looking for the reflection of a complete pulse, the instrument is concerned with the rising edge, which can be very fast. A 1970's technology TDR used steps with a rise time of 25 ps.
Still other TDRs transmit complex signals and detect reflections with correlation techniques. See spread-spectrum time-domain reflectometry.
Example traces.
These traces were produced by a time-domain reflectometer made from common lab equipment connected to approximately 100 feet of 50 ohm coaxial cable. The propagation velocity of this cable is approximately 66% of the speed of light in a vacuum.
Explanation.
Consider the case where the far end of the cable is shorted (that is, terminated into zero ohms impedance). When the rising edge of the pulse is launched down the cable, the voltage at the launching point "steps up" to a given value instantly and the pulse begins propagating down the cable towards the short. When the pulse hits the short, no energy is absorbed at the far end. Instead, an opposing pulse reflects back from the short towards the launching end. It is only when this opposing reflection finally reaches the launch point that the voltage at this launching point abruptly drops back to zero, signalling the fact that there is a short at the end of the cable. That is, the TDR has no indication that there is a short at the end of the cable until its emitted pulse can travel down the cable at roughly the speed of light and the echo can return up the cable at the same speed. It is only after this round-trip delay that the short can be perceived by the TDR. Assuming that one knows the signal propagation speed in the particular cable-under-test, then in this way, the distance to the short can be measured.
A similar effect occurs if the far end of the cable is an open circuit (terminated into an infinite impedance). In this case, though, the reflection from the far end is polarized identically with the original pulse and adds to it rather than cancelling it out. So after a round-trip delay, the voltage at the TDR abruptly jumps to twice the originally-applied voltage.
Note that a theoretical perfect termination at the far end of the cable would entirely absorb the applied pulse without causing any reflection. In this case, it would be impossible to determine the actual length of the cable. Luckily, perfect terminations are very rare and some small reflection is nearly always caused.
The magnitude of the reflection is referred to as the reflection coefficient or ρ. The coefficient ranges from 1 (open circuit) to -1 (short circuit). The value of zero means that there is no reflection. The reflection coefficient is calculated as follows:
formula_1
Where Zo is defined as the characteristic impedance of the transmission medium and Zt is the impedance of the termination at the far end of the transmission line.
Any discontinuity can be viewed as a termination impedance and substituted as Zt. This includes abrupt changes in the characteristic impedance. As an example, a trace width on a printed circuit board doubled at its midsection would constitute a discontinuity. Some of the energy will be reflected back to the driving source; the remaining energy will be transmitted. This is also known as a scattering junction.
Usage.
Time domain reflectometers are commonly used for in-place testing of very long cable runs, where it is impractical to dig up or remove what may be a kilometers-long cable. They are indispensable for preventive maintenance of telecommunication lines, as TDRs can detect resistance on joints and connectors as they corrode, and increasing insulation leakage as it degrades and absorbs moisture, long before either leads to catastrophic failures. Using a TDR, it is possible to pinpoint a fault to within centimetres.
TDRs are also very useful tools for technical surveillance counter-measures, where they help determine the existence and location of wire taps. The slight change in line impedance caused by the introduction of a tap or splice will show up on the screen of a TDR when connected to a phone line.
TDR equipment is also an essential tool in the failure analysis of modern high-frequency printed circuit boards with signal traces crafted to emulate transmission lines. By observing reflections, any unsoldered pins of a ball grid array device can be detected. Short circuited pins can also be detected in a similar fashion.
The TDR principle is used in industrial settings, in situations as diverse as the testing of integrated circuit packages to measuring liquid levels. In the former, the time domain reflectometer is used to isolate failing sites in the same. The latter is primarily limited to the process industry.
TDR in level measurement.
In a TDR-based level measurement device, the device generates an impulse that propagates down a thin waveguide (referred to as a probe) – typically a metal rod or a steel cable. When this impulse hits the surface of the medium to be measured, part of the impulse reflects back up the waveguide. The device determines the fluid level by measuring the time difference between when the impulse was sent and when the reflection returned. The sensors can output the analyzed level as a continuous analog signal or switch output signals. In TDR technology, the impulse velocity is primarily affected by the permittivity of the medium through which the pulse propagates, which can vary greatly by the moisture content and temperature of the medium. In many cases, this effect can be corrected without undue difficulty. In some cases, such as in boiling and/or high temperature environments, the correction can be difficult. In particular, determining the froth (foam) height and the collapsed liquid level in a frothy / boiling medium can be very difficult.
TDR used in anchor cables in dams.
The Dam Safety Interest Group of CEA Technologies, Inc. (CEATI), a consortium of electrical power organizations, has applied Spread-spectrum time-domain reflectometry to identify potential faults in concrete dam anchor cables. The key benefit of Time Domain reflectometry over other testing methods is the non-destructive method of these tests.
TDR used in the earth and agricultural sciences.
A TDR is used to determine moisture content in soil and porous media. Over the last two decades, substantial advances have been made measuring moisture in soil, grain, food stuff, and sediment. The key to TDR’s success is its ability to accurately determine the permittivity (dielectric constant) of a material from wave propagation, due to the strong relationship between the permittivity of a material and its water content, as demonstrated in the pioneering works of Hoekstra and Delaney (1974) and Topp et al. (1980). Recent reviews and reference work on the subject include, Topp and Reynolds (1998), Noborio (2001), Pettinellia et al. (2002), Topp and Ferre (2002) and Robinson et al. (2003). The TDR method is a transmission line technique, and determines apparent permittivity (Ka) from the travel time of an electromagnetic wave that propagates along a transmission line, usually two or more parallel metal rods embedded in soil or sediment. The probes are typically between 10 and 30 cm long and connected to the TDR via coaxial cable.
TDR in geotechnical usage.
Time domain reflectometry has also been utilized to monitor slope movement in a variety of geotechnical settings including highway cuts, rail beds, and open pit mines (Dowding & O'Connor, 1984, 2000a, 2000b; Kane & Beck, 1999). In stability monitoring applications using TDR, a coaxial cable is installed in a vertical borehole passing through the region of concern. The electrical impedance at any point along a coaxial cable changes with deformation of the insulator between the conductors. A brittle grout surrounds the cable to translate earth movement into an abrupt cable deformation that shows up as a detectable peak in the reflectance trace. Until recently, the technique was relatively insensitive to small slope movements and could not be automated because it relied on human detection of changes in the reflectance trace over time. Farrington and Sargand (2004) developed a simple signal processing technique using numerical derivatives to extract reliable indications of slope movement from the TDR data much earlier than by conventional interpretation.
Another application of TDRs in geotechnical engineering is to determine the soil moisture content. This can be done by placing the TDRs in different soil layers and measurement of the time of start of precipitation and the time that TDR indicate an increase in the soil moisture content. The depth of the TDR (d) is a known factor and the other is the time it takes the drop of water to reach that depth (t); therefore the speed of water Infiltration (hydrology) (v) can be determined. This is a good method to assess the effectiveness of Best Management Practices (BMPs) in reducing stormwater Surface runoff.
TDR in semiconductor device analysis.
Time domain reflectometry is used in semiconductor failure analysis as a non-destructive method for the location of defects in semiconductor device packages. The TDR provides an electrical signature of individual conductive traces in the device package, and is useful for determining the location of opens and shorts.
TDR in aviation wiring maintenance.
Time domain reflectometry, specifically spread-spectrum time-domain reflectometry is used on aviation wiring for both preventative maintenance and fault location. Spread spectrum time domain reflectometry has the advantage of precisely locating the fault location within thousands of miles of aviation wiring. Additionally, this technology is worth considering for real time aviation monitoring, as spread spectrum reflectometry can be employed on live wires.
This method has been shown to be useful to locating intermittent electrical faults.

</doc>
<doc id="41799" url="http://en.wikipedia.org/wiki?curid=41799" title="Time standard">
Time standard

A time standard is a specification for measuring time: either the rate at which time passes; or points in time; or both. In modern times, several time specifications have been officially recognized as standards, where formerly they were matters of custom and practice. An example of a kind of time standard can be a time scale, specifying a method for measuring divisions of time. A standard for civil time can specify both time intervals and time-of-day. 
Standardized time measurements are made using a clock to count periods of some cyclic change, which may be either the changes of a natural phenomenon or of an artificial machine.
Historically, time standards were often based on the Earth's rotational period. From the late 17th century to the 19th century it was assumed that the Earth's daily rotational rate was constant. Astronomical observations of several kinds, including eclipse records, studied in the 19th century, raised suspicions that the rate at which Earth rotates is gradually slowing and also shows small-scale irregularities, and this was confirmed in the early twentieth century. Time standards based on Earth rotation were replaced (or initially supplemented) for astronomical use from 1952 onwards by an "ephemeris time" standard based on the Earth's orbital period and in practice on the motion of the Moon. The invention in 1955 of the caesium atomic clock has led to the replacement of older and purely astronomical time standards, for most practical purposes, by newer time standards based wholly or partly on atomic time. 
Various types of second and day are used as the basic time interval for most time scales. Other intervals of time (minutes, hours, and years) are usually defined in terms of these two.
Time standards based on Earth rotation.
Apparent solar time ('apparent' is often used in English-language sources, but 'true' in French astronomical literature) is based on the solar day, which is the period between one solar noon (passage of the real Sun across the meridian) and the next. A solar day is approximately 24 hours of mean time. Because the Earth's orbit around the sun is elliptical, and because of the obliquity of the Earth's axis relative to the plane of the orbit (the ecliptic), the apparent solar day varies a few dozen seconds above or below the mean value of 24 hours. As the variation accumulates over a few weeks, there are differences as large as 16 minutes between apparent solar time and mean solar time (see Equation of time). However, these variations cancel out over a year. There are also other perturbations such as Earth's wobble, but these are less than a second per year.
Sidereal time is time by the stars. A sidereal rotation is the time it takes the Earth to make one revolution with respect to the stars, approximately 23 hours 56 minutes 4 seconds. For accurate astronomical work on land, it was usual to observe sidereal time rather than solar time to measure mean solar time, because the observations of 'fixed' stars could be measured and reduced more accurately than observations of the Sun (in spite of the need to make various small compensations, for refraction, aberration, precession, nutation and proper motion). It is well known that observations of the Sun pose substantial obstacles to the achievement of accuracy in measurement. In former times, before the distribution of accurate time signals, it was part of the routine work at any observatory to observe the sidereal times of meridian transit of selected 'clock stars' (of well-known position and movement), and to use these to correct observatory clocks running local mean sidereal time; but nowadays local sidereal time is usually generated by computer, based on time signals.
Mean solar time was originally apparent solar time corrected by the equation of time. Mean solar time was sometimes derived, especially at sea for navigational purposes, by observing apparent solar time and then adding to it a calculated correction, the equation of time, which compensated for two known irregularities, caused by the ellipticity of the Earth's orbit and the obliquity of the Earth's equator and polar axis to the ecliptic (which is the plane of the Earth's orbit around the sun). 
Greenwich Mean Time (GMT) was originally mean time deduced from meridian observations made at the Royal Greenwich Observatory (RGO). The principal meridian of that observatory was chosen in 1884 by the International Meridian Conference to be the Prime Meridian. GMT either by that name or as 'mean time at Greenwich' used to be an international time standard, but is no longer so; it was initially renamed in 1928 as Universal Time (UT) (partly as a result of ambiguities arising from the changed practice of starting the astronomical day at midnight instead of at noon, adopted as from 1 January 1925). The more current refined version of UT, UT1, is still in reality mean time at Greenwich. Greenwich Mean Time is still the legal time in the UK (in winter, and as adjusted by one hour for summer time). But Coordinated Universal Time (UTC) (an atomic-based time scale which is always kept within 0.9 second of UT1) is in common actual use in the UK, and the name GMT is often inaccurately used to refer to it. (See articles Greenwich Mean Time, Universal Time, Coordinated Universal Time and the sources they cite.)
Universal Time (UT) is a time scale based on the mean solar day, defined to be as uniform as possible despite variations in Earth's rotation.
Time standards for planetary motion calculations.
Ephemeris time and its successor time scales described below have all been intended for astronomical use, e.g. in planetary motion calculations, with aims including uniformity, in particular, freedom from irregularities of Earth rotation. Some of these standards are examples of dynamical time scales and/or of coordinate time scales.
For applications at the Earth's surface, ET's official replacement was Terrestrial Dynamical Time (TDT), since redefined as Terrestrial Time (TT). For the calculation of ephemerides, TDB was officially recommended to replace ET, but deficiencies were found in the definition of TDB (though not affecting Teph), and these led to the IAU defining and recommending further time scales, Barycentric Coordinate Time (TCB) for use in the solar system as a whole, and Geocentric Coordinate Time (TCG) for use in the vicinity of the Earth. As defined, TCB (as observed from the Earth's surface) is of divergent rate relative to all of ET, Teph and TDT/TT; and the same is true, to a lesser extent, of TCG. The ephemerides of sun, moon and planets in current widespread and official use continue to be those calculated at the Jet Propulsion Laboratory (updated as from 2003 to DE405) using as argument Teph. 
In 1991, in order to clarify the relationships between space-time coordinates, new time scales were introduced, each with a different frame of reference. Terrestrial Time is time at Earth's surface. Geocentric Coordinate Time is a coordinate time scale at Earth's center. Barycentric Coordinate Time is a coordinate time scale at the center of mass of the solar system, which is called the barycenter. Barycentric Dynamical Time is a dynamical time at the barycenter.
Constructed time standards.
International Atomic Time () is the primary international time standard from which other time standards, including UTC, are calculated. TAI is kept by the BIPM (International Bureau of Weights and Measures), and is based on the combined input of many atomic clocks , each corrected for environmental and relativistic effects. It is the primary realisation of Terrestrial Time.
Coordinated Universal Time (UTC) is an atomic time scale designed to approximate Universal Time. UTC differs from TAI by an integral number of seconds. UTC is kept within 0.9 second of UT1 by the introduction of one-second steps to UTC, the "leap second". To date these steps have always been positive.
Standard time or civil time in a region deviates a fixed, round amount, usually a whole number of hours, from some form of Universal Time, now usually UTC. The offset is chosen such that a new day starts approximately while the sun is crossing the nadir meridian. See Time zone. Alternatively the difference is not really fixed, but it changes twice a year a round amount, usually one hour, see Daylight saving time.
Other time scales.
Julian day number is a count of days elapsed since Greenwich mean noon on 1 January 4713 B.C., Julian proleptic calendar. The Julian Date is the Julian day number followed by the fraction of the day elapsed since the preceding noon. Conveniently for astronomers, this avoids the date skip during an observation night.
Modified Julian day (MJD) is defined as MJD = JD - 2400000.5. An MJD day thus begins at midnight, civil date. Julian dates can be expressed in UT, TAI, TDT, etc. and so for precise applications the timescale should be specified, e.g. MJD 49135.3824 TAI.

</doc>
<doc id="41800" url="http://en.wikipedia.org/wiki?curid=41800" title="T interface">
T interface

A T-interface or T reference point is used for basic rate access in an Integrated Services Digital Network (ISDN) environment. It is a User–network interface reference point that is characterized by a four-wire, 144 kbit/s (2B+D) user rate.
Other characteristics of a T-interface are:
The T interface is electrically equivalent to the S interface, and the two are jointly referred to as the S/T interface.

</doc>
<doc id="41801" url="http://en.wikipedia.org/wiki?curid=41801" title="Toll switching trunk">
Toll switching trunk

In telecommunication, a toll switching trunk or toll connecting trunk is a trunk connecting one or more end offices to a toll center as the first stage of concentration for intertoll or long distance traffic. 
Operator assistance or participation may be an optional function. In U.S. common carrier telephony service, a toll center designated "Class 4C" is an office where assistance in completing incoming calls is provided in addition to other traffic; a toll center designated "Class 4P" is an office where operators handle only outbound calls, or where switching is performed without operator assistance.

</doc>
<doc id="41802" url="http://en.wikipedia.org/wiki?curid=41802" title="Total harmonic distortion">
Total harmonic distortion

The total harmonic distortion, or THD, of a signal is a measurement of the harmonic distortion present and is defined as the ratio of the sum of the powers of all harmonic components to the power of the fundamental frequency. THD is used to characterize the linearity of audio systems and the power quality of electric power systems. Distortion factor is a closely related term, sometimes used as a synonym.
In audio systems, lower distortion means the components in a loudspeaker, amplifier or microphone or other equipment produce a more accurate reproduction of an audio recording.
In radiocommunications, lower THD means pure signal emission without causing interferences to other
electronic devices. Moreover, the problem of distorted and not eco-friendly radio emissions appear to be also very important in the context of spectrum sharing and spectrum sensing.
In power systems, lower THD means reduction in peak currents, heating, emissions, and core loss in motors.
Definitions and examples.
To understand a system with an input and an output, such as an audio amplifier, we start with an ideal system where the transfer function is linear and time-invariant. When a signal passes through a non-ideal, non-linear device, additional content is added at the harmonics of the original frequencies. THD is a measurement of the extent of that distortion.
When the main performance criterion is the ″purity″ of the original sine wave (in other words, the contribution of the original frequency with respect to its harmonics), the measurement is most commonly defined as the ratio of the RMS amplitude of a set of higher harmonic frequencies to the RMS amplitude of the first harmonic, or fundamental, frequency
where "Vn" is the RMS voltage of "n"th harmonic and "n" = 1 is the fundamental frequency.
In practice, the THDF is commonly used in audio distortion specifications (percentage THD); however, THD is a non-standardized specification and the results between manufacturers are not easily comparable. Since individual harmonic amplitudes are measured, it is required that the manufacturer disclose the test signal frequency range, level and gain conditions, and number of measurements taken. It is possible to measure the full 20–20 kHz range using a sweep (though distortion for a fundamental above 10 kHz is inaudible). For all signal processing equipment, except microphone preamplifiers, the preferred gain setting is unity. For microphone preamplifiers, standard practice is to use maximum gain.
Measurements for calculating the THD are made at the output of a device under specified conditions. The THD is usually expressed in percent or in dB relative to the fundamental as distortion attenuation.
A variant definition uses the fundamental plus harmonics as the reference, though usage is discouraged:
These can be distinguished as THDF (for "fundamental"), and THDR (for "root mean square"). THDR cannot exceed 100%. At low distortion levels, the difference between the two calculation methods is negligible. For instance, a signal with THDF of 10% has a very similar THDR of 9.95%. However, at higher distortion levels the discrepancy becomes large. For instance, a signal with THDF 266% has a THDR of 94%. A pure square wave with infinite harmonics has THDF of 48.3%, or THDR of 43.5%.
Some use the term "distortion factor" as a synonym for THDR, while others use it as a synonym for THDF.
THD+N.
THD+N means total harmonic distortion plus noise. This measurement is much more common and more comparable between devices. It is usually measured by inputting a sine wave, notch filtering the output, and comparing the ratio between the output signal with and without the sine wave:
Like the THD measurement, this is a ratio of RMS amplitudes, and can be measured as THDF (bandpassed or calculated fundamental as the denominator) or, more commonly, as THDR (total distorted signal as the denominator). Audio Precision measurements are THDR, for instance.
A meaningful measurement must include the bandwidth of measurement. This measurement includes effects from ground loop power line hum, high-frequency interference, intermodulation distortion between these tones and the fundamental, and so on, in addition to harmonic distortion. For psychoacoustic measurements, a weighting curve is applied such as A-weighting or ITU-R BS.468, which is intended to accentuate what is most audible to the human ear, contributing to a more accurate measurement.
For a given input frequency and amplitude, THD+N is equal to SINAD, provided that both measurements are made over the same bandwidth.
Measurement.
The distortion of a waveform relative to a pure sinewave can be measured either by using a THD analyzer to analyse the output wave into its constituent harmonics and noting the amplitude of each relative to the fundamental; or by cancelling out the fundamental with a notch filter and measuring the remaining signal, which will be total aggregate harmonic distortion plus noise.
Given a sinewave generator of very low inherent distortion, it can be used as input to amplification equipment, whose distortion at different frequencies and signal levels can be measured by examining the output waveform.
There is electronic equipment both to generate sinewaves and to measure distortion; but a general-purpose digital computer equipped with a sound card can carry out harmonic analysis with suitable software. Different software can be used to generate sinewaves, but the inherent distortion may be too high for measurement of very low-distortion amplifiers.
Interpretation.
For many purposes different types of harmonics are not equivalent. For instance, crossover distortion at a given THD is much more audible than clipping distortion at the same THD, since the harmonics produced are at higher frequencies, which are not as easily masked by the fundamental. A single THD number is inadequate to specify audibility, and must be interpreted with care. Taking THD measurements at different output levels would expose whether the distortion is clipping (which increases with level) or crossover (which decreases with level).
THD is an average of a number of harmonics equally weighted, even though research performed decades ago identifies that lower order harmonics are harder to hear at the same level, compared with higher order ones. In addition, even order harmonics are said to be generally harder to hear than odd order. A number of formulas that attempt to correlate THD with actual audibility have been published, however none have gained mainstream use.
Examples.
For many standard signals, the above criterion may be calculated analytically in a closed-form. For example, a pure square wave has THDF equal to
The sawtooth signal possesses
The pure symmetrical triangle wave has THDF of 
For the rectangular pulse train with the "duty cycle" "μ" (called sometimes the "cyclic ratio"),
the THDF has the form
and logically, reaches the minimum (≈0.483) when the signal becomes symmetrical "μ"=0.5, "i.e." the pure square wave. Appropriate filtering of these signals may drastically reduce the resulting THD. For instance, the pure square wave filtered by the Butterworth low-pass filter of the second-order (with the cutoff frequency set equal to the fundamental frequency) has THDF of 5.3%, while the same signal filtered by the fourth-order filter has THDF of 0.6%. However, analytic computation of the THDF for complicated waveforms and filters often represents a difficult task, and the resulting expressions may be quite laborious to obtain. For example, the closed-form expression for the THDF of the sawtooth wave filtered by the first-order Butterworth low-pass filter is simply
while that for the same signal filtered by the second-order Butterworth filter is given by 
a rather cumbersome formula
Yet, the closed-form expression for the THDF of the pulse train filtered by the "p"th-order Butterworth low-pass filter is even more complicated and has the following form
where "μ" is the duty cycle, 0<"μ"<1, and 
see for more details.

</doc>
<doc id="41804" url="http://en.wikipedia.org/wiki?curid=41804" title="Traffic intensity">
Traffic intensity

In telecommunication networks, traffic intensity is a measure of the average occupancy of a server or resource during a specified period of time, normally a busy hour. It is measured in traffic units (erlangs) and defined as the ratio of the time during which a facility is cumulatively occupied to the time this facility is available for occupancy. 
In a digital network, the traffic intensity is:
where
A traffic intensity greater than one erlang means that the rate at which bits arrive exceeds the rate bits can be transmitted and queuing delay will grow without bound (if the traffic intensity stays the same). If the traffic intensity is less than one erlang, then the router can handle more average traffic. 
Telecommunication operators are vitally interested in traffic intensity, as it dictates the amount of equipment they must supply.

</doc>
<doc id="41805" url="http://en.wikipedia.org/wiki?curid=41805" title="Transceiver">
Transceiver

A transceiver is a device comprising both a transmitter and a receiver which are combined and share common circuitry or a single housing. When no circuitry is common between transmit and receive functions, the device is a transmitter-receiver. The term originated in the early 1920s. Technically, transceivers must combine a significant amount of the transmitter and receiver handling circuitry. Similar devices include transponders, transverters, and repeaters.
Radio technology.
In radio terminology, a transceiver means a unit which contains both a receiver and a transmitter. From the beginning days
of radio the receiver and transmitter were separate units and remained so until around 1920. Amateur radio or "ham" radio operators can build their own equipment and it is now easier to design and build a simple unit containing both of the functions: transmitting and receiving. Almost all modern amateur radio equipment is now a transceiver but there is an active market for pure radio receivers, mainly for shortwave listening (SWL) operators. An example of a transceiver would be a walkie-talkie, or a CB radio.
RF Transceiver.
The RF Transceiver uses RF modules for high speed data transmission. The micro electronic circuits in the digital-RF architecture work at speeds up to 100 GHz. The objective in the design was to bring digital domain closer to the antenna, both at the receive and transmit ends using software defined radio (SDR). The software-programmable digital processors used in the circuits permit conversion between digital baseband signals and analog RF.
Telephony.
On a wired telephone, the handset contains the transmitter and receiver for the audio and in the 20th century was usually wired to the base unit by tinsel wire. The whole unit is colloquially referred to as a "receiver." On a mobile telephone or other radiotelephone, the entire unit is a transceiver, for both audio and radio.
A cordless telephone uses an audio and radio transceiver for the handset, and a radio transceiver for the base station. If a speakerphone is included in a wired telephone base or in a cordless base station, the base also becomes an audio transceiver in addition to the handset.
A modem is similar to a transceiver, in that it sends and receives a signal, but a modem uses modulation and demodulation. It modulates a signal being transmitted and demodulates a signal being received.
Ethernet.
Transceivers are called Medium Attachment Units (MAUs) in IEEE 802.3 documents and were widely used in 10BASE2 and 10BASE5 Ethernet networks. Fiber-optic gigabit and 10 Gigabit Ethernet utilize transceivers known as GBIC, SFP, SFP+, XFP, XAUI and CFP.

</doc>
<doc id="41807" url="http://en.wikipedia.org/wiki?curid=41807" title="Transmission">
Transmission

Transmission may refer to:

</doc>
<doc id="41808" url="http://en.wikipedia.org/wiki?curid=41808" title="Transmission block">
Transmission block

In telecommunication, the term transmission block has the following meanings: 
A transmission block is usually terminated by an End Of Block character (EOB), End Transmission Block character (ETB), End Of Text character (ETX), or End Of Transmission character (EOT).

</doc>
<doc id="41810" url="http://en.wikipedia.org/wiki?curid=41810" title="Transmission level point">
Transmission level point

In a telecommunications system, a transmission level point (TLP) is a test point or interface, "i.e." a physical point in an electronic circuit where a test signal may be inserted or measured, and for which the nominal power of the test signal is specified.
A zero dBm transmission level point (or synonym zero transmission level point) is a transmission level point where the nominal test signal power is 0dBm.
In practice, the abbreviation TLP is usually used, and it is modified by the nominal level for the point in question. For example, where the nominal level is 0 dBm, the expression 0 dBm TLP, or simply, 0TLP, is used. Where the nominal level is −16 dBm, the expression −16 dBm TLP, or −16TLP, is used.
The nominal transmission level at a specified TLP is a function of system design and is an expression of the design gain or loss.
Voice-channel transmission levels, i.e. TLPs, are usually specified for a frequency of approximately 1,000 Hz.
The TLP at a point at which an end instrument, e.g. a telephone set, is connected is usually specified as 0 dBm.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41811" url="http://en.wikipedia.org/wiki?curid=41811" title="Transmission line">
Transmission line

In communications and electronic engineering, a transmission line is a specialized cable or other structure designed to carry alternating current of radio frequency, that is, currents with a frequency high enough that their wave nature must be taken into account. Transmission lines are used for purposes such as connecting radio transmitters and receivers with their antennas, distributing cable television signals, trunklines routing calls between telephone switching centers, computer network connections and high speed computer data buses.
This article covers two-conductor transmission line such as parallel line (ladder line), coaxial cable, stripline, and microstrip. Some sources also refer to waveguide, dielectric waveguide, and even optical fiber as transmission line, however these lines require different analytical techniques and so are not covered by this article; see Waveguide (electromagnetism).
Overview.
Ordinary electrical cables suffice to carry low frequency alternating current (AC), such as mains power, which reverses direction 100 to 120 times per second, and audio signals. However, they cannot be used to carry currents in the radio frequency range or higher, which reverse direction millions to billions of times per second, because the energy tends to radiate off the cable as radio waves, causing power losses. Radio frequency currents also tend to reflect from discontinuities in the cable such as connectors and joints, and travel back down the cable toward the source. These reflections act as bottlenecks, preventing the signal power from reaching the destination. Transmission lines use specialized construction, and impedance matching, to carry electromagnetic signals with minimal reflections and power losses. The distinguishing feature of most transmission lines is that they have uniform cross sectional dimensions along their length, giving them a uniform "impedance", called the characteristic impedance, to prevent reflections. Types of transmission line include parallel line (ladder line, twisted pair), coaxial cable, stripline, and microstrip. The higher the frequency of electromagnetic waves moving through a given cable or medium, the shorter the wavelength of the waves. Transmission lines become necessary when the length of the cable is longer than a significant fraction of the transmitted frequency's wavelength.
At microwave frequencies and above, power losses in transmission lines become excessive, and waveguides are used instead, which function as "pipes" to confine and guide the electromagnetic waves. Some sources define waveguides as a type of transmission line; however, this article will not include them. At even higher frequencies, in the terahertz, infrared and light range, waveguides in turn become lossy, and optical methods, (such as lenses and mirrors), are used to guide electromagnetic waves.
The theory of sound wave propagation is very similar mathematically to that of electromagnetic waves, so techniques from transmission line theory are also used to build structures to conduct acoustic waves; and these are called acoustic transmission lines.
History.
Mathematical analysis of the behavior of electrical transmission lines grew out of the work of James Clerk Maxwell, Lord Kelvin and Oliver Heaviside. In 1855 Lord Kelvin formulated a diffusion model of the current in a submarine cable. The model correctly predicted the poor performance of the 1858 trans-Atlantic submarine telegraph cable. In 1885 Heaviside published the first papers that described his analysis of propagation in cables and the modern form of the telegrapher's equations.
Applicability.
In many electric circuits, the length of the wires connecting the components can for the most part be ignored. That is, the voltage on the wire at a given time can be assumed to be the same at all points. However, when the voltage changes in a time interval comparable to the time it takes for the signal to travel down the wire, the length becomes important and the wire must be treated as a transmission line. Stated another way, the length of the wire is important when the signal includes frequency components with corresponding wavelengths comparable to or less than the length of the wire.
A common rule of thumb is that the cable or wire should be treated as a transmission line if the length is greater than 1/10 of the wavelength. At this length the phase delay and the interference of any reflections on the line become important and can lead to unpredictable behavior in systems which have not been carefully designed using transmission line theory.
The four terminal model.
For the purposes of analysis, an electrical transmission line can be modeled as a two-port network (also called a quadrupole network), as follows:
In the simplest case, the network is assumed to be linear (i.e. the complex voltage across either port is proportional to the complex current flowing into it when there are no reflections), and the two ports are assumed to be interchangeable. If the transmission line is uniform along its length, then its behavior is largely described by a single parameter called the "characteristic impedance", symbol Z0. This is the ratio of the complex voltage of a given wave to the complex current of the same wave at any point on the line. Typical values of Z0 are 50 or 75 ohms for a coaxial cable, about 100 ohms for a twisted pair of wires, and about 300 ohms for a common type of untwisted pair used in radio transmission.
When sending power down a transmission line, it is usually desirable that as much power as possible will be absorbed by the load and as little as possible will be reflected back to the source. This can be ensured by making the load impedance equal to Z0, in which case the transmission line is said to be "matched".
Some of the power that is fed into a transmission line is lost because of its resistance. This effect is called "ohmic" or "resistive" loss (see ohmic heating). At high frequencies, another effect called "dielectric loss" becomes significant, adding to the losses caused by resistance. Dielectric loss is caused when the insulating material inside the transmission line absorbs energy from the alternating electric field and converts it to heat (see dielectric heating). The transmission line is modeled with a resistance (R) and inductance (L) in series with a capacitance (C) and conductance (G) in parallel. The resistance and conductance contribute to the loss in a transmission line.
The total loss of power in a transmission line is often specified in decibels per meter (dB/m), and usually depends on the frequency of the signal. The manufacturer often supplies a chart showing the loss in dB/m at a range of frequencies. A loss of 3 dB corresponds approximately to a halving of the power.
High-frequency transmission lines can be defined as those designed to carry electromagnetic waves whose wavelengths are shorter than or comparable to the length of the line. Under these conditions, the approximations useful for calculations at lower frequencies are no longer accurate. This often occurs with radio, microwave and optical signals, metal mesh optical filters, and with the signals found in high-speed digital circuits.
Telegrapher's equations.
The telegrapher's equations (or just telegraph equations) are a pair of linear differential equations which describe the voltage and current on an electrical transmission line with distance and time. They were developed by Oliver Heaviside who created the "transmission line model", and are based on Maxwell's Equations.
The transmission line model represents the transmission line as an infinite series of two-port elementary components, each representing an infinitesimally short segment of the transmission line:
The model consists of an "infinite series" of the elements shown in the figure, and that the values of the components are specified "per unit length" so the picture of the component can be misleading. formula_1, formula_2, formula_3, and formula_4 may also be functions of frequency. An alternative notation is to use formula_9, formula_10, formula_11 and formula_12 to emphasize that the values are derivatives with respect to length. These quantities can also be known as the primary line constants to distinguish from the secondary line constants derived from them, these being the propagation constant, attenuation constant and phase constant.
The line voltage formula_13 and the current formula_14 can be expressed in the frequency domain as
When the elements formula_1 and formula_4 are negligibly small the transmission line is considered as a lossless structure. In this hypothetical case, the model depends only on the formula_2 and formula_3 elements which greatly simplifies the analysis. For a lossless transmission line, the second order steady-state Telegrapher's equations are:
These are wave equations which have plane waves with equal propagation speed in the forward and reverse directions as solutions. The physical significance of this is that electromagnetic waves propagate down transmission lines and in general, there is a reflected component that interferes with the original signal. These equations are fundamental to transmission line theory.
If formula_1 and formula_4 are not neglected, the Telegrapher's equations become:
where "γ" is the propagation constant
and the characteristic impedance can be expressed as
The solutions for formula_13 and formula_14 are:
The constants formula_33 and formula_34 must be determined from boundary conditions. For a voltage pulse formula_35, starting at formula_36 and moving in the positive formula_37-direction, then the transmitted pulse formula_38 at position formula_37 can be obtained by computing the Fourier Transform, formula_40, of formula_35, attenuating each frequency component by formula_42, advancing its phase by formula_43, and taking the inverse Fourier Transform. The real and imaginary parts of formula_44 can be computed as
where atan2 is the two-parameter arctangent, and
For small losses and high frequencies, to first order in formula_49 and formula_50 one obtains
Noting that an advance in phase by formula_53 is equivalent to a time delay by formula_54, formula_55 can be simply computed as
Input impedance of transmission line.
The characteristic impedance "Z"0 of a transmission line is the ratio of the amplitude of a single voltage wave to its current wave. Since most transmission lines also have a reflected wave, the characteristic impedance is generally not the impedance that is measured on the line.
The impedance measured at a given distance, "l", from the load impedance "ZL" may be expressed as,
where "γ" is the propagation constant and formula_58 is the voltage reflection coefficient at the load end of the transmission line. Alternatively, the above formula can be rearranged to express the input impedance in terms of the load impedance rather than the load voltage reflection coefficient:
Input impedance of lossless transmission line.
For a lossless transmission line, the propagation constant is purely imaginary, , so the above formulas can be rewritten as,
where formula_61 is the wavenumber.
In calculating "β", the wavelength is generally different inside the transmission line to what it would be in free-space and the velocity constant of the material the transmission line is made of needs to be taken into account when doing such a calculation.
Special cases of lossless transmission lines.
Half wave length.
For the special case where formula_62 where n is an integer (meaning that the length of the line is a multiple of half a wavelength), the expression reduces to the load impedance so that
formula_63
for all "n". This includes the case when , meaning that the length of the transmission line is negligibly small compared to the wavelength. The physical significance of this is that the transmission line can be ignored (i.e. treated as a wire) in either case.
Quarter wave length.
For the case where the length of the line is one quarter wavelength long, or an odd multiple of a quarter wavelength long, the input impedance becomes
Matched load.
Another special case is when the load impedance is equal to the characteristic impedance of the line (i.e. the line is "matched"), in which case the impedance reduces to the characteristic impedance of the line so that
for all formula_66 and all formula_67.
Short.
For the case of a shorted load (i.e. formula_68), the input impedance is purely imaginary and a periodic function of position and wavelength (frequency)
Open.
For the case of an open load (i.e. formula_70), the input impedance is once again imaginary and periodic
Stepped transmission line.
A stepped transmission line is used for broad range impedance matching. It can be considered as multiple transmission line segments connected in series, with the characteristic impedance of each individual element to be Z0,i. The input impedance can be obtained from the successive application of the chain relation
where formula_73 is the wave number of the "i"th transmission line segment and li is the length of this segment, and Zi is the front-end impedance that loads the "i"th segment. Because the characteristic impedance of each transmission line segment Z0,i is often different from that of the input cable Z0, the impedance transformation circle is off-centered along the x axis of the Smith Chart whose impedance representation is usually normalized against Z0.
Practical types.
Coaxial cable.
Coaxial lines confine virtually all of the electromagnetic wave to the area inside the cable. Coaxial lines can therefore be bent and twisted (subject to limits) without negative effects, and they can be strapped to conductive supports without inducing unwanted currents in them.
In radio-frequency applications up to a few gigahertz, the wave propagates in the transverse electric and magnetic mode (TEM) only, which means that the electric and magnetic fields are both perpendicular to the direction of propagation (the electric field is radial, and the magnetic field is circumferential). However, at frequencies for which the wavelength (in the dielectric) is significantly shorter than the circumference of the cable, transverse electric (TE) and transverse magnetic (TM) waveguide modes can also propagate. When more than one mode can exist, bends and other irregularities in the cable geometry can cause power to be transferred from one mode to another.
The most common use for coaxial cables is for television and other signals with bandwidth of multiple megahertz. In the middle 20th century they carried long distance telephone connections.
Microstrip.
A microstrip circuit uses a thin flat conductor which is parallel to a ground plane. Microstrip can be made by having a strip of copper on one side of a printed circuit board (PCB) or ceramic substrate while the other side is a continuous ground plane. The width of the strip, the thickness of the insulating layer (PCB or ceramic) and the dielectric constant of the insulating layer determine the characteristic impedance.
Microstrip is an open structure whereas coaxial cable is a closed structure.
Stripline.
A stripline circuit uses a flat strip of metal which is sandwiched between two parallel ground planes. The insulating material of the substrate forms a dielectric. The width of the strip, the thickness of the substrate and the relative permittivity of the substrate determine the characteristic impedance of the strip which is a transmission line.
Balanced lines.
A balanced line is a transmission line consisting of two conductors of the same type, and equal impedance to ground and other circuits. There are many formats of balanced lines, amongst the most common are twisted pair, star quad and twin-lead.
Twisted pair.
Twisted pairs are commonly used for terrestrial telephone communications. In such cables, many pairs are grouped together in a single cable, from two to several thousand. The format is also used for data network distribution inside buildings, but the cable is more expensive because the transmission line parameters are tightly controlled.
Star quad.
Star quad is a four-conductor cable in which all four conductors are twisted together around the cable axis. It is sometimes used for two circuits, such as 4-wire telephony and other telecommunications applications. In this configuration each pair uses two non-adjacent conductors. Other times it is used for a single, balanced circuit, such as audio applications and 2-wire telephony. In this configuration two non-adjacent conductors are terminated together at both ends of the cable, and the other two conductors are also terminated together.
Interference picked up by the cable arrives as a virtually perfect common mode signal, which is easily removed by coupling transformers. Because the conductors are always the same distance from each other, cross talk is reduced relative to cables with two separate twisted pairs.
The combined benefits of twisting, differential signalling, and quadrupole pattern give outstanding noise immunity, especially advantageous for low signal level applications such as long microphone cables, even when installed very close to a power cable. The disadvantage is that star quad, in combining two conductors, typically has double the capacitance of similar two-conductor twisted and shielded audio cable. High capacitance causes increasing distortion and greater loss of high frequencies as distance increases.
Twin-lead.
Twin-lead consists of a pair of conductors held apart by a continuous insulator.
Lecher lines.
Lecher lines are a form of parallel conductor that can be used at UHF for creating resonant circuits. They are a convenient practical format that fills the gap between lumped components (used at HF/VHF) and resonant cavities (used at UHF/SHF).
Single-wire line.
Unbalanced lines were formerly much used for telegraph transmission, but this form of communication has now fallen into disuse. Cables are similar to twisted pair in that many cores are bundled into the same cable but only one conductor is provided per circuit and there is no twisting. All the circuits on the same route use a common path for the return current (earth return). There is a power transmission version of single-wire earth return in use in many locations.
General applications.
Signal transfer.
Electrical transmission lines are very widely used to transmit high frequency signals over long or short distances with minimum power loss. One familiar example is the down lead from a TV or radio aerial to the receiver.
Pulse generation.
Transmission lines are also used as pulse generators. By charging the transmission line and then discharging it into a resistive load, a rectangular pulse equal in length to twice the electrical length of the line can be obtained, although with half the voltage. A Blumlein transmission line is a related pulse forming device that overcomes this limitation. These are sometimes used as the pulsed power sources for radar transmitters and other devices.
Stub filters.
If a short-circuited or open-circuited transmission line is wired in parallel with a line used to transfer signals from point A to point B, then it will function as a filter. The method for making stubs is similar to the method for using Lecher lines for crude frequency measurement, but it is 'working backwards'. One method recommended in the RSGB's radiocommunication handbook is to take an open-circuited length of transmission line wired in parallel with the feeder delivering signals from an aerial. By cutting the free end of the transmission line, a minimum in the strength of the signal observed at a receiver can be found. At this stage the stub filter will reject this frequency and the odd harmonics, but if the free end of the stub is shorted then the stub will become a filter rejecting the even harmonics.
Acoustic transmission lines.
An acoustic transmission line is the acoustic analog of the electrical transmission line, typically thought of as a rigid-walled tube that is long and thin relative to the wavelength of sound present in it.
References.
"Part of this article was derived from Federal Standard 1037C."
 |last= Steinmetz
 |first= Charles Proteus
 |authorlink= Charles Proteus Steinmetz
 |title= The Natural Period of a Transmission Line and the Frequency of lightning Discharge Therefrom
 |journal=The Electrical World
 |date= August 27, 1898
 |pages= 203–205
 |issn=
 |title= Electromagnetism
 |edition= 2nd
 |last=Grant
 |first= I. S.
 |last2= Phillips
 |first2= W. R.
 |publisher= John Wiley
 |isbn= 0-471-92712-0
 |title=Fundamentals of Applied Electromagnetics
 |edition= 2004 media
 |last= Ulaby
 |first= F. T.
 |publisher= Prentice Hall
 |isbn= 0-13-185089-X
 |title=Radio communication handbook
 |year= 1982
 |page= 20
 |chapter= Chapter 17
 |publisher= Radio Society of Great Britain
 |isbn= 0-900612-58-4
 |last= Naredo
 |first= J. L.
 |first2= A. C.
 |last2= Soudack
 |first3= J. R.
 |last3= Marti
 |title= Simulation of transients on transmission lines with corona via the method of characteristics
 |journal= IEE Proceedings. Generation, Transmission and Distribution.
 |volume= 142
 |issue= 1
 |publisher= Institution of Electrical Engineers
 |location= Morelos 
 |date= Jan 1995
 |issn= 1350-2360

</doc>
<doc id="41812" url="http://en.wikipedia.org/wiki?curid=41812" title="Transmission medium">
Transmission medium

A transmission medium is a material substance (solid, liquid, gas, or plasma) that can propagate energy waves. For example, the transmission medium for sounds is usually air, but solids and liquids may also act as transmission media for sound.
The absence of a material medium in vacuum may also constitute a transmission medium for electromagnetic waves such as light and radio waves. While material substance is not required for electromagnetic waves to propagate, such waves are usually affected by the transmission media they pass through, for instance by absorption or by reflection or refraction at the interfaces between media.
The term transmission medium also refers to a technical device that employs the material substance to transmit or guide waves. Thus, an optical fiber or a copper cable is a transmission medium. Not only this but also is able to guide the transmission of networks.
A transmission medium can be classified as a:
Electromagnetic radiation can be transmitted through an optical medium, such as optical fiber, or through twisted pair wires, coaxial cable, or dielectric-slab waveguides. It may also pass through any physical material that is transparent to the specific wavelength, such as water, air, glass, or concrete. Sound is, by definition, the vibration of matter, so it requires a physical medium for transmission, as do other kinds of mechanical waves and heat energy. Historically, science incorporated various aether theories to explain the transmission medium. However, it is now known that electromagnetic waves do not require a physical transmission medium, and so can travel through the "vacuum" of free space. Regions of the insulative vacuum can become conductive for electrical conduction through the presence of free electrons, holes, or ions.
Transmission and reception of data is performed in four steps.
Telecommunications.
A physical medium in data communications is the transmission path over which a signal propagates.
Many transmission media are used as communications channel.
For telecommunications purposes in the United States, Federal Standard 1037C, transmission media are classified as one of the following:
One of the most common physical medias used in networking is copper wire. Copper wire to carry signals to long distances using relatively low amounts of power. The unshielded twisted pair (UTP) is eight strands of copper wire, organized into four pairs.
Another example of a physical medium is optical fiber, which has emerged as the most commonly used transmission medium for long-distance communications. Optical fiber is a thin strand of glass that guides light along its length. Four major factors favor optical fiber over copper- data rates, distance, installation, and costs. Optical fiber can carry huge amounts of data compared to copper. It can be run for hundreds of miles without the need for signal repeaters, in turn, reducing maintenance costs and improving the reliability of the communication system because repeaters are a common source of network failures. Glass is lighter than copper allowing for less need for specialized heavy-lifting equipment when installing long-distance optical fiber. Optical fiber for indoor applications cost approximately a dollar a foot, the same as copper.
Multimode and single mode are two types of commonly used optical fiber. Multimode fiber uses LEDs as the light source and can carry signals over shorter distances, about 2 kilometers. Single mode can carry signals over distances of tens of miles.
Wireless media may carry surface waves or skywaves, either longitudinally or transversely, and are so classified.
In both communications, communication is in the form of electromagnetic waves. With guided transmission media, the waves are guided along a physical path; examples of guided media include phone lines, twisted pair cables, coaxial cables, and optical fibers. Unguided transmission media are methods that allow the transmission of data without the use of physical means to define the path it takes. Examples of this include microwave, radio or infrared. Unguided media provide a means for transmitting electromagnetic waves but do not guide them; examples are propagation through air, vacuum and seawater.
The term direct link is used to refer to the transmission path between two devices in which signals propagate directly from transmitters to receivers with no intermediate devices, other than amplifiers or repeaters used to increase signal strength. This term can apply to both guided and unguided media.
Types of transmissions.
A transmission may be simplex, half-duplex, or full-duplex.
In simplex transmission, signals are transmitted in only one direction; one station is a transmitter and the other is the receiver. In the half-duplex operation, both stations may transmit, but only one at a time. In full duplex operation, both stations may transmit simultaneously. In the latter case, the medium is carrying signals in both directions at same time.
There are two types of transmission media :
• Guided
• Unguided 
Guided Media :
• Unshielded Twisted Pair (UTP)
• Shielded Twisted Pair
• Coaxial Cable
• Optical Fiber
Unguided Media : Transmission media then looking at analysis of using them unguided transmission media is data signals that flow through the air. They are not guided or bound to a channel to follow. Following are unguided media used for data communication :
• Radio Transmission
• Microwave
• Satellite Communication

</doc>
<doc id="41813" url="http://en.wikipedia.org/wiki?curid=41813" title="Transmit-after-receive time delay">
Transmit-after-receive time delay

In telecommunication, transmit-after-receive time delay is the time interval from removal of RF energy at the local receiver input until the local transmitter is automatically keyed on and the transmitted rf signal amplitude has increased to 90% of its steady-state value. "An Exception:" High-frequency (HF) transceiver equipment is normally not designed with an interlock between receiver squelch and transmitter on-off key. The transmitter can be keyed on at any time, independent of whether or not a signal is being received at the receiver input.

</doc>
<doc id="41817" url="http://en.wikipedia.org/wiki?curid=41817" title="Transponder">
Transponder

In telecommunication, a transponder is one of two types of devices. In air navigation or radio frequency identification, a flight transponder is a device that emits an identifying signal in response to an interrogating received signal. In a communications satellite, a transponder gathers signals over a range of uplink frequencies and re-transmits them on a different set of downlink frequencies to receivers on Earth, often without changing the content of the received signal or signals.
The term is a portmanteau for "trans"mitter-res"ponder". It is variously abbreviated as XPDR, XPNDR, TPDR or TP.
Satellite/broadcast communications.
A communications satellite’s channels are called transponders, because each is a separate transceiver or repeater. With digital video data compression and multiplexing, several video and audio channels may travel through a single transponder on a single wideband carrier. Original analog video only has one channel per transponder, with subcarriers for audio and automatic transmission identification service (ATIS). Non-multiplexed radio stations can also travel in single channel per carrier (SCPC) mode, with multiple carriers (analog or digital) per transponder. This allows each station to transmit directly to the satellite, rather than paying for a whole transponder, or using landlines to send it to an earth station for multiplexing with other stations.
Optical communications.
In optical fiber communications, a transponder is the element that sends and receives the optical signal from a fiber. A transponder is typically characterized by its data rate and the maximum distance the signal can travel.
The term 'transponder' can apply to different items with important functional differences, mentioned across academic and commercial literature:
As a result, difference in transponder functionality also might influence the functional description of related optical modules like transceivers and muxponders.
Aviation.
Another type of transponder occurs in identification friend or foe systems in military aviation and in air traffic control secondary surveillance radar (beacon radar) systems for general aviation and commercial aviation. Primary radar works best with large all-metal aircraft, but not so well on small, composite aircraft. Its range is also limited by terrain and rain or snow and also detects unwanted objects such as automobiles, hills and trees. Furthermore it cannot always estimate the altitude of an aircraft. Secondary radar overcomes these limitations but it depends on a transponder in the aircraft to respond to interrogations from the ground station to make the plane more visible.
Depending on the type of interrogation, the transponder sends back a transponder code (or "squawk code", Mode A) or altitude information (Mode C) to help air traffic controllers to identify the aircraft and to maintain separation between planes. Another mode called Mode S (Mode Select) is designed to help avoiding over-interrogation of the transponder (having many radars in busy areas) and to allow automatic collision avoidance. Mode S transponders are 'backwards compatible' with Modes A & C. Mode S is mandatory in controlled airspace in many countries. Some countries have also required, or are moving towards requiring, that all aircraft be equipped with Mode S, even in uncontrolled airspace. However in the field of general aviation there have been objections to these moves, because of the cost, size, limited benefit to the users in uncontrolled airspace, and, in the case of balloons and gliders, the power requirements during long flights.
Marine.
The International Maritime Organization's International Convention for the Safety of Life at Sea requires the Automatic Identification System (AIS) to be fitted aboard international voyaging ships with gross tonnage (GT) of , and all passenger ships regardless of size. Although AIS transmitters/receivers are generally called transponders they generally transmit autonomously, although coast stations can interrogate class B transponders on smaller vessels for additional information. In addition, navigational aids often have transponders called RACON (radar beacons) designed to make them stand out on a ship's radar screen.
Automotive.
Many modern automobiles have keys with transponders hidden inside the plastic head of the key. The user of the car may not even be aware that the transponder is there, because there are no buttons to press. When a key is inserted into the ignition lock cylinder and turned, the car's computer sends a radio signal to the transponder. Unless the transponder replies with a valid code, the computer will not allow the engine to be started. Transponder keys have no battery; they are energized by the radio signal itself.
Road.
Electronic toll collection systems such as E-ZPass in the eastern United States use RFID transponders to identify vehicles. The Highway 407 in Ontario is one of the world's first completely automated toll highways.
Motorsport.
Transponders are used in motorsport for lap timing purposes. A cable loop is dug into the race circuit near to the start/finish line. Each car has an active transponder with a unique ID code. When the racing car passes the start/finish line the lap time and the racing position is shown on the score board.
Passive and active RFID systems are used in off-road events such as Enduro and Hare and Hounds racing, the riders have a transponder on their person, normally on their arm. When they complete a lap they swipe or touch the receiver which is connected to a computer and log their lap time. The Casimo Group Ltd make a system which does this.
NASCAR uses transponders and cable loops placed at numerous points around the track to determine the lineup during a caution period. This system replaced a dangerous race back to the start-finish line.
Underwater.
Sonar transponders operate under water and are used to measure distance and form the basis of underwater location marking, position tracking and navigation.
Gated communities.
Transponders may also be used by residents to enter their gated communities. However, having more than one transponder causes problems. If a resident's car with simple transponder is parked in the vicinity, any vehicle can come up to the automated gate, triggering the gate interrogation signal, which may get an acceptable response from the resident's car. Such units properly installed might involve beamforming, unique transponders for each vehicle, or simply obliging vehicles to be stored away from the gate.

</doc>
<doc id="41819" url="http://en.wikipedia.org/wiki?curid=41819" title="Transposition">
Transposition

Transposition may refer to:

</doc>
<doc id="41820" url="http://en.wikipedia.org/wiki?curid=41820" title="Transverse redundancy check">
Transverse redundancy check

In telecommunications, a transverse redundancy check (TRC) or vertical redundancy check is a redundancy check for synchronized parallel bits applied once per bit time, across the bit streams. This requires additional parallel channels for the check bit or bits.
The term usually applies to a single parity bit, although it could also be used to refer to a larger Hamming code.
The adjective "transverse" is most often used when it is used in combination with additional error control coding, such as a longitudinal redundancy check. Although parity alone can only detect and not correct errors, it can be part of a system for correcting errors.
An example of a TRC is the parity written to the 9th track of a 9 track tape.

</doc>
<doc id="41821" url="http://en.wikipedia.org/wiki?curid=41821" title="Tree structure">
Tree structure

A tree structure or tree diagram is a way of representing the hierarchical nature of a structure in a graphical form. It is named a "tree structure" because the classic representation resembles a tree, even though the chart is generally upside down compared to an actual tree, with the "root" at the top and the "leaves" at the bottom.
A tree structure is conceptual, and appears in several forms. For a discussion of tree structures in specific fields, see Tree (data structure) for computer science: insofar as it relates to graph theory, see tree (graph theory), or also tree (set theory). Other related pages are listed below.
Terminology and properties.
The tree elements are called "nodes".
The lines connecting elements are called "branches". 
Nodes without children are called leaf nodes, "end-nodes", or "leaves".
Every finite tree structure has a member that has no superior. This member is called the "root" or root node. The root is the starting node. But the converse is not true: infinite tree structures may or may not have a root node.
The names of relationships between nodes model the kinship terminology of family relations. The gender-neutral names "parent" and "child" have largely displaced the older "father" and "son" terminology, although the term "uncle" is still used for other nodes at the same level as the parent.
In the example, "encyclopedia" is the parent of "science" and "culture", its children. "Art" and "craft" are siblings, and children of "culture", which is their parent and thus one of their ancestors. Also, "encyclopedia", as the root of the tree, is the ancestor of "science", "culture", "art" and "craft". Finally, "science", "art" and "craft", as leaves, are ancestors of no other node.
Tree structures can depict all kinds of taxonomic knowledge, such as family trees, the biological evolutionary tree, the evolutionary tree of a language family, the grammatical structure of a language (a key example being S → NP VP, meaning a sentence is a noun phrase and a verb phrase, with each in turn having other components which have other components), the way web pages are logically ordered in a web site, mathematical trees of integer sets, et cetera.
The Oxford English Dictionary records use of both the terms "tree structure" and "tree-diagram" from 1965 in Noam Chomsky's "Aspects of the Theory of Syntax".
In a tree structure there is one and only one path from any point to any other point.
Computer science uses tree structures extensively ("see" Tree (data structure) and telecommunications.)
For a formal definition see set theory, and for a generalization in which children are not necessarily successors, see prefix order.
Representing trees.
There are many ways of visually representing tree structures.
Almost always, these boil down to variations, or combinations,
of a few basic styles:
Classical node-link diagrams.
Classical node-link diagrams, that connect nodes together with line segments.
Nested sets.
Nested sets that use enclosure/containment to show parenthood, examples include TreeMaps and fractal maps.
Layered "icicle" diagrams.
Layered "icicle" diagrams that use alignment/adjacency.
Outlines and tree views.
Lists or diagrams that use indentation, sometimes called "outlines" or "tree views".
Nested parentheses.
((art,craft)culture,science)encyclopedia
or
encyclopedia(culture(art,craft),science)
A correspondence to nested parentheses was first noticed by Sir Arthur Cayley.
Radial trees.
Trees can also be represented radially.
Further reading.
Identification of some of the basic styles of tree structures can be found in:

</doc>
<doc id="41822" url="http://en.wikipedia.org/wiki?curid=41822" title="Troposphere">
Troposphere

The troposphere is the lowest portion of Earth's atmosphere. It contains approximately 75% of the atmosphere's mass and 99% of its water vapour and aerosols.
The average depth of the troposphere is approximately 17 km in the middle latitudes. It is deeper in the tropics, up to 20 km, and shallower near the polar regions, approximately 7 km in winter. The lowest part of the troposphere, where friction with the Earth's surface influences air flow, is the planetary boundary layer. This layer is typically a few hundred meters to 2 km deep depending on the landform and time of day. The border between the troposphere and stratosphere, called the tropopause, is a temperature inversion.
The word troposphere derives from the Greek: "tropos" for "change" reflecting the fact that turbulent mixing plays an important role in the troposphere's structure and behaviour. Most of the phenomena we associate with day-to-day weather occur in the troposphere.
Pressure and temperature structure.
Composition.
The chemical composition of the troposphere is essentially uniform, with the notable exception of water vapor. The source of water vapour is at the surface through the processes of evaporation and transpiration. Furthermore the temperature of the troposphere decreases with height, and saturation vapor pressure decreases strongly as temperature drops, so the amount of water vapor that can exist in the atmosphere decreases strongly with height. Thus the proportion of water vapor is normally greatest near the surface and decreases with height.
Pressure.
The pressure of the atmosphere is maximum at sea level and decreases with higher altitude. This is because the atmosphere is very nearly in hydrostatic equilibrium, so that the pressure is equal to the weight of air above a given point. The change in pressure with height, therefore can be equated to the density with this hydrostatic equation:
where:
Since temperature in principle also depends on altitude, one needs a second equation to determine the pressure as a function of height, as discussed in the next section.*
Temperature.
The temperature of the troposphere generally decreases as altitude increases. The rate at which the temperature decreases, formula_2, is called the environmental lapse rate (ELR). 
The ELR is nothing more than the difference in temperature between the surface and the tropopause divided by the height. The reason for this temperature difference is that most absorption of the sun's energy occurs at the ground which then heats the lower levels of the atmosphere, and the radiation of heat occurs at the top of the atmosphere cooling the earth, this process maintaining the overall heat balance of the earth. 
As parcels of air in the atmosphere rise and fall, they also undergo changes in temperature for reasons described below. The rate of change of the temperature in the parcel may be less than or more than the ELR.
When a parcel of air rises, it expands, because the pressure is lower at higher altitudes. As the air parcel expands, it pushes on the air around it, doing work; but generally it does not gain heat in exchange from its environment, because its thermal conductivity is low (such a process is called adiabatic). Since the parcel does work and gains no heat, it loses energy, and so its temperature decreases. (The reverse, of course, will be true for a sinking parcel of air.) 
Since the heat exchanged formula_3 is related to the entropy change formula_4 by formula_5, the equation governing the temperature as a function of height for a thoroughly mixed atmosphere is 
where "S" is the entropy. The rate at which temperature decreases with height under such conditions is called the adiabatic lapse rate.
For "dry" air, which is approximately an ideal gas, we can proceed further. The adiabatic equation for an ideal gas is 
where formula_8 is the heat capacity ratio (formula_8=7/5, for air). Combining with the equation for the pressure, one arrives at the dry adiabatic lapse rate,
If the air contains water vapor, then cooling of the air can cause the water to condense, and the behavior is no longer that of an ideal gas. If the air is at the saturated vapor pressure, then the rate at which temperature drops with height is called the saturated adiabatic lapse rate. More generally, the actual rate at which the temperature drops with altitude is called the environmental lapse rate. 
In the troposphere, the average environmental lapse rate is a drop of about 6.5 °C for every 1 km (1,000 meters) in increased height.
The environmental lapse rate (the actual rate at which temperature drops with height, formula_11) is not usually equal to the adiabatic lapse rate (or correspondingly, formula_12). If the upper air is warmer than predicted by the adiabatic lapse rate (formula_13), then when a parcel of air rises and expands, it will arrive at the new height at a lower temperature than its surroundings. In this case, the air parcel is denser than its surroundings, so it sinks back to its original height, and the air is stable against being lifted. If, on the contrary, the upper air is cooler than predicted by the adiabatic lapse rate, then when the air parcel rises to its new height it will have a higher temperature and a lower density than its surroundings, and will continue to accelerate upward.
The troposphere is heated from below by latent heat, longwave radiation, and sensible heat. Surplus heating and vertical expansion of the troposphere occurs in the tropics. At middle latitudes, tropospheric temperatures decrease from an average of 15°C at sea level to about -55°C at the tropopause. At the poles, tropospheric temperature only decreases from an average of 0ºC at sea level to about -45°C at the tropopause. At the equator, tropospheric temperatures decrease from an average of 20ºC at sea level to about -70 to -75°C at the tropopause. The troposphere is thinner at the poles and thicker at the equator. The average thickness of the tropical tropopause is roughly 7 kilometers greater than the average tropopause thickness at the poles. 
Tropopause.
The tropopause is the boundary region between the troposphere and the stratosphere. 
Measuring the temperature change with height through the troposphere and the stratosphere identifies the location of the tropopause. In the troposphere, temperature decreases with altitude. In the stratosphere, however, the temperature remains constant for a while and then increases with altitude. The region of the atmosphere where the lapse rate changes from positive (in the troposphere) to negative (in the stratosphere), is defined as the tropopause. Thus, the tropopause is an inversion layer, and there is little mixing between the two layers of the atmosphere.
Atmospheric flow.
The flow of the atmosphere generally moves in a west to east direction. This, however, can often become interrupted, creating a more north to south or south to north flow. These scenarios are often described in meteorology as zonal or meridional. These terms, however, tend to be used in reference to localised areas of atmosphere (at a synoptic scale). A fuller explanation of the flow of atmosphere around the Earth as a whole can be found in the three-cell model.
Zonal flow.
A zonal flow regime is the meteorological term meaning that the general flow pattern is west to east along the Earth's latitude lines, with weak shortwaves embedded in the flow. The use of the word "zone" refers to the flow being along the Earth's latitudinal "zones". This pattern can buckle and thus become a meridional flow.
Meridional flow.
When the zonal flow buckles, the atmosphere can flow in a more longitudinal (or meridional) direction, and thus the term "meridional flow" arises. Meridional flow patterns feature strong, amplified troughs and ridges, with more north-south flow in the general pattern than west-to-east flow.
Three-cell model.
The three cells model attempts to describe the actual flow of the Earth's atmosphere as a whole. It divides the Earth into the tropical (Hadley cell), mid latitude (Ferrel cell), and polar (polar cell) regions, dealing with energy flow and global circulation. Its fundamental principle is that of balance - the energy that the Earth absorbs from the sun each year is equal to that which it loses back into space, but this however is not a balance precisely maintained in each latitude due to the varying strength of the sun in each "cell" resulting from the tilt of the Earth's axis in relation to its orbit. It demonstrates that a pattern emerges to mirror that of the ocean - the tropics do not continue to get warmer because the atmosphere transports warm air poleward and cold air equatorward, the effect of which appears to be that of heat and moisture distribution around the planet.
Synoptic scale observations and concepts.
Forcing.
Forcing is a term used by meteorologists to describe the situation where a change or an event in one part of the atmosphere causes a strengthening change in another part of the atmosphere. It is usually used to describe connections between upper, middle or lower levels (such as upper-level divergence causing lower level convergence in cyclone formation), but can sometimes also be used to describe such connections over distance rather than height alone. In some respects, teleconnections could be considered a type of forcing.
Divergence and convergence.
An area of convergence is one in which the total mass of air is increasing with time, resulting in an increase in pressure at locations below the convergence level (recall that atmospheric pressure is just the total weight of air above a given point). Divergence is the opposite of convergence - an area where the total mass of air is decreasing with time, resulting in falling pressure in regions below the area of divergence. Where divergence is occurring in the upper atmosphere, there will be air coming in to try to balance the net loss of mass (this is called the principle of mass conservation), and there is a resulting upward motion (positive vertical velocity). Another way to state this is to say that regions of upper air divergence are conducive to lower level convergence, cyclone formation, and positive vertical velocity. Therefore, identifying regions of upper air divergence is an important step in forecasting the formation of a surface low pressure area.

</doc>
<doc id="41823" url="http://en.wikipedia.org/wiki?curid=41823" title="Tropospheric wave">
Tropospheric wave

In telecommunication, a tropospheric wave is a radio wave that is propagated by reflection from a place of abrupt change in the dielectric constant, or its gradient, in the troposphere. In some cases, a ground wave may be so altered that new components appear to arise from reflection in regions of rapidly changing dielectric constant. When these components are distinguishable from the other components, they are called "tropospheric waves."

</doc>
<doc id="41825" url="http://en.wikipedia.org/wiki?curid=41825" title="Trunk">
Trunk

Trunk may refer to:
In biology:
Containers:
Other uses:

</doc>
<doc id="41826" url="http://en.wikipedia.org/wiki?curid=41826" title="Trusted computing base">
Trusted computing base

The trusted computing base (TCB) of a computer system is the set of all hardware, firmware, and/or software components that are critical to its security, in the sense that bugs or vulnerabilities occurring inside the TCB might jeopardize the security properties of the entire system. By contrast, parts of a computer system outside the TCB must not be able to misbehave in a way that would leak any more privileges than are granted to them in accordance to the security policy.
The careful design and implementation of a system's trusted computing base is paramount to its overall security. Modern operating systems strive to reduce the size of the TCB so that an exhaustive examination of its code base (by means of manual or computer-assisted software audit or program verification) becomes feasible.
Definition and characterization.
The term trusted computing base goes back to Rushby, who defined it as the combination of kernel and trusted processes. The latter refers to processes which are allowed to violate the system's access-control rules.
In the classic paper "Authentication in Distributed Systems: Theory and Practice" Lampson et al. define the TCB of a computer system as simply
Both definitions, while clear and convenient, are neither theoretically exact nor intended to be, as e.g. a network server process under a UNIX-like operating system might fall victim to a security breach and compromise an important part of the system's security, yet is not part of the operating system's TCB. The Orange Book, another classic computer security literature reference, therefore provides a more formal definition of the TCB of a computer system, as
The Orange Book further explains that
In other words, a given piece of hardware or software is a part of the TCB if and only if it has been designed to be a part of the mechanism that provides its security to the computer system. In operating systems, this typically consists of the kernel (or microkernel) and a select set of system utilities (for example, setuid programs and daemons in UNIX systems). In programming languages that have security features designed in such as Java and E, the TCB is formed of the language runtime and standard library.
Properties of the TCB.
Predicated upon the security policy.
It should be pointed out that as a consequence of the above Orange Book definition, the boundaries of the TCB depend closely upon the specifics of how the security policy is fleshed out. In the network server example above, even though, say, a Web server that serves a multi-user application is not part of the operating system's TCB, it has the responsibility of performing access control so that the users cannot usurp the identity and privileges of each other. In this sense, it definitely is part of the TCB of the larger computer system that comprises the UNIX server, the user's browsers and the Web application; in other words, breaching into the Web server through e.g. a buffer overflow may not be regarded as a compromise of the operating system proper, but it certainly constitutes a damaging exploit on the Web application.
This fundamental relativity of the boundary of the TCB is exemplifed by the concept of the target of evaluation (TOE) in the Common Criteria security process: in the course of a Common Criteria security evaluation, one of the first decisions that must be made is the boundary of the audit in terms of the list of system components that will come under scrutiny.
A prerequisite to security.
Systems that don't have a trusted computing base as part of their design do not provide security of their own: they are only secure insofar as security is provided to them by external means (e.g. a computer sitting in a locked room without a network connection may be considered secure depending on the policy, regardless of the software it runs). This is because, as David J. Farber et al. put it, "[i]n a computer system, the integrity of lower layers is typically treated as axiomatic by higher layers". As far as computer security is concerned, reasoning about the security properties of a computer system requires being able to make sound assumptions about what it can, and more importantly, cannot do; however, barring any reason to believe otherwise, a computer is able to do everything that a general Von Neumann machine can. This obviously includes operations that would be deemed contrary to all but the simplest security policies, such as divulging an email or password that should be kept secret; however, barring special provisions in the architecture of the system, there is no denying that the computer "could be programmed" to perform these undesirable tasks.
These special provisions that aim at preventing certain kinds of actions from being executed, in essence, constitute the trusted computing base. For this reason, the Orange Book (still a reference on the design of secure operating systems design as of 2007[ [update]]) characterizes the various security assurance levels that it defines mainly in terms of the structure and security features of the TCB.
Software parts of the TCB need to protect themselves.
As outlined by the aforementioned Orange Book, software portions of the trusted computing base need to protect themselves against tampering to be of any effect. This is due to the von Neumann architecture implemented by virtually all modern computers: since machine code can be processed as just another kind of data, it can be read and overwritten by any program barring special memory management provisions that subsequently have to be treated as part of the TCB. Specifically, the trusted computing base must at least prevent its own software from being written to.
In many modern CPUs, the protection of the memory that hosts the TCB is achieved by adding in a specialized piece of hardware called the memory management unit (MMU), which is programmable by the operating system to allow and deny access to specific ranges of the system memory to the programs being run. Of course, the operating system is also able to disallow such programming to the other programs. This technique is called supervisor mode; compared to more crude approaches (such as storing the TCB in ROM, or equivalently, using the Harvard architecture), it has the advantage of allowing the security-critical software to be upgraded in the field, although allowing secure upgrades of the trusted computing base poses bootstrap problems of its own.
Trusted vs. trustworthy.
As stated above, trust in the trusted computing base is required to make any progress in ascertaining the security of the computer system. In other words, the trusted computing base is “trusted” first and foremost in the sense that it "has" to be trusted, and not necessarily that it is trustworthy. Real-world operating systems routinely have security-critical bugs discovered in them, which attests of the practical limits of such trust.
The alternative is formal software verification, which uses mathematical proof techniques to show the absence of bugs. Researchers at NICTA and its spinout Open Kernel Labs have recently performed such a formal verification of , a member of the L4 microkernel family, proving functional correctness of the C implementation of the kernel.
This makes seL4 the first operating-system kernel which closes the gap between trust and trustworthiness, assuming the mathematical proof and the compiler are free from error.
TCB size.
Due to the aforementioned need to apply costly techniques such as formal verification or manual review, the size of the TCB has immediate consequences on the economics of the TCB assurance process, and the trustworthiness of the resulting product (in terms of the mathematical expectation of the number of bugs not found during the verification or review). In order to reduce costs and security risks, the TCB should therefore be kept as small as possible. This is a key argument in the debate opposing microkernels to monolithic kernels.
Examples.
AIX materializes the trusted computing base as an optional component in its install-time package management system.

</doc>
<doc id="41827" url="http://en.wikipedia.org/wiki?curid=41827" title="Turnkey">
Turnkey

A turnkey or a turnkey project (also spelled turn-key) is a type of project that is constructed so that it could be sold to any buyer as a completed product. This is contrasted with build to order, where the constructor builds an item to the buyer's exact specifications, or when an incomplete product is sold with the assumption that the buyer would complete it.
A turnkey project or contract as described by Duncan Wallace (1984) is:
 …. a contract where the essential design emanates from, or is supplied by, the Contractor and not the owner, so that the legal responsibility for the design, suitability and performance of the work after completion will be made to rest … with the contractor …. 'Turnkey' is treated as merely signifying the design responsibility as the contractor's.
A turnkey computer system is a complete computer including hardware, operating system and application(s) designed and sold to satisfy specific business requirements.
Common usage.
Turnkey refers to something that is ready for immediate use, generally used in the sale or supply of goods or services. The word is a reference to the fact that the customer, upon receiving the product, just needs to turn the ignition key to make it operational. Turnkey is often used to describe a home built on the developer's land with the developer's financing ready for the customer to move in. If a contractor builds a "turnkey home" they frame the structure and finish the interior. Everything is completed down to the cabinets and carpet. "Turnkey" is commonly used in the construction industry, for instance, in which it refers to the bundling of materials and labour by sub-contractors. 'Turnkey' is also commonly used in motorsports to describe a car being sold with drivetrain (engine, transmission, etc.) to contrast with a vehicle sold without one so that other components may be re-used.
Similarly, this term may be used to advertise the sale of an established business, including all the equipment necessary to run it, or by a business-to-business supplier providing complete packages for business start-up. An example would be the creation of a "turnkey hospital" which would be building a complete medical centre with installed medical equipment.
Specific usage.
 This often includes a computer with pre-installed software, various types of hardware, and accessories. Such packages are commonly called appliances. A website with a ready-made solutions and some configurations is called a turnkey website. 
Turnkey products are synonymous to "off-the-shelf" solutions and not customized.
 The turnkey process includes all of the steps involved to open a location including the site selection, negotiations, space planning, construction coordination and complete installation. Turnkey real estate also refers to a type of investment. This process includes the purchase, construction or rehab (of an existing site), the leasing out to tenants, and then the sale of the property to a buyer. The buyer is purchasing an investment property which is producing a stream of income.

</doc>
<doc id="41828" url="http://en.wikipedia.org/wiki?curid=41828" title="Two-out-of-five code">
Two-out-of-five code

In telecommunication, a two-out-of-five code is an m of n code that provides exactly ten possible combinations, and thus is popular for representing decimal digits using five bits. There are ways to assign weights to each bit such that the set bits sum to the desired value, with an exception for zero.
According to Federal Standard 1037C:
The weights give a unique encoding for most digits, but allow two encodings for 3: 0+3 or 10010 and 1+2 or 01100. The former is used to encode the digit 3, and the latter is used to represent the otherwise unrepresentable zero.
The IBM 7070, IBM 7072, and IBM 7074 computers used this code to represent each of the ten decimal digits in a machine word, although they numbered the bit positions 0-1-2-3-4, rather than with weights. Each word also had a sign flag, encoded using a two-out-of-three code, that could be A Alphanumeric, − Minus, or + Plus. When copied to a digit, the three bits were placed in bit positions 0-3-4. (Thus producing the numeric values 3, 6 and 9, respectively.)
A variant is the U.S. Post Office POSTNET barcode, used to represent the ZIP+4 code for automated mail sorting and routing equipment. This uses two tall bars as "ones" and three short bars as "zeros". Here, the weights assigned to the bit positions are 7-4-2-1-0. Again, zero is encoded specially, using the 7+4 combination (binary 11000) that would naturally encode 11. This method was also used in North American telephone Multi-frequency and crossbar switching systems.
The USPS Postal Alpha Numeric Encoding Technique (PLANET) uses the same weights, but with the opposite bar-height convention.
The following table represents decimal digits from 0 to 9 in various two-out-of-five code systems:
The limit on the number of bits set is similar to, but strictly stronger than, a parity check. All constant-weight codes, including the two-out-of-five code, can not only detect any single-bit error, but also detect any unidirectional errors -- any case where all errors in a codeword are of a single type (0→1 or 1→0).

</doc>
<doc id="41829" url="http://en.wikipedia.org/wiki?curid=41829" title="Type 1 product">
Type 1 product

In cryptography, a Type 1 product is a device or system certified by the National Security Agency (NSA) for use in cryptographically securing classified U.S. Government information.
Type 1 certification is a rigorous process that includes testing and formal analysis of (among other things) cryptographic security, functional security, tamper resistance, emissions security (EMSEC/TEMPEST), and security of the product manufacturing and distribution process.
For a historically oriented list of NSA encryption products (most of them Type 1), see "NSA encryption systems". For algorithms that NSA has participated in the development of, see "NSA cryptography".
Types 1 through 4 are defined in the National Information Assurance Glossary (CNSSI No. 4009) which defines Type 1, Type 2, Type 3, and Type 4 products and keys.
A Type 1 product is defined as: 

</doc>
<doc id="41830" url="http://en.wikipedia.org/wiki?curid=41830" title="Type 2 product">
Type 2 product

In cryptography, Type 2 products are unclassified cryptographic equipment, assemblies, or components, endorsed by the National Security Agency (NSA), for use in telecommunications and automated information systems for the protection of national security information. 
"Note:" The term refers only to products, and not to information, key, services, or controls. Type 2 products may not be used for classified information, but contain classified NSA algorithms (e.g. CORDOBA) that distinguish them from products containing unclassified algorithms like DES. Type 2 products are subject to export restrictions in accordance with the International Traffic in Arms Regulations.

</doc>
<doc id="41831" url="http://en.wikipedia.org/wiki?curid=41831" title="Telephony">
Telephony

Telephony ( ) is the field of technology involving the development, application, and deployment of telecommunication services for the purpose of electronic transmission of voice, fax, or data, between distant parties. The history of telephony is intimately linked to the invention and development of the telephone.
Telephony is commonly referred to as the construction or operation of telephones and telephonic systems and as a system of telecommunications in which telephonic equipment is employed in the transmission of speech or other sound between points, with or without the use of wires. The term is also used frequently to refer to computer hardware, software, and computer network systems, that perform functions traditionally performed by telephone equipment. In this context the technology is specifically referred to as Internet telephony, or voice over Internet Protocol (VoIP).
Overview.
The first telephones were connected directly in pairs. Each user had a separate telephone wired to the locations he might wish to reach. This quickly became inconvenient and unmanageable when people wanted to communicate with more than a few people. The inventions of the telephone exchange provided the solution for establishing telephone connections with any other telephone in service in the local area. Each telephone was connected to the exchange via one wire pair, the local loop. Nearby exchanges in other service areas were connected with trunk lines and long distance service could be established by relaying the calls through multiple exchanges.
Initially the switchboards were manually operated by an attendant, a "switchboard operator". When a customer cranked a handle on the telephone, it turned on an indicator on the board in front of the operator who would plug the operator headset into that jack and offer service. The caller had to ask for the called party by name, later by number, and the operator connected one end of a circuit into the called party jack to alert them. If the called station answered the operator disconnected their headset and complete the station-to-station circuit. Trunk calls were made with the assistance of other operators at other exchangers in the network.
In modern times, most telephones are plugged into telephone jacks. The jacks are connected by inside wiring to a drop wire which connects the building to a cable. Cables usually bring a large number of drop wires from all over a district access network to one wire center or telephone exchange. When a telephone user wants to make a telephone call, equipment at the exchange examines the dialed telephone number and connects that telephone line to another in the same wire center, or to a trunk to a distant exchange. Most of the exchanges in the world are interconnected through a system of larger switching systems, forming the public switched telephone network (PSTN).
After the middle of the 20th century, fax and data became important secondary users of the network created to carry voices, and late in the century, parts of the network were upgraded with ISDN and DSL to improve handling of such traffic.
Today, telephony uses digital technology (digital telephony) in the provisioning of telephone services and systems. Telephone calls can be provided digitally, but may be restricted to cases in which the last mile is digital, or where the conversion between digital and analog signals takes place inside the telephone. This advancement has reduced costs in communication, and improved the quality of voice services. The first implementation of this, ISDN, permitted all data transport from end-to-end speedily over telephone lines. This service was later made much less important due to the ability to provide digital services based on the IP protocol.
Since the advent of personal computer technology in the 1980s, computer telephony integration has progressively provided more sophisticated telephony services, initiated and controlled by the computer, such as making and receiving voice, fax, and data calls with telephone directory services and caller identification. The integration of telephony software and computer systems is a major development in the evolution of the automated office. The term is used in describing the computerized services of call centers, such as those that direct your phone call to the right department at a business you're calling. It's also sometimes used to describe the ability to use your personal computer to initiate and manage phone calls (in which case you can think of your computer as your personal call center). CTI is not a new concept and has been used in the past in large telephone networks, but only dedicated call centers could justify the costs of the required equipment installation. Primary telephone service providers are offering information services such as automatic number identification, which is a telephone service architecture that separates CTI services from call switching and will make it easier to add new services. Dialed Number Identification Service (DNIS) on a scale is wide enough for its implementation to bring real value to business or residential telephone usage. A new generation of applications (middleware) is being developed as a result of standardization and availability of low cost computer telephony links.
Recent developments.
The term's scope has been broadened with the advent of the different new communication technologies. In its broadest sense, the terms encompasses phone communication, Internet calling, mobile communication, faxing, voicemail and video conferencing. Telephony's initial idea returns to POTS, (an acronym for "plain old telephone service") technically called the PSTN (public-switched telephone network).
This system is being fiercely challenged by and to a great extent yielding to Voice over IP (VoIP) technology, which is also commonly referred to as IP Telephony and Internet Telephony. IP telephony is a modern form of telephony which uses the TCP/IP protocol popularized by the Internet to transmit digitized voice data. Also, unlike traditional phone service, IP telephony service is relatively unregulated by government. In the United States, the Federal Communications Commission (FCC) regulates phone-to-phone connections, but says they do not plan to regulate connections between a phone user and an IP telephony service provider.Using the Internet, calls travel as packets of data on shared lines, avoiding the tolls of the PSTN. The challenge in IP telephony is to deliver the voice, fax, or video packets in a dependable flow to the user. Much of IP telephony focuses on that challenge.
Digital telephony.
Starting with the introduction of the transistor, invented in 1947 by Bell Laboratories, to amplification and switching circuits in the 1950s, and through development of computer-based electronic switching systems, the public switched telephone network (PSTN) has gradually evolved towards automation and digitization of signaling and audio transmissions.
Digital telephony is the use of digital electronics in the operation and provisioning of telephony systems and services. Since the 1960s a digital core network has replaced the traditional analog transmission and signaling systems, and much of the access network has also been digitized.
Digital telephony has dramatically improved the capacity, quality, and cost of the network. End-to-end analog telephone networks were first modified in the early 1960s by upgrading transmission networks with Digital Signal 1 (DS1/T1) carrier systems, designed to support the basic 3 kHz voice channel by sampling the bandwidth-limited analog voice signal and encoding using PCM. While digitization allows wideband voice on the same channel, the improved quality of a wider analog voice channel did not find a large market in the PSTN.
Later transmission methods such as SONET and fiber optic transmission further advanced digital transmission. Although analog carrier systems existed that multiplexed multiple analog voice channels onto a single transmission medium, digital transmission allowed lower cost and more channels multiplexed on the transmission medium. Today the end instrument often remains analog but the analog signals are typically converted to digital signals at the serving area interface (SAI), central office (CO), or other aggregation point. Digital loop carriers (DLC) place the digital network ever closer to the customer premises, relegating the analog local loop to legacy status.
IP telephony.
A specialization of digital telephony, Internet Protocol (IP) telephony involves the application of digital networking technology that were the foundation to the Internet to create, transmit, and receive telecommunications sessions over computer networks. Internet telephony is commonly known as voice over Internet Protocol (VoIP), reflecting the principle, but it has been referred with many other terms. VoIP has proven to be a disruptive technology that is rapidly replacing traditional telephone infrastructure technologies. As of January 2005, up to 10% of telephone subscribers in Japan and South Korea have switched to this digital telephone service. A January 2005 "Newsweek" article suggested that Internet telephony may be "the next big thing." As of 2006 many VoIP companies offer service to consumers and businesses.
IP telephony uses an Internet connection and hardware IP Phones, analog telephone adapters, or softphone computer applications to transmit conversations encoded as data packets. In addition to replacing plain old telephone service (POTS), IP telephony services are also competing with mobile phone services by offering free or lower cost connections via WiFi hotspots. VoIP is also used on private networks which may or may not have a connection to the global telephone network.
Social impact research.
Direct person-to-person communication includes non-verbal cues expressed in facial and other bodily articulation, that cannot be transmitted in traditional voice telephony. Video telephony restores such interactions to varying degrees. Social Context Cues Theory is a model to measure the success of different types of communication in maintaining the non-verbal cues present in face-to-face interactions. The research examines many different cues, such as the physical context, different facial expressions, body movements, tone of voice, touch and smell.
Various communication cues are lost with the usage of the telephone. The communicating parties are not able to identify the body movements, and lack touch and smell. Although this diminished ability to identify social cues is well known, Wiesenfeld, Raghuram, and Garud point out that there is a value and efficiency to the type of communication for different tasks. They examine work places in which different types of communication, such as the telephone, are more useful than face-to-face interaction.
The expansion of communication to mobile telephone service has created a different filter of the social cues than the land-line telephone. The use of instant messaging, such as "texting", on mobile telephones has created a sense of community. In "The Social Construction of Mobile Telephony" it is suggested that each phone call and text message is more than an attempt to converse. Instead, it is a gesture which maintains the social network between family and friends. Although there is a loss of certain social cues through telephones, mobile phones bring new forms of expression of different cues that are understood by different audiences. New language additives attempt to compensate for the inherent lack of non-physical interaction.

</doc>
<doc id="41832" url="http://en.wikipedia.org/wiki?curid=41832" title="U interface">
U interface

The U interface or U reference point is a Basic Rate Interface (BRI) in the local loop of an Integrated Services Digital Network (ISDN). It is characterized by the use of a 2-wire transmission system that connects the network termination type 1 (NT1) on the customer's premises and the line termination (LT) in the carrier's local exchange. It is not as distance sensitive as a service using an S interface or T interface.
In America, the NT1 is customer premises equipment (CPE) which is purchased and maintained by the user, which makes the U interface a User–network interface (UNI). The American variant is specified by the American National Standards Institute (ANSI) in T1.601. In Europe, the NT1 belongs to the network operator, so the user doesn't have direct access to the U interface. The European variant is specified by the European Telecommunications Standards Institute (ETSI) in recommendation ETR 080. The ITU-T has issued recommendations G.960 and G.961 with world-wide scope, encompassing both the European and American variants of the U interface.
Logical interface.
Like all other ISDN basic rate interfaces, the U interface carries two B (bearer) channels at 64 kbit/s and one D (data) channel at 16 kbit/s for a combined bitrate of 144 kbit/s (2B+D).
Duplex transmission.
While in a four-wire interface such as the ISDN S and T-interfaces one wire pair is available for each direction of transmission, a two-wire interface needs to implement both directions on a single wire pair. To that end, ITU-T recommendation G.961 specifies two duplex transmission technologies for the ISDN U interface, either of which shall be used: Echo cancellation (ECH) and Time Compression Multiplex (TCM).
Echo cancellation (ECH).
When a transmitter applies a signal to the wire-pair, parts of the signal will be reflected as a result of imperfect balance of the hybrid and because of impedance discontinuities on the line. These reflections return to the transmitter as an echo and are indistinguishable from a signal transmitted at the far end. In the echo cancellation (ECH) scheme, the transmitter locally simulates the echo it expects to receive, and subtracts it from the received signal.
Time Compression Multiplex (TCM).
The Time Compression Multiplex (TCM) duplex method, also referred to as "burst mode", solves the echo problem indirectly. The line is operated at a rate at least twice the signal rate and both ends of the line take turns transmitting, in a time-division duplex fashion.
Line Systems.
ITU-T G.961 specifies four line systems for the ISDN U interface: MMS43, 2B1Q, TCM, and SU32. All line systems except TCM use echo cancellation for duplex operation. The American standard ANSI T1.601 specifies the 2B1Q line system, the European ETSI TR 080 recommendation specifies 2B1Q and MMS43.
MMMS43 (4B3T).
The Modified Monitoring State Code mapping 4 bits into 3 ternary symbols (MMS43), which is also referred to as 4B3T (four binary, three ternary) is a line system used in Europe and elsewhere in the world. 4B3T is a "block code" that uses Return-to-Zero states on the line. 4B3T converts each group of 4 data bits into 3 "ternary" line signal states (3 symbols). Echo cancellation techniques allow full-duplex operation on the line.
MMS43 is defined in Appendix I of G.961, Annex B of ETR 080, and other national standards, like Germany's 1TR220. 4B3T can be transmitted reliably at up to over cable or up to over cable. An internal termination impedance of is presented to the line at each end of the U-interface.
A 1 ms frame carrying 144 bits of 2B+D data is mapped to 108 ternary symbols. These symbols are scrambled, with different scrambling codes for the two transmission directions, in order reduce correlation between transmitted and received signal. To this frame, an 11-symbol preamble and a symbol from the CL channel are added, yielding a frame size of 120 ternary symbols and a symbol rate of 120 kilobaud. The CL channel is used to request activation or deactivation of a loopback in either the NT1 or a line regenerator.
In 4B3T coding, there are three states presented to line: a positive pulse (+), a negative pulse (-), or a zero-state (no pulse: 0). An analogy here is that operation is similar to B8ZS or HDB3 in T1/E1 systems, except that there is an actual gain in the information rate by coding 24=16 possible binary states to one of 33=27 ternary states. This added redundancy is used to generate a zero DC-bias signal.
One requirement for line transmission is that there should be no DC build-up on the line, so the accumulated DC build-up is monitored and the codewords are chosen accordingly. Of the 16 binary information words, some are always mapped to a DC-component free (ternary) code word, while others can be mapped to either one of two code words, one with a positive and the other with a negative DC-component. In the latter case, the transmitter chooses whether to send the code-word with negative or positive DC-component based on the accumulated DC-offset.
2B1Q.
2B1Q coding is the standard used in North America, Italy, and Switzerland. 2B1Q means that two bits are combined to form a single Quaternary line state (symbol). 2B1Q combines two bits at a time to be represented by one of four signal levels on the line. Echo cancellation techniques allow full-duplex operation on the line.
2B1Q coding is defined in Appendix II of G.961, ANSI T1.601, and Annex A of ETR 080. It can operate at distances up to about 18,000 feet () with loss up to . An internal termination impedance of 135 ohms is presented to the line at each end of the U-interface.
A 1.5 ms frame carrying 216 scrambled bits of 2B+D data is mapped to 108 quaternary symbols. To this frame, a 9-symbol preamble and 3 symbols from the CL channel are added, yielding a frame size of 120 quaternary symbols and a symbol rate of 80 kilobaud. The CL channel is used for communication between LT and NT1, a 12-bit cyclic redundancy check (CRC), and various other physical layer functions. The CRC covers one 12 ms multiframe (8×1.5 ms frames).
TCM / AMI.
The TCM / AMI ISDN line system, also referred to as TCM-ISDN, is used by Nippon Telegraph and Telephone in its "INS-Net 64" service.
Appendix III of G.961 specifies a line system based on the Time Compression Multiplex (TCM) duplex method and an alternate mark inversion (AMI) line code. The AMI line code maps one input bit to one ternary symbol. Like with MMS43, the ternary symbol can either be a positive (+), zero (0), or negative (-) voltage. A 0 bit is represented by a zero voltage, while a 1 bit is alternatingly represented by a positive and a negative voltage, resulting in a DC-bias free signal. In a 2.5 ms interval, each side can send a 1.178 ms frame representing 360 bits of 2B+D data. To the 2B+D data, an 8-bit preamble, 8 bits from the CL channel, as well as a parity bit are added, yielding a frame size of 377 bits and a baud rate of 320 kilobaud. The CL channel is used for operations and maintenance, as well transmitting a 12-bit CRC covering 4 frames.
SU32.
Appendix IV of G.961 specifies a line system based on echo cancellation and a substitutional 3B2T (SU32) line code, which maps three bits into 2 ternary symbols. As with MMS43 and AMI, the ternary symbol can either be a positive (+), zero (0), or negative (-) voltage. The mapping from 23=8 to 32=9 symbols leaves one unused symbol. When two subsequent input (binary) information words are identical, the (ternary) code word is substituted by the unused code word. A 0.75 ms frame carrying 108 bits of 2B+D data is mapped to 72 ternary symbols. To this frame, a 6-symbol preamble, one CRC symbol, and 2 symbols from the CL channel are added, yielding a frame size of 81 ternary symbols and a symbol rate of 108 kilobaud. The CL channel is used for supervisory and maintenance functions between the LT and NT1. The 15-bit CRC covers 16 frames.

</doc>
<doc id="41833" url="http://en.wikipedia.org/wiki?curid=41833" title="Unavailability">
Unavailability

Unavailability is the probability that an item will not operate correctly at a given time and under specified conditions. It opposes availability.
Numerical values associated with the calculation of availability are often awkward, consisting of a series of 9s before reaching any significant numerical information (e.g. 0.9999999654). For this reason, it is more convenient to use the complement measure of availability, namely, unavailability. Expressed mathematically, unavailability is 1 minus the availability. Therefore, a system with availability 0.9999999654 is more concisely described as having an unavailability of 3.46E-8. 
Unavailability may be expressed mathematically as the ratio,
where MTTR is the mean time to repair, and MTTF is the mean time to failure. Alternatively, this can be written as
where λ' is the critical failure rate (λ without the tick is the total failure rate) and μ' is the critical repair rate (rate at which critical failures are repaired).
In telecommunication, an unavailability is an expression of the degree to which a system, subsystem, or equipment is not operable and not in a committable state at the start of a mission, when the mission is called for at an unknown, i.e. random, time. The conditions determining operability and committability must be specified.
References.
 

</doc>
<doc id="41834" url="http://en.wikipedia.org/wiki?curid=41834" title="Uninterruptible power supply">
Uninterruptible power supply

An uninterruptible power supply, also uninterruptible power source, UPS or battery/flywheel backup, is an electrical apparatus that provides emergency power to a load when the input power source, typically mains power, fails. A UPS differs from an auxiliary or emergency power system or standby generator in that it will provide near-instantaneous protection from input power interruptions, by supplying energy stored in batteries, supercapacitors, or flywheels. The on-battery runtime of most uninterruptible power sources is relatively short (only a few minutes) but sufficient to start a standby power source or properly shut down the protected equipment.
A UPS is typically used to protect hardware such as computers, data centers, telecommunication equipment or other electrical equipment where an unexpected power disruption could cause injuries, fatalities, serious business disruption or data loss. UPS units range in size from units designed to protect a single computer without a video monitor (around 200 volt-ampere rating) to large units powering entire data centers or buildings. The world's largest UPS, the 46-megawatt Battery Electric Storage System (BESS), in Fairbanks, Alaska, powers the entire city and nearby rural communities during outages.
Common power problems.
The primary role of any UPS is to provide short-term power when the input power source fails. However, most UPS units are also capable in varying degrees of correcting common utility power problems:
UPS units are divided into categories based on which of the above problems they address, and some manufacturers categorize their products in accordance with the number of power-related problems they address.
Technologies.
The three general categories of modern UPS systems are "on-line", "line-interactive" or "standby". An on-line UPS uses a "double conversion" method of accepting AC input, rectifying to DC for passing through the rechargeable battery (or battery strings), then inverting back to 120 V/230 V AC for powering the protected equipment. A line-interactive UPS maintains the inverter in line and redirects the battery's DC current path from the normal charging mode to supplying current when power is lost. In a standby ("off-line") system the load is powered directly by the input power and the backup power circuitry is only invoked when the utility power fails. Most UPS below 1 kVA are of the line-interactive or standby variety which are usually less expensive.
For large power units, Dynamic Uninterruptible Power Supplies (DUPS) are sometimes used. A synchronous motor/alternator is connected on the mains via a choke. Energy is stored in a flywheel. When the mains power fails, an eddy-current regulation maintains the power on the load as long as the flywheel's energy is not exhausted. DUPS are sometimes combined or integrated with a diesel generator that is turned on after a brief delay, forming a diesel rotary uninterruptible power supply (DRUPS).
A fuel cell UPS has been developed in recent years using hydrogen and a fuel cell as a power source, potentially providing long run times in a small space.
Offline/Standby.
The offline/standby UPS (SPS) offers only the most basic features, providing surge protection and battery backup. The protected equipment is normally connected directly to incoming utility power. When the incoming voltage falls below or rises above a predetermined level the SPS turns on its internal DC-AC inverter circuitry, which is powered from an internal storage battery. The UPS then mechanically switches the connected equipment on to its DC-AC inverter output. The switchover time can be as long as 25 milliseconds depending on the amount of time it takes the standby UPS to detect the lost utility voltage. The UPS will be designed to power certain equipment, such as a personal computer, without any objectionable dip or brownout to that device.
Line-interactive.
The line-interactive UPS is similar in operation to a standby UPS, but with the addition of a multi-tap variable-voltage autotransformer. This is a special type of transformer that can add or subtract powered coils of wire, thereby increasing or decreasing the magnetic field and the output voltage of the transformer. This is also known as a "Buck–boost transformer".
This type of UPS is able to tolerate continuous undervoltage brownouts and overvoltage surges without consuming the limited reserve battery power. It instead compensates by automatically selecting different power taps on the autotransformer. Depending on the design, changing the autotransformer tap can cause a very brief output power disruption, which may cause UPSs equipped with a power-loss alarm to "chirp" for a moment.
This has become popular even in the cheapest UPSs because it takes advantage of components already included. The main 50/60 Hz transformer used to convert between line voltage and battery voltage needs to provide two slightly different turns ratios: One to convert the battery output voltage (typically a multiple of 12 V) to line voltage, and a second one to convert the line voltage to a slightly higher battery charging voltage (such as a multiple of 14 V). The difference between the two voltages is because charging a battery requires a delta voltage (up to 13–14 V for charging a 12 V battery). Furthermore, it is easier to do the switching on the line-voltage side of the transformer because of the lower currents on that side.
To gain the "buck/boost" feature, all that is required is two separate switches so that the AC input can be connected to one of the two primary taps, while the load is connected to the other, thus using the main transformer's primary windings as an autotransformer. The battery can still be charged while "bucking" an overvoltage, but while "boosting" an undervoltage, the transformer output is too low to charge the batteries.
Autotransformers can be engineered to cover a wide range of varying input voltages, but this requires more taps and increases complexity, and expense of the UPS. It is common for the autotransformer to cover a range only from about 90 V to 140 V for 120 V power, and then switch to battery if the voltage goes much higher or lower than that range.
In low-voltage conditions the UPS will use more current than normal so it may need a higher current circuit than a normal device. For example to power a 1000-W device at 120 V, the UPS will draw 8.33 A. If a brownout occurs and the voltage drops to 100 V, the UPS will draw 10 A to compensate. This also works in reverse, so that in an overvoltage condition, the UPS will need less current.
Online/double-conversion.
In an online UPS, the batteries are always connected to the inverter, so that no power transfer switches are necessary. When power loss occurs, the rectifier simply drops out of the circuit and the batteries keep the power steady and unchanged. When power is restored, the rectifier resumes carrying most of the load and begins charging the batteries, though the charging current may be limited to prevent the high-power rectifier from overheating the batteries and boiling off the electrolyte. The main advantage of an on-line UPS is its ability to provide an "electrical firewall" between the incoming utility power and sensitive electronic equipment.
The online UPS is ideal for environments where electrical isolation is necessary or for equipment that is very sensitive to power fluctuations. Although once previously reserved for very large installations of 10 kW or more, advances in technology have now permitted it to be available as a common consumer device, supplying 500 W or less. The initial cost of the online UPS may be higher, but its total cost of ownership is generally lower due to longer battery life. The online UPS may be necessary when the power environment is "noisy", when utility power sags, outages and other anomalies are frequent, when protection of sensitive IT equipment loads is required, or when operation from an extended-run backup generator is necessary.
The basic technology of the online UPS is the same as in a standby or line-interactive UPS. However it typically costs much more, due to it having a much greater current AC-to-DC battery-charger/rectifier, and with the rectifier and inverter designed to run continuously with improved cooling systems. It is called a "double-conversion" UPS due to the rectifier directly driving the inverter, even when powered from normal AC current.
Other designs.
Hybrid topology / double conversion on demand.
These hybrid Rotary UPS designs do not have official designations, although one name used by HP and Eaton is "double conversion on demand". This style of UPS is targeted towards high-efficiency applications while still maintaining the features and protection level offered by double conversion.
A hybrid (double conversion on demand) UPS operates as an off-line/standby UPS when power conditions are within a certain preset window. This allows the UPS to achieve very high efficiency ratings. When the power conditions fluctuate outside of the predefined windows, the UPS switches to online/double-conversion operation. In double-conversion mode the UPS can adjust for voltage variations without having to use battery power, can filter out line noise and control frequency. Examples of this hybrid/double conversion on demand UPS design are the HP R8000, HP R12000, HP RP12000/3 and the Eaton BladeUPS.
Ferro-resonant.
Ferro-resonant units operate in the same way as a standby UPS unit; however, they are online with the exception that a ferro-resonant transformer is used to filter the output. This transformer is designed to hold energy long enough to cover the time between switching from line power to battery power and effectively eliminates the transfer time. Many ferro-resonant UPSs are 82–88% efficient (AC/DC-AC) and offer excellent isolation.
The transformer has three windings, one for ordinary mains power, the second for rectified battery power, and the third for output AC power to the load.
This once was the dominant type of UPS and is limited to around the 150 kVA range. These units are still mainly used in some industrial settings (oil and gas, petrochemical, chemical, utility, and heavy industry markets) due to the robust nature of the UPS. Many ferro-resonant UPSs utilizing controlled ferro technology may not interact with power-factor-correcting equipment.
DC power.
A UPS designed for powering DC equipment is very similar to an online UPS, except that it does not need an output inverter. Also, if the UPS's battery voltage is matched with the voltage the device needs, the device's power supply will not be needed either. Since one or more power conversion steps are eliminated, this increases efficiency and run time.
Many systems used in telecommunications use an extra-low voltage "common battery" 48 V DC power, because it has less restrictive safety regulations, such as being installed in conduit and junction boxes. DC has typically been the dominant power source for telecommunications, and AC has typically been the dominant source for computers and servers.
There has been much experimentation with 48 V DC power for computer servers, in the hope of reducing the likelihood of failure and the cost of equipment. However, to supply the same amount of power, the current would be higher than an equivalent 115 V or 230 V circuit; greater current requires larger conductors, or more energy lost as heat.
A laptop computer is a classic example of a PC with a DC UPS built in.
High voltage DC (380 V) is finding use in some data center applications, and allows for small power conductors, but is subject to the more complex electrical code rules for safe containment of high voltages.
Rotary.
A rotary UPS uses the inertia of a high-mass spinning flywheel (flywheel energy storage) to provide short-term "ride-through" in the event of power loss. The flywheel also acts as a buffer against power spikes and sags, since such short-term power events are not able to appreciably affect the rotational speed of the high-mass flywheel. It is also one of the oldest designs, predating vacuum tubes and integrated circuits.
It can be considered to be "on line" since it spins continuously under normal conditions. However, unlike a battery-based UPS, flywheel-based UPS systems typically provide 10 to 20 seconds of protection before the flywheel has slowed and power output stops. It is traditionally used in conjunction with standby diesel generators, providing backup power only for the brief period of time the engine needs to start running and stabilize its output.
The rotary UPS is generally reserved for applications needing more than 10,000 W of protection, to justify the expense and benefit from the advantages rotary UPS systems bring. A larger flywheel or multiple flywheels operating in parallel will increase the reserve running time or capacity.
Because the flywheels are a mechanical power source, it is not necessary to use an electric motor or generator as an intermediary between it and a diesel engine designed to provide emergency power. By using a transmission gearbox, the rotational inertia of the flywheel can be used to directly start up a diesel engine, and once running, the diesel engine can be used to directly spin the flywheel. Multiple flywheels can likewise be connected in parallel through mechanical countershafts, without the need for separate motors and generators for each flywheel.
They are normally designed to provide very high current output compared to a purely electronic UPS, and are better able to provide inrush current for inductive loads such as motor startup or compressor loads, as well as medical MRI and cath lab equipment. It is also able to tolerate short-circuit conditions up to 17 times larger than an electronic UPS, permitting one device to blow a fuse and fail while other devices still continue to be powered from the rotary UPS.
Its life cycle is usually far greater than a purely electronic UPS, up to 30 years or more. But they do require periodic downtime for mechanical maintenance, such as ball bearing replacement. In larger systems redundancy of the system ensures the availability of processes during this maintenance. Battery-based designs do not require downtime if the batteries can be hot-swapped, which is usually the case for larger units. Newer rotary units use technologies such as magnetic bearings and air-evacuated enclosures to increase standby efficiency and reduce maintenance to very low levels.
Typically, the high-mass flywheel is used in conjunction with a motor-generator system. These units can be configured as:
In case No. 3 the motor generator can be synchronous/synchronous or induction/synchronous. The motor side of the unit in case Nos. 2 and 3 can be driven directly by an AC power source (typically when in inverter bypass), a 6-step double-conversion motor drive, or a 6-pulse inverter. Case No. 1 uses an integrated flywheel as a short-term energy source instead of batteries to allow time for external, electrically coupled gensets to start and be brought online. Case Nos. 2 and 3 can use batteries or a free-standing electrically coupled flywheel as the short-term energy source.
Form Factors.
UPS systems come in several different forms and sizes. However, the two most common forms are tower and rack-mount.
Tower Model.
Tower models stand upright on the ground or on a desk/shelf, and are typically used in network workstations or desktop computer applications.
Rack-Mount Model.
Rack-mount models can be mounted in standard 19" rack enclosures and can require anywhere from 1U to 12U (rack space). They are typically used in server and networking applications.
Applications.
N+1.
In large business environments where reliability is of great importance, a single huge UPS can also be a single point of failure that can disrupt many other systems. To provide greater reliability, multiple smaller UPS modules and batteries can be integrated together to provide redundant power protection equivalent to one very large UPS. "N+1" means that if the load can be supplied by N modules, the installation will contain N+1 modules. In this way, failure of one module will not impact system operation.
Multiple redundancy.
Many computer servers offer the option of redundant power supplies, so that in the event of one power supply failing, one or more other power supplies are able to power the load. This is a critical point – each power supply must be able to power the entire server by itself.
Redundancy is further enhanced by plugging each power supply into a different circuit (i.e. to a different circuit breaker).
Redundant protection can be extended further yet by connecting each power supply to its own UPS. This provides double protection from both a power supply failure and a UPS failure, so that continued operation is assured. This configuration is also referred to as 1+1 or 2N redundancy. If the budget does not allow for two identical UPS units then it is common practice to plug one power supply into mains power and the other into the UPS.
Outdoor use.
When a UPS system is placed outdoors, it should have some specific features that guarantee that it can tolerate weather with no effect on performance. Factors such as temperature, humidity, rain, and snow among others should be considered by the manufacturer when designing an outdoor UPS system. Operating temperature ranges for outdoor UPS systems could be around −40 °C to +55 °C.
Outdoor UPS systems can be pole, ground (pedestal), or host mounted. Outdoor environment could mean extreme cold, in which case the outdoor UPS system should include a battery heater mat, or extreme heat, in which case the outdoor UPS system should include a fan system or an air conditioning system.
Internal systems.
UPS systems can be designed to be placed inside a computer chassis. There are two types of internal UPS. The first type is a miniaturized regular UPS that is made small enough to fit into a 5.25-inch CD-ROM slot bay of a regular computer chassis. The other type is a re-engineered switching power supply that utilizes dual AC or DC power sources as inputs and has built-in switching control units.
Measuring efficiency.
The way efficiency is measured varies, and there are a number of reasons for this. Many UPS manufacturers claim to have the highest level of efficiency, often using different sets of criteria in order to reach these figures. The industry norm can be argued to be anything between 93%-96% when a UPS is in full operational mode, and to reach these figures companies often put their UPS in an ideal scenario. Efficiency figures on site are often much closer to the 90% mark, due to varying power conditions. 
Difficulties faced with generator use.
Power factor.
A problem in the combination of a "double conversion" UPS and a generator is the voltage distortion created by the UPS. The input of a double conversion UPS is essentially a big rectifier. The current drawn by the UPS is non-sinusoidal. This can cause the voltage from the AC mains or a generator to also become non-sinusoidal. The voltage distortion then can cause problems in all electrical equipment connected to that power source, including the UPS itself. It will also cause more power to be lost in the wiring supplying power to the UPS due to the spikes in current flow. This level of "noise" is measured as a percentage of "Total Harmonic Distortion of the current" (THD(i)). Classic UPS rectifiers have a THD(i) level of around 25–30%. To reduce voltage distortion, this requires heavier mains wiring or generators more than twice as large as the UPS.
There are several solutions to reduce the THD(i) in a double conversion UPS:
Passive power factor correction:
Classic solutions such as passive filters reduce THD(i) to 5–10% at full load. They are reliable, but big and only work at full load, and present their own problems when used in tandem with generators.
Active power factor correction:
An alternative solution is an active filter. Through the use of such a device, THD(i) can drop to 5% over the full power range. The newest technology in double conversion UPS units is a rectifier that doesn't use classic rectifier components (thyristors and diodes) but high frequency components. A double conversion UPS with an IGBT rectifier and inductor can have a THD(i) as small as 2%. This completely eliminates the need to oversize the generator (and transformers), without additional filters, investment cost, losses, or space.
Communication.
Power management (PM) requires
The basic computer-to-UPS control methods are intended for one-to-one signaling from a single source to a single target. For example, a single UPS may connect to a single computer to provide status information about the UPS, and allow the computer to control the UPS. Similarly, the USB protocol is also intended to connect a single computer to multiple peripheral devices.
In some situations it is useful for a single large UPS to be able to communicate with several protected devices. For traditional serial or USB control, a "signal replication" device may be used, which for example allows one UPS to connect to five computers using serial or USB connections. However, the splitting is typically only one direction from UPS to the devices to provide status information. Return control signals may only be permitted from one of the protected systems to the UPS.
As Ethernet has increased in common use since the 1990s, control signals are now commonly sent between a single UPS and multiple computers using standard Ethernet data communication methods such as TCP/IP. The status and control information is typically encrypted so that for example an outside hacker can not gain control of the UPS and command it to shut down.
Distribution of UPS status and control data requires that all intermediary devices such as Ethernet switches or serial multiplexers be powered by one or more UPS systems, in order for the UPS alerts to reach the target systems during a power outage. To avoid the dependency on Ethernet infrastructure, the UPSs can be connected directly to main control server by using GSM/GPRS channel also. The SMS or GPRS data packets sent from UPSs trigger software to shutdown the PCs to reduce the load.
Batteries.
The run-time for a battery-operated UPS depends on the type and size of batteries and rate of discharge, and the efficiency of the inverter. The total capacity of a lead–acid battery is a function of the rate at which it is discharged, which is described as Peukert's law.
Manufacturers supply run-time rating in minutes for packaged UPS systems. Larger systems (such as for data centers) require detailed calculation of the load, inverter efficiency, and battery characteristics to ensure the required endurance is attained.
Common battery characteristics and load testing.
When a lead–acid battery is charged or discharged, this initially affects only the reacting chemicals, which are at the interface between the electrodes and the electrolyte. With time, the charge stored in the chemicals at the interface, often called "interface charge", spreads by diffusion of these chemicals throughout the volume of the active material.
If a battery has been completely discharged (e.g. the car lights were left on overnight) and next is given a fast charge for only a few minutes, then during the short charging time it develops only a charge near the interface. The battery voltage may rise to be close to the charger voltage so that the charging current decreases significantly. After a few hours this interface charge will spread to the volume of the electrode and electrolyte, leading to an interface charge so low that it may be insufficient to start the car.
Due to the interface charge, brief UPS "self-test" functions lasting only a few seconds may not accurately reflect the true runtime capacity of a UPS, and instead an extended "recalibration" or "rundown" test that deeply discharges the battery is needed.
The deep discharge testing is itself damaging to batteries due to the chemicals in the discharged battery starting to crystallize into highly stable molecular shapes that will not re-dissolve when the battery is recharged, permanently reducing charge capacity. In lead acid batteries this is known as sulfation but also affects other types such as nickel cadmium batteries and lithium batteries. Therefore it is commonly recommended that rundown tests be performed infrequently, such as every six months to a year.
Testing of strings of batteries/cells.
Multi-kilowatt commercial UPS systems with large and easily accessible battery banks are capable of isolating and testing individual cells within a "battery string", which consists of either combined-cell battery units (such as 12-V lead acid batteries) or individual chemical cells wired in series. Isolating a single cell and installing a jumper in place of it allows the one battery to be discharge-tested, while the rest of the battery string remains charged and available to provide protection.
It is also possible to measure the electrical characteristics of individual cells in a battery string, using intermediate sensor wires that are installed at every cell-to-cell junction, and monitored both individually and collectively. Battery strings may also be wired as series-parallel, for example two sets of 20 cells. In such a situation it is also necessary to monitor current flow between parallel strings, as current may circulate between the strings to balance out the effects of weak cells, dead cells with high resistance, or shorted cells. For example, stronger strings can discharge through weaker strings until voltage imbalances are equalized, and this must be factored into the individual inter-cell measurements within each string.
Series-parallel battery interactions.
Battery strings wired in series-parallel can develop unusual failure modes due to interactions between the multiple parallel strings. Defective batteries in one string can adversely affect the operation and lifespan of good or new batteries in other strings. These issues also apply to other situations where series-parallel strings are used, not just in UPS systems but also in electric vehicle applications.
Consider a series-parallel battery arrangement with all good cells, and one becomes shorted or dead:
The only way to prevent these subtle series-parallel string interactions is by not using parallel strings at all and using separate charge controllers and inverters for individual series strings.
Series new/old battery interactions.
Even just a single string of batteries wired in series can have adverse interactions if new batteries are mixed with old batteries. Older batteries tend to have reduced storage capacity, and so will both discharge faster than new batteries and also charge to their maximum capacity more rapidly than new batteries.
As a mixed string of new and old batteries is depleted, the string voltage will drop, and when the old batteries are exhausted the new batteries still have charge available. The newer cells may continue to discharge through the rest of the string, but due to the low voltage this energy flow may not be useful, and may be wasted in the old cells as resistance heating.
For cells that are supposed to operate within a specific discharge window, new cells with more capacity may cause the old cells in the series string to continue to discharge beyond the safe bottom limit of the discharge window, damaging the old cells.
When recharged, the old cells recharge more rapidly, leading to a rapid rise of voltage to near the fully charged state, but before the new cells with more capacity have fully recharged. The charge controller detects the high voltage of a nearly fully charged string and reduces current flow. The new cells with more capacity now charge very slowly, so slowly that the chemicals may begin to crystallize before reaching the fully charged state, reducing new cell capacity over several charge/discharge cycles until their capacity more closely matches the old cells in the series string.
For such reasons, some industrial UPS management systems recommend periodic replacement of entire battery arrays potentially using hundreds of expensive batteries, due to these damaging interactions between new batteries and old batteries, within and across series and parallel strings.

</doc>
<doc id="41835" url="http://en.wikipedia.org/wiki?curid=41835" title="Universal Time">
Universal Time

Universal Time (UT) is a time standard based on Earth's rotation. There are several versions of it; a major version, UT1, is used by modern scientists as mean solar time on the Prime Meridian at Greenwich. Civil time signals are transmitted according to another version, Coordinated Universal Time. UT0, UT1 and UT2 tick at the same rate to within fifty milliseconds. Greenwich Mean Time is defined as UT1. All of these versions of UT are based on Earth's rotation relative to distant celestial objects (stars and quasars), but with a scaling factor and other adjustments to make them closer to solar time.
Universal Time and standard time.
Prior to the introduction of standard time, each municipality throughout the civilized world set its official clock, if it had one, according to the local position of the Sun (see solar time). This served adequately until the introduction of rail travel in Britain, which made it possible to travel fast enough over long distances to require continuous re-setting of timepieces as a train progressed in its daily run through several towns. Greenwich Mean Time, where all clocks in Britain were set to the same time, was established to solve this problem. Chronometers or telegraphy were used to synchronize these clocks.
Standard time, as originally proposed by Scottish-Canadian Sir Sandford Fleming in 1879, divided the world into twenty-four time zones, each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. The local time at the Royal Greenwich Observatory in Greenwich, England was chosen as standard at the 1884 International Meridian Conference, leading to the widespread use of Greenwich Mean Time to set local clocks. This location was chosen because by 1884 two-thirds of all nautical charts and maps already used it as their prime meridian. The conference did not adopt Fleming's time zones because they were outside the purpose for which it was called, which was to choose a basis for universal time (as well as a prime meridian).
During the period between 1848 to 1972, all of the major countries adopted time zones based on the Greenwich meridian.
In 1935, the term "Universal Time" was recommended by the International Astronomical Union as a more precise term than Greenwich Mean Time, because GMT could refer to either an astronomical day starting at noon or a civil day starting at midnight. The term "Greenwich Mean Time" persists, however, in common usage to this day in reference to civil timekeeping.
Measurement.
Based on the rotation of the Earth, time can be measured by observing celestial bodies crossing the meridian every day. Astronomers found that it was more accurate to establish time by observing stars as they crossed a meridian rather than by observing the position of the Sun in the sky. Nowadays, UT in relation to International Atomic Time (TAI) is determined by Very Long Baseline Interferometry (VLBI) observations of distant quasars, a method which can determine UT1 to within 4 milliseconds.
The rotation of the Earth and UT are monitored by the International Earth Rotation and Reference Systems Service (IERS). The International Astronomical Union also is involved in setting standards, but the final arbiter of broadcast standards is the International Telecommunication Union or ITU.
The rotation of the Earth is somewhat irregular, and is very gradually slowing due to tidal acceleration. Furthermore, the length of the second was determined from observations of the Moon between 1750 and 1890. All of these factors cause the mean solar day, on the average, to be slightly longer than the nominal 86,400 SI seconds, the traditional number of seconds per day. As UT is slightly irregular in its rate, astronomers introduced Ephemeris Time, which has since been replaced by Terrestrial Time (TT). Because Universal Time is synchronous with night and day, and more precise atomic-frequency standards drift away from this, however, UT is still used to produce a correction (called a leap second) to atomic time, in order to obtain a broadcast form of civil time that carries atomic frequency. Thus, civil broadcast standards for time and frequency usually follow International Atomic Time closely, but occasionally step (or "leap") in order to prevent them from drifting too far from mean solar time.
Barycentric Dynamical Time (TDB), a form of atomic time, is now used in the construction of the ephemerides of the planets and other solar system objects, for two main reasons. First, these ephemerides are tied to optical and radar observations of planetary motion, and the TDB time scale is fitted so that Newton's laws of motion, with corrections for general relativity, are followed. Next, the time scales based on Earth's rotation are not uniform and therefore, are not suitable for predicting the motion of bodies in our solar system.
Versions.
There are several versions of Universal Time:
Adoption in various countries.
The table shows the dates of adoption of time zones based on the Greenwich meridian, including half-hour zones.
Apart from the Nepal Time Zone (UTC+05:45) and the Chatham Standard Time Zone (UTC+12:45) used in New Zealand's Chatham Islands, all timezones in use are defined by an offset from UTC that is a multiple of half an hour, and in most cases a multiple of an hour.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41836" url="http://en.wikipedia.org/wiki?curid=41836" title="Abstract factory pattern">
Abstract factory pattern

The abstract factory pattern provides a way to encapsulate a group of individual factories that have a common theme without specifying their concrete classes. In normal usage, the client software creates a concrete implementation of the abstract factory and then uses the generic interface of the factory to create the concrete objects that are part of the theme. The client doesn't know (or care) which concrete objects it gets from each of these internal factories, since it uses only the generic interfaces of their products. This pattern separates the details of implementation of a set of objects from their general usage and relies on object composition, as object creation is implemented in methods exposed in the factory interface.
An example of this would be an abstract factory class codice_1 that provides interfaces to create a number of products (e.g. codice_2 and codice_3). The system would have any number of derived concrete versions of the codice_1 class like codice_5 or codice_6, each with a different implementation of codice_2 and codice_3 that would create a corresponding object like codice_9 or codice_10. Each of these products is derived from a simple abstract class like codice_11 or codice_12 of which the client is aware. The client code would get an appropriate instance of the codice_1 and call its factory methods. Each of the resulting objects would be created from the same codice_1 implementation and would share a common theme (they would all be fancy or modern objects). The client would only need to know how to handle the abstract codice_11 or codice_12 class, not the specific version that it got from the concrete factory.
A factory is the location of a concrete class in the code at which objects are constructed. The intent in employing the pattern is to insulate the creation of objects from their usage and to create families of related objects without having to depend on their concrete classes. This allows for new derived types to be introduced with no change to the code that uses the base class.
Use of this pattern makes it possible to interchange concrete implementations without changing the code that uses them, even at runtime. However, employment of this pattern, as with similar design patterns, may result in unnecessary complexity and extra work in the initial writing of code. Additionally, higher levels of separation and abstraction can result in systems which are more difficult to debug and maintain.
Definition.
The essence of the Abstract Factory Pattern is to "Provide an interface for creating families of related or dependent objects without specifying their concrete classes.".
Usage.
The "factory" determines the actual "concrete" type of object to be created, and it is here that the object is actually created (in C++, for instance, by the new operator). However, the factory only returns an "abstract" pointer to the created concrete object.
This insulates client code from object creation by having clients ask a factory object to create an object of the desired abstract type and to return an abstract pointer to the object.
As the factory only returns an abstract pointer, the client code (that requested the object from the factory) does not know — and is not burdened by — the actual concrete type of the object that was just created. However, the type of a concrete object (and hence a concrete factory) is known by the abstract factory; for instance, the factory may read it from a configuration file. The client has no need to specify the type, since it has already been specified in the configuration file. In particular, this means:
Structure.
Rational Class Diagram.
The method codice_17 on the codice_18 interface returns objects of type codice_19. What implementation of codice_19 is returned depends on which implementation of codice_18 is handling the method call.
Pseudocode.
It should render a button in either a Windows style or Mac OS X style depending on which kind of factory was used. Note that the Application has no idea what kind of GUIFactory it is given or even what kind of Button that factory creates.
 interface Button is
 method paint()
 interface GUIFactory is
 method createButton()
 output: a "button"
 class WinFactory implementing GUIFactory is
 method createButton() is
 output: a Windows button
 Return a new WinButton
 class OSXFactory implementing GUIFactory is
 method createButton() is
 output: an OS X button
 Return a new OSXButton
 class WinButton implementing Button is
 method paint() is
 Render a button in a Windows style
 class OSXButton implementing Button is
 method paint() is
 Render a button in a Mac OS X style
 class Application is
 constructor Application(factory) is
 input: the GUIFactory "factory" used to create buttons
 Button button := factory.createButton()
 button.paint()
 Read the configuration file
 If the OS specified in the configuration file is Windows, then
 Construct a WinFactory
 Construct an Application with WinFactory
 else
 Construct an OSXFactory
 Construct an Application with OSXFactory
C# Example.
Abstract factory is the extension of basic Factory pattern. It provides Factory interfaces for creating a family of related classes. In other words, here I am declaring interfaces for Factories, which will in turn work in similar fashion as with Factories.
The factory method is also implemented using common interface each of which returns objects.

</doc>
<doc id="41837" url="http://en.wikipedia.org/wiki?curid=41837" title="Telecommunications link">
Telecommunications link

In telecommunications a link is a communications channel that connects two or more communicating devices. This link may be an actual physical link or it may be a logical link that uses one or more actual physical links.
A telecommunications link is generally one of several types of information transmission paths such as those provided by communication satellites, terrestrial radio communications infrastructure and computer networks to connect two or more points.
The term "link" is widely used in computer networking (see data link) to refer to the communications facilities that connect nodes of a network. When the link is a logical link the type of physical link should always be specified (e.g., data link, uplink, downlink, fiber optic link, point-to-point link, etc.)
Types.
Point-to-point.
A point-to-point link is a dedicated link that connects exactly two communication facilities (e.g., two nodes of a network, an intercom station at an entryway with a single internal intercom station, a radio path between two points, etc.).
Broadcast.
Broadcast links connect two or more nodes and support "broadcast transmission", where one node can transmit so that all other nodes can receive the same transmission. Ethernet is an example.
Multipoint.
Also known as a "multidrop" link, a multipoint link is a link that connects "two or more" nodes. Also known as general topology networks, these include ATM and Frame Relay links, as well as X.25 networks when used as links for a network layer protocol like IP.
Unlike broadcast links, there is no mechanism to efficiently send a single message to all other nodes without copying and retransmitting the message.
Point-to-multipoint.
A point-to-multipoint link (or simply a "multipoint") is a specific type of multipoint link which consists of a central connection endpoint (CE) that is connected to multiple peripheral CEs. Any transmission of data that originates from the central CE is received by all of the peripheral CEs while any transmission of data that originates from any of the peripheral CEs is only received by the central CE.
Private and public – accessibility and ownership.
Links are often referred to by terms which refer to the ownership and / or accessibility of the link.
Direction.
Forward link.
A forward link is the link from a fixed location (e.g., a base station) to a mobile user. If the link includes a communications relay satellite, the forward link will consist of both an uplink (base station to satellite) and a downlink (satellite to mobile user).
Reverse link.
The reverse link (sometimes called a "return channel") is the link from a mobile user to a fixed base station.
If the link includes a communications relay satellite, the reverse link will consist of both an uplink (mobile station to satellite) and a downlink (satellite to base station) which together constitute a half hop.

</doc>
<doc id="41842" url="http://en.wikipedia.org/wiki?curid=41842" title="User information bit">
User information bit

In telecommunication, a user information bit is a bit transferred from a source user to a telecommunications system for delivery to a destination user. 
User information bits do not include the overhead bits originated by, or having their primary functional effect within, the telecommunications system. 
User information bits are encoded to form channel bits.

</doc>
<doc id="41844" url="http://en.wikipedia.org/wiki?curid=41844" title="Validation">
Validation

Validation may refer to:

</doc>
<doc id="41845" url="http://en.wikipedia.org/wiki?curid=41845" title="Variable-length buffer">
Variable-length buffer

In telecommunication, a variable length buffer is a buffer into which data may be entered at one rate and removed at another rate without changing the data sequence. 
Most first-in first-out (FIFO) storage devices are variable-length buffers in that the input rate may be variable while the output rate is constant or the output rate may be variable while the input rate is constant. Various clocking and control systems are used to allow control of underflow or overflow conditions.

</doc>
<doc id="41847" url="http://en.wikipedia.org/wiki?curid=41847" title="Video teleconferencing unit">
Video teleconferencing unit

A video teleconferencing unit (VTU) is a piece of electrical equipment that performs videoconferencing functions, such as the coding and decoding of audio and video signals and multiplexing of video, audio, data, and control signals, and that usually does not include Input/Output (I/O) devices, cryptographic devices, network interface equipment, network connections, or the communications network to which the unit is connected.

</doc>
<doc id="41848" url="http://en.wikipedia.org/wiki?curid=41848" title="View">
View

View may refer to:

</doc>
<doc id="41849" url="http://en.wikipedia.org/wiki?curid=41849" title="Viewdata">
Viewdata

Viewdata is a Videotex implementation. It is a type of information retrieval service in which a subscriber can access a remote database via a common carrier channel, request data and receive requested data on a video display over a separate channel. Samuel Fedida was credited as inventor of the system. Fedida had the idea for Viewdata in 1968. The first prototype became operational in 1974. The access, request and reception are usually via common carrier broadcast channels. This is in contrast with teletext.
Technology.
Originally Viewdata was accessed with a special purpose terminal (or emulation software) and a modem running at CCITT V.23 speed (1200 bit/s down, 75 bit/s up). By 2004 it was normally accessed over TCP/IP using Viewdata client software on a personal computer running Microsoft Windows, or using a Web-based emulator.
Travel industry.
As of 2015, Viewdata is still in use in the United Kingdom, mainly by the travel industry. Travel agents use it to look up the price and availability of package holidays and flights. Once they find what the customer is looking for they can place a booking. 
There are a number of factors still holding up a move to a Web based standard. Viewdata is regarded within the industry as low-cost and reliable, travel consultants have been trained to use Viewdata, they would need training to book holidays on the Internet, and the tour operators can't agree on a Web based standard. 
Bulletin board systems.
It was made in the late 1970s and early 1980s to make it easier for travel consultants to check availability and make bookings for holidays.
A number of Viewdata Bulletin Board Systems existed in the 1980s, predominantly in the UK due to the proliferation of the BBC Microcomputer, and a short-lived "Viewdata Revival" appeared in the late 1990s fuelled by the retrocomputing vogue. Some Viewdata boards still exist, with accessibility in the form of Java Telnet clients.

</doc>
<doc id="41850" url="http://en.wikipedia.org/wiki?curid=41850" title="Virtual call capability">
Virtual call capability

In telecommunication, a virtual call capability, sometimes called a virtual call facility, is a service feature in which:

</doc>
<doc id="41851" url="http://en.wikipedia.org/wiki?curid=41851" title="Virtual circuit">
Virtual circuit

A virtual circuit (VC) is a means of transporting data over a packet switched computer network in such a way that it appears as though there is a dedicated physical layer link between the source and destination end systems of this data. The term virtual circuit is synonymous with virtual connection and virtual channel. Before a connection or virtual circuit may be used, it has to be established, between two or more nodes or software applications, by configuring the relevant parts of the interconnecting network. After that, a bit stream or byte stream may be delivered between the nodes; hence, a virtual circuit protocol allows higher level protocols to avoid dealing with the division of data into segments, packets, or frames. 
Virtual circuit communication resembles circuit switching, since both are connection oriented, meaning that in both cases data is delivered in correct order, and signalling overhead is required during a connection establishment phase. However, circuit switching provides a constant bit rate and latency, while these may vary in a virtual circuit service due to factors such as:
Many virtual circuit protocols, but not all, provide reliable communication service through the use of data retransmissions because of error detection and automatic repeat request (ARQ).
An alternate network configuration to virtual circuit is datagram.
Layer 4 virtual circuits.
Connection oriented transport layer datalink protocols such as TCP may rely on a connectionless packet switching network layer protocol such as IP, where different packets may be routed over different paths, and thus be delivered out of order. However, it is possible to use TCP as a virtual circuit, since TCP includes segment numbering that allows reordering on the receiver side to accommodate out-of-order delivery.
Layer 2/3 virtual circuits.
Datalink layer and network layer virtual circuit protocols are based on connection oriented packet switching, meaning that data is always delivered along the same network path, i.e., through the same nodes. Advantages with this over connectionless packet switching are: 
Examples of protocols that provide virtual circuits.
Examples of transport layer protocols that provide a virtual circuit:
Examples of network layer and datalink layer virtual circuit protocols, where data always is delivered over the same path:
Permanent and switched virtual circuits in ATM, frame relay, and X.25.
Switched virtual circuits (SVCs) are generally set up on a per-call basis and are disconnected when the call is terminated; however, a permanent virtual circuit (PVC) can be established as an option to provide a dedicated circuit link between two facilities. PVC configuration is usually preconfigured by the service provider. Unlike SVCs, PVC are usually very seldom broken/disconnected.
A switched virtual circuit (SVC) is a virtual circuit that is dynamically established on demand and is torn down when transmission is complete, for example after a phone call or a file download. SVCs are used in situations where data transmission is sporadic and/or not always between the same data terminal equipment (DTE) endpoints.
A permanent virtual circuit (PVC) is a virtual circuit established for repeated/continuous use between the same DTE. In a PVC, the long-term association is identical to the data transfer phase of a virtual call. Permanent virtual circuits eliminate the need for repeated call set-up and clearing.

</doc>
<doc id="41853" url="http://en.wikipedia.org/wiki?curid=41853" title="Virtual storage">
Virtual storage

Virtual storage can refer to:

</doc>
<doc id="41854" url="http://en.wikipedia.org/wiki?curid=41854" title="Virtual terminal">
Virtual terminal

In open systems, a virtual terminal (VT) is an application service that:
PuTTY is an example of a virtual terminal.
ITU-T defines a virtual terminal protocol based on the OSI application layer protocols. However, the virtual terminal protocol is not widely used on the Internet.
Virtual Terminals for payment card processing.
This term is also used to refer to web interfaces for processing card not present transactions. Such terminals allow call centre agents to enter a customer's credit card details to take a payment.

</doc>
<doc id="41855" url="http://en.wikipedia.org/wiki?curid=41855" title="Voice frequency">
Voice frequency

A voice frequency (VF) or voice band is one of the frequencies, within part of the audio range, that is used for the transmission of speech.
In telephony, the usable voice frequency band ranges from approximately 300 Hz to 3400 Hz. It is for this reason that the ultra low frequency band of the electromagnetic spectrum between 300 and 3000 Hz is also referred to as "voice frequency", being the electromagnetic energy that represents acoustic energy at baseband. The bandwidth allocated for a single voice-frequency transmission channel is usually 4 kHz, including guard bands, allowing a sampling rate of 8 kHz to be used as the basis of the pulse code modulation system used for the digital PSTN. Per the Nyquist–Shannon sampling theorem, the sampling frequency (8 kHz) must be at least twice the highest component of the voice frequency via appropriate filtering prior to sampling at discrete times (4 kHz) for effective reconstruction of the voice signal.
Fundamental frequency.
The voiced speech of a typical adult male will have a fundamental frequency from 85 to 180 Hz, and that of a typical adult female from 165 to 255 Hz. Thus, the fundamental frequency of most speech falls below the bottom of the "voice frequency" band as defined above. However, enough of the harmonic series will be present for the missing fundamental to create the impression of hearing the fundamental tone.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41856" url="http://en.wikipedia.org/wiki?curid=41856" title="Voice frequency primary patch bay">
Voice frequency primary patch bay

In telecommunication, a voice frequency primary patch bay (VF) is a patching facility that provides the first appearance of local-user VF circuits in the technical control facility (TCF). 
The VF primary patch bay provides patching, , and testing for all VF circuits. Signals will have various levels and signaling schemes depending on the user terminal equipment.

</doc>
<doc id="41858" url="http://en.wikipedia.org/wiki?curid=41858" title="Volt-ampere reactive">
Volt-ampere reactive

In electric power transmission and distribution, volt-ampere reactive (var) is a unit in which reactive power is expressed in an AC electric power system. Reactive power exists in an AC circuit when the current and voltage are not in phase. The correct symbol is var and not Var, VAr, or VAR, but all three terms are widely used, and VAR is widely used throughout the power industry infrastructure. The term "var" was proposed by the Romanian electrical engineer Constantin Budeanu and introduced in 1930 by the IEC in Stockholm, which has adopted it as the unit for reactive power.
Vars may be considered as either the imaginary part of apparent power, or the power flowing into a reactive load, where voltage and current are specified in volts and amperes. The two definitions are equivalent.
The unit "var" does not follow the recommended practice of the International System of Units, because the quantity the unit var represents is power, and SI practice is not to include information about the type of power being measured in the unit name. 
Reactive power.
A sinusoidally alternating voltage applied to a purely resistive load results in an alternating current that is fully in phase with the voltage. However, in many applications it is common for there to be a reactive component to the system, that is, the system possesses capacitance, inductance, or both. These electrical properties cause the current to change phase with respect to the voltage: capacitance tending the current to lead the voltage in phase, and inductance to lag it.
For sinusoid currents and voltages at the same frequency, reactive power in vars is the product of the RMS voltage and current, or the apparent power, multiplied by the sine of formula_1 (phase angle between the voltage and the current). The reactive power formula_2 (measured in units of volt-amperes reactive or var) is given by:
where formula_1 is the phase angle between the current and voltage.
Q refers to the maximum value of the instantaneous power absorbed by the reactive component of the load. 
Only effective power, the actual power delivered to or consumed by the load, is expressed in watts. Imaginary power is properly expressed in volt-amperes reactive.
Physical significance of reactive power.
Reactive power (measured in vars) is present in a system containing reactive (inductive or capacitive) components and can be either produced or consumed by different load/generation elements. Though "imaginary", the reactive power has great physical significance and is essential to the operation of the electrical system as a whole. While the real power P is used to supply the energy required to perform actual work (such as running a motor), the reactive power regulates the voltage in the system. If the reactive power is too low, inductive loads such as transformers will be unable to maintain voltages necessary for the generation of electromagnetic fields, leading to a "voltage collapse" that create blackouts . Transmission line impedances also make it necessary to provide reactive power to maintain voltage levels necessary for active power to flow through. Therefore reactive power is essential to move active power through transmission and distribution systems to the customer. However if reactive power in a system is too high, there is increased heat loss in transmission lines and loads as the current flowing through the system is much higher, creating a potentially hazardous breakdown situation. The power factor of a load tells us what fraction of the apparent power is in the form of real power and performs actual work. A high power factor is desirable since it minimizes the amount of reactive power needed by the load, reducing heat losses and maximizing efficiency.

</doc>
<doc id="41859" url="http://en.wikipedia.org/wiki?curid=41859" title="Voice-operated switch">
Voice-operated switch

In telecommunications, a voice operated switch, also known as VOX or Voice Operated eXchange, is a switch that operates when sound over a certain threshold is detected. It is usually used to turn on a transmitter or recorder when someone speaks and turn it off when they stop speaking. It is used instead of a push-to-talk button on transmitters or to save storage space on recording devices. On cell phones, it is used to save battery life. Intercom systems that use a speaker in a room as both a speaker and a microphone will often use VOX on the main console to switch the audio direction during a conversation. The circuit usually includes a delay between the sound stopping and switching direction, to avoid the circuit turning off during short pauses in speech.
A special case exists, if there is enough energy to power the system directly. For example, a microphone may send a voltage, high enough, to directly operate a transmitter. 
Comparison with push-to-talk.
Unlike push-to-talk (PTT) operation, VOX is
automatic; the user can keep his or her hands free while talking. But
VOX also has some significant disadvantages that explain why PTT is
still common.
Most VOX circuits have a sensitivity adjustment, but unwanted (and
sometimes undetected) VOX triggering can still occur on background
noise, heavy breathing or a side conversation. Conversely, it may not
activate when desired on speech that is too weak.
The VOX in a two-way radio can also be triggered by the loudspeaker
carrying the other side of the conversation. This problem can be
minimized with an "anti vox" feature to decrease VOX sensitivity when
the receiver is active.
Transmitters and recorders have short but finite activation times that
may clip the beginnings of phrases. Some modern VOX circuits eliminate
this problem by recording or transmitting a delayed version of the
input signal.
VOX uses a "hang" timer, typically 1-3 seconds, to remain engaged
during brief speech pauses. This means the last several seconds of
each transmission or recorded segment are always silence. A
VOX-activated recorder can delete the end of each segment but the user
of a VOX-activated half duplex radio must wait for the timer to expire before he or she can receive again.

</doc>
<doc id="41860" url="http://en.wikipedia.org/wiki?curid=41860" title="Wafer (electronics)">
Wafer (electronics)

A wafer, also called a slice or substrate, is a thin slice of semiconductor material, such as a crystalline silicon, used in electronics for the fabrication of integrated circuits and in photovoltaics for conventional, wafer-based solar cells. The wafer serves as the substrate for microelectronic devices built in and over the wafer and undergoes many microfabrication process steps such as doping or ion implantation, etching, deposition of various materials, and photolithographic patterning. Finally the individual microcircuits are separated (dicing) and packaged.
History.
By 1960, silicon wafers were being manufactured in the U.S. by companies such as MEMC/SunEdison. In 1965, American engineers Eric O. Ernst, Donald J. Hurd, and Gerard Seeley, while working under IBM, filed Patent US3423629A for the first high-capacity epitaxial apparatus.
Formation.
Wafers are formed of highly pure (99.9999999% purity), 
nearly defect-free single crystalline material. One process for forming crystalline wafers is known as Czochralski growth invented by the Polish chemist Jan Czochralski. In this process, a cylindrical ingot of high purity monocrystalline semiconductor, such as silicon or germanium, is formed by pulling a seed crystal from a 'melt'. Donor impurity atoms, such as boron or phosphorus in the case of silicon, can be added to the molten intrinsic material in precise amounts in order to dope the crystal, thus changing it into n-type or p-type extrinsic semiconductor.
The ingot is then sliced with a wafer saw (wire saw) and polished to form wafers. The size of wafers for photovoltaics is 100–200 mm square and the thickness is 200–300 μm. In the future, 160 μm will be the standard. Electronics use wafer sizes from 100–450 mm diameter. (The largest wafers made have a diameter of 450 mm but are not yet in general use.)
Cleaning, texturing and etching.
Wafers are cleaned with weak acids to remove unwanted particles, or repair damage caused during the sawing process. When used for solar cells, the wafers are textured to create a rough surface to increase their efficiency. The generated PSG (phosphosilicate glass) is removed from the edge of the wafer in the etching.
Wafer properties.
Standard wafer sizes.
Silicon wafers are available in a variety of diameters from 25.4 mm (1 inch) to 300 mm (11.8 inches). Semiconductor fabrication plants (also known as "fabs") are defined by the diameter of wafers that they are tooled to produce. The diameter has gradually increased to improve throughput and reduce cost with the current state-of-the-art fab considered to be 300 mm (12 inch), with the next standard projected to be 450 mm (18 inch). Intel, TSMC and Samsung are separately conducting research to the advent of 450 mm "prototype" (research) fabs, though serious hurdles remain. 
Wafers grown using materials other than silicon will have different thicknesses than a silicon wafer of the same diameter. Wafer thickness is determined by the mechanical strength of the material used; the wafer must be thick enough to support its own weight without cracking during handling.
A unit wafer fabrication step, such as an etch step or a lithography step, can be performed on more chips per wafer as roughly the square of the increase in wafer diameter, while the cost of the unit fabrication step goes up more slowly than the square of the wafer diameter. This is the cost basis for shifting to larger and larger wafer sizes. Conversion to 300 mm wafers from 200 mm wafers began in earnest in 2000, and reduced the price per die about 30-40%.
However, this was not without significant problems for the industry.
There is considerable resistance to moving up to 450 mm despite the expected productivity improvement, mainly because companies feel it would take too long to recoup their investment. Machinery needed to handle and process larger wafers results in increased investment costs to build a single factory. Lithographer Chris Mack claimed in 2012 that the overall price per die for 450 mm wafers would be reduced by only 10-20% compared to 300 mm wafers, because presently over 50% of total wafer processing costs are lithography-related. Converting to larger 450 mm wafers would reduce price per die only for process operations such as etch where cost is related to wafer count, not wafer area. Cost for processes such as lithography is proportional to wafer area, and larger wafers would not reduce the lithography contribution to die cost. Nikon plans to deliver 450-mm lithography equipment in 2015, with volume production in 2017. In November 2013 ASML paused development of 450-mm lithography equipment, citing uncertain timing of chipmaker demand.
The time-line for 450 mm has not been fixed as of 2014. Mark Durcan, CEO of Micron Technology, said in February 2014 that he expects 450 mm adoption to be delayed indefinitely or discontinued. “I am not convinced that 450mm will ever happen but, to the extent that it does, it’s a long way out in the future. There is not a lot of necessity for Micron, at least over the next five years, to be spending a lot of money on 450mm. There is a lot of investment that needs to go on in the equipment community to make that happen. And the value at the end of the day – so that customers would buy that equipment – I think is dubious.” As of March 2014, Intel Corporation expects 450 mm deployment by 2020 (by the end of this decade). Mark LaPedus of semiengineering.com reported in mid-2014 that chipmakers had previously set 2016 to 2018, though this has been delayed “for the foreseeable future.” According to this report some observers expect 2018 to 2020, while “G. Dan Hutcheson, chief executive of VLSI Research, doesn’t see 450mm fabs moving into production until 2020 to 2025.”
The step up to 300 mm required a major change from the past, with fully automated factories using 300 mm wafers versus barely automated factories for the 200 mm wafers. These major investments were undertaken in the economic downturn following the dot-com bubble, resulting in huge resistance to upgrading to 450 mm by the original timeframe. Other initial technical problems in the ramp up to 300 mm included vibrational effects, gravitational bending (sag), and problems with flatness. Among the new problems in the ramp up to 450 mm are that the crystal ingots will be 3 times heavier (total weight a metric ton) and take 2-4 times longer to cool, and the process time will be double. All told, the development of 450 mm wafers require significant engineering, time, and cost to overcome.
Analytical die count estimation.
In order to minimize the cost per die, manufacturers wish to maximize the number of dies that can be made from a single wafer; dies always have a square or rectangular shape due to the constraint of wafer dicing. In general, this is a computationally complex problem with no analytical solution, dependent on both the area of the dies as well as their aspect ratio (square or rectangular) and other considerations such as scribeline size and the space occupied by alignment and test structures. Note that gross DPW formulas account only for wafer area that is lost because it cannot be used to make physically complete dies; gross DPW calculations do "not" account for yield loss due to defects or parametric issues.
Nevertheless, the number of gross die per wafer (DPW) can be estimated starting with the first-order approximation or wafer-to-die area ratio,
where formula_2 is the wafer diameter (typically in mm) and formula_3 the size of each die (mm2). This formula simply states that the number of dies which can fit on the wafer cannot exceed the area of the wafer divided by the area of each individual die. It will always overestimate the true best-case gross DPW, since it includes the area of partially patterned dies which do not fully lie on the wafer surface (see figure). These partially patterned dies don't represent complete ICs, so they cannot be sold as functional parts.
Refinements of this simple formula typically add an edge correction, to account for partial dies on the edge, which in general will be more significant when the area of the die is large compared to the total area of the wafer. In the other limiting case (infinitesimally small dies or infinitely large wafers), the edge correction is negligible.
The correction factor or correction term generally takes one of the forms cited by De Vries,
Studies comparing these analytical formulas to brute-force computational results show that the formulas can be made more accurate, over practical ranges of die sizes and aspect ratios, by adjusting the coefficients of the corrections to values above or below unity, and by replacing the linear die dimension formula_7 with formula_8 (average side length) in the case of dies with large aspect ratio:
Crystalline orientation.
Wafers are grown from crystal having a regular crystal structure, with silicon having a diamond cubic structure with a lattice spacing of 5.430710 Å (0.5430710 nm). When cut into wafers, the surface is aligned in one of several relative directions known as crystal orientations. Orientation is defined by the Miller index with (100) or (111) faces being the most common for silicon.
Orientation is important since many of a single crystal's structural and electronic properties are highly anisotropic. Ion implantation depths depend on the wafer's crystal orientation, since each direction offers distinct paths for transport.
Wafer cleavage typically occurs only in a few well-defined directions. Scoring the wafer along cleavage planes allows it to be easily diced into individual chips ("dies") so that the billions of individual circuit elements on an average wafer can be separated into many individual circuits.
Crystallographic orientation notches.
Wafers under 200 mm diameter have "flats" cut into one or more sides indicating the crystallographic planes of the wafer (usually a {110} face). In earlier-generation wafers a pair of flats at different angles additionally conveyed the doping type (see illustration for conventions). Wafers of 200 mm diameter and above use a single small notch to convey wafer orientation, with no visual indication of doping type.
Impurity doping.
Silicon wafers are generally not 100% pure silicon, but are instead formed with an initial impurity doping concentration between 1013 and 1016 atoms per cm3 of boron, phosphorus, arsenic, or antimony which is added to the melt and defines the wafer as either bulk n-type or p-type. However, compared with single-crystal silicon's atomic density of 5×1022 atoms per cm3, this still gives a purity greater than 99.9999%. The wafers can also be initially provided with some interstitial oxygen concentration. Carbon and metallic contamination are kept to a minimum. Transition metals, in particular, must be kept below parts per billion concentrations for electronic applications.
Compound semiconductors.
While silicon is the prevalent material for wafers used in the electronics industry, other compound III-V or II-VI materials have also been employed. Gallium arsenide (GaAs), a III-V semiconductor produced via the Czochralski process, is also a common wafer material.

</doc>
<doc id="41861" url="http://en.wikipedia.org/wiki?curid=41861" title="Wide area information server">
Wide area information server

Wide Area Information Server or WAIS is a client–server text searching system that uses the ANSI Standard Z39.50 Information Retrieval Service Definition and Protocol Specifications for Library Applications" (Z39.50:1988) to search index databases on remote computers. It was developed in the late 1980s as a project of Thinking Machines, Apple Computer, Dow Jones, and KPMG Peat Marwick. 
WAIS did not adhere to either the standard or its OSI framework (adopting instead TCP/IP) but created a unique protocol inspired by Z39.50:1988.
History.
The WAIS protocol and servers were primarily promoted by Thinking Machines Corporation (TMC) of Cambridge, Massachusetts. TMC produced WAIS servers which ran on their massively parallel CM-2 (Connection Machine) and SPARC-based CM-5 MP supercomputers. WAIS clients were developed for various operating systems and windowing systems including Microsoft Windows, Macintosh, NeXT, X, GNU Emacs, and character terminals. TMC, however, released a free open source version of WAIS to run on Unix in 1991. 
Inspired by the WAIS project on full text databases and emerging SGML projects Z39.50 version 2 or Z39.50:1992 was released. Unlike its 1988 predecessor it was a compatible superset of the ISO 10162/10163 work that had been done internationally.
With the advent of Z39.50:1992, the termination of support for the free WAIS from Thinking Machines and the establishment of WAIS Inc as a commercial venture, the U.S. National Science Foundation funded the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) to create a clearinghouse of information related to Internet search and discovery systems and to promote open source and standards. CNIDR created a new freely available open-source WAIS. This created first the freeWAIS package based on the wais-8-b5 codebase implemented by Thinking Machines Corp and then a wholly new software suite Isite based upon Z39.50:1992 with Isearch as its full text search engine.
Ulrich Pfeifer and Norbert Gövert of the computer science department of the University of Dortmund took the CNIDR freeWAIS code and extended it to become freeWAIS-sf: sf means structured fields and indicated its main improvement. Ulrich Pfeifer rewrote freeWAIS-sf in Perl where it became WAIT.
Inspired also by WAIS, especially its "Directory of Servers", Eliot Christian of USGS envisioned GILS: Government Information Locator Service. GILS (based upon Z39.50:1992 with some WAIS-like extensions) became a U.S. Federal mandate as part of the Paperwork Reduction Act of 1995 (#redirect ).
Directory of Servers.
Thinking Machines Corp provided a service called the Directory of Servers. It was a WAIS server like any other information source but contained information about the other WAIS servers on the Internet. When one would create a WAIS server with the TMC WAIS code it would create a special kind of record containing metadata and some common words to describe the content of the index. It would be uploaded to the central server and indexed along with the records from other public servers. One could search the directory to find servers that might have content relevant to a specific field of interest. This model of searching for (WAIS) servers to search became the role model for GILS and Peter Deutsch's WHOIS++ distributed white pages directory.
People.
Two of the developers of WAIS, Brewster Kahle and Harry Morris, left Thinking Machines to found WAIS Inc in Menlo Park, California with Bruce Gilliat. WAIS Inc. was originally developed as a joint project between Apple Computer, Peat Markwick, Dow Jones, and Thinking Machines. In 1992, the presidential campaign of Ross Perot used WAIS as a campaign wide information system, connecting the field offices to the national office. Later, Perot Systems adopted WAIS to better access the information in its corporate databases. Other early clients were the Environmental Protection Agency, Library of Congress, and the Department of Energy and later the "Wall Street Journal" and "Encyclopædia Britannica".
WAIS Inc was sold to AOL in May 1995 for $15 million. Following the sale, Margaret St. Pierre left WAIS Inc to start Blue Angel Technologies. Her "WAIS variant" formed the basis of MetaStar. Georgios Papadopoulos left to found Atypon. François Schiettecatte left Human Genome Project at Johns Hopkins Hospital and started FS-Consult and developed his own variant of WAIS which eventually became ScienceServer, which was later sold to Elsevier Science. Kahle and Gilliat went on to found the Internet Archive and Alexa Internet.
WAIS and Gopher.
Public WAIS is often used as a full text search engine for individual Internet Gopher servers, supplementing the popular Veronica system which only searches the menu titles of Gopher sites. WAIS and Gopher share the World Wide Web's client–server architecture and a certain amount of its functionality. The WAIS protocol is influenced largely by the z39.50 protocol designed for networking library catalogs. It allows a text-based search, and retrieval following a search. Gopher provides a free text search mechanism, but principally uses menus. A menu is a list of titles, from which the user may pick one. While gopher space is a web containing many loops, the menu system gives the user the impression of a tree.
The Web's data model is similar to the gopher model, except that menus are generalized to hypertext documents. In both cases, simple file servers generate the menus or hypertext directly from the file structure of a server. The Web's hypertext model permits the author more freedom to communicate the options available to the reader, as it can include headings and various forms of list structure.

</doc>
<doc id="41862" url="http://en.wikipedia.org/wiki?curid=41862" title="Warner exemption">
Warner exemption

In telecommunication, a Warner exemption is a statutory exemption pertaining to the acquisition of telecommunications systems that meet the exclusionary criteria of the Warner Amendment, Public Law 97-86, 1 December 1981, which is also known as the Brooks Bill. 
Use of FTS2000 by U.S. Government agencies is mandatory when telecommunications are required. However, the Warner Amendment excludes the mandatory use of FTS2000 in instances related to maximum security.

</doc>
<doc id="41863" url="http://en.wikipedia.org/wiki?curid=41863" title="Waveguide">
Waveguide

A waveguide is a structure that guides waves, such as electromagnetic waves or sound waves. There are different types of waveguides for each type of wave. The original and most common meaning is a hollow conductive metal pipe used to carry high frequency radio waves, particularly microwaves.
The geometry of a waveguide reflects its function. Slab waveguides confine energy to travel only in one dimension, fiber or channel waveguides for two dimensions. The frequency of the transmitted wave also dictates the shape of a waveguide: an optical fiber guiding high-frequency light will not guide microwaves of a much lower frequency. As a rule of thumb, the width of a waveguide needs to be of the same order of magnitude as the wavelength of the guided wave.
Some naturally occurring structures can also act as waveguides. The SOFAR channel layer in the ocean can guide the sound of whale song across enormous distances.
Principle of operation.
Waves propagate in all directions in open space as spherical waves. The power of the wave falls with the distance "R" from the source as the square of the distance(inverse square law). A waveguide confines the wave to propagate in one dimension, so that, under ideal conditions, the wave loses no power while propagating.
The conductors generally used in waveguides have small skin depth and hence large surface conductance. Due to total reflection at the walls, waves are confined to the interior of a waveguide. The propagation inside the waveguide, hence, can be described approximately as a "zigzag" between the walls. This description is exact for electromagnetic waves in a hollow metal tube with a rectangular or circular cross-section.
History.
The first structure for guiding waves was proposed by J. J. Thomson in 1893, and was first experimentally tested by Oliver Lodge in 1894. The first mathematical analysis of electromagnetic waves in a metal cylinder was performed by Lord Rayleigh in 1897.
For sound waves, Lord Rayleigh published a full mathematical analysis of propagation modes in his seminal work, “The Theory of Sound”.
The study of dielectric waveguides (such as optical fibers, see below) began as early as the 1920s, by several people, most famous of which are Rayleigh, Sommerfeld and Debye. 
Optical fiber began to receive special attention in the 1960s due to its importance to the communications industry.
Uses.
The uses of waveguides for transmitting signals were known even before the term was coined. The phenomenon of sound waves guided through a taut wire have been known for a long time, as well as sound through a hollow pipe such as a cave or medical stethoscope. Other uses of waveguides are in transmitting power between the components of a system such as radio, radar or optical devices. Waveguides are the fundamental principle of guided wave testing (GWT), one of the many methods of non-destructive evaluation.
Specific examples:
Propagation modes and cutoff frequencies.
A propagation mode in a waveguide is one solution of the wave equations, or, in other words, the form of the wave. Due to the constraints of the boundary conditions, there are only limited frequencies and forms for the wave function which can propagate in the waveguide. The lowest frequency in which a certain mode can propagate is the cutoff frequency of that mode. The mode with the lowest cutoff frequency is the basic mode of the waveguide, and its cutoff frequency is the waveguide cutoff frequency.
Impedance matching.
In circuit theory, the impedance is a generalization of electrical resistivity in the case of alternating current, and is measured in ohms (formula_1).
A waveguide in circuit theory is described by a transmission line having a length and self impedance. In other words the impedance is the resistance of the circuit component (in this case a waveguide) to the propagation of the wave. This description of the waveguide was originally intended for alternating current, but is also suitable for electromagnetic and sound waves, once the wave and material properties (such as pressure, density, dielectric constant) are properly converted into electrical terms (current and impedance for example). 
Impedance matching is important when components of an electric circuit are connected (waveguide to antenna for example): The impedance ratio determines how much of the wave is transmitted forward and how much is reflected. In connecting a waveguide to an antenna a complete transmission is usually required, so that their impedances are matched.
The reflection coefficient can be calculated using: formula_2, where formula_3 is the reflection coefficient (0 denotes full transmission, 1 full reflection, and 0.5 is a reflection of half the incoming voltage), formula_4 and formula_5 are the impedance of the first component (from which the wave enters) and the second component, respectively.
An impedance mismatch creates a reflected wave, which added to the incoming waves creates a standing wave. An impedance mismatch can be also quantified with the standing wave ratio (SWR or VSWR for voltage), which is connected to the impedance ratio and reflection coefficient by: formula_6, where formula_7 are the minimum and maximum values of the voltage absolute value, and the VSWR is the voltage standing wave ratio, which value of 1 denotes full transmission, without reflection and thus no standing wave, while very large values mean high reflection and standing wave pattern.
Electromagnetic waveguides.
Waveguides can be constructed to carry waves over a wide portion of the electromagnetic spectrum, but are especially useful in the microwave and optical frequency ranges. Depending on the frequency, they can be constructed from either conductive or dielectric materials. Waveguides are used for transferring both power and communication signals.
Optical waveguides.
Waveguides used at optical frequencies are typically dielectric waveguides, structures in which a dielectric material with high permittivity, and thus high index of refraction, is surrounded by a material with lower permittivity. The structure guides optical waves by total internal reflection. An example of an optical waveguide is optical fiber.
Other types of optical waveguide are also used, including photonic-crystal fiber, which guides waves by any of several distinct mechanisms. Guides in the form of a hollow tube with a highly reflective inner surface have also been used as light pipes for illumination applications. The inner surfaces may be polished metal, or may be covered with a multilayer film that guides light by Bragg reflection (this is a special case of a photonic-crystal fiber). One can also use small prisms around the pipe which reflect light via total internal reflection —such confinement is necessarily imperfect, however, since total internal reflection can never truly guide light within a "lower"-index core (in the prism case, some light leaks out at the prism corners).
Acoustic waveguides.
An "acoustic waveguide" is a physical structure for guiding sound waves. A duct for sound propagation also behaves like a transmission line. The duct contains some medium, such as air, that supports sound propagation.
Sound synthesis.
Sound synthesis uses digital delay lines as computational elements to simulate wave propagation in tubes of wind instruments and the vibrating strings of string instruments.

</doc>
<doc id="41864" url="http://en.wikipedia.org/wiki?curid=41864" title="Wave impedance">
Wave impedance

The wave impedance of an electromagnetic wave is the ratio of the transverse components of the electric and magnetic fields (the transverse components being those at right angles to the direction of propagation). For a transverse-electric-magnetic (TEM) plane wave traveling through a homogeneous medium, the wave impedance is everywhere equal to the intrinsic impedance of the medium. In particular, for a plane wave travelling through empty space, the wave impedance is equal to the impedance of free space. The symbol "Z" is used to represent it and it is expressed in units of ohms. The symbol η (eta) may be used instead of "Z" for wave impedance to avoid confusion with electrical impedance.
The wave impedance is given by
where formula_2 is the electric field and formula_3 is the magnetic field, in phasor representation.
In terms of the parameters of an electromagnetic wave and the medium it travels through, the wave impedance is given by
where μ is the magnetic permeability, ε is the electric permittivity and σ is the electrical conductivity of the material the wave is travelling through. In the equation, "j" is the imaginary unit, and ω is the angular frequency of the wave. In the case of a dielectric (where the conductivity is zero), the equation reduces to
As usual for any electrical impedance, the ratio is defined only for the frequency domain and never in the time domain.
Wave impedance in free space.
In free space the wave impedance of plane waves is: 
and:
hence, to the same accuracy as the current definition of formula_8, the value in ohms is:
Wave impedance in an unbounded dielectric.
In a isotropic, homogeneous dielectric with negligible magnetic properties, i.e. formula_10 H/m and formula_11 F/m. So, the value of wave impedance in a perfect dielectric is
In a perfect dielectric, the wave impedance can be found by dividing "Z"0 by the square root of the dielectric constant.
Wave impedance in a waveguide.
For any waveguide in the form of a hollow metal tube, (such as rectangular guide, circular guide, or double-ridge guide), the wave impedance of a travelling wave is dependent on the frequency formula_13, but is the same throughout the guide. For transverse electric (TE) modes of propagation the wave impedance is:
where "f""c" is the cut-off frequency of the mode, and for transverse magnetic (TM) modes of propagation the wave impedance is:
Above the cut-off ("f" > "f""c"), the impedance is real (resistive) and the wave carries energy. Below cut-off the impedance is imaginary (reactive) and the wave is evanescent. These expressions neglect the effect of resistive loss in the walls of the waveguide. For a waveguide entirely filled with a homogeneous dielectric medium, similar expressions apply, but with the wave impedance of the medium replacing "Z"0. The presence of the dielectric also modifies the cut-off frequency "f""c".
For a waveguide or transmission line containing more than one type of dielectric medium (such as microstrip), the wave impedance will in general vary over the cross-section of the line.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="41865" url="http://en.wikipedia.org/wiki?curid=41865" title="White facsimile transmission">
White facsimile transmission

In telecommunication, the term white facsimile transmission has the following meanings: 
Both of these terms became outmoded in the late 20th century except in specialist usage, as most fax machines now use the digital ITU-T fax standards, which encode the image digitally over a QAM-modulated signal. See also Black facsimile transmission.

</doc>
<doc id="41867" url="http://en.wikipedia.org/wiki?curid=41867" title="Wide Area Telephone Service">
Wide Area Telephone Service

In North American telecommunications, Wide Area Telephone Service (WATS) was a flat-rate long distance service offering for customer dial-type telecommunications between a given customer phone (also known as a "station") and stations within specified geographic rate areas employing a single telephone line between the customer location and the serving central office. Each access line could be arranged for outward (OUT-WATS) or inward (IN-WATS) service, or both.
WATS was introduced by the Bell System in 1961 as a primitive long-distance flat-rate plan by which a business could obtain a special line with an included number of hours ('measured time' or 'full-time') of long-distance calling to a specified area. These lines were most often connected to private branch exchanges in large businesses. WATS lines were the basis for the first direct-dial toll free +1-800 numbers (intrastate in 1966, interstate in 1967); by 1976, WATS brought AT&T a billion dollars in annual revenue.
For outbound calls, the 1984 AT&T divestiture brought multiple competitors offering similar services using standard business telephone lines; the special WATS line was ultimately supplanted by other flat-rate offerings. The requirement that an inbound toll-free number terminate at a special WATS line or fixed-rate service was also rendered obsolete by the 1980s due to intelligent network capability and technological improvement in the +1-800 service. A toll-free number may now terminate at a T carrier line, at any standard local telephone number or at one of multiple destinations based on time of day, call origin, cost or other factors.
Outbound WATS.
For Outbound WATS, the United States was divided into geographical Bands 0 through 5, relative to the purchaser. Band zero was intrastate calling and bands 1 through 5 (or 6) were interstate calls that were progressively further from the originating number. Historically the higher band number carried a higher price per month or per minute. These lines could be used for outbound long distance only; not local. In the U.S., interstate WATS lines could not be used for intrastate calls, and vice versa. With wider availability of inexpensive long distance using regular business lines, OutWATS service became obsolete late in the 20th century.
InWATS.
The original North American toll-free number was the Zenith number, published in one distant city (or a few cities) only. Published as "Zenith" and a five-digit number, these collect calls required operator assistance. The called party was charged for the operator-assisted call.
With "inward WATS", introduced for interstate calls by AT&T in 1967, subscribers were issued a toll-free telephone number in a designated toll-free area code. Unlike a standard collect call or a call to a Zenith number, +1-800- normally may be dialled directly with no live operator. Callers within a designated area could call without incurring a toll charge as the recipient paid for the calls at a fixed rate.
The introduction of InWATS fortuitously fell around the same time as the early centralised, automated national airline and hotel reservation systems, including Sabre (American Airlines, 1963), Holidex (Holiday Inn, 1965) and Reservatron (Sheraton, 1969). Hundreds of local reservation numbers for a major chain could be replaced with one central number, backed by a national computerised reservation system.
InWATS exchanges were assigned to Canada and other North American Numbering Plan countries, but the original InWATS in each country accepted domestic calls only. Initially +1-800-NN2-XXXX numbers were US intrastate and specific prefixes (such as +1-800-387 Toronto and +1-800-267 Ottawa) were assigned to Canada. In the 1970s, AT&T's internal routing guides included separate US and Canadian 1-800 exchange maps which looked much like area code maps as each geographic area code had one or more specific freephone exchange prefixes. Sheraton's 800-325-3535, one of the notable early adopters in late 1969, was hard-wired into St. Louis area code 314; 1-800-HOLIDAY at that time could not be a US number if the 1-800-465 prefix was hard-wired to Thunder Bay's area code 807. Any attempt to call a foreign +1-800 gave a pre-recorded error, "the number you have dialled is not available from your calling area."
Like the OutWATS service, AT&T's InWATS was divided into intrastate and interstate, with interstate calls priced into five or six "bands" of calling. This favoured placement of US national call centres in low-population midwestern states such as Nebraska, whose central location meant a carefully situated "band 3" number reaching halfway across the US in every direction could potentially reach 47 states. A San Diego call centre would be less fortunate; even with "band 6" (the most expensive lines) its 'national' number would be unreachable to millions as California is a populous state and intrastate calls needed a separate toll-free number.
The original InWATS system was supplanted by "Advanced 800 Service" in the 1980s. Modern systems eliminated requirements tying toll-free numbers to dedicated flat-rate inbound WATS lines. Direct inward dial, introduced in 1983, allowed one trunk to carry calls for multiple numbers. AT&T's monopoly on US toll-free number routing ended in 1986, encouraging flexibility in order to match rivals Sprint and MCI. By 1989, fixed "bands" of coverage area had been largely replaced by distance-based billing, a growing number of 1-800 numbers were being terminated at standard local business or residence lines and one number could be sent to multiple locations based on call origin, least-cost routing or time of day routing. RespOrgs were established in the US in 1993 and Canada in 1994 to provide toll free number portability using the Service Management System (SMS/800) database. Calls from Canada and the US, intrastate and interstate, could terminate at the same 1-800 number, even via different carriers. Vanity numbers became easier to obtain as a toll-free exchange prefix was no longer tied to a geographic location. By the 21st century, Voice over IP placed toll-free and foreign exchange numbers into the hands of even the smallest users, to whom dedicated inbound lines under the original InWATS model would have been prohibitively expensive.

</doc>
<doc id="41868" url="http://en.wikipedia.org/wiki?curid=41868" title="Wideband modem">
Wideband modem

In telecommunication, the term wideband modem has the following meanings: 

</doc>
<doc id="41869" url="http://en.wikipedia.org/wiki?curid=41869" title="Wildcard character">
Wildcard character

The term wildcard character has several meanings.
Telecommunication.
In telecommunications, a wildcard is a character that may be substituted for any of a defined subset of all possible characters.
Computing.
In computer (software) technology, a wildcard character can be used to substitute for any other character or characters in a string.
File and directory patterns.
When specifying file names (or paths) in CP/M, DOS, Microsoft Windows, and Unix-like operating systems, the asterisk pattern character ("*", also called "star") matches zero or more characters.
In Unix-like and DOS operating systems, the question mark ("?") matches exactly one character; in DOS, it will also match missing (zero) trailing characters. For example, in DOS, the pattern codice_1 will match codice_1 or codice_1, but not codice_1.
In Unix shells and Windows PowerShell, ranges of characters enclosed in square brackets ("[" and "]") match a single character within the range; for example, codice_1 matches any single uppercase or lowercase letter. Unix shells allow negation of the specified character set by using a leading "!" (e.g., codice_1, which will match names like codice_1). In shells that interpret "!" as a history substitution, a leading "^" can be used instead of "!" to negate the character set.
The operation of matching of wildcard patterns to multiple file or path names is referred to as "globbing".
Databases.
In SQL, wildcard characters can be used in "LIKE" expressions; the percent sign (%) matches zero or more characters, and underscore (_) a single character. Transact-SQL also supports square brackets ("[" and "]") to list sets and ranges of characters to match, a leading caret (^) matches only a character not specified within the brackets. In Microsoft Access, wildcard characters can be used in "LIKE" expressions; the asterisk sign (*) matches zero or more characters, and the question mark (?) matches a single character.
Regular expressions.
In regular expressions, the period (".", also called "dot") is the wildcard pattern character that matches a single character. Combined with the asterisk operator (.*) it will match any number of characters.

</doc>
<doc id="41870" url="http://en.wikipedia.org/wiki?curid=41870" title="Wink pulsing">
Wink pulsing

Wink is used both in connection with DC signaling on a trunk, and with indicator lamps on a key telephone.
In telephone switching systems, wink pulsing is recurring pulsing in which the off-condition is relatively short compared to the on-condition. In Wink start trunks, the exchange at the originating end sends an off-hook to alert to a call. The terminating end indicates readiness to receive the dialed telephone number by sending an off-hook of approximately half a second duration, or "wink". Upon receiving this go ahead signal, the originating end uses multi-frequency or other address signalling to send the phone number.
On 1A2 key systems or similar key-operated telephone instruments, the hold position, "i.e.," the hold condition, of a line is often indicated by winking the associated lamp at 120 impulses per minute. During 6% of the pulse period the lamp is off and 94% of the period the lamp is on, "i.e.," 30 ms (milliseconds) off and 470 ms on.

</doc>
<doc id="41871" url="http://en.wikipedia.org/wiki?curid=41871" title="Wireless mobility management">
Wireless mobility management

Wireless mobility management: in Personal Communications Service (PCS), the assigning and controlling of wireless links for terminal network connections. Wireless mobility management provides an "alerting" function for call completion to a wireless terminal, monitors wireless link performance to determine when an automatic link transfer is required, and coordinates link transfers between wireless access interfaces.
One use of this is wireless push technology, by pushing data across wireless networks, this coordinates the link transfers and pushes data between the backend and wireless device only when an established connection is found.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="41872" url="http://en.wikipedia.org/wiki?curid=41872" title="Work station">
Work station

A workstation or work station may refer to:

</doc>
<doc id="41873" url="http://en.wikipedia.org/wiki?curid=41873" title="X-dimension of recorded spot">
X-dimension of recorded spot

In fax systems, the X-dimension of recorded spot is the effective recorded spot dimension measured in the direction of the recorded line. By "effective recorded spot dimension" is meant the largest center-to-center spacing between recorded spots, which gives minimum peak-to-peak variation of density of the recorded line. "X-dimension of recorded spot" implies that the facsimile equipment response to a constant density in the object (original) is a succession of discrete recorded spots.

</doc>
<doc id="41877" url="http://en.wikipedia.org/wiki?curid=41877" title="Zero-dispersion wavelength">
Zero-dispersion wavelength

In a single-mode optical fiber, the zero-dispersion wavelength is the wavelength or wavelengths at which material dispersion and waveguide dispersion cancel one another. In all silica-based optical fibers, minimum material dispersion occurs naturally at a wavelength of approximately 1300nm. Single-mode fibers may be made of silica-based glasses containing dopants that shift the material-dispersion wavelength, and thus, the zero-dispersion wavelength, toward the minimum-loss window at approximately 1550nm. The engineering tradeoff is a slight increase in the minimum attenuation coefficient. Such fiber is called dispersion-shifted fiber.
Another way to alter the dispersion is changing the core size and the refractive indices of the material of core and cladding. Because fiber optic materials are already highly optimized for low scattering and high transparency alternative ways to change the refractive index were investigated. As a straight forward solution tapered fibers and holey fibers or photonic crystal fibers (PCF) were produced. Essentially they replace the cladding by air. This improves the contrast of refractive indices by a factor of 10. Therefore the effective index is changed, especially for longer wavelengths. This type of refractive index change versus wavelength due to different geometry is called waveguide dispersion.
As these narrow waveguides (~1-3 µm core diameter) are combined with ultrashort pulses at the zero-dispersion wavelength pulses are not instantly destroyed by dispersion. After reaching a certain peak power within the pulse the non-linear refractive index starts to play an important role leading to frequency generation processes like self-phase modulation (SPM), modulational instability, soliton generation and soliton fission, cross phase modulation (XPM) and others. All these processes generate new frequency components, meaning that input light with narrow bandwidth expands into a wide range of new colours, through a process called supercontinuum generation.
The term is also used, more loosely, in multi-mode optical fiber. There, it refers to the wavelength at which the material dispersion is minimum, "i.e." "essentially" zero. This is more accurately called the minimum-dispersion wavelength.
Zero-dispersion slope.
The rate of change of dispersion with respect to wavelength at the zero-dispersion point is called the zero-dispersion slope. Doubly and quadruply clad single-mode fibers have two zero-dispersion points, and thus two zero-dispersion slopes.

</doc>
<doc id="41878" url="http://en.wikipedia.org/wiki?curid=41878" title="Zip-cord">
Zip-cord

Zip-cord is a type of electrical cable with two or more conductors held together by an insulating jacket that can be easily separated simply by pulling apart. The term is also used with optical fiber cables consisting of two optical fibers joined in a similar manner. The design of zip-cord makes it easy to keep conductors that carry related electrical or optical signals together and helps avoid tangling of cables. Typical uses include lamp cord and speaker wire. Conductors may be identified by a color tracer on the insulation, or by a ridge molded into the insulation of one wire, or by a colored tracer thread inside the insulation. Zip cords are intended for use on portable equipment, and the US and Canadian electrical codes do not permit their use for permanently installed wiring of line-voltage circuits. 

</doc>
<doc id="41879" url="http://en.wikipedia.org/wiki?curid=41879" title="Absolute pitch">
Absolute pitch

Absolute pitch (AP), widely referred to as perfect pitch, is a rare auditory phenomenon characterized by the ability of a person to identify or re-create a given musical note without the benefit of a reference tone.
AP can be demonstrated via linguistic labeling ("naming" a note), auditory imagery, or sensorimotor responses. For example, an AP possessor can accurately reproduce a heard tone on their musical instrument without "hunting" for the correct pitch. Researchers estimate the occurrence of AP to be 1 in 10,000 people.
Generally, absolute pitch implies some or all of the following abilities, achieved without a reference tone:
People may have absolute pitch along with the ability of relative pitch, and relative and absolute pitch work together in actual musical listening and practice, but strategies in using each skill vary. Those with absolute pitch may train their relative pitch, but there are no reported cases of an adult obtaining absolute pitch ability through musical training; adults who possess relative pitch, but who do not already have absolute pitch, can learn "pseudo-absolute pitch", and become able to identify notes in a way that superficially resembles absolute pitch. Moreover, training pseudo-absolute pitch requires considerable motivation, time, and effort, and learning is not retained without constant practice and reinforcement.
Scientific studies.
History of study and terminologies.
Scientific study of absolute pitch appears to have commenced in the 19th century, focusing on the phenomenon of musical pitch and methods of measuring it. While the term "absolute pitch", or "absolute ear", was in use by the late 19th century by both British and German researchers, its application was not universal; other terms such as "musical ear", "absolute tone consciousness", or "positive pitch" were also used to refer to the ability. The skill is not exclusively musical, or limited to human perception; absolute pitch has been demonstrated in animals such as bats, wolves, gerbils, and birds, for whom specific pitches facilitate identification of mates or meals.
Difference in cognition, not elementary sensation.
Physically and functionally, the auditory system of an absolute listener does not appear to be different from a non-absolute listener. Rather, "it reflects a particular ability to analyze frequency information, presumably involving high-level cortical processing." Absolute pitch is an act of cognition, needing memory of the frequency, a label for the frequency (such as "B-flat"), and exposure to the range of sound encompassed by that categorical label. Absolute pitch may be directly analogous to recognizing colors, phonemes (speech sounds) or other categorical perception of sensory stimuli. Just as most people have learned to recognize and name the color "blue" by the frequencies of the electromagnetic radiation that is perceived as light, it is possible that those who have been exposed to musical notes together with their names early in life will be more likely to identify, for example, the note C. Absolute pitch may also be related to certain genes, possibly an autosomal dominant genetic trait, though it "might be nothing more than a general human capacity whose expression is strongly biased by the level and type of exposure to music that people experience in a given culture."
Influence by music experience.
Absolute pitch sense appears to be influenced by cultural exposure to music, especially in the familiarization of the equal-tempered C-major scale. Most of the absolute listeners that were tested in this respect identified the C-major tones more reliably and, except for B, more quickly than the five "black key" tones, which corresponds to the higher prevalence of these tones in ordinary musical experience. One study of Dutch non-musicians also demonstrated a bias toward using C-major tones in ordinary speech, especially on syllables related to emphasis.
Linguistics.
Absolute pitch is more common among speakers of tonal languages such as most dialects of Chinese or Vietnamese, which often depend on pitch variation as the means of distinguishing words that otherwise sound the same; e.g. Mandarin with four possible pitch variations, Cantonese with nine, Minnan with seven or eight (depending on dialect), and Vietnamese with six. Speakers of Sino-Tibetan languages have been reported to speak a word in the same absolute pitch (within a quarter-tone) on different days; it has therefore been suggested that absolute pitch may be acquired by infants when they learn to speak a tonal language (and possibly also by infants when they learn to speak a pitch-accent language). However, the brains of tonal-language speakers do not naturally process musical sound as language; perhaps such speakers are more likely to acquire absolute pitch for musical tones when they later receive musical training. Many native speakers of a tone language, even those with little musical training, are observed to sing a given song consistently with regard to pitch. Among music students of East Asian ethnic heritage, those who speak a tone language very fluently have a much higher prevalence of absolute pitch than those who do not speak a tone language.
It is possible that African level-tone languages—such as Yoruba, with three pitch levels, and Mambila, with four—may be better suited to study the role of absolute pitch in speech than the pitch and contour tone languages of East Asia.
Speakers of European languages have been found to make subconscious use of an absolute pitch memory when speaking.
Perception.
Absolute pitch is the ability to perceive pitch class and to mentally categorize sounds according to perceived pitch class. "Pitch class" is a tonal quality which recurs among tones which share the relationship of an octave. While the boundaries of musical pitch categories vary among human cultures, the recognition of octave relationships is a natural characteristic of the mammalian auditory system. Accordingly, absolute pitch is not the ability to estimate a pitch value from the dimension of pitch evoking frequency (30–5000 Hz), but to identify a pitch class category within the dimension of pitch class (e.g., C-C♯-D ... B-C).
An absolute listener's sense of hearing is typically no keener than that of a non-absolute ("normal") listener. Absolute pitch does not depend upon a refined ability to perceive and discriminate gradations of sound frequencies, but upon detecting and categorizing a subjective perceptual quality typically referred to as "chroma". The two tasks— of identification (recognizing and naming a pitch) and discrimination (detecting changes or differences in rate of vibration)— are accomplished with different brain mechanisms.
Special populations.
The prevalence of absolute pitch is higher among those who are blind from birth as a result of optic nerve hypoplasia.
Absolute pitch is considerably more common among those whose early childhood was spent in East Asia. This might seem to be a genetic difference; but people of East Asian ancestry who are reared in North America are significantly less likely to develop absolute pitch than those raised in East Asia, so the difference is more probably explained by experience. The language that is spoken may be an important factor; many East Asians speak tonal languages such as Mandarin and Cantonese, while others (such as those in Japan and certain provinces of Korea) speak pitch-accent languages, and the prevalence of absolute pitch may be partly explained by exposure to pitches together with meaningful musical labels very early in life.
Absolute pitch ability has higher prevalence among those with Williams Syndrome and those with an autism spectrum disorder, with rates as high as 30% claimed, stating that the rate among musicians in general is far lower.
Nature vs. nurture.
Absolute pitch might be achievable by any human being during a critical period of auditory development, after which period cognitive strategies favor global and relational processing. Proponents of the critical-period theory agree that the presence of absolute pitch ability is dependent on learning, but there is disagreement about whether training causes absolute skills to occur or lack of training causes absolute perception to be overwhelmed and obliterated by relative perception of musical intervals.
There may be a genetic locus for absolute pitch ability, which locus would suggest a genetic basis for its presence or absence. A genetic basis, should it exist, might represent either a predisposition for learning the ability or signal the likelihood of its spontaneous occurrence.
Researchers have been trying to teach absolute pitch ability in laboratory settings for more than a century, and various commercial absolute-pitch training courses have been offered to the public since the early 1900s. However, no adult has ever been documented to have acquired absolute listening ability, because all adults who have been formally tested after AP training have failed to demonstrate "an unqualified level of accuracy... comparable to that of AP possessors".
Pitch memory related to musical context.
While very few people have the ability to name a pitch with no external reference, pitch memory can be activated by repeated exposure. People who are not skilled singers will often sing popular songs in the correct key, and can usually recognize when TV themes have been shifted into the wrong key. Members of the Venda culture in South Africa also sing familiar children's songs in the key in which the songs were learned.
This phenomenon is apparently unrelated to musical training. The skill may be associated more closely with vocal production. Violin students learning the Suzuki method are required to memorize each composition in a fixed key and play it from memory on their instrument, but they are not required to sing. When tested, these students did not succeed in singing the memorized Suzuki songs in the original, fixed key.
Possible problems.
Musicians with absolute perception may experience difficulties which do not exist for other musicians. Because absolute listeners are capable of recognizing that a musical composition has been transposed from its original key, or that a pitch is being produced at a nonstandard frequency (either sharp or flat), a musician with absolute pitch may become distressed upon perceiving tones they believe to be "wrong" or hearing a piece of music "in the wrong key." This can especially apply to Baroque music that is recorded in Baroque tuning (usually a half step down from standard concert pitch).
An absolute listener may also use absolute strategies for tasks which are more efficiently accomplished with relative strategies, such as transposition or producing harmony for tones whose frequencies do not match standard equal temperament. It is also possible for some musicians to have skewed absolute pitch, where all notes are slightly flat or slightly sharp. When playing in groups with other musicians, this may lead to playing in a tonality that is slightly different from the rest of the group.
Synesthesia.
Absolute pitch shows a genetic overlap with synesthesia/ideasthesia, and some individuals with music-related synesthesia also have perfect pitch. They may associate certain notes or keys with different colors, enabling them to tell what any note or key is. It is unknown how many people with perfect pitch are also synesthetes.
Correlation with musical talent.
Absolute pitch is not a prerequisite for skilled musical performance or composition. However, musicians with absolute pitch tend to perform better on musical transcription tasks (controlling for age of onset and amount of musical training) compared with those without absolute pitch. It has been argued that musicians with absolute pitch perform worse than those without absolute pitch on recognition of musical intervals. However, experiments on which this conclusion was based contained an artifact, and when this artifact was removed, absolute pitch possessors were found to perform better than nonpossessors on recognition of musical intervals.
Owing to uncertainty in the historical record, it is often impossible to determine whether notable composers and musicians had absolute pitch. Since absolute pitch is rare in European musical culture, claims that any particular musician possessed it are difficult to evaluate. Among composers of the Baroque and Classical eras, evidence is available only for Mozart, who is documented to have demonstrated the ability at age 3. Experts have only surmised that Beethoven had it, as indicated from some excerpts from his letters. By the 19th century, it became more common for the presence of absolute pitch to be recorded, identifying the ability to be present in musicians such as Camille Saint-Saëns and John Philip Sousa.
Relative pitch.
Many musicians have quite good relative pitch, a skill which can be learned through ear training. With practice, it is possible to listen to a single known pitch once (from a pitch pipe or a tuning fork) and then have stable, reliable pitch identification by comparing the notes heard to the stored memory of the tonic pitch. Unlike absolute pitch, this skill is dependent on a recently perceived tonal center.

</doc>
<doc id="41881" url="http://en.wikipedia.org/wiki?curid=41881" title="All About Eve">
All About Eve

All About Eve is a 1950 American drama film written and directed by Joseph L. Mankiewicz, and produced by Darryl F. Zanuck. It was based on the 1946 short story "The Wisdom of Eve" by Mary Orr, although screen credit was not given for it.
The film stars Bette Davis as Margo Channing, a highly regarded but aging Broadway star. Anne Baxter plays Eve Harrington, an ambitious young fan who insinuates herself into Channing's life, ultimately threatening Channing's career and her personal relationships. George Sanders, Celeste Holm, Hugh Marlowe, Barbara Bates, Gary Merrill, and Thelma Ritter also appear, and the film provided one of Marilyn Monroe's earliest important roles.
Praised by critics at the time of its release, "All About Eve" was nominated for 14 Academy Awards (a feat unmatched until the 1997 film "Titanic") and won six, including Best Picture. s of 2015[ [update]], "All About Eve" is still the only film in Oscar history to receive four female acting nominations (Davis and Baxter as Best Actress, Holm and Ritter as Best Supporting Actress). "All About Eve" was selected in 1990 for preservation in the United States National Film Registry and was among the first 50 films to be registered. "All About Eve" appeared at #16 on AFI's 1998 list of the 100 best American films.
Plot.
At an awards dinner, Eve Harrington—the newest and brightest star on Broadway—is being presented the Sarah Siddons Award for her breakout performance as Cora in "Footsteps on the Ceiling". Theatre critic Addison DeWitt observes the proceedings and, in a sardonic voiceover, recalls how Eve's star rose as quickly as it did.
The film flashes back a year. Margo Channing is one of the biggest stars on Broadway, but despite her success she is bemoaning her age, having just turned forty and knowing what that will mean for her career. After a performance one night, Margo's close friend Karen Richards, wife of the play's author Lloyd Richards (Hugh Marlowe), meets besotted fan Eve Harrington in the cold alley outside the stage door. Recognizing her from having passed her many times in the alley (as Eve claims to have seen every performance of Margo's current play, "Aged in Wood"), Karen takes her backstage to meet Margo. Eve tells the group gathered in Margo's dressing room—Karen and Lloyd, Margo's boyfriend Bill Sampson, a director who is eight years her junior, and Margo's maid Birdie—that she followed Margo's last theatrical tour to New York after seeing her in a play in San Francisco. She tells a moving story of growing up poor and losing her young husband in the recent war. Moved, Margo quickly befriends Eve, takes her into her home, and hires her as her assistant, leaving Birdie, who instinctively dislikes Eve, feeling put out.
Eve is gradually shown to be working to supplant Margo, scheming to become her understudy behind her back, driving wedges between her and Lloyd and Bill, and conspiring with an unsuspecting Karen to cause Margo to miss a performance. Eve, knowing in advance that she will be the one appearing that night, invites the city's theatre critics to attend that evening's performance, which is a triumph for her. Eve tries to seduce Bill, but he rejects her. Following a scathing newspaper column by Addison, Margo and Bill reconcile, dine with the Richardses, and decide to marry. That same night at the restaurant, Eve blackmails Karen into telling Lloyd to give her the part of Cora, by threatening to tell Margo of Karen's role in Margo's missed performance. Before Karen can talk with Lloyd, Margo announces to everyone's surprise that she does not wish to play Cora and would prefer to continue in "Aged in Wood". Eve secures the role and attempts to climb higher by using Addison, who is beginning to doubt her. Just before the premiere of her play at the Shubert in New Haven, Eve presents Addison with her next plan: to marry Lloyd, who, she claims, has come to her professing his love and his eagerness to leave his wife for her. Now, Eve exults, Lloyd will write brilliant plays showcasing "her". Unseen but mentioned in dialogue, Karen has begun to suspect Eve as a threat to her own marriage to Lloyd, and so she and Addison met for lunch and helped each other put the pieces about Eve together. Addison is infuriated that Eve has attempted to use him and reveals that he knows that her back story is all lies. Her real name is Gertrude Slojinski, she was never married, and she had been paid to leave her hometown over an affair with her boss, a brewer in Wisconsin. Addison blackmails Eve, informing her that she will not be marrying Lloyd or anyone else; in exchange for Addison's silence, she now "belongs" to him.
The film returns to the opening scene in which Eve, now a shining Broadway star headed for Hollywood, is presented with her award. In her speech, she thanks Margo and Bill and Lloyd and Karen with characteristic effusion, while all four stare back at her coldly. After the awards ceremony, Eve hands her award to Addison, skips a party in her honor, and returns home alone, where she encounters a young fan—a high-school girl—who has slipped into her apartment and fallen asleep. The young girl professes her adoration and begins at once to insinuate herself into Eve's life, offering to pack Eve's trunk for Hollywood and being accepted. "Phoebe", as she calls herself, answers the door to find Addison returning with Eve's award. In a revealing moment, the young girl flirts daringly with the older man. Addison hands over the award to Phoebe and leaves without entering. Phoebe then lies to Eve, telling her it was only a cab driver who dropped off the award. While Eve rests in the other room, Phoebe dons Eve's elegant costume robe and poses in front of a multi-paned mirror, holding the award as if it were a crown. The mirrors transform Phoebe into multiple images of herself, and she bows regally, as if accepting the award to thunderous applause, while triumphant music plays.
Production.
Development.
The story of "All About Eve" originated in an anecdote related to Mary Orr by actress Elisabeth Bergner. While performing in "The Two Mrs. Carrolls" during 1943 and 1944, Bergner allowed a young fan to become part of her household and employed her as an assistant, but later regretted her generosity when the woman attempted to undermine her. Referring to her only as "the terrible girl", Bergner related the events to Orr, who used it as the basis for her short story "The Wisdom of Eve" (1946). In the story, Orr gives the girl a more ruthless character and allows her to succeed in stealing the older actress' career. Bergner later confirmed the basis of the story in her autobiography "Bewundert viel, und viel gescholten" ("Greatly Admired and Greatly Scolded").
In 1949, Mankiewicz was considering a story about an aging actress and, upon reading "The Wisdom of Eve", felt the conniving girl would be a useful added element. He sent a memo to Darryl F. Zanuck saying it "fits in with an original idea [of mine] and can be combined. Superb starring role for Susan Hayward." Mankiewicz presented a film treatment of the combined stories under the title "Best Performance". He changed the main character's name from Margola Cranston to Margo Channing and retained several of Orr's characters — Eve Harrington, Lloyd and Karen Richards, and Miss Casswell — while removing Margo Channing's husband completely and replacing him with a new character, Bill Sampson. The intention was to depict Channing in a new relationship and allow Eve Harrington to threaten both Channing's professional and personal lives. Mankiewicz also added the characters Addison DeWitt, Birdie Coonan, Max Fabian, and Phoebe.
Zanuck was enthusiastic and provided numerous suggestions for improving the screenplay. In some sections, he felt Mankiewicz's writing lacked subtlety or provided excessive detail. He suggested diluting Birdie Coonan's jealousy of Eve so the audience would not recognize Eve as a villain until much later in the story. Zanuck reduced the screenplay by about 50 pages and chose the title "All About Eve" from the opening scenes in which Addison DeWitt says he will soon tell "more of Eve ... All about Eve, in fact."
Casting.
Among the actresses originally considered to play Margo Channing were Mankiewicz's original inspiration, Susan Hayward, who was rejected by Zanuck as "too young", Marlene Dietrich, dismissed as "too German", and Gertrude Lawrence, who was ruled out of contention when her lawyer insisted that Lawrence not have to drink or smoke in the film, and that the script would be rewritten to allow her to sing a torch song. Zanuck favored Barbara Stanwyck, but she was not available. Tallulah Bankhead and Ingrid Bergman were also considered, as was Joan Crawford, who was already working on the film "The Damned Don't Cry". 
Eventually, the role went to Claudette Colbert, but when Colbert severely injured her back and was forced to withdraw shortly before filming began, Bette Davis was chosen to replace her. Davis, who had recently ended an 18-year association with Warner Bros. after several poorly received films, immediately accepted the role after realizing it was one of the best she had ever read. Channing had originally been conceived as genteel and knowingly humorous, but with the casting of Davis, Mankiewicz revised the character to be more abrasive. Mankiewicz praised Davis for both her professionalism and the calibre of her performance, but in later years continued to discuss how Colbert would have played the role.
Anne Baxter had spent a decade in supporting roles and had won the 1946 Academy Award for Best Supporting Actress for "The Razor's Edge". She got the role of Eve Harrington after the first choice, Jeanne Crain, became pregnant. Crain was at the height of her popularity and had established a career playing likable heroines; Zanuck believed she lacked the "bitch virtuosity" required by the part, and audiences would not accept her as a deceitful character.
The role of Bill Sampson was originally intended for John Garfield or Ronald Reagan. Reagan's future wife Nancy Davis was considered for Karen Richards and Jose Ferrer for Addison DeWitt. Zsa Zsa Gabor actively sought the role of Phoebe without realizing the producers were considering her, along with Angela Lansbury, for Miss Casswell.
Mankiewicz greatly admired Thelma Ritter and wrote the character of Birdie Coonan for her after working with her on "A Letter to Three Wives" in 1949. As Coonan was the only one immediately suspicious of Eve Harrington, he was confident Ritter would contribute a shrewd characterisation casting doubt on Eve and providing a counterpoint to the more "theatrical" personalities of the other characters. Marilyn Monroe, relatively unknown at the time, was cast as Miss Casswell, referred to by DeWitt as a "graduate of the Copacabana School of Dramatic Art". Monroe got the part after a lobbying campaign by her agent, despite Zanuck's initial antipathy and belief she was better suited to comedy. Angela Lansbury had been originally considered for the role. The inexperienced Monroe was cowed by Bette Davis, and took 11 takes to complete the scene in the theatre lobby with the star; when Davis barked at her, Monroe left the set to vomit. Smaller roles were filled by Gregory Ratoff as the producer Max Fabian, Barbara Bates as Phoebe, a young fan of Eve Harrington, and Walter Hampden as the master of ceremonies at an award presentation.
Response.
Critical reaction.
"All About Eve" received overwhelmingly positive reviews from critics upon its release on October 13, 1950 at a New York City premiere. The film's competitor, "Sunset Boulevard," released the same year, drew similar praise, and the two were often favorably compared. Film critic Bosley Crowther of "The New York Times" loved the film, stating it was "a fine Darryl Zanuck production, excellent music and on air ultra-class complete the superior satire". 
Film critic Roger Ebert of the "Chicago Sun Times" praised the film, saying Bette Davis' character "veteran actress Margo Channing in "All About Eve" was her greatest role". A collection of reviews from the film's release are stored on the website Rottentomatoes.com, and "All About Eve" has garnered 100% positive reviews there, making it "Certified fresh". Boxoffice.com stated that it "is a classic of the American cinema – to this day the quintessential depiction of ruthless ambition in the entertainment industry, with legendary performances from Bette Davis, Anne Baxter and George Sanders anchoring one of the very best films from one of Hollywood's very best Golden Era filmmakers: Joseph L. Mankiewicz. It is a film that belongs on every collector's shelf – whether on video or DVD. It is a classic that deserves better than what Fox has given it."
Thematic content.
Critics and academics have delineated various themes in the film. Rebecca Flint Marx, in her "Allmovie" review, notes the antagonism that existed between Broadway and Hollywood at the time, stating that the "script summoned into existence a whole array of painfully recognizable theatre types, from the aging, egomaniacal grand dame to the outwardly docile, inwardly scheming ingenue to the powerful critic who reeks of malignant charm." Roger Ebert, in his review in "The Great Movies", says Eve Harrington is "a universal type", and focuses on the aging actress plot line, comparing the film to "Sunset Boulevard". Similarly, Marc Lee's 2006 review of the film for "The Daily Telegraph" describes a subtext "into the darker corners of show business, exposing its inherent ageism, especially when it comes to female stars." Kathleen Woodward's 1999 book, "Figuring Age: Women, Bodies, Generations (Theories of Contemporary Culture)", also discusses themes that appeared in many of the "aging actress" films of the 1950s and 1960s, including "All About Eve". She reasons that Margo has three options: "To continue to work, she can perform the role of a young woman, one she no longer seems that interested in. She can take up the position of the angry bitch, the drama queen who holds court (the deliberate camp that Sontag finds in this film). Or she can accept her culture's gendered discourse of aging which figures her as in her moment of fading. Margo ultimately chooses the latter option, accepting her position as one of loss."
Professor Robert J. Corber, who has studied homophobia within the cultural context of the Cold War in the United States, posits that the foundational theme in "All About Eve" is that the defense of the norms of heterosexuality, specifically in terms of patriarchal marriage, must be upheld in the face of challenges from female agency and homosexuality. The nurturing heterosexual relationships of Margo and Bill and of Karen and Lloyd serve to contrast with the loveless relationship predation and sterile careerism of the homosexual characters, Eve and Addison. Eve uses her physical femininity as a weapon to try to break up the marriages of both couples, and Addison's extreme cynicism serves as a model of Eve's future. Even film reviewer Kenneth Geist, despite being critical of the emphasis that Sam Staggs' book "All About All About Eve" places on the film's homosexual elements, nonetheless acknowledged that Eve's lesbianism seemed apparent; specifically, Geist states that "manifestations of Eve’s lesbianism are only twice briefly discernible". Geist asserted that Mankiewicz "was highly contemptuous of both male and female homosexuals", although Mankiewicz himself suggested otherwise in an interview in which he argued that society should "drop its vendetta against them".
Homosexuality was often linked to Communism during the Cold War's Lavender Scare and critics have written about film's subtle, yet central, Cold War narrative. The fair amount of subtlety employed in "All About Eve" is seen as primarily being due to Production Code restrictions on the depiction of homosexuals in the media during this time. However, notwithstanding those restrictions, Corber cites the film as but one example of a recurrent theme within American film of the homosexual as an emotionally bereft predator. The documentary "The Celluloid Closet" also affirms this theme to which Corber refers, including citing numerous other film examples from the same Production Code time period in which "All About Eve" was made.
Another important theme of the film, in terms of war politics and sexuality, involves the post-World War II pressure placed upon women to acquiesce agency. This pressure to resume "traditional" female roles is especially illustrated in this film in the contrast between Margo's mockery of Karen Richards for being a "happy little housewife" and her lengthy and inspired monologue, as a reformed woman later, about the virtuousness of marriage, including how a woman is not truly a woman without having a man beside her. This submissive and effeminate Margo is contrasted with the theatricality, combativeness, and egotism of the earlier career woman Margo, and the film's two homosexual characters. Margo quips that Eve should place her award "where her heart should be", and Eve is shown bereft at the end of the film. At dinner, the two married couples see Eve and Addison in a similarly negative light, with Margo wondering aloud what schemes Eve was constructing in her "feverish little brain". Additionally, Eve's utility as a personal assistant to Margo early in the film, which is a subtle construct of a same-sex intimate relationship, is decried by Birdie, the same working-class character who immediately detected the theatricality in Eve's story about her "husband". Birdie sees such agency as being unnatural, and the film contrasts its predatory nature ("studying you like a blueprint") with the love and warmth of her later reliance upon Bill. The pressure to acquiesce agency and more highly value patriarchy, following the return of men from the war, after having been shown propaganda promoting agency such as Rosie the Riveter and after having occupied traditionally male roles such as bomb-building factory worker, was deemed "the problem that has no name" by well-known feminist Betty Friedan.
Despite what critics such as Corber have described as the homophobia pervasive in the movie, "All About Eve" has long been a favored film among gay audiences, likely due to its campy overtones (in part due to the casting of Davis) and its general sophistication. Davis, who long had a strong gay fan base, expressed support for gay men in her 1972 interview with "The Advocate".
Awards and honors.
Later recognition and rankings.
In 1990, "All About Eve" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant." The film received in 1997 a placement on the Producers Guild of America Hall of Fame. The film also earns a 100% rating on Rotten Tomatoes. The film has been selected by the American Film Institute for many of their 100 Years lists.
When AFI named Bette Davis # 2 on its list of the greatest female American screen legends, "All About Eve" was the film selected to highlight Davis' legendary career.
Sarah Siddons Award.
The film opens with the image of a fictitious award trophy, described by DeWitt as the "highest honor our theater knows: the Sarah Siddons Award for Distinguished Achievement." The statuette is modelled after the famous painting of Siddons costumed as the tragic Muse by Joshua Reynolds, a copy of which hangs in the entrance of Margo's apartment and often visible during the party scene. In 1952, a small group of distinguished Chicago theater-goers began to give an award with that name, which was sculpted to look like the one used in the film. It has been given annually, with past honorees including Bette Davis and Celeste Holm.
Adaptations.
The first radio adaptation was broadcast on the Lux Radio Theatre on NBC on October 1, 1951 starring Bette Davis, Gary Merrill and Anne Baxter.
A second radio version of "All About Eve" starring Tallulah Bankhead as Margo Channing was presented on NBC's "The Big Show" by the Theatre Guild of the Air on November 16, 1952. The production is notable in that Mary Orr, the writer of the original short story that formed the basis for the original film, played the role of Karen Richards. The cast also featured Alan Hewitt as Addison DeWitt (who narrated), Beatrice Pearson as Eve Harrington, Don Briggs as Lloyd Richards, Kevin McCarthy as Bill Samson, Florence Robinson as Birdie Coonan, and Stefan Schnabel as Max Fabian.
In 1970, "All About Eve" was the inspiration for the stage musical "Applause," with book by Betty Comden and Adolph Green, lyrics by Lee Adams, and music by Charles Strouse. The original production starred Lauren Bacall as Margo Channing, and it won the Tony Award for Best Musical that season. It ran for four previews and 896 performances at the Palace Theatre on Broadway. After Bacall left the production, she was replaced by Anne Baxter in the role of Margo Channing.

</doc>
<doc id="41882" url="http://en.wikipedia.org/wiki?curid=41882" title="MIL-STD-188">
MIL-STD-188

MIL-STD-188 is a series of U.S. military standards relating to telecommunications.
Purpose.
Faced with "past technical deficiencies in telecommunications systems and equipment and software…that were traced to basic inadequacies in the application of telecommunication standards and to the lack of a well defined…program for their review, control and implementation", the U.S. Department of Defense looked to develop a series of standards that would alleviate the problem.
By 1988, the U.S. Department of Defense (DoD) issued Instruction 4630.8 (reissued in 1992, 2002, 2004) stating its policy that "all forces for joint and combined operations be supported through compatible, interoperable, and integrated Command, Control, Communications, and Intelligence systems. …[and that all such] systems developed for use by U.S. forces are considered to be for joint use." To achieve this the director of the Defense Information Systems Agency (DISA) is charged with "developing information technology standards to achieve interoperability and compatibility…[and ensure that all] systems and equipment shall conform to technical and procedural standards for interface, interoperability, and compatibility".
The MIL-STD-188 standards were created to "address telecommunication design parameters based on proven technologies." To ensure interoperability, DISA made these standards mandatory for use in all new DoD systems and equipment, or major upgrades.
The mandatory use of these standards will aid significantly in achieving standardization and result in improvements in availability, maintainability, reliability, and supportability. This, in turn, will enhance lifecycle configuration management and logistic support with subsequent reductions in life cycle costs.
Evolution.
When first developed Military Standard 188 (MIL-STD-188) covered technical standards for tactical and long-haul communications, but as it was revised (MIL-STD-188A, MIL-STD-188B) it became a document applicable to tactical communications only (MIL-STD-188C 24 Nov 1969). The Defense Information Systems Agency published circulars which announced both standards and engineering criteria relating to the long-haul Defense Communications System and to the technical support of the Worldwide Military Command and Control System. In line with a decision by the Joint Chiefs of Staff, these standards are published in the MIL-STD-188 series of documents. This series is subdivided into "a MIL-STD-188-100 series covering common standards for tactical and long-haul communications, a MIL-STD-188-200 series covering standards for tactical communications only, and a MIL-STD-188-300 series covering standards for long-haul communications only."
The MIL-STD-188 series standards are encompassed by the DoD’s Joint Technical Architecture.
Deviations and waivers.
For any manufacturer seeking to deviate from the MIL–STD-188 series standards (prior to the manufacture of an item) they must request to do so with the Joint Steering Committee (JSC) which is constituted under the Defense Communications Agency. For any DoD Agency to get a waiver to receive an item that deviates from the standards they also must apply to the JSC.
Relation to other systems of standards.
According to DoD documents, "The MIL-STD-188 series may be based on, or make reference to, Joint Technical Architecture, American National Standards Institute (ANSI) standards, International Telecommunications Union - Telecommunication Standardization Sector (ITU-T) recommendations, North Atlantic Treaty Organization (NATO) Standardization Agreements (STANAG), and other standards wherever applicable."
Current development emphasis.
Currently the DoD is placing its emphasis "on the development of common standards for tactical and long-haul communications (the MIL-STD-188-100 series)."
Documents.
Note: The following list of documents are those that are presently active. Documents with three digit numbers followed by a letter of the alphabet indicate that they are revisions of an older version of that document.
MIL-STD-188-100 series.
According to the DoD the MIL-STD-188-100 series contains "technical standards and design objectives which are common to both the long haul and tactical communications systems."
The current articles in this series include:
MIL-STD-188-200 series.
According to the DoD the MIL-STD-188-200 series "contains current tactical communications, technical standards and design objectives…[this series includes] appropriate unclassified design objectives and tactical communications systems technical standards…[and] Appropriate communications-electronics systems standards and design objectives developed under joint projects…[which are] integrated in the tactical communications standards."
The current articles in this segment include:
MIL-STD-188-300 series.
According to the DoD the MIL-STD-188–300 Series contains "communications system standards and design objectives applicable to the field of long haul and point-to–point communications in support of the Defense Communications System (DCS) and the National Military Command System (NMCS), and also to provide the necessary interface with non-DCS equipment."
The current articles in this series include:

</doc>
<doc id="41885" url="http://en.wikipedia.org/wiki?curid=41885" title="Weser">
Weser

The Weser (]) is a river in north-western Germany. Formed at Hannoversch Münden by the confluence of the rivers Fulda and Werra, it flows through Lower Saxony, then reaching the Hanseatic-town Bremen (see: Hanseatic League), before emptying 50 km further north at Bremerhaven into the North Sea. On the opposite (west) bank is the town of Nordenham at the foot of the Butjadingen Peninsula; thus, the mouth of the river is in Lower Saxony. The Weser has an overall length of 452 km. Together with its Werra tributary, which originates in Thuringia, its length is 744 km.
Etymology.
Linguistically, the name of both rivers, Weser and Werra, goes back to the same source, the differentiation being caused by the old linguistic border between Upper und Lower German, which touched the region of Hannoversch Münden.
The name "Weser" parallels the names of other rivers such as the "Wear" in England and the "Vistula" in Poland, all of which are ultimately derived from the root *"weis-" "to flow", which gave Old English/Old Frisian "wāse" "mud, ooze", Old Norse "veisa" "slime, stagnant pool", Dutch "waas" "lawn", Old Saxon "waso" "wet ground, mire", and Old High German "wasal" "rain".
Course.
The Weser river is the longest river to reach the sea, the course of which lies entirely within German national territory.
The upper part of its course leads through a hilly region called the Weserbergland. It extends from the confluence of the Fulda and the Werra to the Porta Westfalica, where it runs through a gorge between two mountain chains, the Wiehengebirge in the west and the Weserbergland in the east.
Between Minden and the North Sea humans have largely canalised the river, permitting ships of up to 1,200 tons to navigate it. Eight hydroelectric dams stand along its length. It is linked to the Dortmund-Ems Canal via the Coastal Canal, and another canal links it at Bremerhaven to the Elbe River. A large reservoir on the Eder river, the main tributary of the Fulda, is used to regulate water levels on the Weser so as to ensure adequate depth for shipping throughout the year. The dam, built in 1914, was bombed and destroyed by British aircraft in May 1943, causing massive destruction and approximately 70 deaths downstream, but was rebuilt within four months. s of 2013[ [update]] the Edersee reservoir, a major summer resort area, provides substantial hydroelectricity.
The Weser enters the North Sea in the southernmost part of the German Bight. In the North Sea it splits up into two arms representing the ancient riverbed at the end of the last ice age. These sea-arms are called "Alte Weser" (old Weser) and "Neue Weser" (new Weser). They represent the major waterways for ships heading for the harbors of Bremerhaven, Nordenham and Bremen. The "Alte Weser" lighthouse marks the northernmost point of the Weser. This lighthouse replaced the historic and famous "Roter Sand" lighthouse in 1964.
Tributaries.
The largest tributary of the Weser is the Aller, which joins south of Bremen. The tributaries of the Weser and the Werra (from source to mouth) are:
Notable towns.
Towns along the Weser, from the confluence of Werra and Fulda to the mouth, include: Hann. Münden, Beverungen, Höxter, Holzminden, Bodenwerder, Hameln, Hessisch Oldendorf, Rinteln, Vlotho, Bad Oeynhausen, Porta Westfalica, Minden, Petershagen, Nienburg, Achim, Bremen, Brake, Nordenham, Bremerhaven.

</doc>
<doc id="41887" url="http://en.wikipedia.org/wiki?curid=41887" title="Felix Klein">
Felix Klein

Christian Felix Klein (25 April 1849 – 22 June 1925) was a German mathematician and mathematics educator, known for his work in group theory, complex analysis, non-Euclidean geometry, and on the connections between geometry and group theory. His 1872 Erlangen Program, classifying geometries by their underlying symmetry groups, was a hugely influential synthesis of much of the mathematics of the day.
Life.
Felix Klein was born on 25 April 1849 in Düsseldorf, to Prussian parents; his father, Caspar Klein (1809–1889), was a Prussian government official's secretary stationed in the Rhine Province. Klein's mother was Sophie Elise Klein (1819–1890, née Kayser). He attended the Gymnasium in Düsseldorf, then studied mathematics and physics at the University of Bonn, 1865–1866, intending to become a physicist. At that time, Julius Plücker held Bonn's chair of mathematics and experimental physics, but by the time Klein became his assistant, in 1866, Plücker's interest was geometry. Klein received his doctorate, supervised by Plücker, from the University of Bonn in 1868.
Plücker died in 1868, leaving his book on the foundations of line geometry incomplete. Klein was the obvious person to complete the second part of Plücker's "Neue Geometrie des Raumes", and thus became acquainted with Alfred Clebsch, who had moved to Göttingen in 1868. Klein visited Clebsch the following year, along with visits to Berlin and Paris. In July 1870, at the outbreak of the Franco-Prussian War, he was in Paris and had to leave the country. For a short time, he served as a medical orderly in the Prussian army before being appointed lecturer at Göttingen in early 1871.
Erlangen appointed Klein professor in 1872, when he was only 23. In this, he was strongly supported by Clebsch, who regarded him as likely to become the leading mathematician of his day. Klein did not build a school at Erlangen where there were few students, and so he was pleased to be offered a chair at Munich's Technische Hochschule in 1875. There he and Alexander von Brill taught advanced courses to many excellent students, including, Adolf Hurwitz, Walther von Dyck, Karl Rohn, Carl Runge, Max Planck, Luigi Bianchi, and Gregorio Ricci-Curbastro.
In 1875 Klein married Anne Hegel, the granddaughter of the philosopher Georg Wilhelm Friedrich Hegel.
After five years at the Technische Hochschule, Klein was appointed to a chair of geometry at Leipzig. There his colleagues included Walther von Dyck, Rohn, Eduard Study and Friedrich Engel. Klein's years at Leipzig, 1880 to 1886, fundamentally changed his life. In 1882, his health collapsed; in 1883–1884, he was plagued by depression. Nonetheless his research continued; his seminal work on hyperelliptic sigma functions dates from around this period, being published in 1886 and 1888.
Klein accepted a chair at the University of Göttingen in 1886. From then until his 1913 retirement, he sought to re-establish Göttingen as the world's leading mathematics research center. Yet he never managed to transfer from Leipzig to Göttingen his own role as the leader of a school of geometry. At Göttingen, he taught a variety of courses, mainly on the interface between mathematics and physics, such as mechanics and potential theory.
The research center Klein established at Göttingen served as a model for the best such centers throughout the world. He introduced weekly discussion meetings, and created a mathematical reading room and library. In 1895, Klein hired David Hilbert away from Königsberg; this appointment proved fateful, because Hilbert continued Göttingen's glory until his own retirement in 1932.
Under Klein's editorship, "Mathematische Annalen" became one of the very best mathematics journals in the world. Founded by Clebsch, only under Klein's management did it first rival then surpass "Crelle's Journal" based out of the University of Berlin. Klein set up a small team of editors who met regularly, making democratic decisions. The journal specialized in complex analysis, algebraic geometry, and invariant theory (at least until Hilbert killed the subject). It also provided an important outlet for real analysis and the new group theory.
Thanks in part to Klein's efforts, Göttingen began admitting women in 1893. He supervised the first Ph.D. thesis in mathematics written at Göttingen by a woman; she was Grace Chisholm Young, an English student of Arthur Cayley's, whom Klein admired.
Around 1900, Klein began to take an interest in mathematical instruction in schools. In 1905, he played a decisive role in formulating a plan recommending that analytic geometry, the rudiments of differential and integral calculus, and the function concept be taught in secondary schools. This recommendation was gradually implemented in many countries around the world. In 1908, Klein was elected president of the International Commission on Mathematical Instruction at the Rome International Congress of Mathematicians. Under his guidance, the German branch of the Commission published many volumes on the teaching of mathematics at all levels in Germany.
The London Mathematical Society awarded Klein its De Morgan Medal in 1893. He was elected a member of the Royal Society in 1885, and was awarded its Copley Medal in 1912. He retired the following year due to ill health, but continued to teach mathematics at his home for some years more.
Klein bore the title of Geheimrat.
He died in Göttingen in 1925.
Work.
Klein's dissertation, on line geometry and its applications to mechanics, classified second degree line complexes using Weierstrass's theory of elementary divisors.
Klein's first important mathematical discoveries were made in 1870. In collaboration with Sophus Lie, he discovered the fundamental properties of the asymptotic lines on the Kummer surface. They went on to investigate W-curves, curves invariant under a group of projective transformations. It was Lie who introduced Klein to the concept of group, which was to play a major role in his later work. Klein also learned about groups from Camille Jordan.
Klein devised the bottle named after him, a one-sided closed surface which cannot be embedded in three-dimensional Euclidean space, but it may be immersed as a cylinder looped back through itself to join with its other end from the "inside". It may be embedded in Euclidean space of dimensions 4 and higher.
In the 1890s, Klein turned to mathematical physics, a subject from which he had never strayed far, writing on the gyroscope with Arnold Sommerfeld. In 1894 he launched the idea of an encyclopedia of mathematics including its applications, which became the Encyklopädie der mathematischen Wissenschaften. This enterprise, which ran until 1935, provided an important standard reference of enduring value.
Erlangen Program.
In 1871, while at Göttingen, Klein made major discoveries in geometry. He published two papers "On the So-called Non-Euclidean Geometry" showing that Euclidean and non-Euclidean geometries could be considered special cases of a projective surface with a specific conic section adjoined. This had the remarkable corollary that non-Euclidean geometry was consistent if and only if Euclidean geometry was, putting Euclidean and non-Euclidean geometries on the same footing, and ending all controversy surrounding non-Euclidean geometry. Cayley never accepted Klein's argument, believing it to be circular.
Klein's synthesis of geometry as the study of the properties of a space that is invariant under a given group of transformations, known as the "Erlangen Program" (1872), profoundly influenced the evolution of mathematics. This program was set out in Klein's inaugural lecture as professor at Erlangen, although it was not the actual speech he gave on the occasion. The "Program" proposed a unified approach to geometry that has become the accepted modern view. Klein showed how the essential properties of a given geometry could be represented by the group of transformations that preserve those properties. Thus the "Program"'s definition of geometry encompassed both Euclidean and non-Euclidean geometry.
Today the significance of Klein's contributions to geometry is more than evident, but not because those contributions are now seen as strange or wrong. On the contrary, those contributions have become so much a part of our present mathematical thinking that it is hard for us to appreciate their novelty, and the way in which they were not immediately accepted by all his contemporaries.
Complex analysis.
Klein saw his work on complex analysis as his major contribution to mathematics, specifically his work on:
Klein showed that the modular group moves the fundamental region of the complex plane so as to tessellate that plane. In 1879, he looked at the action of PSL(2,7), thought of as an image of the modular group, and obtained an explicit representation of a Riemann surface today called the Klein quartic. He showed that that surface was a curve in projective space, that its equation was "x"3"y" + "y"3"z" + "z"3"x" = 0, and that its group of symmetries was PSL(2,7) of order 168. His "Ueber Riemann's Theorie der algebraischen Funktionen und ihre Integrale" (1882) treats complex analysis in a geometric way, connecting potential theory and conformal mappings. This work drew on notions from fluid dynamics.
Klein considered equations of degree > 4, and was especially interested in using transcendental methods to solve the general equation of the fifth degree. Building on the methods of Hermite and Kronecker, he produced similar results to those of Brioschi and went on to completely solve the problem by means of the icosahedral group. This work led him to write a series of papers on elliptic modular functions.
In his 1884 book on the icosahedron, Klein set out a theory of automorphic functions, connecting algebra and geometry. However Poincaré published an outline of his theory of automorphic functions in 1881, which led to a friendly rivalry between the two men. Both sought to state and prove a grand uniformization theorem that would serve as a capstone to the emerging theory. Klein succeeded in formulating such a theorem and in sketching a strategy for proving it. But while doing this work his health collapsed, as mentioned above.
Klein summarized his work on automorphic and elliptic modular functions in a four volume treatise, written with Robert Fricke over a period of about 20 years.
Bibliography.
Primary:
Secondary

</doc>
<doc id="41890" url="http://en.wikipedia.org/wiki?curid=41890" title="Group theory">
Group theory

In mathematics and abstract algebra, group theory studies the algebraic structures known as groups. 
The concept of a group is central to abstract algebra: other well-known algebraic structures, such as rings, fields, and vector spaces, can all be seen as groups endowed with additional operations and axioms. Groups recur throughout mathematics, and the methods of group theory have influenced many parts of algebra. Linear algebraic groups and Lie groups are two branches of group theory that have experienced advances and have become subject areas in their own right.
Various physical systems, such as crystals and the hydrogen atom, can be modelled by symmetry groups. Thus group theory and the closely related representation theory have many important applications in physics, chemistry, and materials science. Group theory is also central to public key cryptography.
One of the most important mathematical achievements of the 20th century was the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 1980, that culminated in a complete classification of finite simple groups.
History.
Group theory has three main historical sources: number theory, the theory of algebraic equations, and geometry. The number-theoretic strand was begun by Leonhard Euler, and developed by Gauss's work on modular arithmetic and additive and multiplicative groups related to quadratic fields. Early results about permutation groups were obtained by Lagrange, Ruffini, and Abel in their quest for general solutions of polynomial equations of high degree. Évariste Galois coined the term "group" and established a connection, now known as Galois theory, between the nascent theory of groups and field theory. In geometry, groups first became important in projective geometry and, later, non-Euclidean geometry. Felix Klein's Erlangen program proclaimed group theory to be the organizing principle of geometry.
Galois, in the 1830s, was the first to employ groups to determine the solvability of polynomial equations. Arthur Cayley and Augustin Louis Cauchy pushed these investigations further by creating the theory of permutation groups. The second historical source for groups stems from geometrical situations. In an attempt to come to grips with possible geometries (such as euclidean, hyperbolic or projective geometry) using group theory, Felix Klein initiated the Erlangen programme. Sophus Lie, in 1884, started using groups (now called Lie groups) attached to analytic problems. Thirdly, groups were, at first implicitly and later explicitly, used in algebraic number theory.
The different scope of these early sources resulted in different notions of groups. The theory of groups was unified starting around 1880. Since then, the impact of group theory has been ever growing, giving rise to the birth of abstract algebra in the early 20th century, representation theory, and many more influential spin-off domains. The classification of finite simple groups is a vast body of work from the mid 20th century, classifying all the finite simple groups.
Main classes of groups.
The range of groups being considered has gradually expanded from finite permutation groups and special examples of matrix groups to abstract groups that may be specified through a presentation by generators and relations.
Permutation groups.
The first class of groups to undergo a systematic study was permutation groups. Given any set "X" and a collection "G" of bijections of "X" into itself (known as "permutations") that is closed under compositions and inverses, "G" is a group acting on "X". If "X" consists of "n" elements and "G" consists of "all" permutations, "G" is the symmetric group S"n"; in general, any permutation group "G" is a subgroup of the symmetric group of "X". An early construction due to Cayley exhibited any group as a permutation group, acting on itself ("X" = "G") by means of the left regular representation.
In many cases, the structure of a permutation group can be studied using the properties of its action on the corresponding set. For example, in this way one proves that for "n" ≥ 5, the alternating group A"n" is simple, i.e. does not admit any proper normal subgroups. This fact plays a key role in the impossibility of solving a general algebraic equation of degree "n' ≥ 5 in radicals.
Matrix groups.
The next important class of groups is given by "matrix groups", or linear groups. Here "G" is a set consisting of invertible matrices of given order "n" over a field "K" that is closed under the products and inverses. Such a group acts on the "n"-dimensional vector space "K""n" by linear transformations. This action makes matrix groups conceptually similar to permutation groups, and the geometry of the action may be usefully exploited to establish properties of the group "G".
Transformation groups.
Permutation groups and matrix groups are special cases of transformation groups: groups that act on a certain space "X" preserving its inherent structure. In the case of permutation groups, "X" is a set; for matrix groups, "X" is a vector space. The concept of a transformation group is closely related with the concept of a symmetry group: transformation groups frequently consist of "all" transformations that preserve a certain structure.
The theory of transformation groups forms a bridge connecting group theory with differential geometry. A long line of research, originating with Lie and Klein, considers group actions on manifolds by homeomorphisms or diffeomorphisms. The groups themselves may be discrete or continuous.
Abstract groups.
Most groups considered in the first stage of the development of group theory were "concrete", having been realized through numbers, permutations, or matrices. It was not until the late nineteenth century that the idea of an abstract group as a set with operations satisfying a certain system of axioms began to take hold. A typical way of specifying an abstract group is through a presentation by "generators and relations",
A significant source of abstract groups is given by the construction of a "factor group", or quotient group, "G"/"H", of a group "G" by a normal subgroup "H". Class groups of algebraic number fields were among the earliest examples of factor groups, of much interest in number theory. If a group "G" is a permutation group on a set "X", the factor group "G"/"H" is no longer acting on "X"; but the idea of an abstract group permits one not to worry about this discrepancy.
The change of perspective from concrete to abstract groups makes it natural to consider properties of groups that are independent of a particular realization, or in modern language, invariant under isomorphism, as well as the classes of group with a given such property: finite groups, periodic groups, simple groups, solvable groups, and so on. Rather than exploring properties of an individual group, one seeks to establish results that apply to a whole class of groups. The new paradigm was of paramount importance for the development of mathematics: it foreshadowed the creation of abstract algebra in the works of Hilbert, Emil Artin, Emmy Noether, and mathematicians of their school.
Topological and algebraic groups.
An important elaboration of the concept of a group occurs if "G" is endowed with additional structure, notably, of a topological space, differentiable manifold, or algebraic variety. If the group operations "m" (multiplication) and "i" (inversion),
are compatible with this structure, i.e. are continuous, smooth or regular (in the sense of algebraic geometry) maps, then "G" becomes a topological group, a Lie group, or an algebraic group.
The presence of extra structure relates these types of groups with other mathematical disciplines and means that more tools are available in their study. Topological groups form a natural domain for abstract harmonic analysis, whereas Lie groups (frequently realized as transformation groups) are the mainstays of differential geometry and unitary representation theory. Certain classification questions that cannot be solved in general can be approached and resolved for special subclasses of groups. Thus, compact connected Lie groups have been completely classified. There is a fruitful relation between infinite abstract groups and topological groups: whenever a group "Γ" can be realized as a lattice in a topological group "G", the geometry and analysis pertaining to "G" yield important results about "Γ". A comparatively recent trend in the theory of finite groups exploits their connections with compact topological groups (profinite groups): for example, a single "p"-adic analytic group "G" has a family of quotients which are finite "p"-groups of various orders, and properties of "G" translate into the properties of its finite quotients.
Branches of group theory.
Finite group theory.
During the twentieth century, mathematicians investigated some aspects of the theory of finite groups in great depth, especially the local theory of finite groups and the theory of solvable and nilpotent groups. As a consequence, the complete classification of finite simple groups was achieved, meaning that all those simple groups from which all finite groups can be built are now known.
During the second half of the twentieth century, mathematicians such as Chevalley and Steinberg also increased our understanding of finite analogs of classical groups, and other related groups. One such family of groups is the family of general linear groups over finite fields. 
Finite groups often occur when considering symmetry of mathematical or
physical objects, when those objects admit just a finite number of structure-preserving transformations. The theory of Lie groups,
which may be viewed as dealing with "continuous symmetry", is strongly influenced by the associated Weyl groups. These are finite groups generated by reflections which act on a finite-dimensional Euclidean space. The properties of finite groups can thus play a role in subjects such as theoretical physics and chemistry.
Representation of groups.
Saying that a group "G" "acts" on a set "X" means that every element of "G" defines a bijective map on the set "X" in a way compatible with the group structure. When "X" has more structure, it is useful to restrict this notion further: a representation of "G" on a vector space "V" is a group homomorphism:
where GL("V") consists of the invertible linear transformations of "V". In other words, to every group element "g" is assigned an automorphism "ρ"("g") such that "ρ"("g") ∘ "ρ"("h") = "ρ"("gh") for any "h" in "G".
This definition can be understood in two directions, both of which give rise to whole new domains of mathematics. On the one hand, it may yield new information about the group "G": often, the group operation in "G" is abstractly given, but via "ρ", it corresponds to the multiplication of matrices, which is very explicit. On the other hand, given a well-understood group acting on a complicated object, this simplifies the study of the object in question. For example, if "G" is finite, it is known that "V" above decomposes into irreducible parts. These parts in turn are much more easily manageable than the whole "V" (via Schur's lemma).
Given a group "G", representation theory then asks what representations of "G" exist. There are several settings, and the employed methods and obtained results are rather different in every case: representation theory of finite groups and representations of Lie groups are two main subdomains of the theory. The totality of representations is governed by the group's characters. For example, Fourier polynomials can be interpreted as the characters of U(1), the group of complex numbers of absolute value "1", acting on the "L"2-space of periodic functions.
Lie theory.
A Lie group is a group that is also a differentiable manifold, with the property that the group operations are compatible with the smooth structure. Lie groups are named after Sophus Lie, who laid the foundations of the theory of continuous transformation groups. The term "groupes de Lie" first appeared in French in 1893 in the thesis of Lie’s student Arthur Tresse, page 3.
Lie groups represent the best-developed theory of continuous symmetry of mathematical objects and structures, which makes them indispensable tools for many parts of contemporary mathematics, as well as for modern theoretical physics. They provide a natural framework for analysing the continuous symmetries of differential equations (differential Galois theory), in much the same way as permutation groups are used in Galois theory for analysing the discrete symmetries of algebraic equations. An extension of Galois theory to the case of continuous symmetry groups was one of Lie's principal motivations.
Combinatorial and geometric group theory.
Groups can be described in different ways. Finite groups can be described by writing down the group table consisting of all possible multiplications "g" • "h". A more compact way of defining a group is by "generators and relations", also called the "presentation" of a group. Given any set "F" of generators {"g""i"}"i" ∈ "I", the free group generated by "F" subjects onto the group "G". The kernel of this map is called subgroup of relations, generated by some subset "D". The presentation is usually denoted by 〈"F" | "D" 〉. For example, the group Z = 〈"a" | 〉 can be generated by one element "a" (equal to +1 or −1) and no relations, because "n" · 1 never equals 0 unless "n" is zero. A string consisting of generator symbols and their inverses is called a "word".
Combinatorial group theory studies groups from the perspective of generators and relations. It is particularly useful where finiteness assumptions are satisfied, for example finitely generated groups, or finitely presented groups (i.e. in addition the relations are finite). The area makes use of the connection of graphs via their fundamental groups. For example, one can show that every subgroup of a free group is free.
There are several natural questions arising from giving a group by its presentation. The "word problem" asks whether two words are effectively the same group element. By relating the problem to Turing machines, one can show that there is in general no algorithm solving this task. Another, generally harder, algorithmically insoluble problem is the group isomorphism problem, which asks whether two groups given by different presentations are actually isomorphic. For example the additive group Z of integers can also be presented by
it may not be obvious that these groups are isomorphic.
Geometric group theory attacks these problems from a geometric viewpoint, either by viewing groups as geometric objects, or by finding suitable geometric objects a group acts on. The first idea is made precise by means of the Cayley graph, whose vertices correspond to group elements and edges correspond to right multiplication in the group. Given two elements, one constructs the word metric given by the length of the minimal path between the elements. A theorem of Milnor and Svarc then says that given a group "G" acting in a reasonable manner on a metric space "X", for example a compact manifold, then "G" is quasi-isometric (i.e. looks similar from the far) to the space "X".
Connection of groups and symmetry.
Given a structured object "X" of any sort, a symmetry is a mapping of the object onto itself which preserves the structure. This occurs in many cases, for example
The axioms of a group formalize the essential aspects of symmetry. Symmetries form a group: they are closed because if you take a symmetry of an object, and then apply another symmetry, the result will still be a symmetry. The identity keeping the object fixed is always a symmetry of an object. Existence of inverses is guaranteed by undoing the symmetry and the associativity comes from the fact that symmetries are functions on a space, and composition of functions are associative.
Frucht's theorem says that every group is the symmetry group of some graph. So every abstract group is actually the symmetries of some explicit object.
The saying of "preserving the structure" of an object can be made precise by working in a category. Maps preserving the structure are then the morphisms, and the symmetry group is the automorphism group of the object in question.
Applications of group theory.
Applications of group theory abound. Almost all structures in abstract algebra are special cases of groups. Rings, for example, can be viewed as abelian groups (corresponding to addition) together with a second operation (corresponding to multiplication). Therefore group theoretic arguments underlie large parts of the theory of those entities.
Galois theory.
Galois theory uses groups to describe the symmetries of the roots of a polynomial (or more precisely the automorphisms of the algebras generated by these roots). The fundamental theorem of Galois theory provides a link between algebraic field extensions and group theory. It gives an effective criterion for the solvability of polynomial equations in terms of the solvability of the corresponding Galois group. For example, "S"5, the symmetric group in 5 elements, is not solvable which implies that the general quintic equation cannot be solved by radicals in the way equations of lower degree can. The theory, being one of the historical roots of group theory, is still fruitfully applied to yield new results in areas such as class field theory.
Algebraic topology.
Algebraic topology is another domain which prominently associates groups to the objects the theory is interested in. There, groups are used to describe certain invariants of topological spaces. They are called "invariants" because they are defined in such a way that they do not change if the space is subjected to some deformation. For example, the fundamental group "counts" how many paths in the space are essentially different. The Poincaré conjecture, proved in 2002/2003 by Grigori Perelman, is a prominent application of this idea. The influence is not unidirectional, though. For example, algebraic topology makes use of Eilenberg–MacLane spaces which are spaces with prescribed homotopy groups. Similarly algebraic K-theory relies in a way on classifying spaces of groups. Finally, the name of the torsion subgroup of an infinite group shows the legacy of topology in group theory.
Algebraic geometry and cryptography.
Algebraic geometry and cryptography likewise uses group theory in many ways. Abelian varieties have been introduced above. The presence of the group operation yields additional information which makes these varieties particularly accessible. They also often serve as a test for new conjectures. The one-dimensional case, namely elliptic curves is studied in particular detail. They are both theoretically and practically intriguing. Very large groups of prime order constructed in Elliptic-Curve Cryptography serve for public key cryptography. Cryptographical methods of this kind benefit from the flexibility of the geometric objects, hence their group structures, together with the complicated structure of these groups, which make the discrete logarithm very hard to calculate. One of the earliest encryption protocols, Caesar's cipher, may also be interpreted as a (very easy) group operation. In another direction, toric varieties are algebraic varieties acted on by a torus. Toroidal embeddings have recently led to advances in algebraic geometry, in particular resolution of singularities.
Algebraic number theory.
Algebraic number theory is a special case of group theory, thereby following the rules of the latter. For example, Euler's product formula
captures the fact that any integer decomposes in a unique way into primes. The failure of this statement for more general rings gives rise to class groups and regular primes, which feature in Kummer's treatment of Fermat's Last Theorem.
Harmonic analysis.
Analysis on Lie groups and certain other groups is called harmonic analysis. Haar measures, that is, integrals invariant under the translation in a Lie group, are used for pattern recognition and other image processing techniques.
Combinatorics.
In combinatorics, the notion of permutation group and the concept of group action are often used to simplify the counting of a set of objects; see in particular Burnside's lemma.
Music.
The presence of the 12-periodicity in the circle of fifths yields applications of elementary group theory in musical set theory.
Physics.
In physics, groups are important because they describe the symmetries which the laws of physics seem to obey. According to Noether's theorem, every continuous symmetry of a physical system corresponds to a conservation law of the system. Physicists are very interested in group representations, especially of Lie groups, since these representations often point the way to the "possible" physical theories. Examples of the use of groups in physics include the Standard Model, gauge theory, the Lorentz group, and the Poincaré group.
Chemistry and materials science.
In chemistry and materials science, groups are used to classify crystal structures, regular polyhedra, and the symmetries of molecules. The assigned point groups can then be used to determine physical properties (such as polarity and chirality), spectroscopic properties (particularly useful for Raman spectroscopy and infrared spectroscopy), and to construct molecular orbitals.
Molecular symmetry is responsible for many physical and spectroscopic properties of compounds and provides relevant information about how chemical reactions occur. In order to assign a point group for any given molecule, it is necessary to find the set of symmetry operations present on it. The symmetry operation is an action, such as a rotation around an axis or a reflection through a mirror plane. In other words, it is an operation that moves the molecule such that it is indistinguishable from the original configuration. In group theory, the rotation axes and mirror planes are called "symmetry elements". These elements can be a point, line or plane with respect to which the symmetry operation is carried out. The symmetry operations of a molecule determine the specific point group for this molecule.
In chemistry, there are five important symmetry operations. The identity operation (E) consists of leaving the molecule as it is. This is equivalent to any number of full rotations around any axis. This is a symmetry of all molecules, whereas the symmetry group of a chiral molecule consists of only the identity operation. Rotation around an axis (C"n") consists of rotating the molecule around a specific axis by a specific angle. For example, if a water molecule rotates 180° around the axis that passes through the oxygen atom and between the hydrogen atoms, it is in the same configuration as it started. In this case, "n" = 2, since applying it twice produces the identity operation. Other symmetry operations are: reflection, inversion and improper rotation (rotation followed by reflection).

</doc>
<doc id="41891" url="http://en.wikipedia.org/wiki?curid=41891" title="Stable nuclide">
Stable nuclide

Stable nuclides are nuclides that are not radioactive and so (unlike radionuclides) do not spontaneously undergo radioactive decay. When such nuclides are referred to in relation to specific elements, they are usually termed stable isotopes.
The 80 elements with one or more stable isotopes comprise a total of 254 nuclides that have not been known to decay using current equipment (see list at the end of this article). Of these elements, 26 have only one stable isotope; they are thus termed monoisotopic. The rest have more than one stable isotope. Tin has ten stable isotopes, the largest number known for an element.
Definition of stability, and naturally occurring nuclides.
Most naturally occurring nuclides are stable (about 254; see list at the end of this article); and about 34 more (total of 288) are known radioactives with sufficiently long half-lives (also known) to occur primordially. If the half-life of a nuclide is comparable to, or greater than, the Earth's age (4.5 billion years), a significant amount will have survived since the formation of the Solar System, and then is said to be primordial. It will then contribute in that way to the natural isotopic composition of a chemical element. Primordially present radioisotopes are easily detected with half-lives as short as 700 million years (e.g., 235U), although some primordial isotopes have been detected with half-lives as short as 80 million years (e.g., 244Pu). However, this is the present limit of detection, as the nuclide with the next-shortest half-life (niobium-92 with half-life 34.7 million years) has not yet been detected in nature.
Many naturally-occurring radioisotopes (another 51 or so, for a total of about 339) exhibit still shorter half-lives than 80 million years, but they are made freshly, as daughter products of decay processes of primordial nuclides (for example, radium from uranium) or from ongoing energetic reactions, such as cosmogenic nuclides produced by present bombardment of Earth by cosmic rays (for example, carbon-14 made from nitrogen).
Some isotopes that are classed as stable (i.e. no radioactivity has been observed for them) are predicted to have extremely long half-lives (sometimes as high as 1018 years or more). If the predicted half-life falls into an experimentally accessible range, such isotopes have a chance to move from the list of stable nuclides to the radioactive category, once their activity is observed. E.g. bismuth-209 and tungsten-180 were formerly classed as stable, but have been recently (2003) found to be alpha-active. However, such nuclides do not change their status as primordial when they are found to be radioactive.
Most stable isotopes in the earth are believed to have been formed in processes of nucleosynthesis, either in the Big Bang, or in generations of stars that preceded the formation of the solar system. However, some stable isotopes also show abundance variations in the earth as a result of decay from long-lived radioactive nuclides. These decay-products are termed radiogenic isotopes, in order to distinguish them from the much larger group of 'non-radiogenic' isotopes.
The so-called island of stability may reveal a number of long-lived or even stable atoms that are heavier (and with more protons) than lead.
Isotopes per element.
Of the known chemical elements, 80 elements have at least one stable nuclide. These comprise the first 82 elements from hydrogen to lead, with the two exceptions, technetium (element 43) and promethium (element 61), that do not have any stable nuclides. As of December 2011, there were a total of 254 known "stable" nuclides. In this definition, "stable" means a nuclide that has never been observed to decay against the natural background. Thus, these elements have half lives too long to be measured by any means, direct or indirect.
Stable isotopes:
These last 26 are thus called "monoisotopic elements". The mean number of stable isotopes for elements which have at least one stable isotope is 254/80 = 3.2.
"Magic numbers" and odd and even proton and neutron count.
Stability of isotopes is affected by the ratio of protons to neutrons, and also by presence of certain "magic numbers" of neutrons or protons which represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. As in the case of tin, a magic number for Z, the atomic number, tends to increase the number of stable isotopes for the element.
Just as in the case of electrons, which have the lowest energy state when they occur in pairs in a given orbital, nucleons (both protons and neutrons) exhibit a lower energy state when their number is even, rather than odd. This stability tends to prevent beta decay (in two steps) of many even-even nuclides into another even-even nuclide of the same mass number but lower energy (and of course with two more protons and two fewer neutrons), because decay proceeding one step at a time would have to pass through an odd-odd nuclide of higher energy. This makes for a larger number of stable even-even nuclides, up to three for some mass numbers, and up to seven for some atomic (proton) numbers. Conversely, of the 254 known stable nuclides, only five have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10, nitrogen-14, and tantalum-180m. Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138, and lutetium-176. Odd-odd primordial nuclides are rare because most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
Yet another effect of the instability of an odd number of either type of nucleons, is that odd-numbered elements tend to have fewer stable isotopes. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number — the single exception to both rules being beryllium. All of these elements also have an even number of neutrons, with the single exception again being beryllium.
Nuclear isomers, including a "stable" one.
The count of 254 known stable nuclides includes tantalum-180m, since even though its decay and instability is automatically implied by its notation of "metastable", still this has not yet been observed. All "stable" isotopes (stable by observation, not theory) are the ground states of nuclei, with the exception of tantalum-180m, which is a nuclear isomer or excited state (the ground state of this nucleus is radioactive with a very short half-life of 8 hours); but the decay of the excited nuclear isomer is extremely strongly forbidden by spin-parity selection rules. It has been reported experimentally by direct observation that the half-life of 180mTa to gamma decay must be more than 1015 years. Other possible modes of 180mTa decay (beta decay, electron capture and alpha decay) have also never been observed.
Still-unobserved decay.
It is expected that some continual improvement of experimental sensitivity will allow discovery of very mild radioactivity (instability) of some isotopes that are considered to be stable today. For an example of a recent discovery, it was not until 2003 that bismuth-209 (the only naturally-occurring isotope of bismuth) was shown to be very mildly radioactive. Prior to this discovery, there were theoretical predictions from nuclear physics that bismuth-209 would decay very slowly by alpha emission. These calculations were confirmed by the experimental observations in 2003.
Summary table for numbers of each class of nuclides.
This is a summary table from List of nuclides. Note that numbers are not exact, and may change slightly in the future, as nuclides are observed to be radioactive, or new half-lives are determined to some precision. 
List of stable nuclides.
Abbreviations for predicted unobserved decay:
A for alpha decay, B for beta decay, 2B for double beta decay, E for electron capture, 2E for double electron capture, IT for isomeric transition.

</doc>
<doc id="41892" url="http://en.wikipedia.org/wiki?curid=41892" title="Terminal">
Terminal

Terminal may refer to:

</doc>
<doc id="41893" url="http://en.wikipedia.org/wiki?curid=41893" title="Teletype (disambiguation)">
Teletype (disambiguation)

Teletype may refer to:

</doc>
<doc id="41895" url="http://en.wikipedia.org/wiki?curid=41895" title="Craig Barrett">
Craig Barrett

Craig Barrett may refer to:

</doc>
<doc id="41896" url="http://en.wikipedia.org/wiki?curid=41896" title="Helmut Kohl">
Helmut Kohl

Helmut Josef Michael Kohl (]; born 3 April 1930) is a German statesman, who served as Chancellor of Germany from 1982 to 1998 (of West Germany 1982–90 and of the reunited Germany 1990–98) and as the chairman of the Christian Democratic Union (CDU) from 1973 to 1998.
His 16-year tenure was the longest of any German chancellor since Otto von Bismarck, and far and away the longest of any democratically-elected chancellor. He oversaw the end of the Cold War, and is widely regarded as the main architect of the German reunification. Together with French president François Mitterrand, he is also considered the architect of the Maastricht Treaty, which established the European Union.
Kohl and Mitterrand were the joint recipients of the Charlemagne Prize in 1988. In 1996, he won the prestigious Prince of Asturias Award in International Cooperation. In 1998, Kohl was named Honorary Citizen of Europe by the European heads of state or government for his extraordinary work for European integration and cooperation, an honour previously only bestowed on Jean Monnet.
Kohl was described as "the greatest European leader of the second half of the 20th century" by U.S. Presidents George H. W. Bush and Bill Clinton.
Life.
Youth.
Helmut Kohl was born on 3 April 1930 in Ludwigshafen am Rhein (at the time part of Bavaria, now in Rhineland-Palatinate), Germany, the third child of Hans Kohl (1887–1975), a civil servant, and his wife, Cäcilie (née Schnur; 1890–1979). His family was conservative and Roman Catholic, and remained loyal to the Catholic Centre Party before and after 1933. His older brother died in the Second World War as a teenage soldier. In the last weeks of the war, Kohl was also drafted, but he was not involved in any combat.
Kohl attended the Ruprecht elementary school, and continued at the Max-Planck-Gymnasium. In 1946, he joined the recently founded CDU. In 1947, he was one of the co-founders of the Junge Union-branch in Ludwigshafen. After graduating in 1950, he began to study law in Frankfurt am Main. In 1951, he switched to the University of Heidelberg where he majored in History and Political Science. In 1953, he joined the board of the Rhineland-Palatinate branch of the CDU. In 1954, he became vice-chair of the Junge Union in Rhineland-Palatinate. In 1955, he returned to the board of the Rhineland-Palatinate branch of the CDU. 
Life before politics.
After graduating in 1956 he became fellow at the Alfred Weber Institute of the University of Heidelberg where he was an active member of the student society AIESEC. In 1958, he received his doctorate degree for his thesis "The Political Developments in the Palatinate and the Reconstruction of Political Parties after 1945". After that, he entered business, first as an assistant to the director of a foundry in Ludwigshafen and, in 1959, as a manager for the Industrial Union for Chemistry in Ludwigshafen. In this year, he also became chair of the Ludwigshafen branch of the CDU. In the following year, he married Hannelore Renner, whom he had known since 1948, and they had two sons.
Early political career.
In 1960, he was elected into the municipal council of Ludwigshafen where he served as leader of the CDU party until 1969. In 1963, he was also elected into the Landtag and served as leader of the CDU party in that legislature. From 1966 until 1973, he served as the chair of the CDU's state branch, and he was also a member of the Federal CDU board. After his election as party-chair, he was named as the successor to Peter Altmeier, who was minister-president of Rhineland-Palatinate at the time. However, after the Landtag-election which followed, Altmeier remained minister-president.
Minister-President of Rhineland-Palatinate.
On 19 May 1969, Kohl was elected minister-president of Rhineland-Palatinate, as the successor to Peter Altmeier. During his term as minister-president, Kohl founded the University of Trier-Kaiserslautern and enacted territorial reform. Also in 1969, Kohl became the vice-chair of the federal CDU party. In 1971, he was a candidate to become chairman of the federal CDU, but was not elected. Rainer Barzel remained in the position instead. In 1972, Barzel attempted to force a cabinet crisis in the SPD/FDP government, which failed, leading him to step down. In 1973, Kohl succeeded him as federal chairman; he retained this position until 1998. 
The 1976 Bundestag election.
In the 1976 federal election, Kohl was the CDU/CSU's candidate for chancellor. The CDU/CSU coalition performed very well, winning 48.6% of the vote. However they were kept out of government by the centre-left cabinet formed by the Social Democratic Party of Germany and Free Democratic Party (Germany), led by Social Democrat Helmut Schmidt. Kohl then retired as minister-president of Rhineland-Palatinate to become the leader of the CDU/CSU in the Bundestag. He was succeeded by Bernhard Vogel.
Leader of the opposition.
In the 1980 federal elections, Kohl had to play second fiddle, when CSU-leader Franz Josef Strauß became the CDU/CSU's candidate for chancellor. Strauß was also unable to defeat the SPD/FDP alliance. Unlike Kohl, Strauß did not want to continue as the leader of the CDU/CSU and remained Minister-President of Bavaria. Kohl remained as leader of the opposition, under the third Schmidt cabinet (1980–82). On 17 September 1982, a conflict of economic policy occurred between the governing SPD/FDP coalition partners. The FDP wanted to radically liberalise the labour market, while the SPD preferred greater job security. The FDP began talks with the CDU/CSU to form a new government. 
Chancellor of West Germany.
Rise to power.
On 1 October 1982, the CDU proposed a constructive vote of no confidence which was supported by the FDP. The motion carried. Three days later, the Bundestag voted in a new CDU/CSU-FDP coalition cabinet, with Kohl as chancellor. Many of the important details of the new coalition had been hammered out on 20 September, though minor details were reportedly still being hammered out as the vote took place. Though Kohl's election was done according to the Basic Law, it came amid some controversy. The FDP had fought its 1980 campaign on the side of the SPD and even placed Chancellor Schmidt on some of their campaign posters. There were also doubts that the new government had the support of a majority of the people. In answer, the new government aimed at new elections at the earliest possible date. Polls suggested that a clear majority was indeed in reach. As the Basic Law only allows the dissolution of parliament after an unsuccessful confidence motion, Kohl had to take another controversial move: he called for a confidence vote only a month after being sworn in, in which members of his coalition abstained. President Karl Carstens then dissolved the Bundestag and called new elections.
The move was controversial, as the coalition parties denied their votes to the same man they had elected Chancellor a month before and whom they wanted to re-elect after the parliamentary election. However, this step was condoned by the German Federal Constitutional Court as a legal instrument and was again applied (by SPD Chancellor Gerhard Schröder and his Green allies) in 2005.
The second cabinet.
In the federal elections of March 1983, Kohl won a resounding victory. The CDU/CSU won 48.8%, while the FDP won 7.0%. Some opposition members of the Bundestag asked the Federal Constitutional Court to declare the whole proceeding unconstitutional. It denied their claim, but did set restrictions on a similar move in the future. The second Kohl cabinet pushed through several controversial plans, including the stationing of NATO midrange missiles, against major opposition from the peace movement. On 24 January 1984, Kohl spoke before the Israeli Knesset, as the first Chancellor of the post-war generation. In his speech, he used liberal journalist Günter Gaus' famous sentence that he had "the mercy of a late birth" ("Gnade der späten Geburt"). 
On 22 September 1984 Kohl met the French president François Mitterrand at Verdun, where the Battle of Verdun between France and Germany had taken place during World War I. Together, they commemorated the deaths of both World Wars. The photograph, which depicted their minutes long handshake became an important symbol of French-German reconciliation. Kohl and Mitterrand developed a close political relationship, forming an important motor for European integration. Together, they laid the foundations for European projects, like Eurocorps and Arte. This French-German cooperation also was vital for important European projects, like the Treaty of Maastricht and the Euro.
In 1985, Kohl and US President Ronald Reagan, as part of a plan to observe the 40th anniversary of V-E Day, saw an opportunity to demonstrate the strength of the friendship that existed between Germany and its former foe. During a November 1984 visit to the White House, Kohl appealed to Reagan to join him in symbolizing the reconciliation of their two countries at a German military cemetery. As Reagan visited Germany as part of the G6 conference in Bonn, the pair visited Bergen-Belsen concentration camp on 5 May, and more controversially, the German military cemetery at Bitburg, discovered to hold 49 members of the Waffen-SS. 
The third cabinet.
After the federal elections of 1987 Kohl won a slightly reduced majority and formed his third cabinet. The SPD's candidate for chancellor was the Minister-President of North Rhine-Westphalia, Johannes Rau. 
In 1987, Kohl received East German leader Erich Honecker – the first ever visit by an East German head of state to West Germany. This is generally seen as a sign that Kohl pursued "Ostpolitik", a policy of détente between East and West that had been begun by the SPD-led governments (and strongly opposed by Kohl's own CDU) during the 1970s. 
Domestic policy.
Kohl's chancellorship presided over a number of innovative policy measures. Extensions in unemployment benefit for older claimants were introduced, while the benefit for the young unemployed was extended to age 21. In 1986, a child-rearing allowance was introduced to benefit parents when at least one was employed. Informal carers were offered an attendance allowance together with tax incentives, both of which were established with the tax reforms of 1990, and were also guaranteed up to 25 hours a month of professional support, which was supplemented by four weeks of annual holiday relief. In 1984, an early retirement scheme was introduced that offered incentives to employers to replace elderly workers with applicants off the unemployment register. In 1989 a partial retirement plan was introduced under which elderly employees could work half-time and receive 70% of their former salary “and be credited with 90 per cent of the full social insurance entitlement.” In 1984, a Mother and Child Fund was established, providing discretionary grants “to forestall abortions on grounds of material hardship,” and in 1986 a 10 Mrd DM package of Erziehungsgeld (childcare allowance) was introduced, although according to various studies, this latter initiative was heavily counterbalanced by cuts. In 1989, special provisions were introduced for the older unemployed.
Kohl's time as Chancellor, however, also saw some controversial decisions in the field of social policy. Student aid was made reimbursable to the state while the Health Care Reform Act of 1989 introduced the concept by which patients pay up front and are reimbursed, while increasing patient co-payments for hospitalisation, spa visits, dental prostheses, and prescription drugs. In addition, while a 1986 Baby-Year Pensions reform granted women born after 1921 one year of work-credit per child, lawmakers were forced by public protest to phase in supplementary pension benefits for mothers who were born before the cut-off year.
The road to reunification.
Following the breach of the Berlin Wall and the collapse of the East German Communist regime in 1989, Kohl's handling of the East German issue would become the turning point of his chancellorship. Kohl, like most West Germans, was initially caught unaware when the Socialist Unity Party was toppled in late 1989. However, well aware of his constitutional mandate to seek German unity, he immediately moved to make it a reality. Taking advantage of the historic political changes occurring in East Germany, Kohl presented a ten-point plan for "Overcoming of the division of Germany and Europe" without consulting his coalition partner, the FDP, or the Western Allies. In February 1990, he visited the Soviet Union seeking a guarantee from Mikhail Gorbachev that the USSR would allow German reunification to proceed. One month later, the Party of Democratic Socialism — the renamed SED — was roundly defeated by a grand coalition headed by the East German counterpart of Kohl's CDU, which ran on a platform of speedy reunification.
On 18 May 1990, Kohl signed an economic and social union treaty with East Germany. This treaty stipulated that when reunification took place, it would be under the quicker provisions of Article 23 of the Basic Law. That article stated that any new states could adhere to the Basic Law by a simple majority vote. The alternative would have been the more protracted route of drafting a completely new constitution for the newly reunified country, as provided by Article 146 of the Basic Law. However, an Article 146 reunification would have opened up contentious issues in West Germany, and would have been impractical in any case since by then East Germany was in a state of utter collapse. In contrast, an Article 23 reunification could be completed in as little as six months.
Over the objections of Bundesbank president Karl Otto Pöhl, he allowed a 1:1 exchange rate for wages, interest and rent between the West and East Marks. In the end, this policy would seriously hurt companies in the new federal states. Together with Foreign Minister Hans-Dietrich Genscher, Kohl was able to resolve talks with the former Allies of World War II to allow German reunification. He received assurances from Gorbachev that a reunified Germany would be able to choose which international alliance it wanted to join, although Kohl made no secret that he wanted the reunified Germany to inherit West Germany's seats at NATO and the EC.
A reunification treaty was signed on 31 August 1990, and was overwhelmingly approved by both parliaments on 20 September 1990. On 3 October 1990, East Germany officially ceased to exist, and its territory joined the Federal Republic as the five states of Brandenburg, Mecklenburg-Vorpommern, Saxony, Saxony-Anhalt and Thuringia. These states had been the original five states of East Germany before being abolished in 1952, and had been reconstituted in August. East and West Berlin were reunited as the capital of the enlarged Federal Republic. After the fall of the Berlin Wall, Kohl confirmed that historically German territories east of the Oder-Neisse line were definitively part of Poland, thereby relinquishing any claim Germany had to them. In 1993, Kohl confirmed, via treaty with the Czech Republic, that Germany would no longer bring forward territorial claims as to the pre-1945 ethnic German so-called Sudetenland. This treaty was a disappointment for the German Heimatvertriebene ("displaced persons").
Chancellor of reunified Germany.
Reunification placed Kohl in a momentarily unassailable position. In the 1990 elections – the first free, fair and democratic all-German elections since the Weimar Republic era – Kohl won by a landslide over opposition candidate and Minister-President of Saarland, Oskar Lafontaine. He then formed his fourth cabinet.
After the federal elections of 1994 Kohl was reelected with a somewhat reduced majority, defeating Minister-President of Rhineland-Palatinate Rudolf Scharping. The SPD was however able to win a majority in the Bundesrat, which significantly limited Kohl's power. In foreign politics, Kohl was more successful, for instance getting Frankfurt am Main as the seat for the European Central Bank. In 1997, Kohl received the Vision for Europe Award for his efforts in the unification of Europe.
By the late 1990s, the aura surrounding Kohl had largely worn off amid rising unemployment. He was heavily defeated in the 1998 federal elections by the Minister-President of Lower Saxony, Gerhard Schröder.
Retirement and legal troubles.
A red-green coalition government led by Schröder replaced Kohl's government on 27 October 1998. He immediately resigned as CDU leader and largely retired from politics. However, he remained a member of the Bundestag until he decided not to run for reelection in the 2002 election.
CDU finance affair.
Kohl's life after political office in the beginning was dominated by the CDU-party finance scandal. The party financing scandal became public in 1999, when it was discovered that the CDU had received and kept illegal donations during Kohl's leadership.<ref name="Spiegel(08/07/2009)">Gerd Langguth, "", "Spiegel Online International", 8 July 2009</ref>
Life after politics.
In 2002, Kohl left the Bundestag and officially retired from politics. In recent years, Kohl has been largely rehabilitated by his party again. After taking office, Angela Merkel invited her former patron to the Chancellor's Office and Ronald Pofalla, the Secretary-General of the CDU, announced that the CDU will cooperate more closely with Kohl, "to take advantage of the experience of this great statesman", as Pofalla put it. On 5 July 2001, his wife, Hannelore, committed suicide, due to suffering from photodermatitis for many years. On 4 March 2004, he published the first of his memoirs, called "Memories 1930–1982", covering the period 1930 to 1982, when he became chancellor. The second part, published on 3 November 2005, included the first half of his chancellorship (from 1982–90). On 28 December 2004, he was air-lifted by the Sri Lankan Air Force, after having been stranded in a hotel by the 2004 Indian Ocean earthquake. Kohl is a member of the Club of Madrid.
As reported in the German press, he also gave his name to the soon-to-be launched Helmut Kohl Centre for European Studies (currently Centre for European Studies), which is the new political foundation of the European People's Party. In late February 2008, Kohl suffered a stroke in combination with a fall which caused serious head injuries and required his hospitalization, since when he has been reported as bound to a wheelchair due to partial paralysis and with difficulty speaking. He has remained in intensive care since, marrying his 43-year-old partner, Maike Richter, on 8 May 2008, while still in hospital. In 2010, he had a gall bladder operation in Heidelberg, and heart surgery in 2012.
In 2011, Kohl, in spite of his frail health, began giving a number of interviews and issued statements in which he sharply condemned his successor Angela Merkel, whom he had formerly mentored, on her policies in favor of strict austerity in the European debt crisis and later also towards Russia in the Ukrainian crisis, which he sees as opposed to his politics of peaceful bi-lateral European integration during his time as chancellor. He has published the book "Aus Sorge um Europa" ("Out of Concern for Europe") outlining these criticicms of Merkel (while also attacking his immediate successor Gerhard Schröder's Euro policy) and was widely quoted in the press as saying, "Die macht mir mein Europa kaputt." ("She's destroying the Europe that I have built."). Kohl thus joined former German chancellors Gerhard Schröder and Helmut Schmidt in their similar criticisms of Merkel's policies in these two fields.
Political views.
Kohl was committed to European integration, maintaining close relations with the French president Mitterrand. Parallel to this he was committed to German reunification. Although he continued the Ostpolitik of his social-democratic predecessors, Kohl supported Reagan's more aggressive policies in order to weaken the USSR. 
Public perception.
Kohl faced stiff opposition from the West German political left and was also mocked for his provincial background, physical stature and simple language. Similar to historical French cartoons of Louis-Philippe of France, Hans Traxler depicted Kohl as a "pear" in the left-leaning satirical journal "Titanic". The German word "Birne" ("pear") became a widespread nickname and symbol for the Chancellor.

</doc>
<doc id="41901" url="http://en.wikipedia.org/wiki?curid=41901" title="Helmut Schmidt">
Helmut Schmidt

Helmut Heinrich Waldemar Schmidt (]; born 23 December 1918) is a German Social Democratic (SPD) politician who served as Chancellor of West Germany from 1974 to 1982. Prior to becoming Chancellor, he had served as Minister of Defence (1969–72). As Minister of Finance (1972 to 1974), he gained credit for financial policies that consolidated the "Wirtschaftswunder" (economic miracle), giving Germany the most stable currency and economic position in the world. He had also served briefly as Minister of Economics and as acting Foreign Minister. As Chancellor, he focused on international affairs, seeking "political unification of Europe in partnership with the United States". He was an energetic diplomat who sought European co-operation and international economic co-ordination. He was re-elected chancellor in 1976 and 1980, but his coalition fell apart in 1982 with the switch by his coalition allies, the Free Democratic Party. He retired from Parliament in 1987, after clashing with the SPD's left wing, who opposed him on defence and economic issues. In 1986 he was a leading proponent of European monetary union and a European central bank.
Background.
Helmut Schmidt was born in Hamburg in 1918, the first of two sons of two teachers, Ludovica (born Koch) and Gustav Ludwig Schmidt. Schmidt studied at Hamburg Lichtwark School, graduating in 1937. Schmidt's father was the illegitimate son of a German Jewish businessman, although this was kept a family secret for many years. This was confirmed publicly by Helmut Schmidt in 1984, after Valéry Giscard d'Estaing revealed the fact to journalists, apparently with Schmidt's assent. Schmidt himself is a non-practising Lutheran.
Schmidt was a group leader (Scharführer) in the Hitler Youth organization until 1936 when he was demoted and sent on leave because of his anti-Nazi views.
On 27 June 1942, he married his childhood sweetheart Hannelore "Loki" Glaser (3 March 1919 – 21 October 2010). They had two children: Helmut Walter (26 June 1944 – February 1945, died of meningitis), and Susanne (b. 1947), who works in London for Bloomberg Television. Schmidt resumed his education in Hamburg after the war, graduating in economics and political science in 1949.
Military service.
He was conscripted into military service and began serving with an anti-aircraft battery at Vegesack near Bremen during World War II. After brief service on the Eastern Front, including the Siege of Leningrad, he returned to Germany in 1942 to work as a trainer and advisor at the Ministry of Aviation. He attended the People's Court, presided over by Roland Freisler, as an army spectator at some of the show trials for officers involved in 20 July plot where an unsuccessful attempt was made to assassinate Hitler at Rastenburg and was disgusted by the whole process. Toward the end of the war, from December 1944 onwards, he served as an Oberleutnant in the Flakartillery on the Western Front. He was captured by the British in April 1945 on Lüneburg Heath and was a prisoner of war until August. During his service in World War II Schmidt was awarded the Iron Cross.
Political career.
Early years.
Schmidt joined the Social Democratic Party of Germany (SPD) in 1946, and from 1947 to 1948 was leader of the Socialist German Student League, the student organisation of the SPD. Upon leaving the university, he worked for the government of the city-state of Hamburg, working in the department of economic policy. Beginning in 1952, under Karl Schiller, he was a senior figure in the "Behörde für Wirtschaft und Verkehr" (the Hamburg State Ministry for Economy and Transport).
He was elected to the "Bundestag" in 1953, and in 1957 he became a member of the SPD parliamentary party executive. A vocal critic of conservative government policy, his outspoken rhetoric in parliament earned him the nickname "Schmidt-Schnauze" (Schmidt, the loud mouth). In 1958, he joined the national board of the SPD ("Bundesvorstand") and campaigned against nuclear weapons and the equipping of the "Bundeswehr" with such devices. In 1958, he gave up his seat in parliament to concentrate on his tasks in Hamburg.
From 27 February 1958, to 29 November 1961, he was a Member of the European Parliament, which was not directly elected at the time.
Senator.
The government of the city-state of Hamburg is known as the Senate of Hamburg, and from 1961 to 1965 Schmidt was the "Innensenator", that is Minister of the Interior. He gained the reputation as a "Macher" (doer) – someone who gets things done regardless of obstacles – by his effective management during the emergency caused by the 1962 flood. Schmidt used all means at his disposal to alleviate the situation, even when that meant overstepping his legal authority, including federal police and army units (ignoring the German constitution's prohibition on using the army for "internal affairs"; a clause excluding disasters was not added until 1968). Describing his actions, Schmidt said, "I wasn't put in charge of these units – I took charge of them!"
This characteristic, coupled with a pragmatic attitude and opposition to political idealism including those of student protests, was best symbolised by his well known remark "People who have visions should go see a doctor."
Return to federal politics.
In 1965, he was re-elected to the Bundestag. In 1967, after the formation of the Grand Coalition between SPD and Christian Democratic Union (CDU), he became chairman of the Social Democrat parliamentary party, a post he held until the elections of 1969. In 1967, he was elected deputy party chairman.
In October 1969, he entered the government of Willy Brandt as defence minister. During his term in office the military conscription time was reduced from 18 to 15 months. Additionally, Schmidt decided to introduce the Bundeswehr universities in Hamburg and Munich to broaden the academic education of the German officer corps. In July 1972, he succeeded Karl Schiller as Minister for Economics and Finances, but in November 1972, he relinquished the Economics department, which was again made a separate ministry. Schmidt remained Minister of Finances until May 1974.
Between 1968 to 1984, Schmidt was deputy chairman of the SPD. Unlike Willy Brandt and Gerhard Schröder, he never became chairman of the party.
Chancellor.
Schmidt became Chancellor of West Germany on 16 May 1974, after Brandt's resignation in the wake of an espionage scandal. The worldwide economic recession was the main concern of his administration, and Schmidt took a tough and disciplined line. During his term, Germany had to cope with the oil crisis of the 1970s; according to some judgments, Germany managed better than the most of the industrial states. Schmidt was also active in improving relations with France. Together with the French President Valéry Giscard d'Estaing, he was one of the fathers of the world economic summits, the first of which assembled in 1975.
In 1975, he was a signatory of the Helsinki Accords to create the Conference for Security and Co-operation in Europe, the precursor of today's OSCE.
He remained chancellor after the 1976 elections in coalition with the liberal Free Democratic Party (FDP).
He adopted a tough, uncompromising line with the indigenous Red Army Faction (RAF) terrorists. He authorized the GSG 9 anti-terrorist unit to end the Palestinian terrorist hijacking of the Lufthansa aircraft "Landshut", undertaken to secure the release of RAF leaders imprisoned in Stammheim Prison, after it landed in Mogadishu by assaulting the aircraft during the German Autumn of 1977. Three of the four terrorists were killed during the hostage rescue.
Concerned about the Soviet invasion of Afghanistan and the Soviet superiority regarding missiles in Central Europe, Schmidt issued proposals resulting in the NATO Double-Track Decision concerning the deployment of medium-range nuclear missiles in Western Europe should the Soviets not disarm. He was re-elected as chancellor in November 1980. In October 1981, Schmidt was fitted with a cardiac pacemaker.
At the beginning of his period as chancellor, Schmidt was a proponent of Keynesian economics, and pursued expansionary monetary and fiscal policies during his time as chancellor. Between 1979 and 1982, the Schmidt administration pursued such policies in an effort to reduce unemployment. These were moderately successful, as the fiscal measures introduced after 1977, with reductions in income and wealth taxes and an increase in the medium-term public investment programme, was estimated to have created 160,000 additional jobs in 1978–79, or 300,000 if additional public sector employment was included in the figure. The small fall in the unemployment rate, however, was achieved at the cost of a larger budget deficit (which rose from 31.2 billion DM to 75.7 billion DM in 1981), brought about by fiscal expansion.
During the 70s, West Germany was able to weather the global financial storm far better than almost all the other developed countries, with unemployment and inflation kept at comparatively low levels. During the 1976 election campaign, the SPD / FDP coalition was able to win the battle of statistics, whether the figures related to employee's incomes, strikes, unemployment, growth, or public sector debts. Amongst other social improvements, retirement pensions had been doubled between 1969 and 1976, and unemployment pay increased to 68% of previous earnings.
While visiting Saudi Arabia in April 1981, Schmidt made some unguarded remarks about the Israel-Palestine conflict that succeeded in aggravating the always-delicate relations between Israel and West Germany. Asked by a reporter about the moral aspect of German-Israeli relations, he stated that Israel was not in a position to criticize Germany due to its handling of Palestinians and "That won't do. And in particular, it won't do for a German living in a divided nation and laying moral claim to the right of self-determination for the German people. One must then recognize the moral claim of the Palestinian people to the right of self-determination." On May 3, Israeli prime minister Menachem Begin denounced Schmidt as "unprincipled, avaricious, heartless, and lacking in human feeling" and that he had "willingly served in the German armies that murdered millions". Begin was also upset over remarks he had made on West German television the previous week in which he spoke apologetically about the suffering Germany inflicted on various nations during WWII, but made no mention of the Jews. While flying home from Riyadh, Schmidt told his advisers that war guilt could not continue to affect Germany's foreign relations.
Schmidt was the first world leader to call upon newly elected French president François Mitterrand, who visited Bonn in July. The two found themselves in "complete agreement" on foreign policy matters and relations with the United States and the Soviet Union, but came to blows over trade and economic issues.
By the end of his term, however, Schmidt had turned away from deficit spending, due to a deteriorating economic situation, and a number of welfare cuts were carried out, including smaller increases in child benefits and higher unemployment and health contributions. Large sections of the SPD increasingly opposed his security policy while most of the FDP politicians strongly supported that policy; while representatives of the left wing of the Social Democratic Party opposed reduction of the state expenditures, the FDP began proposing a monetarist economic policy. In February 1982, Schmidt won a motion of confidence, however on 17 September 1982, the coalition broke apart, with the four FDP ministers leaving his cabinet. Schmidt continued to head a minority government composed only of SPD members, while the FDP negotiated a coalition with the CDU/CSU. During this time Schmidt also headed the Ministry of Foreign Affairs. On 1 October 1982, parliament approved of a Vote of No-Confidence and elected the CDU chairman Helmut Kohl as the new chancellor. This was the only time in the history of the Federal Republic that a chancellor was ousted from office in this way.
Domestic reforms.
Although Schmidt did not feel that he was in a position to substantially extend the social reforms of the Brandt Administration, due to the economic problems he encountered during his time as chancellor, a wide range of reforms were nevertheless carried out under his administration. Increases were made to pensions, which went up in numerical terms by 11.1% (1975), 11.0% (1976), 9.9% (1977), 4.5% (1979), 4% (1980), 4% (1981), and 5.8% (1982). Adjusted for changes in the annual price index, pensions went up in real terms by 5.1% (1975), 6.7% (1976), 6.2% (1977), 0.4% (1979), and 0.6% (1982). However, the rate of pension was not changed in 1978 (even though prices increased by 2.7%), and in 1980 and 1981 the real value of pensions fell by 1.5% and 2.3%, respectively. Improvements were made in family allowances, with monthly subsidies for children increased by over 100% in 1975.
Improvements were made to invalidity and old-age pension provision for the unemployed, who (from 1977 onwards) were technically insured free of charge under the old-age pension and invalidity scheme. Previously, there had only existed partial and restricted coverage for the unemployed. The Law to Improve Occupational Old Age Pensions (1974) extended coverage of occupational pensions whilst also "co-ordinating them more closely with state pensions and setting minimum standards as regards benefit levels and the preservation of pension rights". By 1976, as a result of this legislation, 65% of private sector employees were covered by occupational schemes and over two-thirds of these workers were eligible for benefits equal to more than 15% of their earnings at retirement. This legislation also acquired that entitlements to occupational pensions must not expire after leaving a form and that occupational pensions must not be reduced as a result of receipt of benefits under the public insurance system. The Social Insurance Law for the Handicapped (1975) extended compulsory coverage to handicapped persons working in special establishments for the handicapped (medical benefits and cash benefits to replace earnings from work). In 1976, a new declaration of social rights was made, and in 1979, an Act was passed which lowered the pensionable age for severely handicapped persons to 61 years, and to 60 years as from 1980.
In October 1974, a was passed with the intention of promoting rehabilitation of the handicapped by extending certain benefits to them. To meet the need for more uniform medical treatment in rural areas and on the peripheral of cities due to a lack of panel doctors in those areas, a bill was passed in December 1976 which improved the possibilities of panel doctors' associations by ensuring that panel doctors were available to provide treatment, while also providing for planning according to need and the participation of the sickness insurances. An Act of August 1975 on criminal law reform introduced "other forms of assistance" such as medical advice on contraception, together with assistance pertaining to sterilisation and abortion. New assistance benefits were created in 1975 for family planning and maternity consultations, whilst a constant attendance allowance was increased. Housing renovation and energy savings legislation was introduced in 1977, while a constitutional reform of 1981 increased federal powers in health and education.
In July 1974, special benefits were introduced to compensate for wages not paid as a result of bankruptcy for a maximum of up three months. Increases in income-limits for housing allowances were carried out, together with housing allowance rates, while major improvements were made in welfare provision for the elderly. By 1982, the purchasing power of the average pension was 2.5% better than in 1975. In 1975, tax allowances were replaced by child benefit, while payment for the first child was introduced. A tax relief act reduced income taxes and provided additional tax benefits for housing allowances. The Schmidt administration also introduced social policy legislation in the late seventies which increased family allowances (though by a smaller amount than in 1974) and maternity leave benefits. The increases in benefits under the Schmidt administration arguably had a positive impact on reducing inequalities, with the percentage of West Germans living in poverty (according to one measurement) falling between 1978 and 1982.
A law of June 1974 on the reform of criminal law obliged the expectant mother, who anticipates an interruption of pregnancy during the first twelve weeks, to enquire about private and public assistance available to mothers and children. The Federal Ministry for Youth, Family Affairs and Health launched, by virtue of complementary measures to the criminal law reform, a pilot scheme of a scientific nature involving the setting-up of 53 advisory centres to help achieve a quantitative and qualitative improvement in this field. Particular attention was also been given to the development of parental and family education, and it was for this reason that the Brandt Administration encouraged the extension of the 'letters to parents' sent by the Ministry whenever a child is born, and subsequently at regular intervals, until the child reaches school age. A law of July 1974 amending the Federal Law on the promotion of vocational training increased the required rates and allowances by 20% on average, along with the lump-sum payments for social security, and extended the assistance given irrespective of parental income to older trainees who can expect little if any support from their parents. On 1 August 1974, the provisions of a law amending the Federal Law for the promotion of vocational training and the Law on the upgrading of employment of 14 November 1973 came into force. This Law provided for the extension of training facilities to new sections of the population and for financial support so that further sections of the population could also acquire vocational training. A law on old people's homes and adult hostels, passed in August 1974, was aimed at safeguarding the interests and needs of people who reside in these establishments, controlling the prices charged there, and providing advice for elderly persons and those in charge of the establishments and to guarantee the necessary medical care. Under the law, the residents could participate in the management of the establishment through a consultative committee. A law of June 1975 amended the Employment Protection Law and the Law on the provision of temporary workers which improved the legal protection of temporary migrants workers in West Germany. A law of December 1975 gave the right to claim under the sickness insurance scheme for medical consultations for family planning purposes. A law of May 1975 extended social security to handicapped persons according to various procedures.
A law of April 1976 on youth employment limited working hours to 40 hours in a 5-day week, raised the minimum working age from 14 t o15, increased leave, improved conditions for release from work for day attendance at vocational training school and for periods of weeks under the block release system, and improved protection at work by restrictions on employment in dangerous or unhealthy work. A law on protection against dismissal was amended by abolishing the minimum age limit of 18, so that young workers under eighteen were now also protected against dismissal. The Ministry for Youth, Family Affairs and Health encouraged a pilot scheme, of a scientific nature, aimed at promoting the development of qualified advisory services on family planning, sexual problems and problems linked with pregnancy. A regulation of June 1976 laid down detailed rules governing 'aid to overcome particular social difficulties'. This measure was specially aimed at marginal social groups, such as former convicts and the homeless, and consisted of providing information, personal guidance, help in obtaining and maintaining a home and in obtaining and keeping a job, in addition to guidance as regards training and the organization of leisure time. The general section of the Social Code, which came into effect in January 1976, introduced basic measures concerning the social services. It laid down an obligation to establish the services and institutions needed by the population and to provide them with information and advice on their social rights. These provisions had already had certain effects, in particular a considerable growth in home help services and social centres. A regulation in application of a 1974 law on old people's homes and adult hostels was introduced, according to which compulsory consultative committees could be set up by the residents to ensure their participation in the running of these establishments in a greater measure than in the past. A law passed in August 1974 supplemented the protection provided for handicapped people under a law passed during the Brandt Administration in April 1974 by providing that, henceforth, the benefits for the purposes of medical and occupational rehabilitation would be the same for all the categories of persons concerned: war victims, the sick, the victims of industrial accidents, congenitally handicapped persons: a total of about 4 million persons in all.
The 1976 Act for the Promotion of Urban Development and the 1977 Housing Modernisation Act, together with the 1971 Act for the Promotion of Urban Development passed by the Brandt Administration, enabled most West German cities by the end of the Seventies to introduce programmes aimed at renovating their pre-war residential areas. Additional tax reforms were introduced that lowered the tax burden on low-income households, and which played an important role "in pre-empting a real decline in the income and purchasing power of workers". A law was passed to encourage low-income home ownership, while 250 million marks was provided in 1978 for the promotion of sports and physical education. That same year, entitlement to educational allowances was extended to all tenth-grade pupils in vocational education.
The Introductory Tax Reform Law (1974) increased bad weather payments, part-time workers' benefits and insurance benefits to 68% of net wages, fixed special benefits during vocational training at 90% of net earnings, increased assistance benefits to 58% of net earnings, and abolished special family benefits "in favour of the inclusion of the unemployed under general child allowance scheme". A special tax credit was introduced in 1978 in cases of particular financial burden due to children, while a substantial increase in the child allowance was made in 1979. Several policy changes were carried out between 1976 and 1982, such as tax credits and family allowances, which compensated unions for wage restraint and "guaranteed the maintenance of a constant income level for employed persons and their families". Increases were made in child benefits, which rose on a regular basis (particularly for families with more than one child) for most of the years that the Schmidt Administration was in office.
In terms of workplace rights, a "parity" system was introduced (although in a weakened form) on the supervisory boards of all companies employing over 2,000 workers, a reform which West German trade unions had long fought for. This law improved employee representation on the supervisory boards of companies outside the steel and coal industries. The main provision of this new piece of legislation was that in the 650 major companies that accounted for 70% of West Germany's output, employee representation on the supervisory boards rose from one-third to one-half. In 1976, the Young Persons (Protection of Employment) Act was passed, which forbade the employment of children and young persons required to attend full-time education, with minor exceptions.
Various measures were also carried out to mitigate the effects of unemployment. Employment creation schemes were introduced to help young workers. The Training Opportunities Act (1976) helped (over a four-year period) to increase the number of vocational training places from 450,000 to 630,000 a year. In 1976, a provisional law was introduced to boost the number of apprentices, which reduced the numbers of young people out of work. An experimental retraining programme was launched on the shop floor (lasting from 1979 to 1981), which benefited 45,680 people.
In June 1974, a reformed food law was passed into law, which aimed to safeguard consumers from physical harm. The Students' Sickness Insurance Law (1975) extended compulsory coverage to students (medical benefits only), while the Artists' Social Insurance Law (1981) introduced compulsory insurance for artists below a certain income-limit. The Detergents Law (1975) and the Effluency Levies Act (1978) were passed to encourage environmental protection. In 1975, the allowable duration of unemployment benefit payment was extended to twenty-four months during periods of general recession. The 1976 law on standard terms of sale gave consumer groups the right to file suits against companies employing unfair terms of sale. The Higher Education Framework Act of 1976 pronounced that scientific continuing education was a task to be implemented by the institutions of the system of higher education, thus exceeding their traditional tasks of research and lecturing. In 1977, an "investment programme for the future" was decided upon by the Schmidt Administration, which provided DM 16 thousand million for the improvement of the transport system, an efficient and ecological energy supply, provisions for water supply, vocational training, and the safeguarding of the environment.
The social protection of civil servants and judges (Bund and Lander) was standardised and improved by a law of August 1974. Under a law of May 1976, victims of acts of violence and their survivors would in future have the right to compensation in respect of the physical and economic consequences in the same manner as protection for war victims. In 1977, DM 8 million was made available by the federal government to welfare bodies to build and modernise holiday homes for families. That same year, the conditions for investment in the privately financed construction of rented dwellings were improved by the reintroduction of decreasing depreciation for buildings. In order to take the situation of the unemployed into account to the maximum possible extent in asset formation policy, certain legal provisions were amended so that in the event of unemployment, personal payments could be made to continue savings plans which entailed employers contributions. In addition, workers who had been unemployed for a year or more could unblock savings plans before the end of the freeze without losing the financial benefits offered by the State. A new special programme with funds of DM 100 million was launched at the start of 1978 to improve training and job opportunities for the handicapped. The budget of the Federal Labour Office was increased exceptionally by more than 20%, whilst special emphasis was placed on measures to promote vocational training, job creation, advanced training and retraining. The aim was to reduce the high proportion of unemployed persons lacking training and increase the chances of this group to obtain employment.
Under a regulation of December 1976, four new occupational diseases were recognised. To expand training opportunities for girls, a pilot scheme was launched in 1978 to open up certain skilled industrial and technical occupations to them. Laws restricting the access of migrant workers to certain regions were repealed in 1977, and the existing provisions were made more flexible in order to allow the children of migrant workers who had entered the Federal Republic of Germany in 1975/76 access to employment. Legislation governing old people's homes and adult assistance establishments was further supplemented by two regulations, one imposing minimum requirements concerning premises, and the other laying down rules for financial management to ensure that residents were not financially exploited.
The Fifth Amendment of July 1979 to the Employment Promotion Law provided among other things for an improvement in conditions governing financial support towards basic vocational training for unemployed young people with at least one year's vocational experience, the expansion of training activities for jobs in which there is a shortage of skilled workers and easier access to further vocational training facilities for problem groups (such as the unskilled, the unemployed, and women generally). In 1979, the Federal Minister for Education and Science made funds available for a new further education establishment to train instructors. Under a law amending the law respecting technical working media and the Industrial Code of August 1979, machines and equipment which had been voluntarily submitted for testing and passed by an established body may bear the marking 'GS' (=safety-tested). For medical equipment, the Federal Minister of Labour and Social Affairs was authorized to issue orders containing further safety provisions, while the resale of hazardous equipment and its display at exhibitions may be prohibited in future by factory inspectors even in the case of trading companies.
In 1979 DM 219 million was set aside for about 80,000 dwellings under the modernisation programme for dwellings worthy of preservation run jointly by the Federal authorities and the individual Lander (50% of this money was earmarked for modernization priority areas). In addition, DM 2 350 million was made available under a five-year programme to improve the housing stock. Loans and higher tax rebates were also used to encourage modernisation of dwellings and energy-saving measures. 577 slum clearance and urban development schemes in 459 municipalities were also accorded financial support amounting to DM 183.5 million under a law on the promotion of urban development. A law of October 1979 granted a lump-sum allowance for the winter of 1979/80 to help low-income groups to meet the additional outlay incurred by the rise in fuel costs. In August 1979, a programme was adopted for foreign refugees, with resources allocated for aid concerning information, legal advice, psycho-social and medical assistance and for measures to facilitate the integration of refugees or their emigration to other countries.
Under a law of July 1980, a farmer's surviving spouse wishing to continue working on the farm could obtain a helper or temporary aid from the agricultural pension fund. Any spouse choosing not to do so was entitled to a survivor's allowance if he or she was no longer able to find suitable paid employment either for reasons of age (over 45) or because there were children to bring up. In other cases, the allowance was designed to facilitate reintegration into working life. This allowance guaranteed the spouse protection under the agricultural sickness
insurance scheme, which also covered self-employed fishermen and beekeepers.
A special programme was introduced, specially designed for young people who, because of their poor level of education and language ability, were unable to find a suitable job or training place. The young people were offered a one-year full-time course of training to qualify them for a training place or job, and in September 1980, approximately 15,000 young people were participating in these courses. From 1980 onwards, parents could deduct the cost of day care for their children (in day nurseries and nursery schools in particular) from their taxable income up to an annual maximum of DM 600 or DM 1,200 depending on whether the income of a single parent or that of a married couple was involved. Major additions were also made to the regulations on dangerous substances, while comprehensive new regulations concerning installations requiring supervision were introduced. The Federal Ministry for Youth, Family Affairs and Health gave particular attention to assisting parents in assuming their educational responsibilities towards their children. For instance, special 'letters to parents' were distributed free of charge to parents of children under 8, with some 3 million sent in 1979. A determined effort was also made to provide better education for socially disadvantaged children by supporting pilot schemes and research projects. Public funds had been allocated from 1979 onwards to a pilot scheme entitled 'Aid to children in need' under which children's communities were set up in Berlin and Giitersloh to protect and care for children who had been or were at risk of being ill-treated by their parents, while at the same time the family education and advisory services were assigned the task of educating these parents.
In 1981, DM 340 million were set aside for subsidies and DM 148 million for low-interest loans, which enabled financial assistance to be granted towards the modernization of some 80 000 dwellings. An amendment to a law of September 1980 on air traffic, adopted in January 1981, prohibited the transport of radioactive substances by air without a special permit. Existing safety regulations were considerably extended and modified by the technical committees responsible for individual specialist areas. Regarding installations requiring supervision, the technical regulations for pressure containers (19 January 1982) and steam boilers (26 January, 18 March and 8 June 1982) were extended and revised, with their most important provisions concerning the oil- and gas-firing of steam boilers. A Directive on connecting lines designed to carry dangerous fluids (11 June 1982) was issued, together with technical regulations on pressure gases (11 June and 9 July 1982) The existing technical regulations on flammable fluids were also modified and by means of new regulations and directives extended (19 April 1982). Other modifications were made to the technical regulations on high-pressure gas pipelines (22 June and 10 September 1982) and on installations. where acetylene is present and calcium carbide is stored (30 September 1982), while new recommended levels for dangerous working substances were incorporated into the regulations governing these substances (10 May 1982).
A wide range of social liberal reforms were also carried out during Schmidt's time in office. A marriage and divorce law of 1976 instituted the principle of maintenance obligations of each economically stronger partner, That same year, A reform of naming for partners after marriage was carried out, together with a reform of marriage law, which eliminated "moral guilt" as a criterion for alimony payment obligations. The First Marriage Reform Law of 1976 stated that pension entitlements acquired during marriage must be shared with the economically weaker spouse following divorce. In 1977, a law was introduced which enabled married women to enter employment without the permission of their husbands, while prison reforms guaranteed inmates access to courts for any violations of their rights, limited sentences in all but the gravest cases to 15 years, and proclaimed rehabilitation to be the objective of incarceration. In 1977, a Sex Discrimination Act was passed. In 1981, a legal aid system was established to facilitate access to courts of law.
An amendment to the legal code for residency permits was made in 1978, which granted foreign residents the right to unlimited residence permits after five years of continuous residency. The amendment also stated that legal residents would be eligible for a residence entitlement after eight years if certain conditions were met, such as language fluency. In 1979, paid parental leave was extended from 2 to 6 months, while the European directive on equal treatment for women in paid employment was adopted that same year. The Maintenance Security Law of 1979 introduced public advance payments for single parents "not in receipt of maintenance payments from the liable parent". These benefits were made payable up to 36 months, and private claims against a parent not meeting a maintenance liability were taken over by the state. In that same year, four months paid parental leave were introduced for working mothers, while job-protected leave after childbirth was increased from 8 weeks to 6 months. In 1980, a "compliance law" was passed that covered discrimination in hiring, promotion and dismissal, and measures to promote equal pay.
Cabinets during Schmidt's chancellorship.
Schmidt's first term as Federal Chancellor, 16 May 1974 – 15 December 1976
Changes
Schmidt's second term as Federal Chancellor, 15 December 1976– 5 November 1980
Changes
Schmidt's third term as Federal Chancellor, 5 November 1980,– 17 September 1982
Changes
Life after politics.
In 1982, along with his friend Gerald Ford, he co-founded the annual AEI World Forum.
In 1983, he joined the nationwide weekly "Die Zeit" newspaper as co-publisher. In 1985, he became Managing Director. With Takeo Fukuda he founded the Inter Action Councils in 1983. He retired from the "Bundestag" in 1986. In December 1986, he was one of the founders of the committee supporting the EMU and the creation of the European Central Bank.
Contrary to the current line of his party, Helmut Schmidt is a determined opponent of Turkey's entry into the EU. He also opposes phasing out nuclear energy, something that the Red-Green coalition of Gerhard Schröder supported. Further, Schmidt regards the climate debate as "hysterical" and is skeptical of the IPCC reports. About the Internet, Schmidt said, he perceives it as "threatening".
Schmidt is author of numerous books on his political life, on foreign policy and political ethics. He remains one of the most renowned political publicists in Germany.
In recent years, Schmidt has been afflicted with increasing deafness.
In 2014, Schmidt said the situation in Ukraine is dangerous, because "Europe, the Americans and also Russia are behaving in the way that the author Christopher Clark, in his book ["The Sleepwalkers: How Europe Went to War in 1914"] that's very much worth reading, describes the start of World War I: like sleepwalkers."
Friendships.
Schmidt called the assassinated Egyptian president Anwar as-Sadat among his friends from the world of politics, and sustains his friendship with ex-president Valéry Giscard d'Estaing of France. His circle also includes former U.S. Secretary of State Henry Kissinger who is on record as stating that he wishes to predecease Helmut Schmidt, because he would not wish to live in a world without Schmidt.
He was also good friends with Canadian Prime Minister Pierre Trudeau. In 2011 Schmidt, accompanied by Jean Chrétien and Tom Axworthy, made a pilgrimage to the Trudeau family vault in St-Rémi-de-Napierville Cemetery.
Personal life.
Schmidt is a great admirer of the philosopher Karl Popper, and contributed a foreword to the 1982 Festschrift in Popper's honor.
The university of Germany's federal armed forces in Hamburg was renamed Helmut Schmidt University – University of the Federal Armed Forces Hamburg in 2003 in honour of the politician who – as minister of defence – had introduced obligatory academic education for German career officers.
Born and raised in Hamburg, Schmidt is regarded in Germany as an embodiment of hanseatic values, according to which he has never accepted a medal or an order of merit (not even the Federal Republic's Federal Cross of Merit).
Schmidt is also a talented pianist, and has recorded piano concertos of both Mozart and Bach with the well-known German pianist and conductor Christoph Eschenbach. Schmidt recorded Mozart's piano concerto for three pianos, K. 242, with the London Philharmonic Orchestra directed by Eschenbach in 1982 with pianists Eschenbach and Justus Franz for EMI Records (CDC 7 47473 2). In that recording, according to the CD's liner notes, Schmidt played the part written for Countess Antonia Lodron's youngest daughter Guiseppina, "almost a beginner" who commissioned the work. The part brilliantly "enables any reasonably practiced amateur to participate in a performance". The same musical notes also indicate that Schmidt and Franz had played duets during Franz's student days.
Schmidt is a smoker. He is well known for lighting up cigarettes on TV interviews and talkshows. In January 2008, German police launched an enquiry after Schmidt was reported by an anti-smoking initiative for defying the recently introduced smoking ban. The initiative claimed that Helmut Schmidt had been flagrantly ignoring laws "for decades". Despite pictures in the press, the case was subsequently dropped after the public prosecution service decided that Schmidt's actions had not been a threat to public health.
On 6 April 2010, with a lifespan of 33,342 days he surpassed Konrad Adenauer in terms of longevity and is now the oldest former chancellor in German history.
His wife, Loki Schmidt, died on 21 October 2010.
At the beginning of August 2012, Schmidt gave an interview on German television and revealed that at 93 years of age he had fallen in love again. His new life-partner is his long-standing associate, Ruth Loah, 79.
Honours and Awards.
In 1988 he received the Freedom medal.

</doc>
<doc id="41902" url="http://en.wikipedia.org/wiki?curid=41902" title="Ludwig Erhard">
Ludwig Erhard

Ludwig Wilhelm Erhard (]; 4 February 1897 – 5 May 1977) was a German politician affiliated with the CDU and Chancellor of the Federal Republic of Germany (West Germany) from 1963 until 1966. He is often famed for leading German postwar economic reforms and economic recovery ("Wirtschaftswunder", German for "economic miracle"), in his role as Minister of Economics under Chancellor Konrad Adenauer in 1949, prior to his own ascension to the Chancellorship in 1963. As Chancellor, however, his fame rapidly declined. Erhard resigned his Chancellorship on 1 December 1966.
Life and work.
Born in Fürth, Kingdom of Bavaria, Erhard was a commercial apprentice from 1913 to 1916. After his apprenticeship he worked as retail salesman in his father's draper's shop.
He joined the German forces during World War I 1916 as an artilleryman, fought in Romania and was seriously injured near Ypres in 1918. Because of his injury he could no longer work as a draper and started learning economics . He received his PhD from Franz Oppenheimer in 1925.
During his time in Frankfurt he married Luise Lotter (1893–1975), widow Schuster, on 11 December 1923. After his graduation they moved to Fürth and he became executive in his parents' company in 1925. After three years he became assistant at the "Institut für Wirtschaftsbeobachtung der deutschen Fertigware", a marketing research institute. Later, he became deputy director of the institute.
During World War II, he worked on concepts for a postwar peace; however, officially such studies were forbidden by the Nazis, who had declared "total war". As a result, Erhard lost his job in 1942 but continued to work on the subject by order of the "Reichsgruppe Industrie". In 1944 he wrote "War Finances and Debt Consolidation" (orig: "Kriegsfinanzierung und Schuldenkonsolidierung"). In this study he assumed that Germany had already lost the war. He sent his thoughts to Carl Friedrich Goerdeler, a central figure in the German resistance against the Nazi government, who recommended Erhard to his comrades. Erhard discussed his concept with Otto Ohlendorf, deputy secretary of state in the Reichsministerium für Wirtschaft, as well. Ohlendorf himself spoke out for "active and courageous entrepreneurship (aktives und wagemutiges Unternehmertum)", which was intended to replace bureaucratic state planning of the economy after the war. Erhard was an outsider who supported the resistance, who personally and professionally rejected Nazism, and who endorsed efforts to effect a sensitive, intelligent approach to economic revival during the approaching postwar period.
Postwar.
After the war, Erhard became economic consultant. After the American and British administration had created the Bizone, Erhard became chairman of the "Sonderstelle Geld und Kredit" in 1947, an expert commission preparing the currency reform. The newly created Special Department for Money and Credit in Germany's western zones of occupation in September 1947, under Erhard, focused attention immediately upon the general theme of monetary and financial recovery, resulting in the adoption of the so-called Homburg plan in April 1948 that set the stage for the recovery of the economy.
In 1948 he was elected Director of Economics by the Bizonal Economic Council. On 20 June 1948, the Deutsche Mark was introduced. Erhard abolished the price-fixing and production controls that had been enacted by the military administration. This exceeded his authority, but he succeeded with this courageous step.
Minister of Economics.
In the first free elections following the Nazi era, Erhard stood for election in a Baden-Württemberg district and was elected. 
A staunch believer in economic liberalism, Erhard joined the Mont Pelerin Society in 1950 and used this influential body of liberal economic and political thinkers to test his ideas for the reorganization of the West German economy. Some of the society's members were members of the Allied High Commission and Erhard was able to make his case directly to them. The Mont Pélerin Society welcomed Erhard because this gave its members a welcome opportunity to have their ideas tested in real life. Late in the 1950s, Erhard's ministry became involved in the struggle within the society between the European and the Anglo-American factions, and sided with the former. Erhard viewed the market itself as social and supported only a minimum of welfare legislation. However Erhard suffered a series of decisive defeats in his effort to create a free, competitive economy in 1957; he had to compromise on such key issues as the anti-cartel legislation. Thereafter, the West German economy evolved into a conventional welfare state from the basis that had been already laid in the 1880s by Bismarck. According to Alfred Mierzejewski the generally accepted view is that Germany has a Social Market Economy, that the post-war German economy has evolved since 1948, but the fundamental characteristics of that economic system have not changed, while in his opinion the Social Market Economy had begun to fade in 1957, disappearing entirely by the late 1960s.
In July 1948, a group of southwest German businessmen had attacked the restrictive credit policy of Erhard as Economic Director. While Erhard had designed this policy to assure currency stability and stimulate the economy via consumption, business feared the scarcity of investment capital would retard economic recovery. Erhard was also deeply critical of a bureaucratic-institutional integration of Europe on the model of the European Coal and Steel Community.
Erhard's decided, as Economic Director for the British and American occupation zones, to lift many price controls in 1948, despite opposition from both the social democratic opposition and Allied authorities. Erhard's financial and economic policies soon proved widely popular as the German economy made a miracle recovery to rapid growth and widespread prosperity in the 1950s, overcoming wartime destruction and successfully integrating millions of refugees from the east.
Chancellor.
After the resignation of Adenauer in 1963, Erhard was elected Chancellor with 279 against 180 votes in the Bundestag on 16 October. In 1965, he was re-elected. From 1966 to 1967, he also headed the Christian Democratic Union as "de facto" chairman, despite the fact that he was never a member of that party (which made his election to the chairmanship irregular and void "de jure"), as he never formally filed a membership application despite pressures from Chancellor Adenauer. The reasons for Erhard's reluctance are unknown, but it is probable that they stemmed from Erhard's general scepticism about party politics. However, Erhard was regarded and treated as a long-time CDU member and as the party chairman by almost everyone in Germany at the time, including the vast majority of the CDU itself. The fact that he was not a member was known only to a very small circle of party leaders at the time, and it did not become known to the public until the year 2007, when the silence was finally broken by Erhard's close advisor Horst Wünsche.
In domestic policy, a number of progressive reforms were carried out during Erhard's time as Chancellor. In education, the number of years of compulsory education was extended, spending on schools was significantly increased, and a standardisation of the school system among the Lander (Hamburg Agreement) was carried out. In the field of social security, Housing Benefit was introduced in 1965, while federally funded child allowances for two or more children were introduced a year earlier.
Foreign policy.
Erhard explored using money to make possible reunification of Germany. Despite Washington's reluctance, Erhard envisaged offering Nikita Khrushchev, the leader in Moscow, massive economic aid in exchange for more political liberty in East Germany and eventually for reunification. Erhard believed that if West Germany were to offer a "loan" worth $25 billion US to the Soviet Union (which Erhard did not expect to be repaid), then the Soviet Union would permit German reunification. The acting American Secretary of State George Wildman Ball described Erhard's plan to essentially buy East Germany from the Soviet Union as "half-baked and unrealistic". Erhard's objective corresponded in time with Khrushchev rethinking his relations to West Germany. The Soviet leader secretly encouraged Erhard to present a realistic proposal for a 'modus vivendi' and officially accepted the chancellor's invitation to visit Bonn. However, Khrushchev fell from power in October 1964, and nothing developed. Perhaps more importantly, by late 1964, the Soviet Union had received a vast series of loans from the international money markets, and no longer felt the need for Erhard's money.
Erhard believed the major world problems were soluble through free trade and the economic unity of Europe (as a prerequisite for political unification); he alienated French president Charles de Gaulle, who wanted the opposite. Support for the American role in the Vietnam War proved fatal for Erhard's coalition. Through his endorsement of the American goal of military victory in Vietnam, Erhard sought closer collaboration with Washington and less with Paris. Erhard's policy complicated Allied initiatives toward German unification, a dilemma that the United States placed on the back burner as it focused on Southeast Asia. Erhard failed to understand that American global interests—not Europe's needs—dictated policy in Washington, D.C., and he rejected Adenauer's policy of fostering good relations with both the United States and France in the pursuit of West German national interest. Faced with a dangerous budget deficit in the 1966–1967 recession, Erhard fell from office in part because of concessions that he made during a visit to U.S. President Lyndon B. Johnson.
In 1961, while vice president, Johnson had hosted Konrad Adenauer some two years before the German statesman vacated the chancellorship of the German Federal Republic. In December 1963, less than a month after he had assumed the American presidency upon the assassination of John F. Kennedy, Johnson staged the first ever presidential barbecue in Erhard's honor. The event was held in and about the Stonewall Elementary School gymnasium in Stonewall in the Texas Hill Country. Among the entertainers was the internationally known concert pianist Van Cliburn, who appeared in a business suit, rather than his usual formal wear. As a member of the Texas House of Representatives, Samuel Ealy Johnson, Jr., Johnson's father, been sensitive to his German-American constituency and had opposed the Creel Committee's attempt to disparage German culture and isolate German-Americans during World War I. Adenauer and Erhard had also stayed at Johnson's ranch in Gillespie County.
Erhard's fall suggested that progress on German unification required a broader approach and a more active foreign policy. Chancellor Willy Brandt in the late 1960s abandoned the Hallstein Doctrine of previous chancellors and employed a new "Ostpolitik", seeking improved relations with the Soviet Union and Eastern Europe and thereby laying the groundwork for détente and coexistence between East and West. In the 1980s Chancellor Helmut Kohl, however, reverted to Erhard's approach in collaborating with the Reagan administration in its hard-line anti-Soviet policy.
Resignation and retirement.
On 26 October 1966, Minister Walter Scheel (FDP) resigned, protesting against the budget released the day before. The other ministers who were members of the FDP followed his example — the coalition was broken. On 1 December, Erhard resigned. His successor was Kurt Georg Kiesinger (CDU), who formed a grand coalition with the SPD.
Erhard continued his political work by remaining a member of the West German parliament until his death in Bonn on 5 May 1977. He was buried in Gmund, near the Tegernsee. The Ludwig Erhard-Berufsschule (professional college) in Paderborn, Fürth and Münster are named in his honour.
Erhard's First Ministry.
Erhard's first ministry was from 16 October 1963 – 26 October 1965.
Erhard's Second Ministry.
Erhard's Second Ministry occurred from 26 October 1965 – 1 December 1966.

</doc>
<doc id="41904" url="http://en.wikipedia.org/wiki?curid=41904" title="Storage">
Storage

Storage may refer to:

</doc>
<doc id="41906" url="http://en.wikipedia.org/wiki?curid=41906" title="Al Pacino">
Al Pacino

Alfredo James "Al" Pacino (; born April 25, 1940) is an American actor and filmmaker. He is well known for playing mobsters, especially Michael Corleone in "The Godfather" films and Tony Montana in "Scarface", and often appeared on the other side of the law—as a police officer, a detective and a lawyer.
Pacino won the Academy Award for Best Actor at the 65th Academy Awards for his performance as Frank Slade in "Scent of a Woman". Prior to his win he had received seven Oscar nominations, including one other that same year.
He made his feature film debut in 1969 in the film "Me, Natalie" in a minor supporting role, before playing the lead role in the 1971 drama "The Panic in Needle Park". Pacino’s major breakthrough came in 1972 with the role of Michael Corleone in "The Godfather", which earned him an Academy Award nomination for Best Supporting Actor. His other Oscar nominations for Best Supporting Actor were for "Dick Tracy" and "Glengarry Glen Ross". Oscar nominations for Best Actor include "The Godfather Part II", "Serpico", "Dog Day Afternoon" and "...And Justice for All".
In addition to a career in film, he has enjoyed a successful career on stage, winning Tony Awards for "Does a Tiger Wear a Necktie?" (1969) and "The Basic Training of Pavlo Hummel" (1977). A longtime fan of Shakespeare, he made his directorial debut with "Looking for Richard", a quasi-documentary on the play "Richard III". Pacino has received numerous lifetime achievement awards, including one from the American Film Institute. He is a method actor, taught mainly by Lee Strasberg and Charlie Laughton at the Actors Studio in New York.
Early life and education.
Pacino was born in Manhattan, New York, to Italian-American parents Salvatore Pacino and Rose, who divorced when he was two years old. His mother moved near the Bronx Zoo to live with her parents, Kate and James Gerardi, who, coincidentally, had come from a town in Sicily named Corleone. His father Salvatore, who was from San Fratello in the Province of Messina, moved to Covina, California, and worked as an insurance salesman and restaurateur.
In his teen years "Sonny", as he was known to his friends, aimed to become a baseball player, and was also nicknamed "The Actor". Pacino dropped out of many classes, but not English. He dropped out of school at age 17. His mother disagreed with his decision; they argued and he left home. He worked at low-paying jobs, messenger, busboy, janitor, and postal clerk, to finance his acting studies. He once worked in the mail room for "Commentary" magazine.
He began smoking at age nine, and drinking, and took up casual cannabis use at age thirteen, but never used hard drugs. His two closest friends died from drug abuse at the ages of 19 and 30. Growing up in The Bronx, he got into occasional fights and was considered something of a troublemaker at school.
He acted in basement plays in New York's theatrical underground but was rejected for the Actors Studio while a teenager. Pacino then joined the Herbert Berghof Studio (HB Studio), where he met acting teacher Charlie Laughton (not to be confused with the British actor Charles Laughton), who became his mentor and best friend. In this period, he was often unemployed and homeless, and sometimes slept on the street, in theaters, or at friends' houses.
In 1962, his mother died at the age of 43. The following year, Pacino's grandfather James Gerardi, one of the most influential people in his life, also died.
Actors Studio training.
After four years at HB Studio, Pacino successfully auditioned for the Actors Studio. The Actors Studio is a membership organization of professional actors, theatre directors and playwrights in the Hell's Kitchen neighborhood of Manhattan in New York City. Pacino studied "method acting" under acting coach Lee Strasberg, who later appeared with Pacino in the films "The Godfather Part II" and in "...And Justice for All".
During later interviews he spoke about Strasberg and the Studio's effect on his career. "The Actors Studio meant so much to me in my life. Lee Strasberg hasn't been given the credit he deserves ... Next to Charlie, it sort of launched me. It really did. That was a remarkable turning point in my life. It was directly responsible for getting me to quit all those jobs and just stay acting."
In another interview he added, "It was exciting to work for him [Lee Strasberg] because he was so interesting when he talked about a scene or talked about people. One would just want to hear him talk, because things he would say, you'd never heard before ... He had such a great understanding ... he loved actors so much."
Pacino is currently co-president, along with Ellen Burstyn and Harvey Keitel, of the Actors Studio.
Stage career.
In 1967, Pacino spent a season at the Charles Playhouse in Boston, performing in Clifford Odets' "Awake and Sing!" (his first major paycheck: $125 a week); and in Jean-Claude Van Itallie's "America, Hurrah", where he met actress Jill Clayburgh on this play. They had a five-year romance and moved together back to New York City.
In 1968, Pacino starred in Israel Horovitz's "The Indian Wants the Bronx" at the Astor Place Theater, playing Murph, a street punk. The play opened January 17, 1968, and ran for 177 performances; it was staged in a double bill with Horovitz's "It's Called the Sugar Plum", starring Clayburgh. Pacino won an Obie Award for Best Actor for his role, with John Cazale winning for Best Supporting actor and Horowitz for Best New Play. Martin Bregman saw the play and became Pacino's manager, a partnership that became fruitful in the years to come, as Bregman encouraged Pacino to do "The Godfather", "Serpico" and "Dog Day Afternoon". “Martin Bregman discovered me off Broadway. I was 26, 25. And he discovered me and became my manager. And that's why I'm here. I owe it to Marty, I really do,” Pacino himself has recently stated about his own career.
Pacino and this production of "The Indian Wants the Bronx" traveled to Italy for a performance at the Festival dei Due Mondi in Spoleto. It was Pacino's first journey to Italy; he later recalled that "performing for an Italian audience was a marvelous experience". Pacino and Clayburgh were cast in "Deadly Circle of Violence", an episode of the ABC television series "N.Y.P.D.", premiering November 12, 1968. Clayburgh at the time was also appearing on the soap opera "Search for Tomorrow", playing the role of Grace Bolton. Her father would send the couple money each month to help.
On February 25, 1969, Pacino made his Broadway debut in Don Petersen's "Does a Tiger Wear a Necktie?" at the Belasco Theater produced by A&P Heir Huntington Hartford. It closed after 39 performances on March 29, 1969, but Pacino received rave reviews and won the Tony Award on April 20, 1969. Pacino continued performing onstage in the 1970s, winning a second Tony Award for "The Basic Training of Pavlo Hummel" and performing the title role in "Richard III". In the 1980s, Pacino again achieved critical success on stage while appearing in David Mamet's "American Buffalo," for which Pacino was nominated for a Drama Desk Award. Since 1990, Pacino's stage work has included revivals of Eugene O'Neill's "Hughie", Oscar Wilde's "Salome" and in 2005 Lyle Kessler's "Orphans".
Pacino made his return to the stage in summer 2010, as Shylock in a Shakespeare in the Park production of "The Merchant of Venice". The acclaimed production moved to Broadway at the Broadhurst Theatre in October, earning US$1 million at the box office in its first week. The performance also garnered him a Tony Award nomination for Best Leading Actor in a Play. In October 2012 Pacino starred in the 30th anniversary Broadway revival of David Mamet's classic play, "Glengarry Glen Ross", which ran through January 20, 2013.
Film career.
Early film career.
Pacino found acting enjoyable and realized he had a gift for it while studying at The Actors Studio. However, his early work was not financially rewarding. After his success on stage, Pacino made his movie debut in 1969 with a brief appearance in "Me, Natalie", an independent film starring Patty Duke. In 1970, Pacino signed with the talent agency Creative Management Associates (CMA).
1970s.
It was the 1971 film "The Panic in Needle Park", in which he played a heroin addict, that brought Pacino to the attention of director Francis Ford Coppola, who cast him as Michael Corleone in the blockbuster Mafia film "The Godfather" (1972). Although several established actors—including Jack Nicholson, Robert Redford, Warren Beatty, and little-known Robert De Niro—also tried out for the part, Coppola selected the relatively unknown Pacino, to the dismay of studio executives.
Pacino was teased on the set because of his short stature. Pacino's performance earned him an Academy Award nomination, and offered a prime example of his early acting style, described by Halliwell's Film Guide as "intense" and "tightly clenched". Pacino boycotted the Academy Award ceremony, insulted at being nominated for the Supporting Acting award, noting that he had more screen time than co-star and Best Actor winner Marlon Brando—who also boycotted the awards, but for unrelated reasons.
In 1973, he co-starred in "Scarecrow", with Gene Hackman, and won the Palme d'Or at the Cannes Film Festival. That same year, Pacino was nominated for an Academy Award for Best Actor after starring in "Serpico", based on the true story of New York City policeman Frank Serpico, who went undercover to expose the corruption of fellow officers. In 1974, Pacino reprised his role as Michael Corleone in the sequel "The Godfather Part II", which was the first sequel to win the Best Picture Oscar; Pacino, meanwhile, was nominated for his third Oscar.
"Newsweek" has described his performance in "The Godfather Part II" as "arguably cinema's greatest portrayal of the hardening of a heart". In 1975, he enjoyed further success with the release of "Dog Day Afternoon", based on the true story of bank robber John Wojtowicz. It was directed by Sidney Lumet, who had directed him in "Serpico" a few years earlier, and Pacino was again nominated for Best Actor.
In 1977, Pacino starred as a race-car driver in "Bobby Deerfield", directed by Sydney Pollack, and received a Golden Globe nomination for Best Actor – Motion Picture Drama for his portrayal of the title role. His next film was the courtroom drama "...And Justice for All", which again saw Pacino lauded by critics for his wide range of acting abilities, and nominated for the Best Actor Oscar for a fourth time. However he lost out that year to Dustin Hoffman in "Kramer vs. Kramer"—a role that Pacino had declined.
During the 1970s, Pacino had four Oscar nominations for Best Actor, for his performances in "Serpico", "The Godfather Part II", "Dog Day Afternoon", and "...And Justice for All".
1980s.
Pacino's career slumped in the early 1980s; his appearances in the controversial "Cruising", a film that provoked protests from New York's gay community, and the comedy-drama "Author! Author!", were critically panned. However, 1983's "Scarface", directed by Brian De Palma, proved to be a career highlight and a defining role. Upon its initial release, the film was critically panned due to violent content, but later received critical acclaim. The film did well at the box office, grossing over US $45 million domestically. Pacino earned a Golden Globe nomination for his role as Cuban drug lord Tony Montana.
In 1985, Pacino worked on his personal project, "The Local Stigmatic", a 1969 Off Broadway play by the English writer Heathcote Williams. He starred in the play, remounting it with director David Wheeler and the Theater Company of Boston in a 50-minute film version. The film was not released theatrically, but was later released as part of the "Pacino: An Actor's Vision" box set in 2007.
His 1985 film "Revolution" about a fur trapper during the American Revolutionary War, was a commercial and critical failure, which Pacino blamed on a rushed production, resulting in a four-year hiatus from films. At this time Pacino returned to the stage. He mounted workshop productions of "Crystal Clear", "National Anthems" and other plays; he appeared in "Julius Caesar" in 1988 in producer Joseph Papp's New York Shakespeare Festival. Pacino remarked on his hiatus from film: "I remember back when everything was happening, '74, '75, doing "The Resistible Rise of Arturo Ui" on stage and reading that the reason I'd gone back to the stage was that my movie career was waning! That's been the kind of ethos, the way in which theater's perceived, unfortunately." Pacino returned to film in 1989's "Sea of Love", when he portrayed a detective hunting a serial killer who finds victims through the singles column in a newspaper. The film earned solid reviews.
1990s.
Pacino received an Academy Award nomination for playing Big Boy Caprice in the box office hit "Dick Tracy" in 1990, of which critic Roger Ebert described Pacino as "the scene-stealer". Later in the year he followed this up in a return to one of his most famous characters, Michael Corleone, in "The Godfather Part III" (1990). The film received mixed reviews, and had problems in pre-production due to script rewrites and the withdrawal of actors shortly before production.
In 1991, Pacino starred in "Frankie and Johnny" with Michelle Pfeiffer, who co-starred with Pacino in "Scarface". Pacino portrays a recently paroled cook who begins a relationship with a waitress (Pfeiffer) in the diner where they work. It was adapted by Terrence McNally from his own Off-Broadway play "Frankie and Johnny in the Clair de Lune" (1987), that featured Kenneth Welsh and Kathy Bates. The film received mixed reviews, although Pacino later said he enjoyed playing the part. Janet Maslin in "The New York Times" wrote, "Mr. Pacino has not been this uncomplicatedly appealing since his "Dog Day Afternoon" days, and he makes Johnny's endless enterprise in wooing Frankie a delight. His scenes alone with Ms. Pfeiffer have a precision and honesty that keep the film's maudlin aspects at bay."
In 1992, Pacino won the Academy Award for Best Actor, for his portrayal of the blind U.S. Army Lieutenant Colonel Frank Slade in Martin Brest's "Scent of a Woman". That year, he was also nominated for Best Supporting Actor for "Glengarry Glen Ross", making Pacino the first male actor ever to receive two acting nominations for two movies in the same year, and to win for the lead role.
Pacino starred alongside Sean Penn in the crime drama "Carlito's Way" in 1993, in which he portrayed a gangster released from prison with the help of his lawyer (Penn) and vows to go straight. Pacino starred in Michael Mann's "Heat" (1995), in which he and Robert De Niro appeared on-screen together for the first time (though both Pacino and De Niro starred in "The Godfather Part II", they did not share any scenes).
In 1996, Pacino starred in his theatrical docudrama "Looking for Richard", a performance of selected scenes of Shakespeare's "Richard III" and a broader examination of Shakespeare's continuing role and relevance in popular culture. The cast brought together for the performance included Alec Baldwin, Kevin Spacey, and Winona Ryder. Pacino played Satan in the supernatural thriller "The Devil's Advocate" (1997) which co-starred Keanu Reeves. The film was a success at the box office, taking US $150 million worldwide. Roger Ebert wrote in the "Chicago Sun-Times", "The satanic character is played by Pacino with relish bordering on glee."
In "Donnie Brasco", Pacino played mafia gangster "Lefty", the true story of undercover FBI agent Donnie Brasco (Johnny Depp) and his work in bringing down the mafia from the inside. Pacino also starred as real life "60 Minutes" producer Lowell Bergman in the multi-Oscar nominated "The Insider" opposite Russell Crowe, before starring in Oliver Stone's "Any Given Sunday" in 1999.
2000s.
Pacino has not received another Academy Award nomination since winning for "Scent of a Woman", but has won three Golden Globes since the year 2000, the first being the Cecil B. DeMille Award in 2001 for lifetime achievement in motion pictures.
In 2000, Pacino released a low-budget film adaptation of Ira Lewis' play "Chinese Coffee" to film festivals. Shot almost exclusively as a one-on-one conversation between two main characters, the project took nearly three years to complete and was funded entirely by Pacino. "Chinese Coffee" was included with Pacino's two other rare films he was involved in producing, "The Local Stigmatic" and "Looking for Richard", on a special DVD box set titled "Pacino: An Actor's Vision", which was released in 2007. Pacino produced prologues and epilogues for the discs containing the films.
Pacino turned down an offer to reprise his role as Michael Corleone in the computer game version of '. As a result, Electronic Arts was not permitted to use Pacino's likeness or voice in the game, although his character does appear in it. He did allow his likeness to appear in the video game adaptation of 1983's "Scarface", quasi-sequel titled '.
Director Christopher Nolan worked with Pacino on "Insomnia", a remake of the Norwegian film of the same name, co-starring Robin Williams. "Newsweek" stated that "he [Pacino] can play small as rivetingly as he can play big, that he can implode as well as explode". The film and Pacino's performance were well received, gaining a favorable rating of 93 percent on the review aggregation website Rotten Tomatoes. The film did moderately well at the box office, taking in $113 million worldwide. His next film, "S1m0ne", did not gain much critical praise or box office success.
He played a publicist in "People I Know", a small film that received little attention despite Pacino's well-received performance. Rarely taking a supporting role since his commercial breakthrough, he accepted a small part in the box office flop "Gigli", in 2003, as a favor to director Martin Brest. "The Recruit", released in 2003, featured Pacino as a CIA recruiter and co-stars Colin Farrell. The film received mostly negative reviews, described by Pacino as something he "personally couldn't follow". Pacino next starred as lawyer Roy Cohn in the 2003 HBO miniseries "Angels in America", an adaptation of Tony Kushner's Pulitzer Prize winning play of the same name. For this performance, Pacino won his third Golden Globe, for Best Performance by an Actor, in 2004.
Pacino starred as Shylock in Michael Radford's 2004 film adaptation of "The Merchant of Venice", choosing to bring compassion and depth to a character traditionally played as a villainous caricature. In "Two for the Money", Pacino portrays a sports gambling agent and mentor for Matthew McConaughey, alongside Rene Russo. The film was released on October 8, 2005, to mixed reviews. Desson Thomson wrote in "The Washington Post", "Al Pacino has played the mentor so many times, he ought to get a kingmaker's award ... the fight between good and evil feels fixed in favor of Hollywood redemption."
On October 20, 2006, the American Film Institute named Pacino the recipient of the 35th AFI Life Achievement Award. On November 22, 2006, the University Philosophical Society of Trinity College, Dublin awarded Pacino the Honorary Patronage of the Society.
Pacino played a spoof role in Steven Soderbergh's "Ocean's Thirteen", alongside George Clooney, Brad Pitt, Matt Damon, Elliott Gould and Andy García, as the villain Willy Bank, a casino tycoon targeted by Danny Ocean and his crew. The film received generally favorable reviews.
"88 Minutes" was released on April 18, 2008, in the United States, after having been released in various other countries in 2007. The film co-starred Alicia Witt and was critically panned, although critics found fault with the plot, and not Pacino's acting. In "Righteous Kill", Pacino and Robert De Niro co-star as New York detectives searching for a serial killer. The film was released to theaters on September 12, 2008. While it was an anticipated return for the two stars, it was not well received by critics. Lou Lumenick of the "New York Post" gave "Righteous Kill" one star out of four, saying: "Al Pacino and Robert De Niro collect bloated paychecks with intent to bore in "Righteous Kill", a slow-moving, ridiculous police thriller that would have been shipped straight to the remainder bin at Blockbuster if it starred anyone else."
2010s.
Pacino played Dr. Jack Kevorkian in an HBO Films biopic entitled "You Don't Know Jack", which premiered April 2010. The film is about the life and work of the physician-assisted suicide advocate. The performance earned Pacino his second Emmy Award for lead actor and his fourth Golden Globe award.
It was announced in May 2011 that Pacino was to be honored with the "Glory to the Film-maker" award at the 68th Venice International Film Festival. The award was presented ahead of the premiere of his film "Wilde Salome", the third film Pacino has directed. Pacino, who plays the role of Herod in the film, describes it as his "most personal project ever".
The United States premiere of "Wilde Salomé" took place on the evening of March 21, 2012, before a full house at the 1,400-seat Castro Theatre in San Francisco's Castro District. Marking the 130th anniversary of Oscar Wilde's visit to San Francisco, the event was a benefit for the GLBT Historical Society.
Pacino most recently starred in a 2013 HBO biographical picture about record producer Phil Spector's murder trial, titled "Phil Spector".
Pacino and Robert De Niro are reportedly set to star in the upcoming project "The Irishman", to be directed by Martin Scorsese and co-star Joe Pesci. It was announced in January 2013 that Pacino will play the late former Penn State University football coach Joe Paterno in the movie tentatively titled "Happy Valley" and based on a 2012 biography of Paterno by sportswriter Joe Posnanski.
Personal life.
Although he has never married, Pacino has three children. The eldest, Julie Marie (born 1989), is his daughter with acting coach Jan Tarrant. He also has twins, son Anton James and daughter Olivia Rose (born January 25, 2001), with actress Beverly D'Angelo, with whom he had a relationship from 1996 until 2003. Pacino had a relationship with Diane Keaton, his co-star in the "Godfather" trilogy. The on-again, off-again relationship ended following the filming of "The Godfather Part III". He has had relationships with Tuesday Weld, Jill Clayburgh, Marthe Keller, Kathleen Quinlan and Lyndall Hobbs.
The Internal Revenue Service filed a tax lien against Pacino, claiming he owes the government a total of $188,000 for 2008 and 2009. A representative for Pacino blamed his former business manager Kenneth Starr for the discrepancy.
Awards and nominations.
Pacino has been nominated and has won many awards during his acting career, including eight Oscar nominations (winning one), 15 Golden Globe nominations (winning four), five BAFTA nominations (winning two), two Primetime Emmy Awards for his work on television, and two Tony Awards for his stage work. In 2007, the American Film Institute awarded Pacino with a lifetime achievement award and, in 2003, British television viewers voted Pacino as the greatest film star of all time in a poll for Channel 4.

</doc>
<doc id="41907" url="http://en.wikipedia.org/wiki?curid=41907" title="Dick Tracy">
Dick Tracy

Dick Tracy is a comic strip featuring Dick Tracy (originally Plainclothes Tracy), a square-jawed, hard-hitting, fast-shooting, intelligent police detective. Created by Chester Gould, the strip made its debut on October 4, 1931, in the "Detroit Mirror". It was distributed by the Chicago Tribune New York News Syndicate. Gould wrote and drew the strip until 1977. Since that time, various artists and writers have continued the strip, which still runs in newspapers today. Dick Tracy has also been the hero in a number of films, notably one in which Warren Beatty played the crime fighter.
Comic strip.
Characters and story.
Although stories often end in gunfights, Tracy uses forensic science, advanced gadgetry, and wits, in an early example of the police procedural mystery story. Stories typically follow a criminal committing a crime and Tracy's relentless pursuit of the criminal. The strip's most popular villain was Flattop Jones, a freelance hitman hired by black marketeers to murder Tracy. When Flattop was killed, fans went into public mourning, and the Flattop Story was reprinted in DC's series of Oversize Comic Reprints in the 1970s. Reflecting film noir, the villains' small crimes led to bigger, out of control situations. Similarly, innocent witnesses were frequently killed, and Tracy's paramour Tess Trueheart was often endangered by the villains. As the story progressed, Tracy adopted an orphan under the name, Dick Tracy Jr., or "Junior" for short, who appeared in investigations until becoming a police forensic artist in his father's precinct, and cultivated a professional partner, the ex-steel worker Pat Patton, who gradually became a detective of skill and courage enough to satisfy Tracy's requirements.
Tracy characters were often caricatures of celebrities. Most famous of these was Breathless Mahoney, modeled after Lauren Bacall. Likewise, B.O. Plenty was inspired by George "Gabby" Hayes (with perhaps a nod to Al St. John also), Vitamin Flintheart by John Barrymore and Spike Dyke by Spike Jones. Others include villains like Rughead (Robert Montgomery), Oodles (Jackie Gleason) and Mumbles (Bing Crosby). Gould even parodied himself as the out-of-shape Pear Shape!
Evolution of the strip.
On January 13, 1946, The 2-Way Wrist Radio, worn as a wristwatch by Tracy and members of the police force, became one of the strip's most immediately recognizable icons, and may have inspired later smartwatches. The 2-Way Wrist Radio was upgraded to a 2-Way Wrist TV in 1964. This development also led to the introduction of an important supporting character, Diet Smith, an eccentric industrialist who financed the development of this equipment. In a conspicuous coincidence, the idea of a radio built into a wrist watch played an important role in the story line of "Superman – The Talking Cat" broadcast on the Mutual Broadcasting System on January 9 through 28, 1946 (episodes 878 through 891).
In late 1948, a botched security detail led to the death of the semi-regular character Brilliant, the blind inventor of the 2-Way Wrist Radio (among other devices); whereupon Chief Brandon, Dick Tracy's superior on the police force and a presence in the strip since 1931, resigned in shame, and Pat Patton, heretofore Tracy's buffoonish partner, was promoted to police chief in Brandon's place. To take Patton's place as Tracy's sidekick, a new character, Sam Catchem, was introduced.
The 1950s.
Gould introduced topical story lines about television, juvenile delinquency, graft, organized crime, and other developments in American life during the 1950s; and elements of soap opera depicted Dick, Tess, and Junior (along with the Tracys' baby daughter, Bonnie Braids), at home as a family. Depictions of family life alternated with the story's crime drama, as in the kidnapping of Bonnie Braids by fugitive Crewy Lou, or Junior's girlfriend, Model, being accidentally killed by her brother.
Gould incurred some controversy when he had Tracy, on a police officer's salary, live in an unaccountably ostentatious manner, and responded with a story wherein Tracy was accused of corruption and had to explain the origin of his possessions in detail. In his book-length examination of the strip, "Dick Tracy – The Official Biography", Jay Maeder suggested that Gould's critics were unsatisfied by his explanation. Nevertheless, the controversy eventually faded, and the cartoonist reduced exposure to Tracy's home life.
With the exception of The Big Boy, the strip's first villain, a fictionalized version of Al Capone, and a few others, Tracy's cases incriminated independent operators rather than organized crime. In the 1950s, after events like the Kefauver Hearings, Tracy opposed a series of big-time mobsters such as the King, George "Mr. Crime" Alpha, Odds Zonn, and Willie "The Fifth" Millyun. As Tess faded into the background, Tracy assumed as assistant, the rookie policewoman Lizz Worthington.
From 1956 to 1964, the "Dick Tracy" Sunday page was accompanied by a topper humor strip called "The Gravies" and drawn by Gould and his assistants.
Space period.
As technology progressed, the methods Tracy and the police used to track and capture criminals took the form of increasingly fanciful atomic-powered gadgets developed by Diet Smith Industries. This eventually led to the 1960s advent of the Space Coupe, a spacecraft with a magnetic propulsion system. This marked the beginning of the strip's "Space Period," which saw Tracy and friends having adventures on the Moon and meeting Moon Maid, the daughter of the leader of a race of humanoid people living in "Moon Valley" in 1964. After an eventual sharing of technological information, Moon technology became standard issue on Tracy's police force, including air cars, flying cylindrical vehicles. The villains became even more exaggerated in power, resulting in an escalating series of stories that no longer resembled the urban crime drama roots of the strip. During this period, Tracy met famed cartoonist Chet Jade, creator of the comic strip "Sawdust", in which the only characters are talking dots.
One of the new characters, Mr. Intro, was only manifested as a disembodied voice. His goal was world domination in the vein of a James Bond villain. Tracy eventually used an atomic laser beam to annihilate Intro and his island base.
Junior married Moon Maid in October 1964. Their daughter, Honey Moon Tracy, had antennae and magnetic hands. In the spring of 1969, Tracy was offered the post of Chief of Police in Moon Valley. However, Tracy ended up back on Earth when the Apollo 11 mission in 1969 showed that the moon was barren of all life. Many of the accoutrements of the space period stories, such as the Space Coupe and much of the high-tech gadgetry, remained for many years afterward. Moon Maid receded from the storyline.
The stories of this period took an increasingly condemnatory tone pertaining to contemporary court decisions concerning the rights of the accused, which often involved Tracy being frustrated by legal technicalities. For example, having caught a gang of diamond thieves red-handed, Tracy was forced to let them walk because he could not "prove", beyond reasonable doubt, that the diamonds were stolen. As he saw the thieves get off without penalty, Tracy was heard to grumble, "Yes, under today's interpretation of the laws, it seems it's the police who are handcuffed!"
1970s.
In the 1970s, Gould modernized Tracy by giving him a longer hair style and mustache, and added a hippie sidekick, Groovy Grove. Groovy's first appearance in print, as it happened, occurred during the same week as the Kent State shootings. Groovy remained with the strip, off and on until his death in 1984.
Shortly before his retirement, Gould drew a strip in which Sam, Lizz, and Groovy held Tracy down to shave off his mustache.
At this time, the standard publication size and space of newspaper comics was sharply reduced; for example, the "Dick Tracy" Sunday strip, which had traditionally been a full-page episode containing 12 panels, was cut in size to a half-page format that offered, at most, eight panels—these new restrictions created challenges for all comic artists.
In one of Max Allan Collins' first stories as the strip's writer, the gangster known as "Big Boy", whose gang members had killed Tess Trueheart's father years ago, learned that he was dying and had less than a year to live. Big Boy, still seeking revenge on the plainclothesman who sent him up the river, wanted to live just long enough to see Tracy's death. He put out an open contract on Tracy's head worth one million dollars, knowing that every small-time hood in the City would take a crack at the famous cop for that amount of money. One of the would-be collectors rigged Tracy's car to explode, but inadvertently killed Moon Maid instead of Tracy in the explosion. A funeral strip for Moon Maid explicitly stated that this officially severed all ties between Earth and the Moon in the strip, thus eliminating the last remnants of the Space Period. Honey Moon received a new hairstyle that covered her antennae, and was ultimately phased out of the strip. Junior later married Sparkle Plenty (the daughter of B.O. and Gravel Gertie Plenty), and had a daughter named Sparkle Plenty Jr. In the 1990s, Tracy's own son, Joseph Flintheart Tracy, took on a role similar to Junior's in the earlier strips. During the late 1970s the strip was thought to have been drawn by a few other artists due to an ailing Gould.
Plenty family.
The Plenty family was a group of goofy redneck yokels headed by the former villain, Bob Oscar ("B.O."), along with Gertrude ("Gravel Gertie") Plenty. Gravel Gertie was introduced as the unwitting dupe (accessory) of the villain, The Brow, who was on the run from Dick Tracy. The family provided a humorous counterpoint to Tracy's adventures. The Plenty sub-story was decades long, and saw Sparkle Plenty grow from an infant to a young married lady, eventually becoming a beautiful fashion model. Sparkle Plenty's May 30, 1947, birth became a significant mainstream media event, with spinoff merchandising and magazine coverage.
The Plenty family appeared with Tracy in a story that occurred in a bank, where "B.O." found a way to prevent thieves from snatching an envelope of money from a counter.
In the April 24, 2011 strip, B.O. and Gertie had a second child, Attitude, a boy who is as ugly as Sparkle is beautiful. His face has yet to be shown.
Crimestoppers' Textbook.
Beginning in the early 1950s, the Sunday strip included a frame devoted to a page from the "Crimestoppers' Textbook", a series of handy illustrated hints for the amateur crime-fighter. This was named after a short-lived youth group seen in the strip during the late 1940s, led by Junior Tracy, called "Dick Tracy's Crimestoppers." This feature ended when Gould retired from the strip in 1977, but Max Allan Collins reinstated it, and it is still part of the comic strip. After Gould's retirement, Collins initially replaced the Textbook with "Dick Tracy's Rogues Gallery," a salute to memorable "Tracy" villains of the past.
Later years.
Chester Gould retired from comics in 1977; his last "Dick Tracy" strip appeared in print on Sunday, December 25 of that year. The following Monday, "Dick Tracy" was taken over by Max Allan Collins and longtime Gould assistant Rick Fletcher. Gould's name remained in the byline for a few years after his retirement as a story consultant.
Collins wrote the 1978 death of Moon Maid, and removed other Gould creations of the 1960s and 1970s (including Groovy Grove, who was gravely wounded in the line of duty and later died in the hospital; Lizz married him before his death). Collins took a generally less cynical view of the justice system than Gould—Tracy came to accept its limitations and requirements as a normal part of the process he could manage. Extreme technology, such as the Space Coupe, were phased out in favor of more realistic advanced tools such as the 2-Way Wrist Computer in 1987.
New semi-regular characters introduced by Collins and Fletcher included: Dr. Will Carver, a plastic surgeon with underworld ties who often worked on known felons; Wendy Wichel, a smarmy newspaper reporter/editorialist with a strong anti-Tracy bias in her articles; and Lee Ebony, an African-American female detective. Vitamin Flintheart, the aged ham actor created by Gould in 1944, who had not been seen in the strip for almost three decades, reappeared occasionally as a comic-relief figure. The Plenty family (B.O., Gravel Gertie, and Sparkle) were also brought back as semi-regulars as well; following the death of Moon Maid, Junior and Sparkle were married, and soon gave birth to their own daughter, Sparkle Plenty, Jr.
Original villains seen during this period included Angeltop (revenge-seeking, psychopathic daughter of the slain Flattop), Torcher (whose scheme was arson-for-profit), and Splitscreen (a video pirate). Collins brought back at least one "classic" Gould villain, or revenge-seeking family member, per year. The revived Gould villains were often provided with full names, and marriages, children, and other family connections were developed, bringing more humanity to many of the originally grotesque brutes. "Flattop", particularly, had a number of relatives, all with his characteristic head structure and facial attributes, who one by one turned up to avenge their ancestor on Tracy.
Rick Fletcher died in 1983 and was succeeded by editorial cartoonist Dick Locher, who had assisted Gould on the strip in the late 1950s and early 1960s. Locher was assisted by his son John, who died in 1986.
In 1992, following a financial reorganization of their comic strip holdings, Max Allan Collins was fired from the strip, and "Tribune" staff writer and columnist Mike Kilian took over the writing. Kilian was paid less than half of what Collins was making per strip , but continued until his death on October 27, 2005. Locher was both author and artist for over three years, beginning on January 9, 2006. On March 16, 2009, Jim Brozman began collaborating with Locher, taking over the drawing duties while Locher continued to write the strip.
In 2005, Tracy was a guest at Blondie and Dagwood's 75th anniversary party in the comic strip "Blondie". Later, Dick Tracy appeared in the comic strip "Gasoline Alley".
On January 19, 2011, Tribune Media Services announced that Locher was retiring from the strip and handing the reins to artist Joe Staton and writer Mike Curtis. The new creative team has previously worked together on "Scooby Doo", "Richie Rich", and" Casper the Friendly Ghost". Their first Dick Tracy strip was published March 14, 2011. Staton and Curtis are assisted by Shelley Pleger, who inks and letters Staton's drawings, along with Shane Fisher, who provides the coloring on the Sunday strips, and Chicago-area policeman Jim Doherty, who provides "Crimestopper" captions for the Sunday strips, and acts as the feature's technical advisor. Doherty also introduced a new feature, "Tracy's Hall of Fame" (which replaces the "Crimestopper" panel approximately once each month), in which a real-life police officer is profiled and honored.
They went on to reintroduce many of the characters of the forties through the sixties, including Mr. Crime and a reformed Mole, while introducing more deformed and grotesque villains like Abner Kadaver, Panda, and The Jumbler. They have also, starting in early 2013, brought back all the gadgets and plot elements of the 1960s space era. They have also done crossovers, with cameos from "Popeye" and "Brenda Starr", and a long sequence involving "Little Orphan Annie".
Awards and honors.
Chester Gould won the Reuben Award for the strip in 1959 and 1977.
The Mystery Writers of America honored Gould and his work with a Special Edgar Award in 1980. This was the first time MWA ever honored a comic strip.
In 1995, the strip was one of 20 included in the Comic Strip Classics series of commemorative postage stamps and postcards.
On May 2, 2011, the Tennessee Senate passed Resolution 30, congratulating Mike Curtis and Joe Staton on their professional accomplishments, including "Dick Tracy".
On September 7, 2013, at the Baltimore Comics Convention, "Dick Tracy" was awarded the Harvey in the "Best Syndicated Strip or Panel" category. "Tracy" was simultaneously the oldest continually running strip, and the first adventure strip ever to win the Harvey Award in this category. On September 6, 2014, "Tracy" was awarded a second Harvey Award in the newspaper strip category, becoming one of only three strips to win in this category in consecutive years.
Other media depictions.
Radio.
"Dick Tracy" had a long run on radio, from 1934 weekdays on NBC's New England stations to the ABC network in 1948. Bob Burlen was the first radio Tracy in 1934, and others heard in the role during the 1930s and 1940s were Barry Thomson, Ned Wever and Matt Crowley. The early shows all had 15-minute episodes.
On CBS, with Sterling Products as sponsor, the serial aired four times a week from February 4, 1935 to July 11, 1935, moving to Mutual from September 30, 1935 to March 24, 1937 with Bill McClintock doing the sound effects. NBC's weekday afternoon run from January 3, 1938 to April 28, 1939 had sound effects by Keene Crockett and was sponsored by Quaker Oats, which brought "Dick Tracy" into primetime (Saturdays at 7 pm and, briefly, Mondays at 8 pm) with 30-minute episodes from April 29, 1939 to September 30, 1939. The series returned to 15-minute episodes on the ABC Blue Network from March 15, 1943 to July 16, 1948, sponsored by Tootsie Roll, which used the music theme of "Toot Toot, Tootsie" for its 30-minute Saturday ABC series from October 6, 1945 to June 1, 1946. Sound effects on ABC were supplied by Walt McDonough and Al Finelli.
Directors of the series included Mitchell Grayson, Charles Powers and Bob White. Cast members at various times included Walter Kinsella as Pat Patton, Helen Lewis as Tess Trueheart and Andy Donnelly and Jackie Kelk as Junior Tracy. Announcers were Ed Herlihy and Dan Seymour.
On July 8, 1945, during a New York newspaper deliverers' strike, New York mayor Fiorello H. La Guardia read a complete "Dick Tracy" strip over the radio.
The beginning of the May 1, 1945 episode ("The Case of the Empty Safe") was interrupted on the Blue Network for a "special news flash" relating that Adolf Hitler had "died of a stroke." Copies of this episode, complete with the mistaken news flash—Hitler had committed suicide the day before, not died of a stroke—still exist today. (See: Death of Adolf Hitler.)
On Feb.15, 1945, Command Performance broadcast the musical comedy Dick Tracy in B-Flat with Bing Crosby as Tracy, Bob Hope as Flattop, Dinah Shore as Tess Trueheart, among the cast. Dick Tracy's wedding is repeatedly interrupted as Tracy chases after one villain after another. In the strip, his marriage wasn't until 1950 and his honeymoon was disrupted by his going after Wormy.
Recordings.
Jim Ameche portrayed Tracy in a two-record set recorded by Mercury Records in 1947. The record sleeves were illustrated with Sunday strips reprinted in black-and-white for children to color.
Film serials.
Dick Tracy made his film debut in "Dick Tracy" (1937), a 15-chapter movie serial by Republic Pictures starring Ralph Byrd. The Spider Gang was on the loose, tired of Dick Tracy's cunning skills. Through the 15-chapter serial, 15 different cases were solved, all plots by the Spider Gang. Dick Tracy was also in search for his missing brother, Gordon Tracy (Carleton Young). The Dick Tracy character proved very popular, and a second serial, "Dick Tracy Returns", appeared in 1938 (reissued in 1948). "Dick Tracy's G-Men" was released in 1939 (reissued in 1955). The last was "Dick Tracy vs. Crime Inc." in 1941 (reissued as "Dick Tracy vs. the Phantom Empire" in 1952).
The sequels were produced under an interpretation of the contract for the first "Dick Tracy" serial, which gave license for "a series or serial". As a result, Chester Gould received no further money for the sequel serials.
In these serials, Dick Tracy is portrayed as an FBI agent, or "G-Man", based in California, rather than as a detective in the police force of a Midwestern city resembling Chicago, and, aside from himself and Junior, no characters from the strip appear in any of the four films.
However, comic relief sidekick "Mike McGurk" bears some resemblance to Tracy's partner from the strip, Pat Patton; Tracy's secretary, Gwen Andrews (played by several actresses in the course of the series, including Jennifer Jones under a variation of her real name, Phyllis Isley), provides the same kind of feminine interest as Tess Trueheart; and FBI Director Clive Anderson (Francis X. Bushman and others) is the same kind of avuncular superior as Chief Brandon.
The first serial, "Dick Tracy", is now in the public domain.
Early feature films.
Six years after the release of the final Republic serial, Dick Tracy headlined four feature films, produced by RKO Radio Pictures. "Dick Tracy" (aka "Dick Tracy, Detective") (1945) was followed by "Dick Tracy vs. Cueball" in 1946, both with Morgan Conway as Tracy. Ralph Byrd returned for the last two features, both released in 1947: "Dick Tracy's Dilemma" and "Dick Tracy Meets Gruesome". "Gruesome" is probably the best known of the four, with the villain portrayed by Boris Karloff. All four movies had many of the visual features associated with film noir: dramatic, shadowy photographic compositions, with many exterior scenes filmed at night (at the RKO Encino movie ranch). Lyle Latell co-starred in all four films as Pat Patton. Anne Jeffreys played Tess Trueheart in the first two, succeeded by Kay Christopher and finally Anne Gwynne; Ian Keith joined the cast as the actor Vitamin Flintheart for two films; Joseph Crehan played Chief Brandon. RKO stocked the films with familiar faces, creating a veritable rogues' gallery of characters: Mike Mazurki as Splitface, Dick Wessel as Cueball, Esther Howard as Filthy Flora, Jack Lambert as hook-handed villain The Claw; baldheaded, pop-eyed Milton Parsons, mild-mannered Byron Foulger, dangerous Trevor Bardette, pockmarked, gently sinister Skelton Knaggs.
Television.
The strip has had limited exposure on television with one early live-action series, two animated series, one unsold pilot that was never picked up, and a proposed TV series currently held up in litigation.
First live-action series.
Ralph Byrd, who had played the square-jawed sleuth in all four Republic movie serials, and in two of the RKO feature-length films, reprised his role in a short-lived live-action "Dick Tracy" series that ran on ABC from 1950 to 1951. Additional episodes intended for first-run syndication continued to be produced into 1952. Produced by P. K. Palmer, who also wrote many of the scripts, the series often featured Gould-created villains such as Flattop, Shaky, the Mole, Breathless Mahoney, Heels Beals, and Influence, all of whom appeared on film for the first time on this series. Other cast members included Joe Devlin as Sam Catchem, Angela Greene as Tess Tracy (née Trueheart), Martin Dean as Junior, and Pierre Watkin as Chief Patton. Criticized for its violence, the series remained popular. It ended, not in response to criticism, but because of Byrd's unexpected, premature death in 1952. The series was filmed on a low budget, with many long hours and a rushed shooting schedule. Many episodes of this series have been released on various Public Domain TV Detective DVD sets.
Animated cartoons.
In the first cartoon series, produced from 1960 to 1961 by UPA, Tracy employed a series of cartoon-like subordinate flatfoots to fight crime each week, contacting them on his two-way wrist radio. Everett Sloane voiced Tracy and supporting characters and villains were voiced by Jerry Hausner, Mel Blanc, Benny Rubin, Johnny Coons, Paul Frees and others. These subordinates included "Go-Go" Gomez, Joe Jitsu, Hemlock Holmes and Heap O'Calorie. 130 five-minute cartoons were designed and packaged for syndication, usually intended for local children's shows.
Since UPA was also the production company behind the Mr. Magoo cartoons, it was possible for them to arrange a meeting between Tracy and Magoo in a 1965 episode of the season-long TV series "The Famous Adventures of Mr. Magoo". In that episode, "Dick Tracy and the Mob," Tracy persuades Magoo (a well-known actor in the context of the "Famous Adventures" series) to impersonate an international hit man whom he resembles, and infiltrate a gang of criminals made up of Flattop, Pruneface, Itchy, Mumbles and others. Unlike the earlier animated Tracy shorts, this longer episode was played relatively straight, with Tracy getting much more screen time. Pitting Tracy against a coalition of several of his foes would be adopted more than two decades later in the 1990 film mentioned below.
A second cartoon series, produced in 1971, was a feature in "Archie's TV Funnies", produced by Filmation, which adhered more closely to the comic strip although hampered by cruder animation, typical of the studio's production standards, than the UPA shorts.
Live-action television pilot.
In 1967, William Dozier, the producer responsible for the 1966 "Batman" television series, produced a pilot for a live-action Dick Tracy series, starring Ray MacDonnell in the title role. While the quality of the pilot ("The Plot To Kill NATO", featuring "Special Guest Villain" Victor Buono as 'Mr. Memory') was slightly above-average, the series was not purchased by either ABC or NBC as ratings for the "Batman" series were dropping, and a similar series featuring "The Green Hornet" had recently flopped. To the networks, the "Hero Camp" or "Batmania" craze was dying, and they chose not to take a risk on another series.
The pilot is notable for the non-appearance of the future Jan Brady (Eve Plumb) as Bonnie Braids. Although cast in the role, she only appears in the title credits at the opening of the show.
1990 film.
In 1990, Warren Beatty directed and starred as the title character in a live action all-star cast film, along with Al Pacino, Dustin Hoffman, and Madonna.
Comic books.
Tracy made his first comic book appearance in 1936 as one of the features included in the first issue of Dell's "Popular Comics". As would be the case with most Tracy comic book appearances, these would be reprints from the newspaper strip, reconfigured to fit the pages of a comic book. Tracy would remain a regular feature in "Popular Comics" through the publication's 21st issue.
The first comic book to feature Tracy exclusively was the "Dick Tracy Feature Book", published in May 1937 by David McKay Publications. McKay's Feature Books were magazines that rotated several popular characters from comics strips through 1938. Three more of McKay's Feature Books starred Tracy in the following months.
In 1939, Dell started a comic magazine series called "Black and White Comics," essentially identical to McKay's "Feature Books." Six of the 15 issues featured Tracy. In 1941, Dell's "Black and White" series was replaced by the "Large Feature Books," the third issue of which featured Tracy. As with the McKay series, the Dell "Black and White" and "Large Feature" series were abridged reprints of the strip.
In 1938, "Tracy" became one of several regular newspaper strips featured in Dell's regular monthly "Super Comics", remaining a regular part of that publication until 1948. In 1939, "Tracy" was the sole feature in the very first issue of Dell's "Four-Color Comics", which put out over 1300 issues starring hundreds of characters between 1939 and 1962. Tracy was featured in seven more "Four-Color" issues throughout the 1940s.
Tracy was frequently featured in comic books used as promotional items by various companies. In 1947, for example, Sig Feuchtwanger produced a comic book that was a giveaway prize in boxes of Quaker Puffed Wheat cereal, sponsor of the popular "Dick Tracy" radio series.
In January 1948, Dell began the first regular "Dick Tracy" comic book series, "Dick Tracy Monthly". This series ultimately ran for 145 issues, the first 24 of which were published by Dell, after which it was picked up by Harvey Comics. Continuing the same numbering, Harvey published the series until 1961. As with most previous Tracy comic book incarnations, these were, with the exception of the last few Dell issues which featured original material, slightly abridged and reconfigured reprints of the newspaper strips.
"Dick Tracy" was revived in 1986 by Blackthorne Publishing and ran for 99 issues. Disney produced a series of three issues as a tie-in for their 1990 film. This miniseries, "True Hearts and Tommy Guns", was drawn by Kyle Baker and edited by Len Wein. The third issue was a direct adaptation of the film.
Recent events.
Media outlets reported a legal battle being waged over rights to the Dick Tracy character. Warren Beatty announced plans to make a sequel to his 1990 movie. At the same time, television producers announced plans for a new "Dick Tracy" TV series. Both sides claimed that they were the legal owners of the rights to Dick Tracy. In May 2005, Beatty sued the Tribune Company, claiming he has owned the rights to the Dick Tracy character since 1985. Pressure from Beatty led to the cancellation of a proposed collaboration between artist Mike Oeming and writer Brian Bendis on a new serialized Dick Tracy comic.
The lawsuit was resolved in Beatty's favor, with a US District judge ruling that Beatty did everything contractually required of him to keep the rights to the character.
Books.
Over the years, many reprints of "Dick Tracy" newspaper strips have been published. Beginning in 2007, IDW Publishing reprinted the complete strip in hardcover volumes.
Other collections include:
Other editions:
Licensed products.
In the 1960s, Aurora produced a plastic model kit of Dick Tracy sliding down a fire escape ladder into an alley, in hot pursuit with gun drawn. A Dick Tracy Space Coupe model came next.
Also in the market were Mattel's Dick Tracy range of toy weapons.
In 1990, Playmates Toys released a line of action figures called Dick Tracy: Coppers and Gangsters to coincide with the Dick Tracy movie. The figures were 5" tall, stylized with exaggerated comicy looks and came with lots of accessories. Two figures in the line had limited availability; Steve the Tramp (called "The Tramp" on the package front) was pulled from the assortment after complaints of portrayal of a homeless person as a criminal. The figure of "The Blank" was added to the assortment well after the film's release to keep the secret of the identity of the character. As a result, only limited quantities made it to store shelves.
The Dick Tracy video game was developed by Titus Software in 1990. It was ported to many platforms including Amiga, Commodore and MS-DOS. Dick Tracy is a side scrolling action shooting game. Player controls Dick Tracy through five stages.
There were also games made for the Nintendo Entertainment System (1990), Sega Master System (1990), Sega Genesis (1990), and the Game Boy (1991).
In 2009, Shocker Toys released a monochromatic Dick Tracy action figure as an exclusive product for the San Diego Comic-Con. The figure appears in a suit with two-way wrist radio. There was also a variant figure released of Dick Tracy in his signature trench coat and fedora with a tommy gun accessory.
References.
Notes
Bibliography

</doc>
<doc id="41908" url="http://en.wikipedia.org/wiki?curid=41908" title="Key Word in Context">
Key Word in Context

KWIC is an acronym for Key Word In Context, the most common format for concordance lines. The term KWIC was first coined by Hans Peter Luhn. The system was based on a concept called "keyword in titles" which was first proposed for Manchester libraries in 1864 by Andrea Crestadoro.
A KWIC index is formed by sorting and aligning the words within an article title to allow each word (except the stop words) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized full text search became common.
For example, a search query including all of the words in the title statement of this article ("KWIC is an acronym for Key Word In Context, the most common format for concordance lines") and the in English ("the free encyclopedia"), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).
A KWIC index is a special case of a permuted index. This term refers to the fact that it indexes all cyclic permutations of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of manual pages, often ended with a permuted index section, allowing the reader to easily find a section by any word from its heading. This practice, also known as KWOC (“Key Word Out of Context”), is no longer common.
References in Literature.
"Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all."

</doc>
<doc id="41909" url="http://en.wikipedia.org/wiki?curid=41909" title="Accrual bond">
Accrual bond

An accrual bond is a fixed-interest bond that is issued at its face value and repaid at the end of the maturity period together with the accrued interest. In Germany, the accrued interest is compounded. In contrast to zero-coupon bonds, accrual bonds have a clearly stated coupon rate.
See also.
 The dictionary definition of Accrual bond at Wiktionary

</doc>
<doc id="41910" url="http://en.wikipedia.org/wiki?curid=41910" title="Aftermarket">
Aftermarket

Aftermarket may refer to:

</doc>
<doc id="41912" url="http://en.wikipedia.org/wiki?curid=41912" title="Allotment">
Allotment

Allotment may refer to:

</doc>
<doc id="41913" url="http://en.wikipedia.org/wiki?curid=41913" title="Subscription (finance)">
Subscription (finance)

Subscription refers to the process of investors signing up and committing to invest in a financial instrument, before the actual closing of the purchase.
The term comes from the Latin word "subscribere".
Historical.
Praenumeration.
An early form was praenumeration, a common business practice in the 18th-century book trade in Germany. The publisher offered to sell a book that was planned but had not yet been printed, usually at a discount, so as to cover their costs in advance. The business practice was particularly common with magazines, helping to determine in advance how many subscribers there would be. Praenumeration is similar to the recent crowdfunding financing model.
New issues.
Subscription agreement.
Subscription to new issues can be covered by a subscription agreement, legally committing the investor to invest in the financial instrument, and committing the company to certain obligations and warranties. In some jurisdictions, it is possible for the issuer and subscriber to use a template subscription agreement as the basis of this agreement, although bespoke contract drafting by a qualified specialist may be required in more complex cases.
Subscription period.
When a new security is to be issued, investors typically have two weeks to submit their subscription orders. At the end of this "subscription period", the issuer announces the offering price and the method of allotment.
Allotment.
Allotment is a method of distributing securities to investors when an issue has been oversubscribed. At the end of the subscription period, the demand for a new issue can exceed the number of shares or bonds being issued. In such cases, the underwriting bank allots the securities with the approval of the issuer, either by lottery or on the basis of a formula. An allotment formula usually takes into account the issuer's preferred target investor groups.
Oversubscription.
A funding round is oversubscribed when the company has obtained for funding commitments from investors that in aggregate amount to more money than the company needs or intends to raise. It may be used informally to describe a state where there is more money available than the company needs.
Oversubscription adjustment.
A company can adjust oversubscription money with his receivable allotment money and calls money. Rest money will be returned to shareholders.

</doc>
<doc id="41914" url="http://en.wikipedia.org/wiki?curid=41914" title="Capital market">
Capital market

Capital markets are financial markets for the buying and selling of long-term debt or equity-backed securities. These markets channel the wealth of savers to those who can put it to long-term productive use, such as companies or governments making long-term investments. Capital markets are defined as markets in which money is provided for periods longer than a year.
Financial regulators, such as the UK's Bank of England (BoE) or the U.S. Securities and Exchange Commission (SEC), oversee the capital markets in their jurisdictions to protect investors against fraud, among other duties. 
Modern capital markets are almost invariably hosted on computer-based electronic trading systems; most can be accessed only by entities within the financial sector or the treasury departments of governments and corporations, but some can be accessed directly by the public. There are many thousands of such systems, most serving only small parts of the overall capital markets. Entities hosting the systems include stock exchanges, investment banks, and government departments. Physically the systems are hosted all over the world, though they tend to be concentrated in financial centres like London, New York, and Hong Kong.
A key division within the capital markets is between the primary markets and secondary markets. In primary markets, new stock or bond issues are sold to investors, often via a mechanism known as underwriting. The main entities seeking to raise long-term funds on the primary capital markets are governments (which may be municipal, local or national) and business enterprises (companies). Governments tend to issue only bonds, whereas companies often issue either equity or bonds. The main entities purchasing the bonds or stock include pension funds, hedge funds, sovereign wealth funds, and less commonly wealthy individuals and investment banks trading on their own behalf. In the secondary markets, existing securities are sold and bought among investors or traders, usually on an exchange, over-the-counter, or elsewhere. The existence of secondary markets increases the willingness of investors in primary markets, as they know they are likely to be able to swiftly cash out their investments if the need arises.
A second important division falls between the stock markets (for equity securities, also known as shares, where investors acquire ownership of companies) and the bond markets (where investors become creditors).
Difference between money markets and capital markets.
The money markets are used for the raising of short term finance, sometimes for loans that are expected to be paid back as early as overnight. Whereas the "capital markets" are used for the raising of long term finance, such as the purchase of shares, or for loans that are not expected to be fully paid back for at least a year.
Funds borrowed from the "money markets" are typically used for general operating expenses, to cover brief periods of illiquidity. For example a company may have inbound payments from customers that have not yet cleared, but may wish to immediately pay out cash for its payroll. When a company borrows from the primary "capital markets", often the purpose is to invest in additional physical capital goods, which will be used to help increase its income. It can take many months or years before the investment generates sufficient return to pay back its cost, and hence the finance is long term.
Together, "money markets" and "capital markets" form the financial markets as the term is narrowly understood. The capital market is concerned with long term finance. In the widest sense, it consists of a series of channels through which the savings of the community are made available for industrial and commercial enterprises and public authorities. Private companies seeking for capital market include The Blackstone Group (CEO Stephen A. Schwarzman) and MidOcean Partners (CEO ).
Difference between regular bank lending and capital markets.
Regular bank lending is not usually classed as a capital market transaction, even when loans are extended for a period longer than a year. A key difference is that with a regular bank loan, the lending is not securitized (i.e., it doesn't take the form of resalable security like a share or bond that can be traded on the markets). A second difference is that lending from banks and similar institutions is more heavily regulated than capital market lending. A third difference is that bank depositors and shareholders tend to be more risk averse than capital market investors. The previous three differences all act to limit institutional lending as a source of finance. Two additional differences, this time favoring lending by banks, are that banks are more accessible for small and medium companies, and that they have the ability to create money as they lend. In the 20th century, most company finance apart from share issues was raised by bank loans. But since about 1980 there has been an ongoing trend for disintermediation, where large and credit worthy companies have found they effectively have to pay out less in interest if they borrow direct from capital markets rather than banks. The tendency for companies to borrow from capital markets instead of banks has been especially strong in the US. According to Lena Komileva writing for "The Financial Times", Capital Markets overtook bank lending as the leading source of long term finance in 2009 - this reflects the additional risk aversion and regulation of banks following the 2008 financial crisis.
Examples of capital market transactions.
A government raising money on the primary markets.
When a government wants to raise long term finance it will often sell bonds to the capital markets. In the 20th and early 21st century, many governments would use investment banks to organize the sale of their bonds. The leading bank would underwrite the bonds, and would often head up a syndicate of brokers, some of whom might be based in other investment banks. The syndicate would then sell to various investors. For developing countries, a multilateral development bank would sometimes provide an additional layer of underwriting, resulting in risk being shared between the investment bank(s), the multilateral organization, and the end investors. However, since 1997 it has been increasingly common for governments of the larger nations to bypass investment banks by making their bonds directly available for purchase over the Internet. Many governments now sell most of their bonds by computerized auction. Typically large volumes are put up for sale in one go; a government may only hold a small number of auctions each year. Some governments will also sell a continuous stream of bonds through other channels. The biggest single seller of debt is the US Government; there are usually several transactions for such sales every second, which corresponds to the continuous updating of the US real time debt clock.
A company raising money on the primary markets.
When a company wants to raise money for long-term investment, one of its first decisions is whether to do so by issuing bonds or shares. If it chooses shares, it avoids increasing its debt, and in some cases the new shareholders may also provide non monetary help, such as expertise or useful contacts. On the other hand, a new issue of shares can dilute the ownership rights of the existing shareholders, and if they gain a controlling interest, the new shareholders may even replace senior managers. From an investor's point of view, shares offer the potential for higher returns and capital gains if the company does well. Conversely, bonds are safer if the company does poorly, as they are less prone to severe falls in price, and in the event of bankruptcy, bond owners are usually paid before shareholders.
When a company raises finance from the primary market, the process is more likely to involve face-to-face meetings than other capital market transactions. Whether they choose to issue bonds or shares, companies will typically enlist the services of an investment bank to mediate between themselves and the market. A team from the investment bank often meets with the company's senior managers to ensure their plans are sound. The bank then acts as an underwriter, and will arrange for a network of brokers to sell the bonds or shares to investors. This second stage is usually done mostly through computerized systems, though brokers will often phone up their favored clients to advise them of the opportunity. Companies can avoid paying fees to investment banks by using a direct public offering, though this is not a common practice as it incurs other legal costs and can take up considerable management time.
Trading on the secondary markets.
Most capital market transactions take place on the secondary market. On the primary market, each security can be sold only once, and the process to create batches of new shares or bonds is often lengthy due to regulatory requirements. On the secondary markets, there is no limit on the number of times a security can be traded, and the process is usually very quick. With the rise of strategies such as high-frequency trading, a single security could in theory be traded thousands of times within a single hour. Transactions on the secondary market don't directly help raise finance, but they do make it easier for companies and governments to raise finance on the primary market, as investors know if they want to get their money back in a hurry, they will usually be easily able to re-sell their securities. Sometimes however secondary capital market transactions can have a negative effect on the primary borrowers - for example, if a large proportion of investors try to sell their bonds, this can push up the yields for future issues from the same entity. An extreme example occurred shortly after Bill Clinton began his first term as President of the United States; Clinton was forced to abandon some of the spending increases he'd promised in his election campaign due to pressure from the bond markets. In the 21st century, several governments have tried to lock in as much as possible of their borrowing into long dated bonds, so they are less vulnerable to pressure from the markets. Following the financial crisis of 2007–08, the introduction of Quantitative easing further reduced the ability of private actors to push up the yields of government bonds, at least for countries with a Central bank able to engage in substantial Open market operations. 
A variety of different players are active in the secondary markets. Regular individuals account for a small proportion of trading, though their share has slightly increased; in the 20th century it was mostly only a few wealthy individuals who could afford an account with a broker, but accounts are now much cheaper and accessible over the internet. There are now numerous small traders who can buy and sell on the secondary markets using platforms provided by brokers which are accessible via web browsers. When such an individual trades on the capital markets, it will often involve a two-stage transaction. First they place an order with their broker, then the broker executes the trade. If the trade can be done on an exchange, the process will often be fully automated. If a dealer needs to manually intervene, this will often mean a larger fee. Traders in investment banks will often make deals on their bank's behalf, as well as executing trades for their clients. Investment banks will often have a division (or department) called "capital markets": staff in this division try to keep aware of the various opportunities in both the primary and secondary markets, and will advise major clients accordingly. Pension and sovereign wealth funds tend to have the largest holdings, though they tend to buy only the highest grade (safest) types of bonds and shares, and often don't trade all that frequently. According to a 2012 "Financial Times" article, hedge funds are increasingly making most of the short term trades in large sections of the capital market (like the UK and US stock exchanges), which is making it harder for them to maintain their historically high returns, as they are increasingly finding themselves trading with each other rather than with less sophisticated investors.
There are several ways to invest in the secondary market without directly buying shares or bonds. A common method is to invest in mutual funds or exchange-traded funds. It's also possible to buy and sell derivatives that are based on the secondary market; one of the most common being contract for difference - these can provide rapid profits, but can also cause buyers to lose more money than they originally invested.
Size of the global capital markets.
All figures given are in Billions of US$ and are sourced to the IMF. There is no universally recognized standard for measuring all of these figures, so other estimates may vary. A GDP column is included as a comparison.
Capital controls.
Capital controls are measures imposed by a state's government aimed at managing capital account transactions - in other words, capital market transactions where one of the counter-parties involved is in a foreign country. Whereas domestic regulatory authorities try to ensure that capital market participants trade fairly with each other, and sometimes to ensure institutions like banks don't take excessive risks, capital controls aim to ensure that the macroeconomic effects of the capital markets don't have a net negative impact on the nation in question. Most advanced nations like to use capital controls sparingly if at all, as in theory allowing markets freedom is a win-win situation for all involved: investors are free to seek maximum returns, and countries can benefit from investments that will develop their industry and infrastructure. However sometimes capital market transactions can have a net negative effect - for example, in a financial crisis, there can be a mass withdrawal of capital, leaving a nation without sufficient foreign currency to pay for needed imports. On the other hand, if too much capital is flowing into a country, it can push up inflation and the value of the nation's currency, making its exports uncompetitive. Some nations such as India have also used capital controls to ensure that their citizens' money is invested at home, rather than abroad.

</doc>
<doc id="41915" url="http://en.wikipedia.org/wiki?curid=41915" title="Primary market">
Primary market

The primary market is the part of the capital market that deals with issuing of new securities. Companies, governments or public sector institutions can obtain funds through the sale of a new stock or bond issues through primary market. This is typically done through an investment bank or finance syndicate of securities dealers. 
The process of selling new issues to investors is called underwriting. In the case of a new stock issue, this sale is an initial public offering (IPO). Dealers earn a commission that is built into the price of the security offering, though it can be found in the prospectus. Primary markets create long term instruments through which corporate entities borrow from capital market.
Once issued the securities typically trade on a secondary market such as a stock exchange, bond market or derivatives exchange.
Features.
Features of primary markets are:

</doc>
<doc id="41916" url="http://en.wikipedia.org/wiki?curid=41916" title="Financial market">
Financial market

A financial market is a market in which people and entities can trade financial securities, commodities, and other fungible items of value at low transaction costs and at prices that reflect supply and demand. Securities include stocks and bonds, and commodities include precious metals or agricultural goods.
In economics, typically, the term "market" means the aggregate of possible buyers and sellers of a certain good or service and the transactions between them.
The term "market" is sometimes used for what are more strictly "exchanges", organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (like the NYSE, BSE, NSE) or an electronic system (like NASDAQ). Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell stock from the one to the other without using an exchange.
Trading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well, similar to stock exchanges.
Types of financial markets.
Within the financial sector, the term "financial markets" is often used to refer just to the markets that are used to raise finance: for long term finance, the "Capital markets"; for short term finance, the "Money markets". Another common use of the term is as a catchall for all the markets in the financial sector, as per examples in the breakdown below.
The capital markets may also be divided into primary markets and secondary markets. Newly formed (issued) securities are bought or sold in primary markets, such as during initial public offerings. Secondary markets allow investors to buy and sell existing securities. The transactions in primary markets exist between issuers and investors, while secondary market transactions exist among investors.
Liquidity is a crucial aspect of securities that are traded in secondary markets. Liquidity refers to the ease with which a security can be sold without a loss of value. Securities with an active secondary market mean that there are many buyers and sellers at a given point in time. Investors benefit from liquid securities because they can sell their assets whenever they want; an illiquid security may force the seller to get rid of their asset at a large discount.
Raising capital.
Financial markets attract funds from investors and channel them to corporations—they thus allow corporations to finance their operations and achieve growth. Money markets allow firms to borrow funds on a short term basis, while capital markets allow corporations to gain long-term funding to support expansion (known as maturity transformation).
Without financial markets, borrowers would have difficulty finding lenders themselves. Intermediaries such as banks, Investment Banks, and Boutique Investment Banks can help in this process. Banks take deposits from those who have money to save. They can then lend money from this pool of deposited money to those who seek to borrow. Banks popularly lend money in the form of loans and mortgages.
More complex transactions than a simple bank deposit require markets where lenders and their agents can meet borrowers and their agents, and where existing borrowing or lending commitments can be sold on to other parties. A good example of a financial market is a stock exchange. A company can raise money by selling shares to investors and its existing shares can be bought or sold.
The following table illustrates where financial markets fit in the relationship between lenders and borrowers:
Lenders.
Who have enough money to lend or to give someone money from own pocket at the condition of getting back the principal amount or with some interest or charge, is the Lender.
Individuals & Doubles.
Many individuals are not aware that they are lenders, but almost everybody does lend money in many ways. A person lends money when he or she:
Companies.
"Companies" tend to be borrowers of capital. When companies have surplus cash that is not needed for a short period of time, they may seek to make money from their cash surplus by lending it via short term markets called money markets.
There are a few companies that have very strong cash These companies tend to be lenders rather than borrowers. Such companies may decide to return cash to surplus (e.g. via a share repurchase.) Alternatively, they may seek to make more money on their cash by lending it (e.g. investing in bonds and stocks).
Borrowers.
Governments borrow by issuing bonds. In the UK, the government also borrows from individuals by offering bank accounts and Premium Bonds. Government debt seems to be permanent. Indeed the debt seemingly expands rather than being paid off. One strategy used by governments to reduce the "value" of the debt is to influence "inflation".
"Municipalities and local authorities" may borrow in their own name as well as receiving funding from national governments. In the UK, this would cover an authority like Hampshire County Council.
"Public Corporations" typically include nationalized industries. These may include the postal services, railway companies and utility companies.
Many borrowers have difficulty raising money locally. They need to borrow internationally with the aid of Foreign exchange markets.
Borrowers having similar needs can form into a group of borrowers. They can also take an organizational form like Mutual Funds. They can provide mortgage on weight basis. The main advantage is that this lowers the cost of their borrowings.
Derivative products.
During the 1980s and 1990s, a major growth sector in financial markets is the trade in so called , or derivatives for short.
In the financial markets, stock prices, bond prices, currency rates, interest rates and dividends go up and down, creating "risk". Derivative products are financial products which are used to "control" risk or paradoxically "exploit" risk. It is also called financial economics.
Derivative products or instruments help the issuers to gain an unusual profit from issuing the instruments. For using the help of these products a contract has to be made. Derivative contracts are mainly 4 types:
Currency markets.
Seemingly, the most obvious buyers and sellers of currency are importers and exporters of goods. While this may have been true in the distant past, when international trade created the demand for currency markets, importers and exporters now represent only 1/32 of foreign exchange dealing, according to the Bank for International Settlements.
The picture of foreign currency transactions today shows:
Analysis of financial markets.
Much effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of "technical analysis" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics, who claim that the evidence points rather to the random walk hypothesis, which states that the next change is not correlated to the last change. The role of human psychology in price variations also plays a significant factor. Large amounts of volatility often indicate the presence of strong emotional factors playing into the price. Fear can cause excessive drops in price and greed can create bubbles. In recent years the rise of algorithmic and high-frequency program trading has seen the adoption of momentum, ultra-short term moving average and other similar strategies which are based on technical as opposed to fundamental or theoretical concepts of market Behaviour.
The scale of changes in price over some unit of time is called the volatility.
It was discovered by Benoît Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time unit to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation.
Role (Financial system and the economy).
One of the important sustainability requisite for the accelerated development of an economy is the existence of a dynamic financial market. A financial market helps the economy in the following manner. 
Constituents of Financial Market.
Based on market levels.
Simply put, primary market is the market where the newly started company issued shares to the public for the first time through IPO (initial public offering). Secondary market is the market where the second hand securities are sold (it's a resale of securities).

</doc>
<doc id="41918" url="http://en.wikipedia.org/wiki?curid=41918" title="Bond">
Bond

Bond, bonds, bonded, and bonding may refer to:

</doc>
<doc id="41919" url="http://en.wikipedia.org/wiki?curid=41919" title="Bonds">
Bonds

Bonds can refer to any of several things:

</doc>
<doc id="41920" url="http://en.wikipedia.org/wiki?curid=41920" title="Compact Disc Digital Audio">
Compact Disc Digital Audio

Compact Disc Digital Audio (CDDA or CD-DA) is the standard format for audio compact discs. The standard is defined in the "Red Book", one of a series of "Rainbow Books" (named for their binding colors) that contain the technical specifications for all CD formats.
Standard.
The "Red Book" specifies the physical parameters and properties of the CD, the optical "stylus" parameters, deviations and error rate, modulation system (eight-to-fourteen modulation, EFM) and error correction facility (cross-interleaved Reed–Solomon coding, CIRC), and the eight subcode channels. These parameters are common to all compact discs and used by all logical formats, such as CD-ROM. The standard also specifies the form of digital audio encoding: 2-channel signed 16-bit Linear PCM sampled at 44,100 Hz. Although rarely used, the specification allows for discs to be mastered with a form of emphasis.
The first edition of the "Red Book" was released in 1980 by Philips and Sony; it was adopted by the Digital Audio Disc Committee and ratified by the International Electrotechnical Commission Technical Committee 100, as an International Standard in 1987 with the reference IEC 60908. The second edition of IEC 60908 was published in 1999 and it cancels and replaces the first edition, amendment 1 (1992) and the corrigendum to amendment 1. The IEC 60908 however does not contain all the information for extensions that is available in the Red Book, such as the details for CD-Text, CD+G and CD+EG.
The standard is not freely available and must be licensed. It is available from Philips and the IEC. s of 2013[ [update]], Philips outsources licensing of the standard to Adminius, which charges US$ for the "Red Book", plus US$ each for the "Subcode Channels R-W" and "CD Text Mode" annexes.
Audio format.
The audio contained in a CD-DA consists of two-channel signed 16-bit Linear PCM sampled at 44,100 Hz.
Sample rate.
The sampling rate is adapted from that attained when recording digital audio on a PAL (or NTSC) videotape with a PCM adaptor, an earlier way of storing digital audio. An audio CD can represent frequencies up to 22.05 kHz, the Nyquist frequency of the 44.1 kHz sample rate.
The selection of the sample rate was based primarily on the need to reproduce the audible frequency range of 20–20,000 Hz (20 kHz). The Nyquist–Shannon sampling theorem states that a sampling rate of more than twice the maximum frequency of the signal to be recorded is needed, resulting in a required rate of at least 40 kHz. The exact sampling rate of 44.1 kHz was inherited from a method of converting digital audio into an analog video signal for storage on U-matic video tape, which was the most affordable way to transfer data from the recording studio to the CD manufacturer at the time the CD specification was being developed. The device that converts an analog audio signal into PCM audio, which in turn is changed into an analog video signal is called a PCM adaptor. This technology could store six samples (three samples per stereo channel) in a single horizontal line. A standard NTSC video signal has 245 usable lines per field, and 59.94 fields/s, which works out to be 44,056 samples/s/stereo channel. Similarly, PAL has 294 lines and 50 fields, which gives 44,100 samples/s/stereo channel. This system could store 14-bit samples with some error correction, or 16-bit samples with almost no error correction.
There was a long debate over the use of 14-bit (Philips) or 16-bit (Sony) quantization, and 44,056 or 44,100 samples/s (Sony) or approximately 44,000 samples/s (Philips). When the Sony/Philips task force designed the Compact Disc, Philips had already developed a 14-bit D/A converter (DAC), but Sony insisted on 16-bit. In the end, 16 bits and 44.1 kilosamples per second prevailed. Philips found a way to produce 16-bit quality using its 14-bit DAC by using four times oversampling.
Pre-emphasis.
Some CDs are mastered with pre-emphasis, an artificial boost of high audio frequencies. The pre-emphasis improves the apparent signal-to-noise ratio by making better use of the channel's dynamic range. On playback, the player applies a de-emphasis filter to restore the frequency response curve to an overall flat one. Pre-emphasis time constants are 50µs or 15µs, and a binary flag in the disc subcode instructs the player to apply de-emphasis filtering if appropriate. Playback of such discs in a computer or 'ripping' to wave files typically does not take into account the pre-emphasis, so such files play back with a distorted frequency response.
Storage capacity and playing time.
The creators of the CD originally aimed at a playing time of 60 minutes with a disc diameter of 100 mm (Sony) or 115 mm (Philips). Sony vice-president Norio Ohga suggested extending the capacity to 74 minutes to accommodate Wilhelm Furtwängler's recording of Ludwig van Beethoven's Symphony No. 9 from the 1951 Bayreuth Festival. The additional 14-minute playing time subsequently required changing to a 120 mm disc. Kees Schouhamer Immink, Philips' chief engineer, however, denies this, claiming that the increase was motivated by technical considerations, and that even after the increase in size, the Furtwängler recording would not have fit on one of the earliest CDs.
According to a "Sunday Tribune" interview, the story is slightly more involved. In 1979, Philips owned PolyGram, one of the world's largest distributors of music. PolyGram had set up a large experimental CD plant in Hannover, Germany, which could produce huge numbers of CDs having a diameter of 115 mm. Sony did not yet have such a facility. If Sony had agreed on the 115-mm disc, Philips would have had a significant competitive edge in the market. The long playing time of Beethoven's Ninth Symphony imposed by Ohga was used to push Philips to accept 120 mm, so that Philips' PolyGram lost its edge on disc fabrication.
The 74-minute playing time of a CD, which was longer than the 22 minutes per side typical of long-playing (LP) vinyl albums, was often used to the CD's advantage during the early years when CDs and LPs vied for commercial sales. CDs would often be released with one or more bonus tracks, enticing consumers to buy the CD for the extra material. However, attempts to combine double LPs onto one CD occasionally resulted in the opposite situation in which the CD would actually offer fewer tracks than the equivalent LP, though bonus tracks were also added to CD re-releases of double LPs as well.
Playing times beyond 74 minutes are achieved by decreasing track pitch beyond the original "Red Book" standard. Most players can accommodate the more closely spaced data. Christian Thielemann's live Deutsche Grammophon recording of Bruckner's Fifth with the Munich Philharmonic in 2004 clocks at 82:34. The Kirov Orchestra recording of Pyotr Ilyich Tchaikovsky's "The Nutcracker" conducted by Valery Gergiev and released by Philips/PolyGram Records (catalogue number 462 114) on October 20, 1998, clocks at 81:14. The Mission of Burma compilation album "Mission of Burma", released in 1988 by Rykodisc, previously held the record at 80:08.
Current manufacturing processes allow an audio CD to contain up to 80 minutes (variable from one replication plant to another) without requiring the content creator to sign a waiver releasing the plant owner from responsibility if the CD produced is marginally or entirely unreadable by some playback equipment. Thus, in current practice, maximum CD playing time has crept higher by reducing minimum engineering tolerances; by and large, this has not unacceptably reduced reliability.
Technical specifications.
Data encoding.
Each audio sample is a signed 16-bit two's complement integer, with sample values ranging from −32768 to +32767. The source audio data is divided into frames, containing twelve samples each (six left and right samples, alternating), for a total of 192 bits (24 bytes) of audio data per frame.
This stream of audio frames, as a whole, is then subjected to CIRC encoding, which segments and rearranges the data and expands it with parity bits in a way that allows occasional read errors to be detected and corrected. CIRC encoding also interleaves the audio frames throughout the disc over several consecutive frames so that the information will be more resistant to burst errors. Therefore, a physical frame on the disc will actually contain information from multiple logical audio frames. This process adds 64 bits of error correction data to each frame. After this, 8 bits of subcode or subchannel data are added to each of these encoded frames, which is used for control and addressing when playing the CD.
CIRC encoding plus the subcode byte generate 33-bytes long frames, called "channel-data" frames. These frames are then modulated through eight-to-fourteen modulation (EFM), where each 8-bit word is replaced with a corresponding 14-bit word designed to reduce the number of transitions between 0 and 1. This reduces the density of physical pits on the disc and provides an additional degree of error tolerance. Three "merging" bits are added before each 14-bit word for disambiguation and synchronization. In total there are 33 × (14 + 3) = 561 bits. A 27-bit word (a 24-bit pattern plus 3 merging bits) is added to the beginning of each frame to assist with synchronization, so the reading device can locate frames easily. With this, a frame ends up containing 588 bits of "channel data" (which are decoded to only 192 bits music).
The frames of channel data are finally written to disc physically in the form of pits and lands, with each pit or land representing a series of zeroes, and with the transition points—the edge of each pit—representing 1.
A Red Book-compatible CD-R has pit-and-land-shaped spots on a layer of organic dye instead of actual pits and lands; a laser creates the spots by altering the reflective properties of the dye.
Data structure.
The audio data stream in an audio CD is continuous, but has three parts. The main portion, which is further divided into playable audio tracks, is the "program area". This section is preceded by a "lead-in" track and followed by a "lead-out" track. The lead-in and lead-out tracks encode only silent audio, but all three sections contain subcode data streams.
The lead-in's subcode contains repeated copies of the disc's Table Of Contents (TOC), which provides an index of the start positions of the tracks in the program area and lead-out. The track positions are referenced by absolute timecode, relative to the start of the program area, in MSF format: minutes, seconds, and fractional seconds called "frames". Each timecode frame is one seventy-fifth of a second, and corresponds to a block of 98 channel-data frames—ultimately, a block of 588 pairs of left and right audio samples. Timecode contained in the subchannel data allows the reading device to locate the region of the disc that corresponds to the timecode in the TOC. The TOC on discs is analogous to the partition table on hard drives. Nonstandard or corrupted TOC records are abused as a form of CD/DVD copy protection, in e.g. the key2Audio scheme.
Tracks.
The largest entity on a CD is called a track. A CD can contain up to 99 tracks (including a data track for mixed mode discs). Each track can in turn have up to 100 indexes, though players which handle this feature are rarely found outside of pro audio, particularly radio broadcasting. The vast majority of songs are recorded under index 1, with the pre-gap being index 0. Sometimes hidden tracks are placed at the end of the last track of the disc, often using index 2 or 3. This is also the case with some discs offering "101 sound effects", with 100 and 101 being indexed as two and three on track 99. The index, if used, is occasionally put on the track listing as a decimal part of the track number, such as 99.2 or 99.3. (Information Society's "Hack" was one of very few CD releases to do this, following a release with an equally obscure CD+G feature.) The track and index structure of the CD were carried forward to the DVD format as title and chapter, respectively.
Tracks, in turn, are divided into timecode frames (or sectors), which are further subdivided into channel-data frames.
Frames and timecode frames.
The smallest entity in a CD is a channel-data "frame", which consists of 33 bytes and contains six complete 16-bit stereo samples: 24 bytes for the audio (two bytes × two channels × six samples = 24 bytes), eight CIRC error-correction bytes, and one subcode byte. As described in the "Data encoding" section, after the EFM modulation the number of bits in a frame totals 588.
On a "Red Book" audio CD, data is addressed using the "MSF scheme", with timecodes expressed in minutes, seconds and another type of "frames" (mm:ss:ff), where one frame corresponds to 1/75th of a second of audio: 588 pairs of left and right samples. This timecode frame is distinct from the 33-byte channel-data frame described above, and is used for time display and positioning the reading laser. When editing and extracting CD audio, this timecode frame is the smallest addressable time interval for an audio CD; thus, track boundaries only occur on these frame boundaries. Each of these structures contains 98 channel-data frames, totaling 98 × 24 = 2,352 bytes of music. The CD is played at a speed of 75 frames (or sectors) per second, thus 44,100 samples or 176,400 bytes per second.
In the 1990s, CD-ROM and related Digital Audio Extraction (DAE) technology introduced the term "sector" to refer to each timecode frame, with each sector being identified by a sequential integer number starting at zero, and with tracks aligned on sector boundaries. An audio CD sector corresponds to 2,352 bytes of decoded data. The "Red Book" does not refer to sectors, nor does it distinguish the corresponding sections of the disc's data stream except as "frames" in the MSF addressing scheme.
The following table shows the relation between tracks, timecode frames (sectors) and channel-data frames:
Bit rate.
The audio bit rate is 1,411.2 kbit/s (as 2 channels × 44,100 samples per second per channel × 16 bits per sample = 1,411,200 bit/s = 1,411.2 kbit/s). Likewise, in a computer, audio data coming in from a CD drive is accessed by sectors, each sector being 2,352 bytes, and with 75 sectors containing 1 second of audio, for the same bit rate of 2,352 × 75 = 176.4 KiB/s (1,411.2 kbit/s). In comparison, the bit rate of a "1×" CD-ROM is defined as 2,048 bytes per sector × 75 sectors per second = 150 KiB/s (1,228.8 kbit/s). The undecoded channel-data rate for a "Red Book" audio CD is 4.3218 Mbit/s, with 2.0338 Mbit/s being the rate of the undecoded audio and subcode.
Data access from computers.
Unlike on a DVD or CD-ROM, there are no "files" on a "Red Book" audio CD; there are only the physical pits and lands, which in turn represent a single encoded data stream, which ultimately represents one continuous stream of LPCM audio data, and a parallel, smaller set of 8 subcode data streams. Computer operating systems, however, may provide access to an audio CD as if it contains files. For example, Windows represents the CD's Table of Contents as a set of Compact Disc Audio track (CDA) files, each file containing indexing information, not audio data.
In a process called ripping, digital audio extraction software can be used to read CD-DA audio data and store it in files. Common audio file formats for this purpose include WAV and AIFF, which simply preface the LPCM data with a short header; FLAC, ALAC, and Windows Media Audio Lossless, which compress the LPCM data in ways that conserve space yet allow it to be restored without any changes; and various lossy, perceptual coding formats like MP3 and AAC, which modify and compress the audio data in ways that irreversibly change the audio, but that exploit features of human hearing to make the changes difficult to discern.
Format variations.
Recording publishers have created CDs that violate the "Red Book" standard. Some do so for the purpose of copy prevention, using systems like Copy Control. Some do so for extra features such as DualDisc, which includes both a CD layer and a DVD layer whereby the CD layer is much thinner, 0.9 mm, than required by the "Red Book", which stipulates a nominal 1.2 mm, but at least 1.1 mm. Philips and many other companies have stated that including the Compact Disc Digital Audio logo on such non-conforming discs may constitute trademark infringement. Either in anticipation or in response, recent copy-protected CDs bear stickers and warnings that the CD is not standard and may not play in all CD players, and no longer display the long-familiar logo.
Super Audio CD was a standard published in 1999 that aimed to provide better audio quality in CDs, but it never became very popular. DVD Audio, an advanced version of the audio CD, emerged in 1999. The format was designed to feature audio of higher fidelity. It applies a higher sampling rate and used 650 nm lasers.
Copyright issues.
There have been moves by the recording industry to make audio CDs (Compact Disc Digital Audio) unplayable on computer CD-ROM drives, to prevent the copying of music. This is done by intentionally introducing errors onto the disc that the embedded circuits on most stand-alone audio players can automatically compensate for, but which may confuse CD-ROM drives. Consumer rights advocates as of October 2001 pushed to require warning labels on compact discs that do not conform to the official Compact Disc Digital Audio standard (often called the Red Book) to inform consumers which discs do not permit full fair use of their content.
In 2005, Sony BMG Music Entertainment was criticised when a copy protection mechanism known as Extended Copy Protection (XCP) used on some of their audio CDs automatically and surreptitiously installed copy-prevention software on computers (see 2005 Sony BMG CD copy protection scandal). Such discs are not legally allowed to be called CDs or Compact Discs because they break the Red Book standard governing CDs, and Amazon.com for example describes them as "copy protected discs" rather than "compact discs" or "CDs".

</doc>
