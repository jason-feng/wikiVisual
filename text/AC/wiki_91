<doc id="40557" url="http://en.wikipedia.org/wiki?curid=40557" title="Campaign finance reform in the United States">
Campaign finance reform in the United States

 Campaign finance reform is the political effort in the United States to change the involvement of money in politics, primarily in political campaigns.
Although attempts to regulate campaign finance by legislation date back to 1867, the first successful attempts nationally to regulate and enforce campaign finance originated in the 1970s. The Federal Election Campaign Act (FECA) of 1972 required candidates to disclose sources of campaign contributions and campaign expenditures. It was amended in 1974 with the introduction of statutory limits on contributions, and creation of the Federal Election Commission (FEC). It attempted to restrict the influence of wealthy individuals by limiting individual donations to $1,000 and donations by political action committees (PACs) to $5,000. These specific election donations are known as ‘hard money.’ The Bipartisan Campaign Reform Act (BCRA) of 2002, also known as "McCain-Feingold", after its sponsors, is the most recent major federal law on campaign finance, which revised some of the legal limits on expenditures set in 1924, and prohibited unregulated contributions (commonly referred to as "soft money") to national political parties. ‘Soft money’ also refers to funds spent by independent organizations that do not specifically advocate the election or defeat of candidates, and funds which are not contributed directly to candidate campaigns.
In early 2010, the United States Supreme Court ruled in "Citizens United v. Federal Election Commission" that corporate funding of independent political broadcasts in candidate elections cannot be limited pursuant to the right of these entities to free speech.
History.
First attempts.
To gain votes from recently enfranchised, unpropertied voters, Andrew Jackson launched his campaign for the 1828 election through a network of partisan newspapers across the nation. After his election, Jackson began a political patronage system that rewarded political party operatives, which had a profound effect on future elections. Eventually, appointees were expected to contribute portions of their pay back to the political machine. During the Jacksonian era, some of the first attempts were made by corporations to influence politicians. Jackson claimed that his charter battle against the Second Bank of the United States was one of the great struggles between democracy and the money power. While it was rumored that The Bank of the United States spent over $40,000 from 1830 to 1832 in an effort to stop Jackson's re-election, Chairman Biddle of the BUS only spent "tens of thousands to distribute information favorable to the bank." This expenditure can be conceived as being spent "against" Jackson, because of the competing ideals, but there is no evidence to support a direct correlation to negative electioneering.
In the 1850s, Pennsylvania Republican Simon Cameron began to develop what became known as the "Pennsylvania idea" of applying the wealth of corporations to help maintain Republican control of the legislature. Political machines across the country used the threat of hostile legislation to force corporate interests into paying for the defeat of the measures. U.S. Senators of the time were elected not by popular vote, but by state legislatures, whose votes could sometimes be bought. Exposed bribery occurred in Colorado, Kansas, Montana and West Virginia.
Abraham Lincoln's attempt to finance his own 1858 Senate run bankrupted him, even though he had arranged a number of $500 expense accounts from wealthy donors. However, he was able to regain enough money in his law practice to purchase an Illinois newspaper to support him in the presidential election of 1860, for which he gained the financial support of businessmen in Philadelphia and New York City.
After the Civil War, parties increasingly relied on wealthy individuals for support, including Jay Cooke, the Vanderbilts, and the Astors. In the absence of a civil service system, parties also continued to rely heavily on financial support from government employees, including assessments of a portion of their federal pay. The first federal campaign finance law, passed in 1867, was a Naval Appropriations Bill which prohibited officers and government employees from soliciting contributions from Navy yard workers. Later, the Pendleton Civil Service Reform Act of 1883 established the civil service and extended the protections of the Naval Appropriations Bill to all federal civil service workers. However, this loss of a major funding source increased pressure on parties to solicit funding from corporate and individual wealth.
In the campaign of 1872, a group of wealthy New York Democrats pledged $10,000 each to pay for the costs of promoting the election. On the Republican side, one Ulysses S. Grant supporter alone contributed one fourth of the total finances. One historian said that never before was a candidate under such a great obligation to men of wealth. Vote buying and voter coercion were common in this era. After more standardized ballots were introduced, these practices continued, applying methods such as requiring voters to use carbon paper to record their vote publicly in order to be paid.
Boies Penrose mastered post-Pendleton Act corporate funding through extortionist tactics, such as squeeze bills (legislation threatening to tax or regulate business unless funds were contributed.) During his successful 1896 U.S. Senate campaign, he raised a quarter million dollars within 48 hours. He allegedly told supporters that they should send him to Congress to enable them to make even more money.
In 1896, a wealthy Ohio industrialist, shipping magnate and political operative, Mark Hanna became Chairman of the Republican National Committee. Hanna directly contributed $100,000 to the nomination campaign of fellow Ohioan William McKinley, but recognized that more would be needed to fund the general election campaign. Hanna systematized fund-raising from the business community. He assessed banks 0.25% of their capital, and corporations were assessed in relation to their profitability and perceived stake in the prosperity of the country. McKinley's run became the prototype of the modern commercial advertising campaign, putting the President-to-be's image on buttons, billboards, posters, and so on. Business supporters, determined to defeat the Democratic-populist William Jennings Bryan, were more than happy to give, and Hanna actually refunded or turned down what he considered to be "excessive" contributions that exceeded a business's assessment.
Twentieth-century Progressive advocates, together with journalists and political satirists, argued to the general public that the policies of vote buying and excessive corporate and moneyed influence were abandoning the interests of millions of taxpayers. They advocated strong antitrust laws, restricting corporate lobbying and campaign contributions, and greater citizen participation and control, including standardized secret ballots, strict voter registration and women's suffrage.
In his first term, President Theodore Roosevelt, following President McKinley's assassination of 1901, began trust-busting and anti-corporate-influence activities, but fearing defeat, turned to bankers and industrialists for support in what turned out to be his 1904 landslide campaign. Roosevelt was embarrassed by his corporate financing and was unable to clear a suspicion of a quid pro quo exchange with E.H. Harriman for what was an eventually unfulfilled ambassador nomination. There was a resulting national call for reform, but Roosevelt claimed that it was legitimate to accept large contributions if there were no implied obligation. However, in his 1905 message to Congress following the election, he proposed that "contributions by corporations to any political committee or for any political purpose should be forbidden by law." The proposal, however, included no restrictions on campaign contributions from the private individuals who owned and ran corporations. Roosevelt also called for public financing of federal candidates via their political parties. The movement for a national law to require disclosure of campaign expenditures, begun by the National Publicity Law Association, was supported by Roosevelt but delayed by Congress for a decade.
This first effort at wide-ranging reform resulted in the Tillman Act of 1907. Named for its sponsor, South Carolina Senator Ben Tillman, the Tillman Act prohibited corporations and nationally chartered (interstate) banks from making direct financial contributions to federal candidates. However, weak enforcement mechanisms made the Act ineffective. Disclosure requirements and spending limits for House and Senate candidates followed in 1910 and 1911. General contribution limits were enacted in the Federal Corrupt Practices Act (1925). An amendment to the Hatch Act of 1939 set an annual ceiling of $3 million for political parties' campaign expenditures and $5,000 for individual campaign contributions. The Smith-Connally Act (1943) and Taft-Hartley Act (1947) extended the corporate ban to labor unions.
FECA and the Watergate amendments.
All of these efforts were largely ineffective, easily circumvented and rarely enforced. In 1971, however, Congress passed the Federal Election Campaign Act, requiring broad disclosure of campaign finance. In 1974, fueled by public reaction to the Watergate Scandal, Congress passed amendments to the Act establishing a comprehensive system of regulation and enforcement, including public financing of presidential campaigns and creation of a central enforcement agency, the Federal Election Commission. Other provisions included limits on contributions to campaigns and expenditures by campaigns, individuals, corporations and other political groups.
The 1976 decision of the US Supreme Court in Buckley v. Valeo struck down various FECA limits on spending as unconstitutional violations of free speech. Among other changes, this removed limits on candidate expenditures unless the candidate accepts public financing.
Reforms of the 1980s and 1990s.
In 1986, several bills were killed in the U.S. Senate by bipartisan maneuvers which did not allow the bills to come up for a vote. The bill would impose strict controls for campaign fund raising. Later in 1988, legislative and legal setbacks on proposals designed to limit overall campaign spending by candidates were shelved after a Republican filibuster. In addition, a constitutional amendment to override a Supreme Court decision failed to get off the ground.
In 1994, Senate Democrats had more bills blocked by Republicans including a bill setting spending limits and authorizing partial public financing of congressional elections. In 1996, bipartisan legislation for voluntary spending limits which rewards those who bare soft money was killed by a Republican filibuster.
In 1997, Senators McCain (R-AZ) and Feingold (D-WI) sought to eliminate soft money and TV advertising expenditures, but the legislation was defeated by a Republican filibuster. Several different proposals were made in 1999 by both parties. The Campaign Integrity Act (H.R. 1867), proposed by Asa Hutchinson (R-AR), would have banned soft money and raised limits on hard money. The Citizen Legislature & Political Act sponsored by Rep. John Doolittle (R-CA) would have repealed all federal freedom act contribution limits and expedited and expanded disclosure (H.R. 1922 in 1999, the 106th Congress, and reintroduced with different numbers through 2007, the 110th Congress). The Shays-Meehan Campaign Reform Act (H.R. 417) evolved into the McCain–Feingold Bipartisan Campaign Reform Act of 2002.
Bipartisan Campaign Reform Act of 2002.
The Congress passed the Bipartisan Campaign Reform Act (BCRA), also called the McCain-Feingold bill after its chief sponsors, John McCain and Russ Feingold. The bill was passed by the House of Representatives on February 14, 2002, with 240 yeas and 189 nays, including 6 members who did not vote. Final passage in the Senate came after supporters mustered the bare minimum of 60 votes needed to shut off debate. The bill passed the Senate, 60-40 on March 20, 2002, and was signed into law by President Bush on March 27, 2002. In signing the law, Bush expressed concerns about the constitutionality of parts of the legislation but concluded, "I believe that this legislation, although far from perfect, will improve the current financing system for Federal campaigns." The bill was the first significant overhaul of federal campaign finance laws since the post-Watergate scandal era. Academic research has used game theory to explain Congress's incentives to pass the Act.
The BCRA was a mixed bag for those who wanted to remove big money from politics. It eliminated all soft money donations to the national party committees, but it also doubled the contribution limit of hard money, from $1,000 to $2,000 per election cycle, with a built-in increase for inflation. In addition, the bill aimed to curtail ads by non-party organizations by banning the use of corporate or union money to pay for "electioneering communications," a term defined as broadcast advertising that identifies a federal candidate within 30 days of a primary or nominating convention, or 60 days of a general election. This provision of McCain-Feingold, sponsored by Maine Republican Olympia Snowe and Vermont Independent James Jeffords, as introduced applied only to for-profit corporations, but was extended to incorporate non-profit issue organizations, such as the Environmental Defense Fund or the National Rifle Association, as part of the "Wellstone Amendment," sponsored by Senator Paul Wellstone.
The law was challenged as unconstitutional by groups and individuals including the California State Democratic Party, the National Rifle Association, and Republican Senator Mitch McConnell (Kentucky), the Senate Majority Whip. After moving through lower courts, in September 2003, the U.S. Supreme Court heard oral arguments in the case, "McConnell v. FEC". On Wednesday, December 10, 2003, the Supreme Court issued a 5-4 ruling that upheld its key provisions.
Since then, campaign finance limitations continue to be challenged in the Courts. In 2005 in Washington state, Thurston County Judge Christopher Wickham ruled that media articles and segments were considered in-kind contributions under state law. The heart of the matter focused on the I-912 campaign to repeal a fuel tax, and specifically two broadcasters for Seattle conservative talker KVI. Judge Wickham's ruling was eventually overturned on appeal in April 2007, with the Washington Supreme Court holding that on-air commentary was not covered by the State's campaign finance laws (No New Gas Tax v. San Juan County).
In 2006, the United States Supreme Court issued two decisions on campaign finance. In "Federal Election Commission v. Wisconsin Right to Life, Inc.", it held that certain advertisements might be constitutionally entitled to an exception from the 'electioneering communications' provisions of McCain-Feingold limiting broadcast ads that merely mention a federal candidate within 60 days of an election. On remand, a lower court then held that certain ads aired by Wisconsin Right to Life in fact merited such an exception. The Federal Election Commission appealed that decision, and in June 2007, the Supreme Court held in favor of Wisconsin Right to Life. In an opinion by Chief Justice John Roberts, the Court declined to overturn the electioneering communications limits in their entirety, but established a broad exemption for any ad that could have a reasonable interpretation as an ad about legislative issues.
Also in 2006, the Supreme Court held that a Vermont law imposing mandatory limits on spending was unconstitutional, under the precedent of "Buckley v. Valeo". In that case, "Randall v. Sorrell", the Court also struck down Vermont's contribution limits as unconstitutionally low, the first time that the Court had ever struck down a contribution limit.
In March 2009, the U.S. Supreme Court heard arguments about whether or not the law could restrict advertising of a documentary about Hillary Clinton. "Citizens United v. Federal Election Commission" was decided in January 2010, the Supreme Court finding that §441b’s restrictions on expenditures were invalid and could not be applied to "".
DISCLOSE Act of 2010.
The DISCLOSE Act (S. 3628) was proposed in July 2010. The bill would have amended the Federal Election Campaign Act of 1971 to prohibit foreign influence in federal elections, prohibit government contractors from making expenditures with respect to such elections, and establish additional disclosure requirements for election spending. The bill would impose new donor and contribution disclosure requirements on nearly all organizations that air political ads independently of candidates or the political parties. The legislation would require the sponsor of the ad to appear in it and to take responsibility for it. President Obama argued that the bill would reduce foreign influence over American elections. Democrats needed at least one Republican to support the measure in order to get the 60 votes to overcome GOP procedural delays, but were unsuccessful.
Current proposals for reform.
Voting with dollars.
The voting with dollars plan would establish a system of modified public financing coupled with an anonymous campaign contribution process. It has two parts: patriot dollars and the secret donation booth. It was originally described in detail by Yale Law School professors Bruce Ackerman and Ian Ayres in their 2002 book "Voting with Dollars: A new paradigm for campaign finance". All voters would be given a $50 publicly funded voucher (Patriot dollars) to donate to federal political campaigns. All donations including both the $50 voucher and additional private contributions, must be made anonymously through the FEC. Ackerman and Ayres include model legislation in their book in addition to detailed discussion as to how such a system could be achieved and its legal basis.
Of the Patriot dollars (e.g. $50 per voter) given to voters to allocate, they propose $25 going to presidential campaigns, $15 to Senate campaigns, and $10 to House campaigns. Within those restrictions the voucher can be split among any number of candidates for any federal race and between the primary and general elections. At the end of the current election cycle any unspent portions of this voucher would expire and could not be rolled over to subsequent elections for that voter. In the context of the 2004 election cycle $50 multiplied by the approximately 120 million people who voted would have yielded about $6 billion in “public financing” compared to the approximate $4 billion spent in 2004 for all federal elections (House, Senate and Presidential races) combined. Ackerman and Ayres argue that this system would pool voter money and force candidates to address issues of importance to a broad spectrum of voters. Additionally they argue this public finance scheme would address taxpayers' concerns that they have "no say" in where public financing monies are spent, whereas in the Voting with dollars system each taxpayer who votes has discretion over their contribution.
Lessig (2011, p. 269) notes that the cost of this is tiny relative to the cost of corporate welfare, estimated at $100 billion in the 2012 US federal budget. However, this considers only direct subsidies identified by the Cato Institute. It ignores tax loopholes and regulatory and trade decisions, encouraging business mergers and other activities that can stifle competition, creativity and economic growth; the direct subsidies can be a tiny fraction of these indirect costs.
The second aspect of the system increases some private donation limits, but all contributions must be made anonymously through the FEC. In this system, when a contributor makes a donation to a campaign, they send their money to the FEC, indicating to which campaign they want it to go. The FEC masks the money and distributes it directly to the campaigns in randomized chunks over a number of days. Ackerman and Ayres compare this system to the reforms adopted in the late 19th century aimed to prevent vote buying, which led to our current secret ballot process. Prior to that time voting was conducted openly, allowing campaigns to confirm that voters cast ballots for the candidates they had been paid to support. Ackerman and Ayres contend that if candidates do not know for sure who is contributing to their campaigns they are unlikely to take unpopular stances to court large donors which could jeopardize donations flowing from voter vouchers. Conversely, large potential donors will not be able to gain political access or favorable legislation in return for their contributions since they cannot prove to candidates the supposed extent of their financial support.
Matching funds.
Another method allows the candidates to raise funds from private donors, but provides matching funds for the first chunk of donations. For instance, the government might "match" the first $250 of every donation. This would effectively make small donations more valuable to a campaign, potentially leading them to put more effort into pursuing such donations, which are believed to have less of a corrupting effect than larger gifts and enhance the power of less-wealthy individuals. Such a system is currently in place in the U.S. presidential primaries. As of February 2008, there were fears that this system provided a safety net for losers in these races, as shown by loan taken out by John McCain's campaign that used the promise of matching funds as collateral. However, in February 2009 the Federal Election Commission found no violation of the law because McCain permissibly withdrew from the Matching Payment Program and thus was released from his obligations. It also found no reason to believe that a violation occurred as a result of the Committee’s reporting of McCain’s loan. The Commission closed the files.
Clean elections.
Another method, which supporters call clean money, clean elections, gives each candidate who chooses to participate a certain, set amount of money. In order to qualify for this money, the candidates must collect a specified number of signatures and small (usually $5) contributions. The candidates are not allowed to accept outside donations or to use their own personal money if they receive this public funding. Candidates receive matching funds, up to a limit, when they are outspent by privately funded candidates, attacked by independent expenditures, or their opponent benefits from independent expenditures. This is the primary difference between clean money public financing systems and the presidential campaign system, which many have called "broken" because it provides no extra funds when candidates are attacked by 527s or other independent expenditure groups. Supporters claim that Clean Elections matching funds are so effective at leveling the playing field in Arizona that during the first full year of its implementation, disproportionate funding between candidates was a factor in only 2% of the races. The U.S. Supreme Court's decision in Davis v. Federal Election Commission, however, cast considerable doubt on the constitutionality of these provisions, and in 2011 the Supreme Court held that key provisions of the Arizona law – most notably its matching fund provisions – were unconstitutional in Arizona Free Enterprise Club's Free Enterprise Club PAC v. Bennett.
This procedure has been in place in races for all statewide and legislative offices in Arizona and Maine since 2000. Connecticut passed a Clean Elections law in 2005, along with the cities of Portland, Oregon and Albuquerque, New Mexico, although Portland's was repealed by voter initiative in 2010. 69% of the voters in Albuquerque voted Yes to Clean Elections. A 2006 poll showed that 85% of Arizonans familiar with their Clean Elections system thought it was important to Arizona voters. However, a clean elections initiative in California was defeated by a wide margin at the November 2006 election, with just 25.7% in favor, 74.3% opposed, and in 2008 Alaska voters rejected a clean elections proposal by a two to one margin.
Many other states (such as New Jersey) have some form of limited financial assistance for candidates, but New Jersey's experiment with Clean Elections was ended in 2008, in part due to a sense that the program failed to accomplish its goals. Wisconsin and Minnesota have had partial public funding since the 1970s, but the systems have largely fallen into desuetude.
A clause in the Bipartisan Campaign Reform Act of 2002 ("McCain-Feingold") required the nonpartisan General Accounting Office to conduct a study of clean elections programs in Arizona and Maine. The ensuing report, created by Trent Lott and Christopher Dodd, issued in May 2003, cautioned that "It is too soon to determine the extent to which the goals of Maine’s and Arizona’s public financing programs are being met... [and] We are not making any recommendations in this report." A 2006 study by the Center for Governmental Studies (an advocate for campaign finance reform) found that Clean Elections programs resulted in more candidates, more competition, more voter participation, and less influence-peddling. In 2008, a series of studies conducted by the Center for Competitive Politics, an organization whose mission is to "oppose so-called reformers’ efforts to limit campaign contributions [and] taxpayer funded political campaigns" found that the programs in Maine, Arizona, and New Jersey had failed to accomplish their stated goals, including electing more women, reducing government spending, reducing special interest influence on elections, bringing more diverse backgrounds into the legislature, or meeting most other stated objectives, including increasing competition or voter participation. These reports confirmed the results of an earlier study by the conservative/libertarian Goldwater Institute on Arizona's program.
Occupy movement-inspired constitutional amendments.
In response to the Occupy Wall Street protests and the worldwide occupy movement calling for U.S. campaign finance reform eliminating corporate influence in politics, among other reforms, Representative Ted Deutch introduced the "Outlawing Corporate Cash Undermining the Public Interest in our Elections and Democracy" (OCCUPIED) constitutional amendment on November 18, 2011. The OCCUPIED amendment would outlaw the use of for-profit corporation money in U.S. election campaigns and give Congress and states the authority to create a public campaign finance system. Unions and non-profit organizations will still be able to contribute to campaigns. On November 1, 2011, Senator Tom Udall also introduced a constitutional amendment in Congress to reform campaign finance which would allow Congress and state legislatures to establish public campaign finance. Two other constitutional campaign finance reform amendments were introduced in Congress in November, 2011. Similar amendments have been advanced by Dylan Ratigan, Karl Auerbach, Cenk Uygur through Wolf PAC, and others.
Harvard law professor and Creative Commons board member Lawrence Lessig had called for a constitutional convention in a September 24–25, 2011 conference co-chaired by the Tea Party Patriots' national coordinator, in Lessig's October 5 book, "Republic, Lost: How Money Corrupts Congress – and a Plan to Stop It", and at the Occupy protest in Washington, DC. Reporter Dan Froomkin said the book offers a manifesto for the Occupy Wall Street protestors, focusing on the core problem of corruption in both political parties and their elections, and Lessig provides credibility to the movement. Lessig's initial constitutional amendment would allow legislatures to limit political contributions from non-citizens, including corporations, anonymous organizations, and foreign nationals, and he also supports public campaign financing and electoral college reform to establish the one person, one vote principle. Lessig's web site allows anyone to propose and vote on constitutional amendments. On October 15, the Occupy Wall Street Demands Working Group, published the "99 Percent Declaration" of demands, goals, and solutions, including a call to amend the U.S. Constitution to reform campaign finance. Occupy movement protesters have joined the call for a constitutional amendment.
"Citizens United v. Federal Election Commission".
In "Citizens United v. Federal Election Commission", on Jan, 2010, the US Supreme Court ruled that corporations and unions can not constitutionally be prohibited from promoting the election of one candidate over another candidate.
Ruling.
Justice Kennedy's majority opinion found that the BCRA §203 prohibition of all independent expenditures by corporations and unions violated the First Amendment's protection of free speech. The majority wrote, "If the First Amendment has any force, it prohibits Congress from fining or jailing citizens, or associations of citizens, for simply engaging in political speech."
Justice Kennedy's opinion for the majority also noted that since the First Amendment (and the Court) do not distinguish between media and other corporations, these restrictions would allow Congress to suppress political speech in newspapers, books, television and blogs. The Court overruled "Austin v. Michigan Chamber of Commerce", 494 U.S. 652 (1990), which had held that a state law that prohibited corporations from using treasury money to support or oppose candidates in elections did not violate the First and Fourteenth Amendments. The Court also overruled that portion of "McConnell v. Federal Election Commission", 540 U.S. 93 (2003), that upheld BCRA's restriction of corporate spending on "electioneering communications". The Court's ruling effectively freed corporations and unions to spend money both on "electioneering communications" and to directly advocate for the election or defeat of candidates (although not to contribute directly to candidates or political parties).
The majority argued that the First Amendment protects associations of individuals as well as individual speakers, and further that the First Amendment does not allow prohibitions of speech based on the identity of the speaker. Corporations, as associations of individuals, therefore have speech rights under the First Amendment.
Dissent.
Justice Stevens, J. wrote, in partial dissent:
Justice Stevens also wrote: "The Court’s ruling threatens to undermine the integrity of elected institutions across the Nation. The path it has taken to reach its outcome will, I fear, do damage to this institution. Before turning to the question whether to overrule Austin and part of McConnell, it is important to explain why the Court should not be deciding that question."
Public response.
Senator McCain, one of the two original sponsors of campaign finance reform, noted after the decisions that "campaign finance reform is dead" – but predicted a voter backlash once it became obvious how much money corporations and unions now could and would pour into campaigns.
Mitch McConnell said "Our democracy depends upon free speech, not just for some but for all."
In a Washington Post-ABC News poll in early February 2010 it was found that roughly 80% of Americans were opposed to the January 2010 Supreme court's ruling. The poll reveals relatively little difference of opinion on the issue among Democrats (85 percent opposed to the ruling), Republicans (76 percent) and independents (81 percent). In response to the ruling, a grassroots, bipartisan group called Move to Amend was created to garner support for a constitutional amendment overturning corporate personhood and declaring that money is not speech.
"McCutcheon et al. v. Federal Election Commission".
On April 2, 2014, the Supreme Court issued a 5-4 ruling that the 1971 FECA's aggregate limits restricting how much money a donor may contribute in total to all candidates or committees violated the First Amendment. The controlling opinion was written by Chief Justice Roberts, and joined by Justices Scalia, Alito and Kennedy; Justice Thomas concurred in the judgment but wrote separately to argue that "all" limits on contributions were unconstitutional. Justice Breyer filed a dissenting opinion, joined by Justices Ginsburg, Kagan and Sotomayor. 
Gyrocopter Pilot Demands Campaign Finance Reform, April 15, 2015.
On April 15, 2015 Douglas Hughes, a mailman, from Ruskin, Florida flew a gyrocopter through protected airspace in Washington, DC and landed on the lawn of the U.S. capitol. The "Tampa Bay Times" reported, "At the root of Hughes' disdain is the Supreme Court's 2010 decision in "Citizens United vs. Federal Election Commission", in which the court decided campaign contributions were a form of "political speech" and struck down limits on how much corporations and unions could give to political contenders. He carried 535 letters—one for each member of Congress.

</doc>
<doc id="40558" url="http://en.wikipedia.org/wiki?curid=40558" title="United States presidential election, 1936">
United States presidential election, 1936

 Franklin D. Roosevelt
Franklin D. Roosevelt
The United States presidential election of 1936 was the 38th quadrennial presidential election, held on Tuesday, November 3, 1936. In terms of electoral votes, it was the most lopsided presidential election in the history of the United States. In terms of the popular vote, it was the second-biggest victory for the winner since the election of 1820, which was not seriously contested.
The election took place as the Great Depression entered its eighth year. Incumbent President and Democratic candidate Franklin D. Roosevelt was still working to push the provisions of his New Deal economic policy through Congress and the courts. However, the New Deal policies he had already enacted, such as Social Security and unemployment benefits, had proven to be highly popular with most Americans. Roosevelt's Republican opponent was Governor Alf Landon of Kansas, a political moderate.
Although some political pundits predicted a close race, Roosevelt went on to win the greatest electoral landslide since the beginning of the current two-party system in the 1850s. Roosevelt carried every state except Maine and Vermont, which together could only return 8 electoral votes.
By winning 523 electoral votes, Roosevelt received 98.49% of the electoral vote, the highest percentage since the uncontested election of 1820. Roosevelt also won the largest number of electoral votes ever recorded at that time, so far only surpassed by Ronald Reagan in the election of 1984, when there were 7 more electoral votes available to contest. In addition, Roosevelt won 60.8% of the national popular vote, the second highest popular-vote percentage won since 1820 (the highest percentage was won by Lyndon Johnson in 1964).
Nominations.
Republican Party nomination.
The 1936 Republican National Convention was held in Cleveland, Ohio, between June 9 and 12. Although many candidates sought the Republican nomination, only two, Governor Landon and Senator William Borah of Idaho, were considered to be serious candidates. While favorite sons County Attorney Earl Warren of California, Governor Warren Green of South Dakota, and Stephen A. Day of Ohio won their respective primaries, the 70-year-old Borah, a well-known progressive and "insurgent," won the Wisconsin, Nebraska, Pennsylvania, West Virginia, and Oregon primaries, while also performing quite strongly in Knox's Illinois and Green's South Dakota. The party machinery almost uniformly backed Landon, however, a wealthy businessman and centrist, who won primaries in Massachusetts and New Jersey and dominated in the caucuses and at state party conventions.
With Knox withdrawing as Landon's selection for vice-president (after the rejection of New Hampshire Governor Styles Bridges) and Day, Green, and Warren releasing their delegates, the tally at the convention was as follows:
Democratic Party nomination.
President Roosevelt faced only one primary opponent other than various favorite sons. Henry Skillman Breckinridge, an anti-New Deal lawyer from New York, filed to run against Roosevelt in four primaries. Breckinridge's challenge of the popularity of the New Deal among Democrats failed miserably. In New Jersey, President Roosevelt did not file for the preference vote and lost that primary to Breckinridge, even though he did receive 19% of the vote on write-ins. Roosevelt's candidates for delegates swept the race in New Jersey and elsewhere. In other primaries, Breckinridge's best showing was 15% in Maryland. Overall, Roosevelt received 93% of the primary vote, compared to 2% for Breckinridge.
The Democratic Party Convention was held in Philadelphia between July 23 and 27. The delegates unanimously re-nominated incumbents President Roosevelt and Vice-President John Nance Garner. At Roosevelt's request, the two-thirds rule, which had given the South a veto power, was repealed.
Other nominations.
Many people expected Huey Long, the colorful Democratic senator from Louisiana, to run as a third-party candidate with his "Share Our Wealth" program as his platform. However, he was assassinated in September 1935. Some historians, including Long biographer T. Harry Williams, contend that Long had never, in fact, intended to run for the presidency in 1936. Instead, he had been plotting with Father Charles Coughlin, a Catholic priest and populist talk radio personality, to run someone else on the soon-to-be-formed "Share Our Wealth" Party ticket. According to Williams, the idea was that this candidate would split the left-wing vote with President Roosevelt, thereby electing a Republican president and proving the electoral appeal of Share Our Wealth. Long would then wait four years and run for president as a Democrat in 1940.
Prior to Long's death, leading contenders for the role of the sacrificial 1936 candidate included Senators Burton K. Wheeler (D-Montana) and William Borah, and Governor Floyd B. Olson of the Minnesota Farmer–Labor Party. After the assassination, however, the two senators lost interest in the idea and Olson was diagnosed with terminal stomach cancer.
Father Coughlin, who had allied himself with Dr. Francis Townsend, a left-wing political activist who was pushing for the creation of an old-age pension system, and Rev. Gerald L. K. Smith, was eventually forced to run Congressman William Lemke (R-North Dakota) as the candidate of the newly created "Union Party". Lemke, who lacked the charisma and national stature of the other potential candidates, fared poorly in the election, barely managing 2% of the vote, and the party was dissolved the following year.
William Dudley Pelley, Chief of the Silver Shirts Legion, ran on the ballot for the Christian Party in Washington State, but won fewer than 2,000 votes.
General election.
Campaign.
The election was held on November 3, 1936.
This election is notable for "The Literary Digest" poll, which was based on 10 million questionnaires mailed to readers and potential readers; 2.3 million were returned. The "Literary Digest", which had correctly predicted the winner of the last 5 elections, announced in its October 31 issue that Landon would be the winner with 370 electoral votes. The cause of this mistake has often been attributed to improper sampling: more Republicans subscribed to the "Literary Digest" than Democrats, and were thus more likely to vote for Landon than Roosevelt. However, a 1976 article in "The American Statistician" demonstrates that the actual reason for the error was that the "Literary Digest" relied on voluntary responses. As the article explains, the 2.3 million "respondents who returned their questionnaires represented only that subset of the population with a relatively intense interest in the subject at hand, and as such constitute in no sense a random sample... it seems clear that the minority of anti-Roosevelt voters felt more strongly about the election than did the pro-Roosevelt majority." A more detailed study in 1988 showed that both the initial sample and non-response bias were contributing factors, and that the error due to the initial sample taken alone would not have been sufficient to predict the Landon victory. This mistake by the "Literary Digest" proved to be devastating to the magazine's credibility, and in fact the magazine went out of existence within a few months of the election.
That same year, George Gallup, an advertising executive who had begun a scientific poll, predicted that Roosevelt would win the election, based on a quota sample of 50,000 people. He also predicted that the "Literary Digest" would mis-predict the results. His correct predictions made public opinion polling a critical element of elections for journalists and indeed for politicians. The Gallup Poll would become a staple of future presidential elections, and remains one of the most prominent election polling organizations.
Results.
Roosevelt won by a landslide, carrying 46 of the 48 states and bringing in many additional Democratic members of Congress. After fellow Democrat Lyndon Johnson's 61.1% share of the popular vote in 1964, Roosevelt's 60.8% is the second-largest percentage in U.S. history since the nearly unopposed election of James Monroe in 1820, and his 98.5% of the electoral vote is the highest in two-party competition. Roosevelt won the largest number of electoral votes ever recorded at that time, so far only surpassed by Ronald Reagan in 1984, when 7 more electoral votes were available to contest. Landon became the second official major-party candidate since the current system was established to win fewer than ten electoral votes by tying fellow Republican William Howard Taft, who won 8 votes in his 1912 re-election campaign. No major-party candidate has won so few electoral votes since this election. The closest anyone has come was Reagan's 1984 opponent, Walter Mondale, who only won 13 electoral votes.
Of the 3,095 counties/independent cities making returns, Roosevelt won in 2,634 (85.11%) while Landon carried 461 (14.89%).
Some political pundits predicted the Republicans, whom many voters blamed for the Great Depression, would soon become an extinct political party. However, the Republicans would make a strong comeback in the 1938 congressional elections and would remain a potent force in Congress, although they were not able to win the presidency again until 1952.
The Electoral College results, in which Landon only won Maine and Vermont, inspired Democratic Party chairman James Farley, who had in fact declared during the campaign that FDR was to lose only these two states, to amend the then-conventional political wisdom of "As Maine goes, so goes the nation" into "As goes Maine, so goes Vermont." Additionally, a prankster posted a sign on Vermont's border with New Hampshire the day after the 1936 election, reading: "You are now leaving the United States." Some of Roosevelt's advisers even joked that America's fiscal woes might be best solved if he offered to sell Vermont and Maine to Canada.
As of 2012, even after many years as a classic "blue" state that usually supports Democratic presidential candidates, Vermont has voted for more Republican presidential nominees than any other state. From 1856 through 1960, Vermont gave the state's electoral votes to the Republican Party nominee in every presidential election. No other state has voted so many times in a row for candidates of the same political party. Maine once held a similar political record. From 1856 through 1960, Maine voted for the Republican candidate in every presidential election but one (in 1912, the state gave Democrat Woodrow Wilson a plurality with 39.43% of the vote). Another state that had been reliably Republican for a very long time up to 1936 was Pennsylvania. Roosevelt was the first Democrat to carry Pennsylvania since "favorite son" James Buchanan did so in 1856.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Close states.
Margin of victory less than 5% (4 electoral votes):
Margin of victory between 5% and 10% (29 electoral votes):

</doc>
<doc id="40559" url="http://en.wikipedia.org/wiki?curid=40559" title="United States presidential election, 1940">
United States presidential election, 1940

 Franklin D. Roosevelt
Franklin D. Roosevelt
The United States presidential election of 1940 was the 39th quadrennial presidential election, held on Tuesday, November 5, 1940. The election was fought in the shadow of World War II in Europe, as the United States was emerging from the Great Depression. Incumbent President Franklin D. Roosevelt, the Democratic candidate, broke with tradition and ran for a third term, which became a major issue. The surprise Republican candidate was maverick businessman Wendell Willkie, a dark horse who crusaded against Roosevelt's perceived failure to end the Depression and his supposed eagerness for war. Roosevelt, acutely aware of strong isolationist and non-interventionism sentiment, promised there would be no involvement in foreign wars if he were re-elected. Willkie conducted an energetic campaign and managed to revive Republican strength in areas of the Midwest and Northeast. However, Roosevelt won a comfortable victory by building strong support from labor unions, urban political machines, ethnic voters, and the traditionally Democratic Solid South, going on to become the first United States president in American history to be elected to a full third term.
Roosevelt won reelection in 1940 thanks to the cities. In the North the cities over 100,000 gave Roosevelt 60% of their votes, while the rest of the North favored Willkie 52%-48%. It was just enough to provide the critical electoral college margin.
Nominations.
Republican Party nomination.
In the months leading up to the opening of the 1940 Republican National Convention in Philadelphia, Pennsylvania, the Republican Party was deeply divided between the party's isolationists, who wanted to stay out of the war at all costs, and the party's interventionists, who felt that the United Kingdom and her allies needed to be given all aid short of war to prevent the Germans from conquering all of Europe. The three leading candidates for the Republican nomination were all isolationists to varying degrees. The three frontrunners were Senator Robert A. Taft of Ohio, Senator Arthur H. Vandenberg of Michigan, and District Attorney Thomas E. Dewey of New York. Taft was the leader of the conservative, isolationist wing of the Republican Party, and his main strength was in his native Midwest and parts of the South. Dewey, the District Attorney for Manhattan, had risen to national fame as the "Gangbuster" prosecutor who had sent numerous infamous Mafia figures to prison, most notably Lucky Luciano, the organized-crime boss of New York City. Dewey had won most of the presidential primaries in the spring of 1940, and he came into the Republican Convention in June with the largest number of delegate votes, although he was still well below the number needed to win. Vandenberg, the senior Republican in the Senate, was the "favorite son" candidate of the Michigan delegation and was considered a possible compromise candidate if Taft or Dewey faltered. Former President Herbert Hoover was also spoken of as a compromise candidate. However, each of these candidates had weaknesses that could be exploited. Taft's outspoken isolationism and opposition to any American involvement in the European war convinced many Republican leaders that he could not win a general election, particularly as France fell to the Nazis in May 1940 and Germany threatened Britain. Dewey's relative youth—he was only 38 in 1940—and lack of any foreign-policy experience caused his candidacy to weaken as the Nazi military emerged as a fearsome threat. In 1940, Vandenberg was also an isolationist (he would change his foreign-policy stance during World War II) and his lackadaisical, lethargic campaign never caught the voters' attention. Hoover still bore the stigma of having presided over the stock market crash of 1929 and the subsequent Great Depression. This left an opening for a dark horse candidate to emerge.
A Wall Street-based industrialist named Wendell Willkie, who had never before run for public office, emerged as the unlikely nominee. Willkie, a native of Indiana and a former Democrat who had supported Franklin Roosevelt in the 1932 presidential election, was considered an improbable choice. Willkie had first come to public attention as an articulate critic of Roosevelt's attempt to break up electrical power monopolies. Willkie was the chief executive officer of the Commonwealth & Southern Corporation, which provided electrical power to customers in eleven states. In 1933, President Roosevelt had created the Tennessee Valley Authority (TVA), which promised to provide flood control and cheap electricity to the impoverished people of the Tennessee River Valley. However, the government-run TVA would compete with Willkie's Commonwealth & Southern, and this led Willkie to criticize and oppose the TVA's attempt to compete with private power companies. Willkie argued that the government had unfair advantages over private corporations, and should thus avoid competing directly against them. However, Willkie did not dismiss all of Roosevelt's social welfare programs, indeed supporting those he believed could not be managed any better by the free enterprise system. Furthermore, unlike the leading Republican candidates, Willkie was a forceful and outspoken advocate of aid to the Allies, especially Britain. His support of giving all aid to the British "short of declaring war" won him the support of many Republicans on the East Coast, who disagreed with their party's isolationist leaders in Congress. Willkie's persuasive arguments impressed these Republicans, who believed that he would be an attractive presidential candidate. Many of the leading press barons of the era, such as Ogden Reid of the "New York Herald Tribune", Roy Howard of the Scripps-Howard newspaper chain and John and Gardner Cowles, publishers of the "Minneapolis Star" and the "Minneapolis Tribune", as well as "The Des Moines Register" and "Look" magazine, supported Willkie in their newspapers and magazines. Even so, Willkie remained a long-shot candidate; the May 8 Gallup Poll showed Dewey at 67% support among Republicans, followed by Vandenberg and Taft, with Willkie at only 3%.
The German Army's rapid blitz into France in May 1940 shook American public opinion, even as Taft was telling a Kansas audience that America needed to concentrate on domestic issues to prevent Roosevelt from using the war crisis to extend socialism at home. Both Dewey and Vandenberg also continued to oppose any aid to Britain that might lead to war with Germany. Nevertheless, sympathy for the embattled British was mounting daily, and this aided Willkie's candidacy. By mid-June, little over one week before the Republican Convention opened, the Gallup poll reported that Willkie had moved into second place with 17%, and that Dewey was slipping. Fueled by his favorable media attention, Willkie's pro-British statements won over many of the delegates. As the delegates were arriving in Philadelphia, Gallup reported that Willkie had surged to 29%, Dewey had slipped five more points to 47%, and Taft, Vandenberg and Hoover trailed at 8%, 8%, and 6% respectively.
Hundreds of thousands, perhaps as many as one million, telegrams urging support for Willkie poured in, many from "Willkie Clubs" that had sprung up across the country. Millions more signed petitions circulating everywhere. At the 1940 Republican National Convention itself, keynote speaker Harold Stassen, the Governor of Minnesota, announced his support for Willkie and became his official floor manager. Hundreds of vocal Willkie supporters packed the upper galleries of the convention hall. Willkie's amateur status and fresh face appealed to delegates as well as voters. Most of the delegations were selected not by primaries, but by party leaders in each state, and they had a keen sense of the fast-changing pulse of public opinion. Gallup found the same thing in polling data not reported until after the convention: Willkie had moved ahead among Republican voters by 44% to only 29% for the collapsing Dewey. As the pro-Willkie galleries repeatedly yelled "We Want Willkie," the delegates on the convention floor began their vote. Dewey led on the first ballot, but steadily lost strength thereafter. Both Taft and Willkie gained in strength on each ballot, and by the fourth ballot it was obvious that either Willkie or Taft would be the nominee. The key moments came when the delegations of large states such as Michigan, Pennsylvania, and New York left Dewey and Vandenberg and switched to Willkie, giving him the victory on the sixth ballot. The voting went like this:
[Table source: Richard C. Bain and Judith H. Parris, "Convention Decisions and Voting Records" (1973), pp. 254–256.]
Willkie's nomination is still considered by historians to have been one of the most dramatic moments in any political convention. Having given little thought to whom he would select as his vice-presidential nominee, Willkie left the decision to convention chairman and Massachusetts Congressman Joseph Martin, the House Minority Leader, who suggested Senate Minority Leader Charles L. McNary of Oregon. Despite the fact that McNary had spearheaded a "Stop Willkie" campaign late in the balloting, the candidate picked him to be his running mate.
Democratic Party nomination.
Democratic candidates:
Throughout the winter, spring, and summer of 1940, there was much speculation as to whether Roosevelt would break with longstanding tradition and run for an unprecedented third term. The two-term tradition, although not yet enshrined in the U.S. Constitution, had been established by President George Washington when he refused to run for a third term in 1796. Roosevelt refused to give a definitive statement as to his willingness to be a candidate again, and he even indicated to some ambitious Democrats, such as James Farley, that he would not run for a third term and that they could seek the Democratic nomination. However, as Nazi Germany swept through Western Europe and menaced the United Kingdom in the spring and summer of 1940, Roosevelt decided that only he had the necessary experience and skills to see the nation safely through the Nazi threat. He was aided by the party's political bosses, who feared that no Democrat except Roosevelt could defeat the popular Willkie.
At the July 1940 Democratic Convention in Chicago, Illinois, Roosevelt easily swept aside challenges from Farley and John Nance Garner, his Vice-President. Garner was a Texas conservative who had turned against Roosevelt in his second term because of his liberal economic and social policies. As a result, Roosevelt decided to pick a new running mate, Henry A. Wallace of Iowa, his Secretary of Agriculture and an outspoken liberal. That choice was strenuously opposed by many of the party's conservatives, who felt Wallace was too radical and "eccentric" in his private life to be an effective running mate (he practiced New Age spiritual beliefs, and often consulted with the controversial Russian spiritual guru Nicholas Roerich). But Roosevelt insisted that without Wallace on the ticket he would decline re-nomination, and when First Lady Eleanor Roosevelt came to Chicago to vouch for Wallace, he won the vice-presidential nomination with 626 votes to 329 for House Speaker William B. Bankhead of Alabama.
General election.
Polling.
The Gallup Poll accurately predicted the election outcome, but the American Institute of Public Opinion, responsible for the Gallup Poll, avoided predicting the outcome citing a four percent margin of error. Polls also indicated that, if there was no war in Europe, voters preferred Willkie over Roosevelt.
The fall campaign.
Willkie crusaded against Roosevelt's attempt to break the two-term presidential tradition, arguing that "if one man is indispensable, then none of us is free." Even some Democrats who had supported Roosevelt in the past disapproved of his attempt to win a third term, and Willkie hoped to win their votes. Willkie also criticized what he claimed was the incompetence and waste in Roosevelt's New Deal welfare programs. He stated that as president he would keep most of Roosevelt's government programs, but would make them more efficient. However, many Americans still blamed business leaders for the Great Depression, and the fact that Willkie symbolized "Big Business" hurt him with many working-class voters. Willkie was a fearless campaigner; he often visited industrial areas where Republicans were still blamed for causing the Great Depression and where Roosevelt was highly popular. In these areas, Willkie frequently had rotten fruit and vegetables thrown at him and was heckled by crowds; still, he was unfazed.
Willkie also accused Roosevelt of leaving the nation unprepared for war, but Roosevelt's military buildup and transformation of the nation into the "arsenal of democracy" removed the "unpreparedness" charge as a major issue. Willkie then reversed his approach and charged Roosevelt with secretly planning to take the nation into World War II. This accusation did cut into Roosevelt's support. In response, Roosevelt, in a pledge that he would later regret, promised that he would "not send American boys into any foreign wars."
Results.
Roosevelt led in all pre-election opinion polls by various margins. On Election Day—November 5, 1940—Roosevelt received 27.3 million votes to Willkie's 22.3 million, and in the Electoral College, Roosevelt defeated Willkie by a margin of 449 to 82. Willkie did get over six million more votes than the Republican nominee in 1936, Alf Landon, and he ran strong in rural areas in the American Midwest, taking over 57% of the farm vote. Roosevelt, meanwhile, carried every American city with a population of more than 400,000 except Cincinnati, Ohio. Of the 106 cities with more than 100,000 population, Roosevelt won 61 percent of the votes cast; in the South as a whole, he won 73 percent Of the total vote. In the remainder of the country (the rural and small-town North), Willkie had a majority of 53 percent. In the cities, there was a class differential, with the white-collar and middle-class voters supporting Republican candidate, and working class, blue-collar voters going for FDR. In the North, Roosevelt won 87 percent of the Jewish vote, 73 percent of the Catholics, and 61 percent of the nonmembers, while all the major Protestant denominations showed majorities for Willkie. 
Of the 3,094 counties/independent cities, Roosevelt won in 1,947 (62.93%) while Willkie carried 1,147 (37.07%).
As a result of Willkie's gains, Roosevelt became the second of only three presidents in United States history to win re-election with a lower percentage of both the electoral vote and the popular vote than in the prior election, preceded by James Madison in 1812 and followed by Barack Obama in 2012. Andrew Jackson in 1832 and Grover Cleveland in 1892 received more electoral votes but fewer popular votes, while Woodrow Wilson in 1916 received more popular votes but fewer electoral votes.
Source (Popular Vote): Leip, David. . ' ().Source (Electoral Vote): . '. ().
Close states.
Margin of victory less than 5% (211 electoral votes):
Margin of victory between 5% and 10% (83 electoral votes):

</doc>
<doc id="40560" url="http://en.wikipedia.org/wiki?curid=40560" title="United States presidential election, 1944">
United States presidential election, 1944

 Franklin D. Roosevelt
Franklin D. Roosevelt
The United States presidential election of 1944 was the 40th quadrennial presidential election, held on Tuesday, November 7, 1944. Incumbent President Franklin D. Roosevelt sought his fourth term in office; he was challenged by Republican Thomas E. Dewey.
The election was set in the backdrop of World War II, which was going well for the United States and its Allies. Roosevelt had already served longer than any other president, but remained popular. Unlike in 1940, there was little doubt that he would run for another term as the Democratic candidate. Dewey, the Governor of New York, campaigned against the New Deal and for a smaller government, but was ultimately unsuccessful in convincing the country to change course. Rumors of Roosevelt's ill health, though somewhat dispelled by his vigorous campaigning, proved to be prescient; Roosevelt would die and be replaced by his new Vice President, Harry S. Truman, within a half-year of winning re-election.
Nominations.
Democratic Party nomination.
Democratic candidates:
Candidates gallery.
President Roosevelt was a popular, war-time incumbent and faced little formal opposition. Although many Southern Democrats distrusted Roosevelt's racial policies, he brought enormous war activities to the region and the end of its marginal status was in sight. No major figure opposed Roosevelt publicly, and he was re-nominated easily when the Democratic Convention met in Chicago. Some pro-segregationist delegates tried to unite behind Virginia senator Harry F. Byrd, but he refused to actively campaign against Roosevelt, and did not get enough delegates to seriously threaten the President's chances.
The obvious physical decline in the president's appearance, as well as rumors of secret health problems, led many delegates and party leaders to oppose Vice President Henry A. Wallace strongly for a second term. Opposition to Wallace came especially from Catholic leaders in big cities and labor unions. Wallace, who was Roosevelt's vice president since January 1941, was regarded by most conservatives as being too left-wing and personally eccentric to be next in line for the presidency. He had performed so poorly as economic coordinator that Roosevelt had to remove him from that post. Numerous party leaders privately sent word to Roosevelt that they would fight Wallace's re-nomination as vice president and proposed instead Senator Harry S. Truman, a moderate from Missouri. Truman was highly visible as the chairman of a Senate wartime committee investigating fraud and inefficiency in the war program. Roosevelt, who personally liked Wallace and knew little about Truman, reluctantly agreed to accept Truman as his running mate to preserve party unity. Even so, many delegates on the left refused to abandon Wallace, and they cast their votes for him on the first ballot. However, enough large Northern, Midwestern, and Southern states supported Truman to give him victory on the second ballot. The fight over the vice presidential nomination proved to be consequential; Roosevelt died in April 1945, and Truman became the nation's 33rd President instead of Wallace.
Source: Richard C. Bain & Judith H. Parris, "Convention Decisions and Voting Records" (Washington DC: The Brookings Institution, 1973), pp. 266–267.
Republican Party.
Republican candidates:
As 1944 began, the frontrunners for the Republican nomination appeared to be Wendell Willkie, the party's 1940 candidate, Senator Robert A. Taft of Ohio, the leader of the party's conservatives, New York Governor Thomas E. Dewey, the leader of the party's moderate eastern establishment, General Douglas MacArthur, then serving as an Allied commander in the Pacific theater of the war, and former Minnesota Governor Harold Stassen, then serving as a U.S. naval officer in the Pacific. Taft surprised many by announcing that he was not a candidate; instead, he voiced his support for a fellow conservative, Governor John W. Bricker of Ohio. With Taft out of the race some Republican conservatives favored General MacArthur. However, MacArthur's chances were limited by the fact that he was leading Allied forces against Japan, and thus could not campaign for the nomination. His supporters entered his name in the Wisconsin primary nonetheless. The Wisconsin primary proved to be the key contest, as Dewey won by a surprisingly wide margin. He took 14 delegates to four for Harold Stassen, while MacArthur won the three remaining delegates. Willkie was shut out in the Wisconsin primary; he did not win a single delegate. His unexpectedly poor showing in Wisconsin forced him to withdraw as a candidate for the nomination. At the 1944 Republican National Convention in Chicago, Illinois, Dewey easily overcame Bricker and was nominated for president on the first ballot. In a bid to maintain party unity, Dewey, a moderate, chose the conservative Bricker as his running mate. Bricker was nominated for vice president by acclamation.
General election.
The Fall Campaign.
The Republicans campaigned against the New Deal, seeking a smaller government and less-regulated economy as the end of the war seemed in sight. Nonetheless, Roosevelt's continuing popularity was the main theme of the campaign. To quiet rumors of his poor health, Roosevelt insisted on making a vigorous campaign swing in October and rode in an open car through city streets. A high point of the campaign occurred when Roosevelt, speaking to a meeting of labor union leaders, gave a speech carried on national radio in which he ridiculed Republican claims that his administration was corrupt and wasteful with tax money. He particularly ridiculed a Republican claim that he had sent a US Navy warship to pick up his Scottish Terrier Fala in Alaska, noting that "Fala was furious" at such rumors. The speech was met with loud laughter and applause from the labor leaders. In response, Dewey gave a blistering partisan speech in Oklahoma City, Oklahoma, a few days later on national radio, in which he accused Roosevelt of being "indispensable" to corrupt big-city Democratic organizations and American Communists; he also referred to members of Roosevelt's cabinet as a "motley crew". However, American battlefield successes in Europe and the Pacific during the campaign, such as the liberation of Paris in August 1944 and the successful Battle of Leyte Gulf in the Philippines in October 1944, made Roosevelt unbeatable.
Results.
Throughout the campaign, Roosevelt led Dewey in all the polls by various margins. On election day, Roosevelt scored a fairly comfortable victory over Dewey. Roosevelt took 36 states for 432 electoral votes, while Dewey won 12 states and 99 electoral votes (266 were needed to win). In the popular vote Roosevelt won 25,612,916 votes to Dewey's 22,017,929.
The important question had been which leader, Roosevelt or Dewey, should be chosen for the critical days of war and the making of peace. A majority of the American people concluded that they should not change from one group and particularly from one leader. They also felt that in view of ever-increasing domestic disagreements it was not safe to do so in "wartime".
However, Roosevelt's percentage of the total vote was less than in 1940, and his margin of the total vote was less. Dewey did better against Roosevelt than any of Roosevelt's previous three Republican opponents, and he did have the personal satisfaction of beating Roosevelt in his hometown of Hyde Park, New York, and of winning Truman's hometown of Independence, Missouri. Dewey would again be the Republican presidential nominee in 1948 and would again lose, though by a much smaller margin.
Of the 3,095 counties/independent cities making returns, Roosevelt won in 1,751 (56.58%) while Dewey carried 1,343 (43.39%). The Texas Regular ticket carried one county (0.03%).
In New York, only the support of the American Labor and Liberal parties (pledged to Roosevelt but otherwise independent of the Democrats so as to keep separate their identity) enabled Roosevelt to win the electoral votes of his home state.
In 1944, the always existent and constantly growing Southern protest against Roosevelt leadership became clearest in Texas, where 135,553 votes were cast against Roosevelt but not for the Republican ticket. The Texas Regular ticket resulted from a split in the Democratic party in its two state conventions, May 23 and September 12 1944. This ticket represented the Democratic element opposing the re-election of President Roosevelt. Its Electors were uninstructed.
As he had in 1940, Roosevelt won re-election with a lower percentage of both the electoral vote and the popular vote than he had received in the prior elections—the second of only three presidents in US history to do so, preceded by James Madison in 1812 and followed by Barack Obama in 2012. Andrew Jackson in 1832 and Grover Cleveland in 1892 had received more electoral votes but fewer popular votes, while Woodrow Wilson in 1916 had received more popular votes but fewer electoral votes.
Source (Popular Vote): Leip, David. . ' ().Source (Electoral Vote): . '. ().
Close states.
Margin of victory less than 5% (190 electoral votes):
Margin of victory between 5% and 10% (138 electoral votes):

</doc>
<doc id="40562" url="http://en.wikipedia.org/wiki?curid=40562" title="United States presidential election, 1948">
United States presidential election, 1948

 Harry S. Truman
Harry S. Truman
The United States presidential election of 1948 was the 41st quadrennial presidential election, held on Tuesday, November 2, 1948. Incumbent President Harry S. Truman, the Democratic nominee, who had succeeded to the presidency after the death of President Franklin D. Roosevelt in 1945, successfully ran for election for a full term against Thomas E. Dewey, the Republican nominee.
The election is considered to be the greatest election upset in American history. Virtually every prediction (with or without public opinion polls) indicated that Truman would be defeated by Dewey. Both parties had severe ideological splits, with the far left and far right of the Democratic Party running third-party campaigns. Truman's surprise victory was the fifth consecutive presidential win for the Democratic Party, the longest winning streak in the history of the party, and second-longest in the history of both modern parties (surpassed only by the Republicans' six consecutive victories from 1860 to 1880). With simultaneous success in the 1948 congressional elections, the Democrats regained control of both houses of Congress, which they had lost in 1946. Truman's feisty campaign style energized his base of traditional Democrats, most of the white South, Catholic, and Jewish voters, and—in a surprise—Midwestern farmers. Thus, Truman's election confirmed the Democratic Party's status as the nation's majority party, a status it would retain until the conservative realignment in 1968.
Nominations.
Republican Party nomination.
As would be experienced by the Democrats, there was a boom for General Dwight D. Eisenhower, the most popular general of World War II and a favorite in the polls. Unlike the latter movement within the Democratic party, however, the Republican draft movement came largely from the grassroots of the party. By January 23, 1948, the grassroots movement had successfully entered Eisenhower's name into every state holding a Republican presidential primary, and polls gave him a significant lead against all other contenders. With the first state primary approaching, Eisenhower was forced to make a quick decision. Stating that soldiers should keep out of politics, Eisenhower declined to run and requested that the grassroots draft movement cease its activities. After a number of failed efforts to get Eisenhower to reconsider, the organization disbanded, with the majority of its leadership endorsing the presidential campaign of the former Governor of Minnesota, Harold Stassen.
With Eisenhower refusing to run, the contest for the Republican nomination was between Stassen, New York Governor Thomas E. Dewey, Ohio Senator Robert A. Taft, California Governor Earl Warren, General Douglas MacArthur, and Arthur H. Vandenberg of Michigan, the senior Republican in the Senate. Dewey, who had been the Republican nominee in 1944, was regarded as the frontrunner when the primaries began. Dewey was the acknowledged leader of the Republican Party's eastern establishment. In 1946 he had been re-elected governor of New York by the largest margin in state history. Dewey's handicap was that many Republicans disliked him on a personal level; he often struck observers as cold, stiff, and calculating. Taft was the leader of the Republican Party's conservative wing, which was strongest in the Midwest and parts of the South. Taft called for abolishing many New Deal welfare programs, which he felt were harmful to business interests, and he was skeptical of American involvement in foreign alliances such as the United Nations. Taft had two major weaknesses: He was a plodding, dull campaigner, and he was viewed by most party leaders as being too conservative and controversial to win a presidential election.
Both Vandenberg and Warren were highly popular in their home states, but each refused to campaign in the primaries, which limited their chances of winning the nomination. Their supporters, however, hoped that in the event of a Dewey-Taft-Stassen deadlock, the convention would turn to their man as a compromise candidate.
General MacArthur, the famous war hero, was especially popular among conservatives. Since he was serving in Japan as the Supreme Commander of the Allied Powers occupying that nation, he was unable to campaign for the nomination. He did make it known, however, that he would not decline the GOP nomination if it were offered to him, and some conservative Republicans hoped that by winning a primary contest he could prove his popularity with voters. They chose to enter his name in the Wisconsin primary.
The "surprise" candidate of 1948 was Stassen, a liberal from Minnesota. Stassen had been elected governor of Minnesota at the age of 31; he resigned as governor in 1943 to serve in the wartime Navy. In 1945 he served on the committee that created the United Nations. Stassen was widely regarded as the most liberal of the Republican candidates, yet during the primaries he was criticized for being vague on many issues. Stassen stunned Dewey and MacArthur in the Wisconsin primary; Stassen's surprise victory virtually eliminated General MacArthur, whose supporters had made a major effort on his behalf. Stassen defeated Dewey again in the Nebraska primary, thus making him the new frontrunner. He then made the strategic mistake of trying to beat Taft in Ohio, Taft's home state. Stassen believed that if he could defeat Taft in his home state, Taft would be forced to quit the race and most of Taft's delegates would support him instead of Dewey.
Taft defeated Stassen in his native Ohio, however, and Stassen earned the hostility of the party's conservatives. Even so, Stassen was still leading Dewey in the polls for the upcoming Oregon primary. Dewey, however, who realized that a defeat in Oregon would end his chances at the nomination, sent his powerful political organization into the state and spent large sums of money on campaign ads in Oregon. Dewey also agreed to debate Stassen in Oregon on national radio. Held on May 17, 1948, it was the first-ever radio debate between presidential candidates. The sole issue of the debate concerned whether to outlaw the Communist Party of the United States. Stassen, despite his liberal reputation, argued in favor of outlawing the party, while Dewey forcefully argued against it; at one point he famously stated that "you can't shoot an idea with a gun." Most observers rated Dewey as the winner of the debate, and four days later Dewey defeated Stassen in Oregon. From this point forward, the New York governor had the momentum he needed to win his party's second nomination.
Republican Convention.
The 1948 Republican National Convention was held in Philadelphia, Pennsylvania. It was the first presidential convention to be shown on national television. As the convention opened, Dewey was believed to have a large lead in the delegate count. His major opponents – Taft, Stassen, and Vandenberg – met in Taft's hotel suite to plan a "stop-Dewey" movement. A key obstacle soon developed, however, as the three men refused to unite behind a single candidate to oppose Dewey. Instead, all three men simply agreed to try to hold their own delegates in the hopes of preventing Dewey from obtaining a majority. This proved to be futile, as Dewey's efficient campaign team methodically gathered the remaining delegates they needed to win the nomination. After the second round of balloting, Dewey was only 33 votes short of victory. Taft then called Stassen and urged him to withdraw from the race and endorse him as Dewey's main opponent. When Stassen refused, Taft wrote a concession speech and had it read at the start of the third ballot; Dewey was then nominated by acclamation. Dewey chose popular Governor (and future Chief Justice) Earl Warren of California as his running mate. Following the convention, most political experts in the news media rated the Republican ticket as an almost-certain winner over the Democrats.
Democratic Party nomination.
Democratic candidates:
On July 12, the Democratic National Convention convened in Philadelphia in the same arena where the Republicans had met a few weeks earlier. Spirits were low; the Republicans had taken control of both houses of the United States Congress and a majority of state governorships during the 1946 mid-term elections, and the public opinion polls showed Truman trailing Republican nominee Dewey, sometimes by double digits. Furthermore, some liberal Democrats had joined Henry A. Wallace's new Progressive Party, and party leaders feared that Wallace would take enough votes from Truman to give the large Northern and Midwestern states to the Republicans.
Conservatives dominated the party in the South, and they were angered by the growing voice of northern labor unions in the party. The hope that Truman would reverse course had faded by 1947, when he vetoed the Taft-Hartley Law to control union power. Truman's appointment of a liberal civil rights commission convinced Southern conservatives that to re-establish their voice they had to threaten third party action to defeat Truman in 1948.
Truman was aware of his unpopularity. In July 1947 he offered to run as Eisenhower's running mate on the Democratic ticket if MacArthur won the Republican nomination, an offer which Eisenhower declined.
As a result of Truman's low standing in the polls, several Democratic party bosses began working to "dump" Truman and nominate a more popular candidate. Among the leaders of this movement were Jacob Arvey, the head of the powerful Cook County (Chicago) Democratic organization; Frank Hague, the boss of New Jersey; James Roosevelt, the eldest son of former President Franklin D. Roosevelt; and liberal Senator Claude Pepper of Florida. The rebels hoped to draft Eisenhower as the Democratic presidential candidate. On July 10, however, Eisenhower officially refused to be a candidate. There was then an attempt to put forward Supreme Court Justice William O. Douglas, but Douglas also declared that he would not be a presidential candidate. Finally, Senator Pepper declared his intention to challenge Truman for the presidential nomination. His candidacy collapsed when the liberal Americans for Democratic Action and the Congress of Industrial Organizations withheld their support, partly due to concerns over Pepper's attacks on Truman's foreign policy decisions regarding the Soviet Union. As a result of the refusal by most of the dump-Truman delegates to support him, Pepper withdrew his candidacy for the nomination on July 14. Lacking a candidate acceptable to all sides, the leaders of the dump-Truman movement reluctantly agreed to support Truman for the nomination.
Democratic Convention.
At the Democratic Convention, Truman initially proposed a civil rights plank that moderated the radical support he had expressed at the NAACP in 1947 and to Congress in February 1948, with language that placed civil rights in the context of the Constitution. This proposal disappointed Northern liberals who wanted radical and swift reform, but also failed to placate the vigorous opposition from Southern conservatives, and other proposals emerged. Former Texas Governor Dan Moody proposed a plank that instead supported the status quo of states' rights; a similar but shorter proposal was made by Cecil Sims of the Tennessee delegation. On the liberal side, Wisconsin Representative Andrew Biemiller proposed a strong civil rights plank which was more explicit and direct in its language than Truman's convention proposal. Minneapolis Mayor Hubert Humphrey led the support for the Biemiller plank. In his speech, Humphrey memorably stated that "the time has come for the Democratic Party to get out of the shadow of states' rights and walk forthrightly into the bright sunshine of human rights!"
Truman and his staff knew it was highly likely that any civil rights plank would lead to Southern delegates staging a walk-out in protest, but Truman believed that civil rights was an important moral cause and ultimately abandoned his advisers' attempts to "soften the approach" with the moderate plank; so the President supported and defended the "Crackpot" Biemiller plank, which passed by 651.5 votes to 582.5. It also received strong support from many of the big-city party bosses, most of whom felt that the civil rights platform would encourage the growing black population in their cities to vote for the Democrats. The passage of the civil rights platform caused some three dozen Southern delegates, led by South Carolina Governor Strom Thurmond, to walk out of the convention. The Southern delegates who remained nominated Senator Richard Russell, Jr., of Georgia for the Democratic nomination as a rebuke to Truman. Nonetheless, 947 Democratic delegates voted for Truman as the Democratic nominee, while Russell received only 266 votes, all from the South.
Truman's first choice for his running mate was Supreme Court Justice William O. Douglas, hoping that it might make the ticket more appealing to liberals. Douglas refused. Needing an alternative, Truman then selected Kentucky Senator Alben W. Barkley, who had delivered the convention's keynote address, as his running mate, with this nomination being made by acclamation.
Progressive Party nomination.
Meanwhile, the Democratic party fragmented. A new Progressive Party (the name had been used earlier by Theodore Roosevelt in 1912 and Robert M. La Follette in 1924), was created afresh in 1948, with the nomination of Henry A. Wallace, who had served as Secretary of Agriculture, Vice President of the United States, and Secretary of Commerce under Franklin D. Roosevelt. In 1946, President Truman had fired Wallace as Secretary of Commerce when Wallace publicly opposed Truman's firm moves to counter the Soviet Union in the Cold War. Wallace's 1948 platform opposed the Cold War policies of President Truman, including the Marshall Plan and Truman Doctrine. The Progressives proposed stronger government regulation and control of Big Business. They also campaigned to end discrimination against blacks and women, backed a minimum wage, and called for the elimination of the House Un-American Activities Committee, which was investigating the issue of communist spies within the U.S. government and labor unions. Wallace and his supporters believed that the committee was violating the civil liberties of government workers and labor unions.
The Progressives, however, also generated a great deal of controversy because of the widespread belief that they were secretly controlled by Communists who were more loyal to the Soviet Union than the United States. Wallace himself denied being a Communist, but he repeatedly refused to disavow their support and, at one point, was quoted as saying that the "Communists are the closest thing to the early Christian martyrs." Walter Reuther, the president of the influential United Auto Workers union, strongly opposed Wallace's candidacy, stating that "people who are not sympathetic with democracy in America are influencing him." Philip Murray, the president of the Congress of Industrial Organizations (CIO), stated in April 1948 that "the Communist Party is directly responsible for the creation of the third party [Progressive Party] in the United States."
Wallace was also hurt when Westbrook Pegler, a prominent conservative newspaper columnist, revealed that Wallace as Vice President had written coded letters discussing prominent politicians such as Franklin Roosevelt and Winston Churchill to his controversial Russian New Age spiritual guru Nicholas Roerich; the letters were nicknamed the "Guru letters." In his book "Out of the Jaws of Victory", the journalist Jules Abels wrote: "Personalities were referred to by symbolic titles—Roosevelt was 'The Flaming One', Churchill 'The Roaring Lion', and Cordell Hull 'The Sour One'... some of the letters were signed 'Wallace', others 'Galahad'", the name that Roerich had assigned Wallace in his cult. This revelation—including direct quotes from the letters—led to much ridicule of Wallace in the national press.
The Progressive Party Convention, which was also held in Philadelphia, was a highly contentious affair; several famous newspaper journalists, such as H. L. Mencken and Dorothy Thompson, publicly accused the Progressives of being covertly controlled by Communists. The party's platform was drafted by Lee Pressman, the convention secretary, he later admitted that he had been a member of the Communist party. John Abt served as legal counsel to the convention's permanent chairman, Albert Fitzgerald; he also testified years later that he was a Communist. Rexford Tugwell, a prominent liberal in President Franklin Roosevelt's New Deal, served as the Chairman of the party's platform committee. He became convinced that the party was being manipulated by Communists, and was "so heartsick about Communist infiltration of the party that he discussed . . . with his wife disaffiliating [from the party] the night before the convention" started. Tugwell later did disassociate himself from the Progressive Party and did not participate in Wallace's fall campaign. A number of other Progressive Party delegates and supporters would quit the party in protest over what they perceived as the undue influence Communists exerted over Wallace, including the prominent American socialist Norman Thomas. In the fall, Thomas would run as the Socialist Party presidential candidate to offer liberals a non-Communist alternative to Wallace.
Senator Glen H. Taylor of Idaho, an eccentric figure who was known as a "singing cowboy" and who had rode his horse "Nugget" up the steps of the United States Capitol after winning election to the Senate in 1944, was named as Wallace's running mate. After receiving the vice-presidential nomination, Taylor told reporters that there was a difference between "pink" Communists and "red" Communists. Taylor claimed that "pink" Communists would support the Wallace-Taylor ticket because they believed in a "peaceful revolution" to turn the government to left-wing beliefs, but "red" Communists would support the Republican ticket in the belief that they would cause another Great Depression, which would give Communists the chance to take over the government.
In the fall campaign the Wallace-Taylor ticket made a Southern tour, where both Wallace and Taylor insisted on speaking to racially integrated audiences, in defiance of Southern custom and law at the time. In several North Carolina cities Wallace was hit by a total of "twenty-seven eggs, thirty-seven tomatoes, six peaches, and two lemons." When he left the state he announced: "As Jesus Christ said, if at any time they will not listen to you willingly, then shake the dust off from your feet and go elsewhere." He ate only in unsegregated restaurants, traveled with a black secretary, and in Mississippi had to be escorted by police for protection. His aide Clark Foreman admitted that Wallace wanted to stir up controversy for the publicity it would receive in more liberal areas in the North and West. As the campaign progressed, however, Wallace's crowds thinned and his standing in the polls dropped. Wallace was hurt by the successful effort of labor unions to keep their members in the Democratic column, and by controversial statements from Progressives supporting "appeasement with Russia." Wallace himself attacked Winston Churchill as a "racist" and "imperialist", and Senator Taylor earned criticism for a speech in which he claimed that the "Nazis are running the US government. So why should Russia make peace with them? If I were a Russian . . . I would not agree to anything . . . we are aggressively preparing for war."
The Wallace-Taylor ticket finished in fourth place in the election, winning 1,157,328 votes (2.4%). This was slightly less than the States' Rights Party, but the Progressive Party received no electoral votes.
States' Rights Democratic Party nomination.
States' Rights Democratic candidates:
Southern Democrats had become increasingly disturbed over President Truman's support of civil rights, particularly following his executive order racially integrating the U.S. armed forces and a civil rights message he sent to Congress in February 1948. At the Southern Governor's Conference in Wakulla Springs, Florida, on February 6, Mississippi Governor Fielding Wright proposed the formation of a new third party to protect racial segregation in the South. On May 10, 1948, the governors of the eleven states of the former Confederacy, along with other high-ranking Southern officials, met in Jackson, Mississippi, to discuss their concerns about the growing civil rights movement within the Democratic Party. At the meeting, South Carolina Governor Strom Thurmond criticized President Truman for his civil rights agenda, and the governors discussed ways to oppose it.
The Southern Democrats who had walked out of the Democratic National Convention to protest the civil rights platform approved by the convention, and supported by Truman, promptly met at Municipal Auditorium in Birmingham, Alabama, on July 17, 1948, and formed yet another political party, which they named the States' Rights Democratic Party. More commonly known as the "Dixiecrats", the party's main goal was continuing the policy of racial segregation in the South and the Jim Crow laws that sustained it. Governor Thurmond, who had led the walkout, became the party's presidential nominee after the convention's initial favorite, Arkansas Governor Benjamin Laney withdrew his name from consideration. Governor Wright of Mississippi received the vice-presidential nomination. The Dixiecrats had no chance of winning the election themselves, since they could not get on the ballot in enough states to win the necessary electoral votes. Their strategy was to take enough Southern states from Truman to force the election into the United States House of Representatives under the provisions of the Twelfth Amendment, where they could then extract concessions from either Truman or Dewey on racial issues in exchange for their support. Even if Dewey won the election outright, the Dixiecrats hoped that their defection would show that the Democratic Party needed Southern support in order to win national elections, and that this fact would weaken the pro-civil rights movement among Northern and Western Democrats. The Dixiecrats were weakened, however, when most Southern Democratic leaders (such as Governor Herman Talmadge of Georgia and "Boss" E. H. Crump of Tennessee) refused to support the party. Despite being an incumbent president, Truman was not placed on the ballot in Alabama. In the states of Louisiana, Mississippi, Alabama, and South Carolina, the party was able to be labeled as the main Democratic Party ticket on the local ballots on election night. Outside of these four states, however, it was only listed as a third-party ticket.
General election.
The fall campaign.
Given Truman's sinking popularity and the seemingly fatal three-way split in the Democratic Party, Dewey appeared unbeatable. Top Republicans believed that all their candidate had to do to win was to avoid major mistakes; in keeping with this advice, Dewey carefully avoided risks. He spoke in platitudes, avoided controversial issues, and was vague on what he planned to do as president. Speech after speech was filled with non-political, optimistic assertions of the obvious, including the now infamous quote "You know that your future is still ahead of you." An editorial in "The (Louisville) Courier-Journal" summed it up as such: "No presidential candidate in the future will be so inept that four of his major speeches can be boiled down to these historic four sentences: Agriculture is important. Our rivers are full of fish. You cannot have freedom without liberty. Our future lies ahead." Truman, trailing in the polls, decided to adopt a slashing, no-holds-barred campaign. He ridiculed Dewey by name, criticized Dewey's refusal to address specific issues, and scornfully targeted the Republican-controlled 80th Congress with a wave of relentless and blistering partisan assaults. He nicknamed the Republican-controlled Congress as the "do-nothing" Congress, a remark which brought strong criticism from Republican Congressional leaders (such as Taft), but no comment from Dewey. In fact, Dewey rarely mentioned Truman's name during the campaign, which fit into his strategy of appearing to be above petty partisan politics.
Under Dewey's leadership, the Republicans had enacted a platform at their 1948 convention that called for expanding Social Security, more funding for public housing, civil rights legislation, and promotion of health and education by the federal government. These positions were, however, unacceptable to the conservative Congressional Republican leadership. Truman exploited this rift in the opposing party by calling a special session of Congress on "Turnip Day" (referring to an old piece of Missouri folklore about planting turnips in late July) and daring the Republican Congressional leadership to pass its own platform. The 80th Congress played into Truman's hands, delivering very little in the way of substantive legislation during this time. Truman simply ignored the fact that Dewey's policies were considerably more liberal than most of his fellow Republicans, and instead he concentrated his fire against what he characterized as the conservative, obstructionist tendencies of the unpopular 80th Congress.
Truman toured much of the nation with his fiery rhetoric, playing to large, enthusiastic crowds. "Give 'em hell, Harry" was a popular slogan shouted out at stop after stop along the tour. The polls and the pundits, however, all held that Dewey's lead was insurmountable, and that Truman's efforts were for naught. Indeed, Truman's own staff considered the campaign a last hurrah. Even Truman's own wife Bess had private doubts that her husband could win. The only person who appears to have considered Truman's campaign to be winnable was the president himself, who confidently predicted victory to anyone and everyone who would listen to him.
In the final weeks of the campaign, American movie theaters agreed to play two short newsreel-like campaign films in support of the two major-party candidates; each film had been created by its respective campaign organization. The Dewey film, shot professionally on an impressive budget, featured very high production values, but somehow reinforced an image of the New York governor as cautious and distant. The Truman film, hastily assembled on virtually no budget by the perpetually cash-short Truman campaign, relied heavily on public-domain and newsreel footage of the president taking part in major world events and signing important legislation. Perhaps unintentionally, the Truman film visually reinforced an image of him as engaged and decisive. Years later, historian David McCullough cited the expensive, but lackluster, Dewey film, and the far cheaper, but more effective, Truman film, as important factors in determining the preferences of undecided voters.
As the campaign drew to a close, the polls showed Truman was gaining. Though Truman lost all nine of the Gallup Poll's post-convention surveys, Dewey's Gallup lead dropped from 17 points in late September to 9 points in mid-October to just 5 points by the end of the month, just above the poll's margin of error. Although Truman was gaining momentum, most political analysts were reluctant to break with the conventional wisdom and say that a Truman victory was a serious possibility. The Roper Poll had suspended its presidential polling at the end of September, barring "some development of outstanding importance," which in their subsequent view, never occurred. Dewey was aware of his slippage, but was convinced by his advisors and family not to counterattack the Truman campaign.
In the campaign's final days, many newspapers, magazines, and political pundits were so confident of Dewey's impending victory they wrote articles to be printed the morning after the election speculating about the new "Dewey Presidency." "Life" magazine printed a large photo in its final edition before the election. Entitled "Our Next President Rides by Ferryboat over San Francisco Bay", the photo showed Dewey and his staff riding across the city's harbor. "Newsweek" polled fifty experts; all fifty predicted a Dewey win. Several well-known and influential newspaper columnists, such as Drew Pearson and Joseph Alsop, wrote columns to be printed the morning after the election speculating about Dewey's possible choices for his cabinet. Walter Winchell reported that gambling odds were 15 to 1 against Truman. More than 500 newspapers, accounting for over 78% of the nation's total circulation, endorsed Dewey. Truman picked up 182 endorsements, accounting for just 10% of America's newspaper readership, being surpassed by Thurmond, who got the remaining 12% from many Southern papers. Alistair Cooke, the distinguished writer for the "Manchester Guardian" newspaper in the United Kingdom, published an article on the day of the election entitled "Harry S. Truman: A Study of a Failure." For its television coverage, NBC News constructed a large cardboard model of the White House containing two elephants that would pop out when NBC announced Dewey's victory; since Truman's defeat was considered certain, no donkeys were placed in the White House model.
As Truman made his way to his hometown of Independence, Missouri to await the election returns, some among his inner circle had already accepted other jobs, and not a single reporter traveling on his campaign train thought that he would win.
Results.
On election night, Dewey, his family, and campaign staff confidently gathered in the Roosevelt Hotel in New York City to await the returns. Truman, aided by the Secret Service, sneaked away from reporters covering him in Kansas City and rode to nearby Excelsior Springs, Missouri. There he took a room in the historic Elms Hotel, had dinner and a Turkish bath, and went to sleep. As the votes came in, Truman took an early lead that he never lost. The leading radio commentators, such as H. V. Kaltenborn of NBC, still confidently predicted that once the "late returns" came in Dewey would overcome Truman's lead and win. At midnight, Truman awoke and turned on the radio in his room; he heard Kaltenborn announce that while Truman was still ahead in the popular vote, he could not possibly win. At 4 a.m. Truman awoke again, heard on the radio that his popular-vote lead was now nearly two million votes, and decided to ride back to Kansas City. For the rest of his life, Truman would gleefully mimic Kaltenborn's voice predicting his defeat throughout that election night. Dewey, meanwhile, realized that he was in trouble when early returns from New England and New York showed him running well behind his expected vote total. He stayed up for the rest of the night and early morning analyzing the votes as they came in. By 10:30 a.m. he was convinced that he had lost; at 11:14 a.m. he sent a gracious telegram of concession to Truman.
The "Chicago Daily Tribune", a pro-Republican newspaper, was so sure of Dewey's victory that on Tuesday afternoon, before any polls closed, it printed "DEWEY DEFEATS TRUMAN" as its headline for the following day. Part of the reason Truman's victory came as such a shock was because of as-yet uncorrected flaws in the emerging craft of public opinion polling. Evidence from recent elections falsely convinced pollsters that voters had already decided whom they would support by September. As a result many pollsters were so confident of Dewey's victory that they simply stopped polling voters weeks before the election, and thus missed a last-minute surge of support for the Democrats. It has been estimated that some 14% of Dewey's supporters switched to Truman in the final days before the election. After 1948, pollsters would survey voters until the day before the election: They would also announce their results on television, more or less in real time.
The key states in the 1948 election were Ohio, California, and Illinois. Truman narrowly won all three states by a margin of less than one percentage point apiece. These three states had a combined total of 78 electoral votes. Had Dewey carried all three states, he would have won the election in the Electoral College, while still losing the popular vote. Had Dewey won any two of the three states, the Dixiecrats would have succeeded in their goal of forcing the election into the House of Representatives. The extreme closeness of the vote in these three states was the major reason why Dewey waited until late on the morning of November 3 to concede. A similarly narrow margin garnered Idaho and Nevada's electoral votes for Truman. Dewey countered by almost as narrowly carrying New York and Pennsylvania, the states with the most electoral votes at the time, as well as his native state of Michigan, but this was too little to give him the election. Dewey would always believe that he lost the election because he lost the rural vote in the Midwest, which he had won in 1944 (the Kaltenborn predictions that Truman would joyously mock had, in fact, taken for granted that the "country vote" would go to Dewey).
Journalist Sidney Lubell found in his post-1948 survey of voters that Truman, not Dewey, seemed the safer, more conservative candidate to the "new middle class" that had developed over the previous 20 years. He wrote that "to an appreciable part of the electorate, the Democrats had replaced the Republicans as the party of prosperity" during and after the war. Lubell quoted a man who, when asked why he did not vote Republican after moving to the suburbs, answered "I own a nice home, have a new car and am much better off than my parents were. I've been a Democrat all my life. Why should I change?" Dewey's promise of a "great house cleaning" in Washington worried an Iowa minister who wanted to retain farm subsidies for parishioners. Worried about the consequences of another depression, he voted Democratic for the first time in his church's history. Truman received a record number of Catholic votes, exceeding even the Catholic support of Al Smith in 1928, in part because Wallace drew leftists away from the Democrats.
Other possible factors for Truman's victory included his aggressive, populist campaign style; Dewey's complacent, distant approach to the campaign, and his failure to respond to Truman's attacks; broad public approval of Truman's foreign policy, notably the Berlin Airlift of that year; and widespread dissatisfaction with the institution Truman labeled as the "do-nothing, good-for-nothing 80th Republican Congress." In addition, after suffering a relatively severe recession in 1946 and 1947 (in which real GDP dropped by 12% and inflation went over 15%), the economy began recovering throughout 1948, thus possibly motivating many voters to give Truman credit for the economic recovery.
1948 was a huge year for the Democrats, as the Democrats not only retained the presidency but also recaptured both houses of Congress. Furthermore, the two third parties did not hurt Truman nearly as much as expected. Thurmond's Dixiecrats carried only four Southern states, a lower total than predicted. The civil rights platform helped Truman win large majorities among black voters in the populous Northern and Midwestern states and may well have made the difference for Truman in states such as Illinois and Ohio. Wallace's Progressives received only 2.4% of the national popular vote, well below their expected vote total and slightly less than the Dixiecrats, and Wallace did not take as many liberal votes from Truman as many political pundits had predicted. Some analysts have even argued that the separate candidacies of Wallace and Thurmond were beneficial to Truman by removing the separate taints of communism and racism from the Democratic Party.
The 1948 election marked only the second time in American presidential election history that the winning candidate won despite losing Pennsylvania and New York (the first time being in 1916; later such elections included 1968, 2000 and 2004). This was also the last time a Democratic candidate won Arizona by a majority of the state's popular vote (Bill Clinton carried the state by a plurality in 1996). Truman became the first Democrat ever to be elected president without carrying Alabama, Louisiana, Mississippi, or South Carolina (the Dixiecrats' states). Truman was also the last Democratic presidential candidate to win without carrying Pennsylvania, Maryland, New York, or Delaware, and the 1948 election marked the last time that the Northeast acted as the "banner region" of the Republican Party. It contrasted with elections from across the world, as Truman was a war leader who managed to win re-election (Churchill and de Gaulle both left office shortly after the end of the war).
Source (Popular Vote): Leip, David. . ' ().Source (Electoral Vote): . '. ().
Results by state.
(a) "In New York, the Truman vote was a fusion of the Democratic and Liberal slates. There, Truman obtained 2,557,642 votes on the Democratic ticket and 222,562 votes on the Liberal ticket." 
(b) "In Mississippi, the Dewey vote was a fusion of the Republican and Independent Republican slates. There, Dewey obtained 2,595 votes on the Republican ticket and 2,448 votes on the Independent Republican ticket."
Close states.
Margin of victory less than 5% (269 electoral votes):
Margin of victory between 5% and 10% (59 electoral votes):

</doc>
<doc id="40563" url="http://en.wikipedia.org/wiki?curid=40563" title="United States presidential election, 1952">
United States presidential election, 1952

 Harry S. Truman
Dwight D. Eisenhower
The United States presidential election of 1952 was the 42nd quadrennial presidential election, held on Tuesday, November 4, 1952. Republican Dwight Eisenhower was the landslide winner, ending a string of Democratic wins that stretched back to 1932. He carried the Republican Party (GOP) to narrow control of the House and Senate. During this time, Cold War tension between the United States and the Soviet Union was at a high level, as was fear of communism in the US, epitomized by the campaign of McCarthyism. Foreign policy was a main issue in the race for the Republican nomination. The nation was polarized over the stalemated Korean War, and the extent of corruption in the federal government became a major issue as well. The economy was prosperous, and thus economic and social issues played little role in the campaign.
Incumbent President Harry S. Truman, who as early as 1950 had decided not to run, had decided to back current Illinois Governor Adlai Stevenson. President Truman, as he had in 1948, reached out to General Dwight D. Eisenhower to see if he had interest in heading the Democratic ticket. Eisenhower demurred at the time and then wound up heading the Republican ticket. The Democratic Party instead nominated Governor Adlai Stevenson of Illinois. Stevenson had gained a reputation in Illinois as an intellectual and eloquent orator, however had vacillated a great deal on whether he even wanted to run for the Presidency. President Truman had several meetings with Stevenson about the President's desire for Stevenson to become the standard bearer for the party. Truman became very frustrated with Stevenson and his high level of indecision before Stevenson actually committed to running. The Republican Party saw a contest between the internationalist and isolationist perspectives. Senator Robert A. Taft said that isolationism was dead, but he saw little role for the United States in the Cold War. Eisenhower the NATO commander and war hero narrowly defeated Taft, then crusaded against the Truman policies he blasted as "Korea, Communism and Corruption." Ike, as they called him, did well in all major demographic and regional groups outside the Deep South.
Nominations.
Republican Party.
Republican candidates:
The fight for the Republican nomination was between General Dwight D. Eisenhower, who became the candidate of the party's moderate eastern establishment; Senator Robert A. Taft of Ohio, the longtime leader of the Republican Party's conservative wing; Governor Earl Warren of California, who appealed to Western delegates and independent voters; and former Governor Harold Stassen of Minnesota, who still had a base of support in the Midwest.
The moderate Eastern Republicans were led by New York Governor Thomas E. Dewey, the party's presidential nominee in 1944 and 1948. The moderates tended to be interventionists who felt that America needed to fight the Cold War overseas and resist Soviet aggression in Europe and Asia; they were also willing to accept most aspects of the social welfare state created by the New Deal in the 1930s. The moderates were also concerned with ending the Republicans' losing streak in presidential elections; they felt that the personally popular Eisenhower had the best chance of beating the Democrats. For this reason, Dewey himself declined the notion of a third run for President, even though he still had a large amount of support within the party.
The conservative Republicans led by Taft were based in the Midwest and parts of the South. The conservatives wanted to abolish many of the New Deal welfare programs; in foreign policy they were often non-interventionists who believed that America should avoid alliances with foreign powers. Taft had been a candidate for the Republican nomination in 1940 and 1948, but had been defeated both times by moderate Republicans from New York (Dewey in 1948, and Wendell Wilkie in 1940). Taft, who was 62 when the campaign began, freely admitted that 1952 was his last chance to win the nomination, and this led his supporters to work hard for him. Taft's weakness, which he was never able to overcome, was the fear of many party bosses that he was too conservative and controversial to win a presidential election.
Warren, although highly popular in California, refused to campaign in the presidential primaries and thus limited his chances of winning the nomination. He did retain the support of the California delegation, and his supporters hoped that, in the event of an Eisenhower-Taft deadlock, Warren might emerge as a compromise candidate.
After being persuaded to run, Eisenhower scored a major victory in the New Hampshire primary, when his supporters wrote his name onto the ballot, giving him an upset victory over Taft. However, from there until the Republican Convention the primaries were divided fairly evenly between the two men, and by the time the convention opened the race for the nomination was still too close to call. Taft won the Nebraska, Wisconsin, Illinois, and South Dakota primaries, while Eisenhower won the New Jersey, Pennsylvania, Massachusetts, and Oregon primaries. Stassen and Warren only won their home states of Minnesota and California respectively, which effectively ended their chances of earning the nomination. General Douglas MacArthur also got ten delegates from various states (mostly Oregon), but had made it clear from early in the race that he had no interest in being nominated.
Republican Convention.
When the 1952 Republican National Convention opened in Chicago, Illinois, most political experts rated Taft and Eisenhower as neck-and-neck in the delegate vote totals. Eisenhower's managers, led by Dewey and Massachusetts Senator Henry Cabot Lodge Jr., accused Taft of "stealing" delegate votes in Southern states such as Texas and Georgia. They claimed that Taft's leaders in these states had unfairly denied delegate spots to Eisenhower supporters and put Taft delegates in their place. Lodge and Dewey proposed to evict the pro-Taft delegates in these states and replace them with pro-Eisenhower delegates; they called this proposal "Fair Play." Although Taft and his supporters angrily denied this charge, the convention voted to support Fair Play 658 to 548, and Taft lost many Southern delegates. Eisenhower also received two more boosts, firstly when several uncommitted state delegations, such as Michigan and Pennsylvania, decided to support him, and secondly when Stassen released his delegates and asked them to support Eisenhower, whose moderate policies he much preferred to those of Taft. The removal of many pro-Taft Southern delegates and the support of the uncommitted states decided the nomination in Eisenhower's favor.
However, the mood at the convention was one of the most bitter and emotional in American history. When Senator Everett Dirksen of Illinois, a Taft supporter, pointed at Dewey on the convention floor and accused him of leading the Republicans "down the road to defeat," mixed boos and cheers rang out from the delegates, and there were even fistfights between some Taft and Eisenhower delegates.
In the end, Eisenhower narrowly defeated Taft on the first ballot. To heal the wounds caused by the battle, he went to Taft's hotel suite and met with him. Taft issued a brief statement congratulating Eisenhower on his victory, but he was bitter about what he felt was the untrue "stealing delegates" charge, and he withheld his active support for Eisenhower for several weeks after the convention. In September 1952 Taft and Eisenhower met again at Morningside Heights in New York City, there Taft promised to support Eisenhower actively in exchange for Eisenhower agreeing to a number of requests. These included a demand that Eisenhower give Taft's followers a fair share of patronage positions if he won the election, and that Eisenhower agree to balance the federal budget and "fight creeping domestic socialism in every field." Eisenhower agreed to the terms, and Taft campaigned hard for the Republican ticket. In fact, Eisenhower and Taft agreed on most domestic issues; their disagreements were primarily in foreign policy.
Though there were initial suggestions that Warren could have earned the party's Vice Presidential slot for the second successive election if he were to withdraw and endorse Eisenhower, he ultimately chose not to do so. Eisenhower himself had been partial to giving the VP nod to Stassen, who had endorsed Eisenhower of his own accord and had generally similar political positions. The party bosses, however, were keen to find a running mate who could mollify Taft's supporters, as the schism between the moderate and conservative wings was so severe that in the worst case it could potentially lead to the conservatives bolting and running Taft as a third-party candidate.
The convention chose young Senator Richard Nixon of California as Eisenhower's running mate; it was felt that Nixon's credentials as a fierce campaigner and anti-Communist would be valuable. Nixon also had ties to both the Eastern moderates (Dewey was a strong supporter) and the conservative Taft wing of the party. As such, it was felt that he could help to reunite the party after the bruising primary and convention battles. Most historians now believe that Eisenhower's nomination was primarily due to the feeling that he was a "sure winner" against the Democrats; most of the delegates were conservatives who would probably have supported Taft if they felt he could have won the general election.
Despite not earning the Presidential or Vice Presidential nominations, Warren would subsequently be appointed as Chief Justice in October 1953, while Stassen would hold various positions within Eisenhower's administration.
The balloting at the Republican Convention went as follows:
Democratic Party.
Democratic candidates:
The expected candidate for the Democratic nomination was incumbent President Harry S. Truman. Since the newly passed 22nd Amendment did not apply to whomever was president at the time of its passage, he was eligible to run again. But Truman entered 1952 with his popularity plummeting, according to polls. The bloody and indecisive Korean War was dragging into its third year, Senator Joseph McCarthy's anti-Communist crusade was stirring public fears of an encroaching "Red Menace," and the disclosure of widespread corruption among federal employees (including some high-level members of Truman's administration) left Truman at a low political ebb. Polls showed that he had a 66% disapproval rating, a record only matched decades later by Richard Nixon and surpassed by George W. Bush.
Truman's main opponent was populist Tennessee Senator Estes Kefauver, who had chaired a nationally-televised investigation of organized crime in 1951 and was known as a crusader against crime and corruption. The Gallup poll of February 15 showed Truman's weakness: nationally Truman was the choice of only 36% of Democrats, compared with 21% for Kefauver. Among independent voters, however, Truman had only 18% while Kefauver led with 36%. In the New Hampshire primary, Kefauver upset Truman, winning 19,800 votes to Truman's 15,927 and capturing all eight delegates. Kefauver graciously said that he did not consider his victory "a repudiation of Administration policies, but a desire...for new ideas and personalities." Stung by this setback, Truman soon announced that he would not seek re-election (however, Truman insisted in his memoirs that he had decided not to run for reelection well before his defeat by Kefauver).
With Truman's withdrawal, Kefauver became the front-runner for the nomination, and he won most of the primaries. Other primary winners were Senator Hubert Humphrey, who won his home state of Minnesota, while Senator Richard Russell Jr., of Georgia won the Florida primary and U.S. diplomat W. Averell Harriman won West Virginia. However, most states still chose their delegates to the Democratic Convention via state conventions, which meant that the party bosses – especially the mayors and governors of large Northern and Midwestern states and cities – were able to choose the Democratic nominee. These bosses (including Truman) strongly disliked Kefauver; his investigations of organized crime had revealed connections between Mafia figures and many of the big-city Democratic political organizations. The party bosses thus viewed Kefauver as a maverick who could not be trusted, and they refused to support him for the nomination.
Instead, with Truman taking the lead, they began to search for other, more acceptable, candidates. However, most of the other candidates had a major weakness. Richard Russell had much Southern support, but his support of racial segregation and opposition to civil rights for Southern blacks led many liberal Northern and Midwestern delegates to reject him. Truman favored W. Averell Harriman of New York, but he had never held an elective office and was inexperienced in politics. Truman next turned to his vice-president, Alben W. Barkley, but at 74 he was rejected as being too old by labor union leaders.
Other minor or favorite son candidates included Oklahoma Senator Robert S. Kerr, Governor Paul A. Dever of Massachusetts, Senator Hubert Humphrey of Minnesota, and Senator J. William Fulbright of Arkansas.
One candidate soon emerged who seemingly had few political weaknesses: Governor Adlai Stevenson of Illinois. The grandson of former Vice-President Adlai E. Stevenson, he came from a distinguished family in Illinois and was well known as a gifted orator, intellectual, and political moderate. In the spring of 1952, Truman tried to convince Stevenson to take the presidential nomination, but Stevenson refused, stating that he wanted to run for re-election as Governor of Illinois. Yet Stevenson never completely took himself out of the race, and as the convention approached, many party bosses, as well as normally apolitical citizens, hoped that he could be "drafted" to run.
Democratic Convention.
The 1952 Democratic National Convention was held in Chicago in the same coliseum the Republicans had gathered in several weeks earlier. Since the convention was being held in his home state, Governor Stevenson – who still proclaimed that he was not a presidential candidate – was asked to give the welcoming address to the delegates. He proceeded to give a witty and stirring address that led his supporters to begin a renewed round of efforts to nominate him, despite his protests. After meeting with Jacob Arvey, the "boss" of the Illinois delegation, Stevenson finally agreed to enter his name as a candidate for the nomination. The party bosses from other large Northern and Midwestern states quickly joined in support. Kefauver led on the first ballot, but had far fewer votes than necessary to win. Stevenson gradually gained strength until he was nominated on the third ballot.
After the delegates nominated Stevenson, the convention then turned to selecting a vice-president. The main candidates for this position were Kefauver, Russell, Barkley, Senator John Sparkman, and Senator A. S. Mike Monroney. After narrowing it down to Senator Sparkman and Senator Monroney, President Truman and a small group of political insiders chose Sparkman, a conservative and segregationist from Alabama, for the nomination. The convention largely complied and nominated Sparkman as Stevenson's running mate, though nominations were made for two other candidates for the Vice Presidency, Vice Chairwoman of the Democratic National Committee India Edwards of California, and District Judge Sarah T. Hughes of Texas. Both withdrew their names in favor of Sparkman. Stevenson then delivered an eloquent acceptance speech in which he famously pledged to "talk sense to the American people."
The following table documents the balloting. Candidates are organized according to their highest total on any single ballot. The 1952 Democratic convention was the last one for either party that needed more than one ballot to select a Presidential nominee.
General election.
Campaign issues.
The Eisenhower campaign was one of the first presidential campaigns to make a major, concerted effort to win the female vote. Many of his radio and television commercials discussed topics such as education, inflation, ending the war in Korea, and other issues that were thought to appeal to women. The Eisenhower campaign made extensive use of female campaign workers. These workers made phone calls to likely Eisenhower voters, distributed "Ike" buttons and leaflets, and gave parties to build support for the GOP ticket in their neighborhoods. On election day Eisenhower won a solid majority of the female vote.
Eisenhower campaigned by attacking "Korea, Communism, and Corruption"—that is, what the Republicans regarded as the failures of the outgoing Truman administration to deal with these issues. The Eisenhower campaign accused the administration of neglecting Latin America and thus leading them into the arms of wily Communist agents waiting to exploit local misery and capitalize on any opening to communize the Americas. Charges that Soviet spies had infiltrated the government plagued the Truman Administration and also became a major campaign issue for Eisenhower. The Republicans blamed the Democrats for the military's failure to be fully prepared to fight in Korea; they accused the Democrats of "harboring" Communist spies within the federal government; and they blasted the Truman Administration for the numbers of officials who had been accused of various crimes.
In return, the Democrats criticized Senator Joseph McCarthy and other Republican conservatives as "fearmongers" who were recklessly trampling on the civil liberties of government employees.
Campaign.
Many Democrats were particularly upset when Eisenhower, on a scheduled campaign swing through Wisconsin, decided not to give a speech he had written criticizing McCarthy's methods, and then allowed himself to be photographed shaking hands with McCarthy as if he supported him. Truman, formerly friends with Eisenhower, never forgot what he saw as a betrayal; he had previously thought Eisenhower would make a good president, but said, "he has betrayed almost everything I thought he stood for."
Despite these mishaps, Eisenhower retained his enormous personal popularity from his leading role in World War II, and huge crowds turned out to see him around the nation. His campaign slogan, "I Like Ike," was one of the most popular in American history. Stevenson concentrated on giving a series of thoughtful speeches around the nation; he too drew large crowds. Although his style thrilled intellectuals and academics, some political experts wondered if he were speaking "over the heads" of most of his listeners, and they dubbed him an "egghead," based on his baldness and intellectual demeanor. Eisenhower maintained a comfortable lead in the polls throughout most of the campaign.
A notable event of the 1952 campaign concerned a scandal that emerged when Richard Nixon, Eisenhower's running mate, was accused by several newspapers of receiving $18,000 in undeclared "gifts" from wealthy donors. In reality, contributions were by design only from early supporters and limited to $1,000, with full accountability. Nixon, who had been accusing the Democrats of hiding crooks, suddenly found himself on the defensive. Eisenhower and his aides considered dropping Nixon from the ticket and picking another running mate.
Nixon saved his political career, however, with a dramatic half-hour speech, the "Checkers speech," on live television. In this speech, Nixon denied the charges against him, gave a detailed account of his modest financial assets, and offered a glowing assessment of Eisenhower's candidacy. The highlight of the speech came when Nixon stated that a supporter had given his daughters a gift – a dog named "Checkers" – and that he would not return it, because his daughters loved it. The "Checkers speech" led hundreds of thousands of citizens nationwide to wire the Republican National Committee urging the Republican Party to keep Nixon on the ticket, and Eisenhower stayed with him.
Both campaigns made use of television ads. A notable ad for Eisenhower was an issue-free, feel-good animated cartoon with a soundtrack song by Irving Berlin called "I Like Ike." For the first time, a presidential candidate's personal medical history was released publicly, as were partial versions of his financial histories, because of the issues raised in Nixon's speech. Near the end of the campaign, Eisenhower, in a major speech, announced that if he won the election he would go to Korea to see if he could end the war. His great military prestige, combined with the public's weariness with the conflict, gave Eisenhower the final boost he needed to win.
Throughout the entire campaign, Eisenhower led in all opinion polls, and by wide margins in most of them.
Citizens for Eisenhower.
To circumvent the local Republican Party apparatus mostly controlled by Taft supporters, the Eisenhower forces created a nationwide network of grass-roots clubs, "Citizens for Eisenhower." Independents and Democrats were welcome, as the group specialized in canvassing neighborhoods and holding small group meetings. Citizens for Eisenhower hoped to revitalize the GOP by expanding its activist ranks and by supporting moderate and internationalist policies. It did not endorse candidates other than Eisenhower. However Eisenhower paid it little attention after he won, and it failed to maintain its impressive starting momentum. Instead it energized the conservative Republicans, leading finally to the Barry Goldwater campaign of 1964. Long-time Republican activists viewed the newcomers with suspicion and hostility. More significantly, activism in support of Eisenhower did not translate into enthusiasm for the party cause.
Results.
On election day, Eisenhower won a decisive victory, winning over 55% of the popular vote and carrying 39 of the 48 states. He took three Southern states that the Republicans had won only once since Reconstruction: Virginia, Florida, and Texas. This election was the first in which a computer (the UNIVAC I) was used to predict the results.
Source (Popular Vote): Leip, David. . ' ().Source (Electoral Vote): . '. ().
Close state races.
Election results in these states were within five percentage points (57 electoral votes). Tennessee and Kentucky were the two closest states. The 1952 Election was the last time to date that Tennessee and Kentucky voted for different candidates.
Election results in these states were between five and ten percentage points (140 electoral votes).

</doc>
<doc id="40564" url="http://en.wikipedia.org/wiki?curid=40564" title="United States presidential election, 1956">
United States presidential election, 1956

 Dwight D. Eisenhower
Dwight D. Eisenhower
The United States presidential election of 1956 was the 43rd quadrennial presidential election, held on Tuesday, November 6, 1956. The popular incumbent President, Republican Dwight D. Eisenhower, successfully ran for re-election. The election was a re-match of 1952, as Eisenhower's opponent in 1956 was former Governor of Illinois Adlai Stevenson, whom Eisenhower had defeated four years earlier.
Eisenhower was popular, but had health conditions that became a quiet issue. Stevenson remained popular with a core of liberal Democrats, but held no office and had no real base. He (and Eisenhower) largely ignored the subject of civil rights. Eisenhower had ended the Korean War and the nation was prosperous, so a landslide for the charismatic Eisenhower was never in doubt.
This was the last presidential election before the admissions of Alaska and Hawaii, which would participate for the first time as states in the 1960 presidential election. It was also the last election in which any of the major candidates was born in the 19th century, or were both renominated for a rematch of the previous presidential election.
Nominations.
Republican Party.
Republican candidates
Early in 1956, there was speculation that Eisenhower would not run for a second term because of concerns about his health. In 1955, Eisenhower had suffered a serious heart attack, and in early 1956 he underwent surgery for ileitis. However, he quickly recovered after both incidents, and after being cleared by his doctors, he decided to run for a second term. Given Eisenhower's enormous popularity, he was re-nominated with no opposition at the 1956 Republican National Convention in San Francisco, California.
The only question among Republicans was whether Vice-President Richard Nixon would again be Eisenhower's running mate. There is some evidence that Eisenhower would have preferred a less controversial running mate, such as Governor Christian Herter of Massachusetts, and according to some historians (such as Stephen E. Ambrose), Eisenhower privately offered Nixon another position in his cabinet, such as Secretary of Defense. However, Harold Stassen was the only Republican to publicly oppose Nixon's re-nomination for Vice-President, and Nixon remained highly popular among the Republican rank-and-file voters. Nixon had also reshaped the vice-presidency, using it as a platform to campaign for Republican state and local candidates across the country, and these candidates came to his defense. In the spring of 1956, Eisenhower publicly announced that Nixon would again be his running mate, and Stassen was forced to second Nixon's nomination at the Republican Convention. Unlike 1952, conservative Republicans (who had supported Robert A. Taft against Eisenhower in 1952) did not attempt to shape the platform. At the convention, one delegate voted for a fictitious "Joe Smith" for Vice-President to prevent a unanimous vote.
Democratic Party.
Democratic candidates
Primaries.
Adlai Stevenson, the Democratic Party's 1952 nominee, fought a tight primary battle with populist Tennessee Senator Estes Kefauver for the 1956 nomination. Kefauver won the New Hampshire primary unopposed (though Stevenson won 15% on write-ins). After Kefauver upset Stevenson in the Minnesota primary, Stevenson, realizing that he was in trouble, agreed to debate Kefauver in Florida. Stevenson and Kefauver held on May 21, 1956, before the Florida primary. Stevenson carried Florida by a 52-48% margin. By the time of the California primary in June 1956, Kefauver's campaign had run low on money and could not compete for publicity and advertising with the well-funded Stevenson. Stevenson won the California primary by a 63-37% margin, and Kefauver soon withdrew from the race.
Popular vote results.
Elvis Presley had 5,000 write-in votes
Democratic National Convention.
At the 1956 Democratic National Convention in Chicago, Illinois, New York Governor W. Averell Harriman, who was backed by former President Harry S. Truman, challenged Stevenson for the nomination. However, Stevenson's delegate lead was much too large for Harriman to overcome, and Stevenson won the nomination on the first ballot.
The roll call, as reported in Richard C. Bain and Judith H. Parris, "Convention Decisions and Voting Records", pp. 294–298:
Vice-Presidential Nomination.
Democratic candidates:
Candidates gallery.
The highlight of the 1956 Democratic Convention came when Stevenson, to create excitement for the ticket, made the surprise announcement that the convention's delegates would choose his running mate. This set off a desperate scramble among several candidates to win the nomination. Potential vice-presidential candidates had only one hectic day to campaign among the delegates before the voting began. The two leading contenders were Senator Kefauver, who retained the support of his primary delegates, and young Senator John F. Kennedy of Massachusetts, who was relatively unknown at that point. Although Stevenson privately preferred Senator Kennedy to be his running mate, he did not attempt to influence the balloting for Kennedy in any way. Kennedy surprised the experts by surging into the lead on the second ballot; at one point, he was only 15 votes shy of winning. However, a number of states then left their "favorite son" candidates and switched to Kefauver, giving him the victory. Kennedy then gave a gracious concession speech. The defeat was a boost for Kennedy's long-term presidential chances: as a serious contender, he gained favorable national publicity, yet by losing to Kefauver he avoided blame for Stevenson's loss to Eisenhower in November. The vote totals in the vice-presidential balloting are recorded in the following table, which also comes from Bain & Parris.
General election.
Campaign.
Stevenson campaigned hard against Eisenhower, with television ads for the first time being the dominant medium for both sides. Because Eisenhower's 1952 election victory was due, in large part, to winning the female vote, there was a plethora of "housewife" focused ads. Some commentators at the time also argued that television's new prominence was a major factor in Eisenhower's decision to run for a second term at age 66, considering his weak health after the heart attack in 1955. Television allowed Eisenhower to reach people across the country without enduring the strain of repeated coast-to-coast travel, making a national campaign more feasible.
Stevenson proposed significant increases in government spending for social programs and treaties with the Soviet Union to lower military spending and end nuclear testing on both sides. He also proposed to end the military draft and switch to an "all-volunteer" military. Eisenhower publicly opposed these ideas, even though in private he was working on a proposal to ban atmospheric nuclear testing. Eisenhower had retained the enormous personal and political popularity he had earned during World War II, and he maintained a comfortable lead in the polls throughout the campaign.
Eisenhower was also helped by his handling of two developing foreign-policy crises that occurred in the weeks before the election. In the Soviet-occupied People's Republic of Hungary, many citizens had risen in revolt in the Revolution of 1956 against Soviet domination, but the Soviets responding by invading the country on October 26. Three days later, a combined force of Israeli, British, and French troops invaded Egypt to topple Gamal Abdel Nasser and seize the recently nationalized Suez Canal. The resolution of the latter crisis rapidly moved to the United Nations, and the Hungarian revolt was brutally crushed within a few days by re-deployed Soviet troops. Eisenhower condemned both actions, but was unable to help Hungary; he did, however, forcefully pressure the western forces to withdraw from Egypt. 
While these two events led many Americans to rally in support of the president and swelled his expected margin of victory, the campaign was seen differently by some foreign governments. The Eisenhower administration had also supported the "Brown v. Board of Education" ruling in 1954; this ruling by the U.S. Supreme Court ended legal segregation in public schools. As a result, Eisenhower won the support of nearly 40% of black voters; he was the last Republican presidential candidate to receive such a level of support from black voters.
Eisenhower led all opinion polls by large margins throughout the campaign. On Election Day Eisenhower took over 57% of the popular vote and won 41 of the 48 states. Stevenson won only six Southern states and the border state of Missouri, becoming the first losing candidate since William Jennings Bryan in 1900 to carry Missouri. Eisenhower carried Louisiana, making him the first Republican presidential candidate to carry the state since Rutherford Hayes had done so in 1876 during Reconstruction.
Results.
Source (Popular Vote): Leip, David. . ' ().Source (Electoral Vote): . '. ().
Close states (margin of victory more than 5%, but less than 10%, totaling 46 electoral votes).
(a) "Alabama faithless elector W. F. Turner, who was pledged to Adlai Stevenson and Estes Kefauver, instead cast his votes for Walter Burgwyn Jones, who was a circuit court judge in Turner's home town, and Herman Talmadge, governor of the neighboring state of Georgia."
Because of the admission of Alaska and Hawaii as states in 1959, the 1956 presidential election was the last in which there were 531 electoral votes.

</doc>
<doc id="40565" url="http://en.wikipedia.org/wiki?curid=40565" title="United States presidential election, 1960">
United States presidential election, 1960

 Dwight D. Eisenhower
John F. Kennedy
The United States presidential election of 1960 was the 44th quadrennial presidential election, held on Tuesday, November 8, 1960. The Republican Party nominated incumbent Vice President Richard Nixon, while the Democratic Party nominated John F. Kennedy, U.S. Senator from Massachusetts. The incumbent President, Republican Dwight D. Eisenhower, was not eligible for re-election after being elected the maximum two times allowed by the Twenty-second Amendment; he was the first President affected by that amendment. This was the first presidential election in which voters in Alaska and Hawaii were able to participate, as both had become states in 1959.
Kennedy received 112,827 (0.17%) more votes than Nixon nationwide and although Nixon won the popular vote contest in more individual states (26 to 22), the electoral votes held by those various states, when cast, gave Kennedy an Electoral College victory of 303 to 219. Nixon was the first candidate in American presidential electoral history to lose an election despite carrying a majority of the states.
The 1960 presidential election was the closest election since 1916. A number of factors explain why the election was so close. Kennedy gained since there was an economic recession which hurt the incumbent Republican and he had the advantage of 17 million more registered Democrats than Republicans. Furthermore, the new votes that Kennedy gained among Catholics almost neutralized the new votes Nixon gained among Protestants. Kennedy's campaigning skills decisively outmatched Nixon's. In the end, Nixon's emphasis on his experience carried little weight, and he wasted energy by campaigning in all 50 states instead of concentrating on the swing states. Kennedy used his large, well-funded campaign organization to win the nomination, secure endorsements, and, with the aid of the last of the big-city bosses, get out the vote in the big cities. He relied on running mate Lyndon B. Johnson to hold the South, and used television effectively.
This election is notable as being the first presidential election in which both major party candidates were born in the 20th century. It was also the first one in which two incumbent U.S. senators (Kennedy and Johnson) were elected as President and Vice President, a phenomenon that was repeated by Barack Obama and Joe Biden in 2008. In both instances, the President-elect was the younger of the two and also the junior senator from his state.
It was also the first presidential election since 1860 that the Vice President was a presidential nominee.
Nominations.
Democratic Party.
Democratic candidates
The major candidates for the 1960 Democratic presidential nomination were Kennedy, Governor Pat Brown of California, Senator Stuart Symington of Missouri, Senator Lyndon B. Johnson of Texas, former Illinois Governor Adlai Stevenson, Senator Wayne Morse of Oregon, and Senator Hubert Humphrey of Minnesota. Several other candidates sought support in their home state or region as "favorite son" candidates without any realistic chance of winning the nomination. Symington, Stevenson, and Johnson all declined to campaign in the presidential primaries. While this reduced their potential delegate count going into the Democratic National Convention, each of these three candidates hoped that the other leading contenders would stumble in the primaries, thus causing the convention's delegates to choose him as a "compromise" candidate acceptable to all factions of the party.
Kennedy was initially dogged by suggestions from some Democratic Party elders (such as former President Harry S. Truman, who was supporting Symington) that he was too youthful and inexperienced to be president; these critics suggested that he should agree to be the running mate for another Democrat. Realizing that this was a strategy touted by his opponents to keep the public from taking him seriously, Kennedy stated frankly, "I'm not running for vice-president, I'm running for president."
The next step was the primaries. Kennedy's Roman Catholic religion was an issue. Kennedy first challenged Minnesota Senator Hubert Humphrey in the Wisconsin primary and defeated him. Kennedy's attractive sisters, brothers, and wife Jacqueline combed the state looking for votes, leading Humphrey to complain that he "felt like an independent merchant competing against a chain store." However, some political experts argued that Kennedy's margin of victory had come almost entirely from Catholic areas, and thus Humphrey decided to continue the contest in the heavily Protestant state of West Virginia. The first televised debate of 1960 was held in West Virginia, and Kennedy outperformed Humphrey. Humphrey's campaign was low on funds and could not compete for advertising and other "get-out-the-vote" drives with Kennedy's well-financed and well-organized campaign. In the end, Kennedy defeated Humphrey with over 60% of the vote, and Humphrey ended his presidential campaign. West Virginia showed that Kennedy, a Catholic, could win in a heavily Protestant state. Although Kennedy had only competed in nine presidential primaries, Kennedy's rivals, Johnson and Symington, failed to campaign in any primaries. Even though Stevenson had twice been the Democratic Party's presidential candidate and retained a loyal following of liberals, his two landslide defeats to Republican Dwight Eisenhower led most party leaders and delegates to search for a "fresh face" who could win a national election. Following the primaries, Kennedy traveled around the nation speaking to state delegations and their leaders. As the Democratic Convention opened, Kennedy was far in the lead, but was still seen as being just short of the delegate total he needed to win.
Democratic convention.
The 1960 Democratic National Convention was held in Los Angeles, California. In the week before the convention opened, Kennedy received two new challengers when Lyndon B. Johnson, the powerful Senate Majority Leader from Texas, and Adlai Stevenson, the party's nominee in 1952 and 1956, officially announced their candidacies (they had both privately been working for the nomination for some time). However, neither Johnson nor Stevenson was a match for the talented and highly efficient Kennedy campaign team led by Robert F. Kennedy. Johnson challenged Kennedy to a televised debate before a joint meeting of the Texas and Massachusetts delegations; Kennedy accepted. Most observers felt that Kennedy won the debate, and Johnson was not able to expand his delegate support beyond the South. Stevenson's failure to launch his candidacy publicly until the week of the convention meant that many liberal delegates who might have supported him were already pledged to Kennedy, and Stevenson—despite the energetic support of former First Lady Eleanor Roosevelt—was unable to break their allegiance. Kennedy won the nomination on the first ballot. 
Then, in a move that surprised many, Kennedy asked Johnson to be his running mate. Kennedy realized that he could not be elected without support of traditional Southern Democrats, most of whom had backed Johnson. Kennedy offered Johnson the vice-presidential nomination at the Los Angeles Biltmore Hotel at 10:15 a.m. on July 14, 1960, the morning after being nominated for president. Robert F. Kennedy, who hated Johnson for his attacks on the Kennedy family, said later that his brother offered the position to Johnson as a courtesy and did not expect him to accept. Arthur M. Schlesinger Jr., and Seymour Hersh quote Robert Kennedy's version of events, writing that John Kennedy would have preferred Stuart Symington as his running-mate and that Johnson teamed with House Speaker Sam Rayburn to pressure Kennedy to offer the nomination. Biographers Robert Caro and W. Marvin Watson offer a different perspective; they write that the Kennedy campaign was desperate to win what was forecast to be a very close race against Richard Nixon and Henry Cabot Lodge II. Johnson was needed on the ticket to help carry Texas and the Southern states. Caro's research showed that on July 14, John Kennedy started the process while Johnson was still asleep. At 6:30 a.m. John Kennedy asked Robert Kennedy to prepare an estimate of upcoming electoral votes, "including Texas." Robert called Pierre Salinger and Kenneth O'Donnell to assist him. Realizing the ramifications of counting Texas votes as their own, Salinger asked him whether he was considering a Kennedy-Johnson ticket, and Robert replied, yes. Some time between 9 and 10 a.m., John Kennedy called Pennsylvania governor David L. Lawrence, a Johnson backer, to request that Lawrence nominate Johnson for vice-president if Johnson were to accept the role and then went to Johnson's suite to discuss a mutual ticket at 10:15 a.m. John Kennedy then returned to his suite to announce the Kennedy-Johnson ticket to his closest supporters and Northern political bosses. He accepted the congratulations of Ohio governor Michael DiSalle, Connecticut governor Abraham A. Ribicoff, Chicago mayor Richard J. Daley, and New York City mayor Robert F. Wagner Jr.. Lawrence said that "Johnson has the strength where you need it most"; he then left to begin writing the nomination speech. O'Donnell remembers being angry at what he considered a betrayal by John Kennedy, who had previously cast Johnson as anti-labor and anti-liberal. Afterward, Robert Kennedy visited with labor leaders who were extremely unhappy with the choice of Johnson and after seeing the depth of labor opposition to Johnson, he ran messages between the hotel suites of his brother and Johnson, apparently trying to undermine the proposed ticket without John Kennedy's authorization and to get Johnson to agree to be the Democratic Party chairman rather than vice president. Johnson refused to accept a change in plans unless it came directly from John Kennedy. Despite his brother's interference, John Kennedy was firm that Johnson was who he wanted as running mate and met with staffers such as Larry O'Brien, his national campaign manager, to say Johnson was to be vice-president. O'Brien recalled later that John Kennedy's words were wholly unexpected, but that after a brief consideration of the electoral vote situation, he thought "it was a stroke of genius".
Norman Mailer attended the convention and wrote his famous profile of Kennedy, "Superman Comes to the Supermart," published in "Esquire".
Republican Party.
With the ratification of the 22nd Amendment in 1951, President Dwight D. Eisenhower could not run for the office of President again; he had been elected in 1952 and 1956.
In 1959, it looked as if Vice-President Richard Nixon might face a serious challenge for the Republican nomination from New York Governor Nelson Rockefeller, the leader of the Republican moderate-liberal wing. However, Rockefeller announced that he would not be a candidate for president after a national tour revealed that the great majority of Republicans favored Nixon.
After Rockefeller's withdrawal, Nixon faced no significant opposition for the Republican nomination. At the 1960 Republican National Convention in Chicago, Illinois, Nixon was the overwhelming choice of the delegates, with conservative Senator Barry Goldwater of Arizona receiving 10 votes from conservative delegates. In earning the nomination, Nixon became the first sitting Vice-President to run for President since John C. Breckinridge exactly a century prior. Nixon then chose former Massachusetts Senator and United Nations Ambassador Henry Cabot Lodge Jr., as his vice-presidential candidate. Nixon chose Lodge because his foreign-policy credentials fit into Nixon's strategy to campaign more on foreign policy than domestic policy, which he believed favored the Democrats. Nixon had previously sought Rockefeller as his running mate, but the governor had no ambitions to be vice-president. However, he later served as Gerald Ford's Vice President from 1974 to 1977.
General election.
Campaign promises.
During the campaign, Kennedy charged that under Eisenhower and the Republicans the nation had fallen behind the Soviet Union in the Cold War, both militarily and economically, and that as president he would "get America moving again." Nixon responded that, if elected, he would continue the "peace and prosperity" that Eisenhower had brought the nation in the 1950s. Nixon also argued that with the nation engaged in the Cold War with the Soviets, that Kennedy was too young and inexperienced to be trusted with the presidency.
Campaign events.
Both Kennedy and Nixon drew large and enthusiastic crowds throughout the campaign. In August 1960, most polls gave Vice-President Nixon a slim lead over Kennedy, and many political pundits regarded Nixon as the favorite to win. However, Nixon was plagued by bad luck throughout the fall campaign. In August, President Eisenhower, who had long been ambivalent about Nixon, held a televised press conference in which a reporter, Charles Mohr of "Time", mentioned Nixon's claims that he had been a valuable administration insider and adviser. Mohr asked Eisenhower if he could give an example of a major idea of Nixon's that he had heeded. Eisenhower responded with the flip comment, "If you give me a week, I might think of one." Although both Eisenhower and Nixon later claimed that he was merely joking with the reporter, the remark hurt Nixon, as it undercut his claims of having greater decision-making experience than Kennedy. The remark proved so damaging to Nixon that the Democrats turned Eisenhower's statement into a television commercial.
At the Republican Convention, Nixon had pledged to campaign in all 50 states. This pledge backfired when, in August, Nixon injured his knee on a car door while campaigning in North Carolina. The knee became infected and Nixon had to cease campaigning for two weeks while the infected knee was injected with antibiotics. When he left Walter Reed Hospital, Nixon refused to abandon his pledge to visit every state; he thus wound up wasting valuable time visiting states that he had no chance to win, or that had few electoral votes and would be of little help in the election, or states that he would almost certainly win regardless. For example, in his effort to visit all 50 states, Nixon spent the vital weekend before the election campaigning in Alaska, which had only three electoral votes, while Kennedy campaigned in large states such as New Jersey, Ohio, Michigan, and Pennsylvania.
Despite the reservations Robert Kennedy had about Johnson's nomination, choosing Johnson as Kennedy's running mate proved to be a masterstroke. Johnson vigorously campaigned for Kennedy and was instrumental in helping the Democrats to carry several Southern states skeptical of him, especially Johnson's home state of Texas. On the other hand, Ambassador Lodge, Nixon's running mate, ran a lethargic campaign and made several mistakes that hurt Nixon. Among them was a pledge—not approved by Nixon—that as President Nixon would name a black person to his cabinet. The remark offended many blacks who saw it as a clumsy attempt to win their votes, while many Southern whites who still supported racial segregation and might have considered voting for Nixon were also angered.
Debates.
The key turning point of the campaign were the four Kennedy-Nixon debates; they were the first presidential debates ever (The Lincoln–Douglas debates of 1858 had been for senator from Illinois), and also the first held on television, and thus attracted enormous publicity. Nixon insisted on campaigning until just a few hours before the first debate started. He had not completely recovered from his hospital stay and thus looked pale, sickly, underweight, and tired. He also refused makeup for the first debate, and as a result his beard stubble showed prominently on the era's black-and-white TV screens. Nixon's poor appearance on television in the first debate is reflected by the fact that his mother called him immediately following the debate to ask if he was sick. Kennedy, by contrast, rested and prepared extensively beforehand, appearing tanned, confident, and relaxed during the debate. An estimated 70 million viewers watched the first debate. It is often claimed that people who watched the debate on television overwhelmingly believed Kennedy had won, while radio listeners (a smaller audience) believed Nixon had won. However, one study has speculated that the viewer/listener disagreement could be due to sample bias in that those without TV could be a skewed subset of the population : 
Evidence in support of this belief ["i.e.", that Kennedy's physical appearance overshadowed his performance during the first debate] is mainly limited to sketchy reports about a market survey conducted by Sindlinger & Company in which 49% of those who listened to the debates on radio said Nixon had won compared to 21% naming Kennedy, while 30% of those who watched the debates on television said Kennedy had won compared to 29% naming Nixon. Contrary to popular belief, the Sindlinger evidence suggests not that Kennedy won on television but that the candidates tied on television while Nixon won on radio. However, no details about the sample have ever been reported, and it is unclear whether the survey results can be generalized to a larger population. Moreover, since 87% of American households had a television in 1960 [and that the] fraction of Americans lacking access to television in 1960 was concentrated in rural areas and particularly in southern and western states, places that were unlikely to hold significant proportions of Catholic voters."
After the first debate, polls showed Kennedy moving from a slight deficit into a slight lead over 
Nixon. For the remaining three debates Nixon regained his lost weight, wore television makeup, and appeared more forceful than in his initial appearance.
However, up to 20 million fewer viewers watched the three remaining debates than the first one. Political observers at the time believed that Kennedy won the first debate, Nixon won the second and third debates, while the fourth debate, which was seen as the strongest performance by both men, was a draw.
The third debate is notable because it brought about a change in the debate process. This debate was a monumental step for television. For the first time ever split screen technology was used to bring two people from opposite sides of the country together so they were able to converse in real time. Nixon was in Los Angeles while Kennedy was in New York. The men appear to be in the same room, thanks to identical sets. Both candidates had monitors in their respective studios containing the feed from the opposite studio so they could respond to questions. Bill Shadel moderated the debate from a third television studio in Chicago. The main topic of this debate was Quemoy and Matsu. It was a question of the US position over whether military force should be used to prevent buffer islands between China and Taiwan from falling under Chinese control.
Campaign issues.
A key concern in Kennedy's campaign was the widespread skepticism among Protestants about his Roman Catholic religion. Some Protestants, especially Southern Baptists and Lutherans, feared that having a Catholic in the White House would give undue influence to the Pope in the nation's affairs. Radio evangelists such as G. E. Lowman wrote that, "Each person has the right to their own religious belief ... [but] ... the Roman Catholic ecclesiastical system demands the first allegiance of every true member and says in a conflict between church and state, the "church" must prevail". The religious issue was so significant that Kennedy made a speech before the nation's newspaper editors in which he criticized the prominence they gave to the religious issue over other topics - especially in foreign policy - that he felt were of greater importance.
To address fears among Protestants that his Roman Catholicism would impact his decision-making, Kennedy famously told the Greater Houston Ministerial Association on September 12, 1960, "I am not the Catholic candidate for president. I am the Democratic Party's candidate for president who also happens to be a Catholic. I do not speak for my Church on public matters – and the Church does not speak for me." He promised to respect the separation of church and state and not to allow Catholic officials to dictate public policy to him. Kennedy also raised the question of whether one-quarter of Americans were relegated to second-class citizenship just because they were Roman Catholic. Kennedy would become the first (and so far only) Roman Catholic to be elected president.
However, Kennedy's campaign did take advantage of an opening when Rev. Martin Luther King Jr., the civil-rights leader, was arrested in Georgia while leading a civil rights march. Nixon refused to become involved in the incident, but Kennedy placed calls to local political authorities to get King released from jail, and he also called King's father and wife. As a result, King's father endorsed Kennedy, and he received much favorable publicity in the black community. On election day, Kennedy won the black vote in most areas by wide margins, and this may have provided his margin of victory in states such as New Jersey, South Carolina, Illinois, and Missouri.
However, it was accepted that the issue which dominated this election was the rising Cold War tensions between the United States and the Soviet Union. In 1957, the Soviets had launched Sputnik, the first man-made satellite to orbit Earth. Soon afterwards, some American leaders warned that the nation was falling behind communist countries in science and technology. In Cuba, the revolutionary regime of Fidel Castro became a close ally of the Soviet Union in 1960, heightening fears of communist subversion in the Western Hemisphere. Public opinion polls revealed that more than half the American people thought war with the Soviet Union was inevitable.
Kennedy took advantage of increased Cold War tension by emphasizing a perceived "missile gap" between the United States and Soviet Union. He argued that under the Republicans the Soviets had developed a major advantage in the numbers of nuclear missiles. He also noted in an October 18 speech that several senior US military officers had long criticized the Eisenhower Administration's defense spending policies.
Both candidates also argued about the economy and ways in which they could increase the economic growth and prosperity of the 1950s and make it accessible to more people (especially minorities). Some historians criticize Nixon for not taking greater advantage of Eisenhower's popularity (which was around 60–65% throughout 1960 and on election day) and for not discussing the prosperous economy of the Eisenhower presidency more often in his campaign. As the campaign moved into the final two weeks, the polls and most political pundits predicted a Kennedy victory. However, President Eisenhower, who had largely sat out the campaign, made a vigorous campaign tour for Nixon over the last 10 days before the election. Eisenhower's support gave Nixon a badly needed boost. Nixon also criticized Kennedy for stating that Quemoy and Matsu, two small islands off the coast of Communist China that were held by Nationalist Chinese forces based in Taiwan, were outside the treaty of protection the United States had signed with the Nationalist Chinese. Nixon claimed the islands were included in the treaty and accused Kennedy of showing weakness towards Communist aggression. Aided by the Quemoy and Matsu issue, and by Eisenhower's support, Nixon began to gain momentum and by election day the polls indicated a virtual tie.
Results.
The election on November 8, 1960, remains one of the most famous election nights in American history. Nixon watched the election returns from his suite at the famed Ambassador Hotel in Los Angeles, while Kennedy watched the returns at the Kennedy Compound in Hyannis Port, Massachusetts. As the early returns poured in from large Northeastern and Midwestern cities such as Boston, New York City, Philadelphia, Pittsburgh, Cleveland, Detroit, and Chicago, Kennedy opened a large lead in the popular and electoral vote, and appeared headed for victory. However, as later returns came in from rural and suburban areas in the Midwest, the Rocky Mountain states, and the Pacific Coast states, Nixon began to steadily close the gap with Kennedy.
Before midnight, "The New York Times" had gone to press with the headline "Kennedy Elected President". As the election again became too close to call, "Times" managing editor Turner Catledge hoped that, as he recalled in his memoirs, "a certain Midwestern mayor would steal enough votes to pull Kennedy through", thus allowing the "Times" to avoid the embarrassment of having announced the wrong winner, as the "Chicago Tribune" had memorably done twelve years earlier in announcing that Thomas E. Dewey had defeated President Truman.
Nixon made a speech at about 3 am, and hinted that Kennedy might have won the election. News reporters were puzzled, because it was not a formal concession speech. It was not until the afternoon of Wednesday, November 9, that Nixon finally conceded the election, and Kennedy claimed victory.
Of the 3,129 counties/independent cities making returns, Nixon won in 1,857 (59.35%) while Kennedy carried 1,200 (38.35%). "Unpledged" electors came first in 71 counties (2.27%) throughout Louisiana and Mississippi and one county (0.03%) in Alaska split evenly between Kennedy and Nixon.
A sample of how close the election was can be seen in Nixon's home state of California. Kennedy appeared to have carried the state by 37,000 votes when all of the voting precincts reported, but when the absentee ballots were counted a week later, Nixon came from behind to win the state by 36,000 votes. Similarly, in Hawaii, it appeared Nixon had won the state (it was actually called for him early Wednesday morning), but in a recount Kennedy was able to come from behind and win the state by an extremely narrow margin of 115 votes.
In the national popular vote, Kennedy beat Nixon by less than two tenths of one percentage point (0.17%)—the closest popular-vote margin of the 20th century. In the Electoral College, Kennedy's victory was larger, as he took 303 electoral votes to Nixon's 219 (269 were needed to win). A total of 15 electors—eight from Mississippi, six from Alabama, and one from Oklahoma—refused to vote for either Kennedy or Nixon. Instead, they cast their votes for Senator Harry F. Byrd of Virginia, a conservative Democrat, even though Byrd had not been a candidate for president. Kennedy carried 12 states by three percentage points or less, while Nixon won six states by similarly narrow margins. Kennedy carried all but three states in the populous Northeast, and he also carried the large states of Michigan, Illinois, and Missouri in the Midwest. With Lyndon Johnson's help, he also carried most of the South, including the large states of North Carolina, Georgia, and Texas. Nixon carried all but three of the Western states (including California), and he ran strong in the farm belt states, where his biggest victory was in Ohio.
"The New York Times", summarizing the discussion late in November, spoke of a "narrow consensus" among the experts that Kennedy had won more than he lost as a result of his Catholicism, as Northern Catholics flocked to Kennedy because of attacks on his religion. Interviewing people who voted in both 1956 and 1960, a University of Michigan team analyzing the election returns discovered that people who voted Democratic in 1956 split 33–6 for Kennedy, while the Republican voters of 1956 split 44–17 for Nixon. That is, Nixon lost 28% (17/61) of the Eisenhower voters, while Kennedy lost only 15% of the Stevenson voters. The Democrats, in other words, did a better job of holding their 1956 supporters.
Kennedy said that he saw the challenges ahead and needed the country's support to get through them. In his victory speech, he declared, "To all Americans, I say that the next four years are going to be difficult and challenging years for us all that a supreme national effort will be needed to move this country safely through the 1960s. I ask your help and I can assure you that every degree of my spirit that I possess will be devoted to the long range interest of the United States and to the cause of freedom around the world."
Controversies.
Many persons believed that Kennedy benefited from vote fraud, especially in Texas, where Kennedy's running mate Lyndon B. Johnson was senator, and Illinois, home of Mayor Richard Daley's powerful Chicago political machine. These two states were important because if Nixon had carried both, he would have earned 270 electoral votes, one more than the 269 needed to win the majority in the Electoral College and the presidency. Republican Senators such as Everett Dirksen and Barry Goldwater also believed that vote fraud played a role in the election, and they believed that Nixon actually won the national popular vote. Republicans tried and failed to overturn the results in both Illinois and Texas at the time—as well as in nine other states. Some journalists also later claimed that mobster Sam Giancana and his Chicago crime syndicate played a role in Kennedy's victory in Illinois.
Nixon's campaign staff urged him to pursue recounts and challenge the validity of Kennedy's victory in several states, especially in Illinois, Missouri and New Jersey, where large majorities in Catholic precincts handed Kennedy the election. However, Nixon gave a speech three days after the election stating that he would not contest the election. The Republican National Chairman, Senator Thruston Ballard Morton of Kentucky, visited Key Biscayne, Florida, where Nixon had taken his family for a vacation, and pushed for a recount. Morton did challenge the results in 11 states, keeping challenges in the courts into the summer of 1961. However, the only result of these challenges was the loss of Hawaii to Kennedy on a recount.
Kennedy won Illinois by less than 9,000 votes out of 4.75 million cast, or a margin of 0.2%. However, Nixon carried 92 of the state's 101 counties, and Kennedy's victory in Illinois came from the city of Chicago, where Mayor Richard J. Daley held back much of Chicago's vote until the late morning hours of November 9. The efforts of Daley and the powerful Chicago Democratic organization gave Kennedy an extraordinary Cook County victory margin of 450,000 votes—more than 10% of Chicago's 1960 population of 3.55 million, although Cook County also includes many suburbs outside of Chicago's borders—thus barely overcoming the heavy Republican vote in the rest of Illinois. Earl Mazo, a reporter for the pro-Nixon "New York Herald Tribune", investigated the voting in Chicago and claimed to have discovered sufficient evidence of vote fraud to prove that the state was stolen for Kennedy.
In Texas, Kennedy defeated Nixon by a narrow 51% to 49% margin, or 46,000 votes. Some Republicans argued that Johnson's formidable political machine had stolen enough votes in counties along the Mexican border to give Kennedy the victory. Kennedy's defenders, such as his speechwriter and special assistant Arthur M. Schlesinger Jr., have argued that Kennedy's margin in Texas (46,000 votes) was simply too large for vote fraud to have been a decisive factor. Russell D. Renka, a former political science professor at Southeastern Missouri State University, acknowledged that it was more than likely that Johnson's political machine in the state's lower Rio Grande Valley counties, including the notorious Duval County, could have managed to produce a significant number of forged votes for Kennedy. However, Renka also acknowledged that Kennedy's margin in the state's initial tally made it far too difficult to prove that voter fraud had determined who won Texas and that any recount would've also been hard to conduct.
Cases of voter fraud were discovered in Texas. For example, Fannin County had only 4,895 registered voters, yet 6,138 votes were cast in that county, three-quarters for Kennedy. In an Angelina County precinct, Kennedy received 187 votes to Nixon's 24, though there were only a total of 86 registered voters in the precinct. When Republicans demanded a statewide recount, they learned that the state Board of Elections, whose members were all Democrats, had already certified Kennedy as the official winner in Texas.
In Illinois, Schlesinger and others have pointed out that, even if Nixon had carried Illinois, the state alone would not have given him the victory, as Kennedy would still have won 276 electoral votes to Nixon's 246 (with 269 needed to win). More to the point, Illinois was the site of the most extensive challenge process, which fell short despite repeated efforts spearheaded by Cook County state's attorney, Benjamin Adamowski, a Republican, who also lost his re-election bid. Despite demonstrating net errors favoring both Nixon and Adamowski (some precincts—40% in Nixon's case—showed errors favoring them, a factor suggesting error, rather than fraud), the totals found fell short of reversing the results for either candidate. While a Daley-connected circuit judge, Thomas Kluczynski (who would later be appointed a federal judge by Kennedy, at Daley's recommendation), threw out a federal lawsuit filed to contend the voting totals, the Republican-dominated State Board of Elections unanimously rejected the challenge to the results. Furthermore, there were signs of possible irregularities in downstate areas controlled by Republicans, which Democrats never seriously pressed, since the Republican challenges went nowhere. More than a month after the election, the Republican National Committee abandoned its Illinois voter fraud claims.
However, a special prosecutor assigned to the case brought charges against 650 people, which did not result in convictions. Three Chicago election workers were convicted of voter fraud in 1962 and served short terms in jail. Mazo, the "Herald-Tribune" reporter, later said that he found names of the dead who had voted in Chicago, along with 56 people from one house. He found cases of Republican voter fraud in southern Illinois, but said that the totals did not match the Chicago fraud he found. After Mazo had published four parts of an intended 12-part voter fraud series documenting his findings which was re-published nationally, he says Nixon requested his publisher stop the rest of the series so as to prevent a constitutional crisis. Nevertheless, the "Chicago Tribune" wrote that "the election of November 8 was characterized by such gross and palpable fraud as to justify the conclusion that [Nixon] was deprived of victory." Had Nixon won both states, he would have ended up with exactly 270 electoral votes and the presidency, with or without a victory in the popular vote.
Alabama popular vote.
The actual number of popular votes received by Kennedy in Alabama is difficult to determine because of the unusual situation in that state. The first minor issue is that, instead of having the voters choose from slates of electors, the Alabama ballot had voters choose the electors individually. Traditionally, in such a situation, a given candidate is assigned the popular vote of the elector who received the most votes. For instance, candidates pledged to Nixon received anywhere from 230,951 votes (for George Witcher) to 237,981 votes (for Cecil Durham); Nixon is therefore assigned 237,981 popular votes from Alabama.
The more important issue is that the statewide Democratic primary had chosen eleven candidates for the Electoral College, five of whom were pledged to vote for Kennedy, and six of whom were free to vote for anyone they chose. All of these candidates won in the general election, and all six unpledged electors voted against Kennedy. The actual number of popular votes received by Kennedy is therefore difficult to allocate. Traditionally, Kennedy is assigned either 318,303 votes (the votes won by the most popular Kennedy elector) or 324,050 votes (the votes won by the most popular Democratic elector); the results table below is based on Kennedy winning 318,303 votes in Alabama.
Georgia popular vote.
The actual number of popular votes received by Kennedy and Nixon in Georgia is also difficult to determine because voters voted for 12 separate electors. The vote totals of 458,638 votes for Kennedy and 274,472 votes for Nixon reflect the number of votes for the Kennedy and Nixon electors who received the highest number of votes. However, the Republican and Democratic electors receiving the highest number of votes were outliers from the other 11 electors from their party. The average vote totals for the 12 electors were 455,629 votes for the Democratic electors and 273,110 votes for the Republican electors. This shrinks Kennedy's election margin in Georgia by 1,647 votes to 182,519.
Unpledged Democratic electors.
Many Southern Democrats were opposed to the national Democratic Party's platform on supporting civil rights and voting rights for African Americans living in the South. Both before and after the convention, they attempted to put unpledged Democratic electors on their states' ballots in the hopes of influencing the race: the existence of such electors might influence which candidate would be chosen by the national convention, and, in a close race, such electors might be in a position to extract concessions from either the Democratic or Republican presidential candidates in return for their electoral votes.
Most of these attempts failed. Alabama put up a mixed slate of five loyal electors and six unpledged electors. Mississippi put up two distinct slates, one of loyalists and one of unpledged electors. Louisiana also put up two distinct slates, although the unpledged slate did not receive the "Democratic" label. Georgia freed its Democratic electors from pledges to vote for Kennedy, but popular Governor Ernest Vandiver, a candidate for elector himself, publicly backed Kennedy.
In total, fourteen unpledged Democratic electors won election from the voters. Because electors pledged to Kennedy had won a clear majority of the Electoral College, the unpledged electors could not influence the results. Nonetheless, they refused to vote for Kennedy. Instead they voted for Virginia Senator Harry F. Byrd, a segregationist Democrat, even though Byrd was not an announced candidate and did not seek their votes. In addition, Byrd received one electoral vote from a faithless Oklahoma elector, for a total of 15 electoral votes.
There were 537 electoral votes, up from 531 in 1956, because of the addition of 2 U.S. Senators and 1 U.S. Representative from each of the new states of Alaska and Hawaii. (The House of Representatives was temporarily expanded from 435 members to 437 to accommodate this, and went back to 435 when reapportioned according to the 1960 census. The reapportionment did not take place until after the 1960 election.)
Source (Popular Vote):Leip, David. . " ().Note: Sullivan / Curtis ran only in Texas. In Washington, Constitution Party ran Curtis for President and B. N. Miller for vice-president, receiving 1,401 votes.
Source (Electoral Vote): . ". ().
(a) "This figure is problematic; see Alabama popular vote above."
(b) "Byrd was not directly on the ballot. Instead, his electoral votes came from unpledged Democratic electors and a faithless elector."
(c) "Oklahoma faithless elector Henry D. Irwin, though pledged to vote for Richard Nixon and Henry Cabot Lodge Jr., instead voted for non-candidate Harry F. Byrd. However, unlike other electors who voted for Byrd and Strom Thurmond as Vice President, Irwin cast his vice presidential electoral vote for Arizona Republican Senator Barry Goldwater.
(d) "In Mississippi, the slate of unpledged Democratic electors won. They cast their 8 votes for Byrd and Thurmond."
Close states.
Margin of victory less than 5% (256 electoral votes):
Margin of victory over 5%, but under 10% (160 electoral votes):

</doc>
<doc id="40566" url="http://en.wikipedia.org/wiki?curid=40566" title="United States presidential election, 1964">
United States presidential election, 1964

 Lyndon B. Johnson
Lyndon B. Johnson
The United States presidential election of 1964 was the 45th quadrennial presidential election. It was held on Tuesday, November 3, 1964. Democratic candidate and incumbent President Lyndon B. Johnson had come to office less than a year earlier following the assassination of his predecessor John F. Kennedy. Johnson, who had successfully associated himself with Kennedy's popularity, won 61.1% of the popular vote, the highest won by a candidate since James Monroe's re-election in 1820. It was the sixth-most lopsided presidential election in the history of the United States in terms of electoral votes; in terms of popular vote, it is first. No candidate for president since has equaled or surpassed Johnson's percentage of the popular vote, and only Richard Nixon in 1972 has won by a greater popular vote margin.
The Republican candidate, Senator Barry Goldwater of Arizona, suffered from a lack of support from his own party and his deeply unpopular conservative political positions. Johnson's campaign advocated social programs and further federal efforts to curb racial segregation, collectively known as the Great Society, and successfully portrayed Goldwater as being a dangerous extremist. Johnson easily won the Presidency, carrying 44 of the 50 states and the District of Columbia.
Goldwater's unsuccessful bid influenced the modern conservative movement and the long-time realignment within the Republican Party, which culminated in the 1980 presidential victory of Ronald Reagan. His campaign received considerable support from former Democratic strongholds in the Deep South and was the first Republican campaign to win Georgia in a presidential election. Conversely, Johnson won Alaska for the Democrats for the first (and only) time, as well as Maine (for the first time since 1912) and Vermont (for the first time since the Democratic Party was founded). Since the 1990s, Vermont and Maine have rested solidly in the Democratic column and Georgia in the Republican fold for presidential elections.
No post-1964 Democratic presidential candidate has been able to match or better Johnson's performance in the electoral college (the only Republicans to do so since have been Nixon in 1972 and Ronald Reagan in 1980 and 1984), or Johnson's performance in the Mountain and Midwestern regions of the United States.
Assassination of President John F. Kennedy.
President Kennedy was assassinated on November 22, 1963 in Dallas, Texas. Supporters were shocked and saddened by the loss of the charismatic Kennedy, while opposition candidates were put in the awkward position of running against the policies of a slain president.
During the following period of mourning, Republican leaders called for a political moratorium, so as not to appear disrespectful. As such, little politicking was done by the candidates of either major party until January 1964, when the primary season officially began. At the time, most political pundits saw Kennedy's assassination as leaving the nation politically unsettled.
Nominations.
Democratic Party.
Democratic candidates
Candidates gallery.
The nomination of Johnson was assured, but he wanted to control the convention and to avoid a public fight over civil rights. Nonetheless, Johnson faced challenges from two sides over civil rights issues over the course of the nomination season.
The segregationist Governor of Alabama, George Wallace, ran in a number of northern primaries against Johnson, and did surprisingly well in primaries in Maryland, Indiana, and Wisconsin against favorite son candidates who were stalking horses for Johnson. All favorite-sons, however, won their primaries. In California Yorty lost to Brown.
Total popular vote:
At the national convention the integrated Mississippi Freedom Democratic Party (MFDP) claimed the seats for delegates for Mississippi, not on the grounds of the Party rules, but because the official Mississippi delegation had been elected by a white primary system. The party's liberal leaders supported an even division of the seats between the two delegations; Johnson was concerned that, while the regular Democrats of Mississippi would probably vote for Goldwater anyway, rejecting them would lose him the South. Eventually, Hubert Humphrey, Walter Reuther and the black civil rights leaders including Roy Wilkins, Martin Luther King Jr., and Bayard Rustin worked out a compromise: the MFDP took two seats; the regular Mississippi delegation was required to pledge to support the party ticket; and no future Democratic convention would accept a delegation chosen by a discriminatory poll. Joseph L. Rauh Jr., the MFDP's lawyer, initially refused this deal, but they eventually took their seats. Many white delegates from Mississippi and Alabama refused to sign any pledge, and left the convention; and many young civil rights workers were offended by any compromise. Johnson carried the South as a whole in the election, but lost Louisiana, Alabama, Mississippi, Georgia and South Carolina.
Johnson also faced trouble from Robert F. Kennedy, President Kennedy's younger brother and the U.S. Attorney General. Kennedy and Johnson's relationship was troubled from the time Robert Kennedy was a Senate staffer. Then-Majority Leader Johnson surmised that Kennedy's hostility was the direct result of the fact that Johnson frequently recounted a story that embarrassed Kennedy's father, Joseph P. Kennedy, the ambassador to the United Kingdom. According to his recounting, Johnson and President Franklin D. Roosevelt misled the ambassador, upon a return visit to the United States, to believe that Roosevelt wished to meet in Washington for friendly purposes; in fact Roosevelt planned to—and did—fire the ambassador, due to the ambassador's well publicized views. The Johnson–Kennedy hostility was rendered mutual in the 1960 primaries and the 1960 Democratic National Convention, when Robert Kennedy had tried to prevent Johnson from becoming his brother's running mate, a move that deeply embittered both men. Hours after John Kennedy's assassination, Johnson opted to call Robert Kennedy to ask the bereaved brother to remind him the exact language of the constitutional oath of office.
In early 1964, despite his personal animosity for the president, Kennedy had tried to force Johnson to accept him as his running mate. Johnson eliminated this threat by announcing that none of his cabinet members would be considered for second place on the Democratic ticket. Johnson also became concerned that Kennedy might use his scheduled speech at the 1964 Democratic Convention to create a groundswell of emotion among the delegates to make him Johnson's running mate; he prevented this by deliberately scheduling Kennedy's speech on the last day of the convention, after his running mate had already been chosen. Shortly after the 1964 Democratic Convention, Kennedy decided to leave Johnson's cabinet and run for the U.S. Senate in New York; he won the general election in November. Johnson chose Senator Hubert Humphrey of Minnesota, a liberal and civil rights activist, as his running mate. (It was noted that the need for a vice-presidential candidate, in the aftermath of John Kennedy's assassination, provided some suspense for the convention.)
Republican Party.
Republican candidates
The primaries.
The Republican Party (GOP) was badly divided in 1964 between its conservative and moderate-liberal factions. Former Vice-President Richard Nixon, who had been beaten by Kennedy in the extremely close 1960 presidential election, decided not to run. Nixon, a moderate with ties to both wings of the GOP, had been able to unite the factions in 1960; in his absence the way was clear for the two factions to engage in an all-out political civil war for the nomination. Barry Goldwater, a Senator from Arizona, was the champion of the conservatives. The conservatives had historically been based in the American Midwest, but beginning in the 1950s the conservatives had been gaining in power in the South and West. The conservatives favored a low-tax, small federal government which supported individual rights and business interests and opposed social welfare programs. The conservatives also resented the dominance of the GOP's moderate wing, which was based in the Northeastern United States. Since 1940, the Eastern moderates had successfully defeated conservative presidential candidates at the GOP's national conventions. The conservatives believed the Eastern moderates were little different from liberal Democrats in their philosophy and approach to government. Goldwater's chief opponent for the Republican nomination was Nelson Rockefeller, the Governor of New York and the longtime leader of the GOP's liberal-moderate faction.
Initially, Rockefeller was considered the front-runner, ahead of Goldwater. However, in 1963, two years after Rockefeller's divorce from his first wife, he married Margarita "Happy" Murphy, who was 15 years younger than he and had just divorced her husband and surrendered her four children to his custody. The fact that Murphy had suddenly divorced her husband before marrying Rockefeller led to rumors that Rockefeller had been having an extramarital affair with her. This angered many social conservatives and female voters within the GOP, many of whom called Rockefeller a "wife stealer". After his remarriage, Rockefeller's lead among Republicans lost 20 points overnight. Senator Prescott Bush of Connecticut, the father of President George H.W. Bush and grandfather of President George W. Bush, was among Rockefeller's critics on this issue: "Have we come to the point in our life as a nation where the governor of a great state—one who perhaps aspires to the nomination for president of the United States—can desert a good wife, mother of his grown children, divorce her, then persuade a young mother of four youngsters to abandon her husband and their four children and marry the governor?"
In the first primary, in New Hampshire, both Rockefeller and Goldwater were considered to be the favorites, but the voters instead gave a surprising victory to the U.S. ambassador to South Vietnam, Henry Cabot Lodge Jr., Nixon's running mate in 1960 and a former Massachusetts senator. Lodge was a write-in candidate. Lodge went on to win the Massachusetts and New Jersey primaries before finally deciding that he didn't want the Republican nomination, he then withdrew his candidacy.
Despite his defeat in New Hampshire, Goldwater pressed on, winning the Illinois, Texas, and Indiana primaries with little opposition, and Nebraska's primary after a stiff challenge from a draft-Nixon movement. Goldwater also won a number of state caucuses and gathered even more delegates. Meanwhile, Nelson Rockefeller won the West Virginia and Oregon primaries against Goldwater, and William Scranton won in his home state of Pennsylvania. Both Rockefeller and Scranton also won several state caucuses, mostly in the Northeast.
The final showdown between Goldwater and Rockefeller was in the California primary. In spite of the previous accusations regarding his marriage, Rockefeller led Goldwater in most opinion polls in California, and he appeared headed for victory when his new wife gave birth to a son, Nelson Rockefeller Jr., three days before the primary. His son's birth brought the issue of adultery front and center, and Rockefeller suddenly lost ground in the polls. Goldwater won the primary by a narrow 51–49% margin, thus eliminating Rockefeller as a serious contender and all but clinching the nomination. With Rockefeller's elimination, the party's moderates and liberals turned to William Scranton, the Governor of Pennsylvania, in the hopes that he could stop Goldwater. However, as the Republican Convention began Goldwater was seen as the heavy favorite to win the nomination.
Total popular vote
Convention.
The 1964 Republican National Convention at Daly City, California's Cow Palace arena was one of the most bitter on record, as the party's moderates and conservatives openly expressed their contempt for each other. Rockefeller was loudly booed when he came to the podium for his speech; in his speech he roundly criticized the party's conservatives, which led many conservatives in the galleries to yell and scream at him. A group of moderates tried to rally behind Scranton to stop Goldwater, but Goldwater's forces easily brushed his challenge aside, and Goldwater was nominated on the first ballot. The presidential tally was as follows:
The vice-presidential nomination went to little-known Republican Party Chairman William E. Miller, a Congressman from upstate New York. Goldwater stated that he chose Miller simply because "he drives [President] Johnson nuts".
In accepting his nomination, Goldwater uttered his most famous phrase (a quote from Cicero suggested by speechwriter Harry Jaffa): "I would remind you that extremism in the defense of liberty is no vice. And let me remind you also that moderation in the pursuit of justice is no virtue." For many GOP moderates, Goldwater's speech was seen as a deliberate insult, and many of these moderates would defect to the Democrats in the fall election.
General election.
Campaign.
Although Goldwater had been successful in rallying conservatives, he was unable to broaden his base of support for the general election. Shortly before the Republican Convention, he had alienated most moderate Republicans by his vote against the Civil Rights Act of 1964, which Johnson championed and signed into law. Goldwater's campaign manager, Denison Kitchel, a Phoenix lawyer and friend of nearly three decades, wrote Goldwater's speech explaining his opposition to the Civil Rights Act. The Johnson camp used the vote against the legislation to paint Goldwater as a racist, even though Goldwater supported the civil rights cause in general, and voted in favor of the 1957 and 1960 Civil Rights acts. Goldwater considered desegregation a matter for the individual states, rather than a national policy. Goldwater was famous for speaking "off-the-cuff" at times, and many of his former statements were given wide publicity by the Democrats. In the early 1960s, Goldwater had called the Eisenhower administration "a dime store New Deal", and the former president never fully forgave him or offered him his full support in the election.
In December 1961, he told a news conference that "sometimes I think this country would be better off if we could just saw off the Eastern Seaboard and let it float out to sea", a remark which indicated his dislike of the liberal economic and social policies that were often associated with that part of the nation. That comment came back to haunt him, in the form of a Johnson television commercial, as did remarks about making Social Security voluntary and selling the Tennessee Valley Authority. In his most famous verbal gaffe, Goldwater once joked that the U.S. military should "lob one [a nuclear bomb] into the men's room of the Kremlin" in the Soviet Union.
Goldwater was also hurt by the reluctance of many prominent moderate Republicans to support him. Governors Nelson Rockefeller of New York and George Romney of Michigan refused to endorse Goldwater and did not campaign for him. On the other hand, former Vice-President Richard Nixon and Governor Scranton of Pennsylvania loyally supported the GOP ticket and campaigned for Goldwater, although Nixon did not entirely agree with Goldwater's political stances and said that it would "be a tragedy" if Goldwater's platform were not "challenged and repudiated" by the Republicans. The "New York Herald-Tribune", a voice for eastern Republicans (and a target for Goldwater activists during the primaries), supported Johnson in the general election. Some moderates even formed a "Republicans for Johnson" organization, although most prominent GOP politicians avoided being associated with it.
Shortly before the Republican convention, CBS reporter Daniel Schorr wrote from Germany that "It looks as though Senator Goldwater, if nominated, will be starting his campaign here in Bavaria, center of Germany's right wing" and "Hitler's stomping ground" and finding that Goldwater would be speaking to a gathering of "right-wing Germans" uniting the extremists in both countries. In fact, there was no such meeting nor trip planned.
"Fact" magazine published an article polling psychiatrists around the country as to Goldwater's sanity. 1,189 psychiatrists appeared to agree that Goldwater was "emotionally unstable" and unfit for office, though none of the members had actually interviewed Goldwater. The article received heavy publicity. In a libel suit, Goldwater was ultimately awarded $75 000 in compensation.
Eisenhower's strong backing could have been an asset to the Goldwater campaign, but instead its absence was clearly noticed. When questioned about the presidential capabilities of the former President's younger brother, university administrator Milton S. Eisenhower, in July 1964, Goldwater replied, "One Eisenhower in a generation is enough." However, Eisenhower did not openly repudiate Goldwater and made one television commercial for Goldwater's campaign. A prominent Hollywood celebrity who vigorously supported Goldwater was Ronald Reagan. Reagan gave a well-received televised speech supporting Goldwater; it was so popular that Goldwater's advisors had it played on local television stations around the nation. Many historians consider this speech - "A Time for Choosing" - to mark the beginning of Reagan's transformation from an actor to a political leader. In 1966, Reagan would be elected Governor of California in a landslide.
Johnson positioned himself as a moderate and succeeded in portraying Goldwater as an extremist. Goldwater had a habit of making blunt statements about war, nuclear weapons, and economics that could be turned against him. Most famously, the Johnson campaign broadcast a television commercial on September 7 dubbed the "Daisy Girl" ad, which featured a little girl picking petals from a daisy in a field, counting the petals, which then segues into a launch countdown and a nuclear explosion. The ads were in response to Goldwater's advocacy of "tactical" nuclear weapons use in Vietnam. "Confessions of a Republican", another Johnson ad, features a monologue from a man who tells us that he had previously voted for Eisenhower and Nixon, but now worries about the "men with strange ideas", "weird groups" and "the head of the Ku Klux Klan" who were supporting Goldwater: he concludes that "either they're not Republicans, or I'm not". Voters increasingly viewed Goldwater as a right wing fringe candidate—his slogan "In your heart, you know he's right" was successfully parodied by the Johnson campaign into "In your guts, you know he's nuts", or "In your heart, you know he might" (as in push the nuclear button), or even "In your heart, he's too far right" (some cynics wore buttons saying "Even "Johnson" is better than Goldwater!")
The Johnson campaign's greatest concern may have been voter complacency leading to low turnout in key states. To counter this, all of Johnson's broadcast ads concluded with the line: "Vote for President Johnson on November 3. The stakes are too high for you to stay home." The Democratic campaign used two other slogans, "All the way with LBJ" and "LBJ for the USA".
The election campaign was disrupted for a week by the death of former president Herbert Hoover on October 20, 1964, because it was considered disrespectful to be campaigning during a time of mourning. Hoover died of natural causes. He had been President of the United States from 1929 to 1933. Both major candidates attended his funeral.
Johnson led in all opinion polls by huge margins throughout the entire campaign.
Results.
The election was held on November 3, 1964. Johnson beat Goldwater in the general election, winning over 61 percent of the popular vote, the highest percentage since the popular vote first became widespread in 1824. In the end, Goldwater won only his native state of Arizona and five Deep South states--Louisiana, Mississippi, Georgia, Alabama and South Carolina—that had been increasingly alienated by Democratic civil rights policies. This was the best showing in the South for a GOP candidate since Reconstruction.
The five Southern states that voted for Goldwater swung over dramatically to support him. For instance, in Mississippi, where Democrat Franklin D. Roosevelt had won 98% of the popular vote in 1936, Goldwater won 87% of the vote. Of these states, Louisiana had been the only state where a Republican had won even once since Reconstruction. Mississippi, Alabama and South Carolina had not voted Republican in any presidential election since Reconstruction, and Georgia had never voted Republican even during Reconstruction (thus making Goldwater the first Republican to ever carry Georgia).
The 1964 election was a major transition point for the South, and an important step in the process by which the Democrats' former "Solid South" became a Republican bastion. Nonetheless, Johnson still managed to eke out a bare popular majority of 51–49% (6.307 to 5.993 million) in the eleven former Confederate states. Conversely, Johnson was the first Democrat ever to carry the state of Vermont in a Presidential election, and only the second Democrat to carry Maine in the 20th century. Maine and Vermont had been the only states that FDR had failed to carry during any of his four successful presidential bids.
Of the 3,126 counties/districts/independent cities making returns, Johnson won in 2,294 (73.38%) while Goldwater carried 826 (26.42%). Unpledged Electors carried six counties in Alabama (0.19%).
The Johnson landslide defeated many conservative Republican congressmen, giving him a majority that could overcome the conservative coalition.
This is the first election to have participation of the District of Columbia under the 23rd Amendment to the US Constitution.
The Johnson campaign broke two American election records previously held by Franklin Roosevelt: the most number of Electoral College votes won by a major-party candidate running for the White House for the first time (with 486 to the 472 won by Roosevelt in 1932) and the largest share of the popular vote under the current Democratic/Republican competition (Roosevelt won 60.8% nationwide, Johnson 61.1%). This first-time electoral count was exceeded when Ronald Reagan won 489 votes in 1980. Johnson retains the highest percentage of the popular vote as of the 2012 election.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Close states.
Margin of victory less than 5% (23 electoral votes):
Margin of victory over 5%, but less than 10% (40 electoral votes):
Consequences.
Although Goldwater was decisively defeated, some political pundits and historians believe he laid the foundation for the conservative revolution to follow. Ronald Reagan's speech on Goldwater's behalf, grassroots organization, and the conservative takeover (although temporary in the 1960s) of the Republican party would all help to bring about the "Reagan Revolution" of the 1980s. Indeed, many of today's leading politicians first entered politics to work for Goldwater, including Hillary Clinton.
Johnson went from his victory in the 1964 election to launch the Great Society program at home, signing the Voting Rights Act of 1965 and starting the War on Poverty. He also escalated the Vietnam War, which eroded his popularity. By 1968, Johnson's popularity had declined and the Democrats became so split over his candidacy that he withdrew as a candidate. Moreover, his support of civil rights for African Americans helped split white union members and Southerners away from Franklin Roosevelt's Democratic New Deal Coalition, which would later lead to the phenomenon of the "Reagan Democrat". Of the twelve presidential elections that followed up to 2012, Democrats would win only five times. Columnist George Will had this to say about the lasting effects of the 1964 election: "It took 16 years to count the votes, and Goldwater won."
The election also furthered the shift of the African-American voting electorate away from the Republican Party, a phenomenon which had begun with the New Deal. Since the 1964 election, Democratic presidential candidates have almost consistently won at least 80-90% of the African-American vote in each presidential election.

</doc>
<doc id="40567" url="http://en.wikipedia.org/wiki?curid=40567" title="United States presidential election, 1968">
United States presidential election, 1968

 Lyndon B. Johnson
Richard Nixon
The United States presidential election of 1968 was the 46th quadrennial presidential election, held on Tuesday, November 5, 1968. The Republican nominee, former Vice President Richard Nixon, won the election over the Democratic nominee, incumbent Vice President Hubert Humphrey. Nixon ran on a campaign that promised to restore law and order to the nation's cities, torn by riots and crime.
Analysts have argued the election of 1968 is a realigning election as it permanently disrupted the New Deal Coalition that had dominated presidential politics for 36 years. Coming four years after Democrat Lyndon B. Johnson won in a historic landslide, the election saw the incumbent president forced out of the race and a Republican elected for the first time in twelve years. It was a wrenching national experience, conducted during a year of violence that included the assassination of civil rights leader Martin Luther King, Jr., and subsequent race riots across the nation, the assassination of Democratic presidential candidate Robert F. Kennedy, widespread opposition to the Vietnam War across university campuses, and violent confrontations between police and anti-war protesters at the 1968 Democratic National Convention as the Democratic party split again and again.
The election featured the strongest third party effort since the 1912 presidential election by former Alabama Governor George Wallace. Wallace was a vocal advocate for racial segregation in public schools - a position which gained much popularity in his home state, and across much of the Deep South.
Because Wallace's campaign opposed federal intervention in the South to end school segregation, he carried the Deep South and ran well in ethnic enclave industrial districts in the North.
This was the last election in which New York had the most votes in the electoral college (43 votes). After the 1970 census, California gained the most electoral votes and has remained the most populous state since then. This was also the last election where at least one state was carried by a third-party candidate. (John Hospers received an electoral vote from Virginia in the next election but did not carry any states.)
This is the most recent election where the victorious national ticket failed to carry the vice presidential candidate's home state. It is also the first (and as of 2012 only) election where an incumbent vice president ran against a former vice president for the presidency.
Historical background.
In the election of 1964, incumbent Democrat Lyndon B. Johnson won the largest popular vote landslide in U.S. Presidential election history over Republican Barry Goldwater. During the presidential term that followed, Johnson was able to achieve many political successes, including the passage of his sweeping Great Society domestic programs (also known as the "War on Poverty"), landmark civil rights legislation, and the continued exploration of space. At the same time, however, the country endured large-scale race riots in the streets of its larger cities, along with a generational revolt of young people and violent debates over foreign policy. The emergence of the hippie counterculture, the rise of New Left activism, and the emergence of the Black Power movement exacerbated social and cultural clashes between classes, generations, and races. Every summer since 1964, major cities erupted in massive race riots that left hundreds dead or injured and destroyed hundreds of millions of dollars in property. Adding to the national crisis, on April 4, 1968, civil rights leader Rev. Martin Luther King, Jr., was assassinated, igniting further mass rioting and chaos, including Washington, D.C., where there was rioting within just a few blocks of the White House and machine guns were stationed on the Capitol steps to protect it.
The most important reason for the precipitous decline of President Johnson's popularity was the Vietnam War, which he greatly escalated during his time in office. By late 1967, over 500,000 American soldiers were fighting in Vietnam. Draftees made up 42 percent of the military in Vietnam, but suffered 58% of the casualties as nearly 1000 Americans a month were killed and many more were injured. Johnson's position was particularly damaged when the national news media began to focus on the high costs and ambiguous results of escalation, despite his repeated efforts to downplay the seriousness of the situation.
In early January 1968, Secretary of Defense Robert McNamara stated that the war would be winding down as the North Vietnamese were losing their will to fight, but shortly thereafter, they launched the Tet Offensive, in which the North Vietnamese and Communist Vietcong forces launched simultaneous attacks on all government strongholds in South Vietnam. The Tet episode led many Americans to ponder whether the war was winnable or worth it. In addition, they felt they could not trust their government's assessment and reporting of the war effort. The Pentagon called for sending several hundred thousand more soldiers to Vietnam. Johnson's approval ratings fell below 35%, and the Secret Service refused to let the president make public appearances on the campuses of American colleges and universities, due to his extreme unpopularity among college students. The Secret Service also prevented Johnson from appearing at the 1968 Democratic National Convention in Chicago, because it could not guarantee his safety from assassination.
Nominations.
Democratic Party nomination.
Democratic candidates
Enter Eugene McCarthy.
Because Lyndon Johnson had been elected to the presidency only once, in 1964, and had served less than two full years of the term before that, the 22nd Amendment did not disqualify him from running for another term; Johnson had served only 12 months following the assassination of John F. Kennedy before being elected in 1964 to a full term. As a result, it was widely assumed when 1968 began that President Johnson would run for another term, and that he would have little trouble winning the Democratic nomination.
Despite growing opposition to Johnson's policies in Vietnam, it appeared that no prominent Democratic candidate would run against a sitting president of his own party. It was also accepted at the beginning of the year that Johnson's record of domestic accomplishments would overshadow public opposition to the Vietnam War and that he would easily boost his public image after he started campaigning. Even Senator Robert F. Kennedy of New York, an outspoken critic of Johnson's policies with a large base of support, initially declined to run against Johnson in the primaries. Poll numbers also suggested that a large share of Americans who opposed the Vietnam War felt the growth of the anti-war hippie movement among younger Americans was not helping their cause. On January 30, however, claims by the Johnson administration that a recent troop surge would soon bring an end to the war were severely discredited when the Tet Offensive broke out. While the American military was eventually able to fend off the attacks, and also inflict heavy losses among the communist opposition, the ability of the North Vietnamese Army and Viet Cong to launch large scale attacks during the Tet Offensive's long duration greatly weakened American support for the military draft and further combat operations in Vietnam. 
In time, only Senator Eugene McCarthy of Minnesota proved willing to challenge Johnson openly. Running as an anti-war candidate in the New Hampshire primary, McCarthy hoped to pressure the Democrats into publicly opposing the Vietnam War. Since New Hampshire was the first presidential primary of 1968, McCarthy poured most of his limited resources into the state. He was boosted by thousands of young college students led by youth coordinator Sam Brown, who shaved their beards and cut their hair to be "Clean for Gene". These students organized get-out-the-vote drives, rang doorbells, distributed McCarthy buttons and leaflets, and worked hard in New Hampshire for McCarthy. On March 12, McCarthy won 42 percent of the primary vote to Johnson's 49 percent, a shockingly strong showing against an incumbent president. Even more impressively, since Johnson had more than 24 supporters running for the Democratic National Convention delegate slots to be filled in the election, while McCarthy's campaign organized more strategically, McCarthy won 20 of the 24 delegates. This gave McCarthy's campaign legitimacy and momentum.
Sensing Johnson's vulnerability, Senator Robert F. Kennedy announced his candidacy four days after the New Hampshire primary. Thereafter, McCarthy and Kennedy engaged in a series of state primaries. Kennedy won most of the primaries in which he and McCarthy were in direct competition.
Johnson withdraws.
On March 31, 1968, following the New Hampshire primary and Kennedy's entry into the election, the president announced to the nation in a televised speech that he was suspending all bombing of North Vietnam in favor of peace talks. Johnson concluded his speech and startled the nation by announcing "With America's sons in the fields far away, with America's future under challenge right here at home, with our hopes and the world's hopes for peace in the balance every day, I do not believe that I should devote an hour or a day of my time to any personal partisan causes or to any duties other than the awesome duties of this office—the presidency of your country. Accordingly, I shall not seek, and I will not accept, the nomination of my party for another term as your President." Not discussed publicly at the time was Johnson's concern he might not survive another term—Johnson's health was poor, and he had suffered a serious heart attack in 1955 while serving in the U.S. Senate. Indeed, he died on January 22, 1973, only two days after the new presidential term would have concluded. Had he been re-elected again, it is entirely possible that he would have died in office. Bleak political forecasts also contributed to Johnson's withdrawal; internal polling by Johnson's campaign in Wisconsin, the next state to hold a primary election, showed the President trailing badly.
Historians have debated just why Johnson quit a few days after his weak showing in New Hampshire. Shesol says Johnson wanted out of the White House but also wanted vindication; when the indicators turned negative he decided to leave. Gould says that Johnson had neglected the party, was hurting it by his Vietnam policies, and underestimated McCarthy's strength until the very last minute, when it was too late for Johnson to recover. Woods said Johnson realized he needed to leave in order for the nation to heal. Dallek says that Johnson had no further domestic goals, and realized that his personality had eroded his popularity. His health was not good, and he was preoccupied with the Kennedy campaign; his wife was pressing for his retirement and his base of support continue to shrink. Leaving the race would allow him to pose as a peacemaker. Bennett, however, says Johnson, "had been forced out of a reelection race in 1968 by outrage over his policy in Southeast Asia.
It has also been reported that Johnson decided to wind down his re-election bid after popular and influential CBS News anchor Walter Cronkite turned against the president's policy in Vietnam and recommended peace negotiations during a CBS News editorial which aired on February 27. After later watching Cronkite's editorial, Johnson allegedly exclaimed "if I've lost Cronkite, I've lost Middle America." Issues surrounding reports of this allegation have raised questions about its accuracy, such as the fact that Johnson was attending Texas Governor John Connally's birthday gala in Austin, Texas, when Cronkite's editorial aired and thus was unable to see the original broadcast. However, Cronkite and CBS News correspondent Bob Schieffer defended reports that the remark had been made and claimed that members of Johnson's inner circle who had watched the editorial with the president, including presidential aide George Christian and journalist Bill Moyers, were able to confirm its accuracy to them at a later time. Schieffer, who was serving as a reporter for the "Star-Telegram"'s WBAP television station in Fort Worth, Texas, when Cronkite's editorial aired, acknowledged reports that the president saw the editorial's original broadcast were inaccurate, but claimed the president was able to watch a taping of it the morning after it aired and then made the remark.
With Johnson's withdrawal, the Democratic Party quickly split into four factions, each of which distrusted the other three.
Since the Vietnam War had become the major issue that was dividing the Democratic Party, and Johnson had come to symbolize the war for many liberal Democrats, Johnson believed that he could not win the nomination without a major struggle, and that he would probably lose the election in November to the Republicans. However, by withdrawing from the race he could avoid the stigma of defeat, and he could keep control of the party machinery by giving the nomination to Humphrey, who had been a loyal vice-president. Milne (2011) argues that, in terms of foreign-policy in the Vietnam War, Johnson at the end wanted Nixon to be president rather than Humphrey, since Johnson agreed with Nixon, rather than Humphrey, on the need to defend South Vietnam from communism. However, Johnson's telephone calls show that Johnson believed the Nixon camp was deliberately sabotaging the Paris peace talks. He told Humphrey, who refused to use allegations based on illegal wiretaps of a presidential candidate. Nixon himself called Johnson and denied the allegations. Dallek concludes that Nixon's advice to Saigon made no difference, and that Humphrey was so closely identified with Johnson's unpopular policies that no last-minute deal with Hanoi could have affected the election.
Contest.
After Johnson's withdrawal, Vice- President Hubert Humphrey announced his candidacy. Kennedy was successful in four state primaries (Indiana, Nebraska, South Dakota, and California) and McCarthy won six (Wisconsin, Pennsylvania, Massachusetts, Oregon, New Jersey, and Illinois). However, in primaries where they campaigned directly against one another, Kennedy won three primaries (Indiana, Nebraska, and California) and McCarthy won one (Oregon). Humphrey did not compete in the primaries, leaving that job to favorite sons who were his surrogates, notably Senator George A. Smathers from Florida, Senator Stephen M. Young from Ohio, and Governor Roger D. Branigin of Indiana. Instead, Humphrey concentrated on winning the delegates in non-primary states, where party leaders such as Chicago Mayor Richard J. Daley controlled the delegate votes in their states. Kennedy defeated Branigin and McCarthy in the Indiana primary, and then defeated McCarthy in the Nebraska primary. However, McCarthy upset Kennedy in the Oregon primary.
After Kennedy's defeat in Oregon, the California primary was seen as crucial to both Kennedy and McCarthy. McCarthy stumped the state's many colleges and universities, where he was treated as a hero for being the first presidential candidate to oppose the war. Kennedy campaigned in the ghettos and barrios of the state's larger cities, where he was mobbed by enthusiastic supporters. Kennedy and McCarthy engaged in a television debate a few days before the primary; it was generally considered a draw. On June 4, Kennedy narrowly defeated McCarthy in California, 46%–42%. However, McCarthy refused to withdraw from the race and made it clear that he would contest Kennedy in the upcoming New York primary, where McCarthy had much support from anti-war activists in New York City. The New York primary quickly became a moot point, however, for in the early morning of June 5, Kennedy was shot shortly after midnight; he died twenty-six hours later. Kennedy had just given his victory speech in a crowded ballroom of the Ambassador Hotel in Los Angeles; he and his aides then entered a narrow kitchen pantry on their way to a banquet room to meet with reporters. In the pantry Kennedy and five others were shot by Sirhan Sirhan, a Christian Palestinian who hated Kennedy because of his support for Israel. Sirhan admitted his guilt, was convicted of murder, and is still in prison. In recent years some have cast a doubt on this, including Sirhan himself, who said he was "brainwashed" into killing Kennedy and was a patsy.
Political historians have debated to this day whether Kennedy could have won the Democratic nomination had he lived. Some historians, such as Theodore H. White and Arthur M. Schlesinger, Jr., have argued that Kennedy's broad appeal and famed charisma would have convinced the party bosses at the Democratic Convention to give him the nomination. Jack Newfield, author of "RFK: A Memoir", stated in a 1998 interview that on the night he was assassinated, "[Kennedy] had a phone conversation with Mayor Daley of Chicago, and Mayor Daley all but promised to throw the Illinois delegates to Bobby at the convention in August 1968. I think he said to me, and Pete Hamill, 'Daley is the ball game, and I think we have Daley.'" However, other writers such as Tom Wicker, who covered the Kennedy campaign for "The New York Times", believe that Humphrey's large lead in delegate votes from non-primary states, combined with Senator McCarthy's refusal to quit the race, would have prevented Kennedy from ever winning a majority at the Democratic Convention, and that Humphrey would have been the Democratic nominee even if Kennedy had lived. The journalist Richard Reeves and historian Michael Beschloss have both written that Humphrey was the likely nominee, and future Democratic National Committee chairman Larry O'Brien wrote in his memoirs that Kennedy's chances of winning the nomination had been slim, even after his win in California.
At the moment of RFK's death, the delegate totals were:
Total popular vote:
Democratic Convention and antiwar protests.
Robert Kennedy's death altered the dynamics of the race. Although Humphrey appeared the prohibitive favorite for the nomination, thanks to his support from the traditional power blocs of the party, he was an unpopular choice with many of the anti-war elements within the party, who identified him with Johnson's controversial position on the Vietnam War. However, Kennedy's delegates failed to unite behind a single candidate who could have prevented Humphrey from getting the nomination. Some of Kennedy's support went to McCarthy, but many of Kennedy's delegates, remembering their bitter primary battles with McCarthy, refused to vote for him. Instead, these delegates rallied around the late-starting candidacy of Senator George McGovern of South Dakota, a Kennedy supporter in the spring primaries who had presidential ambitions himself. This division of the anti-war votes at the Democratic Convention made it easier for Humphrey to gather the delegates he needed to win the nomination.
When the 1968 Democratic National Convention opened in Chicago, thousands of young activists from around the nation gathered in the city to protest the Vietnam War. On the evening of August 28, in a clash which was covered on live television, Americans were shocked to see Chicago police brutally beating anti-war protesters in the streets of Chicago in front of the Conrad Hilton Hotel. While the protesters chanted "The whole world's watching," the police used clubs and tear gas to beat back or arrest the protesters, leaving many of them bloody and dazed. The tear gas wafted into numerous hotel suites; in one of them Vice-President Humphrey was watching the proceedings on television. However, the police claimed that their actions were justified because numerous police officers were being injured by bottles, rocks, and broken glass that were being thrown at them by the protestors. The protestors had also yelled verbal insults at the police, calling them "pigs" and other epithets. The anti-war and police riot divided the Democratic Party's base: some supported the protestors and felt that the police were being heavy-handed, but others disapproved of the violence and supported the police. Meanwhile, the convention itself was marred by the strong-arm tactics of Chicago's mayor Richard J. Daley (who was seen on television angrily cursing Senator Abraham Ribicoff of Connecticut, who made a speech at the convention denouncing the excesses of the Chicago police). In the end, the nomination itself was anti-climactic, with Vice-President Humphrey handily beating McCarthy and McGovern on the first ballot.
After the delegates nominated Humphrey, the convention then turned to selecting a vice-president. The main candidates for this position were Senators Edward M. Kennedy of Massachusetts, Edmund Muskie of Maine, and Fred R. Harris of Oklahoma; Governors Richard Hughes of New Jersey and Terry Sanford of North Carolina; Mayor Joseph Alioto of San Francisco, California; former Deputy Secretary of Defense Cyrus Vance of West Virginia; and Ambassador Sargent Shriver of Maryland. Another idea floated was to tap Republican Governor Nelson Rockefeller of New York, one of the most liberal Republicans. Ted Kennedy was Humphrey's first choice, but the senator turned him down. After narrowing it down to Senator Muskie and Senator Harris, Vice-President Humphrey chose Muskie, a moderate and environmentalist from Maine, for the nomination. The convention complied with the request and nominated Senator Muskie as Humphrey's running mate.
However, the tragedy of the anti-war riots crippled Humphrey's campaign from the start, and it never fully recovered. Before 1968 the city of Chicago had been a frequent host for the political conventions of both parties; since 1968 only once has a national convention been held in the city (in 1996, the Democratic Party held the convention which nominated Bill Clinton there). Many believe that this is due in part to the violence and chaos of the Democratic Convention that year.
Source: Keating Holland, "All the Votes... Really," CNN
Endorsements.
Hubert Humphrey
Robert F. Kennedy
Eugene McCarthy
George McGovern (during convention)
Republican Party nomination.
Primaries.
The front-runner for the Republican nomination was former Vice President Richard Nixon, who formally began campaigning in January 1968. Nixon had worked tirelessly behind the scenes and was instrumental in Republican gains in Congress and governorships in the 1966 midterm elections. Thus, the party machinery and many of the new congressmen and governors supported him. Still, there was wariness in the Republican ranks over Nixon, who had lost the 1960 election and then lost the 1962 California gubernatorial election. Some hoped a more "electable" candidate would emerge. To a great extent the story of the 1968 Republican primary campaign and nomination is the story of one Nixon opponent after another entering the race and then dropping out. Nixon was always clearly the front runner throughout the contest because of his superior organization, and he easily defeated the rest of the field.
Nixon's first challenger was Michigan Governor George W. Romney. A Gallup poll in mid-1967 showed Nixon with 39%, followed by Romney with 25%. However, Romney, after a fact finding trip to Vietnam, told a news reporter that he had been "brainwashed" by the military and the diplomatic corps into supporting the Vietnam War; the remark led to weeks of ridicule in the national news media. Since he had turned against American involvement in Vietnam, Romney planned to run as the anti-war Republican version of Eugene McCarthy. However, following his "brainwashing" comment, Romney's support faded steadily, and with polls showing him far behind Nixon he withdrew from the race on February 28, 1968.
Senator Charles Percy was considered another potential threat to Nixon even before Romney's withdrawal, and had planned on potentially waging an active campaign after securing a role as Illinois's favorite son. Later however Percy declined to have his name presented on the ballot for the Illinois presidential primary, and while he never disclaimed his interest in the presidential nomination, he no longer actively sought it either.
Nixon won a resounding victory in the important New Hampshire primary on March 12, with 78% of the vote. Anti-war Republicans wrote in the name of New York Governor Nelson Rockefeller, the leader of the Republican Party's liberal wing, who received 11% of the vote and became Nixon's new challenger. Rockefeller had originally not intended to run, having discounted a campaign for the nomination in 1965 and planned on making Senator Jacob Javits the favorite son, either in preparation of a presidential campaign or to secure him the second spot on the ticket; as Rockefeller warmed to the idea of entering the race again however, Javits moved his attentions back towards seeking a third term in the Senate. Nixon led Rockefeller in the polls throughout the primary campaign, and though Rockefeller defeated Nixon and Governor John Volpe in the Massachusetts primary on April 30, he otherwise fared poorly in state primaries and conventions, having declared too late to place his name on state ballots.
By early spring, California Governor Ronald Reagan, the leader of the Republican Party's conservative wing, had become Nixon's chief rival. In the Nebraska primary on May 14, Nixon won with 70% of the vote to 21% for Reagan and 5% for Rockefeller. While this was a wide margin for Nixon, Reagan remained Nixon's leading challenger. Nixon won the next primary of importance, Oregon, on May 15 with 65% of the vote and won all the following primaries except for California (June 4), where only Reagan appeared on the ballot. Reagan's victory in California gave him a plurality of the nationwide primary vote, but his poor showing in most other state primaries left him far behind Nixon in the actual delegate count.
Total popular vote:
Republican Convention.
As the 1968 Republican National Convention opened in Miami Beach, Florida, the Associated Press estimated that Nixon had 656 delegate votes - only 11 short of the number he needed to win the nomination. His only remaining obstacles were Reagan and Rockefeller, who were planning to unite their forces in a "stop-Nixon" movement.
Because Goldwater had done well in the Deep South, delegates to the 1968 Republican National Convention would be more Southern and conservative than past conventions. There was a real possibility that the conservative Reagan would be nominated if there was no victor on the first ballot. Nixon narrowly secured the nomination on the first ballot, with the aid of South Carolina Senator Strom Thurmond, who had switched parties in 1964. He selected dark horse Maryland Governor Spiro Agnew as his running mate, a choice which Nixon believed would unite the party, appealing to both Northern moderates and Southerners disaffected with the Democrats. It was also reported that Nixon's first choice for running mate was his longtime friend and ally, Robert Finch, who was the Lieutenant Governor of California at the time, but Finch declined the offer. Finch would later serve as the Secretary of Health, Education, and Welfare in Nixon's Administration. With Vietnam also a key issue, Nixon strongly considered tapping his 1960 running mate, Henry Cabot Lodge, Jr, who was a former U.S. Senator, Ambassador to the UN, and Ambassador twice to South Vietnam.
Candidates for the Vice-Presidential nomination:
As of 2012, this was the last time two siblings (Nelson and Winthrop Rockefeller) ran against each other in a Presidential primary.
George Wallace and the American Independent Party.
The American Independent Party, which was established in 1967 by Bill and Eileen Shearer, nominated former Alabama Governor George Wallace – whose pro-segregation policies had been rejected by the mainstream of the Democratic Party – as the party's candidate for president. The impact of the Wallace campaign was substantial, winning the electoral votes of several states in the Deep South. Wallace was the most popular 1968 presidential candidate among young men. Wallace also proved to be popular among blue-collar workers in the North and Midwest, and he took many votes which might have gone to Humphrey. Although Wallace did not expect to win the election, his strategy was to prevent either major party candidate from winning a preliminary majority in the Electoral College, which would then give him bargaining power to determine the winner. Wallace's running mate was retired U.S. Air Force General Curtis LeMay. Among others who were considered were Ezra Taft Benson, John Wayne and Colonel Sanders.
Prior to deciding on LeMay, Wallace gave serious consideration to former U.S. Senator, Governor, and Baseball Commissioner A.B. Happy Chandler of Kentucky as his running mate. Chandler and Wallace met a number of times, however, Chandler said that he and Wallace were unable to come to an agreement regarding their positions on racial matters. Paradoxically, Chandler supported the segregationist Dixiecrats in the 1948 presidential elections. But, after being reelected governor of Kentucky in 1955, he used National Guard troops to enforce school integration.
LeMay embarrassed Wallace's campaign in the fall by suggesting that nuclear weapons could be used in Vietnam.
Other parties and candidates.
Also on the ballot in two or more states were black activist Eldridge Cleaver (who was ineligible to take office, as he would have only been 33 years of age on January 20, 1969) for the Peace and Freedom Party, Henning Blomen for the Socialist Labor Party, Fred Halstead for the Socialist Workers Party, E. Harold Munn for the Prohibition Party, and Charlene Mitchell – the first African-American woman to run for president – for the Communist Party. Comedians Dick Gregory and Pat Paulsen were notable write-in candidates. A facetious presidential candidate for 1968 was a pig named Pigasus, as a political statement by the Yippies, to illustrate their premise that "one pig's as good as any other."
General election.
Campaign strategies.
Nixon developed a "southern strategy" that was designed to appeal to conservative white southerners, who traditionally voted Democratic, but were deeply angered by Johnson and Humphrey's support for the civil rights movement. Wallace, however, won over many of the voters Nixon targeted, effectively splitting the conservative vote. Indeed, Wallace deliberately targeted many states he had little chance of carrying himself in the hope that by splitting the conservative vote with Nixon he would give those states to Humphrey and, by extension, boost his own chances of denying both opponents an Electoral College majority. The "southern strategy" would prove more effective in subsequent elections and would become a staple of Republican presidential campaigns. Nixon's campaign was also carefully managed and controlled. He often held "town hall" type meetings in cities he visited in which he answered questions from voters that had been screened in advance by his aides.
Since he was well behind Nixon in the polls as the campaign began, Humphrey opted for a slashing, fighting campaign style. He repeatedly - and unsuccessfully - challenged Nixon to a televised debate, and he often compared his campaign to the successful underdog effort of President Harry Truman, another Democrat who had trailed in the polls, in the 1948 presidential election. Humphrey predicted that he, like Truman, would surprise the experts and win an upset victory.
Campaign themes.
Nixon campaigned on a theme to restore "law and order," which appealed to many voters angry with the hundreds of violent riots that had taken place across the country in the previous few years. Following the murder of Dr. King in April 1968, there was severe rioting in Detroit and Washington, D.C., and President Johnson had to call out the U.S. Army to protect lives and property as smoke from burning buildings a few blocks away drifted across the White House lawn. However, Vice-President Humphrey criticized the "law and order" issue, claiming that it was a subtle appeal to white racial prejudice. Nixon also opposed forced busing to desegregate schools. Proclaiming himself a supporter of civil rights, he recommended education as the solution rather than militancy. During the campaign, Nixon proposed government tax incentives to African Americans for small businesses and home improvements in their existing neighborhoods.
During the campaign, Nixon also used as a theme his opposition to the decisions of Chief Justice Earl Warren. Many conservatives were critical of Chief Justice Warren for using the Supreme Court to promote liberal policies in the fields of civil rights, civil liberties, and the separation of church and state. Nixon promised that if he were elected president, he would appoint justices who would take a less-active role in creating social policy. In another campaign promise, he pledged to end the draft. During the 1960s, Nixon had been impressed by a paper he had read by Professor Martin Anderson of Columbia University. Anderson had argued in the paper for an end to the draft and the creation of an all-volunteer army. Nixon also saw ending the draft as an effective way to undermine the anti-Vietnam war movement, since he believed affluent college-age youths would stop protesting the war once their own possibility of having to fight in it was gone.
Humphrey, meanwhile, promised to continue and expand the Great Society welfare programs started by President Johnson, and to continue the Johnson Administration's "War on Poverty." He also promised to continue the efforts of Presidents Kennedy and Johnson, and the Supreme Court, in promoting the expansion of civil rights and civil liberties for minority groups. However, Humphrey also felt constrained for most of his campaign in voicing any opposition to the Vietnam War policies of President Johnson, due to his fear that Johnson would reject any peace proposals he made and undermine his campaign. As a result, early in his campaign Humphrey often found himself the target of anti-war protestors, some of whom heckled and disrupted his campaign rallies.
Humphrey's comeback and the October surprise.
After the Democratic Convention in late August, Humphrey trailed Nixon by double-digits in most polls, and his chances seemed hopeless. According to "Time" magazine, "The old Democratic coalition was disintegrating, with untold numbers of blue-collar workers responding to Wallace's blandishments, Negroes threatening to sit out the election, liberals disaffected over the Vietnam War, the South lost. The war chest was almost empty, and the party's machinery, neglected by Lyndon Johnson, creaked in disrepair." Calling for "the politics of joy," and using the still-powerful labor unions as his base, Humphrey fought back. In order to distance himself from Johnson and to take advantage of the Democratic plurality in voter registration, Humphrey stopped being identified in ads as "Vice-President Hubert Humphrey," instead being labelled "Democratic candidate Hubert Humphrey." Humphrey attacked Wallace as a racist bigot who appealed to the darker impulses of Americans. Wallace had been rising in the polls, and peaked at 21% in September, but his momentum stopped after he selected Curtis LeMay as his running mate. Curtis LeMay's suggestion of tactical nuclear weapons being used in Vietnam conjured up memories of the 1964 Goldwater campaign. Labor unions also undertook a major effort to win back union members who were supporting Wallace, with substantial success. Polls that showed Wallace winning almost one-half of union members in the summer of 1968 showed a sharp decline in his union support as the campaign progressed. As election day approached and Wallace's support in the North and Midwest began to wane, Humphrey finally began to climb in the polls.
In October, Humphrey—who was rising sharply in the polls due to the collapse of the Wallace vote—began to distance himself publicly from the Johnson administration on the Vietnam War, calling for a bombing halt. The key turning point for Humphrey's campaign came when President Johnson officially announced a bombing halt, and even a possible peace deal, the weekend before the election. The "Halloween Peace" gave Humphrey's campaign a badly needed boost. In addition, Senator Eugene McCarthy finally endorsed Humphrey in late October after previously refusing to do so, and by election day the polls were reporting a dead heat.
However, the Nixon campaign had anticipated a possible "October surprise" to boost Humphrey and thwarted any last-minute chances of a "Halloween Peace." Bryce Harlow, former Eisenhower White House staff member, claimed to have "a double agent working in the White House...I kept Nixon informed." Harlow and Nixon's future National Security Advisor and Secretary of State Henry Kissinger, who was friendly with both campaigns and guaranteed a job in either a Humphrey or Nixon administration, separately predicted Johnson's "bombing halt": "The word is out that we are making an effort to throw the election to Humphrey. Nixon has been told of it," Democratic senator George Smathers informed Johnson. According to Robert Dallek in 2007, Kissinger's advice "rested not on special knowledge of decision making at the White House but on an astute analyst's insight into what was happening."
Nixon asked Anna Chennault to be his "channel to Mr. Thieu" in order to advise him to refuse participation in the talks. Thieu was promised a better deal under a Nixon administration. Chennault agreed and periodically reported to John Mitchell that Thieu had no intention of attending a peace conference. On November 2, Chennault informed the South Vietnamese ambassador: "I have just heard from my boss in Albuquerque who says his boss [Nixon] is going to win. And you tell your boss [Thieu] to hold on a while longer." In 1997, Chennault admitted that "I was constantly in touch with Nixon and Mitchell." The effort also involved Texas Senator John Tower and Kissinger, who traveled to Paris on behalf of the Nixon campaign. William Bundy stated that Kissinger obtained "no useful inside information" from his trip to Paris, and "almost any experienced Hanoi watcher might have come to the same conclusion". While Kissinger may have "hinted that his advice was based on contacts with the Paris delegation," this sort of "self-promotion...is at worst a minor and not uncommon practice, quite different from getting and reporting real secrets." In 2007, Conrad Black asserted that there is "no evidence" connecting Kissinger in particular, who was "playing a fairly innocuous double game of self-promotion," with attempts to undermine the peace talks. Black further commented that "the Democrats were outraged at Nixon, but what Johnson was doing was equally questionable," and there is "no evidence" that Thieu "needed much prompting to discern which side he favored in the U.S. election."
Johnson learned of the Nixon-Chennault effort because the NSA was interfering in communications in Vietnam. In 2009, new tapes were declassified revealing that Johnson was enraged and said that Nixon had "blood on his hands" and that Senate Minority Leader Everett Dirksen agreed with Johnson that such action was "treason." Defense Secretary Clark Clifford considered the moves an illegal violation of the Logan Act. In response, Johnson ordered NSA surveillance of Chennault and wire-tapped members the South Vietnamese embassy and members of the Nixon campaign. He did not leak the information to the public because he did not want to "shock America" with the revelation, nor reveal that the NSA was interfering in communications in Vietnam. Johnson did make information available to Humphrey, but at this point Humphrey thought he was going to win the election, so he did not reveal the information to the public. Humphrey later regretted this as a mistake. The South Vietnamese government withdrew from peace negotiations, and Nixon publicly offered to go to Saigon to help the negotiations. Dallek wrote that Nixon's efforts "probably made no difference," because Thieu was unwilling to attend the talks and there was little chance of an agreement being reached before the election. However, his use of information provided by Harlow and Kissinger was morally questionable, and Humphrey's decision not to make Nixon's actions public was "an uncommon act of political decency." Either way, a promising "peace bump" ended up in "shambles" for the Democratic Party.
Election.
The election on November 5, 1968, proved to be extremely close, and it was not until the following morning that the television news networks were able to call Nixon the winner. The key states proved to be California, Ohio, and Illinois, all of which Nixon won by three percentage points or less. Had Humphrey carried all three of these states, he would have won the election. Had Humphrey carried any two of them, or California alone, George Wallace would have succeeded in his aim of preventing an electoral college majority for any candidate, and the decision would have been given to the House of Representatives, at the time controlled by the Democratic Party. Nixon won the popular vote with a plurality of 512,000 votes, or a victory margin of about one percentage point. In the electoral college Nixon's victory was larger, as he carried 32 states with 301 electoral votes, to Humphrey's 13 states and 191 electoral votes and Wallace's five states and 46 electoral votes.
Of the states that Nixon had previously carried in 1960, only Maine and Washington did not vote for him again (He later carried both again in 1972). This was the last time until 1988 that the state of Washington voted Democratic, and last time until 1992 that Connecticut, Maine, and Michigan voted Democratic in a general election. Nixon is also the last Republican candidate to win a presidential election without carrying any of these states: Alabama, Arkansas, Louisiana, Mississippi and Texas.
Of the 3,130 counties/districts/independent cities making returns, Nixon won in 1,859 (59.39%) while Humphrey carried 693 (22.14%). Wallace was victorious in 578 counties (18.47%), all of which were located in the South.
Nixon said that Humphrey left a gracious message congratulating him, noting "I know exactly how he felt. I know how it feels to lose a close one."
Aftermath.
Nixon's victory is often considered a realigning election in American politics. From 1932 to 1964, the Democratic Party was undoubtedly the majority party. During that time period, Democrats won seven out of nine presidential elections, and their agenda influenced policies undertaken by the Republican Eisenhower administration. The election of 1968 reversed the situation completely. From 1968 until 2004, Republicans won seven out of ten presidential elections, and its policies clearly affected those enacted by the Democratic Clinton administration.
The election was a significant event in the long-term realignment in Democratic Party support, especially in the South. Nationwide, the bitter splits over civil rights, the new left, the Vietnam War, and other "culture wars" were slow to heal. Democrats could no longer count on white Southern support for the presidency, as Republicans made major gains in suburban areas and areas filled with Northern migrants. The rural Democratic "courthouse cliques" in the South lost power. While Democrats controlled local and state politics in the South, Republicans usually won the presidential vote. In 1968 Humphrey won less than 10% of the white Southern vote, with 2/3 of his vote in the region coming from blacks, who now were voting in full strength.
From 1968 until 2004, only two Democrats were elected President, both native Southerners - Jimmy Carter of Georgia and Bill Clinton of Arkansas. Not until 2008 did a Northern Democrat, Barack Obama, win a presidential election.
Another important result of the 1968 election was that it led to several reforms in how the Democratic Party chose its presidential nominees. In 1969, the McGovern–Fraser Commission adopted a set of rules for the states to follow in selecting convention delegates. These rules reduced the influence of party leaders on the nominating process and provided greater representation for minorities, women, and youth. The reforms led most states to adopt laws requiring primary elections, instead of party leaders, to choose delegates.
After, 1968 the only way to win the party's presidential nomination was through the primary process; Humphrey turned out to be the last nominee of either major party to win his party's nomination without having directly competed in the primaries.
This was also the last election in which any third party candidate won an entire state's electoral votes, with Wallace carrying five states.
Results.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Close states.
States where margin of victory was less than 5 percentage points (223 electoral votes):
States where margin of victory was more than 5 percentage points, but less than 10 percentage points (155 electoral votes):
Notes: In Alabama, Wallace was the official Democratic Party nominee, while Humphrey ran on the ticket of short-lived National Democratic Party of Alabama, loyal to him as an official Democratic Party nominee
In North Carolina one Nixon Elector cast his ballot for George Wallace (President) and Curtis LeMay (Vice President).
Voters.
Converse et al. (1969) assesses the significance of the 1968 presidential election. Many people were dissatisfied with Johnson but not with the Democratic Party. Their dissatisfaction was connected to the Vietnam War, civil rights, and law and order. Humphrey, the choice of an older generation, suffered from identification with the Johnson administration. Wallace attracted many Richard Nixon-bound Democrats and thus was a threat to Nixon. McCarthy's popularity in the New Hampshire primary was a nondifferentiated anti-Johnson vote from which many voters went to Wallace. These were not, however, the hard-core McCarthyite followers. Wallace's success was the result of the voter's identification with his stand on the primary issues - continuing segregation, getting tough in Vietnam, and law and order. Wallace's supporters were rural and small town residents in the South and skilled labor in the North. They were disgusted with public policy and were often alienated from politics. The Wallace candidacy helped to bend American politics and politicians to conservatism.
National voter demographics.
Source: "Congressional Quarterly Weekly Report." "Group Analysis of the 1968 Presidential Vote" XXVI, No. 48 (November 1968), p. 3218.
Voter demographics in the South.
Source: "Congressional Quarterly Weekly Report." "Group Analysis of the 1968 Presidential Vote", XXVI, No. 48 (November 1968), p. 3218.

</doc>
<doc id="40568" url="http://en.wikipedia.org/wiki?curid=40568" title="United States presidential election, 1972">
United States presidential election, 1972

 Richard Nixon
Richard Nixon
The United States presidential election of 1972 was the 47th quadrennial presidential election, held on Tuesday, November 7, 1972. The Democratic Party's nomination was eventually won by Senator George McGovern of South Dakota, who ran an anti-war campaign against incumbent Republican President Richard Nixon, but was handicapped by his outsider status, limited support from his own party, the perception of many voters that he was a left-wing extremist and the scandal that resulted from the firing of vice-presidential nominee Thomas Eagleton.
Emphasizing a good economy and his successes in foreign affairs, such as coming near to ending American involvement in the Vietnam War and establishing relations with China, Nixon decisively defeated McGovern. Overall, Nixon won 60.7% of the popular vote, a percentage only slightly lower than Lyndon B. Johnson in 1964, but with a larger margin of victory in the popular vote (23.2%), the fourth largest in presidential election history. He received almost 18 million more popular votes than McGovern, the widest margin of any United States presidential election. McGovern only won the electoral votes of Massachusetts and the District of Columbia. No candidate since had managed to equal or surpass Nixon's total percentage or margin of the popular vote, and his electoral vote total and percentage has been surpassed only once, and his state total matched only once, by Ronald Reagan in 1984.
Also in this election, Shirley Chisholm became the first African American to run for a major party nomination, and Patsy Mink was the first Asian American candidate to run for the Democratic Party nomination. It also was the first time that Hawaii was carried by a Republican, becoming the last of the 50 states to do so. Together with the House and Senate elections of 1972, it was the first electoral event in which people aged 18 to 20 could vote in any state, according to the provisions of the Twenty-sixth Amendment to the United States Constitution. This is also the most recent presidential election where at least one electoral vote was won by a candidate who, at the time of the election, was neither a Republican or Democrat.
Democratic nomination.
Overall, 15 people declared their candidacy for the Democratic Party nomination. They were:
Primaries.
Senate Majority Whip Ted Kennedy, the youngest brother of late President John F. Kennedy and late Attorney General Robert F. Kennedy, was the favorite to win the 1972 nomination, but he announced he would not be a candidate. The favorite for the Democratic nomination then became Ed Muskie, the 1968 vice-presidential nominee. Muskie's momentum collapsed just prior to the New Hampshire primary, when the so-called "Canuck letter" was published in the "Manchester Union-Leader". The letter, actually a forgery from Nixon's "dirty tricks" unit, claimed that Muskie had made disparaging remarks about French-Canadians – a remark likely to injure Muskie's support among the French-American population in northern New England. Subsequently, the paper published an attack on the character of Muskie's wife Jane, reporting that she drank and used off-color language during the campaign. Muskie made an emotional defense of his wife in a speech outside the newspaper's offices during a snowstorm. Though Muskie later stated that what had appeared to the press as tears were actually melted snowflakes, the press reported that Muskie broke down and cried, shattering the candidate's image as calm and reasoned.
Nearly two years before the election, South Dakota Senator George McGovern entered the race as an anti-war, progressive candidate. McGovern was able to pull together support from the anti-war movement and other grassroots support to win the nomination in a primary system he had played a significant part in designing.
On January 25, 1972, New York Representative Shirley Chisholm announced she would run, and became the first African American woman to run for the Democratic or Republican presidential nomination. Hawaii Representative Patsy Mink also announced she would run and became the first Asian American to run for the Democratic presidential nomination.
On April 25, George McGovern won the Massachusetts primary. Two days later, journalist Robert Novak claimed in a column that a Democratic senator whom he did not name said of McGovern: "The people don't know McGovern is for amnesty, abortion, and legalization of pot. Once middle America – Catholic middle America, in particular – finds this out, he's dead." The label stuck and McGovern became known as the candidate of "amnesty, abortion, and acid." It became Humphrey's battle cry to stop McGovern — especially in the Nebraska primary.
Alabama Governor George Wallace, an anti-integrationist, did well in the South (he won every county in the Florida primary) and in the North among alienated and dissatisfied voters. What might have become a forceful campaign was cut short when Wallace was shot in an assassination attempt by Arthur Bremer on May 15. Wallace was struck by four bullets and left paralyzed. The day after the assassination attempt, Wallace won the Michigan and Maryland primaries, but the shooting effectively ended his campaign.
In the end, McGovern won the nomination by winning primaries through grassroots support in spite of establishment opposition. McGovern had led a commission to re-design the Democratic nomination system after the divisive nomination struggle and convention of 1968. The fundamental principle of the McGovern Commission—that the Democratic primaries should determine the winner of the Democratic nomination—have lasted throughout every subsequent nomination contest. However, the new rules angered many prominent Democrats whose influence was marginalized, and those politicians refused to support McGovern's campaign (some even supporting Nixon instead), leaving the McGovern campaign at a significant disadvantage in funding compared to Nixon.
Primary results.
Primaries popular vote results:
Notable endorsements.
Edmund Muskie
Hubert Humphrey
George McGovern
George Wallace
Shirley Chisholm
Terry Sanford
Henry M. Jackson
1972 Democratic National Convention.
Results:
The vice presidential vote.
With hundreds of delegates angry at McGovern for one reason or another, the vote was chaotic, with at least three other candidates having their names put into nomination and votes scattered over 70 candidates. The eventual winner was Senator Thomas Eagleton.
The vice-presidential balloting went on so long that McGovern and Eagleton were forced to begin making their acceptance speeches at around 2 am, local time.
After the convention ended, it was discovered that Eagleton had undergone psychiatric electroshock therapy for depression and had concealed this information from McGovern. A "Time" magazine poll taken at the time found that 77 percent of the respondents said "Eagleton's medical record would not affect their vote." Nonetheless, the press made frequent references to his "shock therapy," and McGovern feared that this would detract from his campaign platform. McGovern subsequently consulted confidentially with preeminent psychiatrists, including Eagleton's own doctors, who advised him that a recurrence of Eagleton's depression was possible and could endanger the country should Eagleton become president. McGovern had initially claimed that he would back Eagleton "1000 percent," only to ask Eagleton to withdraw three days later. This perceived lack of conviction in sticking with his running mate was disastrous for the McGovern campaign.
McGovern later approached six different prominent Democrats to run as his vice-president: Ted Kennedy, Edmund Muskie, Hubert Humphrey, Abraham Ribicoff, Larry O'Brien and Reubin Askew. All six declined. Sargent Shriver, brother-in-law to John, Robert, and Ted Kennedy, former Ambassador to France and former Director of the Peace Corps, later accepted. He was officially nominated by a special session of the Democratic National Committee. By this time, McGovern's poll ratings had plunged from 41 to 24 percent.
Republican nomination.
Republican candidates:
Primaries.
Richard Nixon was a popular incumbent president in 1972, as he was credited with achieving détente with the People's Republic of China and the Soviet Union. Polls showed that Nixon held a strong lead in the Republican primaries. He was challenged by two candidates, liberal Pete McCloskey of California and conservative John Ashbrook of Ohio. McCloskey ran as an anti-war candidate, while Ashbrook opposed Nixon's détente policies towards China and the Soviet Union. In the New Hampshire primary McCloskey garnered 19.8% of the vote to Nixon's 67.6%, with Ashbrook receiving 9.7%. Nixon won 1323 of the 1324 delegates to the Republican convention, with McCloskey receiving the vote of one delegate from New Mexico. Vice president Spiro Agnew was re-nominated by acclamation; while both the party's moderate wing and Nixon himself had wanted to replace him with a new running-mate (the moderates favoring Nelson Rockefeller, and Nixon favoring John Connally), it was ultimately concluded that the loss of Angew's base of conservative supporters would be too big of a risk.
Primary results.
Primaries popular vote result:
Convention.
Seven members of Vietnam Veterans Against the War were brought on federal charges for conspiring to disrupt the Republican convention. They were acquitted by a federal jury in Gainesville, Florida.
Third parties.
The only major third party candidate in the 1972 election was conservative Republican Representative John G. Schmitz, who ran on the American Party ticket (the party on whose ballot George Wallace ran in 1968). He was on the ballot in 32 states and received 1,099,482 votes. Unlike Wallace, however, he did not win a majority of votes cast in any state, and received no electoral votes.
John Hospers of the newly formed Libertarian Party was on the ballot only in Colorado and Washington and received 3,573 votes, winning no states. However, he did receive one electoral vote from Virginia from a Republican faithless elector (see below). The Libertarian vice-presidential nominee Theodora "Tonie" Nathan became the first woman in U.S. history to receive an electoral vote.
Linda Jenness was nominated by the Socialist Workers Party, with Andrew Pulley as her running-mate. Benjamin Spock and Julius Hobson were nominated for president and vice-president, respectively by, the People's Party.
General election.
Campaign.
McGovern ran on a platform of immediately ending the Vietnam War and instituting guaranteed minimum incomes for the nation's poor. His campaign was harmed by his views during the primaries (which alienated many powerful Democrats), the perception that his foreign policy was too extreme, and the Eagleton debacle. With McGovern's campaign weakened by these factors, the Republicans successfully portrayed him as a radical left-wing extremist incompetent to serve as president. Nixon led in the polls by large margins throughout the entire campaign. He ran a campaign with an aggressive policy of keeping tabs on perceived enemies, and his aides committed the burglary of Watergate to steal Democratic Party information during the campaign.
Results.
Nixon's percentage of the popular vote was only slightly less than Lyndon Johnson's record in the 1964 election, and his margin of victory was slightly larger. Nixon won a majority vote in 49 states, including McGovern's home state of South Dakota. Only Massachusetts and the District of Columbia voted for the challenger, resulting in an even more lopsided Electoral College tally. The election saw the lowest voter turnout for a presidential election since 1948, with only 55 percent of the electorate voting, perhaps because of voter apathy caused by the foregone conclusion of a Nixon landslide. It was also the first election since 1808 in which New York did not have the largest number of electors in the Electoral College, having fallen to 41 electors versus California's 45.
Although the McGovern campaign believed that its candidate had a better chance of defeating Nixon because of the new Twenty-sixth Amendment to the United States Constitution that lowered the national voting age to 18 from 21, a majority of those under 21 voted for Nixon. The 1972 election was the first in American history in which a Republican candidate carried every single Southern state, continuing the region's transformation from a Democratic bastion into a Republican one. By this time, all the Southern states except Arkansas and Texas had been carried by a Republican in either the previous election or the 1964 election. As a result of this election, Massachusetts was the only state not to be carried by Nixon in any of his three presidential campaigns. As of 2012, this is also the last election where Minnesota was carried by the Republican candidate (Minnesota later being the only state not won by Ronald Reagan in either 1980 or 1984). This election also made Nixon the second former Vice President in American history to be elected and reelected, after Thomas Jefferson in 1800 and 1804.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Source (Close States): http://uselectionatlas.org/RESULTS/stats.php?year=1972&f=0&off=0&elect=0 (Retrieved: January 24, 2013).
(a)A Virginia faithless elector, Roger MacBride, though pledged to vote for Richard Nixon and Spiro Agnew, instead voted for Libertarian candidates John Hospers and Theodora "Tonie" Nathan.
(b)In Arizona, Pima and Yavapai counties had a ballot malfunction that counted many votes for both a major party candidate and Linda Jenness of the Socialist Workers Party. A court ordered that the ballots be counted for both. As a consequence, Jenness received 16% and 8% of the vote in Pima and Yavapai, respectively. 30,579 of her 30,945 Arizona votes are from those two counties. Some sources do not count these votes for Jenness.
Close states.
States where margin of victory was more than 5 percentage points, but less than 10 percentage points (43 electoral votes):
Scandals.
Watergate.
On June 17, five months before election day, five men broke into the Democratic National Convention headquarters at the Watergate hotel in Washington, D.C.; the resulting investigation led to the revelation of attempted cover-ups within the Nixon administration. Known as the Watergate scandal, the exposed corruption cost Nixon public and political support, and he resigned on August 9, 1974, in the face of probable impeachment charges by Congress.
Corporate campaign contributions.
As part of the continuing investigation in 1974–75, Watergate scandal prosecutors offered companies that had given illegal campaign contributions to Nixon's re-election campaign lenient sentences if they came forward. Many companies complied, including Northrop Grumman, 3M, American Airlines and Braniff Airlines. By 1976, prosecutors had convicted 18 American corporations of contributing illegally to Nixon's campaign.

</doc>
<doc id="40569" url="http://en.wikipedia.org/wiki?curid=40569" title="United States presidential election, 1976">
United States presidential election, 1976

 Gerald Ford
Jimmy Carter
The United States presidential election of 1976 was the 48th quadrennial presidential election, held on Tuesday, November 2, 1976. The winner was the relatively unknown former governor of Georgia, Jimmy Carter, the Democratic candidate, over the incumbent President Gerald Ford, the Republican candidate.
President Richard Nixon had resigned in 1974 in the wake of the Watergate scandal, but before doing so, he appointed Ford as Vice President via the Twenty-fifth Amendment after Spiro Agnew resigned in the light of a scandal that implicated him in receiving illegal bribes when he was Governor of Maryland. Ford was thus the only sitting President who had never been previously elected to national office. Saddled with a poor economy, the fall of South Vietnam, and paying a heavy political price for his pardon of Nixon, Ford first faced serious opposition from within his own party, when he was challenged for the Republican Party's nomination by former California governor and future President Ronald Reagan. The race was so close that Ford was not able to secure the nomination until the Party Convention. Carter, who was less well known than other Democratic hopefuls, ran as a Washington outsider and reformer. Carter narrowly won the election, becoming the first president elected from the Deep South since Zachary Taylor in 1848.
Nominations.
Democratic Party.
Democratic candidates
Candidates gallery.
The surprise winner of the 1976 Democratic presidential nomination was Jimmy Carter, a former state senator and governor of Georgia. When the primaries began, Carter was little-known at the national level, and many political pundits regarded a number of better-known candidates, such as Senator Henry M. Jackson of Washington, Congressman Morris Udall of Arizona, Governor George Wallace of Alabama, and California Governor Jerry Brown, as the favorites for the nomination. However, in the wake of the Watergate scandal, Carter realized that his status as a Washington outsider, political centrist, and moderate reformer could give him an advantage over his better-known establishment rivals. Carter also took advantage of the record number of state primaries and caucuses in 1976 to eliminate his better-known rivals one-by-one.
Senator Jackson made a fateful decision not to compete in the early Iowa caucus and New Hampshire primary, which Jimmy Carter won after liberals split their votes among four other candidates. Though Jackson went on to win the Massachusetts and New York primaries, he was forced to quit the race on May 1 after losing the critical Pennsylvania primary to Carter by twelve percentage points. Carter then defeated Governor Wallace, his main conservative challenger, by a wide margin in the North Carolina primary, thus forcing Wallace to end his campaign. Congressman Udall, a liberal, then became Carter's main challenger. He finished second to Carter in the New Hampshire, Massachusetts, Wisconsin, New York, Michigan, South Dakota, and Ohio primaries, and won the caucuses in his home state of Arizona, while running even with Carter in the New Mexico caucuses. However, the fact that Udall finished second to Carter in most of these races meant that Carter steadily accumulated more delegates for the nomination than he did.
As Carter closed in on the nomination, an "ABC" (Anybody But Carter) movement started among Northern and Western liberal Democrats who worried that Carter's Southern upbringing would make him too conservative for the Democratic Party. The leaders of the "ABC" movement - Idaho Senator Frank Church and California Governor Jerry Brown - both announced their candidacies for the Democratic nomination and defeated Carter in several late primaries. However, their campaigns started too late to prevent Carter from gathering the remaining delegates he needed to capture the nomination.
By June 1976, Carter had captured more than enough delegates to win the Democratic nomination. At the 1976 Democratic National Convention, Carter easily won the nomination on the first ballot; Udall finished in second place. Carter then chose Minnesota Senator Walter Mondale, a liberal and political protégé of Hubert Humphrey, as his running mate.
Republican Party.
Republican candidates
Candidates gallery.
The contest for the Republican Party's presidential nomination in 1976 was between two serious candidates: Gerald Ford, the leader of the Republican Party's moderate wing and the incumbent president, from Michigan; and Ronald Reagan, the leader of the Republican Party's conservative wing and the former two-term governor of California. The presidential primary campaign between the two men was hard-fought and relatively even; by the start of the Republican Convention in August 1976, the race for the nomination was still too close to call. Ford defeated Reagan by a narrow margin on the first ballot at the 1976 Republican National Convention in Kansas City, and chose Senator Bob Dole of Kansas as his running mate in place of incumbent Vice-President Nelson Rockefeller, who had announced the previous year that he was not interested in being considered for the Vice Presidential nomination. The 1976 Republican Convention was the last political convention to open with the presidential nomination still being undecided until the actual balloting at the convention.
General election.
Fall campaign.
One of the advantages Ford held over Carter as the general election campaign began was that, as president, he was privileged to preside over events dealing with the United States Bicentennial; this often resulted in favorable publicity for Ford. The Washington, D.C., fireworks display on the Fourth of July was presided over by the president and televised nationally. On July 7, 1976, the president and First Lady served as hosts at a White House state dinner for Elizabeth II and Prince Philip of the United Kingdom, which was televised on the Public Broadcasting Service (PBS) network. These events were part of Ford's "Rose Garden" strategy to win the election; instead of appearing as a typical politician, Ford presented himself as a "tested leader" who was busily fulfilling the role of national leader and Chief Executive. Not until October did Ford leave the White House to campaign actively across the nation.
Jimmy Carter ran as a reformer who was "untainted" by Washington political scandals, which many voters found attractive in the wake of the Watergate scandal, which had led to President Richard Nixon's resignation. Ford, although personally unconnected with Watergate, was seen by many as too close to the discredited Nixon administration, especially after he granted Nixon a presidential pardon for any crimes he might have committed during his term of office. Ford's pardon of Nixon caused his popularity, as measured by public-opinion polls, to plummet. Ford's refusal to explain his reasons for pardoning Nixon publicly (he would do so in his memoirs several years later), also hurt his image.
Ford unsuccessfully asked Congress to end the 1950s-era price controls on natural gas, which caused a dwindling of American natural gas reserves after the 1973 Oil Crisis. Carter stated during his campaign that he opposed the ending of the price controls and thought such a move would be "disastrous."
After the Democratic National Convention, Carter held a huge 33-point lead over Ford in the polls. However, as the campaign continued, the race greatly tightened. During the campaign "Playboy" magazine published a controversial interview with Carter; in the interview, Carter admitted to having "lusted in my heart" for women other than his wife, which cut into his support among women and evangelical Christians. Also, on September 23, Ford performed well in what was the first televised presidential debate since 1960. Polls taken after the debate showed that most viewers felt that Ford was the winner. Carter was also hurt by Ford's charges that he lacked the necessary experience to be an effective national leader, and that Carter was vague on many issues.
However, Ford also committed a costly blunder in the campaign that halted his momentum. During the second presidential debate on October 6, Ford stumbled when he asserted that "there is no Soviet domination of Eastern Europe and there never will be under a Ford administration." He added that he did not "believe that the Poles consider themselves dominated by the Soviet Union", and made the same claim with regards to Yugoslavia and Romania. (Yugoslavia was not a Warsaw Pact member.) Ford refused to retract his statement for almost a week after the debate, as a result his surge in the polls stalled and Carter was able to maintain a slight lead in the polls.
A vice-presidential debate, the first ever formal one of its kind, between Bob Dole and Walter Mondale also hurt the Republican ticket when Dole asserted that military unpreparedness on the part of Democratic presidents was responsible for all of the wars the U.S. had fought in the 20th century. Dole, a World War II veteran, noted that in every 20th-century war from World War I to the Vietnam War, a Democrat had been President. Dole then pointed out that the number of U.S. casualties in "Democrat wars" was roughly equal to the population of Detroit. Many voters felt that Dole's criticism was unfairly harsh and that his dispassionate delivery made him seem cold. Years later, Dole would remark that he regretted the comment, having viewed it as hurting the Republican ticket. One factor which did help Ford in the closing days of the campaign was a series of popular television appearances he did with Joe Garagiola, Sr., a retired baseball star for the St. Louis Cardinals and a well-known announcer for NBC Sports. Garagiola and Ford appeared in a number of shows in several large cities. During the show Garagiola would ask Ford questions about his life and beliefs; the shows were so informal, relaxed, and laid-back that some television critics labelled them the "Joe and Jerry Show." Ford and Garagiola obviously enjoyed one another's company, and they remained friends after the election was over.
Results.
Despite his campaign's blunders, Ford managed to close the remaining gap in the polls and by election day the race was judged to be even. Election day was November 2, and it took most of that night and the following morning to determine the winner. It wasn't until 3:30 am (EST), that the NBC television network was able to pronounce that Carter had carried Mississippi, and had thus accumulated more than the 270 electoral votes needed to
win (seconds later, ABC News also declared Carter the winner based on projections for Carter in Wisconsin and Hawaii; CBS News announced Carter's
victory at 3:45 am). Carter defeated Ford by two percentage points in the national popular vote.
The electoral vote was the closest since 1916; Carter carried 23 states with 297 electoral votes, while Ford won 27 states and 240 electoral votes (one elector from Washington state, pledged to Ford, voted for Reagan). Carter's victory came primarily from his near-sweep of the South (he lost only Virginia and Oklahoma) and his narrow victories in large Northern states such as New York, Ohio, and Pennsylvania. Ford did well in the West, carrying every state except Hawaii. The most tightly contested state in the election was Oregon, which Ford won by a very narrow margin.
A switch of 3,687 votes in Hawaii and 5,559 votes in Ohio from Carter to Ford would have resulted in Ford winning the election with 270 electoral votes. By percentage of the vote, the states that secured Carter's victory were Wisconsin (1.68% margin) and Ohio (.27% margin). Had Ford won these states and all other states he carried, he would have won the presidency. The 27 states Ford won were and remain the most states ever carried by a losing candidate for president of the United States.
Carter was the first Democrat since John F. Kennedy in 1960 to carry the states of the Deep South, and the first since Lyndon B. Johnson in 1964 to carry a majority of all southern states. Carter performed very strongly in his home state of Georgia, carrying 66.7% of the vote and every county in the state. His 50.1% of the vote was the only time since 1964 that a Democrat managed to obtain a majority of the popular vote in a presidential election until Barack Obama won 52.9% of the vote in 2008. Carter is one of five Democrats since the American Civil War to obtain a majority of the popular vote, the others being Samuel J. Tilden, Franklin D. Roosevelt, Lyndon B. Johnson, and Barack Obama.
Had Ford won the election, the provisions of the 22nd amendment would have disqualified him from running in 1980, because he had served more than two years of Nixon's second term.
This election represents the last time to date that Texas, Mississippi, Alabama, and South Carolina would vote Democratic, and the last time North Carolina would vote Democratic until 2008.
Statistics.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
(a) "Mike Padden, a Republican faithless elector from Washington, gave Ronald Reagan one electoral vote." 
(b) "The running mate of McCarthy varied from state to state." 
(c) "Research has not yet determined whether Anderson's home state was Tennessee or Texas at the time of the 1976 election."
Results by state.
<br>
As of 2012, this election represents the second and last time that the winning candidate has received a majority of the electoral votes although the second-place candidate carried a majority of the states. It had previously happened in the 1960 election.
Close states.
States where margin of victory was under 5% (299 electoral votes):
States where margin of victory was more than 5%, but less than 10% (105 electoral votes):
Voter demographics.
Source: CBS News/ New York Times interviews with 12,782 voters as they left the polls, as reported in the New York Times, November 9, 1980, p. 28, and in further analysis. The 1976 data are from CBS News interviews.

</doc>
<doc id="40570" url="http://en.wikipedia.org/wiki?curid=40570" title="United States presidential election, 1980">
United States presidential election, 1980

 Jimmy Carter
Ronald Reagan
The United States presidential election of 1980 was the 49th quadrennial presidential election. It was held on Tuesday, November 4, 1980. The contest was between incumbent Democratic President Jimmy Carter and his Republican opponent, former California Governor Ronald Reagan, as well as Republican Congressman John B. Anderson, who ran as an independent. Reagan, aided by the Iran hostage crisis and a worsening economy at home marked by high unemployment and inflation, won the election in a landslide, receiving the highest number of electoral votes ever won by a non-incumbent presidential candidate.
Carter, after defeating Edward M. Kennedy for the Democratic nomination, attacked Reagan as a dangerous right-wing radical. For his part, Reagan pledged to uplift the pessimistic mood of the nation, and won a decisive victory; in the simultaneous Congressional elections, Republicans won control of the United States Senate for the first time in 28 years. This election marked the beginning of what is called by some the "Reagan Revolution" or Reagan Era, and signified a conservative realignment in national politics.
Background.
Throughout the 1970s, the United States underwent a wrenching period of low economic growth, high inflation and interest rates, and intermittent energy crises. By October 1978, Iran, a major oil supplier to the United States at the time, was experiencing a major uprising that severely damaged its oil infrastructure and greatly weakened its capability to produce oil. In January 1979, shortly after Iran's leader Shah Mohammad Reza Pahlavi fled the country, Iranian opposition figure Ayatollah Ruhollah Khomeini ended his 14-year exile in France and returned to Iran to establish an Islamic Republic, largely hostile to American interests and influence in the country. In the spring and summer of 1979 inflation was on the rise and various parts of the United States were experiencing energy shortages.
With the return of the long gas lines that were last seen just after the 1973 Yom Kippur War, Carter was widely blamed and planned on delivering his fifth major speech on energy; however, he felt that the American people were no longer listening. Carter left for the presidential retreat of Camp David. "For more than a week, a veil of secrecy enveloped the proceedings. Dozens of prominent Democratic Party leaders—members of Congress, governors, labor leaders, academics and clergy—were summoned to the mountaintop retreat to confer with the beleaguered president." His pollster, Pat Caddell, told him that the American people simply faced a crisis of confidence because of the assassinations of John F. Kennedy, Robert F. Kennedy and Martin Luther King, Jr.; the Vietnam War; and Watergate. On July 15, 1979, Carter gave a nationally-televised address in which he identified what he believed to be a "crisis of confidence" among the American people. This came to be known as his "malaise" speech, although Carter never used the word in the speech.
Many expected Senator Kennedy to successfully challenge Carter in the upcoming Democratic Primary. Kennedy's official announcement was scheduled for early November. A television interview with Roger Mudd of CBS a few days before the announcement went badly, however. Kennedy gave an "incoherent and repetitive" answer to the question of why he was running, and the polls, which showed him leading the President by 58-25 in August now had him ahead 49-39.
Meanwhile, an opportunity for political redemption came for Carter as the Khomeini regime again gained public attention and allowed the taking of 52 American hostages by a group of Islamist students and militants at the U.S. embassy in Tehran on November 4, 1979. Carter's calm approach towards handling this crisis resulted in his approval ratings jump in the 60-percent range in some polls, due to a "rally round the flag" effect. By the beginning of the election season, the prolonged Iran hostage crisis had sharpened public perceptions of a national crisis. On April 25, 1980, Carter's ability to use the hostage crisis to regain public acceptance eroded when his attempt to rescue the hostages ended in disaster and drew further skepticism towards his leadership skills.
Following the failed rescue attempt, Jimmy Carter was overwhelmingly blamed for the Iran hostage crisis, in which the followers of the Ayatollah Khomeini burned American flags and chanted anti-American slogans, paraded the captured American hostages in public, and burned effigies of Carter. Carter's critics saw him as an inept leader who had failed to solve the worsening economic problems at home. His supporters defended the president as a decent, well-intentioned man being unfairly criticized for problems that had been building for years.
Nominations.
Democratic Party.
Democratic candidates:
Candidates gallery.
The three major Democratic candidates in early 1980 were incumbent President Jimmy Carter, Senator Ted Kennedy of Massachusetts, and Governor Jerry Brown of California. Brown withdrew on April 2. Carter and Kennedy faced off in 34 primaries. This was the most tumultuous primary race that an elected incumbent president had encountered since President Taft, during the highly contentious election of 1912.
During the summer of 1980, there was a short-lived "Draft Muskie" movement; Secretary of State Edmund Muskie was seen as a favorable alternative to a deadlocked convention. One poll showed that Muskie would be a more popular alternative to Carter than Kennedy, implying that the attraction was not so much to Kennedy as to the fact that he was not Carter. Muskie was polling even with Ronald Reagan at the time, while Carter was seven points behind. Although the underground "Draft Muskie" campaign failed, it became a political legend.
After defeating Kennedy in 24 of 34 primaries, Carter entered the party's convention in New York in August with 60 percent of the delegates pledged to him on the first ballot. Still, Kennedy refused to drop out. At the convention, after a futile last-ditch attempt by Kennedy to alter the rules to free delegates from their first-ballot pledges, Carter was renominated with 2,129 votes to 1,146 for Kennedy. Vice President Walter Mondale was also renominated. In his acceptance speech, Carter warned that Reagan's conservatism posed a threat to world peace and progressive social welfare programs from the New Deal to the Great Society.
Republican Party.
Republican candidates
Former Governor Ronald Reagan was the odds-on favorite to win his party's nomination for president after nearly beating incumbent President Gerald Ford just four years earlier. He won the nomination on the first round at the 1980 Republican National Convention in Detroit, Michigan, in July, then chose George H. W. Bush, his top rival, as his running mate.
Other candidates.
John Anderson.
John Anderson, after being defeated in the Republican primaries, entered the general election as an independent candidate, campaigning as a moderate Republican alternative to Reagan's conservatism. However, his campaign appealed primarily to frustrated anti-Carter voters. His support progressively evaporated through the campaign season as his supporters were pulled away by Carter and Reagan. His running mate was Patrick Lucey, a Democratic former Governor of Wisconsin and then Ambassador to Mexico, appointed by President Carter.
Ed Clark.
The Libertarian Party nominated Ed Clark for President and David H. Koch for Vice President. They received almost one million votes and were on the ballot in all 50 states plus Washington DC. Koch, a co-owner of Koch Industries, pledged part of his personal fortune to the campaign.
The Clark-Koch ticket received 921,128 votes (1.06% of the total nationwide). This is the highest percentage of popular votes a Libertarian Party candidate has ever received in a presidential race to date, and remained the highest overall number of votes earned by a Libertarian candidate until the 2012 election, when Gary Johnson and James P. Gray became the first Libertarian ticket to earn more than a million votes, albeit with a lower overall vote percentage than Clark-Koch. His strongest support was in Alaska, where he came in third place with 11.66% of the vote, finishing ahead of independent candidate John Anderson and receiving almost half as many votes as Jimmy Carter.
Others.
The Socialist Party USA nominated David McReynolds for President and Sister Diane Drufenbrock for Vice President, making McReynolds the first openly gay man to run for President and Drufenbrock the first nun to be a candidate for national office in the U.S.
The Citizens Party ran Barry Commoner for President and La Donna Harris for Vice President.
The Communist Party USA ran Gus Hall for President and Angela Davis for Vice President.
The American Party nominated Percy L. Greaves, Jr. for President and Frank L. Varnum for Vice President.
Rock star Joe Walsh ran a mock campaign as a write-in candidate, promising to make his song "Life's Been Good" the new national anthem if he won, and running on a platform of "Free Gas For Everyone." Though the 33-year-old Walsh was not old enough to actually assume the office, he wanted to raise public awareness of the election.
General election.
Campaign.
Under federal election laws, Carter and Reagan received $29.4 million each, and Anderson was given a limit of $18.5 million with private fund-raising allowed for him only. They were not allowed to spend any other money. Carter and Reagan each spent about $15 million on television advertising, and Anderson under $2 million. Reagan ended up spending $29.2 million in total, Carter $29.4 million, and Anderson spent $17.6 million— partially because he (Anderson) didn't get Federal Election Commission money until after the election.
The 1980 election is considered by some to be a realigning election, reaching a climate of confrontation practically not seen since 1932. Reagan's supporters praise him for running a campaign of upbeat optimism. David Frum says Carter ran an attack-based campaign based on "despair and pessimism" which "cost him the election." Carter emphasized his record as a peacemaker, and said Reagan's election would threaten civil rights and social programs that stretched back to the New Deal. Reagan's platform also emphasized the importance of peace, as well as a prepared self-defense.
Immediately after the conclusion of the primaries,[] a Gallup poll held that Reagan was ahead, with 58% of voters upset by Carter's handling of the Presidency. One analysis of the election has suggested that "Both Carter and Reagan were perceived negatively by a majority of the electorate." While the three leading candidates (Reagan, Anderson and Carter) were religious Christians, Carter had the most support of evangelical Christians according to a Gallup poll. However, in the end, Jerry Falwell's Moral Majority lobbying group is credited with giving Reagan two-thirds of the white evangelical vote. According to Carter: "that autumn [1980] a group headed by Jerry Falwell purchased $10 million in commercials on southern radio and TV to brand me as a traitor to the South and no longer a Christian."
The election of 1980 was a key turning point in American politics. It signaled the new electoral power of the suburbs and the Sun Belt. Reagan's success as a conservative would initiate a realigning of the parties, as liberal Republicans and conservative Democrats would either leave politics or change party affiliations through the 1980s and 1990s to leave the parties much more ideologically polarized. While during Barry Goldwater's 1964 campaign, many voters saw his warnings about a too-powerful government as hyperbolic and only 30% of the electorate agreed that government was too powerful, by 1980 a majority of Americans believed that government held too much power.
Campaign promises.
Reagan promised a restoration of the nation's military strength, at the same time 60% of Americans polled felt defense spending was too low. Reagan also promised an end to "'trust me' government" and to restore economic health by implementing a supply-side economic policy. Reagan promised a balanced budget within three years (which he said would be "the beginning of the end of inflation"), accompanied by a 30% reduction in tax rates over those same years. With respect to the economy, Reagan famously said, "A recession is when your neighbor loses his job. A depression is when you lose yours. And recovery is when Jimmy Carter loses his." Reagan also criticized the "windfall profit tax" that Carter and Congress enacted that year in regards to domestic oil production and promised to attempt to repeal it as president. The tax was not a tax on profits, but on the difference between the price control-mandated price and the market price.
On the issue of women's rights there was much division, with many feminists frustrated with Carter, the only candidate who supported the Equal Rights Amendment. After a bitter Convention fight between Republican feminists and antifeminists the Republican Party dropped their forty-year endorsement of the ERA. Reagan, however, announced his dedication to women's rights and his intention to, if elected, appoint women to his cabinet and the first female justice to the Supreme Court. He also pledged to work with all 50 state governors to combat discrimination against women and to equalize federal laws as an alternative to the ERA. Reagan was convinced to give an endorsement of women's rights in his nomination acceptance speech.
Carter was criticized by his own aides for not having a "grand plan" for the recovery of the economy, nor did he ever make any campaign promises; he often criticized Reagan's economic recovery plan, but did not create one of his own in response.
Campaign events.
In August, after the Republican National Convention, Ronald Reagan gave a campaign speech at the annual Neshoba County Fair on the outskirts of Philadelphia, Mississippi, where three civil rights workers were murdered in 1964. He was the first presidential candidate ever to campaign at the fair. Reagan famously announced, "Programs like education and others should be turned back to the states and local communities with the tax sources to fund them. I believe in states' rights. I believe in people doing as much as they can at the community level and the private level." Reagan also stated, "I believe we have distorted the balance of our government today by giving powers that were never intended to be given in the Constitution to that federal establishment." He went on to promise to "restore to states and local governments the power that properly belongs to them." President Carter criticized Reagan for injecting "hate and racism" by the "rebirth of code words like 'states' rights'".
Two days later, Reagan appeared at the Urban League convention in New York, where he said, "I am committed to the protection and enforcement of the civil rights of black Americans. This commitment is interwoven into every phase of the plans I will propose." He then said that he would develop "enterprise zones" to help with urban renewal.
Reagan made some gaffes during the campaign. When Carter appeared in a small Alabama town, Tuscumbia, Reagan incorrectly claimed the town had been the birthplace of the Ku Klux Klan—it was actually the home of the KKK's national headquarters. Reagan was widely ridiculed by Democrats for saying that trees caused pollution; he later said that he meant only certain types of pollution and his remarks had been misquoted.
Meanwhile, Carter was burdened by a continued weak economy and the Iran hostage crisis. Inflation, high interest rates, and unemployment continued through the course of the campaign, and the ongoing hostage crisis in Iran became, according to David Frum in " How We Got Here: The '70s", a symbol of American impotence during the Carter years. John Anderson's independent candidacy, aimed at eliciting support from liberals, was also seen as hurting Carter more than Reagan, especially in such reliably Democratic states such as Massachusetts and New York.
The debates.
An important event in the 1980 presidential campaign was the lone presidential debate, which was held one week to the day before the election (October 28). Going into the debate, average poll data indicated that Reagan had a two to three point lead over Carter. After the debate, Reagan was able to increase his lead dramatically against the president to win a comfortable Republican victory.
The League of Women Voters, which had sponsored the 1976 Ford/Carter debate series, announced that it would do so again for the next cycle in the spring of 1979. However, Carter was not eager to participate with any debate. He had repeatedly refused to a debate with Senator Edward M. Kennedy during the primary season, and had given ambivalent signals as to his participation in the fall.
The League of Women Voters had announced a schedule of debates similar to 1976, three presidential and one vice presidential. No one had much of a problem with this until it was announced that Rep. John Anderson might be invited to participate along with Carter and Reagan. Carter steadfastly refused to participate with Anderson included, and Reagan refused to debate without him. It took months of negotiations for the League of Women Voters to finally put it together. It was held on September 21, 1980 in the Baltimore Convention Center. Reagan said of Carter's refusal to debate: "He [Carter] knows that he couldn't win a debate even if it were held in the Rose Garden before an audience of Administration officials with the questions being asked by Jody Powell." The League of Women Voters promised the Reagan campaign that the debate stage would feature an empty chair to represent the missing president. Carter was very upset about the planned chair stunt, and at the last minute convinced the League to take it out. The debate was moderated by Bill Moyers. Anderson, who many thought would handily dispatch the former Governor, managed only a draw, according to many in the media at that time. The Illinois congressman, who had been as high as 20% in some polls, and at the time of the debate was over 10%, dropped to about 5% soon after. Anderson failed to substantively engage Reagan, instead he started off by criticizing Carter: "Governor Reagan is not responsible for what has happened over the last four years, nor am I. The man who should be here tonight to respond to those charges chose not to attend," to which Reagan added: "It's a shame now that there are only two of us here debating, because the two that are here are in more agreement than disagreement." 
As September turned into October, the situation remained essentially the same. Governor Reagan insisted Anderson be allowed to participate, and the President remained steadfastly opposed to this. As the standoff continued, the second round was canceled, as was the vice presidential debate.
With two weeks to go to the election, the Reagan campaign decided that the best thing to do at that moment was to accede to all of President Carter's demands, and LWV agreed to exclude Congressman Anderson from the final debate, which was rescheduled for October 28 in Cleveland, Ohio.
Moderated by Howard K. Smith and presented by the League of Women Voters, the presidential debate between President Carter and Governor Reagan ranked among the highest ratings of any television show in the previous decade. Debate topics included the Iranian hostage crisis, and nuclear arms treaties and proliferation. Carter's campaign sought to portray Reagan as a reckless "war hawk," as well as a "dangerous right-wing radical". But it was President Carter's reference to his consultation with 12-year-old daughter Amy concerning nuclear weapons policy that became the focus of post-debate analysis and fodder for late-night television jokes. President Carter said he had asked Amy what the most important issue in that election was and she said, "the control of nuclear arms." A famous political cartoon, published the day after Reagan's landslide victory, showed Amy Carter sitting in Jimmy's lap with her shoulders shrugged asking "the economy? the hostage crisis?"
When President Carter criticized Governor's record, which included voting against Medicare and Social Security benefits, Governor Reagan audibly sighed and replied: "There you go again".
In describing the national debt that was approaching 1 trillion dollars, Reagan stated "a billion is a thousand millions, and a trillion is a thousand billions." When Carter would criticize the content of Reagan's campaign speeches, Reagan began his counter with words: "Well... I don't know that I said that. I really don't."
In his closing remarks, Reagan asked viewers: "Are you better off now than you were four years ago? Is it easier for you to go and buy things in the stores than it was four years ago? Is there more or less unemployment in the country than there was four years ago? Is America as respected throughout the world as it was? Do you feel that our security is as safe, that we're as strong as we were four years ago? And if you answer all of those questions 'yes', why then, I think your choice is very obvious as to whom you will vote for. If you don't agree, if you don't think that this course that we've been on for the last four years is what you would like to see us follow for the next four, then I could suggest another choice that you have."
Endorsements.
In September 1980, former Watergate scandal prosecutor Leon Jaworski accepted a position as honorary chairman of Democrats for Reagan. Five months earlier, Jaworski had harshly criticized Reagan as an "extremist;" he said after accepting the chairmanship, "I would rather have a competent extremist than an incompetent moderate."
Former Democratic Senator Eugene McCarthy of Minnesota (who in 1968 had challenged Lyndon Johnson from the left, causing the then-President to all but abdicate) endorsed Reagan.
Three days before the November 4 voting in the election, the National Rifle Association endorsed a presidential candidate for the first time in its history, backing Reagan. Reagan had received the California Rifle and Pistol Association's Outstanding Public Service Award. Carter had appointed Abner J. Mikva, a fervent proponent of gun control, to a federal judgeship and had supported the Alaska Lands Bill, closing 40000000 acre to hunting.
Results.
The election was held on November 4, 1980.
Ronald Reagan with running mate George H.W. Bush beat Carter by almost 10 percentage points in the popular vote. Republicans also gained control of the Senate for the first time in twenty-five years on Reagan's coattails. The electoral college vote was a landslide, with 489 votes (representing 44 states) for Reagan and 49 votes for Carter (representing 6 states and the District of Columbia).
NBC News projected Reagan as the winner at 8:15 pm EST (5:15 PST), before voting was finished in the West, based on exit polls. (It was the first time a broadcast network used exit polling to project a winner, and took the other broadcast networks by surprise.) Carter conceded defeat at 9:50 pm EST. Carter's loss was the worst performing of an incumbent President since Herbert Hoover lost to Franklin D. Roosevelt in 1932 by a margin of 18%. Carter's defeat was the most lopsided defeat for any incumbent president in an election where only two candidates won electoral votes. Also, Jimmy Carter was the first incumbent Democrat to serve only one full term since James Buchanan and fail to secure re-election since Andrew Johnson (Grover Cleveland served two non-consecutive terms while Harry Truman and Lyndon B. Johnson served one full term in addition to taking over after the deaths of Franklin D. Roosevelt and John F. Kennedy respectively).
John Anderson won 6.6% of the popular vote and failed to win any state outright. He found the most support in New England, fueled by liberal Republicans who felt Reagan was too far to the right; his best showing was in Massachusetts, where he won 15% of the popular vote. Conversely, Anderson performed worst in the South. Anderson failed to achieve the spoiler effect, due to Reagan's strong showing and the fact that he arguably attracted at least as many Democrats to his ticket as Republicans.
Libertarian Party candidate Ed Clark received 921,299 popular votes (1.1%). The Libertarians succeeded in getting Clark on the ballot in all 50 states and the District of Columbia. Clark's best showing was in Alaska, where he received 12% of the vote. The 921,299 votes achieved by the Clark-Koch ticket was the best performance by a Libertarian presidential candidate until 2012 when the Johnson-Gray ticket received 1,273,667 votes.
Reagan won 53% of the vote in reliably Democratic South Boston.
Reagan's electoral college victory of 489 electoral votes (90.9% of the electoral vote) was the most lopsided electoral college victory for a non-incumbent President.
This is the most recent election in which an incumbent president was defeated in two elections in a row. The only other time this happened was in 1892.
Statistics.
Source (Popular Vote): Leip, David. . " ().
Source (Electoral Vote): . ". ().
Close states.
Margin of victory less than 5% (165 electoral votes):
Margin of victory more than 5%, but less than 10% (113 electoral votes):
Voter demographics.
Source: CBS News/ New York Times interviews with 12,782 voters as they left the polls, as reported in the New York Times, November 9, 1980, p. 28, and in further analysis. The 1976 data are from CBS News interviews.

</doc>
<doc id="40571" url="http://en.wikipedia.org/wiki?curid=40571" title="United States presidential election, 1984">
United States presidential election, 1984

 Ronald Reagan
Ronald Reagan
The United States presidential election of 1984 was the 50th quadrennial presidential election. It was held on Tuesday, November 6, 1984. The contest was between the incumbent President Ronald Reagan, the Republican candidate, and former Vice President Walter Mondale, the Democratic candidate.
Reagan carried 49 of the 50 states, becoming one of only two candidates to do so (the other was Richard Nixon in the 1972 presidential election). Reagan touted a strong economic recovery from 1970s stagflation and the 1981-82 recession, as well as the widespread perception that his presidency had overseen a revival of national confidence and prestige. Mondale's only electoral votes came from the District of Columbia, which has never given its electoral votes to a Republican candidate, and his home state of Minnesota, which he won by a mere 3,761 votes. 
Reagan's 525 electoral votes (out of 538) is the highest total ever received by a presidential candidate. His showing ranks fourth by percent electoral votes received (97.58%) out of total available electoral votes, just shy of the 523 out of 531 (98.49%) received by Franklin D. Roosevelt in 1936. Mondale's 13 electoral votes is also the second-fewest ever received by a second-place candidate, second only to Alf Landon's 8 in 1936. In the national popular vote, Reagan received 58.8% to Mondale's 40.6% and his percent margin of victory ranks 7th of all presidential elections. No candidate since then has managed to equal or surpass Reagan's 1984 electoral result. Also, no post-1984 Republican candidate has managed to match Reagan's electoral performance in the Northeastern United States and in the West Coast states.
Nominations.
Primaries.
Ronald Reagan—the incumbent president—was the assured nominee for the Republican Party, with only token opposition. The popular vote from the Republican primaries was as follows:
Reagan was renominated by a vote of 2,233 (two delegates abstained). For the only time in American history, the vice presidential roll call was taken concurrently with the presidential roll call. Vice President George H. W. Bush was overwhelmingly renominated. This was the last time in the 20th century that the Vice Presidential candidate of either major party was nominated by roll call vote.
Primaries.
Only three Democratic candidates won any state primaries: Mondale, Hart, and Jackson. Initially, Massachusetts Senator Ted Kennedy, after a failed bid to win the 1980 Democratic nomination for president, was considered the de facto front-runner of the 1984 primary. But, after Kennedy ultimately declined to run, former Vice-President Mondale was then viewed as the favorite to win the Democratic nomination. Mondale had the largest number of party leaders supporting him, and he had raised more money than any other candidate. However, both Jackson and Hart emerged as surprising, and troublesome, opponents.
South Carolina Senator Ernest Hollings's wit and experience, as well as his call for a budget freeze, won him some positive attention, but his relatively conservative record alienated liberal Democrats, and he was never really noticed in a field dominated by Walter Mondale, John Glenn, and Gary Hart. Hollings dropped out two days after losing badly in New Hampshire, and endorsed Hart a week later. His disdain for his competitors sometimes showed. He notably referred to Mondale as a "lapdog," and to former astronaut Glenn as "Sky King" who was "confused in his capsule."
Jackson was the second African-American (after Shirley Chisholm) to mount a nationwide campaign for the presidency, and he was the first African-American candidate to be a serious contender. He got 3.5 million votes during the primaries, third behind Hart and Mondale. He won the primaries in Virginia, South Carolina, and Louisiana, and split Mississippi, where there were two separate contests for Democratic delegates. Through the primaries, Jackson helped confirm the black electorate's importance to the Democratic Party in the South at the time. During the campaign, however, Jackson made an off-the-cuff reference to Jews as "Hymies" and New York City as "Hymietown," for which he later apologized. Nonetheless, the remark was widely publicized, and derailed his campaign for the nomination. Jackson ended up winning 21% of the national primary vote but received only 8% of the delegates to the national convention, and he initially charged that his campaign was hurt by the same party rules that allowed Mondale to win. He also poured scorn on Mondale, saying that Hubert Humphrey was the "last significant politician out of the St. Paul-Minneapolis" area.
Hart of Colorado was a more serious threat to Mondale, and after winning several early primaries it looked as if he might take the nomination away from Mondale. Hart finished a surprising second in the Iowa caucuses, with 16.5% of the vote. This established him as the main rival to Mondale, effectively eliminating John Glenn, Ernest Hollings and Alan Cranston as alternatives. Hart criticized Mondale as an "old-fashioned" New Deal Democrat who symbolized "failed policies" of the past. Hart positioned himself (just as Bill Clinton would eight years later) as a younger, fresher, and more moderate Democrat who could appeal to younger voters. He emerged as a formidable candidate, winning the key New Hampshire, Ohio, and California primaries as well as several others, especially in the West. However, Hart could not overcome Mondale's financial and organizational advantages, especially among labor union leaders in the Midwest and industrial Northeast.
Hart was also badly hurt in a televised debate with Mondale during the primaries, when the former vice president used a popular television commercial slogan to ridicule Hart's vague "New Ideas" platform. Turning to Hart on camera, Mondale told Hart that whenever he heard Hart talk about his "New Ideas," he was reminded of the Wendy's fast-food slogan "Where's the beef?" The remark drew loud laughter and applause from the viewing audience and caught Hart off-guard. Hart never fully recovered from Mondale's charge that his "New Ideas" were shallow and lacking in specifics.
At a roundtable debate between the three remaining Democratic candidates moderated by Phil Donahue, Mondale and Hart got in such a heated argument over the issue of U.S. policy in Central America that Jackson had to tap his water glass on the table to help get them to stop.
Mondale gradually pulled away from Hart in the delegate count, but, as "Time" reported in late May, "Mondale ... has a wide lead in total delegates (1,564 to 941) ... because of his victories in the big industrial states, his support from the Democratic Establishment and the arcane provisions of delegate-selection rules that his vanguard helped draft two years ago." After the final primary in California, on June 5, which Hart won, Mondale was about 40 delegates short of the total he needed for the nomination. However, at the Democratic National Convention in San Francisco on July 16, Mondale received the overwhelming support of the unelected superdelegates from the party establishment to win the nomination.
This race for the nomination was the closest in two generations, and it has been the most recent occasion that a major party presidential nomination has gone all the way to the convention.
Endorsements.
Note: These are only those endorsements which occurred during or before the Primary Race.
Convention.
These were the convention's nomination tally:
When he made his acceptance speech at the Democratic Convention, Mondale said: "Let's tell the truth. Mr. Reagan will raise taxes, and so will I. He won't tell you. I just did." Although Mondale intended to expose Reagan as hypocritical and position himself as the honest candidate, the choice of taxes as a discussion point likely damaged his electoral chances.
Vice-Presidential nominee.
Mondale chose U.S. Rep. Geraldine A. Ferraro of New York as his running mate, making her the first woman nominated for that position by a major party, and the first Italian American on a major party ticket. Mondale wanted to establish a precedent with his vice presidential candidate, although Tonie Nathan of the Libertarian Party was already the first woman to receive an electoral vote in the 1972 election. Another reason for the nominee to "go for broke" instead of balancing the ticket was Reagan's lead in the polls; Mondale hoped to appeal to women, by 1980 the majority of voters, by choosing Ferraro. In a "much criticized parade of possible Veep candidates" to his home in Minnesota, Mondale considered San Francisco Mayor Dianne Feinstein and Kentucky Governor Martha Layne Collins, also female; Los Angeles Mayor Tom Bradley, an African American; and San Antonio Mayor Henry Cisneros, a Hispanic, as other finalists for the nomination, and chose Ferraro because he hoped that she would attract ethnic voters with her personal background. Unsuccessful nomination candidate Jackson derided Mondale's vice-presidential screening process as a "P.R. parade of personalities," but praised Mondale for his choice, having himself pledged to name a woman to the ticket in the event he was nominated.
Mondale had wanted to choose Governor of New York Mario Cuomo as his running mate, but Cuomo declined and recommended Ferraro, his protégée. The nominee would likely have named Governor of Massachusetts Michael Dukakis had he made a "safe" choice". Others preferred Senator Lloyd Bentsen because he would appeal to more conservative Southern voters. Nomination rival Gary Hart stated before Ferraro's selection that he would accept an invitation to run with Mondale; Hart's supporters claimed he would do better than Mondale against President Reagan, an argument undercut by a June 1984 Gallup poll that showed both men nine points behind the president.
Other parties.
National Unity Party Nomination.
The National Unity Party was an outgrowth of John Anderson's presidential campaign from the 1980 presidential election. Anderson hoped that the party would be able to challenge the "two old parties", which he viewed as being tied to various special interest groups and incapable of responsible fiscal reform. The intention was to organize the new party in California, Oregon, Washington, Illinois, the New England states, and others where his previous candidacy had proven to have experienced the most success. The party was also eligible for $5.8 million in Federal election funds, but its qualification depended on it being on the ballot in at least ten states; however, it remained unclear if National Unity could actually obtain the funds, or if it needed to be Anderson himself.
Anderson initially was against running, hoping that another notable politico would take the party into the 1984 election, and feared that his own candidacy might result in the party being labeled a "personality cult". However no candidate came forward resulting in Anderson becoming the nominee in waiting. While Anderson had managed to find equal support from the Republicans and Democrats in the 1980 election, the grand majority of the former had since switched back, resulting in the new party being supported principally by those who normally would vote Democratic, which it was feared might make him a spoiler candidate. In light of this, in addition to difficulties in getting on the ballot in his targeted states "(Utah and Kentucky were the only two, neither among those he intended to prominently campaign in)", Anderson ultimately declined to run. Later he would endorse the Democratic nominee, Walter Mondale.
Anderson had hoped that the party would continue to grow and later field a candidate in 1988 "(which he declared would not be him)", but it floundered and ultimately dissolved.
Libertarian Party Nomination.
Burns was the initial frontrunner for the nomination, but withdrew citing concerns that the party would not be able to properly finance a campaign. The remaining candidates were: Bergland; Ravenal, who had worked in the Department of Defense under Robert McNamara and Clark Clifford; and Ruwart. Bergland narrowly won the presidential nomination over Ravenal. His running mate was James A. Lewis. The ticket appeared on 39 state ballots.
Communist Party Nomination.
The Communist Party USA ran Gus Hall for President and Angela Davis for Vice President.
General election.
Campaign.
Mondale ran a liberal campaign, supporting a nuclear freeze and the Equal Rights Amendment (ERA). He spoke against what he considered to be unfairness in Reagan's economic policies and the need to reduce federal budget deficits.
While Ferraro's choice was popular among Democratic activists, polls immediately after the announcement showed that only 22% of women were excited about her selection, versus 18% who agreed that it was a "bad idea". 60% of all voters thought that pressure from women's groups had led to Mondale's decision, versus 22% who believed that he had chosen the best available candidate. Some members of the hierarchy of the Roman Catholic Church criticized the Catholic Ferraro for being pro-choice on abortion. Already fighting an uphill battle with voters, Ferraro also faced a slew of allegations, mid-campaign, directed toward her husband, John Zaccaro. These allegations included Zaccaro's possible past involvement in organized crime, pornography distribution, and campaign contribution violations. Ferraro responded to these allegations against her husband by releasing her family tax-returns to the media on August 21, 1984. However, the damage to the campaign was already done.
At a campaign stop in Hammonton, New Jersey, Reagan said, "America's future rests in a thousand dreams inside your hearts. It rests in the message of hope in songs of a man so many young Americans admire, New Jersey's Bruce Springsteen." The Reagan campaign briefly used "Born in the U.S.A.", a song criticizing the treatment of Vietnam War veterans (which they mistakenly thought was devoid of anti-war content), as a campaign song, without permission, until Springsteen, a lifelong Democrat, insisted that they stop.
The Reagan campaign was very skilled at producing effective television advertising. Two of the more memorable ads it produced were commonly known as "Bear in the woods" and "Morning in America".
Reagan was the oldest president to have ever served (he was by this point 73), and there were many questions about his capacity to endure the grueling demands of the presidency, particularly after Reagan had a poor showing in his first debate with Mondale on October 7. He referred to having started going to church "here in Washington", although the debate was in Louisville, Kentucky, referred to military uniforms as "wardrobe," and admitted to being "confused," among other mistakes. In the next debate on October 21, however, Reagan joked "I will not make age an issue of this campaign. I am not going to exploit, for political purposes, my opponent's youth and inexperience." Mondale himself laughed at the joke, and later admitted that Reagan had effectively neutralized the age issue:
If TV can tell the truth, as you say it can, you'll see that I was smiling. But I think if you come in close, you'll see some tears coming down because I knew he had gotten me there. That was really the end of my campaign that night, I think. [I told my wife] the campaign was over, and it was.
Results.
Reagan was re-elected in the November 6 election in an electoral and popular vote landslide, winning 49 states. Reagan won a record 525 electoral votes total (of 538 possible), and received 58.8 percent of the popular vote; despite Ferraro's selection, 55% of women who voted did so for Reagan, and his 54 to 61% of the Catholic vote was the highest for a Republican candidate in history. Mondale's 13 electoral college votes (from his home state of Minnesota—which he won by 0.18%—and the District of Columbia) marked the lowest total of any major Presidential candidate since Alf Landon's 1936 loss to Franklin D. Roosevelt. Mondale's defeat was also the worst for any Democratic Party candidate in American history in the Electoral College (and his 13 electoral votes the fewest any Democrat has won since Stephen A. Douglas claimed 12 in the 1860 election, when the Democratic vote was divided), though others, including Alton B. Parker, James M. Cox, John W. Davis, and George S. McGovern, did worse in the popular vote.
Psephologists attributed the Republican victory to "Reagan Democrats", millions of Democrats who voted for Reagan, as in 1980. They characterized such Reagan Democrats as southern whites and northern blue collar workers who voted for Reagan because they credited him with the economic recovery, saw Reagan as strong on national security issues, and perceived the Democrats as supporting the poor and minorities at the expense of the middle class. The Democratic National Committee commissioned a study after the election that came to these conclusions, but suppressed the report because they were afraid that it would offend its key voters.
When Reagan was asked in December 1984 what he wanted for Christmas he joked, "Well, Minnesota would have been nice". Reagan lost Minnesota in both this election and in 1980, making it the only state he failed to win in either election, and also making him the first two-term president not to carry Minnesota since Woodrow Wilson. This is the last election where the Republican candidate achieved any of the following: Win every state in the Northeastern and Pacific regions of the United States; win at least one county in every state; and win any of the following states: Hawaii, Massachusetts, New York, Oregon, Rhode Island, Washington, and Wisconsin. This was also the last election in which the winning candidate won by a double-digit margin in the percentage of the popular vote, and the last election where the winning candidate won by an eight-digit margin in total popular votes (10 million or more).
Statistics.
Close states.
Margin of victory less than 5% (27 electoral votes) 
Margin of victory more than 5%, but less than 10% (90 electoral votes) 
Aftermath.
After Mondale's loss to Reagan in the general election, Hart quickly emerged as the front-runner for the Democratic Party's 1988 presidential nomination. He maintained that lead until a sex scandal derailed his candidacy in 1987.

</doc>
<doc id="40573" url="http://en.wikipedia.org/wiki?curid=40573" title="NLP">
NLP

NLP may refer to:

</doc>
<doc id="40575" url="http://en.wikipedia.org/wiki?curid=40575" title="Vladimir the Great">
Vladimir the Great

Vladimir Sviatoslavich the Great (Old East Slavic: "Володимѣръ Свѧтославичь" "Volodiměrъ Svętoslavičь", Old Norse as "Valdamarr Sveinaldsson", Russian: Влади́мир, "Vladimir", Ukrainian: Володимир, "Volodymyr", Belarusian: Уладзiмiр, "Uladzimir"; c. 958 – 15 July 1015, Berestove) was a prince of Novgorod, grand prince of Kiev, and ruler of Kievan Rus' from 980 to 1015.
Vladimir's father was prince Sviatoslav of the Rurik dynasty. After the death of his father in 972, Vladimir, who was then prince of Novgorod, was forced to flee to Scandinavia in 976 after his brother Yaropolk had murdered his other brother Oleg and conquered Rus'. In Sweden, with the help from his relative Ladejarl Håkon Sigurdsson, ruler of Norway, he assembled a Varangian army and reconquered Novgorod from Yaropolk. By 980 Vladimir had consolidated the Kievan realm from modern-day Ukraine to the Baltic Sea and had solidified the frontiers against incursions of Bulgarian, Baltic, and Eastern nomads. Originally a follower of Slavic paganism, Vladimir converted to Christianity in 988 and Christianized the Kievan Rus'.
Rise to the throne.
Vladimir, born in 958 in Budutino (Russian: Будутино), was the natural son and youngest son of Sviatoslav I of Kiev by his housekeeper Malusha. Malusha is described in the Norse sagas as a prophetess who lived to the age of 100 and was brought from her cave to the palace to predict the future. Malusha's brother Dobrynya was Vladimir's tutor and most trusted advisor. Hagiographic tradition of dubious authenticity also connects his childhood with the name of his grandmother, Olga Prekrasa, who was Christian and governed the capital during Sviatoslav's frequent military campaigns. His place of birth is identified by different authors either as Budyatichi (modern Volyn_Oblast, Ukraine) or Budyatino (modern Pskov Oblast, Russia)
Transferring his capital to Pereyaslavets in 969, Sviatoslav designated Vladimir ruler of Novgorod the Great but gave Kiev to his legitimate son Yaropolk. After Sviatoslav's death in 972, a fratricidal war erupted in 976 between Yaropolk and his younger brother Oleg, ruler of the Drevlians. In 977 Vladimir fled to his kinsman Haakon Sigurdsson, ruler of Norway, collecting as many Norse warriors as he could to assist him to recover Novgorod. On his return the next year, he marched against Yaropolk. On his way to Kiev he sent ambassadors to Rogvolod (Norse: Ragnvald), prince of Polotsk, to sue for the hand of his daughter Rogneda (Norse: Ragnhild). The high-born princess refused to affiance herself to the son of a bondswoman, so Vladimir attacked Polotsk, slew Rogvolod, and took Ragnhild by force. Polotsk was a key fortress on the way to Kiev, and capturing Polotsk and Smolensk facilitated the taking of Kiev in 978, where he slew Yaropolk by treachery and was proclaimed knyaz of all Kievan Rus.
Years of pagan rule.
Vladimir continued to expand his territories beyond his father's extensive domain. In 981, he conquered the Cherven towns from the Poles; in 981-982 he suppressed a Vyatichi rebellion; in 983, he subdued the Yatvingians; in 984, he conquered the Radimichs; and in 985, he conducted a military campaign against the Volga Bulgars, planting numerous fortresses and colonies on his way.
Although Christianity spread in the region under Oleg's rule, Vladimir had remained a thoroughgoing pagan, taking eight hundred concubines (along with numerous wives) and erecting pagan statues and shrines to gods. He may have attempted to reform Slavic paganism by establishing the thunder-god, Perun, as a supreme deity.
Open abuse of the deities that most people in Rus' revered triggered widespread indignation. A mob killed the Christian Fyodor and his son Ioann (later, after the overall christening of Kievan Rus, people came to regard these two as the first Christian martyrs in Rus', and the Orthodox Church set a day to commemorate them, July 25). Immediately after the murder of Fyodor and Ioann, early medieval Rus' saw persecutions against Christians, many of whom escaped or concealed their belief.
However, Prince Vladimir mused over the incident long after, and not least for political considerations. According to the early Slavic chronicle called Tale of Bygone Years, which describes life in Kyivan Rus' up to the year 1110, he sent his envoys throughout the civilized world to judge first hand the major religions of the time, Islam, Roman Catholicism, Judaism, and Byzantine Orthodoxy. They were most impressed with their visit to Constantinople, saying, "We knew not whether we were in Heaven or on Earth… We only know that God dwells there among the people, and their service is fairer than the ceremonies of other nations."
Christianization of the Kievan Rus'.
The Primary Chronicle reports that in the year 987, after consultation with his boyars, Vladimir the Great sent envoys to study the religions of the various neighboring nations whose representatives had been urging him to embrace their respective faiths. The result is described by the chronicler Nestor. Of the Muslim Bulgarians of the Volga the envoys reported there is no gladness among them, only sorrow and a great stench. He also reported that Islam was undesirable due to its taboo against alcoholic beverages and pork. Vladimir remarked on the occasion: "Drinking is the joy of all Rus'. We cannot exist without that pleasure." Ukrainian and Russian sources also describe Vladimir consulting with Jewish envoys (who may or may not have been Khazars), and questioning them about their religion but ultimately rejecting it as well, saying that their loss of Jerusalem was evidence that they had been abandoned by God. His emissaries also visited Roman Catholic and Orthodox missionaries. Ultimately Vladimir settled on Orthodox Christianity. In the churches of the Germans his emissaries saw no beauty; but at Constantinople, where the full festival ritual of the Byzantine Church was set in motion to impress them, they found their ideal: "We no longer knew whether we were in heaven or on earth," they reported, describing a majestic Divine Liturgy in Hagia Sophia, "nor such beauty, and we know not how to tell of it." If Vladimir was impressed by this account of his envoys, he was even more attracted by the political gains of the Byzantine alliance.
In 988, having taken the town of Chersonesos in Crimea, he boldly negotiated for the hand of emperor Basil II's sister, Anna. Never before had a Byzantine imperial princess, and one "born in the purple" at that, married a barbarian, as matrimonial offers of French kings and German emperors had been peremptorily rejected. In short, to marry the 27-year-old princess to a pagan Slav seemed impossible. Vladimir was baptized at Chersonesos, however, taking the Christian name of Basil out of compliment to his imperial brother-in-law; the sacrament was followed by his wedding to Anna. Returning to Kiev in triumph, he destroyed pagan monuments and established many churches, starting with a church dedicated to St. Basil, the splendid Church of the Tithes (989) and monasteries on Mt. Athos.
Arab sources, both Muslim and Christian, present a different story of Vladimir's conversion. Yahya of Antioch, al-Rudhrawari, al-Makin, Al-Dimashqi, and ibn al-Athir all give essentially the same account. In 987, Bardas Sclerus and Bardas Phocas revolted against the Byzantine emperor Basil II. Both rebels briefly joined forces, but then Bardas Phocas proclaimed himself emperor on 14 September 987. Basil II turned to the Kievan Rus' for assistance, even though they were considered enemies at that time. Vladimir agreed, in exchange for a marital tie; he also agreed to accept Christianity as his religion and to Christianize his people. When the wedding arrangements were settled, Vladimir dispatched 6,000 troops to the Byzantine Empire, and they helped to put down the revolt.
Christian reign.
Vladimir then formed a great council out of his boyars and set his twelve sons over his subject principalities. According to the Primary Chronicle, he founded the city of Belgorod in 991. In 992 he went on a campaign against the Croats, most likely the White Croats that lived on the border of modern Ukraine. This campaign was cut short by the attacks of the Pechenegs on and around Kiev.
In his later years he lived in a relative peace with his other neighbors: Boleslav I of Poland, Stephen I of Hungary, Andrikh the Czech (questionable character mentioned in A Tale of the Bygone Years). After Anna's death, he married again, likely to a granddaughter of Otto the Great.
In 1014 his son Yaroslav the Wise stopped paying tribute. Vladimir decided to chastise the insolence of his son and began gathering troops against him. Vladimir fell ill, however, most likely of old age, and died at Berestovo, near Kiev. The various parts of his dismembered body were distributed among his numerous sacred foundations and were venerated as relics.
Family.
The fate of all Vladimir's daughters, whose number is around nine, is uncertain.
Significance and legacy.
The Roman Catholic and Eastern Orthodox churches celebrate the feast day of St. Vladimir on 15 July.
St Volodymyr's Cathedral, one of the largest cathedrals in Kiev, is dedicated to Vladimir the Great, as was originally the University of Kiev. The Imperial Russian Order of St. Vladimir and Saint Vladimir's Orthodox Theological Seminary in the United States are also named after him.
The memory of Vladimir was also kept alive by innumerable Ukrainian and Russian folk ballads and legends, which refer to him as "Krasno Solnyshko" (the "Fair Sun"). The Varangian period of Eastern Slavic history ceases with Vladimir, and the Christian period begins.
The appropriation of Kievan Rus' as part of national history has also been a topic of contention in Ukrainophile vs. Russophile schools of historiography since the Soviet era.

</doc>
<doc id="40577" url="http://en.wikipedia.org/wiki?curid=40577" title="Christus Dominus">
Christus Dominus

Christus Dominus is the Second Vatican Council's Decree on the Pastoral Office of Bishops. It was approved by a vote of 2,319 to 2 of the assembled bishops and was promulgated by Pope Paul VI on October 28, 1965. The title in Latin means "Christ the Lord," and is from the first line of the decree, as is customary for Roman Catholic documents. (The full text in English is available from the .)
Apostolic College.
The role of the bishops of the Church was brought into renewed prominence, especially when seen collectively, as a college that has succeeded to that of the apostles in teaching and governing the Church. This college does not exist without its head, the successor of St. Peter. http://www.christusrex.org/www1/CDHN/v3.html]
In these days especially bishops frequently are unable to fulfill their office effectively and fruitfully unless they develop a common effort involving constant growth in harmony and closeness of ties with other bishops. Episcopal conferences already established in many nations-have furnished outstanding proofs of a more fruitful apostolate. Therefore, this sacred synod considers it to be supremely fitting that everywhere bishops belonging to the same nation or region form an association which would meet at fixed times. Thus, when the insights of prudence and experience have been shared and views exchanged, there will emerge a holy union of energies in the service of the common good of the churches. ("CD" 37)
Preliminary Note.
Accordingly, claims made by some, that the Council gave the Church two separate earthly heads, the College of Bishops and the Pope, were countered by the "Preliminary Explanatory Note" added to the Dogmatic Constitution on the Church "Lumen Gentium" and printed at the end of the text.
This Note states:
There is no such thing as the college without its head ... and in the college the head preserves intact his function as Vicar of Christ and pastor of the universal Church. In other words it is not a distinction between the Roman Pontiff and the bishops taken together, but between the Roman Pontiff by himself and the Roman Pontiff along with the bishops.
Episcopal conferences.
In many countries, bishops already held regular conferences to discuss common matters. The Council required the setting up of such episcopal conferences, entrusting to them responsibility for the necessary adaptation to local conditions of general norms. Certain decisions of the conferences have binding force for individual bishops and their dioceses, but only if adopted by a two-thirds majority and confirmed by the Holy See.
Regional conferences, such as the CELAM, exist to assist in promoting common action on a regional or continental level, but do not have even that level of legislative power.
Controversy.
After the publication of "Humanae vitae" in 1968, several problems emerged with the notion of collegiality promoted in the document. The fact that several episcopal conferences would openly rebel against the Pope had been unthinkable during the papacy of Pius XII. Prominent members in the Roman Curia deplored the fact that conference leaders appeared to behave as if they were regional popes. This complaint is notably found in the 1985 Ratzinger Report, a series of interviews where Cardinal Joseph Ratzinger deplores the lack of structure, organization and coordination between Rome and the local assemblies of Catholic bishops.

</doc>
<doc id="40579" url="http://en.wikipedia.org/wiki?curid=40579" title="Gill">
Gill

A gill () is a respiratory organ found in many aquatic organisms that extracts dissolved oxygen from water and excretes carbon dioxide. The gills of some species, such as hermit crabs, have adapted to allow respiration on land provided they are kept moist. The microscopic structure of a gill presents a large surface area to the external environment.
Many microscopic aquatic animals, and some larger but inactive ones, can absorb adequate oxygen through the entire surface of their bodies, and so can respire adequately without a gill. However, more complex or more active aquatic organisms usually require a gill or gills.
Gills usually consist of thin filaments of tissue, branches, or slender, tufted processes that have a highly folded surface to increase surface area. A high surface area is crucial to the gas exchange of aquatic organisms, as water contains only a small fraction of the dissolved oxygen that air does. A cubic meter of air contains about 250 grams of oxygen at STP. The concentration of oxygen in water is lower than air and it diffuses more slowly. In fresh water, the dissolved oxygen content is approximately 8 cm3/L compared to that of air which is 210 cm3/L. Water is 777 times more dense than air and is 100 times more viscous. Oxygen has a diffusion rate in air 10,000 times greater than in water. The use of sac-like lungs to remove oxygen from water would not be efficient enough to sustain life. Rather than using lungs, "[g]asesous exchange takes place across the surface of highly vascularised gills over which a one-way current of water is kept flowing by a specialised pumping mechanism. The density of the water prevents the gills from collapsing and lying on top of each other, which is what happens when a fish is taken out of water."
With the exception of some aquatic insects, the filaments and lamellae (folds) contain blood or coelomic fluid, from which gases are exchanged through the thin walls. The blood carries oxygen to other parts of the body. Carbon dioxide passes from the blood through the thin gill tissue into the water. Gills or gill-like organs, located in different parts of the body, are found in various groups of aquatic animals, including mollusks, crustaceans, insects, fish, and amphibians.
Vertebrate gills.
The gills of vertebrates typically develop in the walls of the pharynx, along a series of gill slits opening to the exterior. Most species employ a countercurrent exchange system to enhance the diffusion of substances in and out of the gill, with blood and water flowing in opposite directions to each other. The gills are composed of comb-like filaments, the gill lamellae, which help increase their surface area for oxygen exchange.
When a fish breathes, it draws in a mouthful of water at regular intervals. Then it draws the sides of its throat together, forcing the water through the gill openings, so it passes over the gills to the outside. Fish gill slits may be the evolutionary ancestors of the tonsils, thymus glands, and Eustachian tubes, as well as many other structures derived from the embryonic branchial pouches.
Fish.
Cartilaginous fish.
Sharks and rays typically have five pairs of gill slits that open directly to the outside of the body, though some more primitive sharks have six or seven pairs. Adjacent slits are separated by a cartilaginous gill arch from which projects a long sheet-like septum, partly supported by a further piece of cartilage called the gill ray. The individual lamellae of the gills lie on either side of the septum. The base of the arch may also support gill rakers, small projecting elements that help to filter food from the water.
A smaller opening, the spiracle, lies in the back of the first gill slit. This bears a small pseudobranch that resembles a gill in structure, but only receives blood already oxygenated by the true gills. The spiracle is thought to be homologous to the ear opening in higher vertebrates.
Most sharks rely on ram ventilation, forcing water into the mouth and over the gills by rapidly swimming forward. In slow-moving or bottom-dwelling species, especially among skates and rays, the spiracle may be enlarged, and the fish breathes by sucking water through this opening, instead of through the mouth.
Chimaeras differ from other cartilagenous fish, having lost both the spiracle and the fifth gill slit. The remaining slits are covered by an operculum, developed from the septum of the gill arch in front of the first gill.
Bony fish.
In bony fish, the gills lie in a branchial chamber covered by a bony operculum. The great majority of bony fish species have five pairs of gills, although a few have lost some over the course of evolution. The operculum can be important in adjusting the pressure of water inside of the pharynx to allow proper ventilation of the gills, so bony fish do not have to rely on ram ventilation (and hence near constant motion) to breathe. Valves inside the mouth keep the water from escaping.
The gill arches of bony fish typically have no septum, so the gills alone project from the arch, supported by individual gill rays. Some species retain gill rakers. Though all but the most primitive bony fish lack spiracles, the pseudobranch associated with them often remains, being located at the base of the operculum. This is, however, often greatly reduced, consisting of a small mass of cells without any remaining gill-like structure.
Marine teleosts also use gills to excrete electrolytes. The gills' large surface area tends to create a problem for fish that seek to regulate the osmolarity of their internal fluids. Salt water is less dilute than these internal fluids, so saltwater fish lose large quantities of water osmotically through their gills. To regain the water, they drink large amounts of sea water and excrete the salt. Fresh water is more dilute than the internal fluids of fish, however, so freshwater fish gain water osmotically through their gills.
Other vertebrates.
Lampreys and hagfish do not have gill slits as such. Instead, the gills are contained in spherical pouches, with a circular opening to the outside. Like the gill slits of higher fish, each pouch contains two gills. In some cases, the openings may be fused together, effectively forming an operculum. Lampreys have seven pairs of pouches, while hagfishes may have six to fourteen, depending on the species. In the hagfish, the pouches connect with the pharynx internally and a separate tube which has no respiratory tissue (the pharyngocutaneous duct) develops beneath the pharynx proper, expelling ingested debris by closing a valve at its anterior end.
Tadpoles of amphibians have from three to five gill slits that do not contain actual gills. Usually no spiracle or true operculum is present, though many species have operculum-like structures. Instead of internal gills, they develop three feathery external gills that grow from the outer surface of the gill arches. Sometimes, adults retain these, but they usually disappear at metamorphosis. Lungfish larvae also have external gills, as does the primitive ray-finned fish "Polypterus", though the latter has a structure different from amphibians.
Branchia.
Branchia (pl. branchiae) is the naturalists' name for gills. Galen observed that fish had multitudes of openings ("foramina"), big enough to admit gases, but too fine to give passage to water. Pliny the Elder held that fish respired by their gills, but observed that Aristotle was of another opinion. The word "branchia" comes from the Greek βράγχια, "gills", plural of βράγχιον (in singular, meaning a fin).
Invertebrate gills.
Respiration in the Echinodermata (includes starfish and sea urchins) is carried out using a very primitive version of gills called papulae. These thin protuberances on the surface of the body contain diverticula of the water vascular system. Crustaceans, molluscs, and some aquatic insects have tufted gills or plate-like structures on the surfaces of their bodies.
The gills of aquatic insects are tracheal, but the air tubes are sealed, commonly connected to thin external plates or tufted structures that allow diffusion. The oxygen in these tubes is renewed through the gills. In the larval dragon fly, the wall of the caudal end of the alimentary tract (rectum) is richly supplied with tracheae as a rectal gill, and water pumped into and out of the rectum provides oxygen to the closed tracheae.
Plastron.
A plastron is a type of structural adaptation occurring among some aquatic arthropods (primarily insects), a form of physical gill which holds a thin film of atmospheric oxygen in an area with small openings called spiracles that connect to the tracheal system. The plastron typically consists of dense patches of hydrophobic setae on the body, which prevent water entry into the spiracles, but may also involve scales or microscopic ridges projecting from the cuticle. The physical properties of the interface between the trapped air film and surrounding water allow gas exchange through the spiracles, almost as if the insect were in atmospheric air. Carbon dioxide diffuses into the surrounding water due to its high solubility, while oxygen diffuses into the film as the concentration within the film has been reduced by respiration, and nitrogen also diffuses out as its tension has been increased. Oxygen diffuses into the air film at a higher rate than nitrogen diffuses out. However, water surrounding the insect can become oxygen-depleted if there is no water movement, so many such insects in still water actively direct a flow of water over their bodies.
The physical gill mechanism allows aquatic insects with plastrons to remain constantly submerged. Examples include many beetles in the family Elmidae, aquatic weevils, and true bugs in the family Aphelocheiridae, as well as at least one species of ricinuleid arachnid. A somewhat similar mechanism is used by the diving bell spider, which maintains an underwater bubble that exchanges gas like a plastron. Other diving insects (such as backswimmers, and hydrophilid beetles) may carry trapped air bubbles, but deplete the oxygen more quickly, and thus need constant replenishment.

</doc>
<doc id="40582" url="http://en.wikipedia.org/wiki?curid=40582" title="Polish United Workers' Party">
Polish United Workers' Party

The Polish United Workers' Party (PUWP; Polish: "Polska Zjednoczona Partia Robotnicza", PZPR) was the Communist party which governed the People's Republic of Poland from 1948 to 1989. Ideologically it was based on the theories of Marxism-Leninism.
Program and goals.
Until 1989, the PUWP held dictatorial powers (the amendment to the constitution of 1976 mentioned "a leading national force"), and controlled an unwieldy bureaucracy, the military, the secret police, and the economy.
Its main goal was to create a Communist society and help to propagate Communism all over the world. On paper, the party was organised on the basis of democratic centralism, which assumed a democratic appointment of authorities, making decisions, and managing its activity. Yet in fact, the key roles were played by the Central Committee, its Politburo and Secretariat, which were subject to the strict control of the authorities of the Soviet Union. These authorities decided about the policy and composition of the main organs; although, according to the statute, it was a responsibility of the members of the congress, which was held every five or six years. Between sessions, party conferences of the regional, county, district and work committees were taking place. The smallest organizational unit of the PUWP was the Fundamental Party Organization (FPO), which functioned in work places, schools, cultural institutions, etc.
The main part in the PUWP was played by professional politicians, or the so-called "party's hard core", formed by people who were recommended to manage the main state institutions, social organizations, and trade unions. In the crowning time of the PUWP's development (the end of the 1970s) it consisted of over 3.5 million members. The Political Office of the Central Committee, Secretariat and regional committees appointed the key posts not only within the party, but also in all organizations having ‘state’ in its name – from central offices to even small state and cooperative companies. It was called the nomenklatura system of the state and economy management. In certain areas of the economy, e.g., in agriculture, the nomenklatura system was controlled with an approval of the PUWP and by its allied parties, the United People's Party (agriculture and food production), and the Democratic Party (trade community, small enterprise, some cooperatives). After martial law began, the Patriotic Movement for National Rebirth was founded to organize these and other parties.
History.
Establishment and Sovietisation period.
The Polish United Workers' Party was established at the unification congress of the Communist Polish Workers' Party (PPR) and Polish Socialist Party (PPS) during meetings held from 15 to 21 December 1948. The unification was possible because the PPS activists who opposed unification (or rather absorption by Communists) had been forced out of the party. Similarly, the members of the PPR who were accused of "rightist – nationalistic deviation" were expelled. Thus, for all intents and purposes, the PZPR was the PPR under a new name. 
"Rightist-nationalist deviation" (Polish: odchylenie prawicowo-nacjonalistyczne) was a political propaganda term used by the Polish Stalinists against prominent activists, such as Władysław Gomułka and Marian Spychalski who opposed Soviet involvement in the Polish interior affairs, as well as internationalism displayed by the creation of the Cominform and the subsequent merger that created the PZPR. It is believed that it was Joseph Stalin who put pressure on Bolesław Bierut and Jakub Berman to remove Gomułka and Spychalski as well as their followers from power in 1948. It is estimated that over 25% of socialists were removed from power or expelled from political life.
Bolesław Bierut, an NKVD agent, and a hard Stalinist served as first Secretary General of the ruling PUWP from 1948 to 1956, playing a leading role in the Sovietisation of Poland and the installation of her most repressive regime. He had served as President since 1944 (though on a provisional basis until 1947). After a new constitution abolished the presidency, Bierut took over as Prime Minister. until his death in 1956.
Bierut oversaw the trials of many Polish wartime military leaders, such as General Stanisław Tatar and Brig. General Emil August Fieldorf, as well as 40 members of the Wolność i Niezawisłość (Freedom and Independence) organisation, various Church officials and many other opponents of the new regime including the "hero of Auschwitz", Witold Pilecki, condemned to death during secret trials. Bierut signed many of those death sentences.
Bierut's death in Moscow in 1956 (shortly after attending the 20th Congress of the Communist Party of the Soviet Union) gave rise to much speculation about poisoning or a suicide, and symbolically marked the end of the era of Stalinism in Poland.
Gomułka's autarchic communism.
In 1956, shortly after the 20th Congress of the Communist Party of the Soviet Union, the PUWP leadership split in two factions, dubbed "Natolinians" and "Puławians". The Natolin faction - named after the place where its meetings took place, in a government villa in Natolin - were against the post-Stalinist liberalization programs ("Gomułka thaw") and they proclaimed simple nationalist and antisemitic slogans as part of a strategy to gain power. The most well known members included Franciszek Jóźwiak, Wiktor Kłosiewicz, Zenon Nowak, Aleksander Zawadzki, Władysław Dworakowski, Hilary Chełchowski.
The Puławian faction - the name comes from the Puławska Street in Warsaw, on which many of the members lived - sought great liberalization of socialism in Poland. After the events of Poznań June, they successfully backed the candidature of Władysław Gomułka for First Secretary of party, thus imposing a major setback upon Natolinians. Among the most prominent members were Roman Zambrowski and Leon Kasman. Both factions disappeared towards the end of the 1950s.
Initially very popular for his reforms and seeking a "Polish way to socialism", and beginning an era known as "Gomułka's thaw", he came under Soviet pressure. In the 1960s he supported persecution of the Roman Catholic Church and intellectuals (notably Leszek Kołakowski who was forced into exile). He participated in the Warsaw Pact intervention in Czechoslovakia in 1968. At that time he was also responsible for persecuting students as well as toughening censorship of the media. In 1968 he incited an anti-Zionist propaganda campaign, as a result of Soviet bloc opposition to the Six-Day War.
In December 1970, a bloody clash with shipyard workers in which several dozen workers were fatally shot forced his resignation (officially for health reasons; he had in fact suffered a stroke). A dynamic younger man, Edward Gierek, took over the Party leadership and tensions eased.
Gierek's economic opening.
In the late 1960s, Edward Gierek had created a personal power base and become the recognized leader of the young technocrat faction of the party. When rioting over economic conditions broke out in late 1970, Gierek replaced Władysław Gomułka as party first secretary. Gierek promised economic reform and instituted a program to modernize industry and increase the availability of consumer goods, doing so mostly through foreign loans. His good relations with Western politicians, especially France's Valéry Giscard d'Estaing and West Germany's Helmut Schmidt, were a catalyst for his receiving western aid and loans.
The standard of living increased markedly in the Poland of the 1970s, and for a time he was hailed a miracle-worker. The economy, however, began to falter during the 1973 oil crisis, and by 1976 price increases became necessary. New riots broke out in June 1976, and although they were forcibly suppressed, the planned price increases were canceled. High foreign debts, food shortages, and an outmoded industrial base compelled a new round of economic reforms in 1980. Once again, price increases set off protests across the country, especially in the Gdańsk and Szczecin shipyards. Gierek was forced to grant legal status to Solidarity and to concede the right to strike. (Gdańsk Agreement).
Shortly thereafter, in early September 1980, Gierek was replaced as by Stanisław Kania as General Secretary of the party by the Central Committee, amidst much social and economic unrest.
Kania admitted that the party had made many economic mistakes, and advocated working with Catholic and trade unionist opposition groups. He met with Solidarity Union leader Lech Wałęsa, and other critics of the party. Though Kania agreed with his predecessors that the Communist Party must maintain control of Poland, he never assured the Soviets that Poland would not pursue actions independent of the Soviet Union. On October 18, 1981, the Central Committee of the Party withdrew confidence on him, and Kania was replaced by Prime Minister (and Minister of Defence) Gen. Wojciech Jaruzelski.
Jaruzelski's autocratic rule.
On 11 February 1981, Jaruzelski was elected Prime Minister of Poland and became the first secretary of the Polish United Workers' Party on October 18 the same year. Before initiating the plan, he presented it to Soviet Premier Nikolai Tikhonov. On 13 December 1981, Jaruzelski imposed martial law in Poland
In 1982 Jaruzelski revitalized the Front of National Unity, the organization the Communists used to manage their satellite parties, as the Patriotic Movement for National Rebirth.
In 1985, Jaruzelski resigned as prime minister and defence minister and became chairman of the Polish Council of State, a post equivalent to that of president or a dictator, with his power centered on and firmly entrenched in his coterie of "LWP" generals and lower ranks officers of the Polish Communist Army.
Breakdown of autocracy.
The attempt to impose a naked military dictatorship, notwithstanding, the policies of Mikhail Gorbachev stimulated political reform in Poland. By the close of the tenth plenary session in December 1988, the Communist Party was forced, after strikes, to approach leaders of Solidarity for talks.
From 6 February to 15 April 1989, negotiations were held between 13 working groups during 94 sessions of the roundtable talks.
These negotiations resulted in an agreement which stated that a great degree of political power would be given to a newly created bicameral legislature. It also created a new post of president to act as head of state and chief executive. Solidarity was also declared a legal organization. During the following Polish elections the Communists won 65 percent of the seats in the Sejm, though the seats won were guaranteed and the Communists were unable to gain a majority, while 99 out of the 100 seats in the Senate freely contested were won by Solidarity-backed candidates. Jaruzelski won the presidential ballot by one vote.
Jaruzelski was unsuccessful in convincing Wałęsa to include Solidarity in a "grand coalition" with the Communists, and resigned his position of general secretary of the Polish Communist Party. The Communists' two allied parties broke their long-standing alliance, forcing Jaruzelski to appoint Solidarity's Tadeusz Mazowiecki as the country's first non-Communist prime minister since 1948. Jaruzelski resigned as Poland's President in 1990, being succeeded by Wałęsa in December.
Dissolution of the PUWP.
Starting from January 1990, the collapse of the PUWP became inevitable. All over the country, public occupations of the party buildings started in order to prevent stealing the party's possessions and destroying or taking the archives. On 29 January 1990, XI Congress was held, which was supposed to recreate the party. Finally, the PUWP dissolved, and some of its members decided to establish two new social-democratic parties. They get over $1 million from the Communist Party of the Soviet Union known as the Moscow loan.
The former activists of the PUWP established the Social Democracy of the Republic of Poland (in Polish: Socjaldemokracja Rzeczpospolitej Polskiej, SdRP), of which the main organizers were Leszek Miller and Mieczysław Rakowski. The SdRP was supposed (among other things) to take over all rights and duties of the PUWP, and help to divide out the property of the former PUWP. Up to the end of the 1980s, it had considerable incomes mainly from managed properties and from the RSW company ‘Press- Book-Traffic’, which in turn had special tax concessions. During this period, the income from membership fees constituted only 30% of the PUWP's revenues. After the dissolution of the PUWP and the establishment of the SdRP, the rest of the activists formed the Social Democratic Union of the Republic of Poland (USdRP), which changed its name to the Polish Social Democratic Union, and The 8th July Movement.
At the end of 1990, there was an intense debate in the Sejm on the takeover of the wealth that belonged to the former PUWP. Over 3000 buildings and premises were included in the wealth and almost half of it was used without legal basis. Supporters of the acquisition argued that the wealth was built on the basis of plunder and the Treasury grant collected by the whole society. Opponents of SdRP (Social Democratic Party of the Republic of Poland) claimed that the wealth was created from membership fees; therefore, they demanded wealth inheritance for SdPR which at that time administered the wealth. Personal property and the accounts of the former PUWP were not subject to control of a parliamentary committee.
On 9 November 1990, the Sejm passed "The resolution about the acquisition of the wealth that belonged to the former PUWP". This resolution was supposed to result in a final takeover of the PUWP real estate by the Treasury. As a result, only a part of the real estate was taken over mainly for a local government by 1992, whereas a legal dispute over the other party carried on till 2000. Personal property and finances of the former PUWP practically disappeared. According to the declaration of SdRP MP's, 90-95% of the party's wealth was allocated for gratuity or was donated for a social assistance.
The Polish Communist Party (2002) claims to be the successor of the party.
Building.
The Central Committee had its seat in the "Party's House", a building erected by obligatory subscription from 1948 to 1952 and colloquially called "White House" or the "House of Sheep". Since 1991 the Bank-Financial Center "New World" is located in this building. From 1991-2000 the Warsaw Stock Exchange also had its seat there.
Party leaders.
By the year 1954 the head of the party was the Chair of Central Committee:

</doc>
<doc id="40583" url="http://en.wikipedia.org/wiki?curid=40583" title="Peritoneum">
Peritoneum

The peritoneum is the serous membrane that forms the lining of the abdominal cavity or coelom in amniotes and some invertebrates, such as annelids. It covers most of the intra-abdominal (or coelomic) organs, and is composed of a layer of mesothelium supported by a thin layer of connective tissue. The peritoneum supports the abdominal organs and serves as a conduit for their blood vessels, lymph vessels, and nerves.
The abdominal cavity (the space bounded by the vertebrae, abdominal muscles, diaphragm, and pelvic floor) should not be confused with the intraperitoneal space (located within the abdominal cavity, but wrapped in peritoneum). The structures within the intraperitoneal space are called "intraperitoneal" (e.g. the stomach), the structures in the abdominal cavity that are located behind the intraperitoneal space are called "retroperitoneal" (e.g. the kidneys), and those structures below the intraperitoneal space are called "subperitoneal" or "infraperitoneal" (e.g. the bladder).
Structure.
Types.
Although they ultimately form one continuous sheet, two types or layers of peritoneum and a potential space between them are referenced:
Subdivisions.
Peritoneal folds are omenta, mesenteries and ligaments; they connect organs to each other or to the abdominal wall. There are two main regions of the peritoneum, connected by the epiploic foramen (also known as the "omental foramen" or "foramen of winslow"):
The mesentery is the part of the peritoneum through which most abdominal organs are attached to the abdominal wall and supplied with blood and lymph vessels and nerves.
Other ligaments and folds.
In addition, in the pelvic cavity there are several structures that are usually named not for the peritoneum, but for the areas defined by the peritoneal folds:
Classification of abdominal structures.
The structures in the abdomen are classified as intraperitoneal, retroperitoneal or infraperitoneal depending on whether they are covered with visceral peritoneum and whether they are attached by mesenteries (mensentery, mesocolon).
Structures that are "intraperitoneal" are generally mobile, while those that are "retroperitoneal" are relatively fixed in their location.
Some structures, such as the kidneys, are "primarily retroperitoneal", while others such as the majority of the duodenum, are "secondarily retroperitoneal", meaning that structure developed intraperitoneally but lost its mesentery and thus became retroperitoneal.
Development.
The peritoneum develops ultimately from the mesoderm of the trilaminar embryo. As the mesoderm differentiates, one region known as the lateral plate mesoderm splits to form two layers separated by an intraembryonic coelom. These two layers develop later into the visceral and parietal layers found in all serous cavities, including the peritoneum.
As an embryo develops, the various abdominal organs grow into the abdominal cavity from structures in the abdominal wall. In this process they become enveloped in a layer of peritoneum. The growing organs "take their blood vessels with them" from the abdominal wall, and these blood vessels become covered by peritoneum, forming a mesentery.
Peritoneal folds develop from the ventral and dorsal mesentery of the embryo.
Clinical significance.
Peritoneal dialysis.
In one form of dialysis, called peritoneal dialysis, a glucose solution is sent through a tube into the peritoneal cavity. The fluid is left there for a prescribed amount of time to absorb waste products, and then removed through the tube. The reason for this effect is the high number of arteries and veins in the peritoneal cavity. Through the mechanism of diffusion, waste products are removed from the blood.
Peritonitis.
Peritonitis refers to the inflammation of the peritoneum. It is more commonly associated to infection from a punctured organ of the abdominal cavity. It can also be provoked by the presence of fluids that produce chemical irritation, such as gastric acid or pancreatic juice. Peritonitis causes fever, tenderness, and pain in the abdominal area, which can be localized or diffuse. The treatment involves rehydration, administration of antibiotics, and surgical correction of the underlying cause. Mortality is higher in the elderly and if present for a prolonged time.
Primary peritoneal carcinoma.
Primary peritoneal cancer is a cancer of the cells lining the peritoneum.
History.
Etymology.
"Peritoneum" is derived from Greek via Latin. "Peri-" means "around", while "-ton-" refers to stretching. Thus, peritoneum means "stretched around" or "stretched over".

</doc>
<doc id="40584" url="http://en.wikipedia.org/wiki?curid=40584" title="Pistachio">
Pistachio

The pistachio, (, -, Persian: پسته‎; "Pistacia vera") a member of the cashew family, is a small tree originating from Central Asia and the Middle East. Pistachio trees can be found in regions of Iran, Syria, Lebanon, Turkey, Greece, Xinjiang (China), Tunisia, Kyrgyzstan, Tajikistan, Turkmenistan, India, Egypt, Italy (Sicily), Uzbekistan, Afghanistan (especially in the provinces of Samangan and Badghis), and the United States, specifically in California. It is indigenous to the eastern Mediterranean (Cyprus and Turkey to Israel and Syria), Central Asia (Turkmenistan, Uzbekistan, Afghanistan, Tajikistan, and Kyrgyzstan) and Xinjiang.
The tree produces seeds that are widely consumed as food.
"Pistacia vera" often is confused with other species in the genus "Pistacia" that are also known as pistachio. These species can be distinguished from "P. vera" by their geographic distributions (in the wild), and their seeds which are much smaller and have a shell that is soft.
History.
Pliny the Elder writes in his "Natural History" that pistachio seeds were a common food as early as 6750 BC. asserts that "pistacia", “well known among us,” was one of the trees unique to Syria, and that the seed was introduced into Italy by the Roman Proconsul in Syria, Lucius Vitellius the Elder (in office in 35 AD) and into Hispania at the same time by Flaccus Pompeius. The early sixth-century manuscript "De observatione ciborum" (“On the observance of foods”) by Anthimus implies that "pistacia" remained well known in Europe in Late Antiquity. The pistachio is one of three seeds mentioned in the Bible. The pistachio is mentioned once, in , as is the walnut in , while the almond is mentioned many times.
Archaeologists have found evidence from excavations at Jarmo in northeastern Iraq.
The Hanging Gardens of Babylon were said to have contained pistachio trees during the reign of King Merodach-Baladan about 700 BC. The modern pistachio "P. vera" was first cultivated in Western Asia, where it has long been an important crop in cooler parts of Iran and Iraq. It appears in Dioscurides as "pistakia" πιστάκια, recognizable as "P. vera" by its comparison to pine nuts. Its cultivation spread into the Mediterranean world by way of Persia from Syria.
Additionally, remains of the Atlantic pistachio and pistachio seed along with nut-cracking tools were discovered by archaeologists at the Gesher Benot Ya'aqov site in Israel's Hula Valley, dated to 78,000 years ago.
More recently, the pistachio has been cultivated commercially in many parts of the English-speaking world, in Australia, and in New Mexico and California, of the United States, where it was introduced in 1854 as a garden tree. David Fairchild of the United States Department of Agriculture introduced hardier cultivars collected in China to California in 1904 and 1905, but it was not promoted as a commercial crop until 1929. Walter T. Swingle’s pistachios from Syria had already fruited well at Niles by 1917.
The earliest records of pistachio in English are around roughly year 1400, with the spellings “pistace” and “pistacia”. The word pistachio comes from medieval Italian "pistacchio", which is from classical Latin "pistacium", which is from ancient Greek "pistákion" and "pistákē", which is generally believed to be from Middle Persian, although unattested in Middle Persian. Later in Persian, the word is attested in Persian as "pesteh". As mentioned, the tree came to the ancient Greeks from Western Asia.
Botany.
Habitat.
Pistachio is a desert plant, and is highly tolerant of saline soil. It has been reported to grow well when irrigated with water having 3,000–4,000 ppm of soluble salts. Pistachio trees are fairly hardy in the right conditions, and can survive temperatures ranging between -10 C in winter and 118 F in summer. They need a sunny position and well-drained soil. Pistachio trees do poorly in conditions of high humidity, and are susceptible to root rot in winter if they get too much water and the soil is not sufficiently free-draining. Long, hot summers are required for proper ripening of the fruit. They have been known to thrive in warm moist environments. 
The Jylgyndy Forest Reserve, a preserve protecting the native habitat of "Pistacia vera" groves, is located in the Nooken District of Jalal-Abad Province of Kyrgyzstan.
Characteristics.
The bush grows up to 10 m tall. It has deciduous pinnate leaves 10–20 centimeters (4–8 inches) long. The plants are dioecious, with separate male and female trees. The flowers are apetalous and unisexual, and borne in panicles.
The fruit is a drupe, containing an elongated seed, which is the edible portion. The seed, commonly thought of as a nut, is a culinary nut, not a botanical nut. The fruit has a hard, creamish exterior shell. The seed has a mauvish skin and light green flesh, with a distinctive flavor. When the fruit ripens, the shell changes from green to an autumnal yellow/red, and abruptly splits part way open (see photo). This is known as dehiscence, and happens with an audible pop. The splitting open is a trait that has been selected by humans. Commercial cultivars vary in how consistently they split open.
Each pistachio tree averages around 50 kg of seeds, or around 50,000, every two years.
The shell of the pistachio is naturally a beige color, but it is sometimes dyed red or green in commercial pistachios. Originally, dye was applied by importers to hide stains on the shells caused when the seeds were picked by hand. Most pistachios are now picked by machine and the shells remain unstained, making dyeing unnecessary except to meet ingrained consumer expectations. Roasted pistachio seeds can be artificially turned red if they are marinated prior to roasting in a salt and strawberry marinade, or salt and citrus salts.
Like other members of the Anacardiaceae family (which includes poison ivy, sumac, mango, and cashew), pistachios contain urushiol, an irritant that can cause allergic reactions.
Cultivation.
Iran, the United States and Turkey are the major producers of pistachios. The trees are planted in orchards, and take approximately seven to ten years to reach significant production. Production is alternate-bearing or biennial-bearing, meaning the harvest is heavier in alternate years. Peak production is reached around 20 years. Trees are usually pruned to size to make the harvest easier. One male tree produces enough pollen for eight to 12 drupe-bearing females. Harvesting in the United States and in Greece is often accomplished using equipment to shake the drupes off the tree. After hulling and drying, pistachios are sorted according to open-mouth and closed-mouth shells. Sun-drying has been found to be the best method of drying, then they are roasted or processed by special machines to produce pistachio kernels.
Pistachio trees are vulnerable to a wide variety of diseases. Among these is infection by the fungus "Botryosphaeria", which causes panicle and shoot blight ("i.e.," kills flowers and young shoots), and can damage entire pistachio orchards.
In California, almost all female pistachio trees are the cultivar 'Kerman'. A scion from a mature female 'Kerman' is grafted onto a one-year-old rootstock. Male pistachios may be a different variety.
In 2013, the City Council of Avenal, California, proclaimed the city to be the Pistachio Capital of the World, reflecting the importance of the pistachio industry to the city's economy.
In Greece, the cultivated type of pistachios is different. It has an almost-white shell, a sweet taste, a red-green kernel and a little bit more closed-mouth shell than the 'Kerman' variety. Most of the production in Greece comes from the island of Aegina and the region of Thessaly-Almyros.
Bulk container shipments of pistachio kernels are prone to self-heating and spontaneous combustion because of their high fat and low water contents.
Consumption.
The kernels are often eaten whole, either fresh or roasted and salted, and are also used in pistachio ice cream, kulfi, spumoni, historically in Neapolitan ice cream, pistachio butter, pistachio paste and confections such as baklava, pistachio chocolate, pistachio halva, pistachio lokum or biscotti and cold cuts such as mortadella. Americans make pistachio salad, which includes fresh pistachios or pistachio pudding, whipped cream, and canned fruit.
In July 2003, the Food and Drug Administration (FDA) approved the first qualified health claim specific to seeds lowering the risk of heart disease: "Scientific evidence suggests but does not prove that eating 1.5 oz per day of most nuts, such as pistachios, as part of a diet low in saturated fat and cholesterol may reduce the risk of heart disease".
China is the top pistachio consumer worldwide, with annual consumption of 80,000 tons, while the United States consumes 45,000 tons. Russia (with consumption of 15,000 tons) and India (with consumption of 10,000 tons) are in the third and fourth places.
Nutritional Information.
The fat profile of pistachios is roughly 14% saturated fat, 54% monounsaturated fat and 32% polyunsaturated fat. A 1-cup serving of pistachio seeds contains 6.143 g of saturated palmitic fatty acid, only 0.585 g of stearic fatty acid and trace amounts of arachidic and behenic saturated fatty acids.
Toxin and safety concerns.
As with other tree seeds, aflatoxin is found in poorly harvested or processed pistachios. Aflatoxins are potent carcinogenic chemicals produced by molds such as "Aspergillus flavus" and "Aspergillus parasiticus". The mold contamination may occur from soil, poor storage, and spread by pests. High levels of mold growth typically appear as gray to black filament-like growth. It is unsafe to eat mold-infected and aflatoxin-contaminated pistachios. Aflatoxin contamination is a frequent risk, particularly in warmer and humid environments. Food contaminated with aflatoxins has been found as the cause of frequent outbreaks of acute illnesses in parts of the world. In some cases, such as Kenya, this has led to several deaths.
Pistachio shells typically split naturally prior to harvest, with a hull covering the intact seeds. The hull protects the kernel from invasion by molds and insects, but this hull protection can be damaged in the orchard by poor orchard management practices, by birds, or after harvest, which makes it much easier for pistachios to be exposed to contamination. Some pistachios undergo so-called “early split”, wherein both the hull and the shell split. Damage or early splits can lead to aflatoxin contamination. In some cases, a harvest may be treated to keep contamination below strict food safety thresholds; in other cases, an entire batch of pistachios must be destroyed because of aflatoxin contamination. In September 1997, the European Union placed its first ban on pistachio imports from Iran due to high levels of aflatoxin. The ban was lifted in December 1997 after Iran introduced and improved food safety inspections and product quality.
Pistachio shells may be helpful in cleaning up pollution created by mercury emissions.

</doc>
<doc id="40586" url="http://en.wikipedia.org/wiki?curid=40586" title="Dorididae">
Dorididae

Sea lemon is a loosely-applied common name for a group of medium-sized to large shell-less colorful sea slugs or nudibranchs, specifically dorid nudibranchs in the taxonomic family Dorididae and other closely related families. These are marine gastropod mollusks. 
The Monterey sea lemon is "Doris montereyensis" and the mottled pale sea lemon is "Diaulula lentiginosa".
The common name sea lemon probably comes from these animal's visual similarity to a lemon based on such qualities as the roughened skin, the oval form when seen from above, and the common but not inevitable orange to pale yellow coloration.
Description.
These dorid nudibranchs can be large (up to 20 cm), rather flattened, and oval in shape when seen from above. They have two hornlike projections (rhinophores) on the head, and a rosette-like tuft of gills on the back of the animal. The mantle is sometimes sprinkled with black dots, and it is covered in small bumps, which are called tubercles.
Life habits.
Sea lemons feed on sponges and other sessile animals or even on dead organic matter. They lay ribbons of white or yellow eggs. 
Taxonomically the Dorididae is a family of several genera, the "dorids" named after the mythological ancient Greek sea nymph Doris. (See Ovidius, "Metamorphoses" 2.6)
Genera.
Genera within the family Dorididae include:

</doc>
<doc id="40589" url="http://en.wikipedia.org/wiki?curid=40589" title="Thomas Joannes Stieltjes">
Thomas Joannes Stieltjes

Thomas Joannes Stieltjes (], 29 December 1856 – 31 December 1894) was a Dutch mathematician. He was born in Zwolle and died in Toulouse, France. He was a pioneer in the field of moment problems and contributed to the study of continued fractions.
The Thomas Stieltjes Institute for Mathematics at the University of Leiden is named after him, as is the Riemann–Stieltjes integral.
Biography.
Stieltjes was born in Zwolle on 29 December 1856. His father (who had the same first names) was a civil engineer and politician. Stieltjes Sr. was responsible for the construction of various harbours around Rotterdam, and also seated in the Dutch parliament. Stieltjes Jr. went to university at the Polytechnical School in Delft in 1873. Instead of attending lectures, he spent his student years reading the works of Gauss and Jacobi — the consequence of this being he failed his examinations. There were 2 further failures (in 1875 and 1876), and his father despaired. His father was friends with H. G. van de Sande Bakhuyzen (who was the director of Leiden University), and Stieltjes Jr. was able to get a job as an assistant at Leiden Observatory.
Soon afterwards, Stieltjes began a correspondence with Charles Hermite which lasted for the rest of his life. Stieltjes originally wrote to Hermite concerning celestial mechanics, but the subject quickly turned to mathematics and he began to devote his spare time to mathematical research.
The director of Leiden Observatory, van de Sande-Bakhuyzen, responded quickly to Stieltjes' request on 1 January 1883 to stop his observational work to allow him to work more on mathematical topics. In 1883, he also married Elizabeth Intveld in May. She also encouraged him to move from astronomy to mathematics. And in September, Stieltjes was asked to substitute at University of Delft for F J van den Berg. From then until December of that year, he lectured on analytical geometry and on descriptive geometry. He resigned his post at the observatory at the end of that year.
In 1884, Stieltjes applied for a chair in Groningen. He was initially accepted, but in the end turned down by the Department of Education, since he lacked the required diplomas. In 1884, Hermite and professor Bierens de Haan arranged for an honorary doctorate to be granted to Stieltjes by Leiden University, enabling him to become a professor. In 1885, he was honorably appointed as member of the Royal Dutch Academy of Sciences (Koninklijke Nederlandse Akademie van Wetenschappen, KNAW), and in 1889, he was appointed professor of differential and integral calculus at Toulouse University.
Research.
Stieltjes worked on almost all branches of analysis, continued fractions and number theory, and for his work, he is sometimes called "the father of the analytic theory of continued fractions".
His work is also seen as important as a first step towards the theory of Hilbert spaces. Other important contributions to mathematics that he made involved discontinuous functions and divergent series, differential equations, interpolation, the gamma function and elliptic functions.
Awards.
Stieltjes' work on continued fractions earned him the Ormoy Prize of the Académie des Sciences.

</doc>
<doc id="40590" url="http://en.wikipedia.org/wiki?curid=40590" title="Vladimir of Novgorod">
Vladimir of Novgorod

Vladimir Yaroslavich (Russian: Владимир Ярославич) (1020 – October 4, 1052) reigned as prince of Novgorod from 1036 until his death. He was the eldest son of Yaroslav I the Wise of Kiev by Ingigerd, daughter of king Olof Skötkonung of Sweden. 
In the state affairs he was assisted by the voivode Vyshata and the bishop Luka Zhidiata. In 1042, Vladimir may have been in conflict with Finns, according to some interpretations even making a military campaign in Finland. In the next year he led the Russian armies together with Harald III of Norway against the Byzantine emperor Constantine IX. He predeceased his father by two years and was buried by him in St Sophia Cathedral he had built in Novgorod. His sarcophagus is in a niche on the south side of the main body of the cathedral overlooking the Martirievskii Porch. He is depicted in an early twentieth-century fresco above the sarcophagus and on a new ephigial icon on top of the sarcophagus. The details of his death is unknown, however his son Rostislav and his descendants were in unfriendly relationship with the descendants of the Yaroslaviches triumvirate (Iziaslav, Sviatoslav, and Vsevolod). Three of Vladimir's younger brothers Izyaslav I, Svyatoslav II and Vsevolod I all reigned in Kiev, while other two (Igor and Vyacheslav) died in their early twenties after which their lands were split between the Yaroslaviches triumvirate. Coincidentally, the Vyshata of Novgorod pledged his support to Rostislav in the struggle against the triumvirate.
Vladimir's only son, Rostislav Vladimirovich, was a landless prince who usurped power in Tmutarakan. His descendants were dispossessed by their uncles and were proclaimed as izgoi (outcast), but gradually managed to establish themselves in Halychyna, ruling the land until 1199, when their line went extinct. In order to downplay their claims to Kiev, the records of Vladimir's military campaigns seem to have been obliterated from Kievan chronicles. As a result, medieval historians often confuse him with two more famous namesakes — Vladimir the Great and Vladimir Monomakh. The name of Vladimir's consort is uncertain either. According to Nikolai Baumgarten, Vladimir was married to the daughter of count Leopold of Staden, Ode. Others (Aleksandr Nazarenko) disregard that assumption or claim a different person.
Vladimir's memory was better preserved in foreign sources. In Norse sagas he frequently figures as Valdemar Holti (that is, "the Nimble"). George Cedrenus noticed Vladimir's arrogance in dealing with the Byzantines.

</doc>
<doc id="40591" url="http://en.wikipedia.org/wiki?curid=40591" title="Johann Mühlegg">
Johann Mühlegg

Johann Mühlegg (born 8 November 1970 in Ostallgäu, Germany) is a former top level cross-country skier who competed in international competitions first representing Germany and then Spain, after becoming a Spanish citizen in 1999. He was excluded and disqualified from the 2002 Winter Olympics in Salt Lake City for doping.
Early career.
Mühlegg participated for Germany in the 1992, 1994 and 1998 Winter Olympics, even though he began having trouble with Germany's ski federation in 1993. From the beginning, Mühlegg singled himself out, at one point accusing German head coach Georg Zipfel for "damaging him spiritually" (the so-called "Spiritistenaffäre"). He was thrown off the team in 1995, but was reinstated later. But from that moment on, the ever eccentric Mühlegg insisted on taking a flask of holy water with him at all times, and trusting only his Portuguese cleaning woman/chaperone Justina Agostino. In the end, Mühlegg was branded as a team cancer and was thrown out.
Competing for Spain.
After being ejected from the national team after the 1998 Nagano Games, his good relations with members of the Spanish cross-country skiing team, in particular Juan Jesús Gutiérrez Cuevas and Haritz Zunzunegui, opened the door for Mühlegg to obtain Spanish citizenship.
In late 1999, competing for Spain, he won a World Cup race for the first time. At the 2001 FIS Nordic World Ski Championships in Lahti, he won two medals with a silver in the 10 km + 10 km combined pursuit (stepping up when the original medalist Jari Isometsä was disqualified for hemohes use), and a gold in the 50 km freestyle race.
In the 2002 Winter Olympics in Salt Lake City, Mühlegg won gold medals in the 30 km freestyle and the 10 km + 10 km pursuit races, the successes gaining him congratulations from King Juan Carlos of Spain.
Mühlegg finished first in the 50 km classical race held on the final Saturday of the Salt Lake City Winter Olympic Games on 23 February 2002 but was disqualified from that race and was expelled from the Games the next day, after testing positive for darbepoetin¹ (a medicine which boosts red blood cell count; the substance was not banned at the time since it had only recently been developed).
Doping controversy.
Following the darbepoetin scandal, the International Olympic Committee (IOC) initially let Mühlegg keep his gold medals from the first two races. But in December 2003 a ruling by the Court of Arbitration for Sport (CAS) found that these medals should also be withdrawn. The CAS remitted this case as well as similar ones involving Olga Danilova and Larisa Lazutina (both from Russia) to the IOC Executive Board, which confirmed the rulings in February 2004.

</doc>
<doc id="40592" url="http://en.wikipedia.org/wiki?curid=40592" title="Darbepoetin alfa">
Darbepoetin alfa

Darbepoetin alfa (rINN) is a synthetic form of erythropoietin. It stimulates erythropoiesis (increases red blood cell levels) and is used to treat anemia, commonly associated with chronic renal failure and cancer chemotherapy. Darbepoetin is marketed by Amgen under the trade name Aranesp.
The drug was approved in September 2001 by the Food and Drug Administration for treatment of anemia in patients with chronic renal failure by intravenous or subcutaneous injection. In June 2001, it had been approved by the European Medicines Agency for this indication as well as the treatment of anemia in cancer patients undergoing chemotherapy.
Dr. Reddy's Laboratories launched darbepoetin alfa in India under the brand name ‘Cresp’ in August 2010. This is the world’s first generic darbepoetin alfa. Cresp has been approved in India.
Darbepoetin is produced by recombinant DNA technology in modified Chinese hamster ovary cells. It differs from endogenous erythropoietin (EPO) by containing two more N-linked oligosaccharide chains. It is an erythropoiesis-stimulating 165-amino acid protein.
Contraindications.
Use of darbepoetin alfa is contraindicated in patients with hypersensitivity to the drug, pre-existing uncontrolled hypertension, and pure red cell aplasia.
Adverse effects.
Darbepoetin alfa has black box warnings in the United States for increased risk of death, myocardial infarction, stroke, venous thromboembolism, thrombosis of vascular access, and tumor progression or recurrence. To avoid side effects, it is recommended for patients with chronic renal failure or cancer to use the lowest possible dose needed to avoid red blood cell (RBC) transfusions.
In addition to those listed in the black box warning, use of darbepoetin alfa also increases the risk of cardiovascular problems, including cardiac arrest, arrhythmia, hypertension and congestive heart failure, and edema. A recent study has extended these findings to treatment of patients exhibiting cancer-related anemia (distinct from anemia resulting from chemotherapy). Other reported adverse reactions include increased risk of seizure, hypotension, and chest pain.
Pregnancy and lactation.
Darbepoetin alfa is a Pregnancy Category C drug in the United States. Pregnant women who are taking darbepoetin alfa may enroll in Amgen’s Pregnancy Surveillance Program (800-772-6436).
It is not known if darbepoetin alfa is excreted in breast milk. Mothers who choose to breast-feed are advised to use caution.
Mechanism of action.
Darbepoetin alfa binds to the erythropoietin receptor on erythroid progenitor cells, stimulating RBC production and differentiation.
Safety advisories in anemic cancer patients.
Amgen sent a "dear doctor" letter in January, 2007, that highlighted results from a recent anemia of cancer trial, and warned doctors to consider use in that off-label indication with caution.
Amgen advised the U.S. Food and Drug Administration (FDA) as to the results of the DAHANCA 10 clinical trial. The DAHANCA 10 data monitoring committee found that 3-year loco-regional control in subjects treated with Aranesp was significantly worse than for those not receiving Aranesp (p=0.01).
In response to these advisories, the FDA released a Public Health Advisory
on March 9, 2007, and a clinical alert for doctors on February 16, 2007, about the use of erythropoeisis-stimulating agents (ESAs) such as epoetin alfa (marketed as Epogen) and darbepoetin alfa. The advisory recommended caution in using these agents in cancer patients receiving chemotherapy or off chemotherapy, and indicated a lack of clinical evidence to support improvements in quality of life or transfusion requirements in these settings.
According to the 2010 update to clinical practice guidelines from the American Society of Clinical Oncology (ASCO) and the American Society of Hematology (ASH), use of ESAs such as darbepoetin alfa in cancer patients is appropriate when following stipulations outlined in FDA-approved labeling.
Society and culture.
Like EPO, darbepoetin alfa has the potential to be abused by athletes seeking a competitive advantage. Its use during the 2002 Winter Olympic Games to improve performance led to the disqualification of cross-country skiers Larisa Lazutina and Olga Danilova of Russia and Johann Mühlegg of Spain from their final races.
Economics.
Epogen and Aranesp had more than $6 billion in combined sales in 2006. Procrit sales were about $3.2 billion in 2006.

</doc>
<doc id="40594" url="http://en.wikipedia.org/wiki?curid=40594" title="Pseudonym">
Pseudonym

A pseudonym ( ) is a name that a person or group assumes for a particular purpose, which differs from his or her original or true name (orthonym). Pseudonyms include stage names, screen names, ring names, pen names, nicknames, aliases, superhero identities and code names, gamer identifications, and regnal names of emperors, popes and other monarchs. Historically they have often taken the form of anagrams, Graecisms, and Latinisations, although there are many other methods of choosing a pseudonym.
Pseudonyms are most usually adopted to hide an individual's real identity, as with writers' pen names, graffiti artists' tags, resistance fighters' or terrorists' "noms de guerre", and computer hackers' handles. Actors, musicians, and other performers sometimes use stage names, for example, to mask their ethnic backgrounds. Employers sometimes require employees to use assigned names to help sell products: for example, a company that does business mostly in one country but locates a call center in another country may require its employees to assume names common in the former country to try to draw a more positive or less negative reaction from customers.
In some cases, pseudonyms are adopted because they are part of a cultural or organisational tradition: for example devotional names used by members of some religious institutes, and "cadre names" used by Communist party leaders such as Trotsky and Lenin.
A pseudonym may also be used for purely personal reasons when an individual feels the context and content of the exchange offer no reason, legal or otherwise, to provide their given or legal name.
A "collective name" or "collective pseudonym" is one shared by two or more persons, for example the co-authors of a work, such as Ellery Queen, or Nicolas Bourbaki.
Etymology.
The term is derived from the Greek ψευδώνυμον ("pseudṓnymon"), literally "false name", from ψεῦδος ("pseûdos"), "lie, falsehood" and ὄνομα ("ónoma"), "name". A pseudonym is distinct from an "allonym", which is the (real) name of another person, assumed by the author of a work of art. This may occur when someone is ghostwriting a book or play, or in parody, or when using a "front" name, such as by screenwriters blacklisted in Hollywood in the 1950s and 1960s. See also pseudepigraph, for "falsely" attributed authorship.
Concealment of identity.
Literary pen names.
A pen name (or "nom de plume") is a pseudonym (sometimes a particular form of the real name) adopted by an author (or on the author's behalf by their publishers). Many pen names are used to conceal the author's identity. One famous example of this is Samuel Clemens' writing under the pen name Mark Twain. A pen name may be used if a writer's real name is likely to be confused with the name of another writer or notable individual, or if their real name is deemed to be unsuitable. Authors who write both fiction and non-fiction, or in different genres, may use different pen names to avoid confusing their readers, as in the case of mathematician Charles Dodgson, who wrote fantasy novels under the pen name Lewis Carroll and mathematical treatises under his own name. Some authors, such as Harold Robbins, use several literary pseudonyms.
The Brontë family used pen names for their early work, so as not to reveal their gender (see below) and so that local residents would not know that the books related to people of the neighbourhood. The Brontës used their neighbours as inspiration for characters in many of their books. Anne Brontë published "The Tenant of Wildfell Hall" under the name Acton Bell. Charlotte Brontë published "Shirley" and "Jane Eyre" under the name Currer Bell. Emily Brontë published "Wuthering Heights" as Ellis Bell.
Some female authors used male pen names, in particular in the 19th century, when writing was a male-dominated profession. In contrast, some twentieth and twenty first century male romance novelists have used female pen names. A well-known example of the former is Mary Ann Evans, who wrote as George Eliot. Another example is Amandine Aurore Lucile Dupin, a 19th-century French writer who used the pen name George Sand. Jane Austen used the pseudonym "A Lady" as the author of her first novel "Sense and Sensibility". A few examples of male authors using female pseudonyms include Brindle Chase, Peter O'Donnell (wrote as Madeline Brent) and Christopher Wood (wrote as Penny Sutton and Rosie Dixon).
Some pen names are not strictly pseudonyms, as they are simply variants of the authors' actual names. The authors C. L. Moore and S. E. Hinton were female authors who used the initialised forms of their full names in order to disguise their gender and attract various types of readers, without creating expectations about the content of their work due to some readers' gender-related stereotypes. C. L. Moore was Catherine Lucille Moore, who wrote in the 1930s male-dominated science fiction genre, and S. E. Hinton, (author of "The Outsiders") is Susan Eloise Hinton. "Star Trek" writer D. C. Fontana (Dorothy Catherine) wrote using her abbreviated own name and also under the pen names Michael Richards and J. Michael Bingham. Author V.C. Andrews intended to publish under her given name of Virginia Andrews, but was told that, due to a production error, her first novel was being released under the name of "V.C. Andrews"; later she learned that the publisher had done this deliberately to increase her books' appeal for male readers. Joanne Kathleen Rowling published the "Harry Potter" series under the shortened name J. K. Rowling. Rowling also published a detective novel The Cuckoo's Calling under the pseudonym "Robert Galbraith".
Winston Churchill wrote under the pen name Winston S. Churchill (from his full surname "Spencer-Churchill" which he did not otherwise use) in an attempt to avoid confusion with the American novelist of the same name. In this case the attempt was not entirely successful – and the two are still sometimes confused by booksellers.
A pen name may be used specifically to hide the identity of the author, as in the case of exposé books about espionage or crime, or explicit erotic fiction. Some prolific authors adopt a pseudonym to disguise the extent of their published output, e.g. Stephen King writing as Richard Bachman. Co-authors may choose to publish under a collective pseudonym, e.g., P. J. Tracy and Perri O'Shaughnessy. Frederic Dannay and Manfred Lee used the name Ellery Queen as both a pen name for their collaborative works and as the name of their main character.
A famous case in French literature was Romain Gary. Already a well-known and highly acclaimed writer, he started publishing books under the pen name Émile Ajar. He wanted to test whether his new books would be well received on their own merits and without the aid of his established reputation, and they were. Similarly, Ronnie Barker submitted comedy material under the name of Gerald Wiley.
A collective pseudonym may represent an entire publishing house, or any contributor to a long-running series, especially with juvenile literature. Examples include Watty Piper, Victor Appleton, Erin Hunter, and Kamiru M. Xhan.
Another use of a pseudonym in literature is to present a story as being written by the fictional characters in the story. The series of novels known as A Series Of Unfortunate Events are written by Daniel Handler under the pen name of Lemony Snicket, a character in the series.
An anonymity pseudonym or multiple-use name is a name used by many different people to protect anonymity. It is a strategy that has been adopted by many unconnected radical groups and by cultural groups, where the construct of personal identity has been criticised. This has led to the idea of the "open pop star".
Aliases, fictitious business names, and dummy corporations in criminal activity.
Criminals may use aliases, fictitious business names, and dummy corporations (corporate shells) to hide their identity, or to impersonate other persons or entities in order to commit fraud. Aliases and fictitious business names used for dummy corporations may become so complex that, in the words of the "Washington Post", "getting to the truth requires a walk down a bizarre labyrinth" and multiple government agencies may become involved to uncover the truth.
While governor of Alaska, Sarah Palin used a private Yahoo! e-mail account to skirt government transparency laws. While director of the EPA, Lisa Jackson set up a false identity named Richard Windsor in order to use an official epa.gov email account that was not linked to her office. "Richard Windsor" was awarded certificates for completing training in ethical behaviour and e-mail management.
Noms de guerre.
In Ancien Régime France, a "nom de guerre" ("war name") would be adopted by each new recruit (or assigned to him by the captain of his company) as he enlisted in the French army. These pseudonyms had an official character and were the predecessor of identification numbers: soldiers were identified by their first names, their family names, and their "noms de guerre" (e.g. "Jean Amarault dit Lafidélité"). These pseudonyms were usually related to the soldier's place of origin (e.g. "Jean Deslandes dit Champigny", for a soldier coming from a town named Champigny), or to a particular physical or personal trait (e.g. "Antoine Bonnet dit Prettaboire", for a soldier "prêt à boire", ready to drink). In 1716 a "nom de guerre" was mandatory for every soldier; officers did not adopt "noms de guerre" as they considered them derogatory. In daily life, these aliases could replace the real family name.
"Noms de guerre" were adopted for security reasons by members of the World War II French resistance and Polish resistance. Such pseudonyms are often adopted by military special forces soldiers, such as members of the SAS and other similar units, resistance fighters, terrorists, and guerrillas. This practice hides their identities and may protect their families from reprisals; it may also be a form of dissociation from domestic life. Some well-known men who adopted "noms de guerre" include Carlos the Jackal, for Ilich Ramírez Sánchez; Willy Brandt, Chancellor of West Germany; and Subcomandante Marcos, the spokesman of the Zapatista Army of National Liberation (EZLN). During Lehi's underground fight against the British in Mandatory Palestine, the organization's commander Yitzchak Shamir (later Prime Minister of Israel) adopted the "nom de guerre" "Michael", in honor of Ireland's Michael Collins. Revolutionaries and resistance leaders, such as Lenin, Trotsky, Golda Meir, Moshe Dayan, Philippe Leclerc de Hauteclocque, and Josip Broz, often adopted their "noms de guerre" as their proper names after the struggle. George Grivas, the Greek-Cypriot EOKA militant, adopted the "nom de guerre" Digenis (Διγενής). In the French Foreign Legion, recruits can adopt a pseudonym to break with their past lives. Mercenaries have long used "noms de guerre", even sometimes multiple identities depending on country, conflict and circumstance.
Computer users.
Individuals using a computer online may adopt or be required to use a form of pseudonym known as a "handle" (a term deriving from CB slang), "user name", "login name", "avatar", or, sometimes, "screen name" or "nickname". On the Internet, pseudonymous remailers utilise cryptography that achieves persistent pseudonymity, so that two-way communication can be achieved, and reputations can be established, without linking physical identities to their respective pseudonyms. Aliasing is the use of multiple names for the same data location.
More sophisticated cryptographic systems, such as anonymous digital credentials, enable users to communicate pseudonymously (i.e., by identifying themselves by means of pseudonyms). In well-defined abuse cases, a designated authority may be able to revoke the pseudonyms and reveal the individuals' real identity.
Use of pseudonyms is common among professional eSports players, despite the fact that most professional games are played offline.
Business sales.
People of ethnic minorities in some areas of the world are sometimes told by an employer to use a pseudonym that is common or acceptable in that part of the world when conducting business, as some people might prefer a person of similar ethnic origin or similar background to people of foreign background or foreign ethnicity.
Privacy.
People seeking privacy often use pseudonyms to make appointments and reservations. Those writing to advice columns in newspapers and magazines may use pseudonyms. Steve Wozniak used a pseudonym when attending the University of California, Berkeley after cofounding Apple Computer because, he said, "I knew I wouldn't have time enough to be an A+ student."
Establishment of identity.
The practice of assigning patrilineal and matrilineal names to offspring for the purpose of tracing ancestry or determining inheritance and other relationships may be giving way to a 21st-century preference for self-assigned or quality-assigned names as replacements for birth given names. A common practice of many indigenous peoples was to assign a clan or shamanic name to members in puberty and post-puberty rituals. It is becoming more common for modern authors and others to elect to take names better suited to their own tastes, characters, or other aspects of personal description or preference. "In most legal systems, a name assumed for a non-fraudulent purpose is a legal name and usable as the person's true name...". This is distinct from, though not exclusive of, employing a pseudonym for the purpose of concealment.
Stage names.
When used by an actor, musician, radio disc jockey, model or other performer or "show business" personality a pseudonym is called a "stage name", or, occasionally, a "professional name", or "screen name".
Film, theatre, and related activities.
Members of a marginalized ethnic or religious group have often adopted stage names, typically changing their surname or entire name to mask their original background. The film-making team of Joel and Ethan Coen, for instance, share credit for editing under the alias Roderick Jaynes.
Stage names are also used to create a more marketable name, as in the case of Creighton Tull Chaney, who adopted the pseudonym Lon Chaney, Jr., a reference to his famous father Lon Chaney, Sr. Conversely, Nicolas Cage adopted this stage name instead of his real name, Nicolas Kim Coppola, in order to conceal the appearance of nepotism as the nephew of famous director Francis Ford Coppola. Chris Curtis of Deep Purple fame was christened as Christopher Crummey. In this and similar cases a stage name is adopted simply to avoid an unfortunate pun.
Pseudonyms are also used to comply with the rules of performing arts guilds (Screen Actors Guild (SAG), Writers Guild of America, East (WGA), AFTRA, etc.), which do not allow performers to use an existing name, in order to avoid confusion. For example, these rules required film and television actor Michael Fox to add a middle initial and become Michael J. Fox, to avoid being confused with another actor named Michael Fox. This was also true of author and actress Fannie Flagg, who chose this pseudonym; her real name, Patricia Neal, being the name of another well-known actress; and British actor Stewart Granger, whose real name was James Stewart. Even Dick Van Dyke was called Navckid Keyd at the end of the credits in the 1964 film Mary Poppins.
Some stage names are used to conceal a person's identity, such as the pseudonym Alan Smithee, which is used by directors in the Directors Guild of America (DGA) to remove their name from a film they feel was edited or modified beyond their artistic satisfaction. Actors and actresses in pornographic films use "noms de porn" to conceal their identity as well as to make it more outrageous and memorable (e.g., Dick Nasty). In theatre, the pseudonyms George or Georgina Spelvin, David Agnew, and Walter Plinge are used to hide the identity of a performer, usually when he or she is "doubling" (playing more than one role in the same play).
Music.
Musicians and singers can use pseudonyms to allow artists to collaborate with artists on other labels while avoiding the need to gain permission from their own labels, such as the artist Jerry Samuels, who made songs under Napoleon XIV. Rock singer-guitarist George Harrison, for example, played guitar on Cream's song "Badge" using a pseudonym. In classical music, some record companies issued recordings under a "nom de disque" in the 1950s and 1960s to avoid paying royalties. A number of popular budget LPs of piano music were released under the pseudonym Paul Procopolis. Pseudonyms are also used as stage names in heavy metal bands, such as Tracii Guns in LA Guns, Axl Rose and Slash in Guns N' Roses, Mick Mars in Mötley Crüe or C.C. Deville in Poison. Some of these names have additional meanings, like that of Brian Hugh Warner, more commonly known as Marilyn Manson: Marilyn coming from Marilyn Monroe and Manson from convicted serial killer Charles Manson. Jacoby Shaddix of Papa Roach went under the name "Coby Dick" during the "Infest" era. He changed back to his birth name when "lovehatetragedy" was released.
Elton John (whose given name was Reginald Kenneth Dwight, until it was legally changed in 1972) is known for his use of aliases under various writing and production credits throughout his career. Amongst the many are: Ann Orson; Lord Choc Ice; William A. Bong (a pun on "bill-a-bong", an Australian term for "pond"); Reggae Dwight, and Frank N. Stein.
Ross Bagdasarian, Sr., creator of Alvin and the Chipmunks, wrote original songs, arranged, and produced the records under his real name, but performed on them as David Seville. He also wrote songs using the name Skipper Adams. Danish pop pianist Bent Fabric, whose full name is Bent Fabricius-Bjerre, wrote his biggest instrumental hit "Alley Cat (song)" under the name Frank Bjorn.
For a time, the musician Prince used an unpronounceable "Love Symbol" as a pseudonym ("Prince" is his actual first name rather than a stage name). He wrote the song "Sugar Walls" for Sheena Easton under the alias "Alexander Nevermind" and "Manic Monday" for The Bangles as "Christopher Tracy" (he also produced albums early in his career as "Jamie Starr").
Many Italian-American singer used stage names as their brith names were difficult to pronounce or considered too ethnic for American tases at the time. Singers changing their names included Dean Martin (born Dino Paul Crocetti), Connie Francis (born Concetta Franconero]], Frankie Valli (born Francesco Castelluccio), Tony Bennett (born Anthony Benedetto) and Lady Gaga (born Stefani Germanotta)
In 2009, British rock band Feeder briefly changed their name to Renegades so they could play a whole show featuring a setlist in which 95 percent of the songs played were from their forthcoming new album of the same name, with none of their singles included. Frontman Grant Nicholas felt that if they played as Feeder, there would be an uproar that they did not play any of the singles, so used the pseudonym as a hint. A series of small shows were played in 2010, at 250- to 1,000-capacity venues with the plan not to say who the band really are and just announce the shows as if they are a new band, Grant later hinted it was really Feeder to the fans on their website, which caused a series of rumours that suggested the band changed their name permanently, although "Some people got it straight away", but as intended got people talking.
In many cases, hip-hop and Rap artist prefer to use pseudonyms that represents some variation of their name, personality, or interests. Prime examples include Iggy Azalea (her name comes from her dog name,Iggy and her home street in Mullumbimby,Azalea street) Ol' Dirty Bastard (who was known under at least six aliases), Diddy (previously known at various times as Puffy, P. Diddy, and Puff Daddy), Ludacris, Flo Rida (his name is a tribute to his home state, Florida), LL Cool J, and Chingy. Black metal artists also adopt pseudonyms, usually symbolizing dark values, such as Nocturno Culto, Gaahl, Abbath, and Silenoz. In punk and hardcore punk, singers and band members often replace their real names with "tougher"-sounding stage names, such as Sid Vicious (real name John Simon Ritchie) of the late 1970s band Sex Pistols and "Rat" of the early 1980s band The Varukers and the 2000s re-formation of Discharge. Sid Vicious did not take his name to seem tough but rather because he was anything but Vicious (several sources have indicated that Sid himself hated this nickname). Punk rock band The Ramones also had every member take the last name of Ramone. Rob Crow of the rock band Goblin Cock chose to go by the name "Lord Phallus" during the release of the band's albums. A similar practice occurred in hardcore with musicians taking the names of their bands, like Kevin Seconds of 7 Seconds and Ray Cappo of Youth of Today who, for a while, billed himself as Ray of Today. The Norwegian electronic duo Röyksopp's pseudonym for their "Back to Mine" album was Emmanuel Splice. Australian country musician Tex Morton was born Robert Lane but changed his name to Tex Morton in order to sound more like a cowboy.
Cultural or organizational traditions.
Age.
In many cultures, people go by several different nicknames over the course of their lives, to reflect important parts of their lives. In some cases, a rite of passage or puberty marks the transition from a "milk name" to an adult name. Enrollment in school is another occasion where a child's formal or legal name would begin to be used.
Monarchies.
In many monarchies, the sovereign is allowed to choose a regnal name. This official name may differ from their first name and may not even be one of their given names.
A sovereign may choose not to use their first name for many reasons. Some, such as George VI of the United Kingdom (born Albert Frederick Arthur George), may wish to make a connection between their reign and that of a previous sovereign (in his case, his father, George V). Others, such as Queen Victoria (born Alexandrina Victoria of Kent), may never have been known by their original first name.
In Japan, the Emperor's personal name is never used as a regnal name: he is referred to by the name of his regnal era, and after his death his name is officially changed to that of the era. It is a severe breach of etiquette in Japan to refer to the current Emperor's personal name either in speech or in writing unless absolutely required by law. This does not apply to those outside Japan, which explains why Japanese and non-Japanese use different names for the Emperor. For instance, Emperor Hirohito was known within Japan as Emperor Shōwa.
Religion.
In the tradition of various Roman Catholic religious institutes, members abandon their birth name to assume a new, often unrelated, devotional name, often referring to an admired saint. For women, for example in the Society of the Helpers of the Holy Souls, this reflects the mystical marriage as bride of Christ. Newly elected popes assume a papal name. Most popes choose a name commemorating an admired saint (Benedict XVI, for example) or a predecessor or predecessors (John Paul I), or even a family member (John XXIII).
In Eastern Orthodoxy, a monk or a nun is given a saint's name by their bishop or abbot at the time of their tonsure as the new monk's or nun's first act of monastic obedience. In addition, Orthodox monks and nuns never use their last names, except for legal reasons or for disambiguation. This may also have changed to indicate their brotherhood e.g. a monk at Kykkos Monastery in Cyprus may be known as "Κυκκότης".
In Judaism, a convert adopts a Hebrew name.
In Buddhism, a Dharma name is given during the traditional refuge ceremony.
In Islam, new converts often accept Islamic names. Examples include Muhammad Ali, formerly Cassius Clay; Ivan Aguéli, who became Abd al-Hadi Aqhili; Cat Stevens, who became Yusuf Islam; and Yousuf Youhana, who became Mohammad Yousef. Malcolm X (born Malcolm Little) adopted the Arabic name El-Hajj Malik El-Shabazz when he converted to Islam in 1964.
In Sikhism, adherents adopt the last name Singh for males or Kaur for females.
It is a long-standing tradition in the Western Occult tradition to assume a pseudonym or motto. For instance Alphonse Louis Constant wrote under the name Eliphas Levi, William Wynn Westcott wrote under Frater Sapere Aude, and Aleister Crowley wrote under the name Frater Perdurabo.
Some practitioners of Wicca adopt a "craft name" or "witch name" upon initiation for use within their community. This may be to create a name of their own choosing as opposed to their given name, or to provide anonymity to those who are in the "broom closet." Often a craft name will reflect their personality, interests or feelings.
Sexual minorities.
Members of sexual minority groups have often assumed different names to protect their identity, or to represent a different persona. "Scene names" are still common within the BDSM community, and the use of the Internet for social networking and information exchange among kinky and polyamorous people means that many are often known more by their computer "handles" than their legal names.
Cadre names.
Within Communist parties and Trotskyist organisations, noms de guerre are usually known as "party names" or "cadre names". While the practice originated during the revolutionary years after World War I, to conceal the identity of leaders, by the 1950s and 1960s, the practice was more of a tradition than an identity-concealment strategy. Some famous Communist Party names include Lenin (Vladimir Il'ich Ulyanov); Stalin (Yosif Vissarionovich Dzhugashvili); Trotsky (Lev Davidovich Bronshtein); Max (Yakov Sverdlov); Nahuel Moreno (Hugo Miguel Bressano) and Hua Guofeng (Su Zhu).
Political articles.
From the late-18th to early-19th centuries, it was established practice for political articles to be signed with pseudonyms. A well-known American was the pen name "Publius", used by Alexander Hamilton, James Madison, and John Jay, in writing "The Federalist" Papers. In his youth, Benjamin Franklin wrote a number of letters to his brother's newspaper posing as a widow under the pen name "Silence Dogood". The British political writer "Junius" was never identified but is probably Sir Philip Francis.
Other types.
Pseudonyms are also adopted for other reasons. Criminals often took on (or were given) pseudonyms, such as famed con man Jefferson R. Smith, who was known as Soapy Smith.
Comedians and others performing hoaxes often adopt aliases for their performance role. A notable instance is provided by the comedian and hoaxer Rodney Marks, who in public performances as a corporate comedian has used over one hundred different aliases indicative of the hoax features.
Mervyn's founder Mervin G. Morris was advised by an architect to spell the name of his store chain with a Y instead of an I because the signs would be more pleasing to the eye.
It is not uncommon for a pseudonym to be adopted by a racing car driver. Reasons for this may include keeping their parents or family unaware of their participation in such activities, so members of royalty (who may be otherwise prohibited from such a dangerous activity as racing) can participate, or as a way to remain in relative anonymity. Three-time F1 champion Jackie Stewart's son Paul used a pseudonym when he joined a British racing school for just this reason. Of the many instances of racing drivers assuming false names, two more are Louis Krages, who raced under the name "John Winter" to keep his mother from finding out about his "habit", and former F1 driver Jean Alesi. Alesi, born in France but of Italian descent, went by his real given name of Giovanni until teasing from classmates led him to adopting a more French first name.
Famous pseudonyms of people who were neither authors nor actors include the architect Le Corbusier (né Charles Édouard Jeanneret), and the statistician Student (né William Sealey Gosset), discoverer of Student's t-distribution in statistics (Gosset's employer prohibited publication by employees to prevent trade secrets being revealed).
When used by a radio operator, a pseudonym is called a "handle", especially in Citizens' Band radio; on the Appalachian Trail it is common to adopt or, more usually, be given by others a "trail name".
Pseudonyms should not be confused with new names that replace old ones. Some Jewish politicians adopted Hebrew family names upon making aliyah to Israel, dropping Westernized surnames that may have been in the family for generations. David Ben-Gurion, for example, was born David Grün in Poland. He adopted his Hebrew name in 1910, when he published his first article in a Zionist journal in Jerusalem. In the 1960s, black civil rights campaigner Malcolm X (né Malcolm Little) took the "X" to represent his unknown African ancestral name that was lost when his ancestors were brought to North America as slaves, and then changed his name again to Malik El-Shabazz when he converted to Islam.

</doc>
<doc id="40597" url="http://en.wikipedia.org/wiki?curid=40597" title="Alexander Hamilton">
Alexander Hamilton

Alexander Hamilton (January 11, 1755 or 1757 – July 12, 1804) was a founding father of the United States, chief staff aide to General George Washington, one of the most influential interpreters and promoters of the U.S. Constitution, the founder of the nation's financial system, and the founder of the Federalist Party, the world's first voter-based political party. As Secretary of the Treasury, Hamilton was the primary author of the economic policies of the George Washington administration. Hamilton took the lead in the funding of the states' debts by the Federal government, the establishment of a national bank, a system of tariffs, and friendly trade relations with Britain. He led the Federalist Party, created largely in support of his views; he was opposed by the Democratic-Republican Party, led by Thomas Jefferson and James Madison; it despised Britain and feared that Hamilton's policies of a strong central government would weaken the American commitment to Republicanism.
Born out of wedlock and raised in the West Indies, local wealthy men helped Hamilton get a college education after he was orphaned as a child. Recognized for his abilities and talent, he was sent to King's College (now Columbia University), in New York City. Hamilton played a major role in the American Revolutionary War. At the start of the war in 1775, he organized an artillery company. He soon became the senior aide to General Washington, the American forces' commander-in-chief. Washington sent him on numerous important missions to tell generals what Washington wanted. After the war, Hamilton was elected to the Congress of the Confederation from New York. He resigned, to practice law, and founded the Bank of New York. Hamilton was among those dissatisfied with the weak national government. He led the Annapolis Convention, which successfully influenced Congress to issue a call for the Philadelphia Convention, in order to create a new constitution. He was an active participant at Philadelphia; and he helped achieve ratification by writing 51 of the 85 installments of the "The Federalist" Papers. To this day, it is the single most important reference for Constitutional interpretation.
Hamilton became the leading cabinet member in the new government under President Washington. Hamilton was a nationalist, who emphasized strong central government and successfully argued that the implied powers of the Constitution provided the legal authority to fund the national debt, assume states' debts, and create the government-owned Bank of the United States. These programs were funded primarily by a tariff on imports, and later also by a highly controversial tax on whiskey. Facing well-organized opposition from Jefferson and Madison, Hamilton mobilized a nationwide network of friends of the government, especially bankers and businessmen. It became the Federalist Party. A major issue splitting the parties was the Jay Treaty, largely designed by Hamilton in 1794. It established friendly economic relations with Britain to the chagrin of France and the supporters of the French Revolution. Hamilton played a central role in the Federalist party, which dominated national and state politics until it lost the election of 1800 to Jefferson's Democratic Republicans.
In 1795, he returned to the practice of law in New York. He tried to control the policies of President Adams (1797–1801). In 1798 and 99, Hamilton called for mobilization against France after the XYZ Affair and became commander of a new army, which he readied for war. However, the Quasi-War, while hard-fought at sea, was never officially declared and did not involve army action. In the end, Adams found a diplomatic solution which avoided a war with France. Hamilton's opposition to Adams' re-election helped cause his defeat in the 1800 election. When Jefferson and Aaron Burr tied for the presidency in the electoral college in 1801, Hamilton helped to defeat Burr, whom he found unprincipled, and to elect Jefferson despite philosophical differences. Hamilton continued his legal and business activities in New York City, but lost much of his national prominence within the Federalist party. When Vice President Burr ran for governor of New York state in 1802, Hamilton crusaded against him as unworthy. Taking offense at some of Hamilton's comments, Burr challenged him to a duel in 1804 and mortally wounded Hamilton, who died the next day.
Hamilton's reputation over the centuries has been politicized; Republicans have praised him, and Democrats have reviled him. The partisanship has died down in the 21st century, and Hamilton is recognized as one of the key Founders of the strong national government.
Childhood in the Caribbean.
Alexander Hamilton was born in and spent part of his childhood in Charlestown, the capital of the island of Nevis, in the Leeward Islands; Nevis was one of the British West Indies. Hamilton was born out of wedlock to Rachel Faucette, a married woman of partial French Huguenot descent, and James A. Hamilton, the fourth son of the Scottish laird Alexander Hamilton of Grange, Ayrshire.
His mother moved with the young Hamilton to St. Croix in the Virgin Islands, then ruled by Denmark. It is not certain whether the year of Hamilton's birth was 1757 or 1755; most historical evidence after Hamilton's arrival in North America supports the idea that he was born in 1757, and many historians had accepted this birth date. But, Hamilton's early life in the Caribbean was recorded in documents which were first published in Danish in 1930; this evidence has caused historians since then to favor a birth year of 1755. Hamilton listed his birth year as 1757 when he first arrived in the Thirteen Colonies. He celebrated his birthday on January 11. In later life, he tended to give his age only in round figures. Probate papers from St. Croix in 1768, after the death of Hamilton's mother, list him as then 13 years old, a date that would support a birth year of 1755. Historians have posited reasons for the different dates of birth being used: If 1755 is correct, Hamilton may have been trying to appear younger than his college classmates or perhaps wished to avoid standing out as older; if 1757 is correct, the probate document indicating a birth year of 1755 may have been in error, or Hamilton may have been attempting to pass as 13, in order to be more employable after his mother's death.
Hamilton's mother had been married previously to Johann Michael Lavien of St. Croix. Rachel left her husband and first son, Peter, traveling to St. Kitts in 1750, where she met James Hamilton. Hamilton and Rachel moved together to Rachel's birthplace, Nevis, where she had inherited property from her father. The couple's two sons were James Jr. and Alexander. Because Alexander Hamilton's parents were not legally married, the Church of England denied him membership and education in the church school. Hamilton received "individual tutoring" and classes in a private school led by a Jewish headmistress. Hamilton supplemented his education with a family library of 34 books.
James Hamilton abandoned Rachel and their sons, allegedly to "spar[e] [Rachel] a charge of bigamy ... after finding out that her first husband intend[ed] to divorce her under Danish law on grounds of adultery and desertion." Thereafter, Rachel supported her children in St. Croix, keeping a small store in Christiansted. She contracted a severe fever and died on February 19, 1768, 1:02 am, leaving Hamilton orphaned. This may have had severe emotional consequences for him, even by the standards of an 18th-century childhood. In probate court, Rachel's "first husband seized her estate" and obtained the few valuables Rachel had owned, including some household silver. Many items were auctioned off, but a friend purchased the family's books and returned them to the young Hamilton.
Hamilton became a clerk at a local import-export firm, Beekman and Cruger, which traded with New England; he was left in charge of the firm for five months in 1771, while the owner was at sea. He and his older brother James Jr. were adopted briefly by a cousin, Peter Lytton; but when Lytton committed suicide, the brothers were separated. James apprenticed with a local carpenter, while Alexander was adopted by a Nevis merchant, Thomas Stevens. According to the writer Ron Chernow, some evidence suggests that Stevens may have been Alexander Hamilton's biological father; his son, Edward Stevens, became a close friend of Hamilton. The two boys were described as looking much alike, were both fluent in French, and shared similar interests.
Hamilton continued clerking, but he remained an avid reader, later developing an interest in writing, and began to desire a life outside the small island where he lived. He wrote an essay published in the "Royal Danish-American Gazette", a detailed account of a hurricane which had devastated Christiansted on August 30, 1772. His biographer says that, "Hamilton's famous letter about the storm astounds the reader for two reasons: for all its bombastic excesses, it does seem wondrous the 17-year old self-educated clerk could write with such verve and gusto. Clearly, Hamilton was highly literate and already had considerable fund of verbal riches." The essay impressed community leaders, who collected a fund to send the young Hamilton to the North American colonies for his education.
Education.
In the autumn of 1772, Hamilton arrived at Elizabethtown Academy, a grammar school in Elizabethtown, New Jersey. In 1773 he studied with Francis Barber at Elizabethtown in preparation for college work. He came under the influence of William Livingston, a leading intellectual and revolutionary, with whom he lived for a time at his Liberty Hall. Hamilton matriculated at King's College in New York City (now Columbia University) in late 1773 or early 1774. In what is credited as his first public appearance, on July 7, 1774 at the liberty pole at King's College, Hamilton's friend Robert Troup spoke glowingly of Hamilton's ability to clearly and concisely explain the rights and reasons the patriots have in their case against the British.
When the Church of England clergyman Samuel Seabury published a series of pamphlets promoting the Loyalist cause in 1774, Hamilton responded anonymously with his first political writings, "A Full Vindication of the Measures of Congress" and "The Farmer Refuted". Seabury essentially tried to provoke fear into the colonies and his main objective was to stopgap the potential of a union among the colonies. Hamilton published two additional pieces attacking the Quebec Act as well as fourteen anonymous installments of "The Monitor" for Holt's "New York Journal". Although Hamilton was a supporter of the Revolutionary cause at this prewar stage, he did not approve of mob reprisals against Loyalists. On May 10, 1775, Hamilton won credit for saving his college president Myles Cooper, a Loyalist, from an angry mob by speaking to the crowd long enough for Cooper to escape.
During the Revolutionary War.
Early military career.
In 1775, after the first engagement of American troops with the British in Boston, Hamilton joined a New York volunteer militia company called the Hearts of Oak, which included other King's College students. He drilled with the company, before classes, in the graveyard of nearby St. Paul's Chapel. Hamilton studied military history and tactics on his own and achieved the rank of lieutenant. Under fire from HMS "Asia", he led a successful raid for British cannon in the Battery, the capture of which resulted in the Hearts of Oak becoming an artillery company thereafter. Through his connections with influential New York patriots such as Alexander McDougall and John Jay, he raised the New York Provincial Company of Artillery of sixty men in 1776, and was elected captain. It took part in the campaign of 1776 around New York City, particularly at the Battle of White Plains; at the Battle of Trenton, it was stationed at the high point of town, the meeting of the present Warren and Broad Streets, to keep the Hessians pinned in the Trenton Barracks.
George Washington's staff.
Hamilton was invited to become an aide to Nathanael Greene and to Henry Knox; however, he declined these invitations, believing his best chance for improving his station in life was glory on the battlefield. Hamilton eventually received an invitation he felt he could not refuse: to serve as Washington's aide, with the rank of Lieutenant Colonel. Washington felt, "Aides de camp are persons in whom entire confidence must be placed and it requires men of abilities to execute the duties with propriety and dispatch." Hamilton served for four years as Washington's chief staff aide. He handled letters to Congress, state governors, and the most powerful generals in the Continental Army; he drafted many of Washington's orders and letters at the latter's direction; he eventually issued orders from Washington over Hamilton's own signature. Hamilton was involved in a wide variety of high-level duties, including intelligence, diplomacy, and negotiation with senior army officers as Washington's emissary.
During the war, Hamilton became close friends with several fellow officers. His letters to the Marquis de Lafayette and to John Laurens, employing the sentimental literary conventions of the late eighteenth century and alluding to Greek history and mythology, have been read by Jonathan Katz as revealing a homosocial or perhaps homosexual relationship, but few historians agree.
While on Washington's staff, Hamilton long sought command and a return to active combat. As the war drew nearer to an end, he knew that opportunities for military glory were diminishing. In February 1781, Hamilton was mildly reprimanded by Washington and used this as an excuse to resign his staff position. He asked Washington and others for a field command. This continued until early July 1781, when Hamilton submitted a letter to Washington with his commission enclosed, "thus tacitly threatening to resign if he didn't get his desired command."
On July 31, 1781, Washington relented and assigned Hamilton as commander of a New York light infantry battalion. In the planning for the assault on Yorktown, Hamilton was given command of three battalions, which were to fight in conjunction with the allied French troops in taking Redoubts No. 9 and No. 10 of the British fortifications at Yorktown. Hamilton and his battalions fought bravely and took Redoubt No. 10 with bayonets in a nighttime action, as planned. The French also fought bravely, suffered heavy casualties, and took Redoubt No. 9. These actions forced the British surrender of an entire army at Yorktown, Virginia, effectively ending their major British military operations in North America.
Congress of the Confederation.
After the Battle of Yorktown, Hamilton resigned his commission. He was appointed in July 1782 to the Congress of the Confederation as a New York representative for the term beginning in November 1782. Before his appointment to Congress in 1782, Hamilton was already sharing his criticisms of Congress. He expressed these criticisms in his letter to James Duane dated September 3, 1780. In this letter he wrote, “The fundamental defect is a want of power in Congress…the confederation itself is defective and requires to be altered; it is neither fit for war, nor peace.” While on Washington's staff, Hamilton had become frustrated with the decentralized nature of the wartime Continental Congress, particularly its dependence upon the states for voluntary financial support. Under the Articles of Confederation, Congress had no power to collect taxes or to demand money from the states. This lack of a stable source of funding had made it difficult for the Continental Army both to obtain its necessary provisions and to pay its soldiers. During the war, and for some time after, Congress obtained what funds it could from subsidies from the King of France, from aid requested from the several states (which were often unable or unwilling to contribute), and from European loans.
An amendment to the Articles had been proposed by Thomas Burke, in February 1781, to give Congress the power to collect a 5% impost, or duty on all imports, but this required ratification by all states; securing its passage as law proved impossible after it was rejected by Rhode Island in November 1782. Madison joined Hamilton in persuading Congress to send a delegation to persuade Rhode Island to change its mind. Their report recommending the delegation argued the federal government needed not just some level of financial autonomy, but also the ability to make laws that superseded those of the individual states. Hamilton transmitted a letter arguing that Congress already had the power to tax, since it had the power to fix the sums due from the several states; but Virginia's rescission of its own ratification ended the Rhode Island negotiations.
Congress and the army.
While Hamilton was in Congress, discontented soldiers began to pose a danger to the young United States. Most of the army was then posted at Newburgh, New York. Those in the army were paying for much of their own supplies, and they had not been paid in eight months. Furthermore, the Continental officers had been promised, in May 1778, after Valley Forge, a pension of half their pay when they were discharged. By the early 1780s, due to the structure of the government under the Articles of Confederation, it had no power to tax to either raise revenue or pay its soldiers. In 1782 after several months without pay, a group of officers organized sent a delegation to lobby Congress, led by Capt. Alexander MacDougall. The officers had three demands: the Army's pay, their own pensions, and commutation of those pensions into a lump-sum payment if Congress were unable to afford the half-salary pensions for life. Congress rejected the proposal.
Several Congressmen, including Hamilton, Robert Morris and Gouverneur Morris, attempted to use this Newburgh Conspiracy as leverage to secure support from the states and in Congress for funding of the national government. They encouraged MacDougall to continue his aggressive approach, threatening unknown consequences if their demands were not met, and defeated proposals that would have resolved the crisis without establishing general federal taxation: that the states assume the debt to the army, or that an impost be established dedicated to the sole purpose of paying that debt. Hamilton suggested using the Army's claims to prevail upon the states for the proposed national funding system. The Morrises and Hamilton contacted Knox to suggest he and the officers defy civil authority, at least by not disbanding if the army were not satisfied; Hamilton wrote Washington to suggest that Hamilton covertly "take direction" of the officers' efforts to secure redress, to secure continental funding but keep the army within the limits of moderation. Washington wrote Hamilton back, declining to introduce the army; after the crisis had ended, he warned of the dangers of using the army as leverage to gain support for the national funding plan.
On March 15, Washington defused the Newburgh situation by giving a speech to the officers. Congress ordered the Army officially disbanded in April 1783. In the same month, Congress passed a new measure for a twenty-five-year impost—which Hamilton voted against—that again required the consent of all the states; it also approved a commutation of the officers' pensions to five years of full pay. Rhode Island again opposed these provisions, and Hamilton's robust assertions of national prerogatives in his previous letter were widely held to be excessive.
In June 1783, a different group of disgruntled soldiers from Lancaster, Pennsylvania, sent Congress a petition demanding their back pay. When they began to march toward Philadelphia, Congress charged Hamilton and two others with intercepting the mob. Hamilton requested militia from Pennsylvania's Supreme Executive Council, but was turned down. Hamilton instructed Assistant Secretary of War William Jackson to intercept the men. Jackson was unsuccessful. The mob arrived in Philadelphia, and the soldiers proceeded to harangue Congress for their pay. The President of Congress, John Dickinson, feared that the Pennsylvania state militia was unreliable, and refused its help. Hamilton argued that Congress ought to adjourn to Princeton, New Jersey. Congress agreed, and relocated there.
Frustrated with the weakness of the central government, Hamilton while in Princeton drafted a call to revise the Articles of Confederation. This resolution contained many features of the future US Constitution, including a strong federal government with the ability to collect taxes and raise an army. It also included the separation of powers into the Executive, Legislative, and Judicial branches.
Return to New York.
Hamilton resigned from Congress, and in July 1783 was authorized to practice law in New York after several months of self-directed education. He practiced law in New York City in partnership with Richard Harison. He specialized in defending Tories and British subjects, as in "Rutgers v. Waddington", in which he defeated a claim for damages done to a brewery by the Englishmen who held it during the military occupation of New York. He pleaded for the Mayor's Court to interpret state law consistent with the 1783 Treaty of Paris which had ended the Revolutionary War.
In 1784, he founded the Bank of New York, now the oldest ongoing bank in the United States. Hamilton was one of the men who restored King's College, which had been suspended since 1776 and severely damaged during the War, as Columbia College. Long dissatisfied with the weak Articles of Confederation, he played a major leadership role at the Annapolis Convention in 1786. He drafted its resolution for a constitutional convention, and in doing so brought his longtime desire to have a more powerful, more financially independent federal government one step closer to reality.
Constitution and "The Federalist" Papers.
Constitutional Convention and ratification of the Constitution.
In 1787, Hamilton served as assemblyman from New York County in the New York State Legislature and was chosen as a delegate for the Constitutional Convention by his father-in-law Philip Schuyler. Even though Hamilton had been a leader in calling for a new Constitutional Convention, his direct influence at the Convention itself was quite limited. Governor George Clinton's faction in the New York legislature had chosen New York's other two delegates, John Lansing, Jr. and Robert Yates, and both of them opposed Hamilton's goal of a strong national government. Thus, whenever the other two members of the New York delegation were present, they decided New York's vote, to ensure that there was no major alterations to the Articles of Confederation.
Early in the Convention he made a speech proposing a President-for-Life; it had no effect upon the deliberations of the convention. He proposed to have an elected President and elected Senators who would serve for life, contingent upon "good behavior" and subject to removal for corruption or abuse; this idea contributed later to the hostile view of Hamilton as a monarchist sympathizer, held by James Madison. According to Madison’s notes, Hamilton said in regards to the executive, “The English model was the only good one on this subject. The hereditary interest of the king was so interwoven with that of the nation, and his personal emoluments so great, that he was placed above the danger of being corrupted from abroad…Let one executive be appointed for life who dares execute his powers.” Hamilton argued, “And let me observe that an executive is less dangerous to the liberties of the people when in office during life than for seven years. It may be said this constitutes as an elective monarchy…But by making the executive subject to impeachment, the term ‘monarchy’ cannot apply…” During the convention, Hamilton constructed a draft for the Constitution based on the convention debates, but he never presented it. This draft had most of the features of the actual Constitution. In this draft, the Senate was to be elected in proportion to the population, being two-fifths the size of the House, and the President and Senators were to be elected through complex multistage elections, in which chosen electors would elect smaller bodies of electors; they would hold office for life, but were removable for misconduct. The President would have an absolute veto. The Supreme Court was to have immediate jurisdiction over all law suits involving the United States, and state governors were to be appointed by the federal government.
At the end of the Convention, Hamilton was still not content with the final form of the Constitution, but signed it anyway as a vast improvement over the Articles of Confederation, and urged his fellow delegates to do so also. Since the other two members of the New York delegation, Lansing and Yates, had already withdrawn, Hamilton was the only New York signer to the United States Constitution. He then took a highly active part in the successful campaign for the document's ratification in New York in 1788, which was a crucial step in its national ratification. He first used the popularity of the Constitution by the masses to compel George Clinton to sign, but was unsuccessful. The state convention in Poughskeepsie in June 1788 pitted Hamilton, Jay, James Duane, Robert Livingston, and Richard Morris against the Clintonian faction led by Melancton Smith, Lansing, Yates, and Gilbert Livingston. Hamilton's faction were against any conditional ratification, under the impression that New York would not be accepted into the Union, while Clinton's faction wanted to amend the Constitution, while maintaining the state's right to secede if their attempts failed. During the state convention, New Hampshire and Virginia becoming the ninth and tenth states to ratify the Constitution, respectively, had ensured any adjournment would not happened and a compromised would have to be reached. Hamilton's arguments used for the ratifications were largely iterations of work from "The Federalist" Papers, and Smith eventually went for ratification, though it was more out of necessity than Hamilton's rhetoric. The vote in the state constitution was ratified 30 to 27, on 26 July 1788.
In 1788, Hamilton served yet another term in what proved to be the last session of the Continental Congress under the Articles of Confederation. When the term of Phillip Schuyler was up in 1791, elected in his place was the attorney general of New York, one Aaron Burr. Hamilton blamed Burr for this result, and ill characterizations of Burr appear in his correspondence thereafter. The two men did work together from time to time thereafter on various projects, including Hamilton's army of 1798 and the Manhattan Water Company.
"The Federalist" Papers.
Hamilton recruited John Jay and James Madison to write a series of essays defending the proposed Constitution, now known as "The Federalist" Papers, and made the largest contribution to that effort, writing 51 of 85 essays published (Madison wrote 29, Jay only five). Hamilton supervised the entire project, enlisted the participants, wrote the majority of the essays, and oversaw the publication. During the project each person was responsible for their areas of expertise; Jay covered foreign relations, Madison covered the history of republics and confederacies, along with the anatomy of the new government and Hamilton covered the branches of government most pertinent to him: the executive and judicial branches, with some aspects of the Senate, as well as covering military matters and taxation. The papers first appeared in "The Independent Journal" in October 27, 1787.
Hamilton wrote the first paper signed as Publius, and all of the subsequent papers were signed under the name. Jay wrote the next four papers to elaborate on the confederation's weakness and the necessary for unity against foreign aggression and splitting into rival confederacies, and except for Number 64, was not further involved. Hamilton's highlights included discussion that although republics have been culpable for disorders in the past, advances in the "science of politics" had fostered principles that ensured that those abuses could be prevented, such as the division of powers, legislative checks and balances, an independent judiciary, and legislators that were represented by electors Numbers 7–9). Hamilton also wrote an extensive defense of the constitution (No. 23–36), and discussed the Senate and executive and judicial branches in Numbers 65–85. Hamilton and Madison worked to describe the anarchic state of the confederation in numbers 15–22, and have been described as not being entirely different in thought during this time period in contrast to their stark opposition later in life. Subtle differences appeared with the two when discussing the necessity of standing armies.
Secretary of the Treasury.
 Bureau of Engraving and Printing portrait of Hamilton as Secretary of the Treasury.
President George Washington appointed Hamilton as the first United States Secretary of the Treasury on September 11, 1789. He left office on the last day of January 1795. Much of the structure of the government of the United States was worked out in those five years, beginning with the structure and function of the cabinet itself. Biographer Forrest McDonald argues that Hamilton saw his office, like that of the British First Lord of the Treasury, as the equivalent of a Prime Minister; Hamilton would oversee his colleagues under the elective reign of George Washington. Washington did request Hamilton's advice and assistance on matters outside the purview of the Treasury Department. In 1791 while Secretary, Hamilton was elected a Fellow of the American Academy of Arts and Sciences in 1791. Hamilton submitted various financial reports to Congress. Among these are the First Report on the Public Credit, Operations of the Act Laying Duties on Imports, Report on a National Bank, On the Establishment of a Mint, Report on Manufactures, and the Report on a Plan for the Further Support of Public Credit. So, the great enterprise in Hamilton's project of an administrative republic is the establishment of stability.
Report on Public Credit.
Before the adjournment of the House in September 1789, they requested Hamilton to make a report on suggestions to improve the public credit by January 1790. Hamilton had written to Robert Morris as early as 1781 that fixing the public credit will win their objective of independence. The sources that Hamilton used ranged from Frenchmen such as Jacques Necker and Montesquieu to British writers such as Hume, Hobbes, and Malachy Postlethwayt. While writing the report he also sought out suggestions from contemporaries such as John Knox Witherspoon, and Madison. Although they agreed on additional taxes such as distilleries and duties on imported liquors and land taxes, Madison feared that the securities from the government debt would fall in foreign hands.
In the report, Hamilton felt that the debt that the United States had accrued during the Revolutionary War was the price it paid for its liberty. He argued that liberty and property security were inseparable and that the government should honor the contracts, as they formed the basis of public and private morality. To Hamilton, the proper handling of the government debt would also allow America to borrow at affordable interest rates and would also be a stimulant to the economy. Hamilton divided the debt into national and state, and further divided the national debt into foreign and domestic debt. While there was agreement on how to handle the foreign debt (especially with France), there was not with regards to the national debt held by domestic creditors. During the Revolutionary War, affluent citizens had invested in bonds, and war veterans had been paid with promissory notes and IOUs that plummeted in price during the Confederation. In response, the war veterans sold the securities to speculators for as little as fifteen to twenty cents on the dollar. Hamilton felt the money from the bonds should not go to the soldiers, but the speculators that had bought the bonds from the soldiers, as they had little faith in the country's future. The process of attempting to track down the original bond holders along with the government showing discrimination among the classes of holders if the war veterans were to be compensated also weighed in as factors for Hamilton. As for the state debts, Hamilton suggested to consolidate it with the national debt and label it as federal debt, for the sake of efficiency on a national scale. The last portion of the report dealt with eliminating the debt by utilizing a sinking fund that would retire five percent of the debt annually until it was paid off. Due to the bonds being traded well below their face value, the purchases would benefit the government as the securities rose in price.
When the report was submitted to the House of Representatives, detractors soon began to speak against it. The notion of programs that resembled British practice were wicked along with the power of balance being shifted away from the Representatives to the executive branch were some of the prejudices that resided within the House. William Maclay suspected that that several congressmen were involved in government securities, saw Congress in an unholy league with New York speculators. Congressman James Jackson also spoke against New York with allegations of speculators attempting to swindle those who had yet heard about Hamilton's report. The involvement of those in Hamilton's circle such as Schuyler, William Duer, James Duane, Gouverneur Morris, and Rufus King as speculators was not favorable to those against the report, either, though Hamilton personally did not own or deal a share in the debt. Madison eventually spoke against it by February 1790. Although he was not against current holders of government debt to profit, he wanted the windfall to go to the original holders. Madison did not feel that the original holders had lost faith in the government, but sold their securities out of desperation. The compromise was seen as egregious to both Hamiltonians and their dissidents such as Maclay, and Madison's vote was defeated 36 votes to 13 on February 22.
The fight for the national government to assume state debt was a longer issue, and lasted over four months. During the period, the resources that Hamilton was to apply to the payment of state debts was requested by Alexander White, and was rejected due to Hamilton's not being able to prepare information by March 3, and was even postponed by his own supporters in spite of configuring a report the next day (which consisted of a series of additional duties to meet the interest on the state debts). Some of the other issues involving Hamilton was bypassing the rising issue of Slavery in the United States in Congress after Quakers petitioned for its abolition (though he returned to the issue the following year), having Duer resign as Assistant Secretary of the Treasury, and the vote of assumption being voted down 31 votes to 29 on April 12. The temporary location of the capital from New York City also played a role, as Tench Coxe was sent to speak to Maclay to bargain about the capital being temporarily located to Philadelphia, as a single vote in the Senate was needed and five in the House for the bill to pass. The bill passed in the Senate on July 21 and in the House 34 votes to 28 on July 26, 1790.
Report on a National Bank.
Hamilton's Report on a National Bank was a projection from the first Report on the Public Credit. Although Hamilton had been forming ideas of a national bank as early as 1779, he gathered ideas in various ways over the past eleven years. These included theories from Adam Smith, extensive studies on the Bank of England, the blunders of the Bank of North America and his experience in establishing the Bank of New York. His also used American records from James Wilson, Pelatiah Webster, Gouverneur Morris, and from his assistant Treasury secretary Tench Coxe.
Hamilton suggested that Congress should charter the National Bank with a capitalization of $10 million, one-fifth of which would be handled by the Government. Since the Government did not have the money, it would borrow the money from the bank itself, and repay the loan in ten even annual installments. The rest was to be available to individual investors. The bank was to be governed by a twenty-five member board of directors that was to represent a large majority of the private shareholders, which Hamilton considered essential for his being under a private direction. Hamilton's bank model had many similarities to that of the Bank of England, except Hamilton wanted to exclude the Government from being involved in public debt, but provide a large, firm, and elastic money supply for the functioning of normal businesses and usual economic development, among other differences. For tax revenue to ignite the bank, it was the same as he had previously proposed; increases on imported spirits: rum, liquor, and whiskey.
The bill passed through the Senate practically without a problem, but objections of the proposal increased by the time it reached the House of Representatives. It was generally held by critics that Hamilton was serving the interests of the Northeast by means of the bank, and those of the agrarian lifestyle would not benefit from it. Among those critics was James Jackson of Georgia, who also attempted to refute the report by quoting from "The Federalist" Papers. Madison and Jefferson also opposed the bank bill; however, the potential of the capital not being moved to the Potomac if the bank was to have a firm establishment in Philadelphia (the current capital of the United States) was a more significant reason, and actions that Pennsylvania members of Congress took to keep the capital there made both men anxious. Madison warned the Pennsylvania congress members that he would attack the bill as unconstitutional in the House, and followed up on his threat. Madison argued his case of where the power of a bank could be established within the Constitution, but he failed to sway members of the House, and his authority on the constitution was questioned by a few members. The bill eventually passed in an overwhelming fashion 39 to 20, on February 8, 1791.
Washington hesitated to sign the bill, as he received suggestions from Attorney-General Edmund Randolph and Thomas Jefferson. Jefferson dismissed the 'necessary and proper' clause as reasoning for the creation of a national bank, stating that the enumerated powers "can all be carried into execution without a bank." Along with Randolph and Jefferson's objections, Washington's involvement in the movement of the capital from Philadelphia is also thought to be a reason for his hesitation. In response to the objection of the 'necessary and proper' clause, Hamilton stated that "Necessary often means no more than needful, requisite, incidental, useful, or conductive to", and the bank was a "convenient species of medium in which they (taxes) are to be paid.". Washington would eventually sign the bill into law.
Establishing the U.S. Mint.
In 1791, Hamilton submitted Report on the Establishment of a Mint to the House of Representatives. Most of Hamilton's ideas for this report were from European economists, resolutions from Continental Congress meetings from 1785 and 1786, and from people such as Gouverneur Morris and Thomas Jefferson. Due to the Spanish coin being the most circulated coin in the United States at the time, Alexander Hamilton proposed that the minting of the United States dollar weighing almost as much as the Spanish peso would be the simplest way to introduce a national currency. Hamilton wanted the U.S. dollar system to be set for decimals rather than the eights like the Spanish mint. In spite of preferring a monometallic gold standard, he issued a bimetallic currency at ratio that was to be similar to most European countries. What was different from the European currencies was his desire to overprice the gold on the grounds that the United States would always receive an influx of silver from the West Indies. Hamilton desired the minting of small value coins such as silver ten-cent, copper, and half-cent pieces, for reducing the cost of living for the poor. One of his main objectives was for the general public to become accustomed to handling money on a frequent basis.
By 1792, Hamilton's principles were adopted by Congress, resulting in the Coinage Act of 1792, and the creation of the United States Mint. There was to be a ten dollar Gold Eagle coin, a silver dollar, and fractional money ranging from one-half to fifty cents. The coining of silver and gold was issued by 1795.
Revenue Cutter Service.
Smuggling off American coasts was an issue before the Revolutionary War, and after the Revolution it was more problematic. Along with smuggling, lack of shipping control, pirating, and a revenue unbalance were also major problems. In response, Hamilton proposed to Congress to enact a naval police force called revenue cutters in order to patrol the waters and assist the custom collectors with confiscating contraband. This idea was also proposed to assist in tariff controlling, boosting the American economy, and promote the merchant marine. It is thought that his experience obtained during his apprenticeship with Nicholas Kruger was influential in his decision-making.
Concerning some of the details of the "System of Cutters", Hamilton wanted the first ten cutters in different areas in the United States, from New England to Georgia. Hamilton also wanted those cutters to be armed with ten muskets and bayonets, twenty pistols, two chisels, one broad-ax and two lanterns. Hamilton also wanted the fabric of the sails to be domestically manufactured. Hamilton was also concerned of the employees' food supply and etiquette when boarding ships, and made provisions for each. Congress established the Revenue Cutter Service on August 4, 1790, which is viewed as the birth of the United States Coast Guard.
Whiskey as tax revenue.
One of the principal sources of revenue Hamilton prevailed upon Congress to approve was an excise tax on whiskey. In his first Tariff Bill in January of 1790, Hamilton proposed to raise the three million dollars needed to pay for government operating expenses and interest on domestic and foreign debts by means of an increase on duties on imported wines, distilled spirits, tea, coffee, and domestic spirits. It failed, with Congress complying with most recommendations excluding the excise tax on Whiskey (Madison's tariff of the same year was a modification of Hamilton's that involved only imported duties and was passed in September).
In response of diversifying revenues, as three-fourths of revenue gathered was from commerce with Great Britain, Hamilton attempted once again during his "Report on Public Credit" when presenting it in 1790 to implement an excise tax both imported and domestic spirits. The taxation rate was graduated in proportion to the whiskey proof, and Hamilton intended to equalize the tax burden on imported spirits with imported and domestic liquor. In lieu of the excise on production citizens could pay 60 cents by the gallon of dispensing capacity, along with an exemption on small stills used exclusively for domestic consumption. He realized the loathing that the tax would receive in rural areas, but thought of the taxing of spirits more reasonable than land taxes.
Opposition initially came from Pennsylvania's House of Representatives protesting the tax. William Maclay had noted that not even the Pennsylvanian legislators had been able to enforce excise taxes in the western regions of the state. Hamilton was aware of the potential difficulties and proposed inspectors the ability to search buildings that distillers were designated to store their spirits, and would be able to search suspected illegal storage facilities to confiscate contraband with a warrant. Although the inspectors were not allowed to search houses and warehouses, they were to visit twice a day and file weekly reports in extensive detail. Hamilton cautioned against expedited judicial means, and favored a jury trial with potential offenders. As soon as 1791 locals began to shun or threaten inspectors, as they felt the inspection methods were intrusive. Inspectors were also tarred and feathered, blindfolded, and whipped. Hamilton had attempted to appease the opposition with lowered tax rates, but it did not suffice.
Strong opposition to the whiskey tax by cottage producers in remote, rural regions erupted into the Whiskey Rebellion in 1794; in Western Pennsylvania and western Virginia, whiskey was the basic export product and was fundamental to the local economy. In response to the rebellion, believing compliance with the laws was vital to the establishment of federal authority, Hamilton accompanied to the rebellion's site President Washington, General Henry "Light Horse Harry" Lee, and more federal troops than were ever assembled in one place during the Revolution. This overwhelming display of force intimidated the leaders of the insurrection, ending the rebellion virtually without bloodshed.
Manufacturing and industry.
Hamilton's next report was his "Report on Manufactures". Although he was requested by Congress on January 15, 1790 for a report for manufacturing that would expand the United States' independence, the report was not submitted until December 5, 1791. In the report, Hamilton quoted from "Wealth of Nations" and used the French physiocrats as an example for rejecting agrarianism and the physiocratic theory; respectively. Hamilton also refuted Smith's ideas of government noninterference, as it would have been detrimental for trade with other countries. Hamilton also thought of the United States being a primarily agrarian country would be a disadvantage in dealing with Europe. In response to the agrarian detractors, Hamilton stated that the agriculturists' interest would be advanced by manufactures, and that agriculture was just as productive as manufacturing.
Among the ways that the government could assist in manufacturing, Hamilton mentioned levying protective duties on imported foreign goods that were also manufactured in the United States, to withdraw duties levied on raw materials needed for domestic manufacturing, pecuniary boundaries, and encouraging immigration for people to better themselves in similar employment opportunities. Congress shelved the report without much debate (except for Madison's objection to Hamilton's formulation of the General Welfare clause, which Hamilton construed liberally as a legal basis for his extensive programs).
Subsequently in 1791, with his ideas for manufacturing being a major influence, Hamilton, along with Coxe and several entrepreneurs from New York and Philadelphia helped form the Society for the Establishment of Useful Manufactures, a private industrial corporation. The location at Great Falls of the Passaic River in New Jersey was selected due to access to raw materials, it being densely inhabited, and having access to water power from the falls of the Passaic. The factory town was named Paterson after New Jersey's Governor William Paterson, who signed the charter. The profits were to derive from specific corporates rather than the benefits to be conferred to the nation and the citizens, which was unlike the report. Hamilton also suggested the first stock to be offered at $500,000 and to eventually increase to $1 million, and welcomed state and national government subscriptions alike. The company never materialized any success as numerous shareholders reneged on stock payments, some of the Society's members were soon bankrupt, while William Duer, the governor of the program, wounded up in debtors' prison. In spite of Hamilton's efforts to mend the disaster, the company would expire by 1796.
Emergence of parties.
During Hamilton's tenure as Treasury Secretary, political factions began to emerge. A Congressional caucus, led by James Madison and William Branch Giles, began as an opposition group to Hamilton's financial programs, and Thomas Jefferson joined this group when he returned from France. Hamilton and his allies began to call themselves "Federalists". The opposition group, now called the Democratic-Republican Party by political scientists, was at the time known as "Republicans".
Hamilton assembled a nationwide coalition to garner support for the Administration, including the expansive financial programs Hamilton had made Administration policy and especially the president's policy of neutrality in the European war between Britain and France. Hamilton's public relations campaign attacked the French minister Edmond-Charles Genêt (he called himself "Citizen Genêt") who tried to appeal to voters directly, which Federalists denounced as foreign interference in American affairs. If Hamilton's administrative republic was to succeed, Americans had to see themselves as nation citizens, and they would have to experience an administration that proved firm, less a threat and more an aid to the conceptions of government rooted in interest that prevailed within the United States Constitution. The Federalists did impose some internal direct taxes but they departed from the most implications of the Hamilton administrative republic as risky.
The Jeffersonian Republicans opposed banks and cities, and favored France. They built their own national coalition to oppose the Federalists. Both sides gained the support of local political factions; each side developed its own partisan newspapers. Noah Webster, John Fenno, and William Cobbett were energetic editors for the Federalists; Benjamin Franklin Bache and Philip Freneau were fiery Republican editors. All the newspapers were characterized by intense personal attacks, major exaggerations and invented claims. In 1801, Hamilton established a daily newspaper, the "New York Evening Post" and brought in William Coleman as editor. It is still publishing (as the "New York Post").
The quarrel between Hamilton and Jefferson is the best known and historically the most important in American political history. Hamilton's and Jefferson's incompatibility of was heightened by the unavowed wish of each to be Washington’s principal and most trusted advisor”.
Jay Treaty and Britain.
When France and Britain went to war in early 1793, all four members of the Cabinet were consulted on what to do. They and Washington unanimously agreed to remain neutral, and to send Genêt home. However, in 1794 policy toward Britain became a major point of contention between the two parties. Hamilton and the Federalists wished for more trade with Britain, the new nation's largest trading partner. The Republicans saw Britain as the main threat to republicanism and proposed instead a trade war.
To avoid war, Washington sent Chief Justice John Jay to negotiate with the British; Hamilton largely wrote Jay's instructions. The result was Jay's Treaty. It was denounced by the Republicans but Hamilton mobilized support up and down the land. The Jay Treaty passed the Senate in 1795 by exactly the required two-thirds majority. The Treaty resolved issues remaining from the Revolution, averted war, and made possible ten years of peaceful trade between the United States and Britain. Historian George Herring notes the "remarkable and fortuitous economic and diplomatic gains" produced by the Treaty.
Several European nations had formed a League of Armed Neutrality against incursions on their neutral rights; the Cabinet was also consulted on whether the United States should join it, and decided not to. It kept that decision secret, but Hamilton revealed it in private to George Hammond, the British Minister to the United States, without telling Jay or anyone else. (His act remained unknown until Hammond's dispatches were read in the 1920s). This "amazing revelation" may have had limited effect on the negotiations; Jay did threaten to join the League at one point, but the British had other reasons not to view the League as a serious threat.
Second Report on Public Credit.
Before leaving his post in 1795, Hamilton submitted "Report on a Plan for the Further Support of Public Credit" to Congress to curb the debt problem. Hamilton grew dissatisfied with what he viewed as a lack of a comprehensive plan to fix the public debt. He wished to have new taxes passed with older ones made permanent and stated that the any surplus from the excise tax on liquor would be pledged to lower public debt. His proposals were included into a bill by Congress within slightly over a month after his departure as treasury secretary.
Post-Secretary years.
The Reynolds affair.
In 1791, Hamilton became involved in an affair with Maria Reynolds over a nine-month period that would be revealed to the public several years afterward. Reynolds appeared to Hamilton as a woman who had been abandoned by her husband, James, at New York and wished to return to there. Hamilton did not have any money on his person, so he retrieved her address in order to deliver the funds in person. After the brief dialogue in Reynold's bedroom, he had frequent meetings with her. Hamilton then received two letters on December 15, 1791, one from both Mr. and Mrs. Reynolds. The first letter was Maria warning of her husband's knowledge and of James attempting to blackmail Hamilton. By this point Hamilton contemplated ending the tryst, and briefly ceased to visit, but both apparently were involved in the blackmailing scheme as both sent letters, and at one point James Reynolds requested to 'befriend' her. By May of 1792, James Reynolds had requested for Hamilton to no longer see his wife, but not before receiving fifty and two hundred dollars out of over $1300 in blackmail. Hamilton possibly was aware of both Reynolds' being involved before the blackmailing indicent.
When under suspicion of illegal actions while Secretary of Treasury by associating with William Duer from John J. Beckley and Jacob Clingman, the latter also had alleged evidence of James Reynolds being an agent of Hamilton's, with accompanying letters gathered from Maria Reynolds that were from Hamilton. This information was relayed to James Monroe, who consulted with Congressmen Muhlenberg and Venable on what actions to take. When it was suggested by Clingman that James Reynolds had evidence that would incriminate Hamilton, after both were arrested for counterfeiting and Clingman was released, Monroe and the Congressmen soon confronted Hamilton on 15 December 1792. After Hamilton discussed the affair, the trio were to keep the documents privately with the utmost confidence.
In 1797, however, when James T. Callender published "A History of the United States for the Year 1796", it contained accusations of James Reynolds being an agent of Hamilton using documents from the confrontation on December 15, 1792. On July 5, 1797 he wrote to all three men to confirm that there was nothing that would damage the perception of his integrity while Secretary of Treasury. All complied but Monroe, and the two almost resorted to a duel. When Hamilton did not obtain an explicit response from Monroe, he published a pamphlet in order to preserve his public reputation, and discussed the affair in exquisite detail. His wife forgave him, but not Monroe. Though he faced ridicule from the Democratic-Republican faction, he maintained his availability for public service.
1796 presidential election.
Hamilton's resignation as Secretary of the Treasury in 1795 did not remove him from public life. With the resumption of his law practice, he remained close to Washington as an advisor and friend. Hamilton influenced Washington in the composition of his Farewell Address by writing drafts for Washington to compare with the latter's draft, although when Washington contemplated retirement in 1792, he had consulted James Madison for a draft that was used in a similar manner to Hamilton's.
In the election of 1796, under the Constitution as it stood then, each of the presidential electors had two votes, which they were to cast for different men. The one who received most votes would become President, the second-most, Vice President. This system was not designed with the operation of parties in mind, as they had been thought disreputable and factious. The Federalists planned to deal with this by having all their Electors vote for John Adams, the Vice President, and all but a few for Thomas Pinckney of South Carolina.
Adams resented Hamilton's influence with Washington and considered him overambitious and scandalous in his private life; Hamilton compared Adams unfavorably with Washington and thought him too emotionally unstable to be President. Hamilton took the election as an opportunity: he urged all the northern electors to vote for Adams and Pinckney, lest Jefferson get in; but he cooperated with Edward Rutledge to have South Carolina's electors vote for Jefferson and Pinckney. If all this worked, Pinckney would have more votes than Adams, Pinckney would become President, and Adams would remain Vice President, but it did not work. The Federalists found out about it (even the French minister to the United States knew), and northern Federalists voted for Adams but "not" for Pinckney, in sufficient numbers that Pinckney came in third and Jefferson became Vice President. Adams resented the intrigue since he felt his service to the nation was much more extensive than Pinckney's.
Quasi-War.
During the Quasi-War of 1798–1800, and with Washington's strong endorsement, Adams reluctantly appointed Hamilton a major general of the army; at Washington's insistence, Hamilton was made the senior major general, prompting Henry Knox to decline appointment to serve as Hamilton's junior (Knox had been a major general in the Continental Army and thought it would be degrading to serve beneath him). Hamilton served as inspector general of the United States Army from July 18, 1798, to June 15, 1800; because Washington was unwilling to leave Mount Vernon unless it were to command an army in the field, Hamilton was the "de facto" head of the army, to Adams's considerable displeasure. If full-scale war broke out with France, Hamilton argued that the army should conquer the North American colonies of France's ally, Spain, bordering the United States.
To fund this army, Hamilton wrote regularly to Oliver Wolcott, Jr., his successor at the Treasury; William Loughton Smith, of the House Ways and Means Committee; and Senator Theodore Sedgwick of Massachusetts. He directed them to pass a direct tax to fund the war. Smith resigned in July 1797, as Hamilton scolded him for slowness, and told Wolcott to tax houses instead of land.
The eventual program included a Stamp Act like that of the British before the Revolution and other taxes on land, houses, and slaves, calculated at different rates in different states, and requiring difficult and intricate assessment of houses. This provoked resistance in southeastern Pennsylvania, led primarily by men such as John Fries who had marched with Washington against the Whiskey Rebellion.
Hamilton aided in all areas of the army's development, and after Washington's death he was by default the Senior Officer of the United States Army from December 14, 1799, to June 15, 1800. The army was to guard against invasion from France. Adams, however, derailed all plans for war by opening negotiations with France. Adams had held it proper to retain the members of Washington's cabinet, except for cause; he found, in 1800 (after Washington's death), that they were obeying Hamilton rather than himself, and fired several of them.
1800 presidential election.
In the 1800 election, Hamilton worked to defeat not only the rival Democratic-Republican candidates, but also his party's own nominee, John Adams. In November 1799, the Alien and Sedition Acts had left one Democratic-Republican newspaper functioning in New York City; when the last, the "New Daily Advertiser", reprinted an article saying that Hamilton had attempted to purchase the Philadelphia "Aurora" and close it down, Hamilton had the publisher prosecuted for seditious libel, and the prosecution compelled the owner to close the paper.
Aaron Burr had won New York for Jefferson in May; now Hamilton proposed a rerun of the election under different rules—with carefully drawn districts and each choosing an elector—such that the Federalists would split the electoral vote of New York. (John Jay, a Federalist who had given up the Supreme Court to be Governor of New York, wrote on the back of the letter the words, "Proposing a measure for party purposes which it would not become me to adopt," and declined to reply.)
John Adams was running this time with Charles Cotesworth Pinckney of South Carolina. Hamilton now toured New England, again urging northern electors to hold firm for Pinckney in the renewed hope of making Pinckney president; and he again intrigued in South Carolina. Hamilton's ideas involved coaxing middle-state Federalists to assert their non-support for Adams if there was no support for Pinckney and writing to more of the modest supports of Adams concerning his supposed misconduct while president. Hamilton expected to see southern states such as the Carolinas cast their votes for Pinckney and Jefferson, and would result in the former being ahead of both Adams and Jefferson.
In accordance with the second of the aforementioned plans, and a recent personal rift with Adams, Hamilton wrote a pamphlet called "Letter from Alexander Hamilton, Concerning the Public Conduct and Character of John Adams, Esq. President of the United States" that was highly critical of him, though it closed with a tepid endorsement. He mailed this to two hundred leading Federalists; when a copy fell into the Democratic-Republicans' hands, they printed it. This hurt Adams's 1800 reelection campaign and split the Federalist Party, virtually assuring the victory of the Democratic-Republican Party, led by Jefferson, in the election of 1800; it destroyed Hamilton's position among the Federalists.
Jefferson had beaten Adams, but both he and his running mate, Aaron Burr, had received 73 votes in the Electoral College (Adams finished in third place, Pinckney in fourth, and Jay received one vote). With Jefferson and Burr tied, the United States House of Representatives had to choose between the two men. Several Federalists who opposed Jefferson supported Burr, and for the first 35 ballots, Jefferson was denied a majority. Before the 36th ballot, Hamilton threw his weight behind Jefferson, supporting the arrangement reached by James A. Bayard of Delaware, in which five Federalist Representatives from Maryland and Vermont abstained from voting, allowing those states' delegations to go for Jefferson, ending the impasse and electing Jefferson President rather than Burr. Even though Hamilton did not like Jefferson and disagreed with him on many issues, he viewed Jefferson as the lesser of two evils. Hamilton spoke of Jefferson as being "by far not so a dangerous man", and that Burr was a "mischievous enemy" to the principle measure of the past administration. There is strong circumstantial evidence, however, that what Hamilton really feared was Burr's appeal to the members of the Federalist Party and loss of his control over them. Many Federalists viewed Burr as a moderate who was willing to dialogue with them. It was for that reason, along with the fact that Burr was a northerner and not a Virginian, that many Federalist Representatives voted for him. Hamilton wrote an exceeding number of letters to friends in congress to Convince the members too see otherwise. However, the Federalists rejected Hamilton's diatribe as reasons to not vote for Burr. Nevertheless, Burr would become Vice President of the United States. When it became clear that Jefferson developed his own concerns about Burr and would not support his return to the Vice Presidency, Burr sought the New York governorship in 1804 with Federalist support, against the Jeffersonian Morgan Lewis, but was defeated by forces including Hamilton.
Burr–Hamilton duel.
Soon after the 1804 gubernatorial election in New York—in which Morgan Lewis, greatly assisted by Hamilton, defeated Aaron Burr—the "Albany Register" published Charles D. Cooper's letters, citing Hamilton's opposition to Burr and alleging that Hamilton had expressed "a still more despicable opinion" of the Vice President at an upstate New York dinner party. Cooper claimed that the letter was intercepted after relaying the information, but stated he was 'unusually cautious' in recollecting the information from the dinner. Burr, sensing an attack on his honor, and recovering from his defeat, demanded an apology in letter form. Hamilton wrote a letter in response and ultimately refused because he could not recall the instance of insulting Burr; also, Hamilton would have been accused of recanting Cooper's letter out of cowardice After a series of attempts to reconcile were to no avail, the duel was accepted through liaisons on June 27, 1804.
The night before the duel, Hamilton wrote a defense of his decision to duel. Hamilton viewed his roles of being a father and husband, putting his creditors at risk, placing his family's welfare in jeopardy and his moral and religious stances as reasons not to duel, but he felt it impossible to avoid due to making attacks on Burr and unable to recant, and because of Burr's behavior prior to the duel. He attempted to reconcile his moral and religious reasons and the codes of honor and politics. He intended to accept the duel and throw his fire in order to satisfy his morals and political codes, respectively. His desire to be available for future political matters also played a factor.
The duel began at dawn on July 11, 1804, along the west bank of the Hudson River on a rocky ledge in Weehawken, New Jersey. After the seconds measured the paces, Hamilton, according to William P. Van Ness and Burr, raised his pistol 'as if to try the light' and had to wear his spectacles to prevent his vision from being obscured. Hamilton also refused the hairspring set of dueling pistols (that would make the pulling of the trigger lighter) when offered by Nathaniel Pendleton. Vice President Burr shot Hamilton, delivering what proved to be a fatal wound. Hamilton's shot broke a tree branch directly above Burr's head. Neither of the seconds, Pendleton or Van Ness, could determine who fired first, as each claimed that the other man had fired first. Soon after, they measured and triangulated the shooting, but could not determine from which angle Hamilton fired. Burr's shot, however, hit Hamilton in the lower abdomen above the right hip. The bullet ricocheted off Hamilton's second or third false rib, fracturing it and caused considerable damage to his internal organs, particularly his liver and diaphragm before becoming lodged in his first or second lumbar vertebra. Biographer Ron Chernow considers the circumstances to indicate that Burr fired second, after having taken deliberate aim., while biographer James Earnest Cooke suggested that Burr took careful aim and only after the bullet struck Hamilton did fire his shot while falling.
The paralyzed Hamilton, who knew himself to be mortally wounded, was ferried to the Greenwich Village home of his friend William Bayard Jr., who had been waiting on the dock. After final visits from his family and friends and considerable suffering, Hamilton died on the following afternoon, July 12, 1804 at Bayard's home at what is now 80–82 Jane Street. Gouverneur Morris gave the eulogy at his funeral and secretly established a fund to support his widow and children. Hamilton was buried in the Trinity Churchyard Cemetery in Manhattan.
Personal life.
Family.
While Hamilton was stationed in Morristown, New Jersey in the winter of 1779 and 1780, he met Elizabeth Schuyler, a daughter of Philip Schuyler and Catherine Van Rensselaer. The two were married on December 14, 1780 at the Schuyler Mansion in Albany, New York. He and Elizabeth had eight children, including two named Phillip. The elder Philip, Hamilton's first child (born January 22, 1782), was killed in 1801 in a duel with George I. Eacker, whom he had publicly insulted in a Manhattan theater. The second Philip, Hamilton's last child, was born on June 2, 1802, right after the first Philip was killed. Their other children were Angelica, born September 25, 1784; Alexander, born May 16, 1786; James Alexander (April 14, 1788 – September 1878); John Church, born August 22, 1792; William Stephen, born August 4, 1797; and Eliza, born November 26, 1799.
Hamilton was also close to Elizabeth's older sister, Angelica, who eloped with John Barker Church, an Englishman who made a fortune in North America during the Revolution. She returned with Church to London after the war, where she later became a joint friend of Maria Cosway and Thomas Jefferson.
Hamilton's religion.
Hamilton, as a youth in the West Indies, was an orthodox and conventional Presbyterian of the "New Light" evangelical type (as opposed to the "Old Light" Calvinists); he was being taught by a student of John Witherspoon, a moderate of the New School. He wrote two or three hymns, which were published in the local newspaper. Robert Troup, his college roommate, noted that Hamilton was "in the habit of praying on his knees night and morning."
Gordon Wood says that Hamilton dropped his youthful religiosity during the Revolution and became, "a conventional liberal with theistic inclinations who was an irregular churchgoer at best"; however, he returned to religion in his last years. Chernow says he was nominally an Episcopalian but:
Hamilton made jokes about God at the Constitutional Convention. During the French Revolution, he displayed an "opportunistic religiosity", using Christianity for political ends and insisting that Christianity and Jefferson's democracy were incompatible. After 1801, Hamilton further asserted the truth of Christianity; he proposed a Christian Constitutional Society in 1802, to take hold of "some strong feeling of the mind" to elect ""fit" men" to office, and he wrote of "Christian welfare societies" for the poor. He was not a member of any denomination. After being shot, Hamilton spoke of his belief in God's mercy, and of his desire to renounce dueling; Bishop Moore administered communion to Hamilton.
Hamilton had always had respect for Jews. His birthplace of Charlestown had a large Jewish population with whom Hamilton came into contact on a regular basis. As a boy, he had learned Hebrew and could recite the Ten Commandments in its original language. He believed that Jewish achievement was a result of divine providence and warned that those who discredit the Jews "destroy the Christian religion."
Legacy.
Hamilton's interpretations of the Constitution set forth in the "Federalist Papers" remain highly influential, as seen in scholarly studies and court decisions.
Though the Constitution was ambiguous as to the exact balance of power between national and state governments, Hamilton consistently took the side of greater federal power at the expense of the states. As Secretary of the Treasury, he established—against the intense opposition of Secretary of State Jefferson—the country's first national bank. Hamilton justified the creation of this bank, and other increased federal powers, under Congress's constitutional powers to issue currency, to regulate interstate commerce, and to do anything else that would be "necessary and proper" to enact the provisions of the Constitution. Jefferson, on the other hand, took a stricter view of the Constitution: parsing the text carefully, he found no specific authorization for a national bank. This controversy was eventually settled by the Supreme Court of the United States in "McCulloch v. Maryland", which in essence adopted Hamilton's view, granting the federal government broad freedom to select the best means to execute its constitutionally enumerated powers, specifically the doctrine of implied powers. Though, the American Civil War and the Progressive Era demonstrated the sorts of crises and politics Hamilton's administrative republic sought to avoid.
Hamilton's policies as Secretary of the Treasury greatly affected the United States government and still continue to influence it. His constitutional interpretation, specifically of the Necessary and Proper Clause, set precedents for federal authority that are still used by the courts and are considered an authority on constitutional interpretation. The prominent French diplomat Charles Maurice de Talleyrand, who spent 1794 in the United States, wrote, "I consider Napoleon, Fox, and Hamilton the three greatest men of our epoch, and if I were forced to decide between the three, I would give without hesitation the first place to Hamilton", adding that Hamilton had intuited the problems of European conservatives.
Opinions of Hamilton have run the gamut: both John Adams and Thomas Jefferson viewed him as unprincipled and dangerously aristocratic. Hamilton's reputation was mostly negative in the eras of Jeffersonian democracy and Jacksonian democracy. However by the Progressive era, Herbert Croly, Henry Cabot Lodge, and Theodore Roosevelt praised his leadership of a strong government. Several nineteenth- and twentieth-century Republicans entered politics by writing laudatory biographies of Hamilton.
Historians have generally taken one of two main views of Hamilton. Wilentz says:
The older Jeffersonian view attacks him as a centralizer, to the point sometimes of advocating monarchy.
Monuments and memorials.
Since the beginning of the American Civil War, Hamilton has been depicted on more denominations of US currency than anyone else. He has appeared on the , , , , , and . His likeness also began to appear on US postage in 1870. His portrait has continued to appear on US postage and currency, and most notably appears on the modern $10 bill. Hamilton also appears on the $500 Series EE Savings Bond. The source of the face on the $10 bill is John Trumbull's 1805 portrait of Hamilton, in the portrait collection of New York City Hall.
The first postage stamp to honor Hamilton was issued by the U.S. Post Office in 1870. The portrayals on the 1870 and 1888 issues are from the same engraved die, which was modeled after a bust of Hamilton by Italian sculptor Giuseppe Ceracchi The Hamilton 1870 issue was the first US Postage stamp to honor a Secretary of the Treasury. The three-cent red commemorative issue, which was released on the 200th anniversary of Hamilton's birth in 1957, includes a rendition of the Federal Hall building, located in New York City. On March 19, 1956, the United States Postal Service issued the $5 Liberty Issue postage stamp honoring Hamilton.
The only home Hamilton ever owned was a Federal style mansion designed by John McComb Jr., which he built on his 32-acre country estate in Hamilton Heights in upper Manhattan. He named the house, which was completed in 1802, the "Grange" after his grandfather Alexander's estate in Ayrshire, Scotland. The house remained in the family until 1833 when his widow sold it to Thomas E. Davis, a British born real estate developer, for $25,000. Part of the proceeds were used by Eliza to purchase a new townhouse from Davis (Hamilton-Holly House) in Greenwich Village with her son Alexander. The Grange, first moved from its original location in 1889, was moved again in 2008 to a spot in St. Nicholas Park on land that was once part of the Hamilton estate, in Hamilton Heights, a neighborhood in upper Manhattan. The historic structure was restored to its original 1802 appearance in 2011, and is maintained by the National Park service as Hamilton Grange National Memorial.
Alexander Hamilton served as one of the first trustees of the Hamilton-Oneida Academy in New York state. Later the Academy received a college charter in 1812, and the school was formally renamed Hamilton College.Columbia University, Hamilton's alma mater, has official memorials to Hamilton on its campus in New York City. The college's main classroom building for the humanities is Hamilton Hall, and a large statue of Hamilton stands in front of it. The university press has published his complete works in a multivolume letterpress edition. Columbia University's student group for ROTC cadets and Marine officer candidates is named the Alexander Hamilton Society.
The main administration building of the Coast Guard Academy in New London, Connecticut, is named Hamilton Hall to commemorate Hamilton's creation of the United States Revenue Cutter Service, one of the predecessor services of the United States Coast Guard. The U.S. Army's Fort Hamilton in Brooklyn is named after Hamilton. 
In 1990, the U.S. Custom House in New York City was renamed after Hamilton.
In 1880, his son John Church Hamilton commissioned Carl Conrads to sculpt a granite statue, now located in Central Park, New York City. 
One statue honoring Alexander Hamilton in Chicago was mired in controversy, at least concerning the surrounding architecture. Kate Sturges Buckingham (1858–1937), of the Buckingham Fountain family, commissioned the monument. Its impetus was that Treasury Secretary Hamilton "secured the nation's financial future and made it possible for her own family to make its fortune in grain elevators and banking. Consequently, John Angel was hired to model a figurative sculpture and the Finnish architect Eliel Saarinen was to create a "colossal architectural setting" for it. The proposed 80-foot tall columned shelter was poorly received. By Ms. Buckingham's death in 1937, the sculpture's setting. location and design were uncertain. Conspiracy allegations surfaced, and the matter became mired in litigation. After the courts ordered the construction to be completed by 1953, the trustees hired architect Samuel A. Marx. The structure was completed, had structural problems, and was eventually demolished in 1993. The statue was gilded, and is still on display.
A statue, by James Earle Fraser, was dedicated on May 17, 1923, on the south terrace of the Treasury Building, in Washington.
Popular culture.
Apart from the $10 bill, and an obscure 1931 Hollywood film, Hamilton did not attract much attention in American popular culture. However, reviewers praised the 2015 musical "Hamilton", starring Lin-Manuel Miranda, who was also the writer and composer. It is based on the biography by Ron Chernow. "The New Yorker" said: "the show is an achievement of historical and cultural reimagining. In Miranda's telling, the headlong rise of one self-made immigrant becomes the story of America." "Variety" pointed to "Miranda's amazing vision of his towering historical subject as an ideological contemporary who reflects the thoughts and speaks the language of a vibrant young generation of immigrant strivers. It's a wonderfully humanizing view of history." "The New York Times" review concluded:
On slavery.
Until recently the prevailing scholarly view was that Hamilton, like the Founders generally, lacked a deep concern about slavery. John Patrick Diggins traced this animus of historians against Hamilton to Vernon L. Parrington, who, writing in the 1920s to praise Jefferson and the Enlightenment, denounced a reactionary and unenlightened Hamilton as greedy and evil. Sean Wilentz contends that the consensus has changed sharply in Hamilton's favor in recent years. For example, Michael D. Chan argues that the first U.S. Treasury Secretary was committed to ending slavery, Chernow calls him "a fervent abolitionist", David O. Stewart states he was a "lifelong opponent of slavery", and Braun says he "was a leading anti-slavery advocate". Historian Manning Marable says Hamilton "vigorously opposed the slave trade and slavery's expansion."
Hamilton's first polemic against King George's ministers contains a paragraph that speaks of the evils that "slavery" to the British would bring upon the Americans. McDonald sees this as an attack on the institution of slavery. David Hackett Fischer believes the term is used in a symbolic way at that time.
During the Revolutionary War, Hamilton took the lead in proposals to arm slaves, free them, and compensate their masters. In 1779, Hamilton worked closely with his friend John Laurens of South Carolina to propose that such a unit be formed, under Laurens' command. Hamilton proposed to the Continental Congress that it create up to four battalions of slaves for combat duty, and free them. Congress recommended that South Carolina (and Georgia) acquire up to three thousand slaves for service, if they saw fit. Although the South Carolina governor and Congressional delegation had supported the plan in Philadelphia, they did not implement it. 
Hamilton argued that the natural faculties of blacks were as good as those of free whites, and he warned that the British would arm the slaves if the patriots did not. In his 21st-century biography, Chernow cites this incident as evidence that Hamilton and Laurens saw the Revolution and the struggle against slavery as inseparable. Hamilton attacked his political opponents as demanding freedom for themselves and refusing to allow it to blacks.
In January 1785, Hamilton attended the second meeting of the New York Manumission Society (NYMS). John Jay was president and Hamilton was the first secretary and later became president. Chernow notes how the membership soon included many of Hamilton's friends and associates. Hamilton was a member of the committee of the society that petitioned the legislature to end the slave trade, and that succeeded in passing legislation banning the export of slaves from New York. In the same period, Hamilton felt bound by the rule of law of the time and his law practice facilitated the return of a fugitive slave to Henry Laurens of South Carolina. He opposed the compromise at the 1787 Constitutional Convention by which the federal government could not abolish the slave trade for 20 years, and was disappointed when he lost that argument.
Hamilton never supported forced emigration for freed slaves. Horton has argued from this that he would be comfortable with a multiracial society, and that this distinguished him from his contemporaries. In international affairs, he supported Toussaint L'Ouverture's black government in Haiti after the revolt that overthrew French control, as he had supported aid to the slaveowners in 1791—both measures hurt France. Scant evidence has been interpreted by a few to indicate Hamilton may have owned household slaves, as did many wealthy New Yorkers (the evidence for this is indirect; McDonald interprets it as referring to paid employees).
On economics.
Hamilton has been portrayed as the "patron saint" of the American School of economic philosophy that, according to one historian, dominated economic policy after 1861. He firmly supported government intervention in favor of business, after the manner of Jean-Baptiste Colbert, as early as the fall of 1781. Hamilton opposed the British ideas of free trade, which he believed skewed benefits to colonial and imperial powers, in favor of protectionism, which he believed would help develop the fledgling nation's emerging economy. Henry C. Carey was inspired by his writings. Hamilton influenced the ideas and work of the German Friedrich List. In Hamilton's view, a strong executive, linked to the support of the people, could become the linchpin of an administrative republic. The dominance of executive leadership in the formulation and carrying out of policy was essential to resist the deterioration of republican government. Ian Patrick Austin has explored the similarities between Hamiltonian recommendations hand the development of Meiji Japan after 1860.
Bibliography.
Biographies.
</dl>
Specialized studies.
</dl>

</doc>
<doc id="40598" url="http://en.wikipedia.org/wiki?curid=40598" title="Carl Woese">
Carl Woese

Carl Richard Woese (; July 15, 1928 – December 30, 2012) was an American microbiologist and biophysicist. Woese is famous for defining the Archaea (a new domain or kingdom of life) in 1977 by phylogenetic taxonomy of 16S ribosomal RNA, a technique pioneered by Woese which revolutionized the discipline of microbiology. He was also the originator of the RNA world hypothesis in 1967, although not by that name. He held the Stanley O. Ikenberry Chair and was professor of microbiology at the University of Illinois at Urbana–Champaign.
Life and education.
Woese attended Deerfield Academy in Massachusetts. He received a bachelor's degree in mathematics and physics from Amherst College in 1950. During his time at Amherst, Woese took only one biology course (Biochemistry, in his senior year) and had "no scientific interest in plants and animals" until advised by William M. Fairbank, then an assistant professor of physics at Amherst, to pursue biophysics at Yale.
In 1953, he completed a Ph.D. in biophysics at Yale University, where his doctoral research focused on the inactivation of viruses by heat and ionizing radiation. He studied medicine at the University of Rochester for two years, quitting two days into a pediatrics rotation. Then he became a postdoctoral researcher in biophysics at Yale University investigating bacterial spores. From 1960–63, he worked as a biophysicist at the General Electric Research Laboratory in Schenectady, New York. In 1964, Woese joined the microbiology faculty of the University of Illinois at Urbana–Champaign, where he focused on Archaea, genomics, and molecular evolution as his areas of expertise. He became a professor at the University of Illinois at Urbana–Champaign's Carl R. Woese Institute for Genomic Biology, which was renamed in his honor in 2015, after his death.
Woese died on December 30, 2012, following complications from pancreatic cancer.
Work and discoveries.
Early work on the genetic code.
Woese turned his attention to the genetic code while setting up his lab at General Electric's Knolls Laboratory in the fall of 1960. Interest among physicists and molecular biologists had begun to coalesce around deciphering the correspondence between the twenty amino acids and the four letter alphabet of nucleic acid bases in the decade following James D. Watson and Francis Crick's discovery of the structure of DNA in 1953. Woese published a series of papers on the topic. In one, he deduced a correspondence table between what was then known as "soluble RNA" and DNA based upon their respective base pair ratios. He then re-evaluated experimental data associated with the hypothesis that viruses used one base, rather than a triplet, to encode each amino acid, and suggested 18 codons, correctly predicting one for proline. Other work established the mechanistic basis of protein translation, but in Woese's view, largely overlooked the genetic code's evolutionary origins as an afterthought.
In 1962 Woese spent several months as a visiting researcher the Pasteur Institute in Paris, a locus of intense activity on the molecular biology of gene expression and gene regulation. While in Paris, he met Sol Spiegelman, who invited Woese to visit the University of Illinois after hearing his research goals; at this visit Spiegelman offered Woese a position with immediate tenure beginning in the fall of 1964. With the freedom to patiently pursue more speculative threads of inquiry outside the mainstream of biological research, Woese began to consider the genetic code in evolutionary terms, asking how the codon assignments and their translation into an amino acid sequence might have evolved.
Discovery of the third kingdom.
For much of the 20th century, prokaryotes were regarded as a single group of organisms and classified based on their biochemistry, morphology and metabolism. In a highly influential 1962 paper, Roger Stanier and C. B. van Niel first established the division of cellular organization into prokaryotes and eukaryotes, negatively defining prokaryotes as those organisms lacking a cell nucleus. Adapted from Édouard Chatton's generalization, Stanier and Van Niel's concept was quickly accepted as the most important distinction among organisms; yet they were nevertheless skeptical of microbiologists' attempts to construct a natural phylogenetic classification of bacteria. However, it became generally assumed that all life shared a common prokaryotic (implied by the Greek root πρό (pro-), before, in front of) ancestor.
In 1977, Carl Woese and George E. Fox experimentally disproved this universally held hypothesis about the basic structure of the tree of life. Woese and Fox discovered a kind of microbial life which they called the “archaebacteria” (Archaea). They reported that the archaebacteria comprised "a third kingdom" of life as distinct from bacteria as plants and animals. Having defined Archaea as a new "urkingdom" (later domain) which were neither bacteria nor eukaryotes, Woese redrew the taxonomic tree. His three-domain system, based on phylogenetic relationships rather than obvious morphological similarities, divided life into 23 main divisions, incorporated within three domains: Bacteria, Archaea, and Eucarya.
Acceptance of the validity of Woese's phylogenetically valid classification was a slow process. Prominent biologists including Salvador Luria and Ernst Mayr objected to his division of the prokaryotes. Not all criticism of him was restricted to the scientific level. A decade of labor-intensive oligonucleotide cataloging left him with a reputation as "a crank," and Woese would go on to be dubbed as "Microbiology's Scarred Revolutionary" by a news article printed in the journal "Science". The growing amount of supporting data led the scientific community to accept the Archaea by the mid-1980s. Today, few scientists cling to the idea of a unified Prokarya.
Woese's work on Archaea is also significant in its implications for the search for life on other planets. Before the discovery by Woese and Fox, scientists thought that Archaea were extreme organisms that evolved from the organisms more familiar to us. Now, most believe they are ancient, and may have robust evolutionary connections to the first organisms on Earth. Organisms similar to those archaea that exist in extreme environments may have developed on other planets, some of which harbor conditions conducive to extremophile life.
Notably, Woese's elucidation of the tree of life shows the overwhelming diversity of microbial lineages; single-celled organisms represent the vast majority of the biosphere's genetic, metabolic, and ecologic niche diversity. As microbes are crucial for many biogeochemical cycles and to the continued function of the biosphere, Woese's efforts to clarify the evolution and diversity of microbes provided an invaluable service to ecologists and conservationists. It was a major contribution to the theory of evolution and to our knowledge of the history of life.
Evolution of primary cell types.
Woese also speculated about an era of rapid evolution in which considerable horizontal gene transfer occurred between organisms. First described by Woese and Fox in a 1977 paper, these organisms, or "progenotes", were protocells that exhibited a far lower level of complexity due to their error-prone translation apparatus ("noisy genetic transmission channel") that produced high mutation rates which constrained the specificity of cellular interaction and limited genome size. This early translation apparatus would have produced a group of similar, yet functionally equivalent, proteins, rather than a single protein. Furthermore, because of this reduced specificity, all cellular components were susceptible to HGT, and rapid evolution occurred at the level of the ecosystem.
The transition to modern cells (the "Darwinian Threshold") occurred when organisms evolved translation mechanisms with modern levels of fidelity; improved performance allowed cellular organization to reach a level of complexity and connectedness that made genes from other organisms much less able to displace an individual's own genes. Horizontal gene transfer during this era was responsible for the fast early evolution of complex biological structures.
In later years, Woese's work concentrated on genomic analysis to elucidate the significance of horizontal gene transfer (HGT) for evolution. He worked on detailed analyses of the phylogenies of the aminoacyl-tRNA synthetases and on the effect of horizontal gene transfer on how those key enzymes are distributed among organisms. The goal of the research was to explain how the primary cell types (the archaeal, eubacterial, and eukaryotic) evolved, from some ancestral state in the RNA world.
Perspectives on Biology.
Woese shared his thoughts on the past, present, and future of biology in "Current Biology":
The "important questions" that 21st century biology faces all stem from a single question, the nature and generation of biological organization. . . . Yes, Darwin is back, but in the company of . . . scientists who can see much further into the depths of biology than was possible heretofore. It is no longer a "10,000 species of birds" view of evolution—evolution seen as a procession of forms. The concern is now with the process of evolution itself.
I see the question of biological organization taking two prominent directions today. The first is the evolution of (proteinaceous) cellular organization, which includes sub-questions such as the evolution of the translation apparatus and the genetic code, and the origin and nature of the hierarchies of control that fine-tune and precisely interrelate the panoply of cellular processes that constitute cells. It also includes the question of the number of different basic cell types that exist on earth today: did all modern cells come from a single ancestral cellular organization?
The second major direction involves the nature of the global ecosystem. . . . Bacteria are the major organisms on this planet—in numbers, in total mass, in importance to the global balances. Thus, it is microbial ecology that . . . is most in need of development, both in terms of facts needed to understand it, and in terms of the framework in which to interpret them.
Woese considered biology to have an "all-important" role in society. In his view, biology should serve a broader purpose than the pursuit of "an engineered environment":
What was formally recognized in physics needs now to be recognized in biology: science serves a dual function. On the one hand it is society's servant, attacking the applied problems posed by society. On the other hand, it functions as society's teacher, helping the latter to understand its world and itself. It is the latter function that is effectively missing today.
Honors and scientific legacy.
Woese was a MacArthur Fellow in 1984, was made a member of the National Academy of Sciences in 1988, received the Leeuwenhoek Medal (microbiology's highest honor) in 1992, the Selman A. Waksman Award in Microbiology in 1995 from the National Academy of Sciences, and was a National Medal of Science recipient in 2000. In 2003, he received the Crafoord Prize from the Royal Swedish Academy of Sciences "for his discovery of a third domain of life". In 2006, he was made a foreign member of the Royal Society.
Many microbial species, such as "Pyrococcus woesei", "Methanobrevibacter woesei", and "Conexibacter woesei", are named in his honor.
Microbiologist Justin Sonnenburg of Stanford University said "The 1977 paper is one of the most influential in microbiology and arguably, all of biology. It ranks with the works of Watson and Crick and Darwin, providing an evolutionary framework for the incredible diversity of the microbial world".
With regard to Woese's work on horizontal gene transfer as a primary evolutionary process, Professor Norman Pace of the University of Colorado at Boulder said, "I think Woese has done more for biology than any biologist in history, including Darwin... There's a lot more to learn, and he's been interpreting the emerging story brilliantly".

</doc>
<doc id="40599" url="http://en.wikipedia.org/wiki?curid=40599" title="Otto II, Holy Roman Emperor">
Otto II, Holy Roman Emperor

Otto II (955 – December 7, 983), called the Red ("Rufus"), was Holy Roman Emperor from 973 until his death in 983. A member of the Ottonian dynasty, Otto II was the youngest and sole surviving son of Otto the Great and Adelaide of Italy.
Otto II was made joint-ruler of Germany in 961, at an early age, and his father named him co-Emperor in 967 to secure his succession to the throne. His father also arranged for Otto II to marry the Byzantine Princess Theophanu, who would be his wife until his death. When his father died after a 37-year reign, the eighteen-year-old Otto II became absolute ruler of the Holy Roman Empire in a peaceful succession. Otto II spent his reign continuing his father's policy of strengthening Imperial rule in Germany and extending the borders of the Empire deeper into southern Italy. Otto II also continued the work of Otto I in subordinating the Catholic Church to Imperial control.
Early in his reign, Otto II defeated a major revolt against his rule from other members of the Ottonian dynasty who claimed the throne for themselves. His victory allowed him to exclude the Bavarian line of Ottonians from the line of Imperial succession. This strengthened his authority as Emperor and secured the succession of his own son to the Imperial throne.
With domestic affairs settled, Otto II would focus his attention from 980 onward to annexing the whole of Italy into the Empire. His conquests brought him into conflict with the Byzantine Empire and with the Muslims of the Fatimid Caliphate, who both held territories in southern Italy. After initial successes in unifying the southern Lombard principalities under his authority and in conquering Byzantine-controlled territory, Otto II's campaigns in southern Italy ended in 982 following a disastrous defeat by the Muslims. While he was preparing to counterattack Muslim forces, a major uprising by the Slavs broke out in 983, forcing the Empire to abandon its major territorial holdings east of the Elbe river.
Otto II died suddenly in 983 at the age of 28 after a ten-year reign. He was succeeded as Emperor by his three-year-old son Otto III, plunging the Empire into a political crisis.
Early years.
Birth and youth.
Otto II was born in 955, the third son of the King of Germany Otto I and his second wife Adelaide of Italy. By 957, Otto II's older brothers Henry (born 952) and Bruno (born 953) had died, as well as Otto I's son from his first wife Eadgyth, the Crown Prince Liudolf, Duke of Swabia. With his older brothers dead, the two-year-old Otto II's became the Kingdom's crown prince and Otto I's heir apparent. Otto I entrusted his illegitimate son, Archbishop William of Mainz, with Otto II's literary and cultural education. Margrave Odo, commander of the Eastern March, taught the young crown prince the art of war and the kingdom's legal customs.
Needing to put his affairs in order prior to his descent into Italy, Otto I summoned a Diet at Worms and had Otto II elected, at the age of six, co-regent in May 961. Otto II was later crowned by his uncle Bruno the Great, Archbishop of Cologne, at Aachen Cathedral on May 26, 961. While Otto I had secured succession of the throne, he had violated the Kingdom's unwritten law that succession rights could only be granted to a child who has reached the age of majority. He was likely motivated by the high-risk associated with his expedition into Italy to claim the Imperial title from the Pope. Otto I crossed the Alps into Italy, while Otto II remained in Germany, and the two Archbishops, Bruno and William, were appointed as his regents. After three and a half year absence in Italy, Otto I returned to Germany early in 965 as Holy Roman Emperor. In order to give the hope of dynastic continuity after his death, Otto I again confirmed Otto II as his heir on February 2, 965, the third anniversary of Otto I's coronation as Emperor.
Heir apparent.
Though Otto I was crowned Emperor in 962 and returned to Germany in 965, the political situation in Italy remained unstable. After almost two years in Germany, Otto I made a third expedition to Italy in 966. Bruno was again appointed regent over the eleven-year-old Otto II during Otto I's absence.
With his power over northern and central Italy secured, Otto I sought to clarify his relationship with the Byzantine Empire in the East. The Byzantine Emperor objected to Otto's use of the title "Emperor". The situation between East and West was finally resolved to share sovereignty over southern Italy. Otto I sought a marriage alliance between his Imperial house and the Eastern Macedonian dynasty. A prerequisite for the marriage alliance was the coronation of Otto II as Co-Emperor. Otto I then sent word for Otto II to join him in Italy. In October 967, father and son met in Verona and together marched through Ravenna to Rome. On December 25, 967, Otto II was crowned Co-Emperor by Pope John XIII, securing Otto II's succession to the Imperial crown following his father's death.
Otto II's coronation allowed marriage negotiations to begin with the East. Only in 972, six years later, under the new Byzantine Emperor John I Tzimiskes, was a marriage and peace agreement concluded, however. Though Otto I preferred Byzantine Princess Anna Porphyrogenita, daughter of former Byzantine Emperor Romanos II, as she was born in the purple, her age (then only five years old) prevented serious consideration by the East. The choice of Emperor John I Tzimisces was his niece Theophanu, who was the soldier-emperor's niece by marriage. On April 14, 972, the sixteen-year-old Otto II was married to the twelve-year-old Eastern princess, and Theophanu was crowned empress by the Pope.
Even after his coronation, Otto II remained in the shadow of his overbearing father. Though the nominal co-ruler of the Empire, he was denied any role in its administration. Unlike his earlier son Liudolf, whom Otto I named Duke of Swabia in 950, Otto II was granted no area of responsibility. Otto II was confined primarily to northern Italy during his father's time south of the Alps. After five years away, the Imperial family returned to Saxony in August 972.
On May 7, 973, Otto died of fever, and Otto II succeeded his father as sole Emperor without meeting any opposition. Otto II spent his reign continuing his father's policy of strengthening Imperial rule in Germany and extending it deeper into Italy.
Reign as emperor.
Coronation and domestic strife.
When Otto the Great died, the smooth succession to the imperial throne of Otto II had long been guaranteed. Otto II had been king of Germany for twelve years and Emperor for five at the time of Otto the Great's death. Unlike his father, Otto II did not have any brothers to contest his claims to the throne. On May 8, the nobles of the Empire assembled before Otto II and, according to the Saxon Chronicler Widukind of Corvey, "elected" Otto II as his father's successor. One of Otto II's first acts was to confirm the rights and possessions of the Archbishop of Magdeburg. Although Otto II had succeeded peacefully to the throne, internal divisions of power still remained unaddressed. During his first seven years as Emperor, he was constantly occupied with maintaining Imperial power against internal rivals and external enemies.
The domestic problems Otto the Great faced between 963 and 972 had not been resolved by his death. The Saxon nobility continued to resist the Archdiocese of Magdeburg located along the Empire's eastern border. Though established by Otto I, the exact details of the diocese's boundaries were left to Otto II and his aides. Otto II's marriage to the Byzantine Princess Theophanu proved to be to his disadvantage because the Saxon nobles felt it distanced the Emperor from their interests. Among Otto II's chief advisors, only the Saxon Bishop Dietrich I of Metz had close connections with the old Saxon nobility. His other advisers lacked support from the Empire's various Dukes. The Archbishop of Mainz Willigis, appointed in 975, who had been with Otto II's advisor since Otto the Great's second expedition into Italy in the 960s, had not been born from a noble family. Hildebald of Worms, who had been appointed as Otto II's Chancellor in 977 and then as Bishop of Worms in 979, was also not from a noble family.
Otto the Great also failed to clarify affairs in Italy prior to his death. Otto died soon after the appointment of Pope Benedict VI in 973. In 974 Benedict was imprisoned in the Castel Sant'Angelo, the stronghold of the Crescentii family. When Otto II sent an imperial representative, Count Sicco, to secure his release, Crescentius I and Cardinal-Deacon Franco Ferrucci, who would subsequently become Boniface VII, an antipope, had Benedict murdered while still in prison.
Following his coronation, a rift developed between Otto II and his mother, the Dowager Empress Adelaide of Italy. From the death of Otto the Great until Easter 974, Adelaide accompanied the Emperor at all times, traveling throughout the Empire with him. However, Otto II's mother and his wife Theophano each distrusted the influence the other held over the Emperor, causing friction within the Imperial household. A final meeting between Otto II and Adelaide was arranged shortly before Pentecost in 978, but a peaceful outcome was not achieved, forcing Adelaide to retire to Burgundy and to the protection of her brother King Conrad of Burgundy.
Conflict with Henry II.
Otto II sought continued peace between himself and the descendants of his uncle Henry I, Duke of Bavaria. To ensure domestic tranquillity, Otto II, on June 27, 973, granted his cousin, Henry II, Duke of Bavaria, control over the imperial castles in Bamberg and Stegaurach. This was not enough for the young Bavarian Duke, who wished to extend his influence in the Duchy of Swabia as his father had under Otto the Great. The death of Bishop Ulrich of Augsburg on July 4, 973, brought the conflict between the cousins to a head. Without consulting Otto II, Henry II named his cousin Henry as the new Bishop of Augsburg. Augsburg was located on the western side of the Swabian-Bavarian border, the territory of Henry II's brother-in-law Burchard III, Duke of Swabia. Henry's actions in naming a bishop in a duchy not his own and without Imperial direction brought him into conflict with both Otto II and Burchard III. Not desiring civil war, Otto II, on September 22, 973, invested Henry as bishop.
On November 12, 973, Burchard III died with no heir: his union to Hadwing, sister of Henry II, had produced no children. With no clear successor, Henry II demanded that Otto II name him as the new Duke of Swabia. The Emperor sensed the far-reaching ambitions of his cousin and denied his request. Instead, Otto II named as Duke his nephew Otto, son of his half-brother Liudolf, Duke of Swabia. Prior to his appointment, Otto had been a long-time opponent of Henry II's expanding influence in Swabia. By naming a descendant of his half-brother instead of his cousin, Otto II reinforced his father's policy of appointing close family members to key posts throughout the Empire. This appointment elevated the descendants of Otto the Great above those of Henry I in the selection process, further dividing Otto II and Henry II.
The appointment of Otto as Duke of Swabia was taken by Henry II as an assault on his claim to the Imperial throne and a slight to his honor. He and his advisor, Bishop Abraham of Freising, conspired with the Duke of Poland Mieszko I and the Duke of Bohemia Boleslaus II against Otto II in 974. While the historical sources do not describe the goals of the conspirators, Henry II likely intended to restore his honor and to ensure his position as the second most influential man in the Empire. Upon hearing of the conspiracy, Poppo, the Bishop of Würzburg, demanded Henry II and his followers to submit to Otto II or face excommunication. Otto the Great's efforts to consolidate the Church under Imperial control had made this type of action normal. Henry II and his followers complied and submitted to Otto II before armed conflict broke out. Otto II, however, severely punished this conspirators: Henry II was imprisoned at Ingelheim and Bishop Abraham at Corvey.
By 976, Henry II returned to Bavaria. Whether Otto II released him from prison or if he escape is not known for certain. Upon his return, Henry came into open rebellion against Otto II, claiming rulership over the Empire for himself. Henry II mobilized the Saxon nobility against Otto II. In particular, Henry II had strong connections to Margrave Gunther of Merseburg, Count Egbert the One-Eyed, and Dietrich I of Wettin, who were all displeased with Otto II's lack of adherence to Saxon tradition. In response to the rebellion, Otto II stripped Henry II of his Duchy and had him excommunicated. Otto II then marched his army south to Bavaria and laid siege to Regensburg, Henry II's stronghold. Otto II's army eventually broke through the city's defenses, forcing Henry II to flee to Bohemia.
With Henry II deposed, in July 976 Otto II issued far-reaching edicts on the reorganization of the southern German duchies. Otto II reduced the Duchy of Bavaria in territorial size by almost a third. From the confiscated Bavarian territory, Otto II established the Duchy of Carinthia in southern Germany. By depriving Bavaria of the March of Verona, Otto II considerably reduced the influence of the Bavarian Dukes in northern Italy and in general Imperial policy regarding Italy. Otto II gave the newly diminished Duchy of Bavaria to his relative Otto, the Duke of Swabia, and appointed Henry III, son of the former Bavarian Duke Berthold, as Duke of Carinthia. These appointments continued his policy of appointing of individuals who had no political links to Otto the Great, including those who had even rebelled against him.
With matters in southern Germany settled, Otto II turned his attention to defeating and capturing Henry II. After a failed first invasion into Bohemia, Otto II marched to Bohemia a second time in August 977. While in Bohemia, a revolt broke out in Bavaria. Henry I, Bishop of Augsburg, and the newly appointed Carinthian Duke Henry III joined Henry II in rebellion, forcing Otto II to return from Bohemia. The Emperor, aided by the Duke of Swabia and Bavaria, met the rebels at Passau and, after a long siege, forced them into submission. Otto II the brought the rebels before the Imperial Diet in Quedlinburg on March 31, 978. Boleslaus II was treated with honors and swore loyalty to Otto II. Mieszko I of Poland also submitted to Otto II's overlordship. Henry II, however, was not so fortunate: Otto II imprisoned him under the custody of the Bishop of Utrecht where he would remain until Otto II's death in 983.
While Otto the Great had pardoned rebellious family members for their crimes, Otto II followed a different policy. Instead, Otto II hoped to subordinate the Bavarian line of Ottonians to his Imperial authority. Henry II's four-year-old son, also named Henry, was sent to Hildesheim to study for an ecclesiastical career. It appears Otto II intended to end the Bavarian Ottonians' secular control of Bavaria. Under a new Duke, Bavaria would remain a remote area of the Empire. Otto II would only visit the Duchy three times during his reign, in all cases accompanied by the military.
War with Denmark.
In 950, Otto the Great had subdued the Kingdom of Denmark and forced the Danish King Gorm the Old to accept him as his overlord. Otto the Great also forced the king and his heir apparent Harald Bluetooth to convert to Christianity. Under the reign of Otto the Great, Denmark fulfilled all its obligations and regularly paid tribute to the Germans. When Harald became king in 958, he expanded the control of his kingdom into Norway, becoming king there in 970. With his newly obtained power, the young ruler was no longer willing to accept German supremacy over his kingdom. In summer 974, Harald rebelled against Otto II. With the support of Norwegian troops, Harald was able to cross the Danish border into Germany, defeating the German forces stationed in the north. Otto II attacked Harald's forces, but the joint Danish-Norwegian army repelled the German army. In autumn, however, when the Norwegian allies sailed north to return to Norway, Otto II was able to counter Harald's advances at the Danevirke. As a result of this victory, Otto II officially annexed Denmark into the Empire and exiled Harald to Norway.
War against France.
Before Henry II's civil war in southern Germany erupted, Otto II was faced with disputes in western Germany. The brothers Reginar IV, Count of Mons, and Lambert I, Count of Louvain, demanded that the Emperor restore their confiscated inheritance in the Duchy of Lorraine. Years earlier in 958, Otto the Great banished their father Reginar III, Count of Hainaut, to Bohemia after he attempted a failed revolt. In 973, Otto II granted their request. With both Otto the Great and Count Reginar III dead, it appears Otto II desired a fresh start with the two sons. Lambert I and Reginar IV returned to Lorraine in 973 to reclaim their land by force. After an initial failure, the brother attempted again in 976, this time with the support of King Lothar of France. To help calm the situation in the west, Otto II appointed Charles, his cousin and brother of Lothar, as Duke of Lower Lorraine. The same year, Otto II appointed Egbert as his Imperial Chancellor.
Otto II's support of Charles, however, infuriated the French king, who claimed the Duchy as his own territory. Charles and Lothair were also feuding, with Charles being exiled from France over infidelity allegation concerning Lothair's wife. Charles fled to Otto II's court and paid homage to Otto II. In return, Otto II appointed Charles as Duke and promised to support him in claiming the French throne. Soon after Otto II crushed Henry II's revolt in the south, the Emperor and his wife Theophanu returned to the old capital of Aachen in Lorraine. With the Imperial family near the French border, Lothair invaded Lorraine and marched on Aachen. With the French army in sight, Otto II and Theophano fled to Cologne and then to the Duchy of Saxony. Upon hearing of the French invasion, Otto II’s mother Adelaide of Italy, who was Lothair's mother-in-law, sided with Lothair over her own son and moved to the court of her brother King Conrad of Burgundy. After occupying Aachen for five days, Lothair returned to France after symbolically disgracing the city.
Otto II convened the Imperial Diet in mid-July at Dortmund. There, Otto II declared war against France and prepared his army to march west. In September 978, Otto II retaliated against Lothair by invading France with the aid of Charles. He met with little resistance on French territory, devastating the land around Rheims, Soissons, and Laon. Otto II then had Charles crowned as King of France by Theodoric I, Bishop of Metz. Lothair then fled to the French capital of Paris and was there besieged by Otto II and Charles. Sickness among his troops brought on by winter and a French relief army under Hugh Capet forced Otto II and Charles to lift the siege on November 30, and to return to Germany. On the journey back to Germany, Otto's rearguard was attacked and destroyed by French forces, with their supplies being captured. Despite neither side obtaining a clear victory, Otto II felt his honor was sufficiently restored and opened peace negotiations with the French King. Peace was finally concluded between Otto II and Lothair in 980: in return for renouncing his claims on Lorraine, Otto II would recognize Lothair's son Louis V as the rightful heir to the French throne.
With peace concluded, Otto II returned to Aachen to celebrate Pentecost, and then moved towards Nijmegen. During the journey, in late June or early July 980, the Empress Theophanu gave birth to the Imperial couple's only son: Otto III.
Reign in Italy.
Papal politics.
With his rule north of the Alps secured and with the birth of his heir, Otto II shifted his focus to Italy. The situation south of the Alps was chaotic. Pope Benedict VI, who had been appointed by Otto I, had been imprisoned by the Romans in Castel Sant'Angelo. When Otto II sent an imperial representative, Count Sicco, to secure his release, Crescentius I and Cardinal Franco Ferrucci had Benedict VI murdered while still in prison in 974. Cardinal Franco Ferrucci then crowned himself as Benedict VI's successor, becoming Antipope Boniface VII. A popular revolt, however, forced Boniface VII to flee to Constantinople, taking a vast treasure with him. In October 974, under the direction of Count Sicco, the bishop of Sutri was elected Pope as Pope Benedict VII. Boniface VII was then summarily excommunicated for his unsuccessful attempt to take the papacy.
In 979 Benedict VII's position as ruler of Rome was threatened, forcing the Pope to withdraw from and seek the aid of the Emperor. Accepting the Pope's call for aid, Otto II and Theophano, along with their infant son Otto III, prepared for a march south across the Alps. Otto II appointed Willigis, the Archbishop of Mainz, to serve as his regent over Germany.
In October 980 the Imperial court arrived in Chiavenna and received its first Italian delegations. Otto II arrived in Italy at Pavia on December 5, 980. In Pavia, Otto II and his mother, the dowager empress Adelaide of Italy, were reconciled after years of being apart. Before the imperial family celebrated Christmas together in Ravenna, Otto II received the Iron Crown of Lombardy as the King of Italy. Following the New Year, Otto II led his Imperial court to Rome, reaching the city on February 9, 981, where the Emperor restored Pope Benedict VII to his papal throne without difficulty. In Rome, Otto II held a magnificent court ceremony to mark Easter. The imperial family was joined by Otto II's sister Matilda, Abbess of Quedlinburg, King Conrad of Burgundy and his wife Matilda of France, Duke Hugh Capet of France, Duke Otto of Swabia and Bavaria, and other high secular and religious officials from Germany, Italy and France.
Otto II proceeded to hold court in Rome, making the city his Imperial capital, where he received princes and nobles from all parts of western Europe.
Venetian affairs.
The relationship between the Empire and the Republic of Venice was readdressed during Otto II's reign. In 966, The Doge of Venice Peitro IV married a relative of Otto I. The marriage brought the Empire and Venice into close relationship, with Otto I, in 967, granting a series of commercial agreements to Venice in general and to Pietro IV's family in particular. These agreements strengthened Venice's tie to the Western Empire, which greatly angered the Byzantine Emperor John I Tzimisces as Venice controlled all sea trade between Western Europe and the Byzantine Levant in the East.
Otto I's military protection of Pietro IV ensured his hold over power in Venice despite his autocratic tendencies over the republican city. In 973, however, Otto I died. With Otto II busy suppressing revolts in Germany, the Venetians opposed to Pietro IV found their opportunity to depose him. Imprisoning the Doge within his palace, the Venetians nobles set fire to the building. However, the fire soon spread to Saint Mark's Basilica, resulting in the greater part of the city being burnt. The Doge and his son, also named Pietro, were killed in the blaze, but their bodies were later recovered and respectfully buried. Pietro IV's younger son, Vitale Candiano, survived however, and fled to Otto II's court in Saxony with plans to depose the new pro-Byzantine Doge, Pietro I Orseolo.
Pietro I's conciliating policy towards the Empire was ineffective. After having ruled Venice for four years, Pietro I voluntarily abdicated to become a monk, allowing the pro-Ottonian Vitale to return to Venice as Doge in 977, restoring the city's friendly relationship with the Empire. However, Vitale's reign was short (less than two years) and he too voluntarily abdicated to become a monk. With the position vacant, the pro-Byzantine Tribuno Memmo became the new Doge in 979. With the change in leadership, Otto II was reluctant to renew the city's commercial agreements which his father had previously granted to the city. It was only after the intervention of Otto II's mother, the dowager empress Adelaide of Italy, did the Emperor renew the agreements.
Violence erupted in Venice during 980 when tensions between pro-Ottonian Coloprini family and the pro-Byzantine Morosini family. The Coloprini pleaded with the Emperor for support. Seeing an opportunity to fully incorporate Venice into the Empire, Otto II agreed. Upon arriving in Italy in 981, Otto II immediately imposed a trade embargo against the island republic. While the initial embargo showed little effect on Venice, Otto II imposed a second embargo in 983 which dealt considerable damage to the Venetian economy. The effects were disastrous enough to cause the ruling Venetian families to surrender to Otto II, but Otto II's untimely death that year prevented such action.
Religious policy.
Otto II followed the policy of his father in expanding the importance of the Church in his Empire, in particular the importance of monasticism and monasteries. The Church and its organs served as supporting and stabilizing factor in the Empire's structure. To fulfill these tasks, Otto II strengthened the legal integrity and economic independence of the bishops from the secular nobility. The Ottonians had particular religious interest in Memleben as both Otto II's father Otto I and grandfather Henry I had died there. Otto II and his wife Theophanu enhanced the spiritual importance of the city by establishing a Benedictine Imperial abbey there: the Memleben Abbey. Within a short time, the Memleben Abbey had become one of the richest and most influential of the Imperial abbeys. These measures and the unusual size of the abbey perhaps suggest that Memleben may have been intended as an Imperial Mausoleum for the Ottonians.
Following the suppression of Henry II's rebellion, Otto II used the Empire's monasteries as the location for the treason trials. While his father had founded only one monastery (Otto I later replaced the abbey with the Cathedral of Magdeburg) during his 37 years of reign. Otto II, however, established at least four monasteries: Memleben, Tegernsee, Bergen, and Arneburg. Monasticism became a key part of Otto II's Imperial policy, entrusting the Abbots with key political functions.
Otto II employed monks among his top political advisers, including Ekkehard I and Majolus of Cluny. One of the most important such monks was John Philagathus (the future Antipope John XVI). Of Greek descent, John was the personal chaplain of Otto II's wife Theophanu, accompanying her when she traveled from Constantinople to marry Otto II. Otto II appointed him as his Imperial Chancellor from 980 to 982, as well as the Abbot of the Nonantola Abbey. Following Otto II's death in 983, Theophanu, as her son Otto III's regent, would name John as Otto III's tutor. She would later appoint John as the bishop of Piacenza, and would send him to Constantinople to arrange for a marriage between Otto III and a Byzantine princess.
Southern expansion.
In regard to his Italian policy, Otto II went beyond the goals of his father. Not satisfied with the territorial gains made under Otto I, Otto II wanted more. His policy was based not only on securing his power in Rome, or to cooperate with the Papacy, but also to gain absolute dominion over the whole of Italy. Influenced by his wife, who was hostile to the return of the Macedonian Dynasty in the shape of Byzantine Emperor Basil II after the assassination of John I Tzimisces, Otto II was persuaded to annex the Byzantine controlled southern Italy. However, this policy necessarily meant war with not only the Byzantine Empire but the Muslim Fatimid Caliphate as well, who claimed southern Italy as within their sphere influence.
The Ottonians' chief lieutenant in central and southern Italy had long been the Lombard leader Pandulf Ironhead. Originally appointed by Otto I as Prince of Benevento and Capua in 961, Pandulf waged war against the Byzantines and expanded Ottonian control to include the Duchy of Spoleto in 967. Under Otto II, Pandulf added the Principality of Salerno in 978 to the Empire. His campaigns under Otto I and Otto II incorporated all three of the southern Lombard principalities - Benevento, Capua, and Salerno - into the Holy Roman Empire. As vassal of Otto II, Pandulf ruled a large bloc of territories that stretched as far north as Tuscany and as far south as the Gulf of Taranto.
Pandulf's death in 981 deprived Otto II of one of his primary lieutenants. Pandulf's lands were partitioned among his sons, though further quarrels between the local Lombard princes soon followed. Pandulf's older son Landulf IV received Capua and Benevento while his younger son Pandulf II received Salerno. Upon hearing of Pandulf's death, Otto II, ruling from Rome, traveled south to install Thrasimund IV as Duke of Spoleto. Then, Pandulf's nephew Pandulf II was given Benevento when Otto II partitioned Landulf IV's territory, with Landulf IV keeping Capua. Finally, Duke Manso I of Amalfi deposed Pandulf II of his rule in Salerno in 982.
By 982 the entire area once ruled by Pandulf had collapsed, weakening Otto II's position against the Byzantines. The Byzantines still claimed sovereignty over the Lombard principalities and the lack of singular leader to prevent their advances into Lombard territory allowed the Byzantines to make inroads further north. Otto II attempted on several occasions to reunify the Lombard principalities politically and ecclesiastically into his Empire after Pandulf's death. Though he unsuccessfully besieged Manso I in Salerno, Otto II ultimately obtained the recognition of his authority from all the Lombard principalities.
With his authority reestablished over the Lombard princes, Otto II turned his attention towards the threat from Muslim Sicily. Since 960s the island had been under Muslim rule as the Emirate of Sicily, a state of the Fatimid Caliphate. The ruling Kalbid dynasty had conducted raids against Imperial territories in southern Italy. The death of Pandulf in 981 allowed the Sicilian Emir Abu al-Qasim to increase his raids, hitting targets in Apulia and Calabria. As early as 980 Otto II demanded a fleet from the city of Pisa to help him carry out his war in southern Italy, and in September 981 he marched into southern Italy. Needing allies in his campaign against the Muslims and the Byzantine Empire, Otto II reconciled with Amalfian Duke Manso I, granting Imperial recognition of his rule over Salerno.
Otto II's troops marched on Byzantine-controlled Apulia in January 982 with the purpose of annexing the territory into his Empire. Otto II's march caused the Byzantine Empire to seek an alliance with Muslim Sicily in order to hold onto their southern Italian possessions. The Emperor's army besieged and captured the Byzantine city of Taranto, the administrative center of Apulia, in March 982. After celebrating Easter in Taranto, Otto II moved his army westward, defeating a Muslim army in early July. Emir Abu al-Qasim, who had declared a Holy War ("jihad") against the Empire, retreated when he noticed the unexpected strength of Otto II's troops when the Emperor was not far from Rossano Calabro. Informed of the Muslim retreat, Otto II left his wife Theophanu and young son Otto III (along with the Imperial treasury) in the city and marched his army to pursue the Muslim force.
Unable to flee back to his stronghold in Sicily due an Imperial naval blockade, al-Qasim faced the Imperial army in a pitched battle south of Crotone at Cape Colonna on July 14, 982. After a violent clash, a corps of Otto II's heavy cavalry destroyed the Muslim center and pushed towards al-Qasim's guards, with the Emir killed during the charge. Despite the Emir's death, the Muslim troops did not flee the battlefield. The Muslims regrouped and managed to surround the Imperial soldiers, slaughtering many of them and inflicting a severe defeat upon the Emperor. According to the historian Muslim Ibn al-Athir, Imperial casualties numbered around 4,000. The Lombard Princes Landulf IV of Benevento and Pandulf II of Salerno, German Bishop Henry I of Augsburg, German Margrave Gunther of Merseburg, the Abbot of Fulda, and numerous other Imperial officials were among the battle's casualties.
The Imperial defeat shocked the political makeup of Southern Italy. With two Lombard princes dead, the Principalities of Capua and the Benevento passed to younger branches of the Landulfid family. Though the Muslim troops were forced to retreat to Sicily after their victory, the Muslims remained a presence in southern Italy, harassing the Byzantines and Lombards. The Ottonian defeat, the worst in the history of the Empire at the time, greatly weakened Imperial power in southern Italy. The Byzantines joined forces with the Muslims and regained possession of Apulia from Ottonian forces.
Imperial crisis.
Succession issues.
The defeat at Stilo forced Otto II to flee north to Rome. He then held an Imperial Diet at Verona on Pentecost, 983. He sent his nephew Otto I, Duke of Swabia and Bavaria, back to Germany with the news of the defeat and to call the German nobles to the assembly, but he died "en route" on November 1, 982, in Lucca. News of the battle did cross the Alps, however, reaching as far as Wessex in Britain, signifying the magnitude of the defeat. Duke Bernard I of Saxony was heading south for the assembly when Danish Viking raids forced him to return to face the threat.
At the assembly, Otto II appointed Conrad (a distant relative of Otto II) and Henry III as the new Dukes of Swabia and Bavaria respectively. Henry III had previously been exiled by Otto II following his defeat as part of a two-year revolt against Otto II's rule. The defeat at Stilo cost the Empire many nobles, forcing Otto II to lift the banishment of Henry III in order to stabilize domestic affairs in Germany while he campaigned against the Muslim and Byzantines in southern Italy. Also, the appointment of Conrad I allowed the House of the Conradines to return to power in Swabia for the first time since Emperor Otto I in 948. Otto II and the assembled nobles agreed on a strategy of naval blockade and economic warfare until reinforcement from Germany could arrive. Otto II then prepared for a new campaign against the Muslims and obtained a settlement with the Republic of Venice, whose assistance he needed following the destruction of his army at Stilo. However, the death of Otto II the next year and the resulting civil war prevented the Empire from appropriately responding to the defeat.
The most important action taken by Otto II at the assembly, however, was to secure the "election" of his son Otto III, who was then only three years old, as King of Germany and heir apparent to the Imperial throne. Otto III thus became the only German king elected south of the Alps. The exact reason for this unusual procedure has been lost to history. It is possible that the conditions in southern Italy following the defeat required Otto II to act quickly in designating an Imperial heir to ensure connivance in the Empire's future. It is also conceivable, however, that holding the election in Italy was a deliberate choice on the part of Otto II in order to demonstrate that Italy was an equal part of the Empire on the same level as Germany. His election secured, Otto III and his mother, the Empress Theophanu, traveled north across the Alps heading for Aachen, the traditional coronation site for the Ottonians, in order for Otto III to be officially crowned as king. Otto II stayed in Italy to further address his military campaigns.
Great Slav uprising.
Around the year 982, Imperial authority in Slavic territory extended as far east as the Lusatian Neisse River and as far south as the Ore Mountains. Following the defeat of Otto II at Stilo in 983, the Lutici Federation of Polabian Slavs revolted against their German overlords, sparking a great revolt known as the Great Slav Rising ("Slawenaufstand"). The Polabian Slavs destroyed the bishoprics of Havelberg and Brandenburg. According the German chronicler Bishop Thietmar of Merseburg, the decades-long, forced Germanization and Christianization of the Slavs associated with these two churches was the reason for their destruction. Thietmar blames the uprising on maltreatment of the Slavs by the Germans: "Warriors, who used to be our servants, now free as a consequence of our injustices." In the Obotrite territories along the Elbe River, the Luticians initiated a revolt aimed at the abolition of feudal rule and Christianity, drawing upon considerable support by the Obodrite populace and their leader Mstivoj. In part, the Obrodite revolt was successful: The princely family, though in part remaining Christian, dissolved Christian institutions.
Soldiers from the Northern March, the March of Meissen, the March of Lusatia, as well as from the Bishop of Halberstadt and the Archbishop of Magdeburg, joined forces to defeat the Slavs near Stendal. Nevertheless, the Empire was forced to withdraw to the western banks of the Elbe river. The successes of the Empire's Christianization policy towards the Slavs were nullified, and political control over the Billung March and the Northern March (territories east of the Elbe) was lost. In the decade since his death, Otto I's life work of converting the Slavs was undone. The Slavic territories east of the Elbe would remain pagan for over a century before further missionary work resumed: it would not be until the 12th century that the churches of Havelberg and Brandenburg would be reestablished.
The Danes took advantage of the Slavic revolt and invaded the March of Schleswig along the Empire's northern border while the Sorb Slavs invaded and conquered the March of Zeitz from Saxon control.
Sudden death and political turmoil.
In July 983, Pope Benedict VII, a longtime Ottonian supporter, died of natural causes after having reigned for almost ten years. Otto II returned to Rome in September to name a new Pope, selecting the Bishop of Pavia Pietro Canepanova (who reigned as Pope John XIV) in November or early December. While Otto II was in Rome overseeing the election of a new pope, a malaria outbreak in central Italy prevented the resumption of military activity in southern Italy. The outbreak ultimately led to the death of the Emperor himself: he died in his palace in Rome at the age of 28 on December 7, 983, after having reigned for just over a decade. Otto II's money and possessions were divided among the Catholic Church, the poor of the Empire, his mother Adelaide and sister Matilda, and those nobles loyal to him. Otto II was then buried in the atrium of St. Peter's Basilica, becoming the only German ruler to be buried in a foreign country instead of in Germany.
Otto II's three-year-old son Otto III was crowned as King of Germany in Aachen on Christmas Day in 983, three weeks after his father's death. Otto III was crowned by Willigis, the Archbishop of Mainz, and John, the Archbishop of Ravenna. News of Otto II's death first reached Germany after Otto III's coronation. The unresolved problems in southern Italy and the Slavic uprising on the Empire's eastern border made the Empire's political situation extremely unstable. The arrival of a minor on the Imperial throne threw the Empire into confusion, allowing Otto III's mother, the Byzantine Princess Theophanu, to reign as his regent.
In 976, Otto II had deposed Henry II as Duke of Bavaria and imprisoned him. In early 984, Henry II escaped from his imprisonment by the Bishop of Utrecht. Free from his confinement, he seized the infant Otto III and, as a member of the ruling Ottonian dynasty, claimed the regency of the Empire for himself. Henry II eventually went so far as to claim the German throne outright, obtaining the allegiance of Mieszko I of Poland and Boleslaus II, Duke of Bohemia. Henry II's claims were supported by Archbishop Egbert of Trier, Archbishop Gisilher of Magdeburg, and Bishop Dietrich I of Metz. Otto III's right to the throne, however, was supported by Archbishop Willigis of Mainz and the Dukes of Saxony, Bavaria, and Swabia. The threat of war from Willigis and Conrad I, Duke of Swabia forced Henry II to relinquish Otto III on June 29, 984 and to respect the regency of Theophanu.
The early death of Otto II and the ensuing events proved to be a serious test for Empire. Despite having a child under the regency of his mother as a ruler, the structure established by Emperor Otto the Great remained strong as most of the Empire's most powerful officials stayed loyal to the Imperial system.
Character.
Otto was a man of small stature, by nature brave and impulsive, and by training an accomplished knight. He was generous to the church and aided the spread of Christianity in many ways. According to one of the chroniclers of the time, he was given the epithet of the "Red" when in 981 he invited the most troublesome of the Roman families to a banquet, and proceeded to butcher them at dinner. More sympathetic chroniclers said that it was due to his reddish complexion.
Family and children.
Otto II was a member of the Ottonian dynasty of rulers of Germany (and later the Holy Roman Empire) from 919 to 1024. In relation to the other members of his dynasty, Otto II was the grandson of Henry I, son of Otto I, father of Otto III, and a first-cousin once removed to Henry II.
Otto II had only one known wife. On April 14, 972, Otto II married Theophanu, a Byzantine princess of the Phokas family who was the cousin of reigning Byzantine Emperor John I Tzimiskes. The two had at least five children:

</doc>
<doc id="40604" url="http://en.wikipedia.org/wiki?curid=40604" title="Caprera">
Caprera

Caprera is a small island off the coast of Sardinia, Italy, located in the Maddalena archipelago.
In the area of La Maddalena island in the Strait of Bonifacio, it is a tourist destination and is famous as the place to which Giuseppe Garibaldi retired (1856–82).
This island has been declared a natural reserve for the particular species of seabirds living on it (royal seagull, cormorant and peregrine falcon). The island's name is linked to that of Giuseppe Garibaldi, an Italian patriot and fighter who lived in the 19th century and was one of the fathers of the Italian independence. He bought the island in 1855 and died there in 1882. His house is now a museum and a memorial chapel and the island itself is a national monument. Caprera is linked to La Maddalena island by a 600 metre long causeway.
The island was probably given its name because of the numerous wild goats living on it ("capra" means "goat" in Italian).
It is the second largest island in the archipelago and has a surface of 15.7 km² and 45 km of coastline. Monte Tejalone is the highest point (212 m). On the south-western side there is a very important sailing centre and the many coves and anchorages which can be found along the coastline make the landing easy.
Many remains of Roman cargo ships as well as of the boat of Garibaldi were found there. After the Roman occupation, Caprera remained deserted for centuries before being inhabited by groups of shepherds. Later in 1855 Garibaldi decided to settle there and planted the first trees of the blooming pinewood which covers the island today. A century after Garibaldi's death the island was freed from the numerous existing military restrictions and is now completely open to the public.
Sailing.
Caprera's Porto Palma gulf is home to the Centro Velico Caprera school since 1967.

</doc>
<doc id="40606" url="http://en.wikipedia.org/wiki?curid=40606" title="Sviatoslav I of Kiev">
Sviatoslav I of Kiev

Sviatoslav I Igorevich (Old East Slavic: С~тославъ / Свѧтославъ Игорєвичь, "Sventoslavŭ / Svantoslavŭ Igorevičǐ"; Russian: Святослав Игоревич, "Sviatoslav Igorevich"; Ukrainian: Святослав Ігорович, "Sviatoslav Ihorovych"; Belarusian: Святаслаў Ігаравіч, "Sviataslaŭ Iharavich"; Bulgarian: Светослав, "Svetoslav", Greek: Σφενδοσθλάβος, "Sphendosthlabos") (c. 942 – March 972), also spelled Svyatoslav, Grand prince of Kiev. The son of Igor of Kiev and Olga, Sviatoslav is famous for his incessant campaigns in the east and south, which precipitated the collapse of two great powers of Eastern Europe, Khazaria and the First Bulgarian Empire. He also conquered numerous East Slavic tribes, defeated the Alans and attacked the Volga Bulgars, and at times was allied with the Pechenegs and Magyars.
His decade-long reign over the Kievan Rus' was marked by rapid expansion into the Volga River valley, the Pontic steppe, and the Balkans. By the end of his short life, Sviatoslav carved out for himself the largest state in Europe, eventually moving his capital in 969 from Kiev (modern-day Ukraine) to Pereyaslavets (modern-day Romania) on the Danube. In contrast with his mother's conversion to Christianity, Sviatoslav remained a staunch pagan all of his life. Due to his abrupt death in ambush, his conquests, for the most part, were not consolidated into a functioning empire, while his failure to establish a stable succession led to a fratricidal feud among his sons, resulting in two of his three sons being killed.
Name.
Sviatoslav was the first ruler of the Kievan Rus' recorded in the Primary Chronicle with a name of Slavic origin (as opposed to his predecessors, whose names derived from Old Norse). This name, however, is not recorded in other medieval Slavic countries. Even in Rus', it was attested only among the members of the house of Rurik, as were the names of Sviatoslav's immediate successors: Vladimir, Yaroslav, and Mstislav. This is questionable, however, as these names follow conventions well established in other Slavic lands, and it ignores Vladimir of Bulgaria, who ruled between 889-893. Some scholars speculate that the name of Sviatoslav, composed of the Slavic roots for "holy" and "glory", was an artificial derivation combining those of his predecessors Oleg and Rurik (they mean "holy" and "glorious" in Old Norse, respectively). On the other hand, such a compound structure name was already known from Great Moravia, as in the rulers named Svatopluk. Clearly Sviatoslav's name belongs to this tradition, as he had a son by the name of Yaropolk, of much the same form, and a grandson by the same name, Sviatopolk.
Early life and personality.
Virtually nothing is known about Sviatoslav's childhood and youth, which he spent reigning in Novgorod. Sviatoslav's father, Igor, was killed by the Drevlians around 945, and his mother, Olga, ruled as regent in Kiev until Sviatoslav reached maturity (ca. 963). Sviatoslav was tutored by a Varangian named Asmud (meaning "quick as a leopard"). The tradition of employing Varangian tutors for the sons of ruling princes survived well into the 11th century. Sviatoslav appears to have had little patience for administration. His life was spent with his "druzhina" (roughly, "company") in permanent warfare against neighboring states. According to the Primary Chronicle, he carried on his expeditions neither wagons nor kettles, and he boiled no meat, rather cutting off small strips of horseflesh, game, or beef to eat after roasting it on the coals. Nor did he have a tent, rather spreading out a horse-blanket under him and setting his saddle under his head, and all his retinue did likewise.
Sviatoslav's appearance has been described very clearly by Leo the Deacon, who himself attended the meeting of Sviatoslav with John I Tzimiskes. Following Deacon's memories, Sviatoslav was a blue-eyed male of average height but of stalwart build, much more sturdy than Tzimiskes. He shaved his blond head and his beard but wore a bushy mustache and a sidelock as a sign of his nobility. He preferred to dress in white, and it was noted that his garments were much cleaner than those of his men, although he had a lot in common with his warriors. He wore a single large gold earring bearing a carbuncle and two pearls.
Religious beliefs.
Sviatoslav's mother, Olga, converted to Eastern Orthodox Christianity at the court of Byzantine Emperor Constantine Porphyrogenitus in 957. However, Sviatoslav remained a pagan all of his life. In the treaty of 971 between Sviatoslav and the Byzantine emperor John I Tzimiskes, the Rus' are swearing by Perun and Veles. According to the Primary Chronicle, he believed that his warriors (druzhina) would lose respect for him and mock him if he became a Christian. The allegiance of his warriors was of paramount importance in his conquest of an empire that stretched from the Volga to the Danube.
Family.
Very little is known of Sviatoslav's family life. It is possible that he was not the only (or the eldest) son of his parents. The Russo-Byzantine treaty of 945 mentions a certain Predslava, Volodislav's wife, as the noblest of the Rus' women after Olga. The fact that Predslava was Oleg's mother is presented by Vasily Tatishchev. He also speculated that Predslava was of a Hungarian nobility. George Vernadsky was among many historians to speculate that Volodislav was Igor's eldest son and heir who died at some point during Olga's regency. Another chronicle told that Oleg (? - 944?) was the eldest son of Igor. At the time of Igor's death, Sviatoslav was still a child, and he was raised by his mother or at her instructions. Her influence, however, did not extend to his religious observance.
Sviatoslav had several children, but the origin of his wives is not specified in the chronicle. By his wives, he had Yaropolk and Oleg. By Malusha, a woman of indeterminate origins, Sviatoslav had Vladimir, who would ultimately break with his father's paganism and convert Rus' to Christianity. John Skylitzes reported that Vladimir had a brother named Sfengus; whether this Sfengus was a son of Sviatoslav, a son of Malusha by a prior or subsequent husband, or an unrelated Rus' nobleman is unclear.
Eastern campaigns.
Shortly after his accession to the throne, Sviatoslav began campaigning to expand Rus' control over the Volga valley and the Pontic steppe region. His greatest success was the conquest of Khazaria, which for centuries had been one of the strongest states of Eastern Europe. The sources are not clear about the roots of the conflict between Khazaria and Rus', so several possibilities have been suggested. The Rus' had an interest in removing the Khazar hold on the Volga trade route because the Khazars collected duties from the goods transported by the Volga. Historians have suggested that the Byzantine Empire may have incited the Rus' against the Khazars, who fell out with the Byzantines after the persecutions of the Jews in the reign of Romanus I Lecapenus.
Sviatoslav began by rallying the East Slavic vassal tribes of the Khazars to his cause. Those who would not join him, such as the Vyatichs, were attacked and forced to pay tribute to the Kievan Rus' rather than to the Khazars. According to a legend recorded in the Primary Chronicle, Sviatoslav sent a message to the Vyatich rulers, consisting of a single phrase: "I want to come at you!" (Old East Slavic: "хощю на вы ити") This phrase is used in modern Russian (usually misquoted as "Иду на вы") and in modern Ukrainian ("Іду на ви") to denote an unequivocal declaration of one's intentions. Proceeding by the Oka and Volga rivers, he attackedVolga Bulgaria. He employed Oghuz and Pecheneg mercenaries in this campaign, perhaps to counter the superior cavalry of the Khazars and Bulgars.
Sviatoslav destroyed the Khazar city of Sarkel around 965, possibly sacking (but not occupying) the Khazar city of Kerch on the Crimea as well. At Sarkel he established a Rus' settlement called Belaya Vyezha ("the white tower" or "the white fortress", the East Slavic translation for "Sarkel"). He subsequently destroyed the Khazar capital of Atil. A visitor to Atil wrote soon after Sviatoslav's campaign: "The Rus' attacked, and no grape or raisin remained, not a leaf on a branch." The exact chronology of his Khazar campaign is uncertain and disputed; for example, Mikhail Artamonov and David Christian proposed that the sack of Sarkel came after the destruction of Atil.
Although Ibn Haukal reports the sack of Samandar by Sviatoslav, the Rus' leader did not bother to occupy the Khazar heartlands north of the Caucasus Mountains permanently. On his way back to Kiev, Sviatoslav chose to strike against the Ossetians and force them into subservience. Therefore, Khazar successor statelets continued their precarious existence in the region. The destruction of Khazar imperial power paved the way for Kievan Rus' to dominate north-south trade routes through the steppe and across the Black Sea, routes that formerly had been a major source of revenue for the Khazars. Moreover, Sviatoslav's campaigns led to increased Slavic settlement in the region of the Saltovo-Mayaki culture, greatly changing the demographics and culture of the transitional area between the forest and the steppe.
Campaigns in the Balkans.
The annihilation of Khazaria was undertaken against the background of the Rus'-Byzantine alliance, concluded in the wake of Igor's Byzantine campaign in 944. Close military ties between the Rus' and Byzantium are illustrated by the fact, reported by John Skylitzes, that a Rus' detachment accompanied Byzantine Emperor Nikephoros Phokas in his victorious naval expedition to Crete.
In 967 or 968, Nikephoros sent to Sviatoslav his agent, Kalokyros, with the task of talking Sviatoslav into assisting him in a war against Bulgaria. Sviatoslav was paid 15,000 pounds of gold and set sail with an army of 60,000 men, including thousands of Pecheneg mercenaries.
Sviatoslav defeated the Bulgarian ruler Boris II and proceeded to occupy the whole of northern Bulgaria. Meanwhile, the Byzantines bribed the Pechenegs to attack and besiege Kiev, where Olga stayed with Sviatoslav's son Vladimir. The siege was relieved by the "druzhina" of Pretich, and immediately following the Pecheneg retreat, Olga sent a reproachful letter to Sviatoslav. He promptly returned and defeated the Pechenegs, who continued to threaten Kiev.
Sviatoslav refused to turn his Balkan conquests over to the Byzantines, and the parties fell out as a result. To the chagrin of his boyars and his mother (who died within three days after learning about his decision), Sviatoslav decided to move his capital to Pereyaslavets in the mouth of the Danube due to the great potential of that location as a commercial hub. In the Primary Chronicle record for 969, Sviatoslav explains that it is to Pereyaslavets, the centre of his lands, "all the riches flow: gold, silks, wine, and various fruits from Greece, silver and horses from Hungary and Bohemia, and from Rus' furs, wax, honey, and slaves".
In summer 969, Sviatoslav left Rus' again, dividing his dominion into three parts, each under a nominal rule of one of his sons. At the head of an army that included Pecheneg and Magyar auxiliary troops, he invaded Bulgaria again, devastating Thrace, capturing the city of Philippopolis, and massacring its inhabitants. Nikephoros responded by repairing the defenses of Constantinople and raising new squadrons of armored cavalry. In the midst of his preparations, Nikephoros was overthrown and killed by John Tzimiskes, who thus became the new Byzantine emperor.
John Tzimiskes first attempted to persuade Sviatoslav into leaving Bulgaria, but he was unsuccessful. Challenging the Byzantine authority, Sviatoslav crossed the Danube and laid siege to Adrianople, causing panic on the streets of Constantinople in summer 970. Later that year, the Byzantines launched a counteroffensive. Being occupied with suppressing a revolt of Bardas Phokas in Asia Minor, John Tzimiskes sent his commander-in-chief, Bardas Skleros, who defeated the coalition of Rus', Pechenegs, Magyars, and Bulgarians in the Battle of Arcadiopolis. Meanwhile, John, having quelled the revolt of Bardas Phokas, came to the Balkans with a large army and promoting himself as the liberator of Bulgaria from Sviatoslav, penetrated the impracticable mountain passes and shortly thereafter captured Marcianopolis, where the Rus' were holding a number of Bulgar princes hostage.
Sviatoslav retreated to Dorostolon, which the Byzantine armies besieged for sixty-five days. Cut off and surrounded, Sviatoslav came to terms with John and agreed to abandon the Balkans, renounce his claims to the southern Crimea, and return west of the Dnieper River. In return, the Byzantine emperor supplied the Rus' with food and safe passage home. Sviatoslav and his men set sail and landed on Berezan Island at the mouth of the Dnieper, where they made camp for the winter. Several months later, their camp was devastated by famine, so that even a horse's head could not be bought for less than a half-grivna, reports the Kievan chronicler of the Primary Chronicle. While Sviatoslav's campaign brought no tangible results for the Rus', it weakened the Bulgarian statehood and left it vulnerable to the attacks of Basil the Bulgar-Slayer four decades later.
Death and aftermath.
Fearing that the peace with Sviatoslav would not endure, the Byzantine emperor induced the Pecheneg khan Kurya to kill Sviatoslav before he reached Kiev. This was in line with the policy outlined by Constantine VII Porphyrogenitus in "De Administrando Imperio" of fomenting strife between the Rus' and the Pechenegs. According to the Slavic chronicle, Sveneld attempted to warn Sviatoslav to avoid the Dnieper rapids, but the prince slighted his wise advice and was ambushed and slain by the Pechenegs when he tried to cross the cataracts near Khortitsa early in 972. The Primary Chronicle reports that his skull was made into a chalice by the Pecheneg khan.
Following Sviatoslav's death, tensions between his sons grew. A war broke out between his legitimate sons, Oleg and Yaropolk, in 976, at the conclusion of which Oleg was killed. In 977 Vladimir fled Novgorod to escape Oleg's fate and went to Scandinavia, where he raised an army of Varangians and returned in 980. Yaropolk was killed, and Vladimir became the sole ruler of Kievan Rus'.
Art and literature.
Sviatoslav has long been a hero of Belarusian, Russian, and Ukrainian patriots due to his great military successes. His figure first attracted attention of Russian artists and poets during the Russo-Turkish War (1768–1774), which provided obvious parallels with Sviatoslav's push towards Constantinople. Russia's southward expansion and the imperialistic ventures of Catherine II in the Balkans seemed to have been legitimized by Sviatoslav's campaigns eight centuries earlier.
Among the works created during the war was Yakov Knyazhnin's tragedy "Olga" (1772). The Russian playwright chose to introduce Sviatoslav as his protagonist, although his active participation in the events following Igor's death is out of sync with the traditional chronology. Knyazhnin's rival Nikolai Nikolev (1758–1815) also wrote a play on the subject of Sviatoslav's life. Ivan Akimov's painting "Sviatoslav's Return from the Danube to Kiev" (1773) explores the conflict between military honour and family attachment. It is a vivid example of Poussinesque rendering of early medieval subject matter.
Interest in Sviatoslav's career waned in the 19th century. Klavdiy Lebedev depicted an episode of Sviatoslav's meeting with Emperor John in his well-known painting, while Eugene Lanceray sculpted an equestrian statue of Sviatoslav in the early 20th century. Sviatoslav appears in the 1913 poem of Velimir Khlebnikov "Written before the war" (#70. Написанное до войны) as an epitome of militant Slavdom:
Sviatoslav is the villain of the novel "The Lost Kingdom, or the Passing of the Khazars", by Samuel Gordon, a fictionalized account of the destruction of Khazaria by the Rus'. The Slavic warrior figures in a more positive context in the story "Chernye Strely Vyaticha" by Vadim Viktorovich Kargalov; the story is included in his book "Istoricheskie povesti".
In 2005, reports circulated that a village in the Belgorod region had erected a monument to Sviatoslav's victory over the Khazars by the Russian sculptor Vyacheslav Klykov. The reports described the 13-meter tall statue as depicting a Rus' cavalryman trampling a supine Khazar bearing a Star of David and Kolovrat. This created an outcry within the Jewish community of Russia. The controversy was further exacerbated by Klykov's connections with Pamyat and other anti-Semitic organizations, as well as by his involvement in the "letter of 500", a controversial appeal to the Prosecutor General to review all Jewish organizations in Russia for extremism. The Press Center of the Belgorod Regional Administration responded by stating that a planned monument to Sviatoslav had not yet been constructed but would show "respect towards representatives of all nationalities and religions." When the statue was unveiled, the shield bore a twelve-pointed star.
Svyatoslav is the main character of the books "Knyaz" ("Князь") and "The Hero" ("Герой"), written by Russian writer Alexander Mazin.
On 7 November 2011 Ukrainian fisherman Sergei Pjankow fished up a one metre long frankish sword from the waters of the Dnieper not far from the spot where Svyatoslav is believed to have been killed in 972. The handle is made out of four different metals including gold and silver, and it is very possible that it belonged to Sviatoslav himself.
Notes.
</dl>

</doc>
<doc id="40608" url="http://en.wikipedia.org/wiki?curid=40608" title="William Jennings Bryan">
William Jennings Bryan

William Jennings Bryan (March 19, 1860 – July 26, 1925) was a dominant force in the populist wing of the Democratic Party, standing three times as the Party's candidate for President of the United States (1896, 1900 and 1908). He served two terms as a member of the United States House of Representatives from Nebraska and was United States Secretary of State under President Woodrow Wilson (1913–1915), resigning because of his pacifist position on World War I. Bryan was a devout Presbyterian, a strong advocate of popular democracy, and an enemy of the banks and their gold standard. He demanded "Free Silver" because it reduced power attributed to money and put more money in the hands of the people. He was a peace advocate, a supporter of Prohibition, and an opponent of Darwinism on religious and humanitarian grounds. With his deep, commanding voice and wide travels, he was one of the best-known orators and lecturers of the era. Because of his faith in the wisdom of the common people, he was called "The Great Commoner."
In the intensely fought 1896 and 1900 elections, he was defeated by William McKinley but retained control of the Democratic Party. With over 500 speeches in 1896, Bryan invented the national stumping tour, in an era when other presidential candidates stayed home. In his three presidential bids, he promoted Free Silver in 1896, anti-imperialism in 1900, and trust-busting in 1908, calling on Democrats to fight the trusts (big corporations) and big banks, and embrace anti-elitist ideals of republicanism. President Wilson appointed him Secretary of State in 1913, but Wilson's strong demands on Germany after the "Lusitania" was torpedoed in 1915 caused Bryan to resign in protest. After 1920 he was a supporter of Prohibition and attacked Darwinism and evolution, most famously at the Scopes Trial in 1925. Five days after the end of the case, he died in his bathtub.
Background and early career: 1860–1896.
William Jennings Bryan was born in Salem, Illinois on March 19, 1860, to Silas Lillard Bryan and Mariah Elizabeth (Jennings) Bryan.
Bryan's mother was of English heritage. Mary Bryan joined the Salem Baptists in 1872, so Bryan attended Methodist services on Sunday morning, and in the afternoon, Baptist services. At this point, William began spending his Sunday afternoons at the Cumberland Presbyterian Church. At age 14, Bryan attended a revival, was baptized, and joined the Cumberland Presbyterian Church. In later life, Bryan said the day of his baptism was the most important day in his life, but at the time it caused little change in his daily routine. He later left the Cumberland Presbyterian Church and joined the larger Presbyterian Church in the United States of America.
His father, Silas Bryan, of Scots-Irish and English ancestry, was an avid Jacksonian Democrat. Silas won election to the Illinois State Senate, but was defeated for re-election in 1860. He won election as a state circuit judge, and in 1866 moved to a 520 acre farm north of Salem, living in a ten-room house that was the envy of Marion County.
Until age ten, Bryan was home-schooled, finding in the Bible and McGuffey Readers support for his views that gambling and liquor were evil and sinful. To attend Whipple Academy, which was attached to Illinois College, Bryan was sent to Jacksonville, Illinois in 1874.
Following high school, he entered Illinois College, graduating as valedictorian in 1881. During his time at Illinois College, Bryan was a member of the Sigma Pi literary society and Acacia (fraternity). He studied law at Union Law College in Chicago (which later became Northwestern University School of Law). While preparing for the bar exam, he taught high school and met Mary Elizabeth Baird, a cousin of William Sherman Jennings, the latter of whom was also his own first-cousin. The two were married on October 1, 1884, and they settled in Jacksonville, which at the time had a population of two thousand.
Mary became a lawyer and collaborated with him on all his speeches and writings. He practiced law in Jacksonville from 1883 to 1887, then moved to the boom city of Lincoln, Nebraska. In Lincoln, Bryan met James Dahlman and they became lifelong friends. As chairman of the Nebraska Democratic Party, Dahlman would help carry Nebraska for Bryan in two presidential campaigns. Even when Dahlman became closely associated with Omaha's vice elements, including the breweries as the city's eight-term mayor, he and Bryan maintained a collegial relationship.
In the Democratic landslide of 1890, Bryan was elected to the U.S. House of Representatives, from Nebraska's First Congressional District. The growing prohibitionist movement entered the election of 1890 with its own slate of candidates. In the three-way race in the First Congressional District, Bryan received 6,713 more votes than his nearest opponent. This was a plurality of the vote and was 8,000 votes short of a majority of the vote. Nonetheless, Bryan was elected and was only the second Democrat to be elected to Congress in the history of Nebraska. However in his re-election race in 1892, Bryan was re-elected by a 140-vote majority in a two-person race. He ran for the Senate in 1894, but a Republican landslide led to the state Legislature's choice of a Republican for the Senate seat.
First campaign for the White House: 1896.
Bryan had an innate talent in oratory. He gave speeches, organized meetings, and adopted resounding resolutions that eventually culminated in the founding of the American Bimetallic League, which then evolved into the National Bimetallic Union, and finally the National Silver Committee. At the time many farmers' groups believed that by increasing the amount of currency in circulation, commodities would receive higher prices. They were opposed by banks and bond holders who feared the effects of inflation. The ultimate goal of the league was to garner support on a national level for the reinstatement of the coinage of silver. With others, he made certain that the Democratic platform reflected the now strengthening spirit of Midwestern populism. With his support, Charles H. Jones of the "St. Louis Post-Dispatch" was put on the platform committee and Bryan's "sixteen-to-one" plank for free silver was adopted and silently added to the platform for the 1896 Democratic National Convention in Chicago, in order to avoid controversy. As a minority member of the resolutions committee, Bryan was able to push the Democratic Party from its laissez-faire and small-government roots towards its modern, liberal character. Through these measures, the public and influential Democrats became convinced of his capacity to lead and bring change, resulting in his being mentioned as a possible chairman for the Chicago convention.
In 1893, the repeal of the Sherman Silver Purchase Act had resulted in the collapse of the silver market. Bryan delivered speeches across the country for free silver from 1894 to 1896, building a grass-roots reputation as a powerful champion of the cause.
At the 1896 Democratic National Convention, Bryan lambasted Eastern moneyed classes for supporting the gold standard at the expense of the average worker. His "Cross of Gold" speech made him the sensational new face in the Democratic Party. That same year he became the first presidential candidate to campaign in a car (a donated Mueller) in Decatur, Illinois.
The Bourbon Democrats who supported incumbent Democratic President Grover Cleveland were defeated, and the party's agrarian and silver factions voted for Bryan, giving him the nomination of the Democratic Party. At the age of 36, Bryan became (and still remains) the youngest presidential nominee of a major party in American history.
Disappointed with the direction of their party, Gold Democrats invited Cleveland to run as a third-party candidate, but he declined. Cleveland did, however, support John M. Palmer, nominee of the Gold Democrats, rather than Bryan.
Bryan also formally received the nominations of the Populist Party and the Silver Republican Party. With the three nominations, voters from any party could vote for Bryan without crossing party lines. In 1896, the Populists rejected Bryan's Democratic running mate, Maine banker Arthur Sewall, and named as his running mate Georgia Populist Thomas E. Watson. People could vote for Bryan and Sewall (on the Democratic or Silver Republican lines), or for Bryan and Watson (on the People's Party line).
The Republicans nominated William McKinley on a platform calling for prosperity for everyone through industrial growth, high tariffs, and "sound money" (gold). Republicans ridiculed Bryan as a Populist. However, "Bryan's reform program was not based on the Populists--for example, the Populists Free Silver took Free Silver from Bryan's Democrats, not vice versa--but he used the same crusading rhetoric against railroads, banks, insurance companies and businesses that has often been mistaken for Populism. Bryan remained a staunch Democrat throughout his career." The popular political economist and social reformer Henry George became an influence on Bryan's thinking and campaigned on his behalf.
Bryan demanded Bimetallism and "Free Silver" at a ratio of 16:1. Most leading Democratic newspapers rejected his candidacy. Despite this rejection by the newspapers, Bryan won the Democratic vote.
Republicans discovered in August that Bryan was solidly ahead in the South and West, but far behind in the Northeast. He appeared to be ahead in the Midwest, so the Republicans concentrated their efforts there. They said Bryan was a madman, a religious fanatic surrounded by anarchists, who would wreck the economy. By late September, the Republicans felt they were ahead in the decisive Midwest and began emphasizing that McKinley would bring prosperity to all Americans. McKinley scored solid gains among the middle classes, factory and railroad workers, prosperous farmers, and the German Americans who rejected free silver. Bryan gave 500 speeches in 27 states. McKinley won by a margin of 271 to 176 in the electoral college, but the popular vote was much closer and in some key states, McKinley's margin of victory was narrow.
War and peace: 1898–1900.
With the outbreak of the Spanish–American War in 1898, Bryan was forced to consider his party's stance on foreign policy. On one hand, Bryan was critical of militarism. Yet Spain's suppression of Cuban and Filipino self-government movements went against his view of his country's “Global Mission,” in which he envisioned the United States spreading democracy to the rest of the world. With this idealism in mind, Bryan enthusiastically threw in his support for President McKinley's declaration of war against Spain. According to historian William Leuchtenburg, "few political figures exceeded the enthusiasm of William Jennings Bryan for the Spanish war." Bryan argued that "universal peace cannot come until justice is enthroned throughout the world. Until the right has triumphed in every land and love reigns in every heart, government must, as a last resort, appeal to force." He volunteered for duty and became colonel of a Nebraska militia regiment. He contracted typhoid fever in Florida and stayed there to recuperate, never seeing combat.
Bryan surprised many of his fellow party members by supporting the ratification of the Treaty of Paris, which resulted from the United States' defeat of Spain. The treaty granted the United States control of Puerto Rico, Guam, Cuba, the Philippines and parts of the West Indies. Many of Bryan's supporters were opposed to what they perceived as Republican aspirations of turning the country into an imperial power and criticized Bryan for hypocritically supporting the ratification of the treaty. Bryan justified his decision to support the treaty by arguing that the issue of imperialism should be decided upon by the American people at the ballot boxes and not in Congress. However, when the Bacon Resolution (a proposed supplement to the Treaty of Paris which would allow the Filipinos a “stable and independent government") failed to pass, Bryan began publicly speaking out against the Republican's imperial aspirations.
Bryan gave a speech at the Democratic National Convention in 1900 simply titled "Imperialism." In this speech he discusses his views against the annexation of the Philippines, questioning the United States' right to overpower people of another country just for a military base. He mentions, at the beginning of the speech, that the United States should not try to emulate the imperialism of Great Britain and other European countries.
He stated that America does not need to follow into the footsteps of Great Britain.
Presidential election of 1900.
He ran as an anti-imperialist in 1900, finding himself in alliance with Andrew Carnegie, as well as others who had fought against silver. Republicans mocked Bryan as indecisive, or a coward, a point which L. Frank Baum satirized viciously in the Bryan-like Cowardly Lion in "The Wonderful Wizard of Oz", published in the spring of 1900.
Bryan combined anti-imperialism with free silver, saying:
"The nation is of age and it can do what it pleases; it can spurn the traditions of the past; it can repudiate the principles upon which the nation rests; it can employ force instead of reason; it can substitute might for right; it can conquer weaker people; it can exploit their lands, appropriate their property and kill their people; but it cannot repeal the moral law or escape the punishment decreed for the violation of human rights."
In a typical day he gave four hour-long speeches and shorter talks that added up to six hours of speaking. At an average rate of 175 words a minute, he turned out 63,000 words a day, enough to fill 52 columns of a newspaper. In Wisconsin, he once made 12 speeches in 15 hours.
Despite Bryan's tremendous energy, McKinley and the Republicans were too strong to defeat. The GOP invested ten times as much money into the campaign as did Bryan's Democratic Party. While Bryan declared “Imperialism to be the paramount issue,” he had difficulty differentiating his platform from that of the Republican party. While he argued for his country to take on the role of a protectorate to the Philippines, the Republicans argued that annexation would eventually lead to independence. With the issue of imperialism being defined in these vaguely similar terms, the Republicans' “full pale” platform of a strong American industrial economy proved to be more important to voters than the morality of annexing the Philippines. He held his base in the South, but lost part of the West as McKinley retained the Northeast and Midwest and rolled up a comfortable margin of victory. McKinley won the electoral college with a count of 292 votes compared to Bryan's 155. Bryan's hold on his party was weakened, while his erstwhile allies the Populists had virtually disappeared from the arena.
Presidential election of 1908.
The 1908 election was Bryan’s third attempt at gaining the presidency. The Democrats nominated Bryan by a wide margin at the Democratic convention held in Denver and decided on John Kern, a politician from Indiana, as his running mate. Bryan ran against the Republicans, and Theodore Roosevelt’s hand-picked nominee William Howard Taft.
Bryan launched a special message to Congress, suggesting income and inheritance taxes, publicity on campaign contribution and opposing the use of the navy for the collection of private debts. He campaigned against corporate domination, urging that all corporation contributions be made public before election day, and that failure to cooperate be made a penal offense.
The GOP ran its campaign on the benefits of the Roosevelt administration, creation of a postal service, continuation of "Sound Currency", citizenship for Puerto Rico inhabitants, regulation on big business, and tariff revision in protectionist mode.
Bryan and the Democrats’ platform denounced the wrongs done by the Republican party: Congress spent too much money; Roosevelt hand picked Taft in undemocratic fashion; Republicans wanted centralization; Republicans favored monopolies. In response, Bryan unleashed the slogan, "Shall the People Rule?" In a time of peace and prosperity, and Republican trust-busting, Bryan fared poorly among the voters. He lost the electoral college 321 to 162, his worst defeat yet, and did not carry any of the states in the Northeast.
In his three presidential election bids, Bryan received a total of 493 electoral votes - the most of any candidate in American history who never won the presidency.
Chautauqua circuit: 1900–1912.
Following his defeat in the election of 1900, Bryan needed money, and his powerful voice and 100% name recognition were assets that could be capitalized. For the next 25 years, Bryan was the most popular speaker on the Chautauqua circuit, delivering thousands of paid speeches on current events in hundreds of towns and cities across the country, even while serving as Secretary of State. He usually charged $500 per speech in addition to a percentage of the profits. He mostly spoke about Christianity, but covered a wide variety of topics. His most popular lecture (and his personal favorite) was a lecture entitled "The Prince of Peace", which stressed that Christian theology was the solid foundation of morality, and individual and group morality was the foundation for peace and equality. Another famous lecture from this period, "The Value of an Ideal", was a stirring call to public service.
In a 1905 speech, Bryan warned that "the Darwinian theory represents man reaching his present perfection by the operation of the law of hate, the merciless law by which the strong crowd out and kill off the weak. If this is the law of our development then, if there is any logic that can bind the human mind, we shall turn backward to the beast in proportion as we substitute the law of love. I choose to believe that love rather than hatred is the law of development."
Bryan threw himself into the work of the Social Gospel. He served in organizations containing a large number of theological liberals—he sat on the temperance committee of the Federal Council of Churches and on the general committee of the short-lived Inter-church World Movement.
In 1899 Bryan founded a weekly magazine, "The Commoner", calling on Democrats to dissolve the trusts, regulate the railroads more tightly, and support the Progressive Movement. He regarded prohibition as a "local" issue and did not endorse it until 1910. In London in 1906, he presented a plan to the Inter-Parliamentary Peace Conference for arbitration of disputes that he hoped would avert warfare. He tentatively called for nationalization of the railroads, then backtracked and called only for more regulation. His party nominated Bourbon Democrat Alton B. Parker in 1904, who lost to Roosevelt. For two years following this defeat, Bryan would pursue his public speaking ventures on an international stage. From 1904 to 1906, Bryan traveled globally, preaching, sightseeing with his wife Mary, lecturing, and all while escaping the political upheaval in Washington. Bryan crusaded as well for the initiative and referendum, making a whistle-stop campaign tour of Arkansas in 1910. Bryan's speech to the students of Washington and Lee University began the Washington and Lee Mock Convention.
Bryan owned land in Nebraska and a 240 acre ranch in Texas; both were paid for with earnings from speeches and "The Commoner."
Secretary of State: 1913–1915.
For supporting Woodrow Wilson for the presidency in 1912, Bryan was appointed Secretary of State, the top cabinet position. For all his enormous influence in the Democratic Party, the only powerful office he ever held was his two years as Secretary of State. Historian Richard Hofstadter comments:
Wilson took his measure and only nominally consulted him, making all the major foreign policy decisions from the White House. In the civil war in Mexico in 1914, Bryan supported American military intervention.
Bryan kept busy in 1913-1915, negotiating 28 treaties that promised arbitration of disputes before war broke out between the signatory countries and the United States. He made several attempts to negotiate a treaty with Germany, but ultimately was never able to succeed. The agreements, known officially as "Treaties for the Advancement of Peace," set up procedures for conciliation rather than for arbitration. In September 1914 he wrote President Wilson urging mediation in the World War that had just begun in Europe, with the U.S. as the largest neutral:
It is not likely that either side will win so complete a victory as to be able to dictate terms, and if either side does win such a victory it will probably mean preparation for another war. It would seem better to look for a more rational basis for peace.
Bryan tried to yoke the American credit to the Entente, saying "money is the worst of all contrabands because it commands everything else" but eventually yielded. He also pointed out that by traveling on British vessels "an American citizen can, by putting his own business above his regard for this country, assume for his own advantage unnecessary risks and thus involve his country in international complications" Wilson's demands for "strict accountability for any infringement of [American] rights, intentional or incidental" after the sinking of the "Lusitania" troubled Bryan, who counseled an “evenhanded policy.” Bryan resigned in June 1915, protesting “… why be so shocked by the drowning of a few people, if there is to be no objection to starving a nation.”
Despite their differences, Bryan campaigned as a private citizen for Wilson's reelection in 1916. When war was declared in April 1917, Bryan wrote Wilson, "Believing it to be the duty of the citizen to bear his part of the burden of war and his share of the peril, I hereby tender my services to the Government. Please enroll me as a private whenever I am needed and assign me to any work that I can do." Wilson, however, did not allow the 57-year-old Bryan to rejoin the military, and did not offer him any wartime role.
Prohibition battles: 1916–1925.
Bryan campaigned for the Constitutional amendments on prohibition and women's suffrage. Partly to avoid Nebraska ethnics such as the German-Americans who were "wet" and opposed to prohibition, Bryan moved to Coconut Grove in Miami, Florida in 1913. He called his home on Brickell Avenue "Villa Serena". Later, in 1925, he moved to a new home further south in Coconut Grove on Main Highway called "Marymont". Bryan filled lucrative speaking engagements, including playing the part of spokesman for George E. Merrick's new planned community Coral Gables, addressing large crowds across a Venetian pool for an annual salary of over $100,000. He was also extremely active in Christian organizations. Bryan refused to support the 1920 Democratic presidential nominee, James M. Cox, because he deemed Cox not dry enough. As one biographer explains,
Bryan's national campaigning helped Congress pass the 18th Amendment in 1918, which shut down all saloons as of 1920. But while prohibition was in effect, Bryan did not work to secure better enforcement. He opposed a highly controversial resolution at the 1924 convention, condemning the Ku Klux Klan, expecting it would soon fold. Bryan disliked the Klan but never publicly attacked it. For the nomination in 1924, he opposed the wet Al Smith; Bryan's younger brother, Nebraska Governor Charles W. Bryan, was put on the ticket with John W. Davis as candidate for vice president to keep the Bryanites in line. Bryan was very close to his brother and endorsed him for the vice presidency.
Bryan was the chief proponent of the Harrison Narcotics Tax Act, the precursor to the modern War on Drugs. However, he argued for the act's passage more as an international obligation than on moral grounds.
After his resignation as Secretary of State, until his death, Bryan became an active promoter of Florida real estate, and lived in the Miami area during the colder months. His promotions (in print, speeches and even radio talks) may have played a role in setting the 1920s Florida real estate boom in motion, and "The Great Commoner" Bryan ironically became rich from his real estate investments. The Florida boom collapsed within months after Bryan's death in 1925.
Anti-evolution activism: 1918–1925.
Before World War I, Bryan believed moral progress could achieve equality at home and, in the international field, peace between all the world's nations.
Bryan opposed the theory of evolution for two reasons. First, he believed that what he considered a materialistic account of the descent of man through evolution undermined the Bible. Second, he saw Darwinism applied to society as a great evil force in the world promoting hatred and conflicts, especially the World War.
In his famous Chautauqua lecture, "The Prince of Peace," (1909) Bryan warned that the theory of evolution could undermine the foundations of morality. He concluded, "while I do not accept the Darwinian theory I shall not quarrel with you about it," and, evoking, the design argument, he said "I have a right to assume, and I prefer to assume, a Designer back of the design— a Creator back of the creation."
One book Bryan read at this time convinced him that Darwinism emphasizing the struggle of races had undermined morality in Germany. Bryan was heavily influenced by Vernon Kellogg's 1917 book, "Headquarters Nights: A Record of Conversations and Experiences at the Headquarters of the German Army in Belgium and France", which asserted (on the basis of a conversation with a reserve officer he called "Professor von Flussen") that German intellectuals were totally committed to might-makes-right due to "whole-hearted acceptance of the worst of Neo-Darwinism, the "Allmacht" of natural selection applied to human life and society and "Kultur"."
Bryan also read "The Science of Power" (1918) by British social theorist Benjamin Kidd, which credited the philosophy of Friedrich Nietzsche with German nationalism, materialism, and militarism, which in turn was the outworking of the social Darwinian hypothesis.
In 1920, Bryan told the World Brotherhood Congress that the theory of evolution was "the most paralyzing influence with which civilization has had to deal in the last century" and that Nietzsche, in carrying the theory of evolution to its logical conclusion, "promulgated a philosophy that condemned democracy... denounced Christianity... denied the existence of God, overturned all concepts of morality... and endeavored to substitute the worship of the superhuman for the worship of Jehovah."
By 1921, Bryan saw Darwinism as a major internal threat to the US. The major study which seemed to convince Bryan of this was James H. Leuba's "The Belief in God and Immortality, a Psychological, Anthropological and Statistical Study" (1916). In this study, Leuba shows that during four years of college a considerable number of college students lost their faith. Bryan was horrified that the next generation of American leaders might have the degraded sense of morality which he believed had prevailed in Germany and caused the Great War. Bryan then launched an anti-evolution campaign.
The campaign kicked off in October 1921, when the Union Theological Seminary in Richmond, Virginia invited Bryan to deliver the James Sprunt Lectures. At its heart was a lecture entitled "The Origin of Man", in which Bryan asked, "what is the role of man in the universe and what is the purpose of man?" For Bryan, the Bible was absolutely central to answering this question, and moral responsibility and the spirit of brotherhood could only rest on belief in God.
The Sprunt lectures were published as "In His Image", and sold over 100,000 copies, while "The Origin of Man" was published separately as "The Menace of the theory of evolution" and also sold very well.
Bryan was worried that the theory of evolution was making grounds not only in the universities, but also within the church itself. Many colleges were still church-affiliated at this point. The developments of 19th century liberal theology, and higher criticism in particular, had left the door open to the point where many clergymen were willing to embrace the theory of evolution and claimed that it was not contradictory with their being Christians. Determined to put an end to this, Bryan, who had long served as a Presbyterian elder, decided to run for the position of Moderator of the General Assembly of the Presbyterian Church in the USA, which was at the time embroiled in the Fundamentalist-Modernist Controversy. (Under Presbyterian church governance, clergy and laymen are equally represented in the General Assembly, and the post of Moderator is open to any member of the General Assembly.) Bryan's main competition in the race was the Rev. Charles F. Wishart, president of the College of Wooster, who had loudly endorsed the teaching of the theory of evolution in the college. Bryan lost to Wishart by a vote of 451-427. Bryan then failed in a proposal to cut off funds to schools where the theory of evolution was taught. Instead, the General Assembly announced disapproval of materialistic (as opposed to theistic) evolution.
In his efforts to bring some publicity to his cause, Bryan joined the American Association for the Advancement of Science in 1924 and attended the annual meeting. A featured session at the meeting was a debate on biological evolution between Bryan and Edward Loranus Rice, a developmental biologist from the Methodist-associated Ohio Wesleyan University.
According to historian Ronald L. Numbers, Bryan was not nearly as much of a fundamentalist as many modern-day creationists. Instead he is more accurately described as a "day-age creationist". Numbers says Bryan, "not only read the Mosaic "days" as geological "ages" but allowed for the possibility of organic evolution—so long as it did not impinge on the supernatural origin of Adam and Eve."
Scopes trial: 1925.
Bryan actively lobbied for state laws banning public schools from teaching evolution. The legislatures of several Southern states proved more receptive to his anti-evolution message than the Presbyterian Church had been, and passed such laws after Bryan addressed them. A prominent example was the Butler Act of 1925, which made it unlawful in Tennessee to teach that mankind evolved from lower life forms.
Bryan's participation in the highly publicized 1925 Scopes Trial served as a capstone to his career. He was asked by William Bell Riley to represent the World Christian Fundamentals Association as counsel at the trial. During the trial, Bryan took the stand and was questioned by defense lawyer Clarence Darrow about his views on the Bible. "Asked when the Flood occurred, Bryan consulted "Ussher's Bible Concordance", and gave the date as 2348 B.C., or 4273 years ago. Did not Bryan know, asked Darrow, that Chinese civilization had been traced back at least 7000 years?" Bryan conceded that he did not. When he was asked if the records of any other religion made mention of a flood at the time he cited, Bryan replied: "The Christian religion has always been good enough for me - I never found it necessary to study any competing religion." 
The national media reported the trial in great detail, with H. L. Mencken ridiculing Bryan as a symbol of Southern ignorance (despite his not being from the South) and anti-intellectualism. In a more humorous vein, satirist Richard Armour stated in "It All Started With Columbus" that Darrow had "made a monkey out of" Bryan due to Bryan's ignorance of the Bible.
After the judge retroactively expunged all of Bryan's answers to Darrow's questions, both sides closed without summation. The jury quickly returned a guilty verdict with the defense's encouragement, and Bryan won the case. However, the state Supreme Court reversed the verdict on a technicality and Scopes went free.
Bryan linked Darwinism to what he considered to be the might-makes-right philosophy of Friedrich Nietzsche. Bryan believed that such thinking served not so much as an explanation for injustice but more as an excuse for injustice, particularly in the areas of harming the weak and waging war. In contrast, Bryan was influenced by the social philosophy of Leo Tolstoy, Christian humanitarian and pacifist.
Death.
In the days following the Scopes trial Bryan traveled hundreds of miles, delivering speeches in multiple towns. On Sunday, July 26, 1925, he returned from Chattanooga, Tennessee to his home in Dayton. After attending church services he ate a large meal, then died during a nap that afternoon, five days after the trial's conclusion. When someone remarked to Darrow that Bryan died from a "broken heart", Darrow responded, "Broken heart, hell, he died of a busted belly!" Journalist H.L. Mencken, who disliked Bryan intensely, reportedly boasted to Darrow that "we killed the son of a bitch". 
Bryan is buried in Arlington National Cemetery. His headstone reads, "Statesman. Yet Friend To Truth! Of Soul Sincere. In Action Faithful. And In Honor Clear". He was survived by his daughter, Congresswoman Ruth Bryan Owen, and her four children: John Bryan Leavitt and Ruth Leavitt, by her first husband, Newport, Rhode Island artist William Homer Leavitt; and two children by her second husband, British Royal Engineers officer Reginald A. Owen.
Ruth's eldest son John Bryan Leavitt, who had been adopted by his grandfather after his parents' divorce, became a poet and an actor, working professionally as John Bryan.
Bryan College, a Christian-oriented institution, opened in 1930 in Dayton as a lasting memorial to Bryan.
Popular image.
L. Frank Baum satirized Bryan as the Cowardly Lion in "The Wonderful Wizard of Oz", published in 1900. Baum had been a Republican activist in 1896 and wrote on McKinley's behalf.
"Inherit the Wind", a 1955 play by Jerome Lawrence and Robert Edwin Lee, is a highly fictionalized account of the Scopes Trial written in response to McCarthyism. A populist thrice-defeated Presidential candidate from Nebraska named Matthew Harrison Brady comes to a small town named Hillsboro in Tennessee to help prosecute a young teacher for teaching evolution to his schoolchildren. He is opposed by a famous trial lawyer, Henry Drummond (based on Darrow), and mocked by a cynical newspaperman (based on H.L. Mencken) as the trial assumes a national profile. "Inherit the Wind" is also a 1960 Hollywood film adaptation of the play of the same name, written by Jerome Lawrence and Robert Edwin Lee, directed by Stanley Kramer. It stars Spencer Tracy as lawyer Henry Drummond and Fredric March as his friend and rival Matthew Harrison Brady.
Bryan also appears as a character in Douglas Moore's 1956 opera "The Ballad of Baby Doe" and is briefly mentioned in John Steinbeck's "East of Eden". In addition, he is a (very) minor character in Thomas Wolfe's "Look Homeward, Angel". His death is referred to in Ernest Hemingway's "The Sun Also Rises". In Robert A. Heinlein's "", Bryan's unsuccessful or successful runs for the presidency are seen as the "splitting off" events of the alternate histories through which the protagonists travel.
He also has a biographical part in "The 42nd Parallel" in John Dos Passos' "USA Trilogy".
In political cartoons.
The sheer volume of political propaganda cartoons featuring Bryan is a testament to the amusement and fear he caused among conservatives. Bryan campaigned tirelessly, championing the ideas of the farmers and workers, using his skills as a famed orator to ultimately reshape the Democratic Party into a more progressive one. These political cartoons attacked just about every facet of Bryan’s character and policy. They mocked his religious fervor, his campaign slogans, and even his ability to unify parties for a common cause. As Keen puts it, "The art of propaganda is to create a portrait that incarnates the idea of what we wish to destroy so we will react rather than think, and automatically focus our free-floating hostility, indistinct frustrations, and unnamed fears". Bryan embodied these fears of the Republican Party of the time, which is clearly evident in the lengths they went to deface his character in these cartoons.
The most notable cartoons are of Bryan illustrated as a snake, representing Populism, swallowing a donkey, symbolizing the Democratic Party. Another notable Bryan cartoon is one where he is standing atop a Bible, marketing the sales of a "crown of thorns" and a "cross of gold" both referencing "The Cross of Gold," his most popular speech.
Nicknames.
Bryan had an unusually high number of nicknames given to him in his lifetime; most of these were given by his loyal admirers in the Democratic Party. In addition to his best-known nickname, "The Great Commoner", he was also called "The Silver Knight of the West" (due to his support of the free silver issue) and the "Boy Orator of the Platte" (a reference to his oratorical skills and his home near the Platte River in Nebraska). A derisive nickname given by journalist H.L. Mencken, a prominent Bryan critic, was "The Fundamentalist Pope", a reference to Bryan's devout religious views. He is called "Adam-and-Eve" Bryan in "O Russet Witch!, Tales of the Jazz Age" by F. Scott Fitzgerald.
Legacy.
Michael Kazin considers Bryan the first of the 20th century "celebrity politicians", better known for their personalities and communications skills than their political views. Shannon Jones, on a Socialist website, claims that Bryan never took a principled stand against white supremacy in the Southern United States. Alan Wolfe has concluded that Bryan's "legacy remains complicated". Form and content mix uneasily in Bryan's politics. The content of his speeches leads in a direct line to the progressive reforms adopted by 20th century Democrats. But the form his actions took was a romantic invocation of the American past, a populist insistence on the wisdom of ordinary folk, and a faith-based insistence on sincerity and character.
In his book "They Also Ran", Irving Stone criticizes Bryan as an egocentric who never admitted being wrong. Stone argues that because Bryan led a privileged life, he could not feel the suffering of the common man. He asserts that Bryan only acted as a champion of common men in order to get their votes. Stone claims that none of Bryan's ideas were original, and that he did not have the brains to be an effective president. He calls Bryan one of the nation's worst Secretaries of State. He believes that, as President, Bryan would have supported many radical religious blue laws. In Stone's opinion, Bryan had one of the least disciplined minds of the 19th century, and McKinley, Roosevelt and Taft all made better Presidents than Bryan would have been.
Many prominent leaders, however, have defended Bryan and his legacy. In 1962, journalist Merle Miller interviewed former President Harry Truman. When asked about Bryan, Truman replied that he [Bryan] "was a great one—one of the greatest." Truman also claimed: "If it wasn't for old Bill Bryan, there wouldn't be any liberalism at all in the country now. Bryan kept liberalism alive, he kept it going." In 1900, Truman, aged 16, was a page at the Democratic National Convention in Kansas City. He heard Bryan speak to the delegates and was deeply impressed. In his biography of Truman, historian David McCullough wrote that in 1900 Truman and his father "declared themselves thorough 'Bryan men'... Bryan remained an idol for Harry, as the voice of the common man." Tom L. Johnson, the famed progressive mayor of Cleveland, Ohio, referred to Bryan's campaign in 1896 as "the first great struggle of the masses in our country against the privileged classes." In a 1934 speech dedicating a memorial to Bryan, President Franklin D. Roosevelt said "I think that we would choose the word 'sincerity' as fitting him [Bryan] most of all...it was that sincerity that served him so well in his life-long fight against sham and privilege and wrong. It was that sincerity which made him a force for good in his own generation and kept alive many of the ancient faiths on which we are building today. We...can well agree that he fought the good fight; that he finished the course; and that he kept the faith."
Bryan was one of the best known orators of his time. He was a fixture in the Democratic Party, and a hero to the common man. Starting with his Cross of Gold speech, Bryan brought the Populists into the Democratic Party, and with his common man message he would inevitably draw the African-American and feminist vote into the party. A strong believer in the power of government to improve people’s lives, Bryan expressed his belief to John Reed in 1916 that the government “may properly impose a minimum wage, regulate hours of labor, pass usury laws, and enforce inspection of food, sanitation and housing conditions.” Bryan became the bridge that brought different factions into the party, and paved the way for liberal Democrats such as Franklin D. Roosevelt with his New Deal legislation. As noted by Bryan's biographer Michael Kazin,
“Bryan was the first leader of a major party to argue for permanently expanding the power of the federal government to serve the welfare of ordinary Americans from the working and middle classes...he did more than any other man--between the fall of Grover Cleveland and the election of Woodrow Wilson--to transform his party from a bulwark of laissez-faire to the citadel of liberalism we identify with Franklin D. Roosevelt and his ideological descendants.”
Honors.
Bryan County, Oklahoma is named after him.
Bryan Medical Center (formerly BryanLGH Medical Center and Bryan Memorial Hospital) in Lincoln, Nebraska, Bryan College of Health Science, connected with the hospital in Lincoln, and Bryan College, located in Dayton, Tennessee, are also named for William Jennings Bryan. The William Jennings Bryan House in Nebraska was named a U.S. National Historic Landmark in 1963. A $4,000 scholarship for Creighton University students participating in Speech and Debate at the university is named after William Jennings Bryan. The Bryan Home Museum is a by-appointment only museum at his birthplace in Salem, Illinois. Salem is also home to Bryan Park and a large statue of Bryan. Omaha Bryan High School and Bryan Middle School in Bellevue, Nebraska are named for him. He is also honored by having an elementary school in Mission, Texas named after him, Bryan Elementary School on a street named after him, Bryan Street. His home at Asheville, North Carolina from 1917 to 1920, the William Jennings Bryan House, was listed on the National Register of Historic Places in 1983.
A statue of Bryan represents the state of Nebraska at the National Statuary Hall in the United States Capitol. Bryan was named to the Nebraska Hall of Fame in 1971. A bust of him was dedicated as part of the Hall of Fame in 1974 which currently resides, like other members of the hall of fame, in the Nebraska State Capitol.
He has been honored by the United States Postal Service with a $2 Great Americans series postage stamp.
In 2013, he was inducted into the Gennett Records Walk of Fame to commemorate his recording of the Cross of Gold speech.

</doc>
<doc id="40609" url="http://en.wikipedia.org/wiki?curid=40609" title="Watergate (disambiguation)">
Watergate (disambiguation)

Watergate is the Watergate scandal, a 1972 break-in at the Watergate Hotel by members of President Richard Nixon's administration and the resulting cover-up.
Watergate may also refer to:

</doc>
<doc id="40612" url="http://en.wikipedia.org/wiki?curid=40612" title="Unitatis Redintegratio">
Unitatis Redintegratio

Unitatis Redintegratio is the Second Vatican Council's Decree on Ecumenism. It was passed by a vote of 2,137 to 11 of the bishops assembled and was promulgated by Pope Paul VI on 21 November 1964. The title in Latin means "Restoration of Unity" and is from the first line of the decree, as is customary with major Catholic documents (see incipit).
Contents.
"The numbers given correspond to the section numbers within the text."
Policy on the Eastern Orthodox and Oriental Orthodox.
"Unitatis Redintegratio" calls for the reunion of Christendom and so it is not terribly different from previous calls for unity by Pope Leo XIII in the 1894 encyclical "Praeclara gratulationis publicae". However, the document articulates a different kind of ecclesiology than "Praeclara", focusing on the unity of the people of God and on separate Christian brethren instead of a classical call for schismatics to return to the fold under the unity of the Vicar of Christ.
Reformation communities.
The document acknowledges that there are serious problems facing prospects of reunion with Reformation communities that make no attempt to claim apostolic succession such as the Anglican communion does. Ecclesial communities that adhere to Calvinism are a particular case because they often have important doctrinal differences on key issues such as ecclesiology, liturgy and mariology. Other communities have insoluble doctrinal differences with Catholic Christianity because their theology of the Holy Trinity is manifestly incompatible with the doctrine of the council of Nicea in the early Church. That these serious problems are a barrier to salvation is clarified in the 2004 Vatican document, "The Decree on Ecumenism, Read Anew after Forty Years".
Separated brethren.
The concept and wording was published as late as 1793, in a discourse which examined two papal briefs to the Bishop of Chiusi-Pienza. Frank Flinn wrote, in "Encyclopedia of Catholicism", that in 1959 Pope John XXIII "addressed Protestants as separated brethren," in "Ad Petri cathedram" (APC), which Flinn saw as "an important step toward recognizing Protestants as legitimate partners in a future dialogue." But Pope Leo XIII "was the first to speak of 'separated brothers'" according to John Norman Davidson Kelly's "A Dictionary of Popes". Edward Farrugia, in "Gregorianum", describes the development from Pope Leo XIII's "Orientalium dignitas" (OD) to "Orientalium Ecclesiarum" (OE) to "Unitatis Redintegratio" (UR). "Yet if builds on , differences remain. Whereas " 186 "speaks of 'dissident bretheren' ("fratres dissidentes"), 28 speaks of 'separated bretheren' ("fratres seiunctos"), although it does not go as far as 14, where there is an inchoative use of the language of 'sister Churches' ("inter Ecclesia locales, ut inter sorores")." Farrugia noted Austin Flannery's translations in "Vatican Council II", " 29 speaks of the 'separated Churches' and 25 of 'any separated Eastern Christians', and 29 of 'Eastern separated brethren'." J. M. R. Tillard goes into detail, in "New Catholic Encyclopedia", about "the development of a carefully nuanced vocabulary, consistent with Vatican II Ecclesiology," which evolved from "the idea of membership in favor of that of incorporation" and has its categorization found in the dogmatic constitution "Lumen gentium" (LG) which Tillard describes:
"Every shade of difference in meaning among these terms is important," emphasizes Tillard. "But the terms acquire their full force only in the light of the most authoritative commentaries on them," and "Nostra aetate" (NA). "Then, supposing the nuances indicated, the richness of such expressions as the following becomes clear: 'Churches and ecclesial communities'; 'separated brethren'; 'separated Churches and ecclesial communities'; 'full communion'—'imperfect communion'."
"But thanks to its ecclesiology," wrote Tillard, "Vatican II was able to affirm at the same time that Churches or ecclesial communities separated from the Catholic Church are part of the single Church, and that nevertheless incorporation in Christ and His Church possesses within the Catholic Church the fullness that it does not have elsewhere."
In 2007, the Congregation for the Doctrine of the Faith (CDF) clarified "the authentic meaning" of the ecclesiological expression "Church" which "according to Catholic doctrine," the texts of the Second Vatican Council and those of the Magisterium since the Second Vatican Council do not call Christian Communities born out of the Reformation of the 16th century as "Churches" because "these Communities do not enjoy apostolic succession in the sacrament of Orders, and are, therefore, deprived of a constitutive element of the Church." William Whalen wrote, in "Separated Brethren", that "'separated brethren' refers to Christians united by baptism and committed to Jesus Christ but divided by theological beliefs."(p9) Whalen explained, that Protestant Reformation Christians broke "the bond of common faith" and "they became separated brethren."(p11)
"All Christians who are baptized and believe in Christ but are not professed Catholics" are separated brethren, according to John Hardon in "Modern Catholic Dictionary". "More commonly the term is applied to Protestants." Likewise, "separated brethren" according to Catholic Answers, in "This Rock", "refers to those who, though separated from full communion with the Catholic Church, have been justified through baptism and are thus brethren in Christ." "teaches that 'all who have been justified by faith in baptism are members of Christ's body, and have a right to be called Christian, and so are correctly accepted as brothers by the children of the Catholic Church'."(n.99) J. A. Jungmann and K. Stasiak wrote, in "New Catholic Encyclopedia", that "the Second Vatican Council's call for a greater spirit of ecumenism among churches and ecclesial communities reflects the understanding that Baptism is the effecting and the sign of the fundamental unity of all Christians."
"Because Mormonism is polytheistic and rejects the Trinity," Catholic Answers points out that, "Mormon baptism is not valid, and Mormons are not considered separated brethren." Cardinal Urbano Navarrete Cortés clarified, in "L'Osservatore Romano", "that in all of the effects of the pastoral, administrative and juridical practices of the Church the Mormons are not to be considered as belonging to an 'ecclesial community not in full communion with the Catholic Church', but simply as non-baptized." Baptism conferred by The Christian Community, founded by Rudolf Steiner; The New Church, founded by Emanuel Swedenborg; conferred with the formula "I baptize you in the name of the Creator, and of the Redeemer, and of the Sanctifier"; or, conferred with the formula "I baptize you in the name of the Creator, and of the Liberator, and of the Sustainer" are also deemed not valid.
"Separated brethren" is a term sometimes used by the Roman Catholic Church and its clergy and members to refer to baptized members of other Christian traditions.
The phrase is a translation of the Latin phrase "fratres seiuncti".
Before the Second Vatican Council, per the pronouncements of the Council of Trent, the Roman Catholic Church officially referred to Protestants and other non-Roman Catholic Christians as "heretics" likely not having hope of salvation outside of the "Church of Rome". However, Biblical passages like Romans 2:12-16 point to the importance of conscience in Catholic soteriology, which the Church has always recognized.
In  1960 – c. 1962 preparation work for draft texts of Second Vatican Council documents, a "report urged respectful use of the terms dissidents or separated brethren, in place of heretics and schismatics."
After the Second Vatican Council, however, "that habit of unthinkingly hurling accusations of heresy at Protestants pretty much died out". Since at least the mid-1990s, the term has often been replaced by Roman Catholic officials with phrases such as "other Christians".
At least one Roman Catholic writer does not consider Mormons and members of some other religious groups to be separated brethren. Among the groups not considered to be separated brethren are "Jews, Mormons, Christian Scientists, Muslims, Buddhists, and other groups." By the 21st century, within the Roman Catholic faith, Jews are described as and considered elder brothers in the faith.

</doc>
<doc id="40613" url="http://en.wikipedia.org/wiki?curid=40613" title="Logic analyzer">
Logic analyzer

A logic analyzer is an electronic instrument that captures and displays multiple signals from a digital system or digital circuit. A logic analyzer may convert the captured data into timing diagrams, protocol decodes, state machine traces, assembly language, or may correlate assembly with source-level software. Logic Analyzers have advanced triggering capabilities, and are useful when a user needs to see the timing relationships between many signals in a digital system. 
Overview.
Presently, there are three distinct categories of logic analyzers available on the market:
Operation.
A logic analyzer can be triggered on a complicated sequence of digital events, then capture a large amount of digital data from the system under test (SUT).
When logic analyzers first came into use, it was common to attach several hundred "clips" to a digital system. Later, specialized connectors came into use. The evolution of logic analyzer probes has led to a common footprint that multiple vendors support, which provides added freedom to end users. Introduced in April, 2002, connectorless technology (identified by several vendor-specific trade names: Compression Probing; Soft Touch; D-Max) has become popular. These probes provide a durable, reliable mechanical and electrical connection between the probe and the circuit board with less than 0.5 to 0.7 pF loading per signal.
Once the probes are connected, the user programs the analyzer with the names of each signal, and can group several signals together for easier manipulation. Next, a capture mode is chosen, either "timing" mode, where the input signals are sampled at regular intervals based on an internal or external clock source, or "state" mode, where one or more of the signals are defined as "clocks", and data are taken on the rising or falling edges of these clocks, optionally using other signals to qualify these clocks.
After the mode is chosen, a "trigger condition" must be set. A trigger condition can range from simple (such as triggering on a rising or falling edge of a single signal) to the very complex (such as configuring the analyzer to decode the higher levels of the TCP/IP stack and triggering on a certain HTTP packet).
At this point, the user sets the analyzer to "run" mode, either triggering once, or repeatedly triggering.
Once the data are captured, they can be displayed several ways, from the simple (showing waveforms or state listings) to the complex (showing decoded Ethernet protocol traffic). Some analyzers can also operate in a "compare" mode, where they compare each captured data set to a previously recorded data set, and halt capture or visually notify the operator when this data set is either matched or not. This is useful for long-term empirical testing. Recent analyzers can even be set to email a copy of the test data to the engineer on a successful trigger.
Uses.
Many digital designs, including those of ICs, are simulated to detect defects before the unit is constructed. The simulation usually provides logic analysis displays. Often, complex discrete logic is verified by simulating inputs and testing outputs using boundary scan. Logic analyzers can uncover hardware defects that are not found in simulation. These problems are typically too difficult to model in simulation, or too time consuming to simulate and often cross multiple clock domains.
Field-programmable gate arrays have become a common measurement point for logic analyzers and are also used to debug the logic circuit.
History.
As digital computing and integrated circuits emerged in the 1960s, new and difficult problems began to arise, problems that oscilloscopes had trouble handling. For the first time in computing history, it became essential to simultaneously view large numbers of signals. Early solutions attempted to combine hardware from multiple oscilloscopes into one package, but screen clutter, a lack of definite data interpretation, as well as probing constraints made this solution only marginally usable.
Mixed-signal oscilloscopes.
Mixed-signal oscilloscopes combine the functionality of a digital storage oscilloscope with a logic analyzer. The several benefits of these include the ability to view analog and digital signals together in time, and to trigger on either digital or analog signals and capture on the other. A few limitations of mixed signal oscilloscopes is that they do not capture state-mode data, they have a limited channel count, and do not provide the analytical depth and insight of a logic analyzer.

</doc>
<doc id="40614" url="http://en.wikipedia.org/wiki?curid=40614" title="Network switch">
Network switch

A network switch (also called switching hub, bridging hub, officially MAC bridge) is a computer networking device that connects devices together on a computer network, by using packet switching to receive, process and forward data to the destination device. Unlike less advanced network hubs, a network switch forwards data only to one or multiple devices that need to receive it, rather than broadcasting the same data out of each of its ports.
A network switch is a multiport network bridge that uses hardware addresses to process and forward data at the data link layer (layer 2) of the OSI model. Switches can also process data at the network layer (layer 3) by additionally incorporating routing functionality that most commonly uses IP addresses to perform packet forwarding; such switches are commonly known as layer-3 switches or multilayer switches. Beside most commonly used Ethernet switches, they exist for various types of networks, including Fibre Channel, Asynchronous Transfer Mode, and InfiniBand. The first Ethernet switch was introduced by Kalpana in 1990.
Overview.
A switch is a device used on a computer network to physically connect devices together. Multiple cables can be connected to a switch to enable networked devices to communicate with each other. Switches manage the flow of data across a network by only transmitting a received message to the device for which the message was intended. Each networked device connected to a switch can be identified using a MAC address, allowing the switch to regulate the flow of traffic. This maximises security and efficiency of the network.
Because of these features, a switch is often considered more "intelligent" than a network hub. Hubs neither provide security, or identification of connected devices. This means that messages have to be transmitted out of every port of the hub, greatly degrading the efficiency of the network.
Network design.
An Ethernet switch operates at the data link layer of the OSI model to create a separate collision domain for each switch port. Each computer connected to a switch port can transfer data to any of the other ones at a time, and the transmissions will not interfere – with the limitation that, in half duplex mode, each line can only "either" receive from "or" transmit to its connected computer at a certain time. In full duplex mode, each line can simultaneously transmit "and" receive, regardless of the partner.
In the case of using a repeater hub, only a single transmission could take place at a time for all ports combined, so they would all share the bandwidth and run in half duplex. Necessary arbitration would also result in collisions requiring retransmissions.
Applications.
The network switch plays an integral part in most modern Ethernet local area networks (LANs). Mid-to-large sized LANs contain a number of linked managed switches. Small office/home office (SOHO) applications typically use a single switch, or an all-purpose converged device such as a residential gateway to access small office/home broadband services such as DSL or cable Internet. In most of these cases, the end-user device contains a router and components that interface to the particular physical broadband technology. User devices may also include a telephone interface for Voice over IP (VoIP) protocol.
Microsegmentation.
Segmentation is the use of a bridge or a switch (or a router) to split a larger collision domain into smaller ones in order to reduce collision probability and improve overall throughput. In the extreme, i. e. microsegmentation, each device is located on a dedicated switch port. In contrast to an Ethernet hub, there is a separate collision domain on each of the switch ports. This allows computers to have dedicated bandwidth on point-to-point connections to the network and also to run in full-duplex without collisions. Full-duplex mode has only one transmitter and one receiver per 'collision domain', making collisions impossible.
Role of switches in a network.
Switches may operate at one or more layers of the OSI model, including the data link and network layers. A device that operates simultaneously at more than one of these layers is known as a "multilayer switch".
In switches intended for commercial use, built-in or modular interfaces make it possible to connect different types of networks, including Ethernet, Fibre Channel, RapidIO, ATM, ITU-T G.hn and 802.11. This connectivity can be at any of the layers mentioned. While layer-2 functionality is adequate for bandwidth-shifting within one technology, interconnecting technologies such as Ethernet and token ring is easier at layer 3.
Devices that interconnect at layer 3 are traditionally called routers, so layer-3 switches can also be regarded as (relatively primitive) routers.
Where there is a need for a great deal of analysis of network performance and security, switches may be connected between WAN routers as places for analytic modules. Some vendors provide firewall, network intrusion detection, and performance analysis modules that can plug into switch ports. Some of these functions may be on combined modules.
In other cases, the switch is used to create a mirror image of data that can go to an external device. Since most switch port mirroring provides only one mirrored stream, network hubs can be useful for fanning out data to several read-only analyzers, such as intrusion detection systems and packet sniffers.
Layer-specific functionality.
While switches may learn about topologies at many layers, and forward at one or more layers, they do tend to have common features. Other than for high-performance applications, modern commercial switches use primarily Ethernet interfaces.
At any layer, a modern switch may implement power over Ethernet (PoE), which avoids the need for attached devices, such as a VoIP phone or wireless access point, to have a separate power supply. Since switches can have redundant power circuits connected to uninterruptible power supplies, the connected device can continue operating even when regular office power fails.
Layer 1 (hubs vs. higher-layer switches).
A network hub, or a repeater, is a simple network device that does not manage any of the traffic coming through it. Any packet entering a port is flooded out or "repeated" on every other port, except for the port of entry. Since every packet is repeated on every other port, packet collisions affect the entire network, limiting its overall capacity.
A network switch creates the layer 1 end-to-end connection only virtually, while originally it was mandatory. The bridging function of a switch uses information taken from layer 2 to select for each packet the particular port(s) it has to be forwarded to, removing the requirement that every node is presented with all traffic. As a result, the connection lines are not "switched" literally, instead they only appear that way on the packet level.
There are specialized applications in which a network hub can be useful, such as copying traffic to multiple network sensors. High-end network switches usually have a feature called port mirroring that provides the same functionality.
By the early 2000s, there was little price difference between a hub and a low-end switch.
Layer 2.
A network bridge, operating at the data link layer, may interconnect a small number of devices in a home or the office. This is a trivial case of bridging, in which the bridge learns the MAC address of each connected device.
Single bridges also can provide extremely high performance in specialized applications such as storage area networks.
Classic bridges may also interconnect using a spanning tree protocol that disables links so that the resulting local area network is a tree without loops. In contrast to routers, spanning tree bridges must have topologies with only one active path between two points. The older IEEE 802.1D spanning tree protocol could be quite slow, with forwarding stopping for 30 seconds while the spanning tree reconverged. A Rapid Spanning Tree Protocol was introduced as IEEE 802.1w. The newest standard Shortest path bridging (IEEE 802.1aq) is the next logical progression and incorporates all the older Spanning Tree Protocols (IEEE 802.1D STP, IEEE 802.1w RSTP, IEEE 802.1s MSTP) that blocked traffic on all but one alternative path. IEEE 802.1aq (Shortest Path Bridging SPB) allows all paths to be active with multiple equal cost paths, provides much larger layer 2 topologies (up to 16 million compared to the 4096 VLANs limit), faster convergence, and improves the use of the mesh topologies through increase bandwidth and redundancy between all devices by allowing traffic to load share across all paths of a mesh network.
While "layer 2 switch" remains more of a marketing term than a technical term, the products that were introduced as "switches" tended to use microsegmentation and full duplex to prevent collisions among devices connected to Ethernet. By using an internal forwarding plane much faster than any interface, they give the impression of simultaneous paths among multiple devices. 'Non-blocking' devices use a forwarding plane or equivalent method fast enough to allow full duplex traffic for each port simultaneously.
Once a bridge learns the addresses of its connected nodes, it forwards data link layer frames using a layer 2 forwarding method. There are four forwarding methods a bridge can use, of which the second through fourth method were performance-increasing methods when used on "switch" products with the same input and output port bandwidths:
While there are specialized applications, such as storage area networks, where the input and output interfaces are the same bandwidth, this is not always the case in general LAN applications. In LANs, a switch used for end user access typically concentrates lower bandwidth and uplinks into a higher bandwidth.
Layer 3.
Within the confines of the Ethernet physical layer, a layer-3 switch can perform some or all of the functions normally performed by a router.
The most common layer-3 capability is awareness of IP multicast through IGMP snooping. With this awareness, a layer-3 switch can increase efficiency by delivering the traffic of a multicast group only to ports where the attached device has signaled that it wants to listen to that group.
Layer 4.
While the exact meaning of the term "layer-4 switch" is vendor-dependent, it almost always starts with a capability for network address translation, but then adds some type of load distribution based on TCP sessions.
The device may include a stateful firewall, a VPN concentrator, or be an IPSec security gateway.
Layer 7.
Layer-7 switches may distribute loads based on Uniform Resource Locator URL or by some installation-specific technique to recognize application-level transactions. A layer-7 switch may include a web cache and participate in a content delivery network.
Traffic monitoring on a switched network.
Unless port mirroring or other methods such as RMON, SMON or sFlow are implemented in a switch, it is difficult to monitor traffic that is bridged using a switch because only the sending and receiving ports can see the traffic. These monitoring features are rarely present on consumer-grade switches.
Two popular methods that are specifically designed to allow a network analyst to monitor traffic are:
Another method to monitor may be to connect a layer-1 hub between the monitored device and its switch port. This will induce minor delay, but will provide multiple interfaces that can be used to monitor the individual switch port.

</doc>
<doc id="40616" url="http://en.wikipedia.org/wiki?curid=40616" title="Pigeon sport">
Pigeon sport

There are at least four main types of competitive pigeon sport:
Though not quite a sport, fancy breeds of pigeons are also bred to standards and judged in a competitive fashion. Levi in his book "The Pigeon" describes all aspects of pigeon keeping. For exhibition purposes sport pigeons are sometimes grouped as Flying/Sporting Pigeons.

</doc>
<doc id="40622" url="http://en.wikipedia.org/wiki?curid=40622" title="Ohmmeter">
Ohmmeter

An ohmmeter is an electrical instrument that measures electrical resistance, the opposition to an electric current. Micro-ohmmeters (microhmmeter or microohmmeter) make low resistance measurements. Megaohmmeters (aka megaohmmeter or in the case of a trademarked device Megger) measure large values of resistance. The unit of measurement for resistance is ohms (Ω).
The first ohmmeters were based on a type of meter movement known as a 'ratiometer'. These were similar to the galvanometer type movement encountered in later instruments, but instead of hairsprings to supply a restoring force they used conducting 'ligaments' instead. These provided no net rotational force to the movement. Also, the movement was wound with two coils. One was connected via a series resistor to the battery supply. The second was connected to the same battery supply via a second resistor and the resistor under test. The indication on the meter was proportional to the ratio of the currents through the two coils. This ratio was determined by the magnitude of the resistor under test. The advantages of this arrangement were twofold. First, the indication of the resistance was completely independent of the battery voltage (as long as it actually produced some voltage) and no zero adjustment was required. Second, although the resistance scale was non linear, the scale remained correct over the full deflection range. By interchanging the two coils a second range was provided. This scale was reversed compared to the first. A feature of this type of instrument was that it would continue to indicate a random resistance value once the test leads were disconnected (the action of which disconnected the battery from the movement). Ohmmeters of this type only ever measured resistance as they could not easily be incorporated into a multimeter design. Insulation testers that relied on a hand cranked generator operated on the same principle. This ensured that the indication was wholly independent of the voltage actually produced.
Subsequent designs of ohmmeter provided a small battery to apply a voltage to a resistance via a galvanometer to measure the current through the resistance. The scale of the galvanometer was marked in ohms, because the fixed voltage from the battery assured that as resistance is decreased, the current through the meter would increase. Ohmmeters form circuits by themselves, therefore they cannot be used within an assembled circuit. This design is much simpler and cheaper than the former design, and was simple to integrate into a multimeter design and consequently was by far the most common form of analogue ohmmeter. This type of ohmmeter suffers two inherent disadvantages. First, the meter needs to be zeroed by shorting the measurement points together and performing an adjustment for zero ohms indication prior to each measurement. This is because as the battery voltage decreases with age, the series resistance in the meter needs to be reduced to maintain the zero indication at full deflection. Second, and consequent on the first, the actual deflection for any given resistor under test changes as the internal resistance is altered. It remains correct at the centre of the scale only, which is why such ohmmeter designs always quote the accuracy "at centre scale only".
A more accurate type of ohmmeter has an electronic circuit that passes a constant current (I) through the resistance, and another circuit that measures the voltage (V) across the resistance. According to the following equation, derived from Ohm's Law, the value of the resistance (R) is given by:
For high-precision measurements the above types of meter are inadequate. This is because the meter's reading is the sum of the resistance of the measuring leads, the contact resistances and the resistance being measured. To reduce this effect, a precision ohmmeter has four terminals, called Kelvin contacts. Two terminals carry the current from the meter, while the other two allow the meter to measure the voltage across the resistor. With this type of meter, any voltage drop due to the resistance of the first pair of leads and their contact resistances is ignored by the meter. This four terminal measurement technique is called Kelvin sensing, after William Thomson, Lord Kelvin, who invented the Kelvin bridge in 1861 to measure very low resistances. The Four-terminal sensing method can also be utilized to conduct accurate measurements of low resistances.

</doc>
<doc id="40623" url="http://en.wikipedia.org/wiki?curid=40623" title="Multimeter">
Multimeter

A multimeter or a multitester, also known as a VOM (Volt-Ohm meter or Volt-Ohm-milliammeter ), is an electronic measuring instrument that combines several measurement functions in one unit. A typical multimeter would include basic features such as the ability to measure voltage, current, and resistance. Analog multimeters use a microammeter whose pointer moves over a scale calibrated for all the different measurements that can be made. Digital multimeters (DMM, DVOM) display the measured value in numerals, and may also display a bar of a length proportional to the quantity being measured. Digital multimeters are now far more common but analog multimeters are still preferable in some cases, for example when monitoring a rapidly varying value. 
A multimeter can be a hand-held device useful for basic fault finding and field service work, or a bench instrument which can measure to a very high degree of accuracy. They can be used to troubleshoot electrical problems in a wide array of industrial and household devices such as electronic equipment, motor controls, domestic appliances, power supplies, and wiring systems.
Multimeters are available in a wide range of features and prices. Cheap multimeters can cost less than US$10, while laboratory-grade models with certified calibration can cost more than US$5,000.
History.
The first moving-pointer current-detecting device was the galvanometer in 1820. These were used to measure resistance and voltage by using a Wheatstone bridge, and comparing the unknown quantity to a reference voltage or resistance. While useful in the lab, the devices were very slow and impractical in the field. These galvanometers were bulky and delicate.
The D'Arsonval/Weston meter movement used a fine metal spring to give proportional measurement rather than just detection, and built-in permanent field magnets made deflection independent of the orientation of the meter. Instead of balancing a bridge, values could be directly read off the instruments's scale, which made measurement quick and easy. By adding a series or shunt resistor, more than one range of voltage or current could be measured with one movement.
Multimeters were invented in the early 1920s as radio receivers and other vacuum tube electronic devices became more common. The invention of the first multimeter is attributed to British Post Office engineer, Donald Macadie, who became dissatisfied with having to carry many separate instruments required for the maintenance of the telecommunications circuits. Macadie invented an instrument which could measure amperes (amps), volts and ohms, so the multifunctional meter was then named Avometer. The meter comprised a moving coil meter, voltage and precision resistors, and switches and sockets to select the range.
Macadie took his idea to the Automatic Coil Winder and Electrical Equipment Company (ACWEEC, founded in ~1923). The first AVO was put on sale in 1923, and many of its features remained almost unaltered through to the last Model 8.
Pocket watch style meters were in widespread use in the 1920s, at much lower cost than Avometers. The metal case was normally connected to the negative connection, an arrangement that caused numerous electric shocks. The technical specifications of these devices were often crude, for example the one illustrated has a resistance of just 33 ohms per volt, a non-linear scale and no zero adjustment.
Any meter will load the circuit under test to some extent. For example,a multimeter using a movement with full-scale current of 50 microamps, the highest sensitivity commonly available, must draw at least 50 microamps from the circuit under test to deflect fully. This may load a high-impedance circuit so much as to affect the circuit, and to give a low reading.
"Vacuum Tube Voltmeters" or valve voltmeters (VTVM, VVM) were used for voltage measurements in electronic circuits where high impedance was necessary. The VTVM had a fixed input impedance of typically 1 megohm or more, usually through use of a cathode follower input circuit, and thus did not significantly load the circuit being tested. VTVMs were used before the introduction of digital electronic high-impedance analog transistor and field effect transistor (FET) voltmeters. Modern digital meters and some modern analog meters use electronic input circuitry to achieve high-input impedance—their voltage ranges are functionally equivalent to VTVMs.
Additional scales such as decibels, and measurement functions such as capacitance, transistor gain, frequency, duty cycle, display hold, and buzzers which sound when the measured resistance is small have been included on many multimeters. While multimeters may be supplemented by more specialized equipment in a technician's toolkit, some multimeters include additional functions for specialized applications (temperature with a thermocouple probe, inductance, connectivity to a computer, speaking measured value, etc.).
Operation.
A multimeter is a combination of a multirange DC voltmeter, multirange AC voltmeter, multirange ammeter, and multirange ohmmeter. An un-amplified analog multimeter combines a meter movement, range resistors and switches.
For an analog meter movement, DC voltage is measured with a series resistor connected between the meter movement and the circuit under test. A set of switches allows greater resistance to be inserted for higher voltage ranges. The product of the basic full-scale deflection current of the movement, and the sum of the series resistance and the movement's own resistance, gives the full-scale voltage of the range. As an example, a meter movement that required 1 milliampere for full scale deflection, with an internal resistance of 500 ohms, would, on a 10-volt range of the multimeter, have 9,500 ohms of series resistance.
For analog current ranges, low-resistance shunts are connected in parallel with the meter movement to divert most of the current around the coil. Again for the case of a hypothetical 1-mA, 500-ohm movement on a 1-Ampere range, the shunt resistance would be just over 0.5 ohms.
Moving coil instruments respond only to the average value of the current through them. To measure alternating current, a rectifier diode is inserted in the circuit so that the average value of current is non-zero. Since the rectified average value and the root-mean-square value of a waveform need not be the same, simple rectifier-type circuits may only be calibrated for sinusoidal waveforms. Other wave shapes require a different calibration factor to relate RMS and average value. Since practical rectifiers have non-zero voltage drop, accuracy and sensitivity is poor at low values.
To measure resistance, a small battery within the instrument passes a current through the device under test and the meter coil. Since the current available depends on the state of charge of the battery, a multimeter usually has an adjustment for the ohms scale to zero it. In the usual circuit found in analog multimeters, the meter deflection is inversely proportional to the resistance; so full-scale is 0 ohms, and high resistance corresponds to smaller deflections. The ohms scale is compressed, so resolution is better at lower resistance values.
Amplified instruments simplify the design of the series and shunt resistor networks. The internal resistance of the coil is decoupled from the selection of the series and shunt range resistors; the series network becomes a voltage divider. Where AC measurements are required, the rectifier can be placed after the amplifier stage, improving precision at low range.
Digital instruments, which necessarily incorporate amplifiers, use the same principles as analog instruments for range resistors. For resistance measurements, usually a small constant current is passed through the device under test and the digital multimeter reads the resultant voltage drop; this eliminates the scale compression found in analog meters, but requires a source of significant current. An autoranging digital multimeter can automatically adjust the scaling network so that the measurement uses the full precision of the A/D converter.
In all types of multimeters, the quality of the switching elements is critical to stable and accurate measurements. Stability of the resistors is a limiting factor in the long-term accuracy and precision of the instrument.
Quantities measured.
Contemporary multimeters can measure many quantities. The common ones are:
Additionally, some multimeters measure:
Digital multimeters may also include circuits for:
Various sensors can be attached to multimeters to take measurements such as:
Resolution.
Resolution and accuracy.
The resolution of a multimeter is the smallest part of the scale which can be shown, which is scale dependent. On some digital multimeters it can be configured, with higher resolution measurements taking longer to complete. For example, a multimeter that has a 1 mV resolution on a 10 V scale can show changes in measurements in 1mV increments.
Absolute accuracy is the error of the measurement compared to a perfect measurement. Relative accuracy is the error of the measurement compared to the device used to calibrate the multimeter. Most multimeter datasheets provide relative accuracy. To compute the absolute accuracy from the relative accuracy of a multimeter add the absolute accuracy of the device used to calibrate the multimeter to the relative accuracy of the multimeter.
Digital.
The resolution of a multimeter is often specified in the number of decimal digits resolved and displayed. If the most significant digit cannot take all values from 0 to 9 is often termed a fractional digit. For example, a multimeter which can read up to 19999 (plus an embedded decimal point) is said to read 4½ digits.
By convention, if the most significant digit can be either 0 or 1, it is termed a half-digit; if it can take higher values without reaching 9 (often 3 or 5), it may be called three-quarters of a digit. A 5½ digit multimeter would display one "half digit" that could only display 0 or 1, followed by five digits taking all values from 0 to 9. Such a meter could show positive or negative values from 0 to 199,999. A 3¾ digit meter can display a quantity from 0 to 3,999 or 5,999, depending on the manufacturer.
While a digital display can easily be extended in precision, the extra digits are of no value if not accompanied by care in the design and calibration of the analog portions of the multimeter. Meaningful high-resolution measurements require a good understanding of the instrument specifications, good control of the measurement conditions, and traceability of the calibration of the instrument. However, even if its resolution exceeds the accuracy, a meter can be useful for comparing measurements. For example, a meter reading 5½ stable digits may indicate that one nominally 100,000 ohm resistor is about 7 ohms greater than another, although the error of each measurement is 0.2% of reading plus 0.05% of full-scale value.
Specifying "display counts" is another way to specify the resolution. Display counts give the largest number, or the largest number plus one (so the count number looks nicer) the multimeter's display can show, ignoring a decimal separator. For example, a 5½ digit multimeter can also be specified as a 199999 display count or 200000 display count multimeter. Often the display count is just called the count in multimeter specifications.
The accuracy of a digital multimeter may be stated in a two-term form, such as "±1% of reading +2 counts", reflecting the different sources of error in the instrument. 
Analog.
Analog meters are older and still preferred by many engineers. One reason for this is that analog meters are more sensitive to changes in the circuit that is being measured. A digital multimeter samples the quantity being measured and then displays it. Analog multimeters continuously read the test value. If there are slight changes in readings, the needle of an analog multimeter will track them while digital multimeters may miss them or be difficult to read. This continuous tracking feature becomes important when testing capacitors or coils. A properly functioning capacitor should allow current to flow when voltage is applied, then the current slowly decreases to zero and this "signature" is easy to see on an analog multimeter but not on a digital multimeter. This is similar when testing a coil, except the current starts low and increases. 
Resistance measurements on an analog meter, in particular, are of low precision due to the typical resistance measurement circuit which compresses the scale heavily at the higher resistance values. Inexpensive analog meters may have only a single resistance scale, seriously restricting the range of precise measurements. Typically an analog meter will have a panel adjustment to set the zero-ohms calibration of the meter, to compensate for the varying voltage of the meter battery.
Accuracy.
Digital multimeters generally take measurements with accuracy superior to their analog counterparts. Standard analog multimeters measure with typically ±3% accuracy, though instruments of higher accuracy are made. Standard portable digital multimeters are specified to have an accuracy of typically 0.5% on the DC voltage ranges. Mainstream bench-top multimeters are available with specified accuracy of better than ±0.01%. Laboratory grade instruments can have accuracies of a few parts per million.
Accuracy figures need to be interpreted with care. The accuracy of an analog instrument usually refers to full-scale deflection; a measurement of 30 V on the 100 V scale of a 3% meter is subject to an error of 3 V, 10% of the reading. Digital meters usually specify accuracy as a percentage of reading plus a percentage of full-scale value, sometimes expressed in counts rather than percentage terms.
Quoted accuracy is specified as being that of the lower millivolt (mV) DC range, and is known as the "basic DC volts accuracy" figure. Higher DC voltage ranges, current, resistance, AC and other ranges will usually have a lower accuracy than the basic DC volts figure. AC measurements only meet specified accuracy within a specified range of frequencies.
Manufacturers can provide calibration services so that new meters may be purchased with a certificate of calibration indicating the meter has been adjusted to standards traceable to, for example, the US National Institute of Standards and Technology (NIST), or other national standards organization.
Test equipment tends to drift out of calibration over time, and the specified accuracy cannot be relied upon indefinitely. For more expensive equipment, manufacturers and third parties provide calibration services so that older equipment may be recalibrated and recertified. The cost of such services is disproportionate for inexpensive equipment; however extreme accuracy is not required for most routine testing. Multimeters used for critical measurements may be part of a metrology program to assure calibration.
A multimeter can be assumed to be "average responding" to AC waveforms unless stated as being a "True RMS" type.
An average responding mulimeter will only meet its specified accuracy on AC volts and amps for purely sinusoidal waveforms.
A True RMS responding multimeter on the other hand will meet its specified accuracy on AC volts and current with any waveform type up to a specified crest factor.
A meter's AC voltage and current accuracy may have different specifications for different ranges of frequency.
Sensitivity and input impedance.
When used for measuring voltage, the input impedance of the multimeter must be very high compared to the impedance of the circuit being measured; otherwise circuit operation may be changed, and the reading will also be inaccurate.
Meters with electronic amplifiers (all digital multimeters and some analog meters) have a fixed input impedance that is high enough not to disturb most circuits. This is often either one or ten megohms; the standardization of the input resistance allows the use of external high-resistance probes which form a voltage divider with the input resistance to extend voltage range up to tens of thousands of volts. High-end multimeters generally provide an input impedance >10 Gigaohms for ranges less than or equal to 10 V. Some high-end multimeters provide >10 Gigaohms of impedance to ranges greater than 10 V.
Most analog multimeters of the moving-pointer type are unbuffered, and draw current from the circuit under test to deflect the meter pointer. The impedance of the meter varies depending on the basic sensitivity of the meter movement and the range which is selected. For example, a meter with a typical 20,000 ohms/volt sensitivity will have an input resistance of two million ohms on the 100-volt range (100 V * 20,000 ohms/volt = 2,000,000 ohms). On every range, at full scale voltage of the range, the full current required to deflect the meter movement is taken from the circuit under test. Lower sensitivity meter movements are acceptable for testing in circuits where source impedances are low compared to the meter impedance, for example, power circuits; these meters are more rugged mechanically. Some measurements in signal circuits require higher sensitivity movements so as not to load the circuit under test with the meter impedance.
Sensitivity should not be confused with resolution of a meter, which is defined as the lowest signal change (voltage, current, resistance...) that can change the observed reading.
For general-purpose digital multimeters, the lowest voltage range is typically several hundred millivolts AC or DC, but the lowest current range may be several hundred microamperes, although instruments with greater current sensitivity are available. Multimeters designed for (mains) "electrical" use instead of general electronics engineering use will typically forego the microamps current ranges.
Measurement of low resistance requires lead resistance (measured by touching the test probes together) to be subtracted for best accuracy. This can be done with the "delta", "Zero", or "null" feature of many digital multimeters.
The upper end of multimeter measurement ranges varies considerably; measurements over perhaps 600 volts, 10 amperes, or 100 megohms may require a specialized test instrument.
Burden voltage.
Any ammeter, including a multimeter in a current range, has a certain resistance. Most multimeters inherently measure voltage, and pass a current to be measured through a shunt resistance, measuring the voltage developed across it. The voltage drop is known as the burden voltage, specified in volts per ampere. The value can change depending on the range the meter selects, since different ranges usually use different shunt resistors.
The burden voltage can be significant in very low-voltage circuit areas. To check for its effect on accuracy and on external circuit operation the meter can be switched to different ranges; the current reading should be the same and circuit operation should not be affected if burden voltage is not a problem. If this voltage is significant it can be reduced (also reducing the inherent accuracy and precision of the measurement) by using a higher current range.
Alternating current sensing.
Since the basic indicator system in either an analog or digital meter responds to DC only, a multimeter includes an AC to DC conversion circuit for making alternating current measurements. Basic meters utilize a rectifier circuit to measure the average or peak absolute value of the voltage, but are calibrated to show the calculated root mean square (RMS) value for a sinusoidal waveform; this will give correct readings for alternating current as used in power distribution. User guides for some such meters give correction factors for some simple non-sinusoidal waveforms, to allow the correct root mean square (RMS) equivalent value to be calculated. More expensive multimeters include an AC to DC converter that measures the true RMS value of the waveform within certain limits; the user manual for the meter may indicate the limits of the crest factor and frequency for which the meter calibration is valid. RMS sensing is necessary for measurements on non-sinusoidal periodic waveforms, such as found in audio signals and variable-frequency drives.
Digital multimeters (DMM or DVOM).
Modern multimeters are often digital due to their accuracy, durability and extra features. In a digital multimeter the signal under test is converted to a voltage and an amplifier with electronically controlled gain preconditions the signal. A digital multimeter displays the quantity measured as a number, which eliminates parallax errors.
Modern digital multimeters may have an embedded computer, which provides a wealth of convenience features. Measurement enhancements available include:
Modern meters may be interfaced with a personal computer by IrDA links, RS-232 connections, USB, or an instrument bus such as IEEE-488. The interface allows the computer to record measurements as they are made. Some DMMs can store measurements and upload them to a computer.
The first digital multimeter was manufactured in 1955 by Non Linear Systems.
Analog multimeters.
A multimeter may be implemented with a galvanometer meter movement, or less often with a bargraph or simulated pointer such as an LCD or vacuum fluorescent display. Analog multimeters are common; a quality analog instrument will cost about the same as a DMM. Analog multimeters have the precision and reading accuracy limitations described above, and so are not built to provide the same accuracy as digital instruments.
Analog meters are also useful in situations where it is necessary to pay attention to something other than the meter, and the swing of the pointer can be noticed without looking directly at it. This can happen when accessing awkward locations, or when working on cramped live circuitry.
Analog meter movements are inherently more fragile physically and electrically than digital meters. Many analog meters have been instantly broken by connecting to the wrong point in a circuit, or while on the wrong range, or by dropping onto the floor. Many analog multimeters feature a switch position marked "transit" to protect the meter movement during transportation. This feature works by placing a low resistance across the movement winding, resulting in dynamic braking. Sensitive meter movements may be protected in the same manner by connecting a shorting or jumper wire between the terminals when not in use. Meters which feature a shunt across the winding such as an ammeter may not require further resistance to arrest uncontrolled movements of the meter needle because of the low resistance of the shunt.
The ARRL handbook also says that analog multimeters, with no electronic circuitry, are less susceptible to radio frequency interference.
The meter movement in a moving pointer analog multimeter is practically always a moving-coil galvanometer of the d'Arsonval type, using either jeweled pivots or taut bands to support the moving coil. In a basic analog multimeter the current to deflect the coil and pointer is drawn from the circuit being measured; it is usually an advantage to minimize the current drawn from the circuit. The sensitivity of an analog multimeter is given in units of ohms per volt. For example, a very low cost multimeter with a sensitivity of 1000 ohms per volt would draw 1 milliampere from a circuit at full scale deflection. More expensive, (and mechanically more delicate) multimeters typically have sensitivities of 20,000 ohms per volt and sometimes higher, with a 50,000 ohms per volt meter (drawing 20 microamperes at full scale) being about the upper limit for a portable, general purpose, non-amplified analog multimeter.
To avoid the loading of the measured circuit by the current drawn by the meter movement, some analog multimeters use an amplifier inserted between the measured circuit and the meter movement. While this increased the expense and complexity of the meter, by use of vacuum tubes or field effect transistors the input resistance can be made very high and independent of the current required to operate the meter movement coil. Such amplified multimeters are called VTVMs (vacuum tube voltmeters), TVMs (transistor volt meters), FET-VOMs, and similar names.
Probes.
A multimeter can utilize a variety of test probes to connect to the circuit or device under test. Crocodile clips, retractable hook clips, and pointed probes are the three most common attachments. Tweezer probes are used for closely spaced test points, as in surface-mount devices. The connectors are attached to flexible, thickly insulated leads that are terminated with connectors appropriate for the meter. Probes are connected to portable meters typically by shrouded or recessed banana jacks, while benchtop meters may use banana jacks or BNC connectors. 2mm plugs and binding posts have also been used at times, but are less common today.
The banana jacks are typically placed with a standardized center-to-center distance of 0.75" (19.05mm), to allow standard adapters or devices such as voltage multiplier or thermocouple probes to be plugged in.
Clamp meters clamp around a conductor carrying a current to measure without the need to connect the meter in series with the circuit, or make metallic contact at all. Types to measure AC current use the transformer principle; clamp-on meters to measure small current or direct current require more complicated sensors.
Safety.
Most multimeters include a fuse, or two fuses, which will sometimes prevent damage to the multimeter from a current overload on the highest current range. A common error when operating a multimeter is to set the meter to measure resistance or current, and then connect it directly to a low-impedance voltage source. Unfused meters are often quickly destroyed by such errors; fused meters often survive. Fuses used in meters must carry the maximum measuring current of the instrument, but are intended to disconnect if operator error exposes the meter to a low-impedance fault. Meters with inadequate or unsafe fusing were not uncommon; this situation has led to the creation of the IEC61010 categories to rate the safety and robustness of meters.
Digital meters are rated into four categories based on their intended application, as set forth by IEC 61010-1 and echoed by country and regional standards groups such as the CEN EN61010 standard.
Each category also specifies maximum transient voltages for selected measuring ranges in the meter.
Category-rated meters also feature protections from over-current faults. On meters that allow interfacing with computers, optical isolation may be used to protect attached equipment against high voltage in the measured circuit.
Good quality multimeters designed to meet CAT II and above ratings will include High Rupture Capacity ceramic fuses typically rated at more than 20kA breaking capacity. They will also include high energy overvoltage MOV (Metal Oxide Varistor) protection, and circuit over-current protection in the form of a Polyswitch.
DMM alternatives.
A general-purpose electronics DMM is generally considered adequate for measurements at signal levels greater than one millivolt or one microampere, or below about 100 megohms—levels far from the theoretical limits of sensitivity. Other instruments—essentially similar, but with higher sensitivity—are used for accurate measurements of very small or very large quantities. These include nanovoltmeters, electrometers (for very low currents, and voltages with very high source resistance, such as one teraohm) and picoammeters. These measurements are limited by available technology, and ultimately by inherent thermal noise.
Power supply.
Analog meters can measure voltage and current using power from the test circuit, but require internal power from the meter for resistance testing; electronic meters always require an internal power supply. Hand-held meters use batteries, while bench meters usually use mains power; either arrangement allows the meter to test devices not connected to an active circuit. Testing often requires that the component under test be isolated from the circuit, as otherwise stray or leakage current paths may distort measurements.
Meters intended for testing in hazardous locations or for use on blasting circuits may require use of a manufacturer-specified battery to maintain their safety rating.

</doc>
<doc id="40628" url="http://en.wikipedia.org/wiki?curid=40628" title="Hayley Wickenheiser">
Hayley Wickenheiser

Hayley Wickenheiser OC (born August 12, 1978) is a Canadian women's ice hockey player. She was the first woman to play full-time professional hockey in a position other than goalie. Wickenheiser is a member of the Canada women's national ice hockey team. She has represented Canada at the Winter Olympics five times, capturing four gold and one silver medal and twice being named tournament MVP, and one time at the Summer Olympics in softball. She has the most gold medals of any Canadian Olympian and is widely considered the greatest female ice hockey player in the world. On February 20, 2014, Wickenheiser was elected to the International Olympic Committee's Athletes' Commission.
Hockey career.
Minor.
Wickenheiser started playing minor hockey on outdoor rinks in her hometown of Shaunavon, Saskatchewan when she was five years old. She played exclusively on boys teams until she was 13. Wickenheiser continued playing minor hockey in Calgary, Alberta after moving there with her family. In 1991, she represented Alberta at the 18-and-under Canada Winter Games. Alberta captured the gold medal in the tournament, with Wickenheiser scoring the game-winning goal and being named the Most Valuable Player of the final game.
International.
At the age of 15 (1994), Wickenheiser was named to Canada's National Women's Team for the first time and has remained a member since. Her first international tournament was the 1994 World Championship, held in Lake Placid, New York. She played three games, and picked up her first international point – an assist, and Canada won gold. Her second World Championship in 1997 also produced a gold medal and she earned a spot on the tournament All-Star team, the first of four such honours (1997, 1999, 2000, 2005). In 1999, Wickenheiser helped Canada to another gold medal and was named tournament MVP. Wickenheiser has seven World Championship gold medals (1994, 1997, 1999, 2000, 2004, 2007, 2012) and three silver medals (2005, 2008, 2009). She was named to Team Canada in 2001, but was unable to compete due to an injury, and was also on Canada's roster for the 2003 World Championship which was canceled.
Wickenheiser was a member of Team Canada at the 1998 Winter Olympics, when women's hockey was introduced as a medal sport. She also played 21 games for Team Canada during their pre-Olympic tour. Canada won a silver medal at the event and Wickenheiser was named to the tournament all-star team. Her performance at the 1998 Olympics impressed Men's Team Canada General Manager Bobby Clarke so much, that he invited her to participate in the Philadelphia Flyers rookie camps in 1998 and 1999. 2002 was another chance at Olympic gold, and Wickenheiser was named to Canada's roster for the 2002 Winter Olympics held in Salt Lake City, Utah. On Team Canada's pre-Olympic tour, Wickenheiser played 26 games and racked up 36 points. In a bit of redemption for 1998, Canada won the gold medal by defeating Team USA in the final game. Wickenheiser was named Tournament MVP and she was the top scorer on the Women's side. At the 2006 Winter Olympics, Canada was defending its gold medal status. When the final match was set, Canada was facing off against Sweden, a surprise finalist. They won gold again, and Wickenheiser once more was named tournament MVP, Top Forward, and to a berth on the all-star team. She also led the tournament in scoring.
Wickenheiser captained Canada to a gold medal at the 1998 Christmas Cup (World Women's Under-22 Championship). She has also contributed to at least 10 gold medals for Canada at the 4 Nations Cup tournaments (1996, 1999, 2000, 2001, 2002, 2004, 2005, 2006, 2007, 2010). At the 2006 Four Nations Cup, she served as team captain. On February 17, 2010, Wickenheiser became the all-time leading Olympic goal scorer as Canada defeated Sweden 13–1. Wickenheiser reached her record total of 16 career Olympic goals by scoring once on Wednesday as Canada followed up their 18–0 win over Slovakia and 10–1 defeat of Switzerland.
With a third and fourth consecutive Olympic gold medal in Women's hockey won by defeating the United States of America 2–0 in Vancouver and 3-2 in Sochi, Hayley now has 5 Olympic medals: 4 gold, 1 silver.
Professional.
In 2003, Wickenheiser became the first woman to score a goal playing in a men's professional league. Over the course of the season, Wickenheiser played 23 games, scoring 2 goals and adding 10 assists. Wickenheiser joined a European league to play professional hockey, as the game is more open and less physical than North American leagues. This attempt to play professional hockey was not an entirely smooth process, as Wickenheiser was initially slated to play in Italy, until the Italian Winter Sports Federation ruled that women were ineligible to play in a men's league. She also turned down an offer from Phil Esposito to play for the Cincinnati Cyclones of the ECHL. Finland's Hockey Federation unanimously supported letting women play in a men's league, allowing her to debut with HC Salamat in the Suomi-sarja, the third highest hockey league in Finland, on January 10, 2003. Wickenheiser played briefly with Salamat in 2004. They had won promotion to Mestis, Finland's second tier of professional hockey, and this was not as good a fit for her. She left the team after ten games.
In 2007, Wickenheiser had a week-long tryout contract with Swedish club IFK Arboga IK in the Swedish male third league. After two practice games, where Wickenheiser scored two goals in the first game, she was not offered a contract. In 2008, Wickenheiser signed a one-year contract with Eskilstuna Linden, also in the Swedish men's third league.
Wickenheiser was named one of the "Top 100 Most Influential People in Hockey" by "The Hockey News" (ranked #59 on the 2011 List), one of the "25 Toughest Athletes" by "Sports Illustrated" and one of the "Top 50 Most Powerful Women in Canada" by "The Globe and Mail".
Club.
In 1996, Wickenheiser was named MVP of the Esso National Women's Championship, helping Alberta to a fourth-place finish. In 1997 and 1998, Wickenheiser won Nationals with the Edmonton Chimos and Calgary Oval X-Treme respectively. She was named tournament MVP both years. Between 1999 and 2001, Wickenheiser continued to play for her club teams at the Esso Women's National Championships, winning a gold medal and two silvers. She played 2004–05 with the Calgary Oval X-Treme, in the inaugural season of the Western Women's Hockey League. The X-Treme were league champions. Wickenheiser was the regular season leading scorer and named to the league's all-star team. She also played for Alberta at the Esso National Championships, where they won gold. She led the tournament in scoring and was named MVP.
University.
Wickenheiser joined the 2010–11 University of Calgary Dinos women's ice hockey season that competes in the Canadian Interuniversity Sport (CIS). The Dinos are playing their second season of CIS hockey, and Wickenheiser is expected to provide leadership to a young team. While with the Dinos, Wickenheiser will be playing for her former teammate, Danielle Goyette, who is the team's head coach. Wickenheiser will be working to complete a degree in kinesiology at Calgary. The Dinos were Wickenheiser's choice because the team practices every day, and she was able to stay in Calgary with her family. Under CIS rules, Wickenheiser began her first year of eligibility in 2010 because she had never played university hockey. Players have up to five years of eligibility. In her CIS debut against the University of Regina, Wickenheiser scored two goals and added an assist in a 4–3 victory. A crowd of over 500 people attended her CIS debut in Regina. Wickenheiser was named the Canada West female athlete of the week on November 2, 2010 after scoring three goals and adding an assist in two games against the University of Alberta. Despite only playing in 15 of the Dino's 24 regular season games, Wickenheiser finished tied for the conference lead in scoring with 40 points (17 goals and 23 assists), and finishing with a plus-minus of +22. She scored four short handed goals, and had five game winners. At the end of the year, Wickenheiser was named the Canada West Most Valuable Player, and captured a spot on the conference's First All-Star Team. On March 9, 2011, Wickenheiser was named the Canadian Interuniversity Sport player of the year in women's hockey. She the became the first ever Dino to win the Brodrick Trophy as CIS MVP.
Softball and fastball career.
Wickenheiser is an accomplished softball player. On June 24, 2000, she was named to the Canadian softball team for the 2000 Summer Olympics. This was the culmination of a long ball career. In 1994, she participated at Canadian Midget Nationals, where she was named All-Canadian Shortstop and Top Batter. In 1995, Wickenheiser was a member of Team Canada at the World Junior Fastball Championships, held in Normal, Illinois. Canada finished fifth at this event. In 1997, Wickenheiser participated at Midget Nationals with the Silver Springs 76ers. Her team finished second and Wickenheiser was again named All Star Shortstop and Top Batter. In 1999 she also participated at Senior Nationals, where her team finished fourth. In 2000 Hayley attended and competed for Simon Fraser University, and helped lead the team to a 38 and 13 record, en route to a 3rd-place finish at the NAIA National Championships. Later that summer she competed in the Summer Olympic games in Sydney, Australia, where she led Canada with the team's highest batting average. Canada was competitive, but finished the tournament with a 1–6 record, losing three games by one run. Since that Olympics, Wickenheiser has not been as active in softball.
Personal life.
Her parents are Tom, a physical-education teacher, and Marilyn. She has a brother named Ross and a sister named Jane. Wickenheiser lives in Calgary with her adopted son, Noah. Doug Wickenheiser, the first overall pick in the 1980 NHL Entry Draft, was her cousin. He died of cancer in 1999.
Wickenheiser graduated with a degree in kinesiology in 2013 and has expressed a desire to attend medical school after she is finished playing hockey. On July 15, 2011 her hometown of Shaunavon named a new 14 million dollar recreational complex after her, Crescent Point Wickenheiser Centre. On June 30, 2011, she was named an Officer of the Order of Canada by Governor General David Johnston.
Hayley is the author of "Gold Medal Diary – Inside the World's Greatest Sports Event", outlining her training with Team Canada and the events leading up to, during, and following the 2010 Olympic Games.
Game appearance.
EA Sports officially announced that Wickenheiser would be among the first two real female hockey players in "NHL 13". Along with Angela Ruggiero, she has a playable character in the game which can be added to any team of the user's choice.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="40629" url="http://en.wikipedia.org/wiki?curid=40629" title="Marc Gagnon">
Marc Gagnon

Marc Gagnon (born May 24, 1975 in Chicoutimi, Quebec) is a Canadian short track speed skater. He is a four-time Overall World Champion for 1993, 1994, 1996 and 1998, and winner of three Olympic gold medals.
Biography.
Gagnon started his Olympic career in 1994, when he had already won the 1993 World Championships. He won a bronze in the 1000 m event. Four years later, in Nagano, Japan, Gagnon won a gold medal with the Canadian relay team. The 2002 Salt Lake City Games proved to be Gagnon's best Olympics, with a total of three medals. A bronze in the inaugural 1500 m event, and two golds; in the 500 m and again as a part of the relay team. Even his disqualification in the 1000 m was memorable, as it was the first of an improbable series of events that led to Australian Steven Bradbury winning arguably the most unlikely gold medal in Olympic history. 
Winning a total of five medals in three consecutive Winter Games made him the most decorated Canadian athlete in Winter Olympic history until 2006. He has now been overtaken by long track speed skater Cindy Klassen and long track speed skater/road cyclist Clara Hughes, who each have a total of 6 medals. Tied with track and field athlete Phil Edwards and fellow short track speed skater François-Louis Tremblay, he is one of the five most decorated Canadian athletes in all Olympic Games.
Gagnon won his World Championships in 1993, 1994, 1996 and 1998. He is the first man to have become a four-time Overall World Champion. In addition, he finished 2nd twice, and third once.
In 2008, Gagnon was inducted into Canada's Sports Hall of Fame.

</doc>
<doc id="40630" url="http://en.wikipedia.org/wiki?curid=40630" title="Beach">
Beach

A beach is a landform along the coast of an ocean, sea, lake, or river. It usually consists of loose particles, which are often composed of rock, such as sand, gravel, shingle, pebbles, or cobblestones. The particles comprising a beach are occasionally biological in origin, such as mollusc shells or coralline algae.
Some beaches have man-made infrastructure, such as lifeguard posts, changing rooms, and showers. They may also have hospitality venues (such as resorts, camps, hotels, and restaurants) nearby. Wild beaches, also known as undeveloped or undiscovered beaches, are not developed in this manner. Wild beaches can be valued for their untouched beauty and preserved nature.
Beaches typically occur in areas along the coast where wave or current action deposits and reworks sediments.
Overview.
Although the seashore is most commonly associated with the word "beach", beaches are found by lakes and alongside large rivers.
"Beach" may refer to:
The former are described in detail below; the larger geological units are discussed elsewhere under bars.
There are several conspicuous parts to a beach that relate to the processes that form and shape it. The part mostly above water (depending upon tide), and more or less actively influenced by the waves at some point in the tide, is termed the beach berm. The berm is the deposit of material comprising the active shoreline. The berm has a "crest" (top) and a "face" — the latter being the slope leading down towards the water from the crest. At the very bottom of the face, there may be a "trough", and further seaward one or more long shore bars: slightly raised, underwater embankments formed where the waves first start to break.
The sand deposit may extend well inland from the "berm crest", where there may be evidence of one or more older crests (the "storm beach") resulting from very large storm waves and beyond the influence of the normal waves. At some point the influence of the waves (even storm waves) on the material comprising the beach stops, and if the particles are small enough (sand size or smaller), winds shape the feature. Where wind is the force distributing the grains inland, the deposit behind the beach becomes a "dune".
These geomorphic features compose what is called the "beach profile". The beach profile changes seasonally due to the change in wave energy experienced during summer and winter months. In temperate areas where summer is characterised by calmer seas and longer periods between breaking wave crests, the beach profile is higher in summer. The gentle wave action during this season tends to transport sediment up the beach towards the berm where it is deposited and remains while the water recedes. Onshore winds carry it further inland forming and enhancing dunes.
Conversely, the beach profile is lower in the storm season (winter in temperate areas) due to the increased wave energy, and the shorter periods between breaking wave crests. Higher energy waves breaking in quick succession tend to mobilise sediment from the shallows, keeping it in suspension where it is prone to be carried along the beach by longshore currents, or carried out to sea to form longshore bars, especially if the longshore current meets an outflow from a river or flooding stream. The removal of sediment from the beach berm and dune thus decreases the beach profile.
In tropical areas, the storm season tends to be during the summer months, with calmer weather commonly associated with the winter season.
If storms coincide with unusually high tides, or with a freak wave event such as a tidal surge or tsunami which causes significant coastal flooding, substantial quantities of material may be eroded from the coastal plain or dunes behind the berm by receding water. This flow may alter the shape of the coastline, enlarge the mouths of rivers and create new deltas at the mouths of streams that formerly were not powerful enough to overcome longshore movement of sediment.
The line between beach and dune is difficult to define in the field. Over any significant period of time, sediment is always being exchanged between them. The "drift line" (the high point of material deposited by waves) is one potential demarcation. This would be the point at which significant wind movement of sand could occur, since the normal waves do not wet the sand beyond this area. However, the drift line is likely to move inland under assault by storm waves.
Beaches and recreation.
History.
The development of the beach as a popular leisure resort from the mid-19th century was the first manifestation of what is now the global tourist industry. The first seaside resorts were opened in the 18th century for the aristocracy, who began to frequent the seaside as well as the then fashionable spa towns, for recreation and health. One of the earliest such seaside resorts, was Scarborough in Yorkshire during the 1720s; it had been a fashionable spa town since a stream of acidic water was discovered running from one of the cliffs to the south of the town in the 17th century. The first rolling bathing machines were introduced by 1735.
The opening of the resort in Brighton and its reception of royal patronage from King George IV, extended the seaside as a resort for health and pleasure to the much larger London market, and the beach became a centre for upper-class pleasure and frivolity. This trend was praised and artistically elevated by the new romantic ideal of the picturesque landscape; Jane Austen's unfinished novel "Sanditon" is an example of that. Later, Queen Victoria's long-standing patronage of the Isle of Wight and Ramsgate in Kent ensured that a seaside residence was considered as a highly fashionable possession for those wealthy enough to afford more than one home.
Seaside resorts for the working class.
The extension of this form of leisure to the middle and working class began with the development of the railways in the 1840s, which offered cheap and affordable fares to fast growing resort towns. In particular, the completion of a branch line to the small seaside town Blackpool from Poulton led to a sustained economic and demographic boom. A sudden influx of visitors, arriving by rail, provided the motivation for entrepreneurs to build accommodation and create new attractions, leading to more visitors and a rapid cycle of growth throughout the 1850s and 1860s.
The growth was intensified by the practice among the Lancashire cotton mill owners of closing the factories for a week every year to service and repair machinery. These became known as wakes weeks. Each town's mills would close for a different week, allowing Blackpool to manage a steady and reliable stream of visitors over a prolonged period in the summer. A prominent feature of the resort was the promenade and the pleasure piers, where an eclectic variety of performances vied for the people's attention. In 1863, the North Pier in Blackpool was completed, rapidly becoming a centre of attraction for elite visitors. Central Pier was completed in 1868, with a theatre and a large open-air dance floor.
Many of the popular beach resorts were equipped with bathing machines because even the all-covering beachwear of the period was considered immodest. By the end of the century the English coastline had over 100 large resort towns, some with populations exceeding 50,000.
Expansion around the world.
The development of the seaside resort abroad was stimulated by the well developed English love of the beach. The French Riviera alongside the Mediterranean had already become a popular destination for the British upper class by the end of the 18th century. In 1864, the first railway to Nice was completed, making the Riviera accessible to visitors from all over Europe. By 1874, residents of foreign enclaves in Nice, most of whom were British, numbered 25,000. The coastline became renowned for attracting the royalty of Europe, including Queen Victoria and King Edward VII.
Continental European attitudes towards gambling and nakedness tended to be more lax than in Britain, so British and French entrepreneurs were quick to exploit the possibilites. In 1863, the Prince of Monaco, Charles III and François Blanc, a French businessman, arranged for steamships and carriages to take visitors from Nice to Monaco, where large luxury hotels, gardens and casinos were built. The place was renamed Monte Carlo.
Commercial seabathing also spread to the United States and parts of the British Empire such as Australia where surfing was developed in the early 20th century. By the 1970s cheap and affordable air travel was the catalayst for the growth of a truly global tourism market which benefited areas such as the Spain and the South of France with sunny climates.
Today.
Beaches can be popular on warm sunny days. In the Victorian era, many popular beach resorts were equipped with bathing machines because even the all-covering beachwear of the period was considered immodest. This social standard still prevails in many Muslim countries. At the other end of the spectrum are topfree beaches and nude beaches where clothing is optional or not allowed. In most countries social norms are significantly different on a beach in hot weather, compared to adjacent areas where similar behaviour might not be tolerated and might even be prosecuted.
In more than thirty countries in Europe, South Africa, New Zealand, Canada, Costa Rica, South America and the Caribbean, the best recreational beaches are awarded Blue Flag status, based on such criteria as water quality and safety provision. Subsequent loss of this status can have a severe effect on tourism revenues.
Beaches are often dumping grounds for waste and litter, necessitating the use of beach cleaners and other cleanup projects. More significantly, many beaches are a discharge zone for untreated sewage in most underdeveloped countries; even in developed countries beach closure is an occasional circumstance due to sanitary sewer overflow. In these cases of marine discharge, waterborne disease from fecal pathogens and contamination of certain marine species is a frequent outcome.
Artificial beaches.
Some beaches are artificial; they are either permanent or temporary (For examples see Monaco, Paris, Copenhagen, Rotterdam, Nottingham, Toronto, Hong Kong, Singapore, and Tianjin).
The soothing qualities of a beach and the pleasant environment offered to the beachgoer are replicated in artificial beaches, such as "beach style" pools with zero-depth entry and wave pools that recreate the natural waves pounding upon a beach. In a zero-depth entry pool, the bottom surface slopes gradually from above water down to depth. Another approach involves so-called urban beaches, a form of public park becoming common in large cities. Urban beaches attempt to mimic natural beaches with fountains that imitate surf and mask city noises, and in some cases can be used as a play park.
Beach nourishment involves pumping sand onto beaches to improve their health. Beach nourishment is common for major beach cities around the world; however the beaches that have been nourished can still appear quite natural and often many visitors are unaware of the works undertaken to support the health of the beach. Such beaches are often not recognized (by consumers) as artificial. The Surfrider Foundation has debated the merits of artificial reefs with members torn between their desire to support natural coastal environments and opportunities to enhance the quality of surfing waves. Similar debates surround beach nourishment and snow cannon in sensitive environments.
Restrictions on access.
Public access to beaches is restricted in some parts of the world. For example, most beaches on the Jersey Shore are restricted to people who can purchase beach tags.
Beach formation.
Beaches are the result of wave action by which waves or currents move sand or other loose sediments of which the beach is made as these particles are held in suspension. Alternatively, sand may be moved by saltation (a bouncing movement of large particles).
Beach materials come from erosion of rocks offshore, as well as from headland erosion and slumping producing deposits of scree. Some of the whitest sand in the world, along Florida's Emerald Coast, comes from the erosion of quartz in the Appalachian Mountains.
A coral reef offshore is a significant source of sand particles. Some species of fish that feed on algae attached to coral outcrops and rocks can create substantial quantities of sand particles over their lifetime as they nibble during feeding, digesting the organic matter, and discarding the rock and coral particles which pass through their digestive tracts.
The composition of the beach depends upon the nature and quantity of sediments upstream of the beach, and the speed of flow and turbidity of water and wind.
Sediments are moved by moving water and wind according to their particle size and state of compaction. Particles tend to settle and compact in still water. Once compacted, they are more resistant to erosion. Established vegetation (especially species with complex network root systems) will resist erosion by slowing the fluid flow at the surface layer.
When affected by moving water or wind, particles that are eroded and held in suspension will increase the erosive power of the fluid that holds them by increasing the average density, viscosity and volume of the moving fluid.
The nature of sediments found on a beach tends to indicate the energy of the waves and wind in the locality. Coastlines facing very energetic wind and wave systems will tend to hold only large rocks as smaller particles will be held in suspension in the turbid water column and carried to calmer areas by longshore currents and tides. Coastlines that are protected from waves and winds will tend to allow finer sediments such as clays and mud to precipitate creating mud flats and mangrove forests.
The shape of a beach depends on whether the waves are constructive or destructive, and whether the material is sand or shingle.
Waves are constructive if the period between their wave crests is long enough for the breaking water to recede and the sediment to settle before the succeeding wave arrives and breaks. Fine sediment transported from lower down the beach profile will compact if the receding water percolates or soaks into the beach. Compacted sediment is more resistant to movement by turbulent water from succeeding waves.
Conversely, waves are destructive if the period between the wave crests is short. Sediment that remains in suspension when the following wave crest arrives will not be able to settle and compact and will be more susceptible to erosion by longshore currents and receding tides.
Constructive waves move material up the beach while destructive waves move the material down the beach. During seasons when destructive waves are prevalent, the shallows will carry an increased load of sediment and organic matter in suspension.
On sandy beaches, the turbulent backwash of destructive waves removes material forming a gently sloping beach. On pebble and shingle beaches the swash is dissipated more quickly because the large particle size allows greater percolation, thereby reducing the power of the backwash, and the beach remains steep.
Compacted fine sediments will form a smooth beach surface that resists wind and water erosion. During hot calm seasons, a crust may form on the surface of ocean beaches as the heat of the sun evaporates the water leaving the salt which crystallises around the sand particles. This crust forms an additional protective layer that resists wind erosion unless disturbed by animals, or dissolved by the advancing tide.
Cusps and horns form where incoming waves divide, depositing sand as horns and scouring out sand to form cusps. This forms the uneven face on some sand shorelines.
Beach erosion and accretion.
Natural erosion and accretion.
Causes.
Beaches are changed in shape chiefly by the movement of water and wind. Any weather event that is associated with turbid or fast flowing water, or high winds will erode exposed beaches. Longshore currents will tend to replenish beach sediments and repair storm damage. Tidal waterways generally change the shape of their adjacent beaches by small degrees with every tidal cycle. Over time these changes can become substantial leading to significant changes in the size and location of the beach.
Effects on flora.
Changes in the shape of the beach may undermine the roots of large trees and other flora. Many beach adapted species (such as coconut palms) have a fine root system and large root ball which tends to withstand wave and wind action and tends to stabilize beaches better than other trees with a lesser root ball.
Effects on adjacent land.
Erosion of beaches can expose less resilient soils and rocks to wind and wave action leading to undermining of coastal headlands eventually resulting in catastrophic collapse of large quantities of overburden into the shallows. This material may be distributed along the beach front leading to a change in the habitat as sea grasses and corals in the shallows may be buried or deprived of light and nutrients.
Manmade erosion and accretion.
Coastal areas settled by man inevitably become subject to the effects of man-made structures and processes. Over long periods of time these influences may substantially alter the shape of the coastline, and the character of the beach.
Destruction of flora.
Beach front flora plays a major role in stabilizing the foredunes and preventing beach head erosion and inland movement of dunes. If flora with network root systems (creepers, grasses and palms) are able to become established, they provide an effective coastal defense as they trap sand particles and rainwater and enrich the surface layer of the dunes, allowing other plant species to become established. They also protect the berm from erosion by high winds, freak waves and subsiding flood waters.
Over long periods of time, well stabilized foreshore areas will tend to accrete, while unstabilized foreshores will tend to erode, leading to substantial changes in the shape of the coastline. These changes usually occur over periods of many years. Freak wave events such as tsunami, tidal waves, and storm surges may substantially alter the shape, profile and location of a beach within hours.
Destruction of flora on the berm by the use of herbicides, excessive pedestrian or vehicular traffic, or disruption to fresh water flows may lead to erosion of the berm and dunes. While the destruction of flora may be a gradual process that is imperceptible to regular beach users, it often becomes immediately apparent after storms associated with high winds and freak wave events that can rapidly move large volumes of exposed and unstable sand, depositing them further inland, or carrying them out into the permanent water forming offshore bars, lagoons or increasing the area of the beach exposed at low tide.
Large and rapid movements of exposed sand can bury and smother flora in adjacent areas, aggravating the loss of habitat for fauna, and enlarging the area of instability. If there is an adequate supply of sand, and weather conditions do not allow vegetation to recover and stabilize the sediment, wind-blown sand can continue to advance, engulfing and permanently altering downwind landscapes.
Sediment moved by waves or receding flood waters can be deposited in coastal shallows, engulfing reed beds and changing the character of underwater flora and fauna in the coastal shallows.
Burning or clearance of vegetation on the land adjacent to the beach head, for farming and residential development, changes the surface wind patterns, and exposes the surface of the beach to wind erosion.
Farming and residential development are also commonly associated with changes in local surface water flows. If these flows are concentrated in storm water drains emptying onto the beach head, they may erode the beach creating a lagoon or delta.
Dense vegetation tends to absorb rainfall reducing the speed of runoff and releasing it over longer periods of time. Destruction by burning or clearance of the natural vegetation tends to increase the speed and erosive power of runoff from rainfall. This runoff will tend to carry more silt and organic matter from the land onto the beach and into the sea. If the flow is constant, runoff from cleared land arriving at the beach head will tend to deposit this material into the sand changing its color, odor and fauna.
Creation of beach access points.
The concentration of pedestrian and vehicular traffic accessing the beach for recreational purposes may cause increased erosion at the access points if measures are not taken to stabilize the beach surface above high-water mark. Recognition of the dangers of loss of beach front flora has caused many local authorities responsible for managing coastal areas to restrict beach access points by physical structures or legal sanctions, and fence off foredunes in an effort to protect the flora. These measures are often associated with the construction of structures at these access points to allow traffic to pass over or through the dunes without causing further damage.
Concentration of runoff.
Beaches provide a filter for runoff from the coastal plain. If the runoff is naturally dispersed along the beach, water borne silt and organic matter will be retained on the land and will feed the flora in the coastal area. Runoff that is dispersed along the beach will tend to percolate through the beach and may emerge from the beach at low tide.
The retention of the fresh water may also help to maintain underground water reserves and will resist salt water incursion. If the surface flow of the runoff is diverted and concentrated by drains that create constant flows over the beach above the sea or river level, the beach will be eroded and ultimately form an inlet unless longshore flows deposit sediments to repair the breach.
Once eroded, an inlet may allow tidal inflows of salt water to pollute areas inland from the beach and may also affect the quality of underground water supplies and the height of the water table.
Deprivation of runoff.
Some flora naturally occurring on the beach head requires fresh water runoff from the land. Diversion of fresh water runoff into drains may deprive these plants of their water supplies and allow sea water incursion, increasing the saltiness of the ground water. Species that are not able to survive in salt water may die and be replaced by mangroves or other species adapted to salty environments.
Inappropriate beach nourishment.
Beach nourishment is the importing and deposition of sand or other sediments in an effort to restore a beach that has been damaged by erosion. Beach nourishment often involves excavation of sediments from riverbeds or sand quarries. This excavated sediment may be substantially different in size and appearance to the naturally occurring beach sand.
In extreme cases, beach nourishment may involve placement of large pebbles or rocks in an effort to permanently restore a shoreline subject to constant erosion and loss of foreshore. This is often required where the flow of new sediment caused by the longshore current has been disrupted by construction of harbors, breakwaters, causeways or boat ramps, creating new current flows that scour the sand from behind these structures, and deprive the beach of restorative sediments. If the causes of the erosion are not addressed, beach nourishment can become a necessary and permanent feature of beach maintenance.
During beach nourishment activities, care must be taken to place new sediments so that the new sediments compact and stabilize before aggressive wave or wind action can erode them. Material that is concentrated too far down the beach may form a temporary groyne that will encourage scouring behind it. Sediments that are too fine or too light may be eroded before they have compacted or been integrated into the established vegetation. Foreign unwashed sediments may introduce flora or fauna that are not usually found in that locality.
Brighton Beach, on the south coast of England, is a shingle beach that has been nourished with very large pebbles in an effort to withstand erosion of the upper area of the beach. These large pebbles made the beach unwelcoming for pedestrians for a period of time until natural processes integrated the naturally occurring shingle into the pebble base.
Beach access design.
Beach access is an important consideration where substantial numbers of pedestrians or vehicles require access to the beach. Allowing random access across delicate foredunes is seldom considered good practice as it is likely to lead to destruction of flora and consequent erosion of the fore dunes.
A well designed beach access should:
Concrete ramp or steps.
A concrete ramp should follow the natural profile of the beach to prevent it from changing the normal flow of waves, longshore currents, water and wind. A ramp that is below the beach profile will tend to become buried and cease to provide a good surface for vehicular traffic. A ramp or stair that protrudes above the beach profile will tend to disrupt longshore currents creating deposits in front of the ramp, and scouring behind. Concrete ramps are the most expensive vehicular beach accesses to construct requiring use of a quick drying concrete or a coffer dam to protect them from tidal water during the concrete curing process. Concrete is favored where traffic flows are heavy and access is required by vehicles that are not adapted to soft sand (e.g. road registered passenger vehicles and boat trailers). Concrete stairs are commonly favored on beaches adjacent to population centers where beach users may arrive on the beach in street shoes, or where the foreshore roadway is substantially higher than the beach head and a ramp would be too steep for safe use by pedestrians. A composite stair ramp may incorporate a central or side stair with one or more ramps allowing pedestrians to lead buggies or small boat dollies onto the beach without the aid of a powered vehicle or winch. Concrete ramps and steps should be maintained to prevent buildup of moss or algae that may make their wet surfaces slippery and dangerous to pedestrians and vehicles.
Corduroy (beach ladder).
A corduroy or beach ladder (or board and chain) is an array of planks (usually hardwood or treated timber) laid close together and perpendicular to the direction of traffic flow, and secured at each end by a chain or cable to form a pathway or ramp over the sand dune. Corduroys are cheap and easy to construct and quick to deploy or relocate. They are commonly used for pedestrian access paths and light duty vehicular access ways. They naturally conform to the shape of the underlying beach or dune profile, and adjust well to moderate erosion, especially longshore drift. However, they can cease to be an effective access surface if they become buried or undermined by erosion by surface runoff coming from the beach head. If the corduroy is not wide enough for vehicles using it, the sediment on either side may be displaced creating a spoon drain that accelerates surface run off and can quickly lead to serious erosion. Significant erosion of the sediment beside and under the corduroy can render it completely ineffective and make it dangerous to pedestrian users who may fall between the planks.
Fabric ramp.
Fabric ramps are commonly employed by the military for temporary purposes where the underlying sediment is stable and hard enough to support the weight of the traffic. A sheet of porous fabric is laid over the sand to stabilize the surface and prevent vehicles from bogging. Fabric Ramps usually cease to be useful after one tidal cycle as they are easily washed away, or buried in sediment.
Foliage ramp.
A foliage ramp is formed by planting resilient species of hardy plants such as grasses over a well formed sediment ramp. The plants may be supported while they become established by placement of layers of mesh, netting, or coarse organic material such as vines or branches. This type of ramp is ideally suited for intermittent use by vehicles with a low wheel loading such as dune buggies or agricultural vehicles with large tyres. A foliage ramp should require minimal maintenance if initially formed to follow the beach profile, and not overused.
Gravel ramp.
A gravel ramp is formed by excavating the underlying loose sediment and filling the excavation with layers of gravel of graduated sizes as defined by John Loudon McAdam. The gravel is compacted to form a solid surface according to the needs of the traffic. Gravel ramps are less expensive to construct than concrete ramps and are able to carry heavy road traffic provided the excavation is deep enough to reach solid subsoil. Gravel ramps are subject to erosion by water. If the edges are retained with boards or walls and the profile matches the surrounding beach profile, a gravel ramp may become more stable as finer sediments are deposited by percolating water.
Longest beaches.
Amongst the world's longest beaches are:-
Beach wildlife.
A beach is an unstable environment that exposes plants and animals to changeable and potentially harsh conditions. Some animals burrow into the sand and feed on material deposited by the waves. Crabs, insects and shorebirds feed on these beach dwellers. The endangered piping plover and some tern species rely on beaches for nesting. Sea turtles also bury their eggs in ocean beaches. Seagrasses and other beach plants grow on undisturbed areas of the beach and dunes.
Ocean beaches are habitats with organisms adapted to salt spray, tidal overwash, and shifting sands. Some of these organisms are found only on beaches. Examples of these beach organisms in the southeast US include plants like sea oats, sea rocket, beach elder, beach morning glory ("Ipomoea pes-caprae"), and beach peanut, and animals such as mole crabs ("Hippoidea"), coquina clams ("Donax"), ghost crabs, and white beach tiger beetles.

</doc>
<doc id="40633" url="http://en.wikipedia.org/wiki?curid=40633" title="Redundancy">
Redundancy

Redundancy or redundant may refer to:

</doc>
<doc id="40634" url="http://en.wikipedia.org/wiki?curid=40634" title="Convex hull">
Convex hull

In mathematics, the convex hull or convex envelope of a set "X" of points in the Euclidean plane or Euclidean space is the smallest convex set that contains "X". For instance, when "X" is a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around "X".
Formally, the convex hull may be defined as the intersection of all convex sets containing "X" or as the set of all convex combinations of points in "X". With the latter definition, convex hulls may be extended from Euclidean spaces to arbitrary real vector spaces; they may also be generalized further, to oriented matroids.
The algorithmic problem of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces is one of the fundamental problems of computational geometry.
Definitions.
A set of points is defined to be convex if it contains the line segments connecting each pair of its points. The convex hull of a given set "X" may be defined as
It is not obvious that the first definition makes sense: why should there exist a unique minimal convex set containing "X", for every "X"? However, the second definition, the intersection of all convex sets containing "X" is well-defined, and it is a subset of every other convex set "Y" that contains "X", because "Y" is included among the sets being intersected. Thus, it is exactly the unique minimal convex set containing "X". Each convex set containing "X" must (by the assumption that it is convex) contain all convex combinations of points in "X", so the set of all convex combinations is contained in the intersection of all convex sets containing "X". Conversely, the set of all convex combinations is itself a convex set containing "X", so it also contains the intersection of all convex sets containing "X", and therefore the sets given by these two definitions must be equal.
In fact, according to Carathéodory's theorem, if "X" is a subset of an "N"-dimensional vector space, convex combinations of at most "N" + 1 points are sufficient in the definition above. Therefore, the convex hull of a set "X" of three or more points in the plane is the union of all the triangles determined by triples of points from "X", and more generally in "N"-dimensional space the convex hull is the union of the simplices determined by at most "N" + 1 vertices from X.
If the convex hull of "X" is a closed set (as happens, for instance, if "X" is a finite set or more generally a compact set), then it is the intersection of all closed half-spaces containing "X". The hyperplane separation theorem proves that in this case, each point not in the convex hull can be separated from the convex hull by a half-space. However, there exist convex sets, and convex hulls of sets, that cannot be represented in this way. One example is an open halfspace together with a single point on its boundary.
More abstractly, the convex-hull operator Conv() has the characteristic properties of a closure operator:
Convex hull of a finite point set.
The convex hull of a finite point set formula_1 is the set of all convex combinations of its points. In a convex combination, each point formula_2 in formula_1 is assigned a weight or coefficient formula_4 in such a way that the coefficients are all non-negative and sum to one, and these weights are used to compute a weighted average of the points. For each choice of coefficients, the resulting convex combination is a point in the convex hull, and the whole convex hull can be formed by choosing coefficients in all possible ways. Expressing this as a single formula, the convex hull is the set:
The convex hull of a finite point set formula_6 forms a convex polygon when "n" = 2, or more generally a convex polytope in formula_7. Each point formula_2 in formula_1 that is not in the convex hull of the other points (that is, such that formula_10) is called a vertex of formula_11. In fact, every convex polytope in formula_7 is the convex hull of its vertices.
If the points of formula_1 are all on a line, the convex hull is the line segment joining the outermost two points.
When the set formula_1 is a nonempty finite subset of the plane (that is, two-dimensional), we may imagine stretching a rubber band so that it surrounds the entire set formula_1 and then releasing it, allowing it to contract; when it becomes taut, it encloses the convex hull of formula_1.
In two dimensions, the convex hull is sometimes partitioned into two polygonal chains, the upper hull and the lower hull, stretching between the leftmost and rightmost points of the hull. More generally, for points in any dimension in general position, each facet of the convex hull is either oriented upwards (separating the hull from points directly above it) or downwards; the union of the upward-facing facets forms a topological disk, the upper hull, and similarly the union of the downward-facing facets forms the lower hull.
Computation of convex hulls.
In computational geometry, a number of algorithms are known for computing the convex hull for a finite set of points and for other geometric objects.
Computing the convex hull means constructing an unambiguous, efficient representation of the required convex shape. The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and h, the number of points on the convex hull.
For points in two and three dimensions, output-sensitive algorithms are known that compute the convex hull in time O("n" log "h"). For dimensions "d" higher than 3, the time for computing the convex hull is formula_17, matching the worst-case output complexity of the problem.
Minkowski addition and convex hulls.
The operation of taking convex hulls behaves well with respect to the Minkowski addition of sets.
More generally, the "Minkowski sum" of a finite family of (non-empty) sets Sn is the set formed by element-wise addition of vectors
This result holds more generally for each finite collection of non-empty sets
In other words, the operations of Minkowski summation and of forming convex hulls are commuting operations.
These results show that "Minkowski addition" differs from the "union "operation of set theory; indeed, the union of two convex sets need "not" be convex: The inclusion Conv(S) ∪ Conv(T) ⊆ Conv(S ∪ T) is generally strict. The convex-hull operation is needed for the set of convex sets to form a lattice, in which the "join" operation is the convex hull of the union of two convex sets
Relations to other structures.
The Delaunay triangulation of a point set and its dual, the Voronoi diagram, are mathematically related to convex hulls: the Delaunay triangulation of a point set in R"n" can be viewed as the projection of a convex hull in R"n"+1.
Topologically, the convex hull of an open set is always itself open, and the convex hull of a compact set is always itself compact; however, there exist closed sets for which the convex hull is not closed. For instance, the closed set
has the open upper half-plane as its convex hull.
Applications.
The problem of finding convex hulls finds its practical applications in pattern recognition, image processing, statistics, geographic information system, game theory, construction of phase diagrams, and static code analysis by abstract interpretation. It also serves as a tool, a building block for a number of other computational-geometric algorithms such as the rotating calipers method for computing the width and diameter of a point set.
The convex hull is commonly known as the minimum convex polygon (MCP) in ethology, where it is a classic, though perhaps simplistic, approach in estimating an animal's home range based on points where the animal has been observed. Outliers can make the MCP excessively large, which has motivated relaxed approaches that contain only a subset of the observations (e.g., find an MCP that contains at least 95% of the points).

</doc>
<doc id="40642" url="http://en.wikipedia.org/wiki?curid=40642" title="NeXTSTEP">
NeXTSTEP

NeXTSTEP (also stylized as NeXTstep, NeXTStep, and NEXTSTEP) is an object-oriented, multitasking operating system which was developed by NeXT Computer to run on its range of proprietary workstation computers, such as the NeXTcube. It was later ported to several other computer architectures.
Overview.
NeXTSTEP is a combination of several parts:
NeXTSTEP is notable for having been a preeminent implementation of the latter three items. The toolkits offer considerable power, and are the canonical development system for all of the software on the machine. Many developers found that the distinctive features of the Objective-C language made the writing of applications with NeXTSTEP far easier than on many competing systems, which made NeXTStep a paragon of computer development.
NeXTSTEP's user interface is considered to be refined and consistent. It introduced the idea of the Dock (carried through OpenStep and into today's OS X) and the Shelf. NeXTSTEP also originated or innovated a large number of other GUI concepts which became common in other operating systems: 3D "chiseled" widgets, large full-color icons, system-wide drag and drop of a wide range of objects beyond file icons, system-wide piped services, real-time scrolling and window dragging, properties dialog boxes called "inspectors", and window modification notices (such as the saved status of a file). The system is among the first general-purpose user interfaces to handle publishing color standards, transparency, sophisticated sound and music processing (through a Motorola 56000 DSP), advanced graphics primitives, internationalization, and modern typography, in a consistent manner across all applications.
Additional kits were added to the product line to make the system more attractive. These include Portable Distributed Objects (PDO), which allow easy remote invocation, and Enterprise Objects Framework, a powerful object-relational database system. The kits made the system particularly interesting to custom application programmers, and NeXTSTEP had a long history in the financial programming community.
History.
A preview release of NeXTSTEP (version 0.8) was shown with the launch of the NeXT Computer on October 12, 1988. The first full release, NeXTSTEP 1.0, shipped on September 18, 1989. The last version, 3.3, was released in early 1995, by which time it ran on not only the Motorola 68000 family processors used in NeXT computers, but also on Intel x86, Sun SPARC, and HP PA-RISC-based systems.
NeXTSTEP was later modified to separate the underlying operating system from the higher-level object libraries. The result was the OpenStep API, which ran on multiple underlying operating systems, including NeXT's own OPENSTEP. NeXTSTEP's legacy stands today in the form of its direct descendents, Apple's OS X and iOS operating systems.
Legacy.
The first web browser, WorldWideWeb, was developed on the NeXTSTEP platform.
 1990 CERN: A Joint proposal for a hypertext system is presented to the management. Mike Sendall buys a NeXT cube for evaluation, and gives it to Tim Berners-Lee. Tim's prototype implementation on NeXTStep is made in the space of a few months, thanks to the qualities of the NeXTStep software development system. This prototype offers WYSIWYG browsing/authoring! Current Web browsers used in "surfing the Internet" are mere passive windows, depriving the user of the possibility to contribute. During some sessions in the CERN cafeteria, Tim and I try to find a catching name for the system. I was determined that the name should not yet again be taken from Greek mythology. Tim proposes "World-Wide Web". I like this very much, except that it is difficult to pronounce in French...
 — Robert Cailliau, 2 November 1995
Some features and keyboard shortcuts now commonly found in web browsers can be traced back to NeXTSTEP conventions. The basic layout options of HTML 1.0 and 2.0 are attributable to those features available in NeXT's Text class.
In the 1990s, the pioneering PC games "Wolfenstein 3D", "Doom" (with its WAD level editor), "Doom II", and "Quake" (with its respective level editor) were developed by id Software on NeXT machines. Other games based on the "Doom" engine such as "Heretic" and its sequel "Hexen" by Raven Software as well as "Strife" by Rogue Entertainment were also developed on NeXT hardware using id's tools.
Altsys made a NeXTSTEP application called Virtuoso, version 2 of which was ported to Mac OS and Windows to become Macromedia FreeHand version 4. The modern "Notebook" interface for Mathematica, and the advanced spreadsheet Lotus Improv, were developed using NeXTSTEP. The software that controlled MCI's Friends and Family calling plan program was developed using NeXTSTEP.
About the time of the release of NeXTSTEP 3.2, NeXT partnered with Sun Microsystems to develop OpenStep. It is the product of an effort to separate the underlying operating system from the higher-level object libraries to create a cross-platform object-oriented API standard derived from NeXTSTEP. The OpenStep API targets multiple underlying operating systems, including NeXT's own OPENSTEP. Implementations of that standard were released for Sun's Solaris, Windows NT, and NeXT's version of the Mach kernel. NeXT's implementation is called "OPENSTEP for Mach" and its first release (4.0) superseded NeXTSTEP 3.3 on NeXT, Sun and Intel IA-32 systems.
Following an announcement on December 20, 1996, Apple Computer acquired NeXT on February 4, 1997 for $429 million. Based upon the "OPENSTEP for Mach" operating system, and developing the OPENSTEP API to become Cocoa, Apple created the basis of OS X, and eventually, in turn, of iOS.
Apple purchased NeXT in 1996, to use its more advanced operating system to replace classic Mac OS, which Apple had been unable to modernize internally. Apple's OS X and iOS are direct descendants of NeXTSTEP, through the OPENSTEP lineage; and several OS X apps such as TextEdit, Mail, and Chess are descended from NeXTSTEP applications.
A free software implementation of the OpenStep standard, GNUstep, also exists.
In popular culture.
The anime series Serial Experiments Lain was influenced by NeXTSTEP and Mac OS. References may be found throughout the show and its affiliated media, most notably the slogan for the Lain PSX Game, "Close the world, Open the NeXT".
The interface of NeXTSTEP 3.3 made a brief appearance in the animated film "", as Ritsuko Akagi's workstation desktop.
Release history.
Versions up to 4.1 are general releases. OPENSTEP 4.2 pre-release 2 is a bug-fix release published by Apple and was supported for five years after its September 1997 release.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="40643" url="http://en.wikipedia.org/wiki?curid=40643" title="Non-uniform memory access">
Non-uniform memory access

Non-uniform memory access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. Under NUMA, a processor can access its own local memory faster than non-local memory (memory local to another processor or memory shared between processors). The benefits of NUMA are limited to particular workloads, notably on servers where the data are often associated strongly with certain tasks or users.
NUMA architectures logically follow in scaling from symmetric multiprocessing (SMP) architectures. They were developed commercially during the 1990s by Burroughs (later Unisys), Convex Computer (later Hewlett-Packard), Honeywell Information Systems Italy (HISI) (later Groupe Bull), Silicon Graphics (later Silicon Graphics International), Sequent Computer Systems (later IBM), Data General (later EMC), and Digital (later Compaq, now HP). Techniques developed by these companies later featured in a variety of Unix-like operating systems, and to an extent in Windows NT.
The first commercial implementation of a NUMA-based Unix system was the Symmetrical Multi Processing XPS-100 family of servers, designed by Dan Gielan of VAST Corporation for Honeywell Information Systems Italy.
Basic concept.
Modern CPUs operate considerably faster than the main memory they use. In the early days of computing and data processing, the CPU generally ran slower than its own memory. The performance lines of processors and memory crossed in the 1960s with the advent of the first supercomputers. Since then, CPUs increasingly have found themselves "starved for data" and having to stall while waiting for data to arrive from memory. Many supercomputer designs of the 1980s and 1990s focused on providing high-speed memory access as opposed to faster processors, allowing the computers to work on large data sets at speeds other systems could not approach.
Limiting the number of memory accesses provided the key to extracting high performance from a modern computer. For commodity processors, this meant installing an ever-increasing amount of high-speed cache memory and using increasingly sophisticated algorithms to avoid cache misses. But the dramatic increase in size of the operating systems and of the applications run on them has generally overwhelmed these cache-processing improvements. Multi-processor systems without NUMA make the problem considerably worse. Now a system can starve several processors at the same time, notably because only one processor can access the computer's memory at a time.
NUMA attempts to address this problem by providing separate memory for each processor, avoiding the performance hit when several processors attempt to address the same memory. For problems involving spread data (common for servers and similar applications), NUMA can improve the performance over a single shared memory by a factor of roughly the number of processors (or separate memory banks). Another approach to addressing this problem, utilized mainly by non-NUMA systems, is the multi-channel memory architecture; multiple memory channels are increasing the number of simultaneous memory accesses.
Of course, not all data ends up confined to a single task, which means that more than one processor may require the same data. To handle these cases, NUMA systems include additional hardware or software to move data between memory banks. This operation slows the processors attached to those banks, so the overall speed increase due to NUMA depends heavily on the nature of the running tasks.
Intel announced NUMA compatibility for its x86 and Itanium servers in late 2007 with its Nehalem and Tukwila CPUs. Both CPU families share a common chipset; the interconnection is called Intel Quick Path Interconnect (QPI). AMD implemented NUMA with its Opteron processor (2003), using HyperTransport. Freescale's NUMA for PowerPC is called CoreNet.
Cache coherent NUMA (ccNUMA).
Nearly all CPU architectures use a small amount of very fast non-shared memory known as cache to exploit locality of reference in memory accesses. With NUMA, maintaining cache coherence across shared memory has a significant overhead. Although simpler to design and build, non-cache-coherent NUMA systems become prohibitively complex to program in the standard von Neumann architecture programming model.
Typically, ccNUMA uses inter-processor communication between cache controllers to keep a consistent memory image when more than one cache stores the same memory location. For this reason, ccNUMA may perform poorly when multiple processors attempt to access the same memory area in rapid succession. Support for NUMA in operating systems attempts to reduce the frequency of this kind of access by allocating processors and memory in NUMA-friendly ways and by avoiding scheduling and locking algorithms that make NUMA-unfriendly accesses necessary.
Alternatively, cache coherency protocols such as the MESIF protocol attempt to reduce the communication required to maintain cache coherency. Scalable Coherent Interface (SCI) is an IEEE standard defining a directory-based cache coherency protocol to avoid scalability limitations found in earlier multiprocessor systems. For example, SCI is used as the basis for the NumaConnect technology.
As of 2011, ccNUMA systems are multiprocessor systems based on the AMD Opteron processor, which can be implemented without external logic, and the Intel Itanium processor, which requires the chipset to support NUMA. Examples of ccNUMA-enabled chipsets are the SGI Shub (Super hub), the Intel E8870, the HP sx2000 (used in the Integrity and Superdome servers), and those found in NEC Itanium-based systems. Earlier ccNUMA systems such as those from Silicon Graphics were based on MIPS processors and the DEC Alpha 21364 (EV7) processor.
NUMA vs. cluster computing.
One can view NUMA as a tightly coupled form of cluster computing. The addition of virtual memory paging to a cluster architecture can allow the implementation of NUMA entirely in software. However, the inter-node latency of software-based NUMA remains several orders of magnitude greater (slower) than that of hardware-based NUMA.
Software support.
Since NUMA largely influences memory access performance, certain software optimizations are needed to allow scheduling threads and processes close to their in-memory data.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="40645" url="http://en.wikipedia.org/wiki?curid=40645" title="Haar measure">
Haar measure

In mathematical analysis, the Haar measure assigns an "invariant volume" to subsets of locally compact topological groups, consequently defining an integral for functions on those groups.
This measure was introduced by Alfréd Haar in 1933. Haar measures are used in many parts of analysis, number theory, group theory, representation theory, statistics, and ergodic theory.
Preliminaries.
Let ("G.") be a locally compact Hausdorff topological group. The σ-algebra generated by all open sets of "G" is called the Borel algebra. An element of the Borel algebra is called a Borel set. If "g" is an element of "G" and "S" is a subset of "G", then we define the left and right translates of "S" as follows:
Left and right translates map Borel sets into Borel sets.
A measure μ on the Borel subsets of "G" is called "left-translation-invariant" if for all Borel subsets "S" of "G" and all "g" in "G" one has
A similar definition is made for right translation invariance.
Haar's theorem.
There is, up to a positive multiplicative constant, a unique countably additive, nontrivial measure μ on the Borel subsets of "G" satisfying the following properties:
Such a measure on "G" is called a "left Haar measure." It can be shown as a consequence of the above properties that μ("U") > 0 for every non-empty open subset "U". In particular, if "G" is compact then μ("G") is finite and positive, so we can uniquely specify a left Haar measure on "G" by adding the normalization condition μ("G") = 1.
Some authors define a Haar measure on Baire sets rather than Borel sets. This makes the regularity conditions unnecessary as Baire measures are automatically regular. Halmos rather confusingly uses the term "Borel set" for elements of the σ-ring generated by compact sets, and defines Haar measure on these sets.
The left Haar measure satisfies the inner regularity condition for all σ-finite Borel sets, but may not be inner regular for "all" Borel sets. For example, the product of the unit circle (with its usual topology) and the real line with the discrete topology is a locally compact group with the product topology and Haar measure on this group is not inner regular for the closed subset {1} x [0,1]. (Compact subsets of this vertical segment are finite sets and points have measure 0, so the measure of any compact subset of this vertical segment is 0. But, using outer regularity, one can show the segment has infinite measure.)
The existence and uniqueness (up to scaling) of a left Haar measure was first proven in full generality by André Weil. Weil's proof used the axiom of choice and Henri Cartan furnished a proof which avoided its use. Cartan's proof also proves the existence and the uniqueness simultaneously. A simplified and complete account of Cartan's argument was given by Alfsen in 1963. The special case of invariant measure for second countable locally compact groups had been shown by Haar in 1933.
Construction of Haar measure.
A construction using compact subsets.
The following method of constructing Haar measure is more or less the method used by Haar and Weil.
For any subsets "T", "U" of "G" with "U" nonempty define ["T":"U"] to be the smallest number of left translates of "U" that cover "T" (so this is a non-negative integer or infinity). This is not additive on compact sets "T", though it does have the property that ["S":"U"]+["T":"U"]=["S"∪"T":"U"] for disjoint compact sets "S" and "T" provided that "U" is a sufficiently small open neighborhood of the identity (depending on "S" and "T"). The idea of Haar measure is to take a sort of limit of ["T":"U"] as "U" becomes smaller to make it additive on all pairs of disjoint compact sets, though it first has to be normalized so that the limit is not just infinity. So fix a compact set "A" with non-empty interior (which exists as the group is locally compact) and for a compact set "T" define
where the limit is taken over a suitable directed set of open neighborhoods of the identity eventually contained in any given neighborhood; the existence of a directed set such that the limit exists follows using Tychonoff's theorem.
The function μ"A" is additive on disjoint compact sets of "G", which implies that it is a regular content. From a regular content one can construct a measure by first extending μ"A" to open sets by inner regularity, then to all sets by outer regularity, and then restricting it to Borel sets. (Even for open sets "T", the corresponding measure μ"A"("T") need not be given by the lim sup formula above. The problem is that the function given by the lim sup formula is not countably subadditive in general and in particular is infinite on any set without compact closure, so is not an outer measure.)
A construction using compactly supported functions.
Cartan introduced another way of constructing Haar measure as a Radon measure (a positive linear functional on compactly supported continuous functions) which is similar to the construction above except that "A", "S", "T", and "U" are positive continuous functions of compact support rather than subsets of "G". In this case we define ["T":"U"] to be the infimum of numbers "c"1+...+"c""n" such that "T"("g") is less than the linear combination "c"1"U"("g"1"g")+...+"c""n""U"("g""n""g") of left translates of "U" for some "g"1...,"g""n".
As before we define
The fact that the limit exists takes some effort to prove, though the advantage of doing this is that the proof avoids the use of the axiom of choice and also gives uniqueness of Haar measure as a by-product. The functional μA extends to a positive linear functional on compactly supported continuous functions and so gives a Haar measure. (Note that even though the limit is linear in "T", the individual terms ["T":"U"] are not usually linear in "T".)
A construction using mean values of functions.
Von Neumann gave a method of constructing Haar measure using mean values of functions, though it only works for compact groups. The idea is that given a function "f" on a compact group, one can find a convex combination Σ"a""i""f"("g""i""g") (where Σ"a""i"=1) of its left translates that differs from a constant function by at most some small number ε. Then one shows that as ε tends to zero the values of these constant functions tend to a limit, which is called the mean value (or integral) of the function "f".
For groups that are locally compact but not compact this construction does not give Haar measure as the mean value of compactly supported functions is zero. However something like it does work for almost periodic functions on the group which do have a mean value, though this is not given by with respect to Haar measure.
A construction on Lie groups.
On an "n"-dimensional Lie group, Haar measure can be constructed easily as the measure induced by a left-invariant "n"-form. This was known before Haar's theorem.
The right Haar measure.
It can also be proved that there exists a unique (up to multiplication by a positive constant) right-translation-invariant Borel measure ν satisfying the above regularity conditions and being finite on compact sets, but it need not coincide with the left-translation-invariant measure μ. The left and right Haar measures are the same only for so-called "unimodular groups" (see below). It is quite simple, though, to find a relationship between μ and ν.
Indeed, for a Borel set "S", let us denote by formula_8 the set of inverses of elements of "S". If we define 
then this is a right Haar measure. To show right invariance, apply the definition:
Because the right measure is unique, it follows that μ-1 is a multiple of ν and so
for all Borel sets "S", where "k" is some positive constant.
The modular function.
The "left" translate of a right Haar measure is a right Haar measure. More precisely, if ν is a right Haar measure, then
is also right invariant. Thus, by uniqueness of the Haar measure, there exists a function Δ from the group to the positive reals, called the Haar modulus, modular function or modular character, such that for every Borel set "S"
Since right Haar measure is well-defined up to a positive scaling factor, this equation shows the modular function is independent of the choice of right Haar measure in the above equation.
The modular function is a continuous group homomorphism into the multiplicative group of positive real numbers. A group is called unimodular if the modular function is identically 1, or, equivalently, if the Haar measure is both left and right invariant. Examples of unimodular groups are abelian groups, compact groups, discrete groups (e.g., finite groups), semisimple Lie groups and connected nilpotent Lie groups. An example of a non-unimodular group is the group of affine transformations
on the real line. This example shows that a solvable Lie group need not be unimodular.
In this group a left Haar measure is given by "dadb"/"a"2, and a right Haar measure by "dadb"/|"a"|.
Measures on homogeneous spaces.
If the locally compact group "G" acts transitively on a space "G"/"H", one can ask if this space has an invariant measure, or more generally a relatively invariant measure with the property that μ("gE") = χ("g")μ("E") for some character χ of "G". A necessary and sufficient condition for the existence of such a measure is that χ=Δ/δ on "H", where Δ and δ are the modular functions of "G" and "H". 
In particular an invariant measure on "Q" exists if and only if the modular function of "G" restricted to "H" is the modular function of "H".
Example. If "G" is the group "SL"2(R) and "H" the subgroup of upper triangular matrices, then the modular function of "H" is nontrivial but the modular function of "G" is trivial. The quotient of these cannot be extended to any character of "G", so the quotient space "G"/"H" (which can be thought of as 1-dimensional real projective space) does not even have a relatively invariant measure.
Haar integral.
Using the general theory of Lebesgue integration, one can then define an integral for all Borel measurable functions "f" on "G". This integral is called the Haar integral. If μ is a left Haar measure, then
for any integrable function "f". This is immediate for indicator functions, being essentially the definition of left invariance.
Uses.
In the same issue of "Annals of Mathematics" and immediately after Haar's paper, the Haar theorem was used to solve Hilbert's fifth problem for compact groups by John von Neumann.
Unless "G" is a discrete group, it is impossible to define a countably additive left-invariant regular measure on "all" subsets of "G", assuming the axiom of choice, according to the theory of non-measurable sets.
Abstract harmonic-analysis.
The Haar measures are used in harmonic analysis on locally compact groups, particularly in the theory of Pontryagin duality. To prove the existence of a Haar measure on a locally compact group "G" it suffices to exhibit a left-invariant Radon measure on "G".
Mathematical statistics.
In mathematical statistics, Haar measures are used for prior measures, which are prior probabilities for compact groups of transformations. These prior measures are used to construct admissible procedures, by appeal to the characterization of admissible procedures as Bayesian procedures (or limits of Bayesian procedures) by Wald. For example, a right Haar measure for a family of distributions with a location parameter results in the Pitman estimator, which is best equivariant. When left and right Haar measures differ, the right measure is usually preferred as a prior distribution. For the group of affine transformations on the parameter space of the normal distribution, the right Haar measure is the Jeffreys prior measure. Unfortunately, even right Haar measures sometimes result in useless priors, which cannot be recommended for practical use, like other methods of constructing prior measures that avoid subjective information.
Another use of Haar measure in statistics is in conditional inference, in which the sampling distribution of a statistic is conditioned on another statistic of the data. In invariant-theoretic conditional inference, the sampling distribution is conditioned on an invariant of the group of transformations (with respect to which the Haar measure is defined). The result of conditioning sometimes depends on the order in which invariants are used and on the choice of a maximal invariant, so that by itself a statistical principle of invariance fails to select any unique best conditional statistic (if any exist); at least another principle is needed. 
For non-compact groups, statisticians have extended Haar-measure results using amenable groups.
Weil's converse theorem.
In 1936 Weil proved a converse (of sorts) to Haar's theorem, by showing that if a group has a left invariant measure for which one can define a convolution product, then one can define a topology on the group, and the completion of the group is locally compact and the given measure is essentially the same as Haar measure on this completion.

</doc>
<doc id="40647" url="http://en.wikipedia.org/wiki?curid=40647" title="Viggo Brun">
Viggo Brun

Viggo Brun (13 October 1885, Lier – 15 August 1978, Drøbak) was a Norwegian mathematician.
He studied at the University of Oslo and began research at the University of Göttingen in 1910. In 1923, Brun became a professor at the Technical University in Trondheim and in 1946 a professor at the University of Oslo. He retired in 1955 at the age of 70.
In 1915, he introduced a new method, based on Legendre's version of the sieve of Eratosthenes, now known as the "Brun sieve", which addresses additive problems such as Goldbach's conjecture and the twin prime conjecture. He used it to prove that there exist infinitely many integers n such that n and n+2 have at most nine prime factors (9-almost primes); and that all large even integers are the sum of two 9 (or smaller)-almost primes.
He also showed that the sum of the reciprocals of twin primes converges to a finite value, now called Brun's constant: by contrast, the sum of the reciprocals of all primes is divergent. 
He developed a multi-dimensional continued fraction algorithm in 1919/20 and applied this to problems in musical theory.
He also served as praeses of the Royal Norwegian Society of Sciences and Letters in 1946.

</doc>
<doc id="40648" url="http://en.wikipedia.org/wiki?curid=40648" title="Buffy Summers">
Buffy Summers

Buffy Anne Summers is a fictional character from Joss Whedon's "Buffy the Vampire Slayer" franchise. She first appeared in the 1992 film "Buffy the Vampire Slayer" before going on to appear in the television series and subsequent comic book of the same name. The character has also appeared in the spin-off series "Angel", as well as numerous non-canon expanded universe material, such as novels, comics, and video games. Buffy was portrayed by Kristy Swanson in the film, and later by Sarah Michelle Gellar in the television series. Giselle Loren has lent her voice to the character in both the "Buffy" video games and an unproduced animated series, while Kelly Albanese lent her voice to the character in the "Buffy the Vampire Slayer Season Eight" motion comics.
Buffy is the protagonist of the story, and the series depicts her life and adventures as she grows up. In the film, she is a high school cheerleader who learns that she is the Slayer (a Chosen One gifted with the strength and skills to fight vampires, demons, and the forces of darkness). The television series shows Buffy carrying out her destiny in a small town built atop a portal to hell (Hellmouth), surrounded by a group of friends and family who support her in her mission. In the comic book continuation, she is a young woman who has accepted her duties and is now responsible for training others like her.
Buffy was created to subvert the stereotypical female horror film victim; Whedon wanted to create a strong female cultural icon. In 2004, Buffy was ranked at number 13 in Bravo's list of The 100 Greatest TV Characters. In June 2010, "Entertainment Weekly" ranked her third in its list of the 100 Greatest Characters of the Last 20 Years. AOL named her the sixth Most Memorable Female TV Character. She was ranked at No. 5 in AfterEllen.com's Top 50 Favorite Female TV Characters.
Appearances.
Film.
The character of Buffy first appears in the 1992 film "Buffy the Vampire Slayer", played by Kristy Swanson. The film, written by Joss Whedon, depicts Buffy as a shallow high school cheerleader who is informed by a man named Merrick (Donald Sutherland) that she has been chosen by fate to battle the undead. Buffy reluctantly undergoes training in her abilities by Merrick, and as her responsibility as the Slayer causes her to become alienated from her valley girl peers, she finds friendship and romance with fellow outcast Pike (Luke Perry). Merrick eventually comes to respect Buffy's rebellious nature, and she defeats vampire king Lothos (Rutger Hauer) by relying on her own contemporary style as opposed to traditional Slayer conventions. Although this film is not in continuity with the later television series, in 1999, author Christopher Golden adapted Joss Whedon's original script into a comic book entitled "The Origin", which Whedon later confirmed to be "pretty much" canonical.
On May 25, 2009, "The Hollywood Reporter" revealed Roy Lee and Doug Davison of Vertigo Entertainment would be working with Fran Rubel Kuzui and Kazi Kuzui on a relaunch of the "Buffy" series for the big screen. The series would not be a sequel or prequel to the existing movie or television franchise and Joss Whedon will have no involvement in the project. None of the cast or original characters from the television series will be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Joss into taking reins of the project. Studio interest in the project has continued, however. A script was rejected in 2011.
Television.
Buffy returned in Joss Whedon's television series "Buffy the Vampire Slayer", this time played by Sarah Michelle Gellar for all of the show's 144 episodes. In season one (1997), Buffy begins to accept the responsibilities and dangers of her calling as the Slayer after moving to the small California town of Sunnydale. She becomes best friends with Xander Harris (Nicholas Brendon) and Willow Rosenberg (Alyson Hannigan), and meets her new Watcher, Rupert Giles (Anthony Stewart Head). Together, they form the Scooby Gang, and work together to battle various supernatural occurrences which plague Sunnydale High. In the season finale, Buffy battles the villain known as the Master (Mark Metcalf), and is drowned in the process. She is resuscitated by Xander and rises to defeat the vampire. In the show's second season (1997–1998), Buffy continues to come to terms with her destiny, finds forbidden love with benevolent vampire Angel (David Boreanaz), and clashes with new villains Spike (James Marsters) and Drusilla (Juliet Landau). In the episode "Surprise", Buffy loses her virginity to Angel, an event which triggers the loss of his soul and unleashes his sadistic alter-ego, Angelus. Angelus proceeds to subject the characters to mental and physical torture for the remainder of the season. In the final episode of season two, Buffy is forced to reveal her identity as the Slayer to her mother (Kristine Sutherland), and send the newly good Angel to hell in order to save the world. She then leaves Sunnydale for Los Angeles in the hopes of escaping her life as the Slayer. Season three (1998–1999) sees Buffy reconnect to her calling, her friends, and her family after her departure, as well as make difficult life decisions regarding her relationship with the resurrected Angel. She must also deal with the introduction of rebellious new Slayer Faith (Eliza Dushku), who becomes increasingly destructive and disloyal over the course of the season. In the season finale, Buffy stabs Faith in an attempt to save Angel's life, and leads her classmates into a climactic battle against the demonic Mayor of Sunnydale (Harry Groener). Angel then leaves Sunnydale in hopes that Buffy can have a more normal life without him.
In the fourth season (1999–2000), Buffy balances her Slayer duties with her new life as a college student at UC Sunnydale. She experiences some difficulty adjusting to college life, and becomes increasingly disconnected from her friends, who all seem to be moving in different directions. Buffy eventually finds a new love interest in the form of Riley Finn (Marc Blucas), a soldier in the demon-hunting government task force known as the Initiative. She briefly joins forces with Riley's team, until they discover one of the Initiative's experiments, Adam (George Hertzberg), is creating an army of demon-human hybrids. Buffy literally unites with her friends to defeat Adam in a spell which invokes the power of the First Slayer. During "Buffy" season four, Buffy also appears in the first season of spin-off series "Angel" (1999–2000), guest starring in the episodes "I Will Remember You" and "Sanctuary". In season five (2000–2001), Buffy battles the hell-goddess Glory (Clare Kramer), and fully embraces her destiny for the first time. A younger sister named Dawn (Michelle Trachtenberg) mysteriously appears in Buffy's household, her existence having been seamlessly integrated with memories of the other characters. Buffy suffers emotional turmoil throughout this season, including the realization Dawn is not actually her sister, the deterioration of her relationship with Riley, the discovery that Spike has become obsessed with her, and her mother's death from a brain aneurysm. While on a quest to learn more about her nature as the Slayer, Buffy is told "death is her gift, a message she has difficulty understanding until the episode "The Gift", in which she sacrifices herself to save Dawn and the world by diving into Glory's interdimensional portal and closing it.
Season six (2001–2002) depicts Buffy's struggle with depression after her friends, believing she was trapped in a Hell dimension, performed a spell to bring her back from the dead; however, she was actually in Heaven, and feels great loss after being ripped out. Forced to take a mundane and degrading job slinging burgers at the Doublemeat Palace after realizing her family were in financial ruin, she sinks into a deep depression amid feelings of self-loathing, embarking on a violent sexual relationship with the vampire Spike which leaves neither satisfied and spawns dire consequences for the both of them. As the season draws to a close, Buffy is forced to battle her best friend when Willow becomes psychotic with dark magics after the human, Warren (Adam Busch) shoots and kills Willow's girlfriend Tara (Amber Benson) and wounds Buffy in the process. Willow then tries to destroy the world to end all suffering, although Xander gets through to her in the end. Buffy then promises to change her self-destructive behavior in order to be there for her sister. In the final season of the show (2002–2003), things start to come around for Buffy when Principal Robin Wood (D. B. Woodside) hires her as a school counselor for the newly rebuilt Sunnydale High School and she has repaired her relationships with Dawn and her friends. However, she is also confronted with the threat of the First Evil and becomes a reluctant leader to the Potential Slayers, who are initially respectful of her, but become increasingly more alienated by her tactics and decisions throughout the season. She unexpectedly becomes emotionally close with Spike, who has sought out his soul in an effort to prove himself to her. In the show's final episode "Chosen", Buffy shares her power with her fellow Slayers before leading them into an epic battle against an army of Turok-Han vampires. She also confesses her love to a disbelieving Spike before he sacrifices himself to save the world; as he dies, Buffy escapes Sunnydale's destruction with the surviving characters. Following the end of "Buffy the Vampire Slayer", the character maintains a presence in the fifth season of "Angel" (2003–2004), but does not appear onscreen. In the episode "The Girl in Question", Angel and a resurrected Spike travel to Rome to find her, where they learn she is apparently now dating the Immortal. Sarah Michelle Gellar was approached to appear as Buffy in "Angel"'s one hundredth episode, but declined, so the character of Cordelia Chase (Charisma Carpenter) was used instead. She was asked to appear in the second to last episode of the series, "Power Play", but had to decline due to outside conflicts.
Between 2001 and 2004, Joss Whedon and Jeph Loeb developed a 4-minute pilot episode for "Buffy the Animated Series", which was set during the show's first season. Had the series been picked up by a network, the series would have focused upon Buffy (voiced by Giselle Loren) in more high-school adventures. Following a 2008 leak of the pilot to YouTube, Loeb expressed some hope the series may be resurrected in some form.
Literature.
As the main character of the franchise, Buffy appears in almost all "Buffy the Vampire Slayer" literature. This includes a Dark Horse ongoing comic book and a series of novels. Buffy's debut into literature came in the comic "Dark Horse Presents 1998 Annual" on August 26, 1998, while her first prose appearance was in "Halloween Rain" by Christopher Golden and Nancy Holder on October 5, 1998. Most of these stories occur between episodes and seasons of the television series, however, some are set outside the timeline of the show to explore in depth other areas of Buffy's history. Christopher Golden adapted the film into a comic entitled "The Origin" (1999) which more closely resembles Joss Whedon's original script. In 2003, Scott Lobdell and Fabian Nicieza wrote a Year One-style run on the "Buffy" comic book series which filled the gap between the film and the first season of the show. These stories explain how Buffy's relationship with Pike ended, as well as fleshing out events alluded to in the television series, such as the time she spent in a mental institution and her parents' divorce. The novel "Queen of the Slayers" (2005) by Nancy Holder offers a potential follow-up to the television series; set after season seven, it depicts Buffy living in Italy with the morally ambiguous Immortal.
Buffy also makes appearances in literature outside of her own titular series. In the "Tales of the Slayers" comic one-shot "Broken Bottle of Djinn" (2002) by Doug Petrie and Jane Espenson, Buffy battles a spirit in Sunnydale High, while the "Tales of the Vampires" comic book story "Antique" (2004) by Drew Goddard sees her breaking into Dracula's castle to rescue Xander from the infamous vampire. Volume II of the similar series of novels "Tales of the Slayer" (2003) features two stories about Buffy; the character battles a mummified spirit in Todd A. McIntosh's "All That You Do Comes Back Unto Thee," while Jane Espenson's "Again Sunnydale" sees a season six-era Buffy sent back in time to high school, when her mother is still alive but Dawn does not exist.
In 2007, Buffy's story was continued when Joss Whedon resurrected "Buffy the Vampire Slayer" as a comic book. These comics differ from previous "Buffy" literature in that they are the official continuation of the television series and are considered canon. In "Season Eight" (2007–11), it establishes Buffy is not living with the Immortal in Rome which is simply a cover story to ensure her safety as she is now the leader of a global organization which recruits and trains Slayers to deal with demonic threats worldwide. However, a mysterious group led by the masked villain 'Twilight' believe the Slayers themselves are the danger, should they begin to consider themselves superior to mankind. In the story "Wolves at the Gate", Buffy shares a sexual encounter with fellow Slayer Satsu; however Satsu leaves soon after because she realizes Buffy cannot return her feelings. Earlier in the series, the audience glimpsed Buffy's most personal sexual fantasy in her dreamspace: a threesome with Angel and Spike. The series' final two arcs are in some sense are about Buffy's relationships with Angel and Spike. In "Twilight", Buffy mysteriously obtains super powers and discovers that Twilight is in fact Angel. Despite his actions, Buffy succumbs to her passions and engages in an act of passionate airborne sex with Angel, only having them return on waking up in a paradise dimension which will replace the existing universe. In spite of the lure of eternal happiness together in this new dimension, the two return to Earth to assist their friends against extra-dimensional demons, when Spike arrives. In "Last Gleaming", Spike's information leads them to source both of magic and of Twilight's power, a mystical "seed" buried beneath Sunnydale. Giles plans to destroy it, but Twilight possesses Angel and snaps his neck. Distraught, Buffy smashes the seed herself. Twilight is stopped but magic is also removed from the universe. Though Slayers and vampires retain their powers, witches for example are left entirely powerless. A pariah in the community of Slayers and former witches, Buffy moves to San Francisco where she lives with her sister and Xander, and resumes her former duties as Slayer: patrolling at night for vampires.
In "Buffy the Vampire Slayer Season Nine" (2011–13), Buffy adjusts to the new status quo of a world without magic. As a consequence of demons no longer being able to enter their dimension and fully possess human bodies, newly sired vampires are feral and mindless creatures, which Xander dubs "zompires". Buffy faces a new antagonist in Severin, who works with the rogue slayer Simone Doffler and has the power to drain others of their mystical energy. In the story arc "Apart (of Me)", Buffy mistakenly believes that she is pregnant and makes the difficult decision to have an abortion. Spike also leaves San Francisco after it becomes difficult for him to be around Buffy anymore. Willow also leaves town on a mission to restore her abilities, leaving Buffy to patrol for vampires with new friends, including SFPD officer Robert Dowling and teenager Billy Lane. As the season goes on, Buffy teams with a council of powerful supernatural beings to take on Severin. However, Severin is able to steal the powers of Illyria, a member of that group, including her time travel abilities. When Buffy learns Dawn is dying as a result of the end of magic and Willow returns with restored powers, they venture to the Deeper Well, a tomb for ancient demons in England, in the hopes of finding magic to save Dawn, who begins fading out of existence. Desperate to save Dawn, Xander informs Simone and Severin of Buffy's plan because Severin wants the same magic to avert the events of "Season Eight" using time travel. Within the Well, Buffy fights and kills Simone who had intentionally became a vampire; Severin and Illyria, in an act of redemption, sacrifice their lives to restore magic to the universe. After this, Willow is able to bring Dawn back from nonexistence. Buffy and Willow begin to realize however that the rules of magic and vampire abilities have been entirely rewritten by their actions. In "Season Ten" (2014–present), Buffy and her friends grapple with the new rules of magic, the resulting new threats and the obligation to make new alliances.
Concept and creation.
The character of Buffy was conceived by Joss Whedon as a way of subverting the cliché of "the little blonde girl who goes into a dark alley and gets killed in every horror film". Whedon stated "Rhonda the Immortal Waitress" was the first incarnation of Buffy in his head, "the idea of a seemingly insignificant female who in fact turns out to be extraordinary." When asked how he came up with the name of "Buffy," Whedon states "It was the name that I could think of that I could take the least seriously. There is no way you could hear the name Buffy and think, "This is an important person. To juxtapose that with Vampire Slayer, just felt like that kind of thing—a B movie. But a B movie that had something more going on. That was my dream." Whedon claims the title was criticized for being too silly, and the television network begged him to change it. He refused, insisting "You don't understand. It has to be this. This is what it is." Jason Middleton feels that Buffy avoids the "final girl" character trope seen in horror films, where the androgynous and celibate heroine gets to outlive her friends and exact revenge on their killer; in Middleton's words, "she... gets to have sex with boys and "still" kill the monster".
Whedon always intended for the character to become an icon, claiming "I wanted her to be a hero that existed in people's minds the way Wonder Woman or Spider-Man does, you know? I wanted her to be a doll or an action figure. I wanted Barbie with Kung Fu grip! I wanted her to enter the mass consciousness and the imaginations of growing kids because I think she's a cool character, and that was always the plan. I wanted Buffy to be a cultural phenomenon, period." In developing Buffy, Whedon was greatly inspired by Kitty Pryde, a character from the pages of the superhero comic "X-Men". He admits, "If there's a bigger influence on Buffy than Kitty, I don't know what it was... She was an adolescent girl finding out she has great power and dealing with it." In a 2009 interview, Whedon revealed he only recently realised how much he saw of himself in Buffy. After years of relating more to Xander, he says, "Buffy was always the person that I was in that story because I'm not in every way." Whedon openly wonders why his identification figure is a woman, but describes it as "a real autobiographical kind of therapy for me" to be writing a strong female character like Buffy.
According to Whedon, Buffy "had been brewing in [him] for many years" before finally appearing in the "Buffy the Vampire Slayer" film played by Kristy Swanson. However, he was not satisfied with the character's treatment in the film, feeling "that's not quite her. It's a start, but it's not quite the girl." Although Whedon's vision of female empowerment was not as apparent as he would have liked in the 1992 film, he was given a second chance when Gail Berman approached him with the idea of re-creating it as a television series. Adapting the concept of the movie into a television series, Whedon decided to reinvent the character of Buffy slightly. The shallow cheerleader of the original film had grown more mature and open-minded, identifying with social outcasts such as Willow and Xander, and instead, the character of Cordelia was created to embody what Buffy once was. Early in the television series, make-up supervisor Todd McIntosh was instructed to make Buffy "a soft and sort of earthy character." He gave Gellar a soft, muted green make-up and kept her look very natural. However, it was later decided this was inappropriate for the character, and Buffy needed to look more like a valley girl. McIntosh switched her make-up around, giving her frosted eyeshadow and lip colors, bright turquoise and aqua marines, bubblegum colored nails, and bleach-blonde hair, causing the character to "blossom."

</doc>
<doc id="40650" url="http://en.wikipedia.org/wiki?curid=40650" title="Scale (anatomy)">
Scale (anatomy)

In most biological nomenclature, a scale (Greek λεπίς "lepis", Latin "squama") is a small rigid plate that grows out of an animal's skin to provide protection. In lepidopteran (butterfly and moth) species, scales are plates on the surface of the insect wing, and provide coloration. Scales are quite common and have evolved multiple times through convergent evolution, with varying structure and function.
Scales are generally classified as part of an organism's integumentary system. There are various types of scales according to shape and to class of animal.
Fish scales.
Fish scales are dermally derived, specifically in the mesoderm. This fact distinguishes them from reptile scales paleontologically.
Genetically, the same genes involved in tooth and hair development in mammals are also involved in scale development.
Cosmoid scales.
True cosmoid scales can only be found on the extinct Crossopterygians. The inner layer of the scale is made of lamellar bone. On top of this lies a layer of spongy or vascular bone and then a layer of dentine-like material called cosmine. The upper surface is keratin. The coelacanth has modified cosmoid scales that lack cosmine and are thinner than true cosmoid scales.
Ganoid scales.
Ganoid scales can be found on gars (family Lepisosteidae) and bichirs and reedfishes (family Polypteridae). Ganoid scales are similar to cosmoid scales, but a layer of ganoin lies over the cosmine layer and under the enamel. Ganoin scales are diamond-shaped, shiny, and hard. 
Within the ganoin are guanine compounds, iridescent derivatives of guanine found in a DNA molecule. The iridescent property of these chemicals provide the ganoin its shine.
Placoid scales.
Placoid scales are found on cartilaginous fish including sharks. These scales, also called denticles, are similar in structure to teeth, and have one median spine and two lateral spines.
Leptoid scales.
Leptoid scales are found on higher-order bony fish. As they grow they add concentric layers. They are arranged so as to overlap in a head-to-tail direction, like roof tiles, allowing a smoother flow of water over the body and therefore reducing drag. They come in two forms:
Reptilian scales.
Reptile scale types include: cycloid, granular (which appear bumpy), and keeled (which have a center ridge). Scales usually vary in size, the stouter, larger scales cover parts that are often exposed to physical stress (usually the feet, tail and head), while scales are small around the joints for flexibility. Most snakes have extra broad scales on the belly, each scale covering the belly from side to side.
The scales of all reptiles have an epidermal component (what one sees on the surface), but many lizards have osteoderms underlying the epidermal scale, as do crocodilians and turtles. Such scales are more properly termed scutes. Snakes, tuataras and many lizards lack osteoderms. All reptilian scales have a dermal papilla underlying the epidermal part, and it is there that the osteoderms, if present, would be formed.
Arthropod scales.
Butterflies and moths - the order Lepidoptera (Greek "scale-winged") - have membranous wings covered in delicate, powdery scales, which are modified setae. Each scale consists of a series of tiny stacked platelets of organic material, and butterflies tend to have the scales broad and flattened, while moths tend to have the scales narrower and more hair-like. Scales are usually pigmented, but some types of scales are metallic, or iridescent, without pigments; because the thickness of the platelets is on the same order as the wavelength of visible light the plates lead to structural coloration and iridescence through the physical phenomenon described as thin-film optics. The most common color produced in this fashion is blue, such as in the "Morpho" butterflies. Other colors can be seen on the Sunset moth.
Avian scales.
Birds' scales are found mainly on the toes and metatarsus, but may be found further up on the ankle in some birds. The scales and scutes of birds were thought to be homologous to those of reptiles, but are now agreed to have evolved independently, being degenerate feathers.
Mammalian scales.
An example of a scaled mammal is the pangolin. Its scales are made of keratin and are used for protection, similar to an armadillo's armor. They have been convergently evolved, being unrelated to the animal's distant reptile ancestors, except that they use a similar gene.
On the other hand, the musky rat-kangaroo, a marsupial with a number of especially primitive traits, has scales on its feet and tail that appear to be linked to ancestral reptile scales. It also has five toes (unlike any other kangaroo or rat-kangaroo), and engages in a more primitive-seeming hopping behavior than its kangaroo cousins.
Bibliography.
</dl>

</doc>
<doc id="40651" url="http://en.wikipedia.org/wiki?curid=40651" title="Scale (music)">
Scale (music)

In music, a scale is any set of musical notes ordered by fundamental frequency or pitch. A scale ordered by increasing pitch is an ascending scale, while descending scales are ordered by decreasing pitch. Some scales contain different pitches when ascending than when descending (for instance, see Melodic minor scale).
Often, especially in the context of the common practice period, part or all of a musical work including melody and/or harmony, is built using the notes of a single scale, which can be conveniently represented on a staff with a standard key signature.
Due to the principle of octave equivalence, scales are generally considered to span a single octave, with higher or lower octaves simply repeating the pattern. A musical scale represents a division of the octave space into a certain number of scale steps, a scale step being the recognizable distance (or interval) between two successive notes of the scale. However, there is no need for scale steps to be equal within any scale and, particularly as demonstrated by microtonal music, there is no limit to how many notes can be injected within any given musical interval. Ideas that, for example. "(all) scales only have eight notes" or that "an octave is made from twelve semitones" are misconceptions based on cultural conventions, and have no scientific grounding.
A measure of the width of each scale step provides a method to classify scales. For instance, in a chromatic scale each scale step represents a semitone interval, while a major scale is defined by the interval pattern T–T–S–T–T–T–S, where T stands for whole tone (an interval spanning two semitones), and S stands for semitone. Based on their interval patterns, scales are put into categories including diatonic, chromatic, major, minor, and others.
A specific scale is defined by its characteristic interval pattern and by a special note, known as its first degree (or tonic). The tonic of a scale is the note selected as the beginning of the octave, and therefore as the beginning of the adopted interval pattern. Typically, the name of the scale specifies both its tonic and its interval pattern. For example, C-major indicates a major scale in which C is the tonic.
Background.
Scales, steps, and intervals.
Scales are typically listed from low to high. Most scales are "octave-repeating", meaning their pattern of notes is the same in every octave (the Bohlen–Pierce scale is one exception). An octave-repeating scale can be represented as a circular arrangement of pitch classes, ordered by increasing (or decreasing) pitch class. For instance, the increasing C major scale is C–D–E–F–G–A–B–[C], with the bracket indicating that the last note is an octave higher than the first note, and the decreasing C major scale is C–B–A–G–F–E–D–[C], with the bracket indicating an octave lower than the first note in the scale.
The distance between two successive notes in a scale is called a scale step.
The notes of a scale are numbered by their steps from the root of the scale. For example, in a C major scale the first note is C, the second D, the third E and so on. Two notes can also be numbered in relation to each other: C and E create an interval of a third (in this case a major third); D and F also create a third (in this case a minor third).
Scales and pitch.
A single scale can be manifested at many different pitch levels. For example, a C major scale can be started at C4 (middle C; see scientific pitch notation) and ascending an octave to C5; or it could be started at C6, ascending an octave to C7. As long as all the notes can be played, the octave they take on can be altered.
Types of scale.
Scales may be described according to the intervals they contain:
or by the number of different pitch classes they contain:
"The number of the notes that make up a scale as well as the quality of the intervals between successive notes of the scale help to give the music of a culture area its peculiar sound quality." "The pitch distances or intervals among the notes of a scale tell us more about the sound of the music than does the mere number of tones."
Harmonic content.
The notes of a scale form intervals with each of the other notes of the chord in combination. A 5-note scale has 10 of these harmonic intervals, a 6-note scale has 15, a 7-note scale has 21, an 8-note scale has 28. Though the scale is not a chord, and might never be heard more than one note at a time, still the absence, presence, and placement of certain key intervals plays a large part in the sound of the scale, the natural movement of melody within the scale, and the selection of chords taken naturally from the scale.
A musical scale containing tritones is called tritonic; one without tritones is atritonic. A scale or chord containing semitones is called hemitonic; one without semitones is anhemitonic. The significance of these categories lies in their bases of semitones and tritones being the severest of dissonances, avoidance of which is often desirable. The most used scales across the planet are anhemitonic.
Scales in composition.
Scales can be abstracted from performance or composition. They are also often used precompositionally to guide or limit a composition. Explicit instruction in scales has been part of compositional training for many centuries. One or more scales may be used in a composition, such as in Claude Debussy's "L'Isle Joyeuse". To the right, the first scale is a whole tone scale, while the second and third scales are diatonic scales. All three are used in the opening pages of Debussy's piece.
Western music.
Scales in traditional Western music generally consist of seven notes and repeat at the octave. Notes in the commonly used scales (see just below) are separated by whole and half step intervals of "tones" and "semitones." The harmonic minor scale includes a three-semitone step; the anhemitonic pentatonic includes two of those and no semitones.
Western music in the Medieval and Renaissance periods (1100–1600) tends to use the white-note diatonic scale C–D–E–F–G–A–B. Accidentals are rare, and somewhat unsystematically used, often to avoid the tritone.
Music of the common practice periods (1600–1900) uses three types of scale:
These scales are used in all of their transpositions. The music of this period introduces "modulation," which involves systematic changes from one scale to another. Modulation occurs in relatively conventionalized ways. For example, major-mode pieces typically begin in a "tonic" diatonic scale and modulate to the "dominant" scale a fifth above.
In the 19th century (to a certain extent), but more in the 20th century, additional types of scales were explored:
A large variety of other scales exists, some of the more common being:
Scales such as the pentatonic scale may be considered "gapped" relative to the diatonic scale. An "auxiliary scale" is a scale other than the primary or original scale. See: modulation (music) and Auxiliary diminished scale.
Naming the notes of a scale.
In many musical circumstances, a specific note of the scale will be chosen as the tonic—the central and most stable note of the scale, also known as the root note. Relative to a choice of tonic, the notes of a scale are often labeled with numbers recording how many scale steps above the tonic they are. For example, the notes of the C major scale (C, D, E, F, G, A, B) can be labeled {1, 2, 3, 4, 5, 6, 7}, reflecting the choice of C as tonic. The expression scale degree refers to these numerical labels. Such labeling requires the choice of a "first" note; hence scale-degree labels are not intrinsic to the scale itself, but rather to its modes. For example, if we choose A as tonic, then we can label the notes of the C major scale using A = 1, B = 2, C = 3, and so on. When we do so, we create a new scale called the A minor scale. See the musical note article for how the notes are customarily named in different countries.
The scale degrees of a heptatonic (7-note) scale can also be named using the terms tonic, supertonic, mediant, subdominant, dominant, submediant, subtonic. If the subtonic is a semitone away from the tonic, then it is usually called the leading-tone (or leading-note); otherwise the leading-tone refers to the raised subtonic. Also commonly used is the (movable do) solfège naming convention in which each scale degree is denoted by a syllable. In the major scale, the solfege syllables are: Do, Re, Mi, Fa, So (or Sol), La, Ti (or Si), Do (or Ut).
In naming the notes of a scale, it is customary that each scale degree be assigned its own letter name: for example, the A major scale is written A–B–C♯–D–E–F♯–G♯ rather than A–B–D♭–D–E–E–G♯. However, it is impossible to do this with scales containing more than seven notes.
Scales may also be identified by using a binary system of twelve zeros or ones to represent each of the twelve notes of a chromatic scale. It is assumed that the scale is tuned using 12-tone equal temperament (so that, for instance, C♯ is the same as D♭), and that the tonic is in the leftmost position. For example the binary number 101011010101, equivalent to the decimal number 2773, would represent any major scale (such as C–D–E–F–G–A–B). This system includes scales from 100000000000 (2048) to 111111111111 (4095), providing a total of 2048 possible species, but only 352 unique scales containing from 1 to 12 notes.
Scales may also be shown as semitones (or fret positions) from the tonic. For instance, 0 2 4 5 7 9 11 denotes any major scale such as C–D–E–F–G–A–B, in which the first degree is, obviously, 0 semitones from the tonic (and therefore coincides with it), the second is 2 semitones from the tonic, the third is 4 semitones from the tonic, and so on. Again, this implies that the notes are drawn from a chromatic scale tuned with 12-tone equal temperament.
Scalar transposition.
Composers often transform musical patterns by moving every note in the pattern by a constant number of scale steps: thus, in the C major scale, the pattern C–D–E might be shifted up, or transposed, a single scale step to become D–E–F. This process is called "scalar transposition" and can often be found in musical sequences. Since the steps of a scale can have various sizes, this process introduces subtle melodic and harmonic variation into the music. This variation is what gives scalar music much of its complexity.
Jazz and blues.
Through the introduction of blue notes, jazz and blues employ scale intervals smaller than a semitone. The blue note is an interval that is technically neither major nor minor but "in the middle", giving it a characteristic flavour. For instance, in the key of E, the blue note would be either a note between G and G♯ or a note moving between both. In blues a pentatonic scale is often used. In jazz many different modes and scales are used, often within the same piece of music. Chromatic scales are common, especially in modern jazz.
Non-Western scales.
In Western music, scale notes are often separated by equally tempered tones or semitones, creating 12 notes per octave. Many other musical traditions use scales that include other intervals or a different number of pitches. These scales originate within the derivation of the harmonic series. Musical intervals are complementary values of the harmonic overtones series. Many musical scales in the world are based on this system, except most of the musical scales from Indonesia and the Indochina Peninsulae, which are based on inharmonic resonance of the dominant metalophone and xylophone instruments. A common scale in Eastern music is the pentatonic scale, consisting of five notes. In the Middle Eastern Hejaz scale, there are some intervals of three semitones. Gamelan music uses a small variety of scales including Pélog and Sléndro, none including equally tempered nor harmonic intervals. Indian classical music uses a moveable seven-note scale. Indian Rāgas often use intervals smaller than a semitone. Arabic music maqamat may use quarter tone intervals. In both rāgas and maqamat, the distance between a note and an inflection (e.g., śruti) of that same note may be less than a semitone.
Microtonal scales.
The term "microtonal music" usually refers to music with roots in traditional Western music that uses non-standard scales or scale intervals. Mexican composer Julián Carrillo created in the late 19th century microtonal scales which he called "Sonido 13", The composer Harry Partch made custom musical instruments to play compositions that employed a 43-note scale system, and the American jazz vibraphonist Emil Richards experimented with such scales in his Microtonal Blues Band in the 1970s. Easley Blackwood has written compositions in all equal-tempered scales from 13 to 24 notes. Erv Wilson introduced concepts such as Combination Product Sets (Hexany), Moments of Symmetry and golden horagrams, used by many modern composers. Microtonal scales are also used in traditional Indian Raga music, which has a variety of modes which are used not only as modes or scales but also as defining elements of the song, or raga.

</doc>
<doc id="40655" url="http://en.wikipedia.org/wiki?curid=40655" title="Leigh Brackett">
Leigh Brackett

Leigh Douglass Brackett (December 7, 1915 – March 18, 1978) was an American writer, particularly of science fiction. She was also a screenwriter, known for her work on such films as "The Big Sleep" (1945), "Rio Bravo" (1959), "The Long Goodbye" (1973) and "The Empire Strikes Back" (1980).
Life.
Leigh Brackett was born December 7, 1915 in Los Angeles, California and grew up there. On December 31, 1946, at age 31, she married Edmond Hamilton in San Gabriel, California, and moved with him to Kinsman, Ohio. She died of cancer in 1978 in Lancaster, California.
Career.
Fiction writer.
Brackett was first published in her mid-twenties. Her first published science fiction story was "Martian Quest", which appeared in the February 1940 issue of "Astounding Science Fiction". Her earliest years as a writer (1940–42) were her most productive in numbers of stories written. Occasional stories have social themes, such as "The Citadel of Lost Ships" (1943), which considers the effects on the native cultures of alien worlds of Earth's expanding trade empire.
Brackett's first novel, "No Good from a Corpse", published in 1944, was a hard-boiled mystery novel in the tradition of Raymond Chandler. This led to her first major screenwriting assignment. At the same time, though, Brackett's science fiction stories were becoming more ambitious. "Shadow Over Mars" (1944) was her first novel-length science fiction story, and though still somewhat rough-edged, marked the beginning of a new style, strongly influenced by the characterization of the 1940s detective story and film noir.
In 1946, the same year that Brackett married science fiction author Edmond Hamilton, "Planet Stories" published the novella "Lorelei of the Red Mist". Brackett only finished the first half before turning it over to Ray Bradbury, so that she could leave to work on "The Big Sleep". The story's main character is a thief called Hugh Starke.
Brackett returned from her break from science-fiction writing, caused by her cinematic endeavors, in 1948. From then on to 1951, she produced a series of science fiction adventure stories that were longer than her previous work. To this period belong such classic representations of her planetary settings as "The Moon that Vanished" and the novel-length "Sea-Kings of Mars" (1949), later published as "The Sword of Rhiannon", a vivid description of Mars before its oceans evaporated.
With "Queen of the Martian Catacombs" (1949), Brackett created a character that she later returned to, Eric John Stark. Stark, an orphan from Earth, is raised by the semi-sentient aboriginals of Mercury, who are later killed by Earthmen. He is saved from the same fate by a Terran official, who adopts Stark and becomes his mentor. When threatened, however, Stark frequently reverts to the primitive N'Chaka, the "man without a tribe" that he was on Mercury. From 1949 to 1951, Stark (whose name echoes that of the hero in "Lorelei") appeared in three tales, all published in "Planet Stories"; the aforementioned "Queen", "Enchantress of Venus", and finally "Black Amazon of Mars". With this last story, Brackett's period of writing high adventure ended.
Brackett's stories thereafter adopted a more elegiac tone. They no longer celebrated the conflicts of frontier worlds, but lamented the passing away of civilizations. The stories now concentrated more upon mood than on plot. The reflective, retrospective nature of these stories is indicated in the titles: "The Last Days of Shandakor"; "Shannach — the Last"; "Last Call from Sector 9G".
This last story was published in the very last issue (Summer 1955) of "Planet Stories", always Brackett's most reliable market for science fiction. With the disappearance of "Planet Stories" and, later in 1955, of "Startling Stories" and "Thrilling Wonder Stories", the market for Brackett's brand of story dried up, and the first phase of her career as a science fiction author ended. A few other stories trickled out over the next decade, and old stories were revised and published as novels. A new production of this period was one of Brackett's most critically acclaimed science fiction novels, "The Long Tomorrow" (1955). This novel describes an agrarian, deeply technophobic society that develops after a nuclear war.
But most of Brackett's writing after 1955 was for the more lucrative film and television markets. In 1963 and 1964, she briefly returned to her old Martian milieu with a pair of stories; "The Road to Sinharat" can be regarded as an affectionate farewell to the world of "Queen of the Martian Catacombs", while the other – with the intentionally ridiculous title of "Purple Priestess of the Mad Moon" – borders on parody. She and her husband shared Guest of Honor duties at the 22nd World Science Fiction Convention in Oakland, California.
After another hiatus of nearly a decade, Brackett returned to science fiction in the seventies with the publication of "The Ginger Star" (1974), "The Hounds of Skaith" (1974), and "The Reavers of Skaith" (1976), collected as "The Book of Skaith" in 1976. This trilogy brought Eric John Stark back for adventures upon the extrasolar planet of Skaith (rather than his old haunts of Mars and Venus).
"Brackett's Solar System".
Often referred to as the Queen of Space Opera, Brackett also wrote planetary romance. Almost all of her planetary romances take place within a common invented universe, the Leigh Brackett Solar System, which contains richly detailed fictional versions of the consensus Mars and Venus of science fiction in the 1930s–1950s. Mars thus appears as a marginally habitable desert world, populated by ancient, decadent, and mostly humanoid races; Venus as a primitive, wet jungle planet, occupied by vigorous, primitive tribes and reptilian monsters. Brackett's Skaith combines elements of Brackett's other worlds with fantasy elements.
Though the influence of Edgar Rice Burroughs is apparent in Brackett's Mars stories, the differences between their versions of Mars are great. Brackett's Mars is set firmly in a world of interplanetary commerce and competition, and one of the most prominent themes of Brackett's stories is the clash of planetary civilizations; the stories both illustrate and criticize the effects of colonialism on civilizations which are either older or younger than those of the colonizers, and thus they have relevance to this day. Burroughs' heroes set out to remake entire worlds according to their own codes; Brackett's heroes (often antiheroes) are at the mercy of trends and movements far bigger than they are.
Screenwriter.
Shortly after Brackett broke into science fiction writing, she also wrote her first screenplays. Hollywood director Howard Hawks was so impressed by her novel "No Good from a Corpse" that he had his secretary call in "this guy Brackett" to help William Faulkner write the script for "The Big Sleep" (1946). The film, written by Brackett, William Faulkner, and Jules Furthman, and starring Humphrey Bogart, is considered one of the best movies ever made in the genre. However, after her marriage, Brackett took a long break from screenwriting.
When she returned to screenwriting in the mid-1950s, she wrote for both TV and movies. Howard Hawks hired her to write or co-write several John Wayne pictures, including "Rio Bravo" (1959), "Hatari!" (1962), "El Dorado" (1966) and "Rio Lobo" (1970). Because of her background with "The Big Sleep", Robert Altman hired her to adapt Raymond Chandler's novel "The Long Goodbye" for the screen.
"The Empire Strikes Back".
Brackett worked on the screenplay for the first "Star Wars" sequel "The Empire Strikes Back". The film won the Hugo Award in 1981. This script was a departure for Brackett, since until then, all of her science fiction had been in the form of novels and short stories.
The exact role which Brackett played in writing the script for "Empire" is the subject of some dispute. What is agreed on by all is that George Lucas asked Brackett to write the screenplay based on his story outline. It is also known that Brackett wrote a finished first draft which was delivered to Lucas shortly before Brackett's death from cancer on March 18, 1978. Two drafts of a new screenplay were written by Lucas and, following the delivery of the screenplay for "Raiders of the Lost Ark", turned over to Lawrence Kasdan for a new approach. Both Brackett and Kasdan (though not Lucas) were given credit for the final script.
Some fans were reported to believe that they could detect traces of Brackett's influence in both the dialogue and the treatment of the space opera genre in "Empire". However, Laurent Bouzereau, in his book "Star Wars: The Annotated Screenplays", states that Lucas disliked the direction of Brackett's screenplay and discarded it. He then produced two screenplays before turning the results over to Kasdan. 
Brackett's screenplay has never been officially or legally published. According to Stephen Haffner, it can be read at one of two locations: the Jack Williamson Special Collections library at Eastern New Mexico University in Portales, New Mexico (but may not be copied or checked out); and the archives at Lucasfilm, Ltd. in California.

</doc>
<doc id="40656" url="http://en.wikipedia.org/wiki?curid=40656" title="13th century BC">
13th century BC

The 13th century BC was the period from 1300 to 1201 BC.
Significant persons.
Although many human societies were literate in this period, some individual persons mentioned in this article ought to be considered legendary rather than historical.
Sovereign States.
See: List of sovereign states in the 13th century BC.

</doc>
<doc id="40657" url="http://en.wikipedia.org/wiki?curid=40657" title="14th century BC">
14th century BC

The 14th century BC is a century which lasted from the year 1400 BC until 1301 BC.
Sovereign States.
See: List of sovereign states in the 14th century BC.

</doc>
<doc id="40658" url="http://en.wikipedia.org/wiki?curid=40658" title="15th century BC">
15th century BC

The 15th century BC is a century which lasted from 1500 BC to 1401 BC.
Sovereign States.
See: List of sovereign states in the 15th century BC.

</doc>
<doc id="40659" url="http://en.wikipedia.org/wiki?curid=40659" title="Kingsbury Commitment">
Kingsbury Commitment

The Kingsbury Commitment of 1913 was an out-of-court settlement of the government's antitrust challenge of AT&T's growing vertical monopoly over the phone industry. In return for the government's agreement not to pursue its case against AT&T as a monopolist, AT&T agreed to divest the controlling interest it had acquired in the Western Union telegraph company, and to allow non-competing independent telephone companies to interconnect with the AT&T long distance network.
The government had been increasingly worried that AT&T and the other Bell Companies were monopolizing the industry. Under Theodore N. Vail from 1907, AT&T had bought Bell-associated companies and organized them into new hierarchies. AT&T had also acquired many of the independents, and bought control of Western Union, giving it a monopolistic position in both telephone and telegraph communication. A key strategy was to refuse to connect its long distance network — technologically, by far the finest and most extensive in the land — with local independent carriers. Without the prospect of long distance services, the market position of many independents became untenable. Vail stated that there should be "one policy, one system [AT&T's] and universal service, no collection of separate companies could give the public the service that [the] Bell... system could give." 
AT&T's strategies prompted complaints and attracted the attention of the Justice Department. Faced with a government investigation for antitrust violations, AT&T entered into negotiations.
In the Kingsbury Commitment, actually a letter from AT&T Vice President Nathan Kingsbury of December 19, AT&T agreed with the Attorney General to divest itself of Western Union, to provide long distance services to independent exchanges under certain conditions and to refrain from acquisitions if the Interstate Commerce Commission objected. 
The Commitment did not settle all the differences between independents and Bell companies, but it did avert the federal takeover many had expected. AT&T was allowed to buy market-share, as long as it sold an equal number of subscribers to independents. Crucially, while the Kingsbury Commitment obliged it to connect its long distance service to independent local carriers, AT&T did not agree to interconnect its local services with other local providers. Nor did AT&T agree to any interconnection with independent long distance carriers.
Consequently, AT&T was able to consolidate its control over both the most profitable urban markets and long distance traffic. The Willis Graham Act allowed AT&T to begin acquiring more local telephone systems, with the genial oversight of the Interstate Commerce Commission. By 1924, the ICC approved AT&T’s acquisition of 223 of the 234 independent telephone companies. Between 1921 and 1934, the ICC approved 271 of the 274 purchase requests of AT&T. With the creation of the Federal Communications Commission in 1934, the government regulated the rates charged by AT&T.
The entire network was nationalized during World War I from June 1918 to July 1919. Following re-privatization, AT&T resumed its near-monopoly position. In 1956, AT&T and the Justice Department agreed on a consent decree to end an antitrust suit brought against AT&T in 1949. Under the decree, AT&T restricted its activities to those related to running the national telephone system, and special projects for the federal government. 
In 1968, FCC regulators intervened when the Bell System tried to prevent a mobile communications system, the Carterfone, from connecting to telephone lines. That decision established the principle that customers could connect any lawful device to the telephone network, even to offer a competing service. In the mid 1970s, emerging long distance competitors like MCI and Sprint faced the same tactic of denying interconnection, which regulators quashed, followed by a series of efforts by the Bell System phone companies to escalate the costs of interconnection as an indirect means of excluding competition. These battles resulted a large amount of antitrust litigation and ultimately led to the 1982 breakup of the Bell System. 
In 1982, AT&T and the Justice Department agreed on tentative terms for settlement of anti-trust suit filed against AT&T in 1974, under which AT&T divested itself of its local telephone operations, which became known as the "Baby Bells." In return, the Justice Department agreed to lift the restrictions on AT&T activities contained in the 1956 Consent Decree.

</doc>
<doc id="40664" url="http://en.wikipedia.org/wiki?curid=40664" title="USS Merrimack">
USS Merrimack

USS "Merrimack, or variant spelling USS "Merrimac, may be any one of several ships commissioned in the United States Navy and named after the Merrimack River.

</doc>
<doc id="40667" url="http://en.wikipedia.org/wiki?curid=40667" title="Accidental (music)">
Accidental (music)

In music, an accidental is a note whose pitch (or pitch class) is not a member of the scale or mode indicated by the most recently applied key signature. In musical notation, the sharp (♯), flat (♭), and natural (♮) symbols, among others, are used to mark such notes, and those symbols may themselves be called accidentals. In the measure (bar) in which it appears, an accidental sign raises or lowers the immediately following note (and any repetition of it in the bar) from its normal pitch, overriding sharps or flats (or their absence) in the key signature. A note is usually raised or lowered by a semitone, although microtonal music may use "fractional" accidental signs. One occasionally sees double sharps or flats, which raise or lower the indicated note by a whole tone. Accidentals apply within the measure and octave in which they appear, unless canceled by another accidental sign, or tied into a following measure. If a note has an accidental and the note is repeated in a different octave within the same measure, the accidental does not apply to the same note of the different octave.
 the first seven letters of the alphabet represent the basic diatonic pitches, with additional symbols called accidentals. In addition to the sharp (♯) and flat (♭) used in Europe to indicate the displacement of a scale degree by a semitone up or down, respectively, Arabic theorists have added accidentals representing a lowering of a pitch by a quarter-tone () and raising it by a quarter-tone sharp (). (Iranians use different symbols.)
The modern accidental signs derive from the round and square small letter "b" used in Gregorian chant manuscripts to signify the two pitches of B, the only note that could be altered. The round "b" became the flat sign, while the square "b" diverged into the sharp and natural signs.
Sometimes the black keys on a musical keyboard are called accidentals or "sharps", and the white keys are called "naturals".
Standard use of accidentals.
In most cases, a sharp raises the pitch of a note one semitone while a flat lowers it a semitone. A natural is used to cancel the effect of a flat or sharp. This system of accidentals operates in conjunction with the key signature, whose effect continues throughout an entire piece, unless canceled by another key signature. An accidental can also be used to cancel a previous accidental or reinstate the flats or sharps of the key signature.
Accidentals apply to subsequent notes on the same staff position for the remainder of the measure in which they occur, unless explicitly changed by another accidental, as shown at right. Notes on other staff positions. Once a barline is passed, the effect of the accidental ends, except when a note affected by an accidental is tied to the same note across a barline. Subsequent notes at the same staff position in the second or later bars are not affected by the accidental carried through with the tied note. 
Though this convention is still in use particularly in tonal music, it may be cumbersome in music that features frequent accidentals, as is often the case in non-tonal music. As a result, an alternative system of note-for-note accidentals has been adopted, with the aim of reducing the number of accidentals required to notate a bar. The system is as follows:
Because seven of the twelve notes of the chromatic equal-tempered scale are naturals (the "white notes", A; B; C; D; E; F; and G on a piano keyboard) this system can significantly reduce the number of naturals required in a notated passage.
Occasionally an accidental may change the note by more than a semitone: for example, if a G♯ is followed in the same measure by a G♭, the flat sign on the latter note means it will be two semitones lower than if no accidental were present. Thus the effect of the accidental has to be understood in relation to the "natural" meaning of the note's staff position. For the sake of clarity, some composers put a natural in front of the accidental. Thus, if in this example the composer wanted the note a semitone lower than G-natural, he might put first a ♮ sign to cancel the previous G♯, then the ♭. However, in most contexts, an F♯ could be used instead.
Double accidentals raise or lower the pitch of a note by two semitones, an innovation developed as early as 1615. This applies to the written note, ignoring key signature. An F with a double sharp applied raises it a whole step so it is enharmonically equivalent to a G. Usage varies on how to notate the situation in which a note with a double sharp is followed in the same measure by a note with a single sharp: some publications simply use the single accidental for the latter note, whereas others use a combination of a natural and a sharp, with the natural being understood to apply to only the second sharp.
The double accidental with respect to a specific key signature raises or lowers the notes containing a sharp or flat by a semitone. For example, when in the key of C♯ minor or E major, F, C, G, and D contain a sharp; adding a double accidental (double sharp) to F for example in this case would only raise the note F♯ by one further semitone, creating G natural. Conversely, if a double sharp were added to any other note not containing a sharp or flat as indicated by the key signature, then the note would be raised two semitones with respect to the chromatic scale. For example, in the aforementioned key signature any note that is not F, C, G, and D will be raised by two semitones instead of one, so an A double sharp raises the note A natural to the enharmonic equivalent of B natural.
Courtesy accidentals.
Although a barline is nowadays understood to cancel the effect of an accidental (except for a tied note), often publishers will use a "courtesy accidental" (also referred to as a "cautionary accidental" or a "reminder accidental") as a reminder of the correct pitch if the same note occurs in the following measure. This usage varies, although a few situations are construed to require a courtesy accidental, such as
Other uses are inconsistently applied.
Courtesy accidentals are sometimes enclosed in parentheses to emphasize their role as reminders.
Publishers of free jazz music and some atonal music sometimes eschew all courtesy accidentals.
Microtonal notation.
Composers of microtonal music have developed a number of notations for indicating the various pitches outside of standard notation. One such system for notating quarter tones, used by the Czech Alois Hába and other composers, is shown on the right.
In the 19th and early 20th centuries, when Turkish musicians switched from their traditional notation systems — which were not staff-based — to the European staff-based system, they created a refinement to the European accidental system in order to notate Turkish scales which make use of intervals smaller than the tempered semitone. There are several such systems which vary as to the division of the octave they presuppose or merely the graphical shape of the accidentals. The most widely used system (created by Rauf Yekta Bey) uses a system of 4 sharps (roughly +25 cents, +75 cents, +125 cents and +175 cents) and 4 flats (roughly −25 cents, −75 cents, −125 cents and −175 cents), none of which correspond to the tempered sharp and flat. They presuppose a Pythagorean division of the octave taking the Pythagorean comma (about an 8th of the tempered tone, actually closer to 24 cents, defined as the difference between 7 octaves and 12 just-intonation fifths) as the basic interval. The Turkish systems have also been adopted by some Arab musicians.
Ben Johnston created a system of notation for pieces in just intonation where the unmarked C, F, and G major chords are just major chords (4:5:6) and accidentals are used to create just tuning in other keys. Between 2000 and 2003, Wolfgang von Schweinitz and Marc Sabat developed the Extended Helmholtz-Ellis JI Pitch Notation, a modern adaptation and extension of the notation principles first used by Hermann von Helmholtz, Arthur von Oettingen and Alexander John Ellis which is rapidly becoming adopted by musicians working in extended Just Intonation (for more details, see the Wikipedia article on Just Intonation).
History of notation of accidentals.
The three principal symbols indicating whether a note should be raised or lowered in pitch are derived from variations of the small letter "b": the sharp (♯) and natural (♮) signs from the square "b quadratum" 𝖇, and the flat sign (♭) from the round "b rotundum" 𝒃.
In the early Middle Ages, a widespread musical tradition was based on the hexachord system defined by Guido of Arezzo. The basic system, called "musica recta", had three overlapping hexachords. Change from one hexachord to another was possible, called a "mutation". A major problem with the system was that mutation from one hexachord to another could introduce intervals like the tritone which were considered to be undesirable, and then dissonance could arise. To avoid the dissonance, a practice called musica ficta arose from the late 12th century onward. This introduced modifications of the hexachord, so that "false" or "feigned" notes could be sung, partly to avoid dissonance. At first only B could be flattened, moving from the "hexachordum durum" (the "hard hexachord") G–A–B–C–D–E where B is natural, to the "hexachordum molle" (the "soft hexachord") F–G–A–B♭–C–D where it is flat. The note B is not present in the third hexachord "hexachordum naturale" (the "natural hexachord") C–D–E–F–G–A.
The different kinds of B were eventually written differently, so as to distinguish them in music theory treatises and in notation. The flat sign ♭ derives from a round "b" that signified the soft hexachord, "hexachordum molle", particularly the presence of B♭. The name of the flat sign in French is "bémol" from medieval French "bé mol" which in modern French is "bé mou" "soft b". The natural sign ♮ and the sharp sign ♯ derive from variations of a square "b" that signified the hard hexachord, "hexachordum durum", where the note in question is B♮. The name of the natural sign in French is "bécarre" from medieval French "bé quarre" which in modern French is "bé carré" "square b". In German music notation the letter "B" or "b" always designates B♭ while the letter "H" or "h" – a deformation of a square "b" – designates B♮.
As polyphony became more complex, notes other than B needed to be altered in order to avoid undesirable harmonic or melodic intervals (especially the augmented fourth, or tritone, that music theory writers referred to as "diabolus in musica", i.e., "the devil in music"). The first sharp in use was F♯, then came the second flat E♭, then C♯, G♯, etc.; by the 16th century B♭, E♭, A♭, D♭, G♭ and F♯, C♯, G♯, D♯ and A♯ were all in use to a greater or lesser extent.
However, those accidentals were often not notated in vocal part-books (but the correct pitches were always notated in tablature). The notational practice of not marking implied accidentals, leaving them to be supplied by the performer instead, was called musica ficta (i.e., "feigned music").
Strictly speaking the medieval signs ♮ and ♭ indicated that the melody is progressing inside a (fictive) "hexachord" of which the signed note is the "mi" or the "fa" respectively. That means they refer to a group of notes "around" the marked note, rather than indicating that the note itself is necessarily an accidental. For example, when a semitone relationship is indicated between F and G, either by placing a mi-sign (♮) on F or a fa-sign (♭) on G, only the context can determine whether this means, in modern terms, F♯-G or F-G♭, or even F♭–G. The use of either the mi-sign on F or the fa-sign on G means only that "some kind of F goes to some kind of G, proceeding by a semitone".
The convention according to which an accidental continues in force through a measure developed only gradually over the 18th century. Prior to that time, accidentals only applied to immediately repeated notes or short groups when the composer felt it was obvious that the accidental should continue. The older practice continued in use well into the 18th century by many composers, notably Johann Sebastian Bach. The newer convention did not achieve general currency until early in the 19th century.

</doc>
<doc id="40671" url="http://en.wikipedia.org/wiki?curid=40671" title="List of sports history organisations">
List of sports history organisations

This is a list of sports history organizations

</doc>
<doc id="40673" url="http://en.wikipedia.org/wiki?curid=40673" title="American goldfinch">
American goldfinch

The American goldfinch ("Spinus tristis"), also known as the eastern goldfinch, is a small North American bird in the finch family. It is migratory, ranging from mid-Alberta to North Carolina during the breeding season, and from just south of the Canadian border to Mexico during the winter.
The only finch in its subfamily to undergo a complete molt, the American goldfinch displays sexual dimorphism in its coloration; the male is a vibrant yellow in the summer and an olive color during the winter, while the female is a dull yellow-brown shade which brightens only slightly during the summer. The male displays brightly colored plumage during the breeding season to attract a mate.
The American goldfinch is a granivore and adapted for the consumption of seedheads, with a conical beak to remove the seeds and agile feet to grip the stems of seedheads while feeding. It is a social bird, and will gather in large flocks while feeding and migrating. It may behave territorially during nest construction, but this aggression is short-lived. Its breeding season is tied to the peak of food supply, beginning in late July, which is relatively late in the year for a finch. This species is generally monogamous, and produces one brood each year.
Human activity has generally benefited the American goldfinch. It is often found in residential areas, attracted to bird feeders which increase its survival rate in these areas. Deforestation also creates open meadow areas which are its preferred habitat.
Taxonomy.
The American goldfinch was one of the many species originally described by Linnaeus in the landmark 1758 10th edition of his work "Systema Naturae". It was initially included in the genus "Spinus", a group containing New World goldfinches and siskins, but in 1976, "Spinus" was merged into the genus "Carduelis" as a subgenus. Recent studies resurrect the genus "Spinus". Its closest relatives are the lesser goldfinch "(S. psaltria)", Lawrence's goldfinch "(S. lawrencei)", and the siskins. Although it shares a name with the European goldfinch, the two are in separate subgenera and are not directly related. "Carduelis" is derived from "carduus", the Latin word for thistle; the species name "tristis" is Latin for 'sorrowful'. There are four recognized subspecies of the American goldfinch:
This seems to be the most ancient extant species of the Meso American Spinus/Carduelis evolutive radiation, whose parental species is Carduelis/Spinus lawrencei.
Description.
The American goldfinch is a small finch, 11 - long, with a wingspan of 19 -. It weighs between 11 -. Among standard measurements, the wing chord is 6.5 to, the tail is 4.2 to, the culmen is 0.9 to and the tarsus is 1.2 to. The beak is small, conical, and pink for most of the year, but turns bright orange with the spring molt in both sexes.
The shape and size of the beak aid in the extraction of seeds from the seed heads of thistles, sunflowers, and other plants.
The American goldfinch undergoes a molt in the spring and autumn. It is the only cardueline finch to undergo a molt twice a year. During the winter molt it sheds all its feathers; in the spring, it sheds all but the wing and tail feathers, which are dark brown in the female and black in the male. The markings on these feathers remain through each molt, with bars on the wings and white under and at the edges of the short, notched tail. The sexual dimorphism displayed in plumage coloration is especially pronounced after the spring molt, when the bright color of the male's summer plumage is needed to attract a mate.
Once the spring molt is complete, the body of the male is a brilliant lemon yellow, a color produced by carotenoid pigments from plant materials in its diet, with a striking jet black cap and white rump that is visible during flight. The female is mostly brown, lighter on the underside with a yellow bib. After the autumn molt, the bright summer feathers are replaced by duller plumage, becoming buff below and olive-brown above, with a pale yellow face and bib. The autumn plumage is almost identical in both sexes, but the male has yellow shoulder patches. In some winter ranges, the goldfinches lose all traces of yellow, becoming a predominantly medium tan-gray color with an olive tinge evident only on close viewing.
The immature American goldfinch has a dull brown back, and the underside is pale yellow. The shoulders and tail are dull black with buff-colored, rather than white, markings on wings and rump. This coloration is the same in both genders.
The song of the American goldfinch is a series of musical warbles and twitters, often with a long note. A "tsee-tsi-tsi-tsit" call is often given in flight; it may also be described as "per-chic-o-ree". While the female incubates the eggs, she calls to her returning mate with a soft continuous "teeteeteeteete" sound. The young begin to use a call of "chick-kee" or "chick-wee" shortly before fledging, which they use until they have left the nest entirely. There are two defense calls made by adults during nesting; a "sweeet" call made to rally other goldfinches to the nest and distract predators, and a "bearbee" used to signal to the nestlings to quiet them and get them to crouch down in the nest to become less conspicuous.
Distribution and habitat.
The American goldfinch prefers open country where weeds thrive, such as fields, meadows, flood plains, as well as roadsides, orchards, and gardens. It may also be found in open deciduous and riparian woodlands and areas of secondary growth. This habitat preference continues during the spring and autumn migrations.
The summer breeding range stretches across North America from coast to coast. It is bounded on the north by Saskatchewan and stretches south across North America to North Carolina on the east coast, and northern California on the west coast. The American goldfinch is a short-distance migrant, moving south in response to colder weather and lessened food supply. The migration is completed in compact flocks, which travel in an erratic, wavelike flight pattern.
Its winter range includes southern Canada and stretches south through the United States to parts of Mexico. In winter, in the northern part of its range, the finch may move nearer to feeders if they are available. In southern ranges, during winter, they remain in areas similar to the fields and flood plains where they live during the summer months.
Attempts were made to introduce the American goldfinch into Bermuda in the 19th century, and Tahiti in 1938, but the species failed to become established.
Behavior.
The American goldfinch flies in a distinctive undulating pattern, creating a wave-shaped path. This normally consists of a series of wing beats to lift the bird, then folding in the wings and gliding in an arc before repeating the pattern. Birds often vocalize during the flapping phase of the pattern and then go silent during the coasting phase. The call made during flight is "per-twee-twee-twee", or "ti-di-di-di", punctuated by the silent periods.
The American goldfinch is gregarious during the non-breeding season, when it is often found in large flocks, usually with other finches. During the breeding season, it lives in loose colonies. While the nest is being constructed, the male will act aggressively toward other males who intrude into his territory, driving them away, and the female reacts in the same way toward other females. This aggressiveness subsides once the eggs have been laid.
The American goldfinch does not act aggressively toward predators within its territory; its only reaction is alarm calling. Predators include snakes, weasels, squirrels, and blue jays, which may destroy eggs or kill young, and hawks and cats, which pose a threat to both young and adults. As of 2007, the oldest known American goldfinch was 10 years and 5 months old.
Diet.
The American goldfinch is a diurnal feeder. According to the Cornell Lab of Ornithology, the species is one of the strictest vegetarians in the bird world. It is mainly granivorous, but will occasionally eat insects, which are also fed to its young to provide protein. Its diet consists of the seeds from a wide variety of annual plants, often those of weeds grasses and trees, such as thistle, teasel, dandelion, ragweed, mullein, cosmos, goatsbeard, sunflower, and alder. However, it also consumes tree buds, maple sap, and berries. It will eat at bird feeders provided by humans, particularly in the winter months, preferring Niger seed (commonly and erroneously called thistle seed).
Unlike some finch species, the American goldfinch uses its feet extensively in feeding. It frequently hangs from seedheads while feeding in order to reach the seeds more easily. In the spring, the American goldfinch feeds on the catkins hanging from birches and alders by pulling one up with its beak and using its toes to hold the catkin still against the branch. This dexterity enables it to take advantage of food sources relatively inaccessible to potential competitors, increasing its chances of survival.
Reproduction.
The American goldfinch begins its breeding season later in the year than any other finch and later than any other native North American bird, besides occasionally the sedge wren. This may be related to the abundance of seeds in the late summer months, as seeds represent the majority of their diet.
The courtship rituals of the American goldfinch include aerial maneuvers and singing by males, who begin courtship in late July. The flight displays begin as the male pursues the female, who flies in zigzagging evasive patterns. The male is able to signal his quality and fitness, both in the short term (current body condition) and long term (genes), through ornamentation (bill color and plumage). If a female accepts the male as a mate, the pair will fly in wide circles, as the male warbles throughout the flight.
Once a male has found a mate, he selects a territory, marking the boundaries by warbling as he flies from perch to perch. After circling the perimeter, he performs two flight displays, first repeating a low, flat flight, then flying in an exaggerated version of normal flight, tucking his wings close to his body, plummeting earthwards and catching himself as he spreads his wings to glide upward in a series of loops. Two or three pairs may group their territories together in a loose colony, perhaps to aid in defense against predators.
The nest is built in late summer by the female in the branches of a deciduous shrub or tree at a height of up to 10 m. The nest-building lasts approximately six days, during which time the female works in 10–40 minute increments. The male frequently flies with the female as she collects nesting materials, and though he may carry some materials back to the nest, he leaves its construction to the female. The outer shell of the nest is built of bark, weeds, vines, and grass. The inside diameter of the finished nest is about 6.5 cm. The rim is reinforced with bark bound by spiderwebs and caterpillar silk, and the cup is lined with plant down from milkweed, thistle, or cattail. The nest is so tightly woven that it can hold water, and it is possible for nestlings to drown following a rainstorm if the parents do not cover the nest.
American goldfinches lay four to six bluish-white eggs, which are oval in shape and about 16 x, roughly the size of a peanut. It is thought that they are laid during the night. The eggs are incubated by the female alone, though the male brings her food as she nests, and most mating pairs raise only one brood each year.
The chicks hatch 12–14 days after incubation begins. Like all passerines, the chicks are altricial; they are born naked, with reddish bodies, pale grey down, and closed eyes. The mother bird feeds her young regurgitated seeds and insects as they grow. The hatchlings develop quickly, opening their eyes after three days, and completing the growth of olive-brown juvenile plumage after 11–15 days, at which time they begin to practice short flights close to the nest. For up to three weeks after fledging, they are still fed by the male, who locates them by listening for their fledging call. The chicks stop giving this call when they become entirely independent.
American goldfinches are occasionally victims of brood parasites, particularly brown-headed cowbirds. One study found that 9% of nests had brown-headed cowbird eggs in them. American goldfinches make very poor hosts for brood parasites, with studies showing low hatching rates of brown-headed cowbird eggs and no fledging success. This is despite the fact that the American goldfinch has no known behavioral adaptations against brood parasites. It is thought that the inability of brown-headed cowbird chicks to survive is due to a failure to get enough nutrition; the seed-rich diet of American goldfinch chicks varies from the usual insect-rich diet of other hosts.
Relationship with humans.
The American goldfinch is found in residential areas throughout its range. Backyard birders attract it using feeders containing Nyjer thistle seed, or by planting grasses and perennial plants, such as zinnias, cosmos, bee balm, or globe thistle, which produce seedheads favored by finches. Although some controversy surrounds bird feeding (see bird feeder for details), an increase in backyard feeding by humans has generally been beneficial to this species.
The American goldfinch is not threatened by human activity, and is widespread throughout its range. The clearing of forests by humans, though harmful to many species, has benefited the American goldfinch. Clearing of woodlands causes declines in numbers of neotropical migrants, while favoring short-distance migrants and permanent residents. This benefits the American goldfinch both as a short-distance migrant, and because the created open areas are the preferred environment of the bird, where weeds thrive which produce the primary food source of the American goldfinch.
State bird.
The American goldfinch is the state bird of Iowa and New Jersey, where it is called the "eastern goldfinch", and Washington, where it is called the "willow goldfinch". It was chosen by schoolchildren in Washington in 1951.
External links.
Listen to this article ()
This audio file was created from a revision of the "American goldfinch" article dated 2009-04-13, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="40674" url="http://en.wikipedia.org/wiki?curid=40674" title="Meissen">
Meissen

Meissen (in German orthography: "Meißen") is a town of approximately 30,000 about 25 km northwest of Dresden on both banks of the Elbe river in the Free State of Saxony, in eastern Germany. Meissen is the home of Meissen porcelain, the Albrechtsburg castle, the Gothic Meissen Cathedral and the Meissen Frauenkirche. The "Grosse Kreisstadt" is the capital of the Meissen district.
History.
Historical affiliations
 Margraviate of Meissen 968–1002<br>
Duchy of Poland 1002<br>
 Margraviate of Meissen 1002–1423<br>
 Electorate of Saxony 1423–1697<br>
 Poland-Saxony 1697–1706<br>
 Electorate of Saxony 1706–1709<br>
 Poland-Saxony 1709–1763<br>
 Electorate of Saxony 1763–1806<br>
 Kingdom of Saxony 1806–1871<br>
 German Empire 1871–1918<br>
 Weimar Republic 1918–1933<br>
 Nazi Germany 1933–1945<br>
 Allied-occupied Germany 1945–1949<br>
 East Germany 1949–1990<br>
 Germany 1990–present
Meissen is sometimes known as the "cradle of Saxony". The city grew out of the early Slavic settlement of "Mis(s)ni", named for the small river Mis(s)na today Meis(s)abach (see Miesbach/Musbach/Mosbach), inhabited by the Slavic Glomacze tribe and was founded as a German town by King Henry the Fowler in 929. In 968, the Diocese of Meissen was founded, and Meissen became the episcopal see of a bishop. The Catholic bishopric was suppressed in 1581 after the diocese accepted the Protestant Reformation (1559), but re-created in 1921 with its seat first at Bautzen and now at the Katholische Hofkirche in Dresden. Meissen is the literal plural form of the modern English word "moss"—translating literally as mosses or simply as marsh. Hence Meissner or Meisner—one who works the marsh—porcelain maker.
The Margraviate of Meissen was founded in 968 as well, with the city as the capital of the Margraves of Meissen. A market town by 1000, Meissen passed to the Duchy of Poland in 1002 under Boleslaw I the Brave, afterwards into hands of Henry II a few months later and the House of Wettin in 1089. The city was at the forefront of the Ostsiedlung, or intensive German settlement of the rural Slavic lands east of the Elbe, and its reception of city rights dates to 1332.
The construction of Meissen Cathedral was begun in 1260 on the same hill as the Albrechtsburg castle. The resulting lack of space led to the cathedral being one of the smallest cathedrals in Europe. The church is also known as being one of the purest examples of Gothic architecture.
During World War II, a subcamp of Flossenbürg concentration camp was located in Meissen.
Porcelain.
Meissen, Meissen porcelain, is famous for the manufacture of porcelain, based on extensive local deposits of china clay (kaolin) and potter's clay (potter's earth). Meissen porcelain was the first high quality porcelain to be produced outside of the Orient.
The first European porcelain was manufactured in Meissen in 1710, when the Royal Porcelain Factory was opened in the Albrechtsburg. In 1861, it was moved to the "Triebisch" river valley of Meissen, where the porcelain factory can still be found today. Along with porcelain, other ceramics are also manufactured.
Main sights.
The Albrechtsburg, the former residence of the House of Wettin, is regarded as being the first castle to be used as a royal residence in the German-speaking world. Built between 1472 and 1525, it is a fine example of late Gothic style. It was redecorated in the 19th century with a range of murals depicting Saxon history. Today the castle is a museum. Nearby is the 13th-century Gothic Meissen Cathedral ("Meißner Dom"), whose chapel is one of the most famous burial places of the Wettin family. The hill on which the castle and the cathedral are built offers a view over the roofs of the old town.
Meissen's historical district is located mostly around the market at the foot of the castle hill. It contains many buildings of Renaissance architecture. Also imposing is the view from the 57 metre high tower of the "Frauenkirche" (Church of Our Lady), situated in the old market-place. This church, not to be confused with the Dresden Frauenkirche, was first mentioned in a 1205 deed issued by Bishop Dietrich II and after a blaze about 1450 rebuilt in the Late Gothic style of a hall church. Its tower hosts the world's first porcelain carillon, manufactured in 1929 on the occasion of the town's 1000-years-jubilee. Another popular tourist sight is the world-famous Meissen porcelain factory.
From spring to autumn, several festivals take place in Meissen, such as the pottery market or the "Weinfest", which celebrates the wine harvest. Meissen wine is produced at the vineyards in the river valley ("Elbtal") around the town, part of the Saxonian wine region, one of the northernmost in Europe.
Sister cities.
Meissen is twinned with:
 Vitry Sur Seine, France, since 1973

</doc>
<doc id="40676" url="http://en.wikipedia.org/wiki?curid=40676" title="Luc Besson">
Luc Besson

Luc Besson (]; born 18 March 1959) is a French film director, writer, and producer. He is known for directing and producing thrillers and action films that are visually rich. Critics cite Besson as a pivotal figure in the Cinéma du look movement, a specific, highly visual style produced from the 1980s into the early 1990s. "Subway" (1985), "The Big Blue" (1988) and "Nikita" (1990) are all considered to be of this stylistic school. Besson had been nominated for Best Director and Best Picture César Awards for his films ' and '. He won Best Director and Best French Director for his sci-fi action film "The Fifth Element" (1997). His action thriller film "Taken 2" (2012) is France's biggest export success.
In 1980, he founded his own production company, called "Les Films du Loup," and later "Les Films du Dauphin." This was superseded in 2000 by his co-founding EuropaCorp film company with his longtime collaborator, Pierre-Ange Le Pogam. Besson has been involved with filmmaking for more than 50 films, spanning 26 years, as writer, director, and/or producer. He also been declared the John Hughes of action films due to his screen written skills are more famous than his directing. 
Early life.
Besson was born in Paris, to parents who both worked as Club Med scuba-diving instructors. Influenced by this milieu, as a child Besson planned to become a marine biologist. He spent much of his youth traveling with his parents to tourist resorts in Italy, Yugoslavia, and Greece. The family returned to France when Besson was 10. His parents promptly divorced and each remarried.
"Here there is two families, and I am the only bad souvenir of something that doesn't work," he said in the "International Herald Tribune". "And if I disappear, then everything is perfect. The rage to exist comes from here. I have to do something! Otherwise I am going to die."
At the age of 17, Besson had a diving accident that left him unable to dive.
Career.
Out of boredom, Besson started writing stories, including the background to what he later developed as "The Fifth Element" (1997), one of his most popular movies. The film is inspired by the French comic books which Besson read as a teenager. He reportedly worked on the first drafts of "Le Grand Bleu" while still in his teens. Besson directed and co-wrote the screenplay of this science fiction thriller with the screenwriter, Robert Mark Kamen.
At 18, Besson returned to his birthplace of Paris. There he took odd jobs in film to get a feel for the industry. He worked as an assistant to directors including Claude Faraldo and Patrick Grandperret. Besson directed three short films, a commissioned documentary, and several commercials.
After this, he moved to the United States for three years, but returned to Paris, where he formed his own production company. He first named it "Les Films du Loup," but changed it to "Les Films du Dauphin". In the early 1980s, Besson met Éric Serra and asked him to compose the score for his first short film, "L'Avant dernier". He later used Serra as a composer for other films of his.
Since the late 20th century, Besson has written and produced numerous action movies, including the "Taxi" (1998–2007) and "The Transporter" (2002–2008) series, and the Jet Li films "Kiss of the Dragon" and "Unleashed/Danny the Dog." His English-language films "Taken" and "Taken 2," both starring Liam Neeson, have been major successes, with "Taken 2" becoming the largest-grossing export French film. Besson produced the promotional movie for the Paris bid for the 2012 Summer Olympics.
Besson won Best Director and Best French Director for his film "The Fifth Element" (1997). he was nominated for Best Director and Best Picture César Awards for his films ' (1994) and ' (1999).
Cinéma du look.
Critics cite Besson as a pivotal figure in the Cinéma du look movement, a specific, highly visual style produced from the 1980s into the early 1990s. "Subway" (1985), "The Big Blue" (1988) and "Nikita" (1990) are all considered to be of this stylistic school. The term was coined by critic Raphaël Bassan in a 1989 essay in "La Revue du Cinema n° 449." A partisan of the experimental cinema and friend of the New Wave ("nouvelle vague") directors, Bassan grouped Besson with Jean-Jacques Beineix and Leos Carax as three directors who shared the style of "le look." These directors were later described critically as favoring style over substance, and spectacle over narrative.
Besson, along with most of the filmmakers so categorized, was uncomfortable with the label, particularly in light of the achievements of their forebears: France's New Wave. "Jean-Luc Godard and François Truffaut were rebelling against existing cultural values and used cinema as a means of expression simply because it was the most avant-garde medium at the time," said Besson in a 1985 interview in "The New York Times". "Today, the revolution is occurring entirely within the industry and is led by people who want to change the look of movies by making them better, more convincing and pleasurable to watch.
"Because it's becoming increasingly difficult to break into this field, we have developed a psychological armor and are ready to do anything in order to work", he added in this same interview. "I think our ardor alone is going to shake the pillars of the moviemaking establishment."
Besson directed a biopic of Aung San Suu Kyi called "The Lady" (original title "Dans la Lumiere"), which was released in the fall of 2011. He also worked on "Lockout", which was released in April 2012.
Work.
Many of Besson's films have achieved popular, if not critical, success. One such release was "Le Grand Bleu". 
"When the film had its premiere on opening night at the 1988 Cannes Film Festival, it was mercilessly drubbed, but no matter; it was a smash," observed the "International Herald Tribune" in a 2007 profile of Besson. "Embraced by young people who kept returning to see it again, the movie sold 10 million tickets and quickly became what the French call a 'film générationnel,' a defining moment in the culture."
Besson created the Arthur series, which comprises "Arthur and the Minimoys", "Arthur and the Forbidden City", "Arthur and the Vengeance of Maltazard" and "Arthur and the War of the Two Worlds". He directed "Arthur and the Invisibles", an adaptation of the first two books of the collection. A film with live action and animation, it was released in the UK and the US and starred Freddie Highmore, Madonna, Snoop Dogg, Mia Farrow, Robert De Niro and David Bowie.
Critical evaluation.
Besson has been described as "the most Hollywood of French filmmakers." Tobias Scott wrote that his "slick, commercial" action movies were "so interchangeable—drugs, sleaze, chuckling supervillainy, and Hong Kong-style effects—that each new project probably starts with white-out on the title page."
American film critic Armond White has praised Besson, whom he ranks as one of the best film producers, for refining and revolutionizing action film. He wrote that Besson dramatizes the struggle of his characters "as a conscientious resistance to human degradation".
Personal life.
Besson has been married four times; first, in 1986, to actress Anne Parillaud who would star in Besson's "Nikita" (1990). Besson and Parillaud had a daughter, Juliette, born in 1987.The couple divorced in 1991.
Besson's second wife was actress Maïwenn Le Besco, who was 15 when they began dating in 1991. They were married in late 1992 when Le Besco was pregnant with their daughter Shanna, who was born on 3 January 1993. Le Besco later claimed that their relationship inspired Besson's film "" (1994). Their marriage ended in 1997, when Besson became involved with American actress Milla Jovovich during the filming of "The Fifth Element (1997)." He married the 22-year-old on 14 December 1997, at the age of 38, but they divorced in 1999.
On 28 August 2004, at the age of 45, Besson married film producer Virginie Silla. The couple has three children: Talia, Satine, and Mao Besson.
Legacy and honors.
Among Besson's awards are the Brussels International Festival of Fantasy Film Critics Prize, Fantasporto Audience Jury Award-Special Mention, Best Director, and Best Film, for "Le Dernier Combat" in 1983; the Italian National Syndicate of Film Journalists Silver Ribbon-Best Director-Foreign Film, for "La Femme Nikita", 1990; the Alexander Korda Award for Best British Film, "Nil by Mouth", 1997; and the Best Director Cesar Award, for "The Fifth Element", 1997.
Film company.
In 2000, Besson superseded his production company by co-founding EuropaCorp with Pierre-Ange Le Pogam, with whom he had frequently worked since 1985. Le Pogam had then been Distribution Director with Gaumont. EuropaCorp has had strong growth based on several English-language films, with international distribution. It has production facilities in Paris, Normandy, and Hollywood, and is establishing distribution partnerships in Japan and China.

</doc>
<doc id="40677" url="http://en.wikipedia.org/wiki?curid=40677" title="Cornish">
Cornish

Cornish is the adjective and demonym associated with Cornwall, the most southwesterly part of the United Kingdom. It may refer to:

</doc>
<doc id="40679" url="http://en.wikipedia.org/wiki?curid=40679" title="Abort">
Abort

Abort can mean:

</doc>
<doc id="40680" url="http://en.wikipedia.org/wiki?curid=40680" title="Absolute gain">
Absolute gain

Absolute gain may refer to:

</doc>
<doc id="40682" url="http://en.wikipedia.org/wiki?curid=40682" title="Access">
Access

Access may refer to:
</small>

</doc>
<doc id="40684" url="http://en.wikipedia.org/wiki?curid=40684" title="Access control">
Access control

In the fields of physical security and information security, access control is the selective restriction of access to a place or other 
resource. The act of "accessing" may mean consuming, entering, or using. Permission to access a resource is called "authorization".
Locks and login credentials are two analogous mechanisms of access control.
Physical security.
Geographical access control may be enforced by personnel (e.g., border guard, bouncer, ticket checker), or with a device such as a turnstile. There may be fences to avoid circumventing this access control. An alternative of access control in the strict sense (physically controlling access itself) is a system of checking authorized presence, see e.g. Ticket controller (transportation). A variant is exit control, e.g. of a shop (checkout) or a country.
The term access control refers to the practice of restricting entrance to a property, a building, or a room to authorized persons. Physical access control can be achieved by a human (a guard, bouncer, or receptionist), through mechanical means such as locks and keys, or through technological means such as access control systems like the mantrap. Within these environments, physical key management may also be employed as a means of further managing and monitoring access to mechanically keyed areas or access to certain small assets.
Physical access control is a matter of who, where, and when. An access control system determines who is allowed to enter or exit, where they are allowed to exit or enter, and when they are allowed to enter or exit. Historically, this was partially accomplished through keys and locks. When a door is locked, only someone with a key can enter through the door, depending on how the lock is configured. Mechanical locks and keys do not allow restriction of the key holder to specific times or dates. Mechanical locks and keys do not provide records of the key used on any specific door, and the keys can be easily copied or transferred to an unauthorized person. When a mechanical key is lost or the key holder is no longer authorized to use the protected area, the locks must be re-keyed.
Electronic access control uses computers to solve the limitations of mechanical locks and keys. A wide range of credentials can be used to replace mechanical keys. The electronic access control system grants access based on the credential presented. When access is granted, the door is unlocked for a predetermined time and the transaction is recorded. When access is refused, the door remains locked and the attempted access is recorded. The system will also monitor the door and alarm if the door is forced open or held open too long after being unlocked.
Access control system operation.
When a credential is presented to a reader, the reader sends the credential’s information, usually a number, to a control panel, a highly reliable processor. The control panel compares the credential's number to an access control list, grants or denies the presented request, and sends a transaction log to a database. When access is denied based on the access control list, the door remains locked. If there is a match between the credential and the access control list, the control panel operates a relay that in turn unlocks the door. The control panel also ignores a door open signal to prevent an alarm. Often the reader provides feedback, such as a flashing red LED for an access denied and a flashing green LED for an access granted.
The above description illustrates a single factor transaction. Credentials can be passed around, thus subverting the access control list. For example, Alice has access rights to the server room, but Bob does not. Alice either gives Bob her credential, or Bob takes it; he now has access to the server room. To prevent this, two-factor authentication can be used. In a two factor transaction, the presented credential and a second factor are needed for access to be granted; another factor can be a PIN, a second credential, operator intervention, or a biometric input.
There are three types (factors) of authenticating information:
Passwords are a common means of verifying a user's identity before access is given to information systems. In addition, a fourth factor of authentication is now recognized: someone you know, whereby another person who knows you can provide a human element of authentication in situations where systems have been set up to allow for such scenarios. For example, a user may have their password, but have forgotten their smart card. In such a scenario, if the user is known to designated cohorts, the cohorts may provide their smart card and password, in combination with the extant factor of the user in question, and thus provide two factors for the user with the missing credential, giving three factors overall to allow access.
Credential.
A credential is a physical/tangible object, a piece of knowledge, or a facet of a person's physical being, that enables an individual access to a given physical facility or computer-based information system. Typically, credentials can be something a person knows (such as a number or PIN), something they have (such as an access badge), something they are (such as a biometric feature) or some combination of these items. This is known as multi-factor authentication. The typical credential is an access card or key-fob, and newer software can also turn users' smartphones into access devices. There are many card technologies including magnetic stripe, bar code, Wiegand, 125 kHz proximity, 26-bit card-swipe, contact smart cards, and contactless smart cards. Also available are key-fobs, which are more compact than ID cards, and attach to a key ring. Biometric technologies include fingerprint, facial recognition, iris recognition, retinal scan, voice, and hand geometry. The built-in biometric technologies found on newer smartphones can also be used as credentials in conjunction with access software running on mobile devices. In addition to older more traditional card access technologies, newer technologies such as Near field communication (NFC) and Bluetooth low energy also have potential to communicate user credentials to readers for system or building access.
Access control system components.
An access control point, which can be a door, turnstile, parking gate, elevator, or other physical barrier, where granting access can be electronically controlled. Typically, the access point is a door. An electronic access control door can contain several elements. At its most basic, there is a stand-alone electric lock. The lock is unlocked by an operator with a switch. To automate this, operator intervention is replaced by a reader. The reader could be a keypad where a code is entered, it could be a card reader, or it could be a biometric reader. Readers do not usually make an access decision, but send a card number to an access control panel that verifies the number against an access list. To monitor the door position a magnetic door switch can be used. In concept, the door switch is not unlike those on refrigerators or car doors. Generally only entry is controlled, and exit is uncontrolled. In cases where exit is also controlled, a second reader is used on the opposite side of the door. In cases where exit is not controlled, free exit, a device called a request-to-exit (REX) is used. Request-to-exit devices can be a push-button or a motion detector. When the button is pushed, or the motion detector detects motion at the door, the door alarm is temporarily ignored while the door is opened. Exiting a door without having to electrically unlock the door is called mechanical free egress. This is an important safety feature. In cases where the lock must be electrically unlocked on exit, the request-to-exit device also unlocks the door.
Access control topology.
Access control decisions are made by comparing the credential to an access control list. This look-up can be done by a host or server, by an access control panel, or by a reader. The development of access control systems has seen a steady push of the look-up out from a central host to the edge of the system, or the reader. The predominant topology circa 2009 is hub and spoke with a control panel as the hub, and the readers as the spokes. The look-up and control functions are by the control panel. The spokes communicate through a serial connection; usually RS-485. Some manufactures are pushing the decision making to the edge by placing a controller at the door. The controllers are IP enabled, and connect to a host and database using standard networks.
Types of readers.
Access control readers may be classified by the functions they are able to perform:
Some readers may have additional features such as an LCD and function buttons for data collection purposes (i.e. clock-in/clock-out events for attendance reports), camera/speaker/microphone for intercom, and smart card read/write support.
Access control readers may also be classified by their type of identification technology.
Access control system topologies.
1. Serial controllers. Controllers are connected to a host PC via a serial RS-485 communication line (or via 20mA current loop in some older systems). External RS-232/485 converters or internal RS-485 cards have to be installed, as standard PCs do not have RS-485 communication ports.
Advantages:
Disadvantages: 
2. Serial main and sub-controllers. All door hardware is connected to sub-controllers (a.k.a. door controllers or door interfaces). Sub-controllers usually do not make access decisions, and instead forward all requests to the main controllers. Main controllers usually support from 16 to 32 sub-controllers.
Advantages:
Disadvantages: 
3. Serial main controllers & intelligent readers. All door hardware is connected directly to intelligent or semi-intelligent readers. Readers usually do not make access decisions, and forward all requests to the main controller. Only if the connection to the main controller is unavailable, will the readers use their internal database to make access decisions and record events. Semi-intelligent reader that have no database and cannot function without the main controller should be used only in areas that do not require high security. Main controllers usually support from 16 to 64 readers. All advantages and disadvantages are the same as the ones listed in the second paragraph. 
4. Serial controllers with terminal servers. In spite of the rapid development and increasing use of computer networks, access control manufacturers remained conservative, and did not rush to introduce network-enabled products. When pressed for solutions with network connectivity, many chose the option requiring less efforts: addition of a terminal server, a device that converts serial data for transmission via LAN or WAN.
Advantages:
Disadvantages:
All the RS-485-related advantages and disadvantages also apply.
5. Network-enabled main controllers. The topology is nearly the same as described in the second and third paragraphs. The same advantages and disadvantages apply, but the on-board network interface offers a couple of valuable improvements. Transmission of configuration and user data to the main controllers is faster, and may be done in parallel. This makes the system more responsive, and does not interrupt normal operations. No special hardware is required in order to achieve redundant host PC setup: in the case that the primary host PC fails, the secondary host PC may start polling network controllers. The disadvantages introduced by terminal servers (listed in the fourth paragraph) are also eliminated. 
6. IP controllers. Controllers are connected to a host PC via Ethernet LAN or WAN.
Advantages:
Disadvantages: 
7. IP readers. Readers are connected to a host PC via Ethernet LAN or WAN.
Advantages:
Disadvantages:
The advantages and disadvantages of IP controllers apply to the IP readers as well.
Security risks.
The most common security risk of intrusion through an access control system is by simply following a legitimate user through a door, and this is referred to as "tailgating". Often the legitimate user will hold the door for the intruder. This risk can be minimized through security awareness training of the user population, or more active means such as turnstiles. In very high security applications this risk is minimized by using a sally port, sometimes called a security vestibule or mantrap, where operator intervention is required presumably to assure valid identification.
The second most common risk is from levering a door open. This is surprisingly simple and effective on most doors. The lever could be as small as a screwdriver or big as a crow bar. Fully implemented access control systems include forced door monitoring alarms. These vary in effectiveness, usually failing from high false positive alarms, poor database configuration, or lack of active intrusion monitoring.
Similar to levering is crashing through cheap partition walls. In shared tenant spaces the divisional wall is a vulnerability. A vulnerability along the same lines is the breaking of sidelights.
Spoofing locking hardware is fairly simple and more elegant than levering. A strong magnet can operate the solenoid controlling bolts in electric locking hardware. Motor locks, more prevalent in Europe than in the US, are also susceptible to this attack using a doughnut shaped magnet. It is also possible to manipulate the power to the lock either by removing or adding current. 
Access cards themselves have proven vulnerable to sophisticated attacks. Enterprising hackers have built portable readers that capture the card number from a user’s proximity card. The hacker simply walks by the user, reads the card, and then presents the number to a reader securing the door. This is possible because card numbers are sent in the clear, no encryption being used.
Finally, most electric locking hardware still have mechanical keys as a fail-over. Mechanical key locks are vulnerable to bumping.
The need-to-know principle.
The need to know principle can be enforced with user access controls and authorization procedures and its objective is to ensure that only authorized individuals gain access to information or systems necessary to undertake their duties. See Principle of least privilege.
Computer security.
In computer security, general access control includes authorization, authentication, access approval, and audit. A more narrow definition of access control would cover only access approval, whereby the system makes a decision to grant or reject an access request from an already authenticated subject, based on what the subject is authorized to access. Authentication and access control are often combined into a single operation, so that access is approved based on successful authentication, or based on an anonymous access token. Authentication methods and tokens include passwords, biometric scans, physical keys, electronic keys and devices, hidden paths, social barriers, and monitoring by humans and automated systems.
In any access-control model, the entities that can perform actions on the system are called "subjects", and the entities representing resources to which access may need to be controlled are called "objects" (see also Access Control Matrix). Subjects and objects should both be considered as software entities, rather than as human users: any human users can only have an effect on the system via the software entities that they control.
Although some systems equate subjects with "user IDs", so that all processes started by a user by default have the same authority, this level of control is not fine-grained enough to satisfy the principle of least privilege, and arguably is responsible for the prevalence of malware in such systems (see computer insecurity).
In some models, for example the object-capability model, any software entity can potentially act as both subject and object.
s of 2014[ [update]], access-control models tend to fall into one of two classes: those based on capabilities and those based on access control lists (ACLs).
Both capability-based and ACL-based models have mechanisms to allow access rights to be granted to all members of a "group" of subjects (often the group is itself modeled as a subject).
Access control systems provide the essential services of "authorization", "identification and authentication" ("I&A"), "access approval", and "accountability" where:
Access Control.
Access to accounts can be enforced through many types of controls.
Telecommunication.
In telecommunication, the term "access control" is defined in U.S. Federal Standard 1037C with the following meanings: 
This definition depends on several other technical terms from Federal Standard 1037C.
Public policy.
In public policy, access control to restrict access to systems ("authorization") or to track or monitor behavior within systems ("accountability") is an implementation feature of using trusted systems for security or social control.

</doc>
<doc id="40687" url="http://en.wikipedia.org/wiki?curid=40687" title="Access time">
Access time

Access time is the time delay or latency between a request to an electronic system, and the access being completed or the requested data returned.
See also.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40688" url="http://en.wikipedia.org/wiki?curid=40688" title="Baud">
Baud

In telecommunication and electronics, baud (, unit symbol Bd) is the unit for symbol rate or modulation rate in "symbols per second" or "pulses per second". It is the number of distinct symbol changes (signaling events) made to the transmission medium per second in a digitally modulated signal or a line code.
Digital data modem manufacturers commonly define the baud as the modulation rate of data transmission and express it as bits per second.
Baud is related to gross bit rate expressed as bits per second.
Definitions.
The symbol duration time, also known as unit interval, can be directly measured as the time between transitions by looking into an eye diagram of an oscilloscope. The symbol duration time "T"s can be calculated as:
where "f"s is the symbol rate.
There is also a chance of miscommunication which leads to ambiguity.
In digital systems (i.e., using discrete/discontinuous values) with binary code, 1 Bd = 1 bit/s. By contrast, non-digital (or analog) systems use a continuous range of values to represent information and in these systems the exact informational size of 1 Bd varies.
The baud unit is named after Émile Baudot, the inventor of the Baudot code for telegraphy, and is represented in accordance with the rules for SI units. That is, the first letter of its symbol is uppercase (Bd), but when the unit is spelled out, it should be written in lowercase (baud) except when it begins a sentence.
The baud is scaled using standard decimal prefixes, so that for example
Relationship to gross bit rate.
The symbol rate is related to gross bit rate expressed in bit/s.
The term baud has sometimes incorrectly been used to mean bit rate, since these rates are the same in old modems as well as in the simplest digital communication links using only one bit per symbol, such that binary "0" is represented by one symbol, and binary "1" by another symbol. In more advanced modems and data transmission techniques, a symbol may have more than two states, so it may represent more than one bit. A bit (binary digit) always represents one of two states.
If "N" bits are conveyed per symbol, and the gross bit rate is "R", inclusive of channel coding overhead, the symbol rate "f"s can be calculated as
In that case "M"=2"N" different symbols are used. In a modem, these may be sinewave tones with unique combinations of amplitude, phase and/or frequency. For example, in a 64QAM modem, "M"=64, and so the bit rate is "N"=6 ( 6=log2(64) ) times the baud. In a line code, these may be "M" different voltage levels.
The ratio might not even be an integer; in 4B3T coding, the bit rate is 4/3 baud. (A typical basic rate interface with a 160 kbit/s raw data rate operates at 120 kBd.) On the other hand, Manchester coding has a bit rate equal to 1/2 the baud.
By taking information per pulse "N" in bit/pulse to be the base-2-logarithm of the number of distinct messages "M" that could be sent, Hartley constructed a measure of the gross bitrate "R" as

</doc>
<doc id="40690" url="http://en.wikipedia.org/wiki?curid=40690" title="Acoustic coupler">
Acoustic coupler

In telecommunications, an acoustic coupler is an interface device for coupling electrical signals by acoustical means—usually into and out of a telephone instrument.
The link is achieved through converting electric signals from the phone line to sound and reconvert sound to electric signals needed for the end terminal, such as a teletypewriter, and back, rather than through direct electrical connection.
History and applications.
Prior to its breakup in 1984, Bell System's legal monopoly over telephony in the United States allowed the company to impose strict rules on how consumers could access their network. Customers were prohibited from connecting equipment not made or sold by Bell to the network. The same set-up was operative in nearly all countries, where the telephone companies were nationally owned. In many households, telephones were hard-wired to wall terminals before connectors like RJ11 and BS 6312 became standardised.
The situation was similar in other countries. In Australia, until 1975 the PMG, a Government monopoly, owned all telephone wiring and equipment in user premises and prohibited attachment of third party devices, and while most handsets were connected by 600 series connectors, these were peculiar to Australia so imported equipment could not be directly connected in any case, despite the general electrical compatibility. 
It was not until a landmark court ruling regarding the Hush-A-Phone in 1956 that the use of a phone attachment (by a third party vendor) was allowed for the first time; though AT&T's right to regulate any device connected to the telephone system was upheld by the courts, they were instructed to cease interference towards Hush-A-Phone users. A second court decision in 1968 regarding the Carterfone further allowed "any device not harmful to the system" to be connected directly to the AT&T network. This decision enabled the proliferation of later innovations like answering machines, fax machines, and modems.
Robert Weitbrecht created a workaround for the Bell restrictions in 1963. He developed a coupling device that converted sound from the ear piece of the telephone handset to electrical signals, and converts the electrical pulses coming from the teletypewriter to sound that goes into the mouth piece of the telephone handset. His acoustic coupler is known as the Weitbrecht Modem.
The Weitbrecht Modem inspired other engineers to develop other modems to work with 8-bit ASCII terminals at a faster rate. Such modems or couplers were developed around 1966 by John van Geen at the Stanford Research Institute (now SRI International), that mimicked handset operations. An early commercial model was built by Livermore Data Systems in 1968. One would dial the computer system (which would have telephone company datasets) on one's phone, and when the connection was established, place the handset into the acoustic modem. 
Since the handsets were all supplied by the telephone company, most had the same shape, simplifying the physical interface. A microphone and a speaker inside the modem box would pick up and transmit the signaling tones, and circuitry would convert those audio shift-key encoded frequency binary signals for an RS232 output socket. With luck one could get 300 baud (~bits/second) transmission rates, but 150 baud was more typical. 
That speed was sufficient for typewriter-based terminals, as the IBM 2741, running at 134.5 baud, or a teleprinter, running at 110 baud.
The practical upper limit for acoustic-coupled modems was 1200-baud, first made available in 1973 by Vadic and 1977 by AT&T. It became widespread in 1985 with advent of the Hayes Smartmodem 1200A. Such devices facilitated the creation of dial-up bulletin board systems, a forerunner of modern internet chat rooms, message boards, and e-mail.
Design.
Usually, a standard telephone handset was placed into a cradle that had been engineered to fit closely (by the use of rubber seals) around the microphone and earpiece of the handset. A modem would modulate a loudspeaker in the cup attached to the handset's microphone, and sound from the loudspeaker in the telephone handset's earpiece would be picked up by a microphone in the cup attached to the earpiece. In this way signals could be passed in both directions.
Acoustic couplers were sensitive to external noise and depended on the widespread standardisation of the dimensions of telephone handsets. Direct electrical connections to telephone networks, once they were made legal, rapidly became the preferred method of attaching modems, and the use of acoustic couplers dwindled. Acoustic couplers are still used by people travelling in areas of the world where electrical connection to the telephone network is illegal or impractical. Many models of TDDs (Telecommunications Device for the Deaf) still have a built-in acoustic coupler, which allow more universal use with pay phones and for 911 calls by deaf people.
An acoustic coupler is prominently shown early in the 1983 film "WarGames", when character David Lightman (depicted by actor Matthew Broderick) places a telephone handset into the cradle of a film prop acoustic modem to accentuate the act of using telephone lines for interconnection to the developing computer networks of the period—in this case, a military command computer.

</doc>
<doc id="40692" url="http://en.wikipedia.org/wiki?curid=40692" title="Active laser medium">
Active laser medium

The active laser medium (also called gain medium or lasing medium) is the source of optical gain within a laser. The gain results from the stimulated emission of electronic or molecular transitions to a lower energy state from a higher energy state
previously populated by a pump source.
Examples of active laser media include:
In order to lase, the active gain medium must be in a nonthermal energy distribution known as a population inversion. The preparation of this state requires an external energy source and is known as laser pumping. Pumping may be achieved with electrical currents (e.g. semiconductors, or gases via high-voltage discharges) or with light, generated by discharge lamps or by other lasers (semiconductor lasers). More exotic gain media can be pumped by chemical reactions, nuclear fission, or with high-energy electron beams.
Example of a model of gain medium.
A universal model valid for all laser types does not exist. 
The simplest model includes two systems of sub-levels: upper and lower. Within each sub-level system, the fast transitions ensure that thermal equilibrium is reached quickly, leading to the Maxwell–Boltzmann statistics of excitations among sub-levels in each system "(fig.1)". The upper level is assumed to be metastable.
Also, gain and refractive index are assumed independent of a particular way of excitation. 
For good performance of the gain medium, the separation between sub-levels should be larger than working temperature; then, at pump frequency formula_1, the absorption dominates.
In the case of amplification of optical signals, the lasing frequency is called "signal frequency." However, the same term is used even in the laser oscillators, when amplified radiation is used to transfer energy rather than information. The model below seems to work well for most optically-pumped solid-state lasers.
Cross-sections.
The simple medium can be characterized with effective cross-sections of absorption and emission at frequencies formula_1 and formula_3.
The relative concentrations can be defined as formula_8 and formula_9.
The rate of transitions of an active center from ground state to the excited state can be expressed with formula_10 and
The rate of transitions back to the ground state can be expressed with formula_11,
where formula_12 and formula_13 are effective cross-sections of absorption at the frequencies of the signal and the pump. 
formula_14 and formula_15 are the same for stimulated emission;
formula_16 is rate of the spontaneous decay of the upper level.
Then, the kinetic equation for relative populations can be written as follows: 
formula_17
The dynamic saturation intensities can be defined:
formula_18,
formula_19.
The absorption at strong signal:
formula_20.
The gain at strong pump:
formula_21,
where formula_22
is determinant of cross-section. 
Gain never exceeds value formula_23, and absorption never exceeds value formula_24.
At given intensities formula_25, formula_26 of pump and signal, the gain and absorption
can be expressed as follows:
formula_27,
formula_28,
where 
formula_29, 
formula_30, 
formula_31, 
formula_32 .
Identities.
The following identities take place:
formula_33, formula_34
The state of gain medium can be characterized with a single parameter, such as population of the upper level, gain or absorption.
Efficiency of the gain medium.
The efficiency of a gain medium can be defined as
formula_35.
Within the same model, the efficiency can be expressed as follows:
formula_36.
For the efficient operation both intensities, pump and signal should exceed their saturation intensities;
formula_37, and formula_38.
The estimates above are valid for a medium uniformly filled with pump and signal light. The spatial hole burning may slightly reduce the efficiency because some regions are pumped well, but the pump is not efficiently withdrawn by the signal in the nodes of
the interference of counter-propagating waves.

</doc>
<doc id="40693" url="http://en.wikipedia.org/wiki?curid=40693" title="Adaptive communications">
Adaptive communications

Adaptive communications can mean any communications system, or portion thereof, that automatically uses feedback information obtained from the system itself or from the signals carried by the system to modify dynamically one or more of the system operational parameters to improve system performance or to resist degradation. 
The modification of a system parameter may be discrete, as in hard-switched diversity reception, or may be continuous, as in a 
predetection combining algorithm.

</doc>
<doc id="40694" url="http://en.wikipedia.org/wiki?curid=40694" title="Adaptive predictive coding">
Adaptive predictive coding

Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
APC is related to linear predictive coding (LPC) in that both use adaptive predictors. However, APC uses fewer prediction coefficients, thus requiring a higher sampling rate than LPC.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40695" url="http://en.wikipedia.org/wiki?curid=40695" title="Adder–subtractor">
Adder–subtractor

In digital circuits, an adder–subtractor is a circuit that is capable of adding or subtracting numbers (in particular, binary).
Below is a circuit that does adding "or" subtracting" depending on a control signal.
It is also possible to construct a circuit that performs both addition and subtraction at the same time.
Construction.
Having an "n"-bit adder for formula_1 and formula_2, then formula_3.
Then, assume the numbers are in two's complement.
Then to perform formula_4, two's complement theory says to invert each bit with a NOT gate then add one.
This yields formula_5, which is easy to do with a slightly modified adder.
By preceding each formula_1 input bit on the adder with a 2-to-1 multiplexer where:
that has control input formula_11 and the initial carry connect is also connected to formula_11 then:
This works because when formula_14 the formula_1 input to the adder is really formula_17 and the carry in is formula_18. Adding formula_2 to formula_17 and formula_18 yields the desired subtraction of formula_22.
A way you can mark number formula_1 as positive or negative without using a multiplexer on each bit is to use a XOR (Exclusive OR) gate to precede each bit instead. 
This produces the same Truth table for the bit arriving at the adder as the multiplexer solution does.
As when formula_13 the XOR Gate output will be what the input bit is set to.
and when formula_14 it will effectively invert the input bit
Role in the arithmetic logic unit.
Adders are a part of the core of an arithmetic logic unit (ALU).
The control unit decides which operations an ALU should perform (based on the op code being executed) and sets the ALU operation.
The formula_11 input to the adder–subtractor above would be one such control line from the control unit.
The adder–subtractor above could easily be extended to include more functions.
For example, a 2-to-1 multiplexer could be introduced on each formula_28 that would switch between zero and formula_28; this could be used (in conjunction with formula_14) to yield the two's complement of formula_1 since formula_32.
A further step would be to change the 2-to-1 mux on formula_1 to a 4-to-1 with the third input being zero, then replicating this on formula_28 thus yielding the following output functions:
By adding more logic in front of the adder, a single adder can be converted into much more than just an adder—an ALU.

</doc>
<doc id="40696" url="http://en.wikipedia.org/wiki?curid=40696" title="Address">
Address

Address or The Address may refer to:

</doc>
<doc id="40699" url="http://en.wikipedia.org/wiki?curid=40699" title="Adjacent-channel interference">
Adjacent-channel interference

Adjacent-channel interference (ACI) is interference caused by extraneous power from a signal in an adjacent channel. ACI may be caused by inadequate filtering (such as incomplete filtering of unwanted modulation products in FM systems), improper tuning or poor frequency control (in the reference channel, the interfering channel or both). 
ACI is distinguished from crosstalk.
Origin.
The adjacent-channel interference which receiver A experiences from a transmitter B is the sum of the power that B emits into A's channel—known as the "unwanted emission", and represented by the ACLR (Adjacent Channel Leakage Ratio)—and the power that A picks up from B's channel, which is represented by the ACS (Adjacent Channel Selectivity). B emitting power into A's channel is called adjacent-channel leakage (unwanted emissions). It occurs for two reasons. First, because RF filters require a roll-off, and do not eliminate a signal completely. Second, due to intermodulation in B's amplifiers, which cause the transmitted spectrum to spread beyond what was intended. Therefore, B emits some power in the adjacent channel which is picked up by A. A receives some emissions from B's channel due to the roll off of A's selectivity filters. Selectivity filters are designed to "select" a channel. Similarly, B's signal suffers intermodulation distortion passing through A's RF input amplifiers, leaking more power into adjacent frequencies.
Avoidance procedure.
Broadcast regulators frequently manage the broadcast spectrum in order to minimize adjacent-channel interference. For example, in North America, FM radio stations in a single region cannot be licensed on adjacent frequencies — that is, if a station is licensed on 99.5 MHz in a city, the first-adjacent frequencies of 99.3 MHz and 99.7 MHz cannot be used anywhere within a certain distance of that station's transmitter, and the second-adjacent frequencies of 99.1 MHz and 99.9 MHz are restricted to specialized usages such as low-power stations. Similar restrictions formerly applied to third-adjacent frequencies as well (i.e. 98.9 MHz and 100.1 MHz in the example above), but these are no longer observed. 

</doc>
<doc id="40700" url="http://en.wikipedia.org/wiki?curid=40700" title="Advanced Data Communication Control Procedures">
Advanced Data Communication Control Procedures

In telecommunication, Advanced Data Communication Control Procedures (or Protocol) (ADCCP) is a bit-oriented data link layer protocol used to provide point-to-point and point-to-multipoint transmission of data frames that contain error control information. It places data on a network and ensures proper delivery to a destination. ADCCP is based on the IBM's SDLC protocol. The HDLC by ISO and LAPB by ITU/CCITT are based on ADCCP.
ADCCP is an ANSI standard, X3.66, derived from IBM's Synchronous Data Link Control (SDLC) protocol, and is functionally equivalent to the ISO High-Level Data Link Control (HDLC) standard.
ADCCP has 3 main modes – NRM (Normal Response mode akin to SDLC), ABM (Asynchronous Balanced mode - akin to HDLC) and ARM (Asynchronous Response mode)
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40701" url="http://en.wikipedia.org/wiki?curid=40701" title="Aerial insert">
Aerial insert

In telecommunications an aerial insert is a segment of cabling that rises from ground to a point above ground, followed by an overhead run, e.g. on poles, followed by a drop back into the ground. An aerial insert is used in places where it is not possible or practical to place a cable underground. Ariel inserts might be encountered in crossing deep ditches, canals, rivers, or subway lines.

</doc>
<doc id="40702" url="http://en.wikipedia.org/wiki?curid=40702" title="Aeronautical Emergency Communications System Plan">
Aeronautical Emergency Communications System Plan

In telecommunication, the Aeronautical Emergency Communications System Plan (AECS) provides for the operation of aeronautical communications stations, on a voluntary, organized basis, to provide the President and the Federal Government, as well as heads of state and local governments, or their designated representatives, and the aeronautical industry with an expeditious means of communications during an emergency.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40703" url="http://en.wikipedia.org/wiki?curid=40703" title="AIOD leads">
AIOD leads

In land-line telephony, AIOD leads are Terminal equipment leads used solely to transmit automatic identified outward dialing (AIOD) data from a PBX to the public switched telephone network or to switched service networks ("e.g.," EPSCS), so that a vendor can provide a detailed monthly bill identifying long distance calling usage by individual PBX stations, tie trunks, or the attendant. It resembles common channel signalling in that the AIOD leads provide data for all trunks, but is used only for billing, thus resembling automatic number identification.

</doc>
<doc id="40704" url="http://en.wikipedia.org/wiki?curid=40704" title="Airborne radio relay">
Airborne radio relay

Airborne radio relay is a technique employing aircraft fitted with radio relay stations for the purpose of increasing the range, flexibility, or physical security of communications systems.
Use in Vietnam.
One of the first uses of airborne radio relay was by the United States Army's 1st Cavalry Division in the Battle of Ia Drang during the Vietnam War, which employed the technique to improve communications with commanders at headquarters. The action of war had shifted to the borders of Laos and Cambodia, where the hilly terrain made the monetary and human cost of seizing and holding high ground, and airlifting and installing radio relay equipment prohibitive. In 1968, the Department of the Army provided four specially-equipped relay aircraft to the Division, which proved invaluable throughout the country, in particular, during the 1st Cavalry Division's relief of Khe Sanh in 1968.
The use of airborne radio relay was a great success, although two problems arose during the Vietnam War. The first was the limitations of the aircraft used as relays. The 1st Cavalry Division had originally used C-7 Caribous as the relay aircraft, but when these planes were turned over to the Air Force, the equipment was installed in single-engine Otter aircraft, which were too underpowered to carry the heavy equipment required for relay. Eventually, the 1st Signal Brigade was provided with six specially-equipped U-21 aircraft for use in relay operations. The second problem was that of radio frequency interference: the limited frequency spectrum in use for combat radios meant that relay aircraft often interfered with the communication of ground units when their frequencies were overridden by the airborne units. The Army eventually assigned certain frequencies for airborne relay only, although this further limited the frequencies available to ground units.

</doc>
<doc id="40706" url="http://en.wikipedia.org/wiki?curid=40706" title="Alarm sensor">
Alarm sensor

In telecommunication, the term alarm sensor has the following meanings: 
1. In communications systems, a device that can sense an abnormal condition within the system and provide a signal indicating the presence or nature of the abnormality to either a local or remote alarm indicator, and (b) may detect events ranging from a simple contact opening or closure to a time-phased automatic shutdown and restart cycle. 
2. In a physical security system, an approved device used to indicate a change in the physical environment of a facility or a part thereof. 
3. In electronic security systems, a physical device or change/presence of any electronic signal/logic which causes trigger to electronic circuit to perform application specific operation. In electronic alarm systems the use of this trigger event done by such devices is to turn on the alarm or siren producing sound and/or perform a security calling through telephone lines.
"Note:" Alarm sensors may also be redundant or chained, such as when one alarm sensor is used to protect the housing, cabling, or power protected by another alarm sensor.
Source: from Federal Standard 1037C and from MIL-STD-188 and from TRISHAM Software Systems

</doc>
<doc id="40707" url="http://en.wikipedia.org/wiki?curid=40707" title="A-law algorithm">
A-law algorithm

An A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, "i.e.," modify, the dynamic range of an analog signal for digitizing.
It is similar to the μ-law algorithm used in North America and Japan.
For a given input "x", the equation for A-law encoding is as follows,
where "A" is the compression parameter. In Europe, formula_2.
A-law expansion is given by the inverse function,
The reason for this encoding is that the wide dynamic range of speech does not lend itself well to efficient linear digital encoding. A-law encoding effectively reduces the dynamic range of the signal, thereby increasing the coding efficiency and resulting in a signal-to-distortion ratio that is superior to that obtained by linear encoding for a given number of bits.
Comparison to μ-law.
The μ-law algorithm provides a greater larger dynamic range than the A-law at the cost of worse proportional distortion for small signals. By convention, A-law is used for an international connection if at least one country uses it.

</doc>
<doc id="40708" url="http://en.wikipedia.org/wiki?curid=40708" title="Allan variance">
Allan variance

The Allan variance (AVAR), also known as two-sample variance, is a measure of frequency stability in clocks, oscillators and amplifiers. It is named after David W. Allan. It is expressed mathematically as
The Allan deviation (ADEV) is the square root of Allan variance. It is also known as "sigma-tau", and is expressed mathematically as
The "M-sample variance" is a measure of frequency stability using M samples, time T between measures and observation time formula_3. "M"-sample variance is expressed as
The "Allan variance" is intended to estimate stability due to noise processes and not that of systematic errors or imperfections such as frequency drift or temperature effects. The Allan variance and Allan deviation describe frequency stability, i.e. the stability in frequency. See also the section entitled "Interpretation of value" below.
There are also different adaptations or alterations of "Allan variance", notably the modified Allan variance MAVAR or MVAR, the total variance, and the Hadamard variance. There also exist time stability variants such as time deviation TDEV or time variance TVAR. Allan variance and its variants have proven useful outside the scope of timekeeping and are a set of improved statistical tools to use whenever the noise processes are not unconditionally stable, thus a derivative exists.
The general "M"-sample variance remains important since it allows dead time in measurements and bias functions allows conversion into Allan variance values. Nevertheless, for most applications the special case of 2-sample, or "Allan variance" with formula_5 is of greatest interest.
Background.
When investigating the stability of crystal oscillators and atomic clocks it was found that they did not have a phase noise consisting only of white noise, but also of white frequency noise and flicker frequency noise. These noise forms become a challenge for traditional statistical tools such as standard deviation as the estimator will not converge. The noise is thus said to be divergent. Early efforts in analysing the stability included both theoretical analysis and practical measurements.
An important side-consequence of having these types of noise was that, since the various methods of measurements did not agree with each other, the key aspect of repeatability of a measurement could not be achieved. This limits the possibility to compare sources and make meaningful specifications to require from suppliers. Essentially all forms of scientific and commercial uses were then limited to dedicated measurements which hopefully would capture the need for that application.
To address these problems, David Allan introduced the M-sample variance and (indirectly) the two-sample variance. While the two-sample variance did not completely allow all types of noise to be distinguished, it provided a means to meaningfully separate many noise-forms for time-series of phase or frequency measurements between two or more oscillators. Allan provided a method to convert between any M-sample variance to any N-sample variance via the common 2-sample variance, thus making all M-sample variances comparable. The conversion mechanism also proved that M-sample variance does not converge for large M, thus making them less useful. IEEE later identified the 2-sample variance as the preferred measure.
An early concern was related to time and frequency measurement instruments which had a dead time between measurements. Such a series of measurements did not form a continuous observation of the signal and thus introduced a systematic bias into the measurement. Great care was spent in estimating these biases. The introduction of zero dead time counters removed the need, but the bias analysis tools have proved useful.
Another early aspect of concern was related to how the bandwidth of the measurement instrument would influence the measurement, such that it needed to be noted. It was later found that by algorithmically changing the observation formula_3, only low formula_3 values would be affected while higher values would be unaffected. The change of formula_3 is done by letting it be an integer multiple formula_9 of the measurement timebase formula_10.
The physics of crystal oscillators was analyzed by D. B. Leeson and the result is now referred to as Leeson's equation. The feedback in the oscillator will make the white noise and flicker noise of the feedback amplifier and crystal become the power-law noises of formula_12 white frequency noise and formula_13 flicker frequency noise respectively. These noise forms have the effect that the standard variance estimator does not converge when processing time error samples. This mechanics of the feedback oscillators was unknown when the work on oscillator stability started but was presented by Leeson at the same time as the statistical tools was made available by David W. Allan. For a more thorough presentation on the Leeson effect see modern phase noise literature.
Interpretation of value.
Allan variance is defined as one half of the time average of the squares of the differences between successive readings of the frequency deviation sampled over the sampling period. The Allan variance depends on the time period used between samples: therefore it is a function of the sample period, commonly denoted as τ, likewise the distribution being measured, and is displayed as a graph rather than a single number. A low Allan variance is a characteristic of a clock with good stability over the measured period.
Allan deviation is widely used for plots (conveniently in log-log format) and presentation of numbers. It is preferred as it gives the relative amplitude stability, allowing ease of comparison with other sources of errors.
An Allan deviation of 1.3×10−9 at observation time 1 s (i.e. τ = 1 s) should be interpreted as there being an instability in frequency between two observations a second apart with a relative root mean square (RMS) value of 1.3×10−9. For a 10-MHz clock, this would be equivalent to 13 mHz RMS movement. If the phase stability of an oscillator is needed then the time deviation variants should be consulted and used.
One may convert the Allan variance and other time-domain variances into frequency-domain measures of time (phase) and frequency stability. The following link shows these relationships and how to perform these conversions:
http://www.allanstime.com/Publications/DWA/Conversion_from_Allan_variance_to_Spectral_Densities.pdf
Definitions.
M-sample variance.
The formula_14-sample variance is defined using (here in a modernized notation form) as
where formula_16 is the phase angle (in radians) measured at time formula_17, or with average fractional frequency time series
where formula_14 is the number of frequency samples used in variance, formula_20 is the time between each frequency sample and formula_3 is the time-length of each frequency estimate.
An important aspect is that formula_14-sample variance model counter dead-time by letting the time formula_20 be different from that of formula_3.
Allan variance.
The Allan variance is defined as
which is conveniently expressed as
where formula_3 is the observation period, formula_28 is the "n"th fractional frequency average over the observation time formula_3.
The samples are taken with no dead-time between them, which is achieved by letting
Allan deviation.
Just as with standard deviation and variance, the Allan deviation is defined as the square root of the Allan variance.
Supporting definitions.
Oscillator model.
The oscillator being analysed is assumed to follow the basic model of
The oscillator is assumed to have a nominal frequency of formula_33, given in cycles per second (SI unit: hertz). The nominal angular frequency formula_34 (in radians per second) is given by
The total phase can be separated into a perfectly cyclic component formula_36, along with a fluctuating component formula_37:
Time error.
The time error function "x"("t") is the difference between expected nominal time and actual normal time
For measured values a time error series TE("t") is defined from the reference time function "T"REF("t") as
Frequency function.
The frequency function formula_41 is the frequency over time defined as
Fractional frequency.
The fractional frequency "y"("t") is the normalized difference between the frequency formula_41 and the nominal frequency formula_33:
Average fractional frequency.
The average fractional frequency is defined as
where the average is taken over observation time "τ", the "y"("t") is the fractional frequency error at time "t" and "τ" is the observation time.
Since "y"("t") is the derivative of "x"("t"), we can without loss of generality rewrite it as
Estimators.
The definition is based on the statistical expected value, integrating over infinite time. Real world situation does not allow for such time-series, in which case a statistical estimator needs to be used in its place. A number of different estimators will be presented and discussed.
Conventions.
The relation between the number of fractional frequency samples and time error series is fixed in the relationship
where "T" is the time between measurements. For Allan variance, the time being used has "T" set to the observation time "τ".
The time error sample series let "N" denote the number of samples ("x"0 ..."x""N-1") in the series. The traditional convention uses index 1 through "N".
which gives
For the Allan variance assumption of "T" being "τ" it becomes
The average fractional frequency sample series let "M" denote the number of samples (formula_54) in the series. The traditional convention uses index 1 through "M".
As a shorthand is average fractional frequency often written without the average bar over it. This is however formally incorrect as the fractional frequency and average fractional frequency is two different functions. A measurement instrument able to produce frequency estimates with no dead-time will actually deliver a frequency average time series which only needs to be converted into average fractional frequency and may then be used directly.
Fixed τ estimators.
A first simple estimator would be to directly translate the definition into
or for the time series
These formulas however only provide the calculation for the "τ" = "τ"0 case. To calculate for a different value of "τ", a new time-series needs to be provided.
Non-overlapped variable τ estimators.
If taking the time-series and skipping past "n" − 1 samples a new (shorter) time-series would occur with "τ"0 as the time between the adjacent samples, for which the Allan variance could be calculated with the simple estimators. These could be modified to introduce the new variable "n" such that no new time-series would have to be generated, but rather the original time series could be reused for various values of "n". The estimators become
with formula_58,
and for the time series
with formula_60.
These estimators have a significant drawback in that they will drop a significant amount of sample data as only 1/"n" of the available samples is being used.
Overlapped variable τ estimators.
A technique presented by J.J. Snyder provided an improved tool, as measurements was overlapped in "n" overlapped series out of the original series. The overlapping Allan variance estimator was introduced in. This can be shown to be equivalent to averaging the time or normalized frequency samples in blocks of "n" samples prior to processing. The resulting predictors becomes
or for the time series
The overlapping estimators have far superior performance over the non-overlapping estimators as "n" rises and the time-series is of moderate length. The overlapped estimators have been accepted as the preferred Allan variance estimators in IEEE, ITU-T and ETSI standards for comparable measurements such as needed for telecommunication qualification.
Modified Allan variance.
In order to address the inability to separate white phase modulation from flicker phase modulation using traditional Allan variance estimators an algorithmic filtering to reduce the bandwidth by "n". This filtering provides a modification to the definition and estimators and is now identifies as a separate class of variance called modified Allan variance. The modified Allan variance measure is a frequency stability measure, just as the Allan variance.
Time stability estimators.
The Allan variance and Allan deviation provides the frequency stability variance and deviation. The time stability variants can be provided by using frequency to time scaling from the modified (Mod.) Allan variance to time variance
and similarly for Allan deviation to time deviation
Other estimators.
Further developments have produced improved estimation methods for the same stability measure, the variance/deviation of frequency, but these are known by separate names such as the Hadamard variance, modified Hadamard variance, the total variance, modified total variance and the Theo variance. These distinguish themselves in better use of statistics for improved confidence bounds or ability to handle linear frequency drift.
Confidence intervals and equivalent degrees of freedom.
Statistical estimators will calculate an estimated value on the sample series used. The estimates may deviate from the true value and the range of values which for some probability will contain the true value is referred to as the confidence interval. The confidence interval depends on the number of observations in the sample series, the dominant noise type, and the estimator being used. The width is also dependent on the statistical certainty for which the confidence interval values forms a bounded range, thus the statistical certainty that the true value is within that range of values. For variable-τ estimators, the "τ"0 multiple "n" is also a variable.
Confidence interval.
The confidence interval can be established using chi-squared distribution by using the distribution of the sample variance:
where "s""2" is the sample variance of our estimate, "σ"2 is the true variance value, "d.f." is the degrees of freedom for the estimator and "χ"2 is the degrees of freedom for a certain probability. For a 90% probability, covering the range from the 5% to the 95% range on the probability curve, the upper and lower limits can be found using the inequality:
which after rearrangement for the true variance becomes:
Effective degrees of freedom.
The degrees of freedom represents the number of free variables capable of contributing to the estimate. Depending on the estimator and noise type, the effective degrees of freedom varies. Estimator formulas depending on "N" and "n" has been empirically found to be:
Power-law noise.
The Allan variance will treat various power-law noise types differently, conveniently allowing them to be identified and their strength estimated. As a convention, the measurement system width (high corner frequency) is denoted "f""H".
As found in and in modern forms.
The Allan variance is unable to distinguish between WPM and FPM, but is able to resolve the other power-law noise types. In order to distinguish WPM and FPM, the modified Allan variance needs to be employed.
The above formulas assume that
and thus that the bandwidth of the observation time is much lower than the instruments bandwidth. When this condition is not met, all noise forms depend on the instrument's bandwidth.
α-μ mapping.
The detailed mapping of a phase modulation of the form
where
or frequency modulation of the form
into the Allan variance of the form
can be significantly simplified by providing a mapping between α and μ. A mapping between α and "K"α is also presented for convenience:
The mapping is taken from.
General Conversion from Phase Noise.
A signal with spectral phase noise formula_73 with units rad2/Hz can be converted to Allan Variance by:
formula_74
Linear response.
While Allan variance is intended to be used to distinguish noise forms, it will depend on some but not all linear responses to time. They are given in the table:
Thus, linear drift will contribute to output result. When measuring a real system, the linear drift or other drift mechanism may need to be estimated and removed from the time-series prior to calculating the Allan variance.
Time and frequency filter properties.
In analysing the properties of Allan variance and friends, it has proven useful to consider the filter properties on the normalize frequency. Starting with the definition for Allan variance for
where
Replacing the time series of formula_77 with the Fourier transformed variant formula_78 the Allan variance can be expressed in the frequency domain as
Thus the transfer function for Allan variance is
Bias functions.
The "M"-sample variance, and the defined special case Allan variance, will experience systematic bias depending on different number of samples "M" and different relationship between "T" and "τ". In order address these biases the bias-functions "B"1 and "B"2 has been defined and allows for conversion between different "M" and "T" values.
These bias functions are not sufficient for handling the bias resulting from concatenating "M" samples to the "Mτ"0 observation time over the "MT"0 with has the dead-time distributed among the "M" measurement blocks rather than at the end of the measurement. This rendered the need for the "B"3 bias.
The bias functions are evaluated for a particular µ value, so the α-µ mapping needs to be done for the dominant noise form as found using noise identification. Alternatively as proposed in and elaborated in the µ value of the dominant noise form may be inferred from the measurements using the bias functions.
B1 bias function.
The "B"1 bias function relates the "M"-sample variance with the 2-sample variance (Allan variance), keeping the time between measurements "T" and time for each measurements "τ" constant, and is defined as
where
The bias function becomes after analysis
B2 bias function.
The "B"2 bias function relates the 2-sample variance for sample time "T" with the 2-sample variance (Allan variance), keeping the number of samples "N" = 2 and the observation time "τ" constant, and is defined
where
The bias function becomes after analysis
"B"3 bias function.
The "B"3 bias function relates the 2-sample variance for sample time "MT"0 and observation time "Mτ"0 with the 2-sample variance (Allan variance) and is defined as
where
The "B"3 bias function is useful to adjust non-overlapping and overlapping variable "τ" estimator values based on dead-time measurements of observation time "τ"0 and time between observations "T"0 to normal dead-time estimates.
The bias function becomes after analysis (for the "N" = 2 case)
where
τ bias function.
While formally not formulated, it has been indirectly inferred as a consequence of the α-µ mapping. When comparing two Allan variance measure for different τ assuming same dominant noise in the form of same µ coefficient, a bias can be defined as
The bias function becomes after analysis
Conversion between values.
In order to convert from one set of measurements to another the "B"1, "B"2 and τ bias functions can be assembled. First the "B"1 function converts the ("N"1, "T"1, "τ"1) value into (2, "T"1, "τ"1), from which the "B"2 function converts into a (2, "τ"1, "τ"1) value, thus the Allan variance at "τ"1. The Allan variance measure can be converted using the τ bias function from "τ"1 to "τ"2, from which then the (2, "T"2, "τ"2) using "B"2 and then finally using "B"1 into the ("N"2, "T"2, "τ"2) variance. The complete conversion becomes
where
Similarly, for concatenated measurements using M sections, the logical extension becomes
Measurement issues.
When making measurements to calculate Allan variance or Allan deviation a number of issues may cause the measurements to degenerate. Covered here is the effects specific to Allan variance, where results would be biased.
Measurement bandwidth limits.
A measurement system is expected to have a bandwidth at or below that of the Nyquist rate as described within the Shannon–Hartley theorem. As can be seen in the power-law noise formulas, the white and flicker noise modulations both depends on the upper corner frequency formula_98 (these systems is assumed to be low-pass filtered only). Considering the frequency filter property it can be clearly seen that low-frequency noise has greater impact on the result. For relatively flat phase modulation noise types (e.g. WPM and FPM), the filtering has relevance, whereas for noise types with greater slope the upper frequency limit becomes of less importance, assuming that the measurement system bandwidth is wide relative the formula_3 as given by
When this assumption is not met, the effective bandwidth formula_98 needs to be notated alongside the measurement. The interested should consult NBS TN394.
If however one adjust the bandwidth of the estimator by using integer multiples of the sample time formula_102 then the system bandwidth impact can be reduced to insignificant levels. For telecommunication needs, such methods have been required in order to ensure comparability of measurements and allow some freedom for vendors to do different implementations. The ITU-T Rec. G.813 for the TDEV measurement.
It can be recommended that the first formula_10 multiples be ignored such that the majority of the detected noise is well within the passband of the measurement systems bandwidth.
Further developments on the Allan variance was performed to let the hardware bandwidth be reduced by software means. This development of a software bandwidth allowed for addressing the remaining noise and the method is now referred to modified Allan variance. This bandwidth reduction technique should not be confused with the enhanced variant of modified Allan variance which also changes a smoothing filter bandwidth.
Dead time in measurements.
Many measurement instruments of time and frequency have the stages of arming time, time-base time, processing time and may then re-trigger the arming. The arming time is from the time the arming is triggered to when the start event occurs on the start channel. The time-base then ensures that minimum amount of time goes prior to accepting an event on the stop channel as the stop event. The number of events and time elapsed between the start event and stop event is recorded and presented during the processing time. When the processing occurs (also known as the dwell time) the instrument is usually unable to do another measurement. After the processing has occurred, an instrument in continuous mode triggers the arm circuit again. The time between the stop event and the following start event becomes dead time during which the signal is not being observed. Such dead time introduces systematic measurement biases, which needs to be compensated for in order to get proper results. For such measurement systems will the time "T" denote the time between the adjacent start events (and thus measurements) while formula_3 denote the time-base length, i.e. the nominal length between the start and stop event of any measurement.
Dead time effects on measurements have such an impact on the produced result that much study of the field have been done in order to quantify its properties properly. The introduction of zero dead-time counters removed the need for this analysis. A zero dead-time counter has the property that the stop-event of one measurement is also being used as the start-event of the following event. Such counters creates a series of event and time timestamp pairs, one for each channel spaced by the time-base. Such measurements have also proved useful in order forms of time-series analysis.
Measurements being performed with dead time can be corrected using the bias function "B"1, "B"2 and "B"3. Thus, dead time as such is not prohibiting the access to the Allan variance, but it makes it more problematic. The dead time must be known such that the time between samples "T" can be established.
Measurement length and effective use of samples.
Studying the effect on the confidence intervals that the length "N" of the sample series have, and the effect of the variable τ parameter "n" the confidence intervals may become very large since the effective degree of freedom may become small for some combination of "N" and "n" for the dominant noise-form (for that τ).
The effect may be that the estimated value may be much smaller or much greater than the real value, which may lead to false conclusions of the result.
It is recommended that the confidence interval is plotted along with the data, such that the reader of the plot is able to be aware of the statistical uncertainty of the values.
It is recommended that the length of the sample sequence, i.e. the number of samples "N" is kept high to ensure that confidence interval is small over the τ-range of interest.
It is recommended that the τ-range as swept by the "τ"0 multiplier "n" is limited in the upper end relative "N" such that the read of the plot is not being confused by highly unstable estimator values.
It is recommended that estimators providing better degrees of freedom values be used in replacement of the Allan variance estimators or as complementing them where they outperform the Allan variance estimators. Among those the Total variance and Theo variance estimators should be considered.
Dominant noise type.
A large number of conversion constants, bias corrections and confidence intervals depends on the dominant noise type. For proper interpretation shall the dominant noise type for the particular τ of interest be identified through noise identification. Failing to identify the dominant noise type will produce biased values. Some of these biases may be of several order of magnitude, so it may be of large significance.
Linear drift.
Systematic effects on the signal is only partly cancelled. Phase and frequency offset is cancelled, but linear drift or other high degree forms of polynomial phase curves will not be cancelled and thus form a measurement limitation. Curve fitting and removal of systematic offset could be employed. Often removal of linear drift can be sufficient. Use of linear drift estimators such as the Hadamard variance could also be employed. A linear drift removal could be employed using a moment based estimator.
Measurement instrument estimator bias.
Traditional instruments provided only the measurement of single events or event pairs. The introduction of the improved statistical tool of overlapping measurements by J.J. Snyder allowed for much improved resolution in frequency readouts, breaking the traditional digits/time-base balance. While such methods is useful for their intended purpose, using such smoothed measurements for Allan variance calculations would give a false impression of high resolution, but for longer τ the effect is gradually removed and the lower τ region of the measurement has biased values. This bias is providing lower values than it should, so it is an overoptimistic (assuming that low numbers is what one wishes) bias reducing the usability of the measurement rather than improving it. Such smart algorithms can usually be disabled or otherwise circumvented by using time-stamp mode which is much preferred if available.
Practical measurements.
While several approaches to measurement of Allan variance can be devised, a simple example may illustrate how measurements can be performed.
Measurement.
All measurements of Allan variance will in effect be the comparison of two different clocks. Lets consider a reference clock and a device under test (DUT), and both having a common nominal frequency of 10 MHz. A time-interval counter is being used to measure the time between the rising edge of the reference (channel A) and the rising edge of the device under test.
In order to provide evenly spaced measurements will the reference clock be divided down to form the measurement rate, triggering the time-interval counter (ARM input). This rate can be 1 Hz (using the 1 PPS output of a reference clock) but other rates like 10 Hz and 100 Hz can also be used. The speed of which the time-interval counter can complete the measurement, output the result and prepare itself for the next arm will limit the trigger frequency.
A computer is then useful to record the series of time-differences being observed.
Post-processing.
The recorded time-series require post-processing to unwrap the wrapped phase, such that a continuous phase error is being provided. If necessary should also logging and measurement mistakes be fixed. Drift estimation and drift removal should be performed, the drift mechanism needs to be identified and understood for the sources. Drift limitations in measurements can be severe, so letting the oscillators become stabilized by long enough time being powered on is necessary.
The Allan variance can then be calculated using the estimators given, and for practical purposes the overlapping estimator should be used due to its superior use of data over the non-overlapping estimator. Other estimators such as Total or Theo variance estimators could also be used if bias corrections is applied such that they provide Allan variance compatible results.
To form the classical plots, the Allan deviation (square root of Allan variance) is plotted in log-log format against the observation interval tau.
Equipment and software.
The time-interval counter is typically an off the shelf counter commercially available. Limiting factors involve single-shot resolution, trigger jitter, speed of measurements and stability of reference clock. The computer collection and post-processing can be done using existing commercial or public domain software. Highly advanced solutions exists which will provide measurement and computation in one box.
Research history.
The field of frequency stability has been studied for a long time, however it was found during the 1960s that there was a lack of coherent definitions. The NASA-IEEE Symposium on Short-Term Stability in 1964 was followed with the IEEE Proceedings publishing a special issue on Frequency Stability in its February 1966 issue.
The NASA-IEEE Symposium on Short-Term Stability in November 1964 brought together many fields and uses of short and long term stability with papers from many different contributors. The articles and panel discussions is interesting in that they concur on the existence of the frequency flicker noise and the wish to achieve a common definition for short and long term stability (even if the conference name only reflect the short-term stability intention).
The IEEE proceedings on Frequency Stability 1966 included a number of important papers including those of David Allan, James A. Barnes, L. S. Cutler and C. L. Searle and D. B. Leeson. These papers helped shape the field.
The classical "M"-sample variance of frequency was analysed by David Allan in along with an initial bias function. This paper tackles the issues of dead-time between measurements and analyses the case of M frequency samples (called "N" in the paper) and variance estimators. It provides the now standard "α" to "µ" mapping. It clearly builds on James Barnes work as detailed in his article in the same issue. The initial bias functions introduced assumes no dead-time, but the formulas presented includes dead-time calculations. The bias function assumes the use of the 2-sample variance as a base-case, since any other variants of "M" may be chosen and values may be transferred via the 2-sample variance to any other variance for of arbitrary "M". Thus, the 2-sample variance was only implicitly used and not clearly stated as the preference even if the tools where provided. It however laid the foundation for using the 2-sample variance as the base case of comparison among other variants of the "M"-sample variance. The 2-sample variance case is a special case of the "M"-sample variance which produces an average of the frequency derivative.
The work on bias functions was significantly extended by James Barnes in in which the modern B1 and B2 bias functions was introduced. Curiously enough, it refers to the "M"-sample variance as "Allan variance" while referencing to Allan's paper "Statistics of Atomic Frequency Standards". With these modern bias functions, full conversion among "M"-sample variance measures of variating "M", "T" and τ values could used, by conversion through the 2-sample variance.
James Barnes and David Allan further extended the bias functions with the B3 function in to handle the concatenated samples estimator bias. This was necessary to handle the new use of concatenated sample observations with dead time in between.
The IEEE Technical Committee on Frequency and Time within the IEEE Group on Instrumentation & Measurements provided a summary of the field in 1970 published as NBS Technical Notice 394. This paper could be considered first in a line of more educational and practical papers aiding the fellow engineers in grasping the field. In this paper the 2-sample variance with "T" = "τ" is being the recommended measurement and it is referred to as Allan variance (now without the quotes). The choice of such parametrisation allows good handling of some noise forms and to get comparable measurements, it is essentially the least common denominator with the aid of the bias functions B1 and B2.
An improved method for using sample statistics for frequency counters in frequency estimation or variance estimation was proposed by J.J. Snyder. The trick to get more effective degrees of freedom out of the available dataset was to use overlapping observation periods. This provides a square-root "n" improvement. It was included into the overlapping Allan variance estimator introduced in. The variable τ software processing was also included in. This development improved the classical Allan variance estimators likewise providing a direct inspiration going into the work on modified Allan variance.
The confidence interval and degrees of freedom analysis, along with the established estimators was presented in.
Educational and practical resources.
The field of time and frequency and its use of Allan variance, Allan deviation and friends is a field involving many aspects, for which both understanding of concepts and practical measurements and post-processing requires care and understanding. Thus, there is a realm of educational material stretching some 40 years available. Since these reflect the developments in the research of their time, they focus on teaching different aspect over time, in which case a survey of available resources may be a suitable way of finding the right resource.
The first meaningful summary is the NBS Technical Note 394 "Characterization of Frequency Stability". This is the product of the Technical Committee on Frequency and Time of the IEEE Group on Instrumentation & Measurement. It gives the first overview of the field, stating the problems, defining the basic supporting definitions and getting into Allan variance, the bias functions "B"1 and "B"2, the conversion of time-domain measures. This is useful as it is among the first references to tabulate the Allan variance for the five basic noise types.
A classical reference is the NBS Monograph 140 from 1974, which in chapter 8 has "Statistics of Time and Frequency Data Analysis". This is the extended variant of NBS Technical Note 394 and adds essentially in measurement techniques and practical processing of values.
An important addition will be the "Properties of signal sources and measurement methods". It covers the effective use of data, confidence intervals, effective degree of freedom likewise introducing the overlapping Allan variance estimator. It is a highly recommended reading for those topics.
The IEEE standard 1139 "Standard definitions of Physical Quantities for Fundamental Frequency and Time Metrology" is beyond that of a standard a comprehensive reference and educational resource.
A modern book aimed towards telecommunication is Stefano Bregni "Synchronisation of Digital Telecommunication Networks". This summarises not only the field but also much of his research in the field up to that point. It aims to include both classical measures likewise telecommunication specific measures such as MTIE. It is a handy companion when looking at telecommunication standard related measurements.
The NIST Special Publication 1065 "Handbook of Frequency Stability Analysis" of W.J. Riley is a recommended reading for anyone wanting to pursue the field. It is rich of references and also covers a wide range of measures, biases and related functions that a modern analyst should have available. Further it describes the overall processing needed for a modern tool.
Uses.
Allan variance is used as a measure of frequency stability in a variety of precision oscillators, such as crystal oscillators, atomic clocks and frequency-stabilized lasers over a period of a second or more. Short term stability (under a second) is typically expressed as phase noise. The Allan variance is also used to characterize the bias stability of gyroscopes, including fiber optic gyroscopes and MEMS gyroscopes.
50th Anniversary.
In 2016, IEEE-UFFC is going to be publishing a "Special Issue to celebrate the 50th anniversary of the Allan Variance (1966-2016)". A guest editor for that issue will be David's former colleague at NIST, Judah Lavine, who is the most recent recipient of the I. I. Rabi Award.

</doc>
<doc id="40711" url="http://en.wikipedia.org/wiki?curid=40711" title="Alternate party">
Alternate party

Alternate party diversion is an optional feature of telephone services, whereby a call may be routed to a different number based on time-out and precedence schemes set up by the customer.
Technical definition.
Alternate party: In multilevel precedence and preemption, the call receiver, i.e. the destination user, to which a precedence call will be diverted. Diversion will occur when the response timer expires, when the call receiver is busy on a call of equal or higher precedence, or when the call receiver is busy with access resources that are non-preemptable. Alternate party diversion is an optional terminating feature that is subscribed to by the call receiver. Thus, the alternate party is specified by the call receiver at the time of subscription. 
Source: From Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40712" url="http://en.wikipedia.org/wiki?curid=40712" title="Ambient noise level">
Ambient noise level

In atmospheric sounding and noise pollution, ambient noise level (sometimes called background noise level, reference sound level, or room noise level) is the background sound pressure level at a given location, normally specified as a reference level to study a new intrusive sound source.
Ambient sound levels are often measured in order to map sound conditions over a spatial regime to understand their variation with locale. In this case the product of the investigation is a sound level contour map. Alternatively ambient noise levels may be measured to provide a reference point for analyzing an intrusive sound to a given environment. For example, sometimes aircraft noise is studied by measuring ambient sound without presence of any overflights, and then studying the noise addition by measurement or computer simulation of overflight events. Or roadway noise is measured as ambient sound, prior to introducing a hypothetical noise barrier intended to reduce that ambient noise level. 
Ambient noise level is measured with a sound level meter. It is usually measured in dB relative to a reference pressure of 0.00002 Pa, "i.e.," 20 μPa (micropascals) in SI units. A pascal is a newton per square meter. The centimeter-gram-second system of units, the reference sound pressure for measuring ambient noise level is 0.0002 dyn/cm2. Most frequently ambient noise levels are measured using a frequency weighting filter, the most common being the A-weighting scale, such that resulting measurements are denoted dB(A), or decibels on the A-weighting scale.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40713" url="http://en.wikipedia.org/wiki?curid=40713" title="Amplitude distortion">
Amplitude distortion

Amplitude distortion is distortion occurring in a system, subsystem, or device when the output amplitude is not a linear function of the input amplitude under specified conditions. 
Generally, output is a linear function of input only for a fixed portion of the transfer characteristics. In this region, Ic=βIb where Ic is collector current and Ib is base current, following linear relation y=mx.
When output is not in this portion, two forms of amplitude distortion might arise
Due to the additional outputs, this form of distortion is definitely unwanted in audio, radio and telecommunication amplifiers, and it occurs for more than two waves as well.
In a narrowband system such as a radio communication system, unwanted outputs such as X-Y and 2X+Y will be remote from the wanted band and so be ignored by the system. In contrast, 2X-Y and 2Y-X will be close to the wanted signals. These so-called third order distortion products (third order as m+n = 3) tend to dominante the non-linear distortion of narrowband systems.
Amplitude distortion is measured with the system operating under steady-state conditions with a sinusoidal input signal. When other frequencies are present, the term "amplitude" refers to that of the fundamental only.

</doc>
<doc id="40716" url="http://en.wikipedia.org/wiki?curid=40716" title="Angular misalignment loss">
Angular misalignment loss

In waveguide design and construction, angular misalignment loss is power loss caused by the deviation from optimum angular alignment of the axes of source-to-waveguide, waveguide-to-waveguide, or waveguide-to-detector. The waveguide may be dielectric (an optical fiber) or metallic. Angular misalignment loss does not include lateral offset loss and longitudinal offset loss.
Source: from Federal Standard 1037C

</doc>
<doc id="40717" url="http://en.wikipedia.org/wiki?curid=40717" title="Antenna blind cone">
Antenna blind cone

In telecommunications, antenna blind cone (sometimes called a cone of silence or antenna blind spot) is the volume of space, usually approximately conical with its vertex at the antenna, that cannot be scanned by an antenna because of limitations of the antenna radiation pattern and mount.
"Note:" An example of an antenna blind cone is that of an Air Route Surveillance Radar (ARSR). The horizontal radiation pattern of an ARSR antenna is very narrow. The vertical radiation pattern is fan-shaped, reaching approximately 70° of elevation above the horizontal plane. As the fan antenna is rotated about a vertical axis, it can illuminate targets only if they are 70° or less from the horizontal plane. Above that elevation, they are in the antenna blind cone. 
Further reading.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40719" url="http://en.wikipedia.org/wiki?curid=40719" title="Antenna height above average terrain">
Antenna height above average terrain

In United States telecommunication terminology, antenna height above average terrain is the antenna height above the average terrain elevations from 2 to from the antenna for the eight directions spaced evenly for each 45° of azimuth starting with true north.
In general, a different antenna height above average terrain will be determined in each direction from the antenna. The average of these eight heights is the antenna height above average terrain. In some cases, such as seashore, fewer than eight directions may be used.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40720" url="http://en.wikipedia.org/wiki?curid=40720" title="Antenna noise temperature">
Antenna noise temperature

In telecommunication, antenna noise temperature is the temperature of a hypothetical resistor at the input of an ideal noise-free receiver that would generate the same output noise power per unit bandwidth as that at the antenna output at a specified frequency. In other words, antenna noise temperature is a parameter that describes how much noise an antenna produces in a given environment. This temperature is not the physical temperature of the antenna. Moreover, an antenna does not have an intrinsic "antenna temperature" associated with it; rather the temperature depends on its gain pattern and the thermal environment that it is placed in.
Antenna noise temperature has contributions from several sources:
Galactic noise is high below 1000 MHz. At around 150 MHz, it is approximately 1000K. At 2500 MHz, it has leveled off to around 10K.
Earth has an accepted standard temperature of 290K.
The level of the sun's contribution depends on the solar flux. It is given by
The antenna noise temperature depends on antenna coupling to all noise sources in its environment as well as on noise generated within the antenna. That is, in a directional antenna, the portion of the noise source that the antenna's main and side lobes intersect contribute proportionally.
For example, a satellite antenna may not receive noise contribution from the earth in its main lobe, but sidelobes will contribute a portion of the 290K earth noise to its overall noise temperature.

</doc>
<doc id="40721" url="http://en.wikipedia.org/wiki?curid=40721" title="Aperture-to-medium coupling loss">
Aperture-to-medium coupling loss

In telecommunication, aperture-to-medium coupling loss is the difference between the theoretical antenna gain of a very large antenna, such as the antennas in beyond-the-horizon microwave links, and the gain that can be realized in practice. 
"Note 1:" Aperture-to-medium coupling loss is related to the ratio of the scatter angle to the antenna beamwidth. 
"Note 2:" The "very large antennas" are referred to in wavelengths; thus, this loss can apply to line-of-sight systems also. 

</doc>
<doc id="40723" url="http://en.wikipedia.org/wiki?curid=40723" title="Area broadcast shift">
Area broadcast shift

In telecommunication, an area broadcast shift or radio watch shift is the changing from listening to radio transmissions intended for one broadcast area to listening to transmissions intended for another broadcast area.
An area broadcast shift may occur when a ship or aircraft crosses the boundary between listening areas.
Shift times, on the date a ship or aircraft is expected to pass into another area, must be strictly observed or the ship or aircraft will miss messages intended for it.
Source: from Federal Standard 1037C

</doc>
<doc id="40724" url="http://en.wikipedia.org/wiki?curid=40724" title="Arithmetic overflow">
Arithmetic overflow

The term arithmetic overflow or simply overflow has the following meanings. 
Most computers distinguish between two kinds of overflow conditions. A carry occurs when the result of an addition or subtraction, considering the operands and result as unsigned numbers, does not fit in the result. Therefore, it is useful to check the carry flag after adding or subtracting numbers that are interpreted as unsigned values. An "overflow" proper occurs when the result does not have the sign that one would predict from the signs of the operands (e.g. a negative result when adding two positive numbers). Therefore, it is useful to check the overflow flag after adding or subtracting numbers that are represented in two's complement form (i.e. they are considered signed numbers).
There are several methods of handling overflow:
Division by zero is "not" a form of arithmetic overflow. Mathematically, division by zero within reals is explicitly undefined.
Overflow bugs.
Unanticipated arithmetic overflow is a fairly common cause of program errors. Such overflow bugs may be hard to discover and diagnose because they may manifest themselves only for very large input data sets, which are less likely to be used in validation tests.
For example, an unhandled arithmetic overflow in the engine steering software was the primary cause of the crash of the 1996 maiden flight of the Ariane 5 rocket. The software had been considered bug-free since it had been used in many previous flights, but those used smaller rockets which generated lower acceleration than Ariane 5.

</doc>
<doc id="40725" url="http://en.wikipedia.org/wiki?curid=40725" title="Arithmetic shift">
Arithmetic shift

In computer programming, an arithmetic shift is a shift operator, sometimes known as a signed shift (though it is not restricted to signed operands). The two basic types are the arithmetic left shift and the arithmetic right shift. For binary numbers it is a bitwise operation that shifts all of the bits of its operand; every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled in. Instead of being filled with all 0s, as in logical shift, when shifting to the right, the leftmost bit (usually the sign bit in signed integer representations) is replicated to fill in all the vacant positions (this is a kind of sign extension).
Some authors prefer the terms "sticky right-shift" and "zero-fill right-shift".
Arithmetic shifts can be useful as efficient ways of performing multiplication or division of signed integers by powers of two. Shifting left by "n" bits on a signed or unsigned binary number has the effect of multiplying it by 2"n". Shifting right by "n" bits on a two's complement "signed" binary number has the effect of dividing it by 2"n", but it always rounds down (towards negative infinity). This is different from the way rounding is usually done in signed integer division (which rounds towards 0). This discrepancy has led to bugs in more than one compiler.
For example, in the x86 instruction set, the SAR instruction (arithmetic right shift) divides a signed number by a power of two, rounding towards negative infinity. However, the IDIV instruction (signed divide) divides a signed number, rounding towards zero. So a SAR instruction cannot be substituted for an IDIV by power of two instruction nor vice versa.
Formal definition.
The formal definition of an arithmetic shift, from Federal Standard 1037C is that it is:
An important word in the FS 1073C definition is "usually".
Equivalence of arithmetic left shift and multiplication.
Arithmetic "left" shifts are equivalent to multiplication by a (positive, integral) power of the radix (e.g. a multiplication by a power of 2 for binary numbers). Arithmetic left shifts are, with two exceptions, identical in effect to logical left shifts. The first exception is the minor trap that arithmetic shifts may trigger arithmetic overflow whereas logical shifts do not. Obviously that exception only hits in real world use cases if a trigger signal for such an overflow is needed by the design it is used for. The second exception is the MSB is preserved. Processors typically do not offer logical and arithmetic left shift operations with a significant difference, if any.
Non-equivalence of arithmetic right shift and division.
However, arithmetic "right" shifts are major traps for the unwary, specifically in the treatment of rounding of negative integers. For example, in the usual two's complement representation of negative integers, −1 is represented as all 1's; for an 8-bit signed integer this is 1111 1111. An arithmetic right-shift by 1 (or 2, 3, …, 7) yields 1111 1111 again, which is still −1. This corresponds to rounding down (towards negative infinity), but is not the usual convention for division.
It is frequently stated that arithmetic right shifts are equivalent to division by a (positive, integral) power of the radix (e.g. a division by a power of 2 for binary numbers), and hence that division by a power of the radix can be optimized by implementing it as an arithmetic right shift. (A shifter is much simpler than a divider. On most processors, shift instructions will execute more quickly than division instructions.) Guy L. Steele quotes a large number of 1960s and 1970s programming handbooks, manuals, and other specifications from companies and institutions such as DEC, IBM, Data General, and ANSI that make such statements. However, as Steele points out, they are all wrong.
Logical right shifts are equivalent to division by a power of the radix (usually 2) only for positive or unsigned numbers. Arithmetic right shifts are equivalent to logical right shifts for positive signed numbers. Arithmetic right shifts for negative numbers in N−1's complement (usually two's complement) is roughly equivalent to division by a power of the radix (usually 2), where for odd numbers rounding downwards is applied (not towards 0 as usually expected).
Arithmetic right shifts for negative numbers would be equivalent to division using rounding towards 0 in one's complement representation of signed numbers as was used by some historic computers, but this is no longer in general use.
Handling the issue in programming languages.
The (1999) ISO standard for the C programming language defines the C language's right shift operator in terms of divisions by powers of 2. Because of the aforementioned non-equivalence, the standard explicitly excludes from that definition the right shifts of signed numbers that have negative values. It doesn't specify the behaviour of the right shift operator in such circumstances, but instead requires each individual C compiler to specify the behaviour of shifting negative values right.
Applications.
In applications where consistent rounding down is desired, arithmetic right shifts for signed values are useful. An example is in downscaling raster coordinates by a power of two, which maintains even spacing. For example, right shift by 1 sends 0, 1, 2, 3, 4, 5, … to 0, 0, 1, 1, 2, 2, …, and −1, −2, −3, −4, … to −1, −1, −2, −2, …, maintaining even spacing as −2, −2, −1, −1, 0, 0, 1, 1, 2, 2, … By contrast, integer division with rounding towards zero sends −1, 0, and 1 all to 0 (3 points instead of 2), yielding −2, −1, −1, 0, 0, 0, 1, 1, 2, 2, … instead, which is irregular at 0.
References.
Sources used.
 This article incorporates public domain material from websites or documents of the .
</dl>

</doc>
<doc id="40726" url="http://en.wikipedia.org/wiki?curid=40726" title="Automatic repeat request">
Automatic repeat request

Automatic Repeat reQuest (ARQ), also known as Automatic Repeat Query, is an error-control method for data transmission that uses acknowledgements (messages sent by the receiver indicating that it has correctly received a data frame or packet) and timeouts (specified periods of time allowed to elapse before an acknowledgment is to be received) to achieve reliable data transmission over an unreliable service. If the sender does not receive an acknowledgment before the timeout, it usually re-transmits the frame/packet until the sender receives an acknowledgment or exceeds a predefined number of re-transmissions.
The types of ARQ protocols include
All three protocols usually use some form of sliding window protocol
to tell the transmitter to determine which (if any) packets need to be retransmitted.
These protocols reside in the Data Link or Transport Layers of the OSI model.
A number of patents exist for the use of ARQ in live video contribution environments. In these high throughput environments negative acknowledgements are used to drive down overheads.
Examples.
The Transmission Control Protocol uses a variant of Go-Back-N ARQ to ensure reliable transmission of data over the Internet Protocol, which does not provide guaranteed delivery of packets; with Selective Acknowledgement (SACK), it uses Selective Repeat ARQ.
The ITU-T G.hn standard, which provides a way to create a high-speed (up to 1 Gbit/s) local area network using existing residential wiring (power lines, telephone lines, and coaxial cables), uses Selective Repeat ARQ to ensure reliable transmission over noisy media.
ARQ systems were widely used on shortwave radio to ensure reliable delivery of data such as for telegrams. These systems came in forms called ARQ-E and ARQ-M, which also included the ability to multiplex two or four channels.

</doc>
<doc id="40727" url="http://en.wikipedia.org/wiki?curid=40727" title="Articulation score">
Articulation score

In telecommunication, an articulation score (AS) is a subjective measure of the intelligibility of a voice system in terms of the percentage of words correctly understood over a channel perturbed by interference.
Articulation scores have been experimentally obtained as functions of varying word content, bandwidth, audio signal-to-noise ratio and the experience of the talkers and listeners involved.

</doc>
<doc id="40728" url="http://en.wikipedia.org/wiki?curid=40728" title="Artificial transmission line">
Artificial transmission line

In telecommunication, an artificial transmission line (art line) is a four-terminal electrical network that has the characteristic impedance, transmission time delay, phase shift, or other parameter(s) of a real transmission line. It can be used to simulate a real transmission line in one or more of these respects.
Source: from Federal Standard 1037C

</doc>
<doc id="40731" url="http://en.wikipedia.org/wiki?curid=40731" title="Asynchronous operation">
Asynchronous operation

In telecommunications, asynchronous operation or asynchronous working is where a sequence of operations is executed such that the operations are executed out of time coincidence with any event. It can also be an operation that occurs without a regular or predictable time relationship to a specified event; e.g., the calling of an error diagnostic routine that may receive control at any time during the execution of a computer program.
Sources.
from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40732" url="http://en.wikipedia.org/wiki?curid=40732" title="Atmospheric duct">
Atmospheric duct

In telecommunication, an atmospheric duct is a horizontal layer in the lower atmosphere in which the vertical refractive index gradients are such that radio signals (and light rays) are guided or ducted, tend to follow the curvature of the Earth, and experience less attenuation in the ducts than they would if the ducts were not present. The duct acts as an atmospheric dielectric waveguide and limits the spread of the wavefront to only the horizontal dimension.
Atmospheric ducting is a mode of propagation of electromagnetic radiation, usually in the lower layers of Earth’s atmosphere, where the waves are bent by atmospheric refraction. In over-the-horizon radar, ducting causes part of the radiated and target-reflection energy of a radar system to be guided over distances far greater than the normal radar range. It also causes long distance propagation of radio signals in bands that would normally be limited to line of sight.
Normally radio "ground waves" propagate along the surface as creeping waves. That is, they are only diffracted around the curvature of the earth. This is one reason that early long distance radio communication used long wavelengths. The best known exception is that HF (3–30 MHz.) waves are reflected by the ionosphere.
The reduced refractive index due to lower densities at the higher altitudes in the Earth's atmosphere bends the signals back toward the Earth. Signals in a higher refractive index layer, "i.e.," duct, tend to remain in that layer because of the reflection and refraction encountered at the boundary with a lower refractive index material. In some weather conditions, such as inversion layers, density changes so rapidly that waves are guided around the curvature of the earth at constant altitude. 
Phenomena of atmospheric optics related to atmospheric ducting include the green flash, Fata Morgana, superior mirage, mock mirage of astronomical objects and the Novaya Zemlya effect.

</doc>
<doc id="40733" url="http://en.wikipedia.org/wiki?curid=40733" title="Attack time">
Attack time

In telecommunication, attack time is the time between the instant that a signal at the input of a device or circuit exceeds the activation threshold of the device or circuit and the instant that the device or circuit reacts in a specified manner, or to a specified degree, to the input. Attack time occurs in devices such as clippers, peak limiters, compressors, and voxes.

</doc>
<doc id="40734" url="http://en.wikipedia.org/wiki?curid=40734" title="ARJ">
ARJ

ARJ (Archived by Robert Jung) is a software tool designed by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for DOS and 3.20 for Windows and supports 16-bit, 32-bit and 64-bit Intel architectures. 
ARJ was one of two mainstream archivers for DOS and Windows during the early and mid-90s, with PKZIP being its competition. Parts of ARJ were covered by U.S. Patent . Generally ARJ was less popular than PKZIP, but it did enjoy a niche market during the BBS era and in the warez scene. This was largely due to ARJ's creation and handling of multi-volume archives (archives which are split into smaller files which are then suitable for dial-up transfers and floppy distribution) being more robust than PKZIP's.
File format support in other software.
ARJ compressed files with the Filename extension .arj can be unpacked with various tools other than the ARJ software. There exists a free software re-implement of the tool. The software 7-Zip can also unpack .arj files.

</doc>
<doc id="40735" url="http://en.wikipedia.org/wiki?curid=40735" title="Attenuation">
Attenuation

In physics, attenuation (in some contexts also called extinction) is the gradual loss in intensity of any kind of flux through a medium. For instance, sunlight is attenuated by dark glasses, X-rays are attenuated by lead, and light and sound are attenuated by water.
In electrical engineering and telecommunications, attenuation affects the propagation of waves and signals in electrical circuits, in optical fibers, and in air (radio waves).
Background.
In many cases, attenuation is an exponential function of the path length through the medium. In chemical spectroscopy, this is known as the Beer–Lambert law. In engineering, attenuation is usually measured in units of decibels per unit length of medium (dB/cm, dB/km, etc.) and is represented by the attenuation coefficient of the medium in question. Attenuation also occurs in earthquakes; when the seismic waves move farther away from the epicenter, they grow smaller as they are attenuated by the ground.
Ultrasound.
One area of research in which attenuation figures strongly is in ultrasound physics. Attenuation in ultrasound is the reduction in amplitude of the ultrasound beam as a function of distance through the imaging medium. Accounting for attenuation effects in ultrasound is important because a reduced signal amplitude can affect the quality of the image produced. By knowing the attenuation that an ultrasound beam experiences traveling through a medium, one can adjust the input signal amplitude to compensate for any loss of energy at the desired imaging depth. 
Wave equations which take acoustic attenuation into account can be written on a fractional derivative form, see the article on acoustic attenuation or e.g. the survey paper.
Attenuation coefficient.
Attenuation coefficients are used to quantify different media according to how strongly the transmitted ultrasound amplitude decreases as a function of frequency. The attenuation coefficient (formula_1) can be used to determine total attenuation in dB in the medium using the following formula:
As this equation shows, besides the medium length and attenuation coefficient, attenuation is also linearly dependent on the frequency of the incident ultrasound beam. Attenuation coefficients vary widely for different media. In biomedical ultrasound imaging however, biological materials and water are the most commonly used media. The attenuation coefficients of common biological materials at a frequency of 1 MHz are listed below:
There are two general ways of acoustic energy losses: absorption and scattering, for instance light scattering.
Ultrasound propagation through homogeneous media is associated only with absorption and can be characterized with absorption coefficient only. Propagation through heterogeneous media requires taking into account scattering. Fractional derivative wave equations can be applied for modeling of lossy acoustical wave propagation, see also acoustic attenuation and Ref.
Light attenuation in water.
Shortwave radiation emitted from the sun have wavelengths in the visible spectrum of light that range from 360 nm (violet) to 750 nm (red). When the sun’s radiation reaches the sea-surface, the shortwave radiation is attenuated by the water, and the intensity of light decreases exponentially with water depth. The intensity of light at depth can be calculated using the Beer-Lambert Law.
In clear open waters, visible light is absorbed at the longest wavelengths first. Thus, red, orange, and yellow wavelengths are absorbed at higher water depths, and blue and violet wavelengths reach the deepest in the water column. Because the blue and violet wavelengths are absorbed last compared to the other wavelengths, open ocean waters appear deep-blue to the eye.
In near-shore (coastal) waters, sea water contains more phytoplankton than the very clear central ocean waters. Chlorophyll-a pigments in the phytoplankton absorb light, and the plants themselves scatter light, making coastal waters less clear than open waters. Chlorophyll-a absorbs light most strongly in the shortest wavelengths (blue and violet) of the visible spectrum. In near-shore waters where there are high concentrations of phytoplankton, the green wavelength reaches the deepest in the water column and the color of water to an observer appears green-blue or green.
Earthquake.
The energy with which an earthquake affects a location depends on the running distance. The attenuation in the signal of ground motion intensity plays an important role in the assessment of possible strong groundshaking. A seismic wave loses energy as it propagates through the earth (attenuation). This phenomenon is tied in to the dispersion of the seismic energy with the distance. There are two types of dissipated energy:
Electromagnetic.
Attenuation decreases the intensity of electromagnetic radiation due to absorption or scattering of photons. Attenuation does not include the decrease in intensity due to inverse-square law geometric spreading. Therefore, calculation of the total change in intensity involves both the inverse-square law and an estimation of attenuation over the path.
The primary causes of attenuation in matter are the photoelectric effect, compton scattering, and, for photon energies of above 1.022 MeV, pair production.
Radiography.
See Attenuation coefficient.
Optics.
Attenuation in fiber optics, also known as transmission loss, is the reduction in intensity of the light beam (or signal) with respect to distance travelled through a transmission medium. Attenuation coefficients in fiber optics usually use units of dB/km through the medium due to the relatively high quality of transparency of modern optical transmission media. The medium is typically a fiber of silica glass that confines the incident light beam to the inside. Attenuation is an important factor limiting the transmission of a digital signal across large distances. Thus, much research has gone into both limiting the attenuation and maximizing the amplification of the optical signal.
Empirical research has shown that attenuation in optical fiber is caused primarily by both scattering and absorption. 
 Attenuation in fiber optics can be quantified using the following equation:
Light scattering.
The propagation of light through the core of an optical fiber is based on total internal reflection of the lightwave. Rough and irregular surfaces, even at the molecular level of the glass, can cause light rays to be reflected in many random directions. This type of reflection is referred to as "diffuse reflection", and it is typically characterized by wide variety of reflection angles. Most objects that can be seen with the naked eye are visible due to diffuse reflection. Another term commonly used for this type of reflection is "light scattering". Light scattering from the surfaces of objects is our primary mechanism of physical observation.
Light scattering from many common surfaces can be modelled by lambertian reflectance.
Light scattering depends on the wavelength of the light being scattered. Thus, limits to spatial scales of visibility arise, depending on the frequency of the incident lightwave and the physical dimension (or spatial scale) of the scattering center, which is typically in the form of some specific microstructural feature. For example, since visible light has a wavelength scale on the order of one micrometer (one millionth of a meter), scattering centers will have dimensions on a similar spatial scale. 
Thus, attenuation results from the incoherent scattering of light at internal surfaces and interfaces. In (poly)crystalline materials such as metals and ceramics, in addition to pores, most of the internal surfaces or interfaces are in the form of grain boundaries that separate tiny regions of crystalline order. It has recently been shown that, when the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent. This phenomenon has given rise to the production of transparent ceramic materials.
Likewise, the scattering of light in optical quality glass fiber is caused by molecular-level irregularities (compositional fluctuations) in the glass structure. Indeed, one emerging school of thought is that a glass is simply the limiting case of a polycrystalline solid. Within this framework, "domains" exhibiting various degrees of short-range order become the building-blocks of both metals and alloys, as well as glasses and ceramics. Distributed both between and within these domains are microstructural defects that will provide the most ideal locations for the occurrence of light scattering. This same phenomenon is seen as one of the limiting factors in the transparency of IR missile domes.
UV-Vis-IR absorption.
In addition to light scattering, attenuation or signal loss can also occur due to selective absorption of specific wavelengths, in a manner similar to that responsible for the appearance of color. Primary material considerations include both electrons and molecules as follows:
The selective absorption of infrared (IR) light by a particular material occurs because the selected frequency of the light wave matches the frequency (or an integral multiple of the frequency) at which the particles of that material vibrate. Since different atoms and molecules have different natural frequencies of vibration, they will selectively absorb different frequencies (or portions of the spectrum) of infrared (IR) light.
Applications.
In optical fibers, attenuation is the rate at which the signal light decreases in intensity. For this reason, glass fiber (which has a low attenuation) is used for long-distance fiber optic cables; plastic fiber has a higher attenuation and, hence, shorter range. There also exist optical attenuators that decrease the signal in a fiber optic cable intentionally.
Attenuation of light is also important in physical oceanography. This same effect is an important consideration in weather radar, as raindrops absorb a part of the emitted beam that is more or less significant, depending on the wavelength used.
Due to the damaging effects of high-energy photons, it is necessary to know how much energy is deposited in tissue during diagnostic treatments involving such radiation. In addition, gamma radiation is used in cancer treatments where it is important to know how much energy will be deposited in healthy and in tumorous tissue.
Radio.
Attenuation is an important consideration in the modern world of wireless telecommunication. Attenuation limits the range of radio signals and is affected by the materials a signal must travel through (e.g., air, wood, concrete, rain). See the article on path loss for more information on signal loss in wireless communication.

</doc>
<doc id="40737" url="http://en.wikipedia.org/wiki?curid=40737" title="Attenuator">
Attenuator

An attenuator could mean:

</doc>
<doc id="40738" url="http://en.wikipedia.org/wiki?curid=40738" title="Attribute">
Attribute

Attribute may refer to:

</doc>
<doc id="40741" url="http://en.wikipedia.org/wiki?curid=40741" title="Audit (telecommunication)">
Audit (telecommunication)

In telecommunications, an audit is one of:
The simplest audits consist of comparing current telecommunications billing and usage to the underlying rate structure whether that is dictated by contract, tariff, or price list. Complex audits utilize software applications, direct bargaining with service providers and activity reports that include detail down to an individual employee's usage.
Auditing methods and consultants.
In business, companies with significant telecommunications costs or a telecommunications focus normally either conduct audits internally or hire a consultant. No matter the method, typical audits encompass one or more of the following:
A common misconception is that a Telecom Audit only relates to the area of telecom cost, when if fact it encompasses just about every communications service that a business expends its budget on.
Audits may focus on mobile phones and devices, Internet service or land line telephony, or they may encompass all three.
Independent telecom auditing firms are not affiliated with telecom companies that sell mobile phones and devices, Internet service, long distance calling or land line telephony. They are independent and work on contingency.

</doc>
<doc id="40742" url="http://en.wikipedia.org/wiki?curid=40742" title="Audit trail">
Audit trail

An audit trail (also called audit log) is a security-relevant chronological record, set of records, and/or destination and source of records that provide documentary evidence of the sequence of activities that have affected at any time a specific operation, procedure, or event. Audit records typically result from activities such as financial transactions, scientific research and health care data transactions, or communications by individual people, systems, accounts, or other entities.
The process that creates an audit trail is typically required to always run in a privileged mode, so it can access and supervise all actions from all users; a normal user should not be allowed to stop/change it. Furthermore, for the same reason, trail file or database table with a trail should not be accessible to normal users. Another way of handling this issue is through the use of a role-based security model in the software. The software can operate with the closed-looped controls, or as a 'closed system,' as required by many companies when using audit trail functionality.
Industry uses of the audit trail.
In telecommunication, the term means a record of both completed and attempted accesses and service, or data forming a logical path linking a sequence of events, used to trace the transactions that have affected the contents of a record.
In information or communications security, information audit means a chronological record of system activities to enable the reconstruction and examination of the sequence of events and/or changes in an event.
In nursing research, it refers to the act of maintaining a running log or journal of decisions relating to a research project, thus making clear the steps taken and changes made to the original protocol.
In accounting, it refers to documentation of detailed transactions supporting summary ledger entries. This documentation may be on paper or on electronic records.
In online proofing, it pertains to the version history of a piece of artwork, design, photograph, video, or web design proof in a project.
In Clinical Research, server based system call CTMS - Clinical Trial Management System required audit trial. Regulatory and QA - QC required the audit trail.

</doc>
<doc id="40743" url="http://en.wikipedia.org/wiki?curid=40743" title="Aurora (disambiguation)">
Aurora (disambiguation)

An aurora is a natural light display in the sky based around the polar regions.
Aurora may also refer to:

</doc>
<doc id="40745" url="http://en.wikipedia.org/wiki?curid=40745" title="Authenticator">
Authenticator

An authenticator is a way to prove to a computer system that you really are who you are (called authentication). It is either:
Authenticator tokens are common when one program needs to authenticate itself to a larger server or cloud repeatedly. For instance, you (the human) might sign on to a secure website with your name and password, after which you can surf around inside the secure server, visiting different web pages. Every time you move to a new page, however, the server must believe that you are the same person who originally signed in (otherwise it will refuse). Your browser keeps an authenticator token, which it sends upon every page request (often as a browser cookie), that does this. 
More complex situations might involve a program that runs automatically (say, at 4:00am every morning) that similarly requires authentication to get at the data it needs, but there's no human around to log in for them. An authenticator token must be prepared in advance that this program uses. Ultimately, some human must authenticate to create such a token. 

</doc>
<doc id="40746" url="http://en.wikipedia.org/wiki?curid=40746" title="Automated information system">
Automated information system

An automated information system (AIS) is an assembly of computer hardware, software, firmware, or any combination of these, configured to accomplish specific information-handling operations, such as communication, computation, dissemination, processing, and storage of information. Included are computers, word processing systems, networks, or other electronic information handling systems, and associated equipment. Management information systems are a common example of automated information systems.

</doc>
<doc id="40747" url="http://en.wikipedia.org/wiki?curid=40747" title="Automated information systems security">
Automated information systems security

In telecommunication, automated information systems security comprises measures and controls that ensure confidentiality, integrity, and availability of the information processed and stored by automated information systems. The unauthorized disclosure, modification, or destruction may be accidental or intentional.
Automated information systems security includes consideration of all computer hardware and software functions, characteristics and features; operational procedures; accountability procedures; and access controls at the central computer facility, remote computer, and terminal facilities; management constraints; physical structures and devices, such as computers, transmission lines, and power sources; and personnel and communications controls needed to provide an acceptable level of risk for the automated information system and for the data and information contained in the system. Automated information systems security also includes the totality of security safeguards needed to provide an acceptable protection level for an automated information system and for the data handled by an automated information system.
In INFOSEC, automated information systems security is a synonym for computer security.

</doc>
<doc id="40748" url="http://en.wikipedia.org/wiki?curid=40748" title="Automatic callback">
Automatic callback

In telecommunication, an automatic callback is a computer telephony calling feature that permits a user, when encountering a busy condition or other condition where the called individual is unavailable, to instruct the system to retain the called number and to establish the call when there is an available line or when the called number is no longer busy. Automatic callback may be implemented in the terminal, in the telephone exchange, or shared between them. Automatic callback is not the same as camp-on.
Source: from Federal Standard 1037C
Using callback on popular business telephone systems.
Comdial Digitech, DSU, Impact 
Place an intercom call and press CAMP. Your phone will disconnect from the attempted call. When the phone you rang is available, your phone will ring with five ring bursts. Press intercom to ring the other phone. 
To use with calls made in the voice-announce mode, press intercom before the camp button.
To cancel, press intercom and dial "#6".
Comdial ExecuTech System 2000
Make an intercom call. At the busy signal, dial "*6". Hang up. When the desired extension becomes idle, the calling telephone receives five tone bursts. To answer callback rings, lift the handset. The called telephone will ring.
To cancel auto call back before it rings, press "ITCM", dial "#6" and hang up. 
Comdial Digital Impression
When you reach a station that is busy or does not answer, press CAMP. When the phone you wish to reach becomes idle, your phone will ring with five short tones. Press ITCM to cause the other phone to ring. 
To cancel the callback, press ITCM and dial "#6".
If the extension you call in voice announce mode is not answered, press ITCM before pressing CAMP.
Database Systems Corp. PACER Phone System
Custom callback is integrated into the CRM application that signals the phone system to redial a number on a particular date and time. Call is automatically assigned to the original agent or assigned to a hunt group associated with a particular campaign.
Executone Encore CX
Press CALLBK when you hear the busy tone. Answer the callback by lifting the handset or pressing MON. 
Inter-Tel Eclipse IDS Integrated Operator Terminal
Press the Call Back key at the busy signal. Press the RLS key. When your line is free and the extension you called is idle, your extension will ring. When the calls rings back to you, press the RLS key.
Inter-Tel Eclipse2
Associate Display and Basic Digital Phone
Press "6" at the busy signal and hang up. Your phone will ring when the extension if available. Press "6" again to cancel before you get your callback.
Isoetec Digital Systems
Display/Data Phone
Press "Cb." Soft key at the busy signal. Replace the handset or press "HF". Wait for the double tone. When the extension is no longer busy, it will automatically call you back.
Isoetec IDS M Series Telephones
When you hear the busy signal, press the "CALL BACK" key. Hang up. When you are signaled, lift the handset or press the "HF" key. Press the blinking "CALL BACK" key. 
PCS Digital Telephone 
Press "cbck" at the busy signal. When a station is available, pick up the handset.
To cancel, press "del".
Vodavi StarPlus Phone System
Press the pre-programmed CALL BACK button. Hang up. When the busy station becomes available, you will be signaled. 
Source: from MainResource.com

</doc>
<doc id="40749" url="http://en.wikipedia.org/wiki?curid=40749" title="Automatic call distributor">
Automatic call distributor

In telephony, an automatic call distributor (ACD) or automated call distribution system, is a device or system that distributes incoming calls to a specific group of terminals or agents based on the customers selection, customers telephone number, selected incoming line to the system, or time of day the call was processed. It is often part of a computer telephony integration (CTI) system
Routing incoming calls is the task of the ACD system. ACD systems are often found in offices that handle large volumes of incoming phone calls from callers who have no need to talk to a specific person but who require assistance from any of multiple persons (e.g., customer service representatives) at the earliest opportunity.
The system consists of hardware for the terminals and switches, phone lines, and software for the routing strategy. The routing strategy is a rule-based set of instructions that tells the ACD how calls are handled inside the system. Typically this is an algorithm that determines the best available employee or employees to respond to a given incoming call. To help make this match, additional data are solicited and reviewed to find out why the customer is calling. Sometimes the caller's caller ID or ANI is used; more often a simple IVR is used to ascertain the reason for the call.
Originally, the ACD function was internal to the Private Branch Exchange of the company. However, the closed nature of these systems limited their flexibility. A system was then designed to enable common computing devices, such as server PCs, to make routing decisions. For this, generally the PBX would issue information about incoming calls to this external system and receive a direction of the call in response.
An additional function for these external routing applications is to enable CTI. This allows improved efficiency for call center agents by matching incoming phone calls with relevant data on their PC via screen pop.
A common protocol to achieve this is CSTA; however, almost every PBX vendor has its own flavor of CSTA, and CSTA is quite hard to program because of its complex nature. Various vendors have developed intermediate software that hides these complexities and expedites the work of programmers.
Also, these protocols enable call centers consisting of PBXs from multiple vendors to be treated as one virtual contact center. All real-time and historical statistical information can then be shared amongst call center sites.
One of the first large and separate ACDs was a modified 5XB switch used by New York Telephone in the early 1970s to distribute calls among hundreds of 4-1-1 information operators.
Distribution methods.
There are multiple choices for distributing incoming calls from a queue.

</doc>
<doc id="40751" url="http://en.wikipedia.org/wiki?curid=40751" title="Automatic data processing">
Automatic data processing

Automatic data processing (ADP) may refer to:

</doc>
<doc id="40752" url="http://en.wikipedia.org/wiki?curid=40752" title="Automatic data processing equipment">
Automatic data processing equipment

Automatic data processing equipment, legally defined, is any equipment or interconnected system or subsystems of equipment that is used in the
of data or information
which
The term includes
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40753" url="http://en.wikipedia.org/wiki?curid=40753" title="Automatic link establishment">
Automatic link establishment

Automatic Link Establishment, commonly known as ALE, is the worldwide de facto standard for digitally initiating and sustaining HF radio communications. ALE is a feature in an HF communications radio transceiver system, that enables the radio station to make contact, or initiate a circuit, between itself and another HF radio station or network of stations. The purpose is to provide a reliable rapid method of calling and connecting during constantly changing HF ionospheric propagation, reception interference, and shared spectrum use of busy or congested HF channels.
Mechanism.
A standalone ALE radio combines an HF SSB radio transceiver with an internal microprocessor and MFSK modem. It is programmed with a unique ALE Address, similar to a phone number (or on newer generations, a username). When not actively in contact with another station, the HF SSB transceiver constantly scans through a list of HF frequencies called channels, listening for any ALE signals transmitted by other radio stations. It decodes calls and soundings sent by other stations and uses the Bit error rate to store a quality score for that frequency and sender-address.
To reach a specific station, the caller enters the ALE Address. On many ALE radios this is similar to dialing a phone number. The ALE controller selects the best available idle channel for that destination address. After confirming the channel is indeed idle, it then sends a brief selective calling signal identifying the intended recipient. When the distant scanning station detects ALE activity, it stops scanning and stays on that channel until it can confirm whether or not the call is for it. The two stations' ALE controllers automatically handshake to confirm that a link of sufficient quality has been established, then notify the operators that the link is up. If the callee fails to respond or the handshaking fails, the originating ALE node usually selects another frequency either at random or by making a guess of varying sophistication.
Upon successful linking, the receiving station generally emits an audible alarm and shows a visual alert to the operator, thus indicating the incoming call. It also indicates the callsign or other identifying information of the linked station, similar to Caller ID. The operator then un-mutes the radio and answers the call then can talk in a regular conversation or negotiates a data link using voice or the ALE built-in short text message format. Alternatively, digital data can be exchanged via a built-in or external modem (such as a STANAG 5066 or MIL-STD-188-110B serial tone modem) depending on needs and availability. The ALE built-in text messaging facility can be used to transfer short text messages as an "orderwire" to allow operators to coordinate external equipment such as phone patches or non-embedded digital links, or for short tactical messages. 
Operator skill.
Due to the vagaries of ionospheric communications, HF radio as used by large governmental organizations in the mid-20th century was traditionally the domain of highly skilled and trained radio operators. One of the new characteristics that embedded microprocessors and computers brought to HF radio via ALE, was alleviation of the need for the radio operator to constantly monitor and change the radio frequency manually to compensate for ionospheric conditions or interference. For the average user of ALE, after learning how to work the basic functions of the HF transceiver, it became similar to operating a cellular mobile phone. For more advanced functions and programming of ALE controllers and networks, it became similar to the use of menu-enabled consumer equipment or the optional features typically encountered in software. In a professional or military organization, this does not eliminate the need for skilled and trained communicators to coordinate the per-unit authorized frequency lists and node addresses - it merely allows the deployment of relatively unskilled technicians as "field communicators" and end-users of the existing coordinated architecture.
Common applications.
An ALE radio system enables connection for voice conversation, alerting, data exchange, texting, instant messaging, email, file transfer, image, geo-position tracking, or telemetry. With a radio operator initiating a call, the process normally takes a few minutes for the ALE to pick an HF frequency that is optimum for both sides of the communication link. It signals the operators audibly and visually on both ends, so they can begin communicating with each other immediately. In this respect, the longstanding need in HF radio for repetitive calling on pre-determined time schedules or tedious monitoring static is eliminated. It is useful as a tool for finding optimum channels to communicate between stations in real-time. In modern HF communications, ALE has largely replaced HF prediction charts, propagation beacons, chirp sounders, propagation prediction software, and traditional radio operator educated guesswork. ALE is most commonly used for hooking up operators for voice contacts on SSB (single sideband modulation), HF internet connectivity for email, SMS phone texting or text messaging, real-time chat via HF text, Geo Position Reporting, and file transfer. High Frequency Internet Protocol or HFIP may be used with ALE for internet access via HF.
Techniques.
The essence of ALE techniques is the use of automatic channel selection, scanning receivers, selective calling, handshaking, and robust burst modems. An ALE node decodes all received ALE signals heard on the channel(s) it monitors. It utilizes the fact that all ALE messages utilize Forward error correction (FEC) redundancy. By noting how much error-correction occurred in each received and decoded message, an ALE node can detect the "quality" of the path between the sending station and itself. This information is coupled with the ALE address of the sending node and the channel the message was received on, and stored in the node's Link Quality Analysis (LQA) memory. When a call is initiated, the LQA lookup table is searched for matches involving the target ALE address and the best historic channel is used to call the target station. This reduces the likelihood that the call has to be repeated on alternate frequencies. Once the target station has heard the call and responded, a bell or other signalling device will notify both operators that a link has been established. At this point, the operators may coordinate further communication via orderwire text messages, voice, or other means. If further digital communication is desired, it may take place via external data modems or via optional modems built into the ALE terminal.
This unusual usage of FEC redundancy is the primary innovation that differentiates ALE from previous selective calling systems which either decoded a call or failed to decode due to noise or interference. A binary outcome of "Good enough" or not gave no way of automatically choosing between two channels, both of which are currently good enough for minimum communications. The redundancy-based scoring inherent in ALE thus allows for selecting the "best" available channel and (in more advanced ALE nodes) using all decoded traffic over some time window to sort channels into a list of decreasing probability-to-contact, significantly reducing co-channel interference to other users as well as dramatically decreasing the time needed to successfully link with the target node.
Techniques used in the ALE standard include automatic signaling, automatic station identification (sounding), polling, message store-and-forward, linking protection and anti-spoofing to prevent hostile denial of service by ending the channel scanning process. Optional ALE functions include polling and the exchange of orderwire commands and messages. The orderwire message, known as AMD (Automatic Message Display), is the most commonly used text transfer method of ALE, and the only universal method that all ALE controllers have in common for displaying text. It is common for vendors to offer extensions to AMD for various non-standard features, although dependency on these extensions undermines interoperability. As in all interoperability scenarios, care should be taken to determine if this is acceptable before using such extensions.
History and precedents.
ALE evolved from older HF radio selective calling technology. It combined existing channel-scanning selective calling concepts with microprocessors (enabling FEC decoding and quality scoring decisions), burst transmissions (minimizing co-channel interference), and transponding (allowing unattended operation and incoming-call signalling). Early ALE systems were developed in the late 1970s and early 1980s by several radio manufacturers. The first ALE-family controller units were external rack mounted controllers connected to control military radios, and were rarely interoperable across vendors.
Various methods and proprietary digital signaling protocols were used by different manufacturers in first generation ALE, leading to incompatibility. Later, a cooperative effort among manufacturers and the US government resulted in a second generation of ALE that included the features of first generation systems, while improving performance. The second generation 2G ALE system standard in 1986, MIL-STD-188-141A, was adopted in FED-STD-1045 for US federal entities. In the 1980s, military and other entities of the US government began installing early ALE units, using ALE controller products built primarily by US companies. The primary application during the first 10 years of ALE use was government and military radio systems, and the limited customer base combined with the necessity to adhere to MILSPEC standards kept prices extremely high. Over time, demand for ALE capabilities spread and by the late 1990s, most new government HF radios purchased were designed to meet at least the minimum ALE interoperability standard, making them eligible for use with standard ALE node gear. Radios implementing at least minimum ALE node functionality as an option internal to the radio became more common and significantly more affordable. As the standards were adopted by other governments worldwide, more manufacturers produced competitively priced HF radios to meet this demand. The need to interoperate with government organizations prompted many non-government organizations (NGOs) to at least partially adopt ALE standards for communication. As non-military experience spread and prices came down, other civilian entities started using 2G ALE. By the year 2000, there were enough civilian and government organizations worldwide using ALE that it became a de facto HF interoperability standard for situations where a priori channel and address coordination is possible.
In the late 1990s, a third generation 3G ALE with significantly improved capability and performance was included in MIL-STD-188-141B, retaining backward compatibility with 2G ALE, and was adopted in NATO STANAG 4538. Civilian and non-government adoption rates are much lower than 2G ALE due to the extreme cost as compared to surplus or entry-level 2G gear as well as the significantly increased system and planning complexity necessary to realize the benefits inherent in the 3G specification. For many militaries, whose needs for maximized intra-organizational capability and capacity always strain existing systems, the additional cost and complexity of 3G is far more compelling.
Reliability.
ALE enables rapid unscheduled communication and message passing without requiring complex message centers, multiple radios and antennas, or highly trained operators. With the removal of these potential sources of failure, the tactical communication process becomes much more robust and reliable. The effects extend beyond mere Force multiplication of existing communications methods; units such as helicopters, when outfitted with ALE radios, can now reliably communicate in situations where the crew are too busy to operate a traditional non-line of sight radio. This ability to enable tactical communication in conditions where dedicated trained operators and hardware are inappropriate is often considered to be the true improvement offered by ALE.
ALE is a critical path toward increased interoperability between organizations. By enabling a station to participate nearly simultaneously in many different HF networks, ALE allows for convenient cross-organization message passing and monitoring without requiring dedicated separate equipment and operators for each partner organization. This dramatically reduces staffing and equipment considerations, while enabling small mobile or portable stations to participate in multiple networks and subnetworks. The result is increased resilience, decreased fragility, increased ability to communicate information effectively, and the ability to rapidly add to or replace communication points as the situation demands.
When combined with Near Vertical Incidence Skywave (NVIS) techniques and sufficient channels spread across the spectrum, an ALE node can provide greater than 95% success linking on the first call, nearly on par with SATCOM systems. This is significantly more reliable than cellphone infrastructure during disasters or wars yet is mostly immune to such considerations itself.
Standards and protocols.
Global standards for ALE are based on the original US MIL-STD 188-141A and FED-1045, known as 2nd Generation (2G) ALE. 2G ALE uses non-synchronised scanning of channels, and it takes several seconds to half a minute to repeatedly scan through an entire list of channels looking for calls. Thus it requires sufficient duration of transmission time for calls to connect or link with another station that is unsynchronised with its calling signal. The vast majority of ALE systems in use in the world at the present time are 2G ALE.
2G technical characteristics.
The more common 2G ALE signal waveform is designed to be compatible with standard 3 kHz SSB narrowband voice channel transceivers. The modulation method is 8ary Frequency Shift Keying or 8FSK, also sometimes called Multi Frequency Shift Keying MFSK, with eight orthogonal tones between 750 and 2500 Hz. Each tone is 8 ms long, resulting in a transmitted over-the-air symbol rate of 125 baud or 125 symbols per second, with a raw data rate of 375 bits per second. The ALE data is formatted in 24-bit frames, which consist of a 3 bit preamble followed by three ASCII characters, each seven bits long. The received signal is usually decoded using digital signal processing techniques that are capable of recovering the 8FSK signal at a negative decibel signal to noise ratio (i.e., the signal may be recovered even when it is below the noise level). The over-the-air layers of the protocol involve the use of forward error correction, redundancy, and handshaking transponding similar to those used in ARQ techniques.
3G technical characteristics.
Newer standards of ALE called 3rd Generation or 3G ALE, use accurate time synchronization (via a defined time-synch protocol as well as the option of GPS-locked clocks) to achieve faster and more dependable linking. Through synchronization, the calling time to achieve a link may be reduced to less than 10 seconds. The 3G ALE modem signal also provides better robustness and can work in channel conditions that are less favorable than 2G ALE. Dwell groups, limited callsigns, and shorter burst transmissions enable more rapid intervals of scanning. All stations in the same group scan and receive each channel at precisely the same time window. Although 3G ALE is more reliable and has significantly enhanced channel-time efficiency, the existence of a large installed base of 2G ALE radio systems and the wide availability of moderately priced (often military surplus) equipment, has made 2G the baseline standard for global interoperability.
Basis for HF interoperability communications.
Interoperability is a critical issue for the disparate entities which use radiocommunications to fulfill the needs of organizations. Largely due to the ubiquity of 2G ALE, it became the primary method for providing interoperability on HF between governmental and non-governmental disaster relief and emergency communications entities, and amateur radio volunteers. With digital techniques increasingly employed in communications equipment, a universal digital calling standard was needed, and ALE filled the gap. Nearly every major HF radio manufacturer in the world builds ALE radios to the 2G standard to meet the high demand that new installations of HF radio systems conform to this standard protocol. Disparate entities that historically used incompatible radio methods were then able to call and converse with each other using the common 2G ALE platform. Some manufacturers and organizations have utilized the AMD feature of ALE to expand the performance and connectivity. In some cases, this has been successful, and in other cases, the use of proprietary preamble or embedded commands has led to interoperability problems.
Tactical communication and resource management.
ALE serves as a convenient method of beyond line of sight communication. Originally developed to support military requirements, ALE is useful to many organizations who find themselves managing widely-located units. United States Immigration and Customs Enforcement and United States Coast Guard are two members of the Customs Over the Horizon Enforcement Network(COTHEN), a MIL-STD 188-141A ALE network. All U.S. armed forces operate multiple similar networks. Similarly, shortwave utility listeners have documented frequency and callsign lists for many nations' military and guard units, as well as networks operated by oil exploration and production companies and public utilities in many countries.
Emergency / disaster relief or extraordinary situation response communications.
ALE radio communication systems for both HF regional area networks and HF interoperability communications are in service among emergency and disaster relief agencies as well as military and guard forces. Extraordinary response agencies and organizations use ALE to respond to situations in the world where conventional communications may have been temporarily overloaded or damaged. In many cases, it is in place as alternative back-channel for organizations that may have to respond to situations or scenarios involving the loss of conventional communications. Earthquakes, storms, volcanic eruptions, and power or communication infrastructure failures are typical situations in which organizations may deem ALE necessary to operations. ALE networks are common among organizations engaged in extraordinary situation response such as: natural and man-made disasters, transportation, power, or telecommunication network failures, war, peacekeeping, or stability operations. Organizations known to use ALE for Emergency management, disaster relief, ordinary communication or extraordinary situation response include: Red Cross, FEMA, Disaster Medical Assistance Teams, NATO, Federal Bureau of Investigation, United Nations, AT&T, Civil Air Patrol, , State of California , other US States' Offices of Emergency Services or Emergency Management Agencies, and Amateur Radio Emergency Service(ARES).
International HF telecommunications for disaster relief.
The International Telecommunications Union (ITU), in response to the need for interoperation in international disaster response spurred largely by humanitarian relief, included ALE in its Telecommunications for Disaster Relief recommendations. The increasing need for instant connectivity for logistical and tactical disaster relief response communications, such as the 2004 Indian Ocean earthquake tsunami led to ITU actions of encouragement to countries around the world toward loosening restrictions on such communications and equipment border transit during catastrophic disasters. The IARU Global Amateur Radio Emergency Communications Conferences (GAREC) and IARU Global Simulated Emergency Tests have included ALE.
Use in amateur radio.
Amateur radio operators began sporadic ALE operation on a limited basis in the early to mid-1990s, with commercial ALE radios and ALE controllers. In 2000, the first widely available software ALE controller for the Personal Computer, "PCALE", became available, and hams started to set up stations based on it. In 2001, the first organized and coordinated global ALE nets for International Amateur Radio began. In August 2005, ham radio operators supporting communications for emergency Red Cross shelters used ALE for Disaster Relief operations during the Hurricane Katrina disaster. After the event, hams developed more permanent ALE emergency/disaster relief networks, including internet connectivity, with a focus on interoperation between organizations. The amateur radio system uses an open net protocol to enable all amateur radio operators and amateur radio nets worldwide to participate in ALE and share the same ALE channels legally and interoperably. Amateur radio operators may use it to call each other for voice or data communications.
Amateur radio interoperability adaptations.
Amateur radio operators commonly provide local, regional, national, and international emergency / disaster relief communications. The need for interoperability on HF led to the adoption of open networks by hams. Amateur radio adapted 2G ALE techniques, by utilizing the common denominators of the 2G ALE protocol, with a limited subset of features found in the majority of all ALE radios and controllers. Each amateur radio ALE station uses the operator's callsign as the address, also known as the ALE Address, in the ALE radio controller. The lowest common denominator technique enables any manufacturer's ALE radios or software to be utilized for HF interoperability communications and networking. Known as Ham-Friendly ALE, the amateur radio ALE standard is used to establish radio-communications, through a combination of active ALE on internationally recognized automatic data frequencies, and passive ALE scanning on voice channels. In this technique, active ALE frequencies include pseudo-random periodic polite station identification, while passive ALE frequencies are silently scanned for selective calling. ALE systems include Listen Before Transmit as a standard function, and in most cases this feature provides better busy channel detection of voice and data signals than the human ear. Ham-Friendly ALE technique is also known as 2.5G ALE, because it maintains 2G ALE compatibility while employing some of the adaptive channel management features of 3G ALE, but without the accurate GPS time synchronization of 3G ALE.
Disaster relief HF network.
Hot standby nets are in constant operation 24/7/365 for International Emergency and Disaster Relief communications. The , which began service in June 2007, is the world's largest intentionally open ALE network for internet connectivity. It is a free open network staffed by volunteers, and utilized by amateur radio operators supporting disaster relief organizations.
International coordination.
International amateur radio ALE High Frequency channels are frequency coordinated with all Regions of the (IARU entity of ITU), for international, regional, national, and local use in the Amateur Radio Service. All Amateur Radio ALE channels use "USB" Upper Sideband standard. Different rules, regulations, and bandplans of the region and local country of operation apply to use of various channels. Some channels may not be available in every country. Primary or global channels are in common with most countries and regions.
International channels.
"This listing is current as of March 2014. See for more information about Amateur Radio Automatic Link Establishment."
References.
</dl>

</doc>
<doc id="40754" url="http://en.wikipedia.org/wiki?curid=40754" title="Automatic message exchange">
Automatic message exchange

Automatic message exchange (AME): In an adaptive high-frequency (HF) radio network, an automated process allowing the transfer of a message from message injection to addressee reception, without human intervention. Through the use of machine-addressable transport guidance information, "i.e.," the message header, the message is automatically routed through an on-line direct connection through single or multiple transmission media.
Source: from Federal Standard 1037C

</doc>
<doc id="40755" url="http://en.wikipedia.org/wiki?curid=40755" title="Automatic redial">
Automatic redial

In telecommunication, an automatic redial is a service feature that allows the user to dial, by depressing a single key or a few keys, the most recent telephone number dialed at that instrument.
"Note:" Automatic redial is often associated with the telephone instrument, but may be provided by a PBX, or by the central office. "Synonym" last number redial. "Contrast with" automatic calling unit.
Often one must subscribe to a caller ID for use of this function on a landline.
References.
 This article incorporates public domain material from websites or documents of the .

</doc>
<doc id="40756" url="http://en.wikipedia.org/wiki?curid=40756" title="Automatic sounding">
Automatic sounding

In telecommunication, automatic sounding is the testing of selected channels for quality by providing a very brief identifying transmission that may be used by other stations to evaluate connectivity, and availability, and to identify known working channels for immediate or later use for communications or calling. Often used to maintain connectivity in digital communications high frequency radio networks.
Automatic soundings are primarily intended to increase the efficiency of the automatic link establishment (ALE) function, thereby increasing system throughput. 
In ALE, the sounding information consists of a heavily error-corrected short message identifying the sender. Recipients decode it and use the bit error rate to calculate and store a (channel, node, quality) tuple. As ionospheric conditions and mobile-node locations change, these quality tuples will shift. The stored data can be used to maximize the chance that the best channel to link with a given partner will be chosen first.

</doc>
<doc id="40757" url="http://en.wikipedia.org/wiki?curid=40757" title="Automatic switching system">
Automatic switching system

In data communications, an automatic switching system is a switching system in which all the operations required to execute the three phases of Information transfer transactions are automatically executed in response to signals from a user end-instrument. 
In an automatic switching system, the information-transfer transaction is performed without human intervention, except for initiation of the access phase and the disengagement phase by a user. 
In telephony, it refers to a telephone exchange in which all the operations required to set up, supervise, and release connections required for telephone calls are automatically performed in response to signals from a calling device. This distinction lost importance as manual switching declined during the 20th century.

</doc>
<doc id="40758" url="http://en.wikipedia.org/wiki?curid=40758" title="Auxiliary power">
Auxiliary power

Articles on Auxiliary power include:

</doc>
<doc id="40760" url="http://en.wikipedia.org/wiki?curid=40760" title="Availability">
Availability

In reliability theory and reliability engineering, the term availability has the following meanings:
For example, a unit that is capable of being used 100 hours per week (168 hours) would have an availability of 100/168. However, typical availability values are specified in decimal (such as 0.9998). In high availability applications, a metric known as nines, corresponding to the number of nines following the decimal point, is used. With this convention, "five nines" equals 0.99999 (or 99.999%) availability.
Introduction.
Availability of a system is typically measured as a factor of its reliability - as reliability increases, so does availability. Availability of a system may also be increased by the strategy on focusing on increasing testability & maintainability and not on reliability. Improving maintainability is generally easier than reliability. Maintainability estimates (Repair rates) are also generally more accurate. However, because the uncertainties in the reliability estimates are in most cases very large, it is likely to dominate the availability (prediction uncertainty) problem, even while maintainability levels are very high.
When reliability is not under control more complicated issues may arise, like manpower (maintainers / customer service capability) shortage, spare part availability, logistic delays, lack of repair facilities, extensive retro-fit and complex configuration management costs and others. The problem of unreliability may be increased also due to the "domino effect" of maintenance induced failures after repairs. Only focusing on maintainability is therefore not enough. If failures are prevented, none of the others are of any importance and therefore reliability is generally regarded as the most important part of availability.
Reliability needs to be evaluated and improved related to both availability and the cost of ownership (due to cost of spare parts, maintenance man-hours, transport costs, storage cost, part obsolete risks etc.). Often a trade-off is needed between the two. There might be a maximum ratio between availability and cost of ownership. Testability of a system should also be addressed in the availability plan as this is the link between reliability and maintainability. The maintenance strategy can influence the reliability of a system (e.g. by preventive and/or predictive maintenance), although it can never bring it above the inherent reliability. So, Maintainability and Maintenance strategies influences the availability of a system. In theory this can be almost unlimited if one would be able to always repair any fault in an infinitely short time. This is in practice impossible. Repair-ability is always limited due to testability, manpower and logistic considerations.
An availability plan should clearly provide a strategy for availability control. Whether only Availability or also Cost of Ownership is more important depends on the use of the system. For example, a system that is a critical link in a production system - e.g. a big oil platform – is normally allowed to have a very high cost of ownership if this translates to even a minor increase in availability, as the unavailability of the platform results in a massive loss of revenue which can easily exceed the high cost of ownership. A proper reliability plan should always address RAMT analysis in its total context. RAMT stands in this case for Reliability, Availability, Maintainability/Maintenance and Testability in context to the customer needs.
Representation.
The most simple representation for availability is as a ratio of the expected value of the uptime of a system to the aggregate of the expected values of up and down time, or
If we define the status function formula_2 as
therefore, the availability "A"("t") at time "t">0 is represented by
Average availability must be defined on an interval of the real line. If we consider an arbitrary constant formula_5, then average availability is represented as
Limiting (or steady-state) availability is represented by
Limiting average availability is also defined on an interval formula_8 as,
Availability is the probability that an item will be in an operable and commitable state at the start of a mission when the mission is called for at a random time, and is generally defined as uptime divided by total time (uptime plus downtime).
Definitions within Systems Engineering.
Availability, Inherent (Ai) 
The probability that an item will operate satisfactorily at a given point in time when used under stated conditions in an ideal support environment. It excludes logistics time, waiting or administrative downtime, and preventive maintenance downtime. It includes corrective maintenance downtime. Inherent availability is generally derived from analysis of an engineering design and is calculated as the mean time to failure (MTTF) divided by the mean time to failure plus the mean time to repair (MTTR). It is based on quantities under control of the designer.
Availability, Achieved (Aa) 
The probability that an item will operate satisfactorily at a given 
point in time when used under stated conditions in an ideal support environment (i.e., that personnel, tools, spares, etc. are instantaneously available). It excludes logistics time and waiting or administrative downtime. It includes active preventive and corrective maintenance downtime.
Availability, Operational (Ao) 
The probability that an item will operate satisfactorily at a given point in time when used in an actual or realistic operating and support environment. It includes logistics time, ready time, and waiting or administrative downtime, and both preventive and corrective maintenance downtime. This value is equal to the mean time between failure (MTBF) divided by the mean time between failure plus the mean downtime (MDT). This measure extends the definition of availability to elements controlled by the logisticians and mission planners such as quantity and proximity of spares, tools and manpower to the hardware item.
Refer to Systems Engineering for more details
Example.
If we are using equipment which has a mean time to failure (MTTF) of 81.5 years and mean time to repair (MTTR) of 1 hour:
MTTF in hours = 81.5*365*24=713940 (This is a reliability parameter and often has a high level of uncertainty!)
Inherent Availability (Ai) = MTTF/(MTTF+MTTR) = 713940/713941 =99.999859%
Inherent Unavailability = 0.000141%
Outage due to equipment in hours per year = 1/rate = 1/MTTF = 0.01235 hours per year.
Literature.
Availability is well established in the literature of stochastic modeling and optimal maintenance. Barlow and Proschan [1975] define availability of a repairable system as "the probability that the system is operating at a specified time t." Blanchard [1998] gives a qualitative definition of availability as "a measure of the degree of a system which is in the operable and committable state at the start of mission when the mission is called for at an unknown random point in time." This definition comes from the MIL-STD-721. Lie, Hwang, and Tillman [1977] developed a complete survey along with a systematic classification of availability.
Availability measures are classified by either the time interval of interest or the mechanisms for the system downtime. If the time interval of interest is the primary concern, we consider instantaneous, limiting, average, and limiting average availability. The aforementioned definitions are developed in Barlow and Proschan [1975], Lie, Hwang, and Tillman [1977], and Nachlas [1998]. The second primary classification for availability is contingent on the various mechanisms for downtime such as the inherent availability, achieved availability, and operational availability. (Blanchard [1998], Lie, Hwang, and Tillman [1977]). Mi [1998] gives some comparison results of availability considering inherent availability.
Availability considered in maintenance modeling can be found in Barlow and Proschan [1975] for replacement models, Fawzi and Hawkes [1991] for an R-out-of-N system with spares and repairs, Fawzi and Hawkes [1990] for a series system with replacement and repair, Iyer [1992] for imperfect repair models, Murdock [1995] for age replacement preventive maintenance models, Nachlas [1998, 1989] for preventive maintenance models, and Wang and Pham [1996] for imperfect maintenance models.
Applications.
Availability is used extensively in power plant engineering. For example, the North American Electric Reliability Corporation implemented the Generating Availability Data System in 1982.
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40762" url="http://en.wikipedia.org/wiki?curid=40762" title="Backbone">
Backbone

Backbone may refer to:

</doc>
<doc id="40765" url="http://en.wikipedia.org/wiki?curid=40765" title="Back-to-back connection">
Back-to-back connection

A back-to-back connection is the direct connection of the output of one device to the input of a similar or related device.
Telecommunications.
In telecommunications, a back-to-back connection can be formed by connecting a transmitter directly to a receiver without a transmission line in between. This is used for equipment measurements and testing purposes. The back-to-back connection eliminates the effects of the transmission channel or medium.
In some cases, the output of a receiving device is instead connected to the input of a transmitting device.
Power transmission.
A back-to-back connection for electric power transmission is a high-voltage direct-current (HVDC) system with both ends in the same switchyard. This is used to couple asynchronously operated power grids or for connecting power grids of different frequencies where no DC transmission line is necessary.
Electronics.
In electronics, a back-to-back connection is the connection of two identical or similar components in series with the opposite polarity. This is used to convert polarised components to non-polar use. Common examples include:

</doc>
<doc id="40766" url="http://en.wikipedia.org/wiki?curid=40766" title="Backward channel">
Backward channel

In a data transmission circuit a backward channel is the channel that passes data in a direction opposite to that of its associated forward channel. The backward channel is usually used for transmission of request, supervisory, acknowledgement, or error-control signals. The direction of flow of these signals is opposite to that in which user information is being transferred. The backward-channel bandwidth is usually less than that of the primary channel, that is, the forward (user information) channel. For example, ADSL's upstream channel, considered a backward channel for some types of analysis, typically has a bandwidth less than one-fourth of the downstream channel.
In data transmission, it is a secondary channel in which the direction of transmission is constrained to be opposite to that of the primary, "i.e.", the forward (user-information) channel. The direction of transmission in the backward channel is restricted by the control interchange circuit that controls the direction of transmission in the primary channel.
See also.
"Source: Partly from Federal Standard 1037C and from MIL-STD-188"

</doc>
<doc id="40767" url="http://en.wikipedia.org/wiki?curid=40767" title="Balanced line">
Balanced line

In telecommunications and professional audio, a balanced line or balanced signal pair is a transmission line consisting of two conductors of the same type, each of which have equal impedances along their lengths and equal impedances to ground and to other circuits. The chief advantage of the balanced line format is good rejection of external noise when fed to a differential amplifier. Common forms of balanced line are twin-lead, used for radio frequency signals and twisted pair, used for lower frequencies. They are to be contrasted to unbalanced lines, such as coaxial cable, which is designed to have its return conductor connected to ground, or circuits whose return conductor actually is ground. Balanced and unbalanced circuits can be interconnected using a transformer called a balun.
Circuits driving balanced lines must themselves be balanced to maintain the benefits of balance. This may be achieved by transformer coupling or by merely balancing the impedance in each conductor.
Lines carrying symmetric signals (those with equal but opposite voltages to ground on each leg) are often incorrectly referred to as "balanced", but this is actually differential signaling. Balanced lines and differential signaling are often used together, but they are not the same thing. Differential signalling does not make a line balanced, nor does noise rejection in balanced cables require differential signalling.
Explanation.
Transmission of a signal over a balanced line reduces the influence of noise or interference due to external stray electric fields. Any external signal sources tend to induce only a common mode signal on the line, and the balanced impedances to ground minimizes differential pickup due to stray electric fields. The conductors are sometimes twisted together to ensure that each conductor is equally exposed to any external magnetic fields that could induce unwanted noise.
Some balanced lines also have electromagnetic shielding to reduce the amount of noise introduced.
A balanced line allows a differential receiver to reduce the noise on a connection by rejecting common-mode interference. The lines have the same impedance to ground, so the interfering fields or currents induce the same voltage in both wires. Since the receiver responds only to the difference between the wires, it is not influenced by the induced noise voltage. If twisted pair becomes unbalanced, for example due to insulation failure, noise will be induced. Examples of twisted pairs include Category 5 cable.
Compared to unbalanced circuits, balanced lines reduce the amount of noise per distance, allowing a longer cable run to be practical. This is because electromagnetic interference will affect both signals the same way. Similarities between the two signals are automatically removed at the end of the transmission path when one signal is subtracted from the other.
Telephone systems.
The first application for balanced lines was for telephone lines. Interference that was of little consequence on a telegraph system (which is in essence digital) could be very disturbing for a telephone user. The initial format was to take two single-wire unbalanced telegraph lines and use them as a pair. This proved insufficient, however, with the growth of electric power transmission which tended to use the same routes. A telephone line running alongside a power line for many miles will inevitably have more interference induced in one leg than the other since one of them will be nearer to the power line. This issue was addressed by swapping the positions of the two legs every few hundred yards with a cross-over, thus ensuring that both legs had equal interference induced and allowing common-mode rejection to do its work. As the telephone system grew, it became preferable to use cable rather than open wires to save space, and also to avoid poor performance during bad weather. The cable construction used for balanced telephone cables was twisted pair, however, this did not become widespread until repeater amplifiers became available. For an unamplified telephone line, a twisted pair cable could only manage a maximum distance of 30 km. Open wires, on the other hand, with their lower capacitance had been used for enormous distances - the longest was the 1500 km from New York to Chicago built in 1893. Loading coils were used to improve the distance achievable with cable but the problem was not finally overcome until amplifiers started to be installed in 1912. Twisted pair balanced lines are still widely used for the telephone subscribers local end.
Telephone trunk lines, and especially frequency division multiplexing carrier systems, are usually 4-wire circuits rather than 2-wire circuits (or at least they were before fibre-optic became widespread) and require a different kind of cable. This format requires the conductors to be arranged in two pairs, one pair for the sending (go) signal and the other for the return signal. The greatest source of interference on this kind of transmission is usually the crosstalk between the go and return circuits themselves. The most common cable format is star quad, where the diagonally opposite conductors form the pairs. This geometry gives maximum common mode rejection between the two pairs. An alternative format is DM quad which consists of two twisted pairs with the twisting at different pitches.
Audio systems.
An example of balanced lines is the connection of microphones to a mixer in professional systems. Classically, both dynamic and condenser microphones used transformers to provide a differential-mode signal. While transformers are still used in the large majority of modern dynamic microphones, more recent condenser microphones are more likely to use electronic drive circuitry. Each leg, irrespective of any signal, should have an identical impedance to ground. Pair cable (or a pair-derivative such as star quad) is used to maintain the balanced impedances and close twisting of the cores ensures that any interference is common to both conductors. Providing that the receiving end (usually a mixing console) does not disturb the line balance, and is able to ignore common-mode (noise) signals, and can extract differential ones, then the system will have excellent immunity to induced interference.
Typical professional audio sources, such as microphones, have three-pin XLR connectors. One is the shield or chassis ground, while the other two are signal connections. These signal wires carry two copies of the same signal, but with opposite polarity. (They are often termed "hot" and "cold," and the AES14-1992(r2004) Standard [and EIA Standard RS-297-A] suggest that the pin that carries the positive signal that results from a positive air pressure on a transducer will be deemed 'hot'. Pin 2 has been designated as the 'hot' pin, and that designation serves useful for keeping a consistent polarity in the rest of the system.) Since these conductors travel the same path from source to destination, the assumption is that any interference is induced upon both conductors equally. The appliance receiving the signals compares the difference between the two signals (often with disregard to electrical ground) allowing the appliance to ignore any induced electrical noise. Any induced noise would be present in equal amounts and in identical polarity on each of the balanced signal conductors, so the two signals’ difference from each other would be unchanged. The successful rejection of induced noise from the desired signal depends in part on the balanced signal conductors receiving the same amount and type of interference. This typically leads to twisted, braided, or co-jacketed cables for use in balanced signal transmission.
Balanced and differential.
Most explanations of balanced lines assume symmetric (antiphase) signals but this is an unfortunate confusion - signal symmetry and balanced lines are quite independent of each other. Essential in a balanced line is matched impedances in the driver, line and receiver. These conditions ensure that external noise affects each leg of the differential line equally and thus appears as a common mode signal that is removed by the receiver. There are balanced drive circuits that have excellent common-mode impedance matching between "legs" but do "not" provide symmetric signals. Symmetric differential signals exist to prevent interference "to" other circuits - the electromagnetic fields are canceled out by the equal and opposite currents. But they are not necessary for interference rejection "from" other circuits.
Baluns.
To convert a signal from balanced to unbalanced requires a balun. For example, baluns can be used to send line level audio or E-carrier level 1 signals over coaxial cable (which is unbalanced) through 300 ft of Category 5 cable by using a pair of baluns at each end of the CAT5 run. The balun takes the unbalanced signal, and creates an inverted copy of that signal. It then sends these 2 signals across the CAT5 cable as a balanced signal. Upon reception at the other end, the balun takes the difference of the two signals, thus removing any noise picked up along the way and recreating the unbalanced signal.
A once common application of a radio frequency balun was found at the antenna terminals of a television receiver. Typically a 300-ohm balanced twin lead antenna input could only be connected to a coaxial cable from a cable TV system through a balun.
Characteristic impedance.
The characteristic impedance formula_1 of a transmission line is an important parameter at higher frequencies of operation. For a parallel 2-wire transmission line, 
where formula_3 is half the distance between the wire centres, formula_4 is the wire radius and formula_5, formula_6 are respectively the permeability and permittivity of the surrounding medium. A commonly used approximation that is valid when the wire separation is much larger than the wire radius and in the absence of magnetic materials is
where formula_8 is the relative permittivity of the surrounding medium.
Electric power lines.
In electric power transmission, the three conductors used for three-phase power transmission are referred to as a balanced line since the instantaneous sum of the three line voltages is nominally zero. However, "balance" in this field is referring to the symmetry of the source and load: it has nothing to do with the impedance balance of the line itself, the sense of the meaning in telecommunications.
For the transmission of single-phase electric power as used for railway electrification systems, two conductors are used to carry in-phase and out-of-phase voltages such that the line is balanced.
Bipolar HVDC lines at which each pole is operated with the same voltage toward ground are also balanced lines.

</doc>
<doc id="40768" url="http://en.wikipedia.org/wiki?curid=40768" title="Balance return loss">
Balance return loss

In telecommunications, balance return loss is one of two things:
References.
 This article incorporates public domain material from websites or documents of the ( in support of MIL-STD-188).

</doc>
<doc id="40769" url="http://en.wikipedia.org/wiki?curid=40769" title="Balancing network">
Balancing network

In a hybrid set, hybrid coil, or resistance hybrid, balancing network is a circuit used to match, "i.e.", to balance, the impedance of a uniform transmission line, (e.g., a twisted metallic pair, coaxial cable, etc.) over a selected range of frequencies. A balancing network is required to ensure isolation between the two ports of the four-wire side of the hybrid.
A balancing network can also be a device used between a balanced device or line and an unbalanced device or line for the purpose of transforming from balanced to unbalanced or from unbalanced to balanced.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40771" url="http://en.wikipedia.org/wiki?curid=40771" title="Bandwidth compression">
Bandwidth compression

In telecommunication, the term bandwidth compression has the following meanings: 
Bandwidth compression implies a reduction in normal bandwidth of an information-carrying signal without reducing the information content of the signal. This can be accomplished with lossless data compression techniques. For more information read the Increasing speeds section in the Modem article. Bandwidth Compression is a core feature of WAN Optimization appliances to improve bandwidth efficiency.
Bandwidth Compression Software.
Wanos Networks

</doc>
<doc id="40772" url="http://en.wikipedia.org/wiki?curid=40772" title="Barrage jamming">
Barrage jamming

Barrage jamming is radio jamming accomplished by transmitting a band of frequencies that is large with respect to the bandwidth of a single emitter. Barrage jamming may be accomplished by presetting multiple jammers on adjacent frequencies, by using a single wideband transmitter, or by using a transmitter capable of frequency sweep fast enough to appear radiating simultaneously over wide band (e.g. a carcinotron). Barrage jamming makes it possible to jam emitters on different frequencies simultaneously and reduces the need for operator assistance or complex control equipment. These advantages are gained at the expense of reduced jamming power at any given frequency. Barrage jamming has to be used against frequency-agile radars, which change frequencies too quickly to follow them in a conventional way. The use of barrage jamming may also affect the communications capability of the jamming source in a negative fashion.
Photomultiplier tubes were popularized during World War II since they could be used to as high bandwidth (up to several hundred MHz) noise sources. Although wide bandwidth sources typically suffer from low spectral power per unit frequency, the photomultiplier tube offers high gain (about formula_1) amplification of photon shot noise, making it advantageous over other lower gain noise sources.

</doc>
