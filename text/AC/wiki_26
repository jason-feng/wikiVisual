<doc id="29357" url="http://en.wikipedia.org/wiki?curid=29357" title="Send in the Clowns">
Send in the Clowns

"Send in the Clowns" is a song written by Stephen Sondheim for the 1973 musical "A Little Night Music", an adaptation of Ingmar Bergman's film "Smiles of a Summer Night". It is a ballad from Act II in which the character Desirée reflects on the ironies and disappointments of her life. Among other things, she looks back on an affair years earlier with the lawyer Fredrik. Meeting him after so long, she finds that he is now in an unconsummated marriage with a much younger woman. Desirée proposes marriage to rescue him from this situation, but he declines, citing his dedication to his bride. Reacting to his rejection, Desirée sings this song. The song is later reprised as a coda after Fredrik's young wife runs away with his son, and Fredrik is finally free to accept Desirée's offer.
Sondheim wrote the song specifically for the actress Glynis Johns, who created the role of Desirée on Broadway. The song is structured with four verses and a bridge, and uses a complex compound meter. It became Sondheim's most popular song after Frank Sinatra recorded it in 1973 and Judy Collins' version charted in 1975 and 1977. Subsequently, Sarah Vaughan, Shirley Bassey, Judi Dench, Grace Jones, Barbra Streisand, Zarah Leander, Tiger Lillies, Joyce Castle, Ray Conniff, Glenn Close, Cher, Bryn Terfel, Plácido Domingo and many other artists recorded the song and it has become a jazz standard.
Meaning of title.
The "clowns" in the title do not refer to circus clowns. Instead, they symbolize fools, as Sondheim explained in a 1990 interview:
I get a lot of letters over the years asking what the title means and what the song's about; I never thought it would be in any way esoteric. I wanted to use theatrical imagery in the song, because she's an "actress," but it's not supposed to be a circus [...] [I]t's a theater reference meaning "if the show isn't going well, let's send in the clowns"; in other words, "let's do the jokes." I always want to know, when I'm writing a song, what the end is going to be, so "Send in the Clowns" didn't settle in until I got the notion, "Don't bother, they're here", which means that "We are the fools."
In a 2008 interview, Sondheim further clarified:
As I think of it now, the song could have been called "Send in the Fools". I knew I was writing a song in which Desirée is saying, "aren't we foolish" or "aren't we fools?" Well, a synonym for fools is clowns, but "Send in the Fools" doesn't have the same ring to it.
Context.
In an interview with Alan Titchmarsh, Judi Dench, who performed the role of Desirée in London, commented on the context of the song. The play is "a dark play about people who, at the beginning, are with wrong partners and in the end it is hopefully going to become right, and she (Desiree) mistimes her life in a way and realizes when she re-meets the man she had an affair with and had a child by (though he does not know that), that she loves him and he is the man she wants."
Some years before the play begins, Desirée was a young, attractive actress, whose passions were the theater and men. She lived her life dramatically, flitting from man to man. Fredrik was one of her many lovers and fell deeply in love with Desirée, but she declined to marry him. The play implies that when they parted Desirée may have been pregnant with his child.
A few months before the play begins, Fredrik married a beautiful woman who at 18 years old was much younger than he. In Act One, Fredrik meets Desirée again, and is introduced to her daughter, a precocious adolescent suggestively named Fredrika. Fredrik explains to Desirée that he is now married to the young woman, whom he loves, but who is still a virgin and refuses to have sex with him. Desirée and Fredrik then make love.
Act Two begins days later, and Desirée realizes that she truly loves Fredrik. She tells Fredrik that he needs to be rescued from his marriage, and she proposes to him. Fredrik explains to Desirée that he has been swept off the ground and is "in the air" in love with his beautiful, young wife, and apologizes for having misled her. Fredrik walks across the room, while Desirée remains sitting on the bed; as she feels both intense sadness and anger, at herself, her life and her choices, she sings, "Send in the Clowns." Not long thereafter, Fredrik's young wife runs away with his son, and he is free to accept Desirée's proposal, and the song is reprised as a coda.
Score.
History.
Sondheim wrote the lyrics and music over a two-day period during rehearsals for the play's Broadway debut, specifically for the actress Glynis Johns, who created the role of Desirée. According to Sondheim, "Glynis had a lovely, crystal voice, but sustaining notes was not her thing. I wanted to write short phrases, so I wrote a song full of questions" and the song's melody is within a small music range:
Lyrics.
The lyrics of the song are written in four verses and a bridge and sung by Desirée. As Sondheim explains, Desirée experiences both deep regret and furious anger:
"Send in the Clowns" was never meant to be a soaring ballad; it's a song of regret. And it's a song of a lady who is too upset and too angry to speak– meaning to sing for a very long time. She is furious, but she doesn't want to make a scene in front of Fredrik because she recognizes that his obsession with his 18-year-old wife is unbreakable. So she gives up; so it's a song of regret and anger, and therefore fits in with short-breathed phrases.
Meter and key.
The song was originally performed in the key of D flat major.
The song uses an unusual and complex meter, which alternates between 12/8 and 9/8. These are two complex compound meters that evoke the sense of a waltz used throughout the score of the show. Sondheim tells the story:
Styles.
"Send in the Clowns" is performed in two completely different styles: dramatic and lyric. The dramatic style is the theatrical performance by Desirée, and this style emphasizes Desirée's feelings of anger and regret, and the dramatic style acts as a cohesive part of the play. The lyric style is the concert performance, and this style emphasizes the sweetness of the melody and the poetry of the lyrics. Most performances are in concert, so they emphasize the beauty of the melody and lyrics.
Sondheim teaches both dramatic and lyric performers several important elements for an accurate rendition:
The dramatic performer must take on the character of Desirée: a woman who finally realizes that she has misspent her youth on the shallow life. She is both angry and sad, and both must be seen in the performance. Two important examples are the contrast between the lines, "Quick, send in the clowns" and "Well, maybe next year." Sondheim teaches that the former should be steeped in self-loathing, while the latter should emphasize regret. Thus, the former is clipped, with a break between "quick" and "send," while the latter "well" is held pensively.
Sondheim himself apologizes for flaws in his composition. For example, in the line, "Well, maybe next year," the melodic emphasis is on the word "year" but the dramatic emphasis must be on the word "next":
The word "next" is important: "Maybe "next" year" as opposed to ""this" year". [Desirée means,] "All right, I've screwed it up this year. Maybe next year I'll do something right in my life." So [it's] "well, maybe "next" year" even though it isn't accented in the music. This is a place where the lyric and the music aren't as apposite as they might be, because the important word is "next", and yet the accented word is "year". That's my fault, but [something the performer must] overcome.
Another example arises from Sondheim's roots as a speaker of American rather than British English: The line "Don't you love farce?" features two juxtaposed labiodental fricative sounds (the former ["v"] voiced, the latter ["f"] devoiced). American concert and stage performers will often fail to "breathe" and/or "voice" between the two fricatives, leading audiences familiar with British slang to hear "Don't you love arse?," misinterpreting the lyric or at the least perceiving an unintended double entendre. Sondheim agrees that "[i]t's an awkward moment in the lyric, but that "v" and that "f" should be separated."
In the line of the fourth verse, "I thought that you'd want what I want. Sorry, my dear," the performer must communicate the connection between the "want" and the "sorry". Similarly, Sondheim insists that performers separately enunciate the adjacent "t"s in the line, "There ought to be clowns."
Popular success.
In 1973, the musical, with the song, debuted on Broadway. The song become popular with theater audiences but had not become a pop hit. Sondheim explained how the song became a hit:
First of all, it wasn't a hit for two years. I mean, the first person to sing it was Bobby Short, who happened to see the show in Boston, and it was exactly his kind of song: He's a cabaret entertainer. And then my memory is that Judy Collins picked it up, but she recorded it in England; Sinatra heard it and recorded it. And between the two of them, they made it a hit.
In 1973, Frank Sinatra recorded "Send in the Clowns" on his comeback album, "Ol' Blue Eyes Is Back", which hit gold status. Gordon Jenkins arranged the song. It was also released as a single, with "Let Me Try Again" on side B.
Two years later in 1975, Judy Collins recorded "Send In the Clowns" and included it in her album, "Judith".
The song was released as a single, which soon became a major pop hit. It remained on the "Billboard" Hot 100 for 11 weeks in 1975, reaching Number 36.
Then, in 1977, the song again reached the Billboard Hot 100, where it remained for 16 weeks and reached Number 19.
At the Grammy Awards of 1976, the Judy Collins performance of the song was named "Song of the Year".
After Sinatra and Collins recorded the song, it was recorded by Kenny Rogers, Lou Rawls and many others.
In 1985, Sondheim added a verse for Barbra Streisand to use in her concert performances
and recording, which was featured on "The Broadway Album". In 1986, her version became a Number 25 Billboard Hot Adult Contemporary hit.
The song has become a jazz standard with performances by Count Basie, Sarah Vaughan, the Stan Kenton Orchestra and many others.
Recordings.
The song occurs on over 900 records by hundreds of performers in a wide variety of arrangements. Among these are:

</doc>
<doc id="29359" url="http://en.wikipedia.org/wiki?curid=29359" title="Sinhalese people">
Sinhalese people

The Sinhalese (Sinhala:සිංහල ජාතිය "Sinhala Jathiya") are an ethnic group native to the island of Sri Lanka. They constitute 75% of the Sri Lankan population and number greater than 15 million. The Sinhalese identity is based on language, historical heritage and religion. The Sinhalese speak Sinhala, an Indo-Aryan language, and are predominantly Theravada Buddhists, although a small percentage of Sinhalese follow branches of Christianity. The Sinhalese are mostly found in North central, Central, South, and West Sri Lanka. According to the Mahavamsa the Sinhalese are the descendants of the exiled Prince Vijaya who arrived from East India to Sri Lanka in 543 BCE. However, in folklore accounts the Sinhalese people predate this event, being the descendants of earlier inhabitants and Vijaya and other Indo Aryans migrants from India. Modern genetic investigations suggest that the Sinhalese are most closely related to the Bengali people.
Etymology.
The Sinhalese are also known as "Hela" or "Sinhala". These synonyms find their origins in the two words Sinha (meaning "lion") and Hela (meaning "pristine"). The name Sinhala translates to "lion people" and refers to the myths regarding the descent of the legendary founder of the Sinhalese people, the Prince Vijaya. The royal dynasty from ancient times on the island was the Sinha (Lion) royal dynasty and the word Sinha finds its origins here.Sri Lanka has got its old name "Sinhale" or "Heladiva" after Sinhalese who built up the civilisation of the island.The former names of the country "Serendib", "Seylan" and "Ceylon" have derived from the old name "Sinhale"
History.
Pre History.
The folklore of the Sinhalese people also speaks of many royal dynasties prior to the Sinhala royal dynasties: Manu, Tharaka, Mahabali, Raavana, etc. as per the oldest Indian epic poem Ramayan and consists many places in relation to this story such as Ram Setu, Sita eliya and falls (where Princes Sita stayed and bathed) Ravana Falls (where King Ravana Bathed and enjoyed), 
Ancient history.
Early recorded history of the Sinhalese is chronicled in two documents, the Mahavamsa, written in Pāli around the 4th century CE, and the much later Culavamsa (probably penned in the 13th century CE by the Buddhist monk Dhammakitti). These are ancient sources which cover the histories of the powerful ancient Sinhalese kingdoms of Anuradhapura and Polonnaruwa which lasted for 1500 years. The Mahavamsa describes the existence of fields of rice and reservoirs, indicating a well-developed agrarian society.
Early kingdoms.
Prince Vijaya and his 700 followers left Suppāraka, landed on the island at a site believed to be in the district of Chilaw, near modern-day Mannar, and founded the Kingdom of Tambapanni. It is recorded the Vijaya made his landing on the day of Buddha's death. Vijaya claimed Tambapanni his capital and soon the whole island come under this name. Tambapanni was originally inhabited and governed by Yakkhas, having their capital at Sirīsavatthu and their queen Kuveni. According to the Samyutta Commentary, Tambapanni was one hundred leagues in extent.
After landing in Tambapanni Vijaya met Kuveni the queen of the Yakkhas, who was disguised as a beautiful woman but was really a 'yakkini' (devil) named Sesapathi.
At the end of his reign Vijaya, having trouble choosing a successor, sent a letter to the city of his ancestors, Sinhapura, in order to invite his brother Sumitta to take over the throne. However Vijaya had died before the letter had reached its destination so the elected minister of the people Upatissa, the Chief government minister or prime minister and leading chief among the Sinhalese became regent and acted as regent for a year. After his coronation, which was held in the Kingdom of Tambapanni, he left it, building another one bearing his own name. While his was king, Upatissa established the new capital Upatissa Nuwara, in which the kingdom was moved to from the Kingdom of Tambapanni. When Vijaya's letter arrived Sumitta had already succeeded his father as king of his country, and so he sent his son Panduvasdeva to rule Upatissa Nuwara.
Upatissa Nuwara was seven or eight miles further north of the Kingdom of Tambapanni.
It was named after the regent king Upatissa, who was the prime minister of Vijaya, and was founded in 505 BC after the death of Vijaya and the end of the Kingdom of Tambapanni.
Anuradhapura.
In 377 BC, King Pandukabhaya (437–367 BC) moved the capital to Anuradhapura and developed it into a prosperous city. Anuradhapura (Anurapura) was named after the minister who first established the village and after a grandfather of Pandukabhaya who lived there. The name was also derived from the city's establishment on the auspicious asterism called Anura. Anuradhapura was the capital of all the monarchs who ruled from the dynasty.
Medieval history.
During the middle ages Sri Lanka was well known for its agricultural prosperity under the Parakramabahu in Polonnaruwa during which period the island was famous around the world as the rice mill of the east. Later in the 13th century the country's administrative provinces were divided into three independent kingdoms: Kingdom of Sitawaka, Kingdom of Kotte and the Kandyan kingdom. The invasion by Magha in the 13th century led to migrations by the Sinhalese to areas not under his control. This migration was followed by a period of conflict among the Sinhalese chiefs who tried to exert political supremacy. Parakramabahu VI in the 15th century was the only Sinhalese king during this time who could bring back the unity of the whole island. Trade also increased during this period, as Sri Lanka began to trade Cinnamon and a large number of Muslim traders were bought into the island.
In the 15th century a Kandyan Kingdom formed which divided the Sinhalese politically into low-country and up-country.
Modern history.
The Sinhalese have a stable birth rate and a population that has been growing at a slow pace relative to India and other Asian countries.
Culture.
Sinhalese culture is a unique one dating as far back as 2600 years and has been nourished by Theravada Buddhism. Its main domains are sculpture, fine arts, literature, dancing, poetry and a wide variety of folk beliefs and rituals traditionally. Ancient Sinhalese stone sculpture and inscriptions are known worldwide and is a main foreign attraction in modern tourism. Sigirirya is famous for its frescoes. Folk poems were sung by workers to accompany their work and narrate the story of their lives. Ideally these poems consisted of four lines and, in the composition of these poems, special attention had been paid to the rhyming patterns. Buddhist festivals are dotted by unique music using traditionally Sinhala instruments. More ancient rituals like tovils (devil exorcism) continue to enthral audiences today and often praised and admired the good and the power of Buddha and gods in order to exorcise the demons.
Dress.
Traditionally during recreation the Sinhalese wear a sarong ("sarama" in Sinhala). Men may wear a long-sleeved shirt with the sarong, while women wear a tight-fitting, short-sleeved jacket with a wrap-around called the "cheeththaya". In the more populated areas, Sinhalese men also wear Western-style clothing — wearing suits while the women wear skirts and blouses. For formal and ceremonial occasions women wear the traditional Kandyan (Osaria) style, which consists of a full blouse which covers the midriff completely, and is partially tucked in at the front. However, modern intermingling of styles has led to most wearers baring the midriff. The Kandyan style is considered as the national dress of Sinhalese women. In many occasions and functions, even the "saree" plays an important role in women's clothing and has become the de facto clothing for female office workers especially in government sector. An example of its use is the uniform of air hostesses of Sri Lankan Airlines.
Cuisine.
Sinhalese cuisine is one of the most complex cuisines of South Asia. Due to its proximity to South India, the cuisine of Sinhalese shows some influence, yet is in many ways quite distinct. As a major trade hub, it draws influence from colonial powers that were involved in Sri Lanka and by foreign traders. Rice, which is consumed daily, can be found at any occasion, while spicy curries are favourite dishes for lunch and dinner. Some of the Sri Lankan dishes have striking resemblance to Kerala cuisine, which could be due to the similar geographic and agricultural features with Kerala. A well-known rice dish with Sinhalese is Kiribath, meaning "Milk Rice." In addition to sambols, Sinhalese eat "Mallung", chopped leaves mixed with grated coconut and red onions. coconut milk is found in most Sri Lankan dishes to give the cuisine its unique flavour.
Sri Lanka has long been renowned for its spices. The best known is cinnamon which is native to Sri Lanka. In the 15th and 16th centuries, spice and ivory traders from all over the world who came to Sri Lanka brought their native cuisines to the island, resulting in a rich diversity of cooking styles and techniques. Lamprais rice boiled in stock with a special curry, accompanied by "frikkadels" (meatballs), all of which is then wrapped in a banana leaf and baked as a Dutch-influenced Sri Lankan dish. Dutch and Portuguese sweets also continue to be popular. British influences include roast beef and roast chicken. Also, the influence of the Indian cooking methods and food have played a major role in what Sri Lankans eat.
The island nation's cuisine mainly consists of boiled or steamed rice served with curry. This usually consists of a "main curry" of fish or chicken, as well as several other curries made with vegetables, lentils and even fruit curries. Side-dishes include pickles, chutneys and "sambols". The most famous of these is the coconut sambol, made of ground coconut mixed with chili peppers, dried Maldive fish and lime juice. This is ground to a paste and eaten with rice, as it gives zest to the meal and is believed to increase appetite.
Language and literature.
The Sinhalese speak Sinhala, also known as "Helabasa"; this language has two varieties, spoken and written. Sinhala is an Indo-Aryan language brought to Sri Lanka by northeast Indians who settled on the island in the 6th century BCE. Sinhala developed in a way different from the other Indo-Aryan languages because of the geographic separation from its Indo-Aryan sister languages. Sinhala was influenced by many languages, prominently Pali, the sacred language of Southern Buddhism, and Sanskrit. Many early Sinhala texts such as the "Hela Atuwa" were lost after their translation into Pali. Other significant Sinhala texts include "Amāvatura", "Kavu Silumina", "Jathaka Potha" and "Sala Liheeniya". Sinhala has also borrowed words from other Indian languages and the colonial languages Portuguese, Dutch, and English.
Sandesha Kavyas written by Buddhist priests of Sri Lanka are regarded as some of the most sophisticated and versatile works of literature in the world. The Sinhala language was mainly inspired by Sanskrit and Pali, and many words of the Sinhala language derive from these languages. Today some English words too have come in as a result of the British occupation during colonial times, and the exposure to foreign cultures through television and Hollywood movies. Additionally many Dutch and Portuguese words can be seen in the coastal areas.
Folk tales like "Mahadana Muttha saha Golayo" and "Kawate Andare" continue to entertain children today. "Mahadana Muttha" tells the tale of a fool cum Pundit who travels around the country with his followers ("Golayo") creating mischief through his ignorance. "Kawate Andare" tells the tale of a witty court jester and his interactions with the royal court and his son.
In the modern period, Sinhala writers such as Martin Wickremasinghe and G. B. Senanayake have drawn widespread acclaim. Other writers of repute include Mahagama Sekera and Madewela S. Ratnayake. Martin Wickramasinghe wrote the immensely popular children's novel "Madol Duwa". Munadasa Cumaratunga's "Hath Pana" is also widely known.
Art and architecture.
Many forms of Sri Lankan arts and crafts take inspiration from the Island's long and lasting Buddhist culture which in turn has absorbed and adopted countless regional and local traditions. In most instances Sri Lankan art originates from religious beliefs, and is represented in many forms such as painting, sculpture, and architecture. One of the most notable aspects of Sri Lankan art are caves and temple paintings, such as the found at Sigiriya, and religious paintings found in temples in Dambulla and Temple of the Tooth Relic in Kandy. Other popular forms of art have been influenced by both natives as well as outside settlers. For example, traditional wooden handicrafts and clay pottery are found around the hill country while Portuguese-inspired lacework and Indonesian-inspired Batik have become notable. It has many different and beautiful drawings.
Developed upon Indo-Aryan architectural skills in the late 6th century BCE Sinhalese people who lived upon greater kingdoms such as Anuradhapura and Polonnaruwa have built so many architectural examples such as Ruwanwelisaya, Jetavanaramaya - second tallest brick building in the ancient world after Pyramid of Giza, and Abayagiriya - third tallest brick building in the ancient world. And also with the ancient hydraulic technology which is also unique to Sinhalese people to build ancient tanks, systematic ponds with fountains moats and Irrigational reservoirs such as Parakrama Samudra, Kawdulla and Kandalama. Sigirya which consider as the 8th wonder of the world is a combination of natural and man made fortress, which consists so many architectural aspects.
Music.
Concerning popular music, Ananda Samarakoon developed the reflective and poignant Sarala gee style with his work in the late 1930s/early 1940s. He has been followed by artists of repute such as Sunil Shantha, W. D. Amaradeva, Premasiri Khemadasa, Nanda Malini, Victor Ratnayake, Austin Munasinghe, T. M. Jayaratne, Sanath Nandasiri, Sunil Edirisinghe, Neela Wickremasinghe, Gunadasa Kapuge, Malini Bulathsinghala and Edward Jayakody.
Film and theater.
Dramatist Ediriweera Sarachchandra revitalised the drama form with "Maname" in 1956. The same year, film director Lester James Peries created the artistic masterwork "Rekava" which sought to create a uniquely Sinhala cinema with artistic integrity. Since then, Peries and other directors like Vasantha Obeysekera, Dharmasena Pathiraja, Mahagama Sekera, W. A. B. de Silva, Dharmasiri Bandaranayake, Sunil Ariyaratne, Siri Gunasinghe, G. D. L. Perera, Piyasiri Gunaratne, Titus Thotawatte, D. B. Nihalsinghe, Ranjith Lal, Dayananda Gunawardena, Mudalinayake Somaratne, Ashoka Handagama, and Prasanna Vithanage have developed an artistic Sinhala cinema. Sinhala cinema is often made colourful with the incorporation of songs and dance adding more uniqueness to the industry.
Performing arts of the Sinhalese people can be categorised into few groups:
Martial arts.
Angampora is the traditional martial art of Sinhalese. It combines combat techniques, self-defense, sport, exercise and meditation. Key techniques observed in "Angampora" are: "Angam", which incorporates hand-to-hand fighting, and "Illangam", which uses indigenous weapons such as "Velayudaya", staves, knives and swords. Its most distinct feature is the use of pressure point attacks to inflict pain or permanently paralyse the opponent. Fighters usually make use of both striking and grappling techniques, and fight until the opponent is caught in a submission lock that they cannot escape. Usage of weapons is discretionary. Perimeters of fighting are defined in advance, and in some of the cases is a pit. "Angampora" became nearly extinct after the country came under British rule in 1815, but survived in a few families until the country regained the independence.
Science and education.
The Sinhalese have a long history of literacy and formal learning. Instruction in basic fields like writing and reading by Buddhist Monks pre-date the birth of Christ. This traditional system followed religious rule and was meant to foster Buddhist understanding. Training of officials in such skills as keeping track of revenue and other records for administrative purposes occurred under this institution.
Technical education such as the building of reservoirs and canals was passed down from generation to generation through home training and outside craft apprenticeships.
The arrival of the Portuguese and Dutch and the subsequent colonisation maintained religion as the centre of education though in certain communities under Catholic and Presbyterian hierarchy. The British in the 1800s initially followed the same course. Following 1870 however they began a campaign for better education facilities in the region. Christian missionary groups were at the forefront of this development contributing to a high literacy among Christians.
By 1901 schools in the South and the North were well tended. The inner regions lagged behind however. Also, English education facilities presented hurdles for the general populace through fees and lack of access.
Medicine.
It is believed that the Sinhala Medicine started in the Era of Great Hela King Ravana. King Ravana was an Emperor ruled in Sri Lanka. Traditional Sinhalese villages in early days had at least one chief Medical personnel called Weda Mahaththaya (Doctor). These people practice their clinical activities by inheritance. The Sinhala Medicine resembles some of Ayurvedic practices in contrast for some treatments they use Buddhist Chantings (Pirith) in order to strengthen the effectiveness.
According to the Mahavamsa, the ancient chronicle, Pandukabhaya of Sri Lanka (437 BC-367 BC) had lying-in-homes and Ayurvedic hospitals (Sivikasotthi-Sala) built in various parts of the country. This is the earliest documentary evidence we have of institutions specifically dedicated to the care of the sick anywhere in the world. Mihintale Hospital is the oldest in the world.
Religion.
The form of Buddhism in Sri Lanka is known as Theravada (school of elders). The Pali chronicles (e.g., the Mahavansa) claim that the Sinhalese as an ethnic group are destined to preserve and protect Buddhism. In 1988 almost 93% of the Sinhalese speaking population in Sri Lanka were Buddhist. Observations of current religious beliefs and practices demonstrate that Sinhalese as a religious community have complex worldview as Buddhists. Due to the proximity and on some occasions similarity of certain doctrines, there are many areas where Buddhists and Hindus share religious views and practices. This can lead to the opinion that Buddhists have adopted religious elements from Hindu traditions in their religious practices. Some of these practices may relate to ancient indigenous beliefs and traditions on spirits, worship of deities and godlings and some figures appear to demons. Some of these demonic figures are used in healing rituals and may be native to the island.
Prominent Sri Lankan anthropologists Gananath Obeyesekere and Kitsiri Malalgoda used the term "Protestant Buddhism" to describe a type of Buddhism that appeared among the Sinhalese in Sri Lanka as a response to Protestant Christian missionaries and their evangelical activities during the British colonial period. This kind of Buddhism involved emulating the Protestant strategies of organising religious practices. They saw the need to establish Buddhist schools for educating Buddhist youth and organising Buddhists with new organisations such as the Young Men's Buddhist Association, as well as printing pamphlets to encourage people to participate in debates and religious controversies to defend Buddhism.
There is a significant Sinhalese Christian community, in the maritime provinces of Sri Lanka. Christianity was brought to the Sinhalese by Portuguese, Dutch, and British missionary groups during their respective periods of rule. Sinhalese Christians mainly follow Roman Catholicism, followed by Protestantism. Their cultural centre is Negombo.
Religion is considered very important among the Sinhalese. According to a 2008 Gallup poll, 99% of Sri Lankans considered religion an important aspect of their daily lives.
Geographic distribution.
Sri Lanka.
Within Sri Lanka the majority of the Sinhalese reside in the South, Central and Western parts of the country. This coincides with the largest Sinhalese populations areas in Sri Lanka. Cities with a > 90% population include Hambantota, Galle, Gampaha, Kurunegala, Moneragala, Anuradhapura and Polonnaruwa.
Diaspora.
Sinhalese people have emigrated out to many countries for a variety of reasons. The larger diaspora communities are situated in the United Kingdom, Australia, United States and Canada among others. In addition to this there are many Sinhalese, who reside in the Middle East, Southeast Asia and Europe, temporarily in connection with employment and/or education. They are often employed as guest workers in the Middle East and professionals in the other regions.
The largest population centres of Sinhalese people are mainly situated in Europe, North America and Australia. The city of Melbourne contains just under half of the Sri Lankan Australians. The 2011 census recorded 86,412 Sri Lanka born in Australia. There are 73,849 Australians (0.4 of the population) who reported having Sinhalese ancestry in 2006. The Sinhalese language was also reported to be the 29th-fastest-growing language in Australia (ranking above Somali but behind Hindi and Belarusian). Sinhalese Australians have an exceptionally low rate of return migration to Sri Lanka. In the 2011 Canadian Census, 7,220 people identified themselves as of Sinhalese ancestry, out of 139,415 Sri Lankans. There are a small amount of Sinhalese people in India, scattered around the country, but mainly living in and around the northern and southern regions. Sri Lankan New Zealanders comprised 3% of the Asian population of New Zealand in 2001. The numbers arriving continued to increase, and at the 2006 census there were over 7,000 Sri Lankans living in New Zealand. The Sinhalese number about 12,000 in the USA. The New York City Metropolitan Area contains the largest Sri Lankan community in the United States, receiving the highest legal permanent resident Sri Lankan immigrant population, followed by Central New Jersey and the Los Angeles metropolitan area. Many Sinhalese have migrated to Italy since the 1970s. Italy was attractive to the Sinhalese due to perceived easier employment opportunities and entry, compared to other European countries. It is estimated that there are 30,000-33,000 Sinhalese in Italy. The major Sinhalese communities in Italy are located in Lombardia (In the districts Loreto and Lazzaretto), Milan, Lazio, Rome, Naples, and Southern Italy (Particularly Palermo, Messina and Catania). Though Sinhala people in particular and Sri Lankans in general have migrated to the UK over the centuries beginning from the colonial times, the number of Sinhalese people in the UK cannot be estimated accurately due to inadequacies of census in the UK. The UK government does not record statistics on the basis of language or ethnicity and all Sri Lankans are classified into one group as Asian British or Asian Other.
Ethnic origins.
Folklore and national mythology.
According to the Mahavamsa, the Sinhalese are descended from the exiled Prince Vijaya and his party of seven hundred followers who arrived on the island in 543 BCE. Vijaya and his followers were said to have arrived in Sri Lanka after being exiled from the city of Sinhapura in West Bengal, East India. The modern Sinhalese people as said in the Mahavamsa were found to be most closely related to the people of North-East India (Bengal). It is thought throughout Sri Lanka's history, since the founding of the Sinhalese in the 5th century BC that an influx of Indians from North India came to the island. This is further supported from the Sinhalese language being part of the Indo-Aryan language group which consists of other North Indian languages such as Hindi, Bengali and Marathi. Sinhalese derives from the Maharashtri Prakrit, along with Marathi, Konkani and Dhivehi.
Genetic studies.
Modern studies point towards a predominantly Bengali contribution and a minor Tamil and North Western Indian (Gujarati & Punjabi) contribution. 
It is debatable whether the Sri Lankan population have genetic links to Far East Asian populations however due to their close links to North East India, there is a likelihood of some traces of East Asian genes.

</doc>
<doc id="29364" url="http://en.wikipedia.org/wiki?curid=29364" title="Spiel des Jahres">
Spiel des Jahres

The Spiel des Jahres (German for "Game of the Year") is an award for board and card games, created in 1978 with the stated purpose of rewarding excellence in game design, and promoting top-quality games in the German market. It is thought that the existence and popularity of the award is one of the major drivers of the quality of games coming out of Germany. A Spiel des Jahres nomination can increase the typical sales of a game from 500-3000 copies to around 10,000; and the winner can usually expect to sell 300,000 to 500,000 copies.
Award criteria.
The award is given by a jury of German-speaking board game critics (from Germany, Austria, Switzerland), who review games released in Germany in the preceding twelve months. The games considered for the award are family-style games. War games, role-playing games, collectible card games, and other complicated, highly competitive, or hobbyist games are outside the scope of the award. Since 1989, there has been a separate award for children's games. 
On occasion, the jury has awarded a special prize for more complex games, such as Agricola in 2008 or World Without End in 2010. Prior to 2011, this award was an exceptional award, not necessarily awarded annually. In 2011, however, this practice was formalized when the jury created a new category for more complex games entitled "Kennerspiel des Jahres" (roughly "Connoisseur-Enthusiast Game of the Year"). Along with the nominations, the jury also gives a list of recommended games, and occasionally gives out special prizes for games which will not be considered for the main award.
The criteria on which a game is evaluated are,:
2014 awards.
The nominations for the 2014 awards were announced on May 19, 2014. The Children's Game of the Year was announced on June 23, and the Game of the Year and Connoisseur's Game of the Year were announced on July 14.
2013 awards.
The nominations for the 2013 awards were announced on May 21, 2013 and the Spiel and Kennerspiel winners were announced on July 8, 2013. The Kinderspiel (Children's) Game of the Year was announced on June 12, 2013.
2012 awards.
The nominations for the 2012 awards were announced on May 21, 2012 and winners on July 9, 2012.
2011 awards.
The nominations for the 2011 awards were announced on May 23, 2011 and winners on June 27, 2011. This was the first year the Connoisseur-gamer Game of the Year award was given, an award for more complex games.
2010 awards.
The nominations for the 2010 award were announced on May, 31 2010 and winner on June 28, 2010.
2009 awards.
The nominations for the 2009 award were announced on May 24, 2009 and winner on June 29, 2009.
2008 awards.
The nominations for the 2008 award were announced on May 25, 2008 and winner on June 30, 2008.
2007 awards.
The nominations for the 2007 award were announced on May 20, 2007. The five games nominated were:
Zooloretto was announced as winner on June 25, 2007.
2006 awards.
The nominations for the 2006 award were announced on May 28, 2006. The five games nominated were:
Thurn and Taxis was announced as winner on July 17, 2006.
Along with the nominations, the jury also assigned two special prizes for games which it felt were too demanding to count as 'family style' games.
2005 awards.
The nominations for the 2005 award were announced on May 8, 2005. The five games nominated were:
Niagara was announced to be the winner on June 27, 2005.

</doc>
<doc id="29365" url="http://en.wikipedia.org/wiki?curid=29365" title="Synthetic element">
Synthetic element

In chemistry, a synthetic element is a chemical element that does not occur naturally on Earth, and can only be created artificially. So far, 20 synthetic elements have been created (those with atomic numbers 99–118). All are unstable, decaying with half-lives ranging from a year to a few milliseconds.
Nine other elements were first created artificially and thus considered synthetic, but later discovered to exist naturally (in trace quantities) as well; among them plutonium—first synthesized in 1940—the one best known to laypeople, because of its use in atomic bombs and nuclear reactors.
Properties.
Synthetic elements are radioactive and decay rapidly into lighter elements—possessing half-lives so short, relative to the age of the Earth (which formed billions of years ago), that any atoms of these elements that may have existed when the Earth formed have long since decayed. Atoms of synthetic elements only occur on Earth as the product of atomic bombs or experiments that involve nuclear reactors or particle accelerators, via nuclear fusion or neutron absorption.
Atomic mass for natural life is based on weighted average abundance of natural isotopes that occur in the Earth's crust and atmosphere. For "synthetic" elements, the isotope depends on the means of synthesis, so the concept of natural isotope abundance has no meaning. Therefore, for synthetic elements the total nucleus count (protons plus neutrons) of the most stable isotope, i.e. the isotope with the longest half-life—is listed in brackets as the atomic mass.
Not all radioactive elements are synthetic. For instance, uranium and thorium have no stable isotopes but occur naturally in the Earth's crust and atmosphere. Unstable elements such as polonium, radium, and radon—which form through the decay of uranium and thorium—are also found in nature, despite their short half-lives.
History.
The first element discovered through synthesis was technetium—its discovery being definitely confirmed in 1936. This discovery filled a gap in the periodic table, and the fact that no stable isotopes of technetium exist explains its natural absence on Earth (and the gap). With the longest-lived isotope of technetium, Tc-98, having a 4.2-million-year half-life, no technetium remains from the formation of the Earth. Only minute traces of technetium occur naturally in the Earth's crust—as a spontaneous fission product of uranium-238 or by neutron capture in molybdenum ores—but technetium is present naturally in red giant stars.
The first discovered synthetic elements were einsteinium and fermium in 1952, by a team of scientists led by Albert Ghiorso in 1952 while studying the radioactive debris from the detonation of the first hydrogen bomb. The isotopes discovered were einsteinium-253, with a half-life of 20.5 days, and fermium-255, with a half-life of about 20 hours.
The discoveries of mendelevium, lawrencium, and nobelium followed.
Then, during the height of the cold war, the Soviet Union and United States independently discovered rutherfordium and dubnium. The naming and credit for discovery of those elements remained unresolved for many years but eventually shared credit was recognized by IUPAC/IUPAP in 1992. In 1997, IUPAC decided to give dubnium its current name honoring the city of Dubna where the Russian team made their discoveries since American-chosen names had already been used for many existing synthetic elements, while the name "rutherfordium" (chosen by the American team) was accepted for element 104.
No element with an atomic number greater than 98, (Californium), has been proven to have any use outside of scientific research. Synthetic elements with an atomic mass greater than 92 have
been shown to have little or no use commercially, other than Plutonium, Neptunium, Americium, and Californium. Elements 99-118 are generally extremely short half-lived elements with no use other than research purposes.
List of synthetic elements.
The following elements do not occur naturally on Earth. All are transuranium elements and have atomic numbers of 99 and higher.
Other elements usually produced through synthesis.
All elements with atomic numbers 1 through 98 are naturally occurring at least in trace quantities, but the following elements are usually produced through synthesis. Except for francium, they were all discovered through synthesis before being found in nature.

</doc>
<doc id="29366" url="http://en.wikipedia.org/wiki?curid=29366" title="Shoghi Effendi">
Shoghi Effendi

Shoghí Effendí Rabbání (March 1, 1897 – November 4, 1957), better known as Shoghi Effendi, was the Guardian and appointed head of the Bahá'í Faith from 1921 until his death in 1957. After the death of `Abdu'l-Bahá in 1921, the leadership of the Bahá'í community changed from that of a single individual to an administrative order with executive and legislative branches, the head of each being the Guardianship and the Universal House of Justice. Shoghi Effendi was referred to as the "Guardian", and had the authority to interpret the writings of the three central figures of the religion and define the sphere of legislative authority. His writings are effectively limited to commentaries on the works of the central figures, and broad directives for the future.
Future hereditary Guardians were permitted in the Bahá'í scripture by appointment from one to the next, but a prerequisite that appointees be male descendants of Bahá'u'lláh left no suitable living candidates, and Shoghi Effendi died without making an appointment. The Universal House of Justice, the only institution authorized to adjudicate on situations not covered in scripture, later announced that it could not legislate to make possible the appointment of a successor to Shoghi Effendi. Shoghi was the first and last person acknowledged as Guardian of the Bahá'í Faith.
Background.
Born in `Akká in the Sanjak of Acre in March 1897, Shoghi Effendi was related to the Báb through his father, Mírzá Hádí Shírází, and to Bahá'u'lláh through his mother, Ḍíyá'íyyih Khánum, the eldest daughter of `Abdu'l-Bahá. `Abdu'l-Bahá, who provided much of his initial training, greatly influenced Shoghi Effendi from the early years of his life. Shoghi Effendi learned prayers from his grandfather `Abdu'l-Bahá, who encouraged him to chant. `Abdu'l-Bahá also insisted that people address the child as "Shoghi Effendi", ("Effendi" signifies "Sir"), rather than simply as "Shoghi", as a mark of respect towards him.
From his early years, Shoghi Effendi was introduced to the suffering which accompanied the Bahá'ís in Akká, including the attacks by Mírzá Muhammad `Alí against `Abdu'l-Bahá. As a young boy, he was aware of the desire of Sultán `Abdu'l-Hamíd (reigned 1876-1909) to banish `Abdu'l-Bahá to the deserts of North Africa where he was expected to perish. At one point, Shoghi Effendi was warned not to drink coffee in the homes of any of the Bahá'ís in the fear that he would be poisoned.
Tablet from `Abdu'l-Bahá.
As the eldest grandson of `Abdu'l-Bahá, Shoghi Effendi from his earliest childhood had a special relationship with his grandfather. Dr. Baghdadi reports that when Shoghi Effendi was only 5 years old, he pestered his grandfather to write a tablet for him, which was common practice for `Abdu'l-Bahá. He wrote the following for his grandson:
He is God!
O My Shoghi, I have no time to talk, leave me alone! You said 'write' - I have written. What else should be done? Now is not the time for you to read and write, it is the time for jumping about and chanting 'O My God!', therefore memorize the prayers of the Blessed Beauty and chant them that I may hear them, because there is no time for anything else.
Shoghi Effendi then set out to memorize a number of prayers, and chanted them as loud as he could. This caused family members to ask `Abdu'l-Bahá to quieten him down, a request which he apparently refused.
Education.
Shoghi Effendi received his early education at home with the other children in the household, then attended a French Christian Brothers school in Haifa, and later boarded at another Catholic school in Beirut. Shoghi Effendi later attended the Syrian Protestant College (later known as the American University of Beirut) for his final years of high school and first years of university, where he earned an arts degree in 1918. He reports being very unhappy in school and often returned on vacations to Haifa to spend time with `Abdu'l-Bahá.
During his studies, he dedicated himself to mastering English — adding this language to the Persian, Turkish, Arabic and French languages in which he was already fluent - so that he could translate the letters of `Abdu'l-Bahá and serve as his secretary.
After studying at the American University of Beirut he later went to Balliol College, Oxford in England, where he matriculated in "Economics and Social Sciences", while still perfecting his translation skills.
Death of `Abdu'l-Bahá and Guardianship.
The issue of successorship to `Abdu'l Bahá was in the minds of early Bahá'ís, and although the Universal House of Justice was an institution mentioned by Bahá'u'lláh, the institution of the Guardianship was not introduced until the Will and Testament of `Abdu'l-Bahá was publicly read after his death. Bahá'u'lláh's own will mentions Mírzá Muhammad `Alí as following `Abdu'l Bahá in leadership, but Bahá'ís excommunicated him as a covenant-breaker and shunned him.
While studying in England, on 29 November 1921, the news of `Abdu'l-Bahá's death reached Shoghi Effendi, which, according to Wellesley Tudor Pole, the deliverer of the cable, left him "in a state of collapse". After spending a couple of days with John Esslemont, and after some passport difficulties, he sailed from England on December 16 and arrived in Haifa on 29 December. A few days later he opened `Abdu'l-Bahá's Will and Testament, which was addressed to Shoghi Effendi.
In the will Shoghi Effendi found that he had been designated as "the Sign of God, the chosen branch, the Guardian of the Cause of God". He also learned that he had been designated as this when he was still a small child. As Guardian he was appointed as head of the religion, someone whom the Bahá'ís had to look to for guidance. `Abdu'l-Bahá's Will and Testament is considered one of the three charters of the Bahá'í administrative order, and in it `Abdu'l-Bahá laid down the authority of the Guardian and the Universal House of Justice, the elected governing body of the Bahá'í Faith that had been written about by Bahá'u'lláh, and had not yet been established:
...The Guardian of the Cause of God, as well as the Universal House of Justice to be universally elected and established, are both under the care and protection of the Abha Beauty... Whatsoever they decide is of God. Whoso obeyeth him not, neither obeyeth them, hath not obeyed God; whoso rebelleth against him and against them hath rebelled against God; whoso opposeth him hath opposed God; whoso contendeth with them hath contended with God; whoso disputeth with him hath disputed with God; whoso denieth him hath denied God; whoso disbelieveth in him hath disbelieved in God; whoso deviateth, separateth himself and turneth aside from him hath in truth deviated, separated himself and turned aside from God.
Shoghi Effendi later expressed to his wife and others that he had no foreknowledge of the existence of the Institution of Guardianship, least of all that he was appointed as Guardian. The most he expected was perhaps, because he was the eldest grandson, `Abdu'l-Bahá might have left instructions as to how the Universal House of Justice was to be elected and he might have been designated as Convener of the gathering which would elect it.
Accomplishments.
From the time of his appointment as Guardian until his death the Bahá'í Faith grew from 100,000 to 400,000 members, capitalizing on prior growth and setting the stage for more, and the countries in which Bahá'ís had representation went from 35 to 250. As Guardian and head of the religion, while Shoghi Effendi communicated his vision to the Bahá'ís of the world through his numerous letters and his meetings with pilgrims to Palestine. During the 1920s he first started to systematize and extend the Bahá'í administration throughout the world; the Bahá'í community was relatively small and undeveloped when he assumed leadership of the religion, and he strengthened and developed it over many years to support the administrative structure envisioned by `Abdu'l-Bahá. Under Shoghi Effendi's direction, National Spiritual Assemblies were formed, and many thousands of Local Spiritual Assemblies were created. During the 1930s he worked on projects translating the works of Bahá'u'lláh into English. Starting in 1937, he set into motion a series of systematic plans to establish Bahá'í communities in all countries. A Ten Year Crusade was carried out from 1953 to 1963 with the aim of electing the Universal House of Justice as its paramount aim. Starting in the late 1940s, after the independence of Israel, he started to develop the Bahá'í World Centre in Haifa, including the construction of the superstructure of the Shrine of the Báb and the building of the International Archives as well as beautifying the gardens at Bahji, where the Shrine of Bahá'u'lláh is located, as well as developing plans and resources to raise several of the continental Bahá'í Houses of Worship around the world; these plans continued through the 1950s. In the 1950s he also continued building the Bahá'í administration, establishing in 1951 the International Bahá'í Council to act as a precursor to the Universal House of Justice, as well as appointing 32 living Hands of the Cause — Bahá'ís appointed to the highest rank of service available, whose main function was to propagate and protect the religion. He also acted as the official representative of the religion to legal authorities in Israel as well as designated other representatives to work with the UN.
In a more secular cause, prior to World War II he supported the work of restoration-forester Richard St. Barbe Baker to reforest Palestine, introducing him to religious leaders from the major faiths of the region, from whom backing was secured for reforestation.
Translations and writings.
In his lifetime, Shoghi Effendi translated into English many of the writings of the Báb, Bahá'u'lláh and `Abdu'l-Bahá, including the "Hidden Words" in 1929, the "Kitáb-i-Íqán" in 1931, "Gleanings" in 1935 and "Epistle to the Son of the Wolf" in 1941. He also translated such historical texts as "The Dawn-breakers". His significance is not just that of a translator, but he was also the designated and authoritative interpreter of the Bahá'í writings. His translations therefore are a guideline for all future translations of the Bahá'í writings.
The vast majority of his writings were in the style of letters with Bahá'ís from all parts of the globe. These letters, of which 17,500 have been collected thus far and are believed to number of 30,000, ranged from routine correspondence regarding the affairs of Bahá'ís around the world to lengthy letters to the Bahá'ís of the world addressing specific themes. Some of his longer letters include "World Order of Bahá'u'lláh", regarding the nature of Bahá'í administration, "Advent of Divine Justice", regarding teaching the religion, and "Promised Day is Come" regarding Bahá'u'lláh's letters to world leaders. Other letters included statements on Bahá'í beliefs, history, morality, principles, administration and law. He also wrote obituaries of some distinguished Bahá'ís. Many of his letters to individuals and assemblies have been compiled into several books which stand out as significant sources of literature for Bahá'ís around the world.
The only actual book he ever wrote was "God Passes By" in 1944 to commemorate the centennial anniversary of the religion. The book, which is in English, is an interpretive history of the first century of the Bábí and Bahá'í Faiths. A shorter Persian language version was also written.
Leadership.
As a young student of twenty-four, Shoghi Effendi was initially shocked at the appointment as Guardian. He was also mourning the death of his grandfather to whom he had great attachment. The trauma of this culminated in him making retreats to the Swiss Alps. However, despite his youth, Shoghi Effendi had a clear idea of the goal he had for the religion. Oxford educated and Western in his style of dress, Shoghi Effendi was a stark contrast to his grandfather `Abdu'l-Bahá. He distanced himself from the local clergy and notability, and travelled little to visit Bahá’ís unlike his grandfather. Correspondence and pilgrims were the way that Shoghi Effendi conveyed his messages. His talks are the subject to a great number of "pilgrim notes".
He also was concerned with matters dealing with Bahá'í belief and practice — as Guardian he was empowered to interpret the writings of Bahá'u'lláh and `Abdu'l-Bahá, and these were authoritative and binding, as specified in `Abdu'l-Bahá's will. His leadership style was however, quite different from that of `Abdu'l-Bahá, in that he signed his letters to the Bahá'ís as "your true brother", and he did not refer to his own personal role, but instead to the institution of the guardianship. He requested that he be referred in letters and verbal addresses always as Shoghi Effendi, as opposed to any other appellation. He also distanced himself as a local notable. He was critical of the Bahá'ís referring to him as a holy personage, asking them not to celebrate his birthday or have his picture on display.
Private life.
Shoghi Effendi's personal life was largely subordinate to his work as Guardian of the religion. His lack of secretarial support with the mass of correspondence had left a pattern of hard work in Haifa interspersed with occasional summer breaks to Europe — in the early years often to the Swiss Alps. In 1929 and 1940 he also travelled through Africa from south to north.
Shoghi Effendi had a great love for the English language. Carrying with him a short note-book, he would note down words and sentences he liked. He was an avid fan of music and English literature, and enjoyed reading the King James Bible. His favourite book was "The History of the Decline and Fall of the Roman Empire". Whilst a young man studying in Oxford, Shoghi Effendi was part of a debating society and enjoyed playing tennis. He was noted for speaking English in subtle received pronunciation, and Persian in an Isfahani dialect, inherited from his grandmother.
Shoghi Effendi held Iranian nationality throughout his life and traveled on an Iranian passport, although he never visited Iran.
Marriage.
In March 1937, Shoghi Effendi married Mary Maxwell, entitled Rúhíyyih Khanum, a Canadian. She was the only child of May Maxwell, a disciple of `Abdu'l-Bahá, and William Sutherland Maxwell, a Canadian architect. Shoghi Effendi had first met Mary as a girl when she came on pilgrimage with her mother in 1923. Mary was an active Bahá'í teacher and youth worker, and a letter written to Shoghi Effendi described her as "a beautiful and most refreshing girl to know". Whilst on her third pilgrimage in 1937 the two began a discreet courtship. Then herself 26 years old, Mary was a tall, athletic woman. The couple married in the room of Bahíyyih Khánum in the House of `Abdu'l-Bahá in Haifa. The ceremony was a short, simple and quiet one of which Rúhíyyih Khanum wore black. Very few knew the wedding was taking place apart from the witnesses and a small group of residents of Haifa. Therefore the marriage came as a great surprise to the world-wide Bahá'í community when the mother of Shoghi Effendi cabled the Bahá'ís:
Announce Assemblies celebration marriage beloved Guardian. Inestimable honour conferred upon handmaid of Bahá'u'lláh Ruhiyyih Khanum Miss Mary Maxwell. Union of East and West proclaimed by Bahá'í Faith cemented. Ziaiyyih mother of Guardian.
While Shoghi Effendi and Rúhíyyih Khanum never had children, Rúhíyyih Khanum became his constant companion and helpmate; in 1941, she became Shoghi Effendi's principal secretary in English. After Shoghi Effendi's death, Rúhíyyih Khanum published parts of her personal diaries to show glimpses of Shoghi Effendi's life. She recalls a great deal of pain and suffering caused by his immediate family, and Bahá'ís in Haifa.
If the friends only knew how the Master and the Guardian both suffered through the calibre of the local Bahá'ís. Some of them were good. But some were rotten. It's as if, when someone was unsound in the Covenant, they attacked the very body of the Manifestation, or the Exemplar, or the Guardian. I have seen this. It is like poison. He recovers from it, but it causes him untold suffering and it was from such things that the Master described Himself in His Will as 'this broken-winged bird.'
They [`Abdu'l-Baha's family] have gone a long way to crushing every ounce of spirit out of the Guardian. By nature he is cheerful and energetic... But the perpetual strife of life with the Master's family... have clouded over him... Shoghi Effendi has been abused. That is the only word for it, abused, abused, abused. By now he has reached the point of a man fighting with his back to the wall. He says he will fight it out to the last round.
Throughout Shoghi Effendi's life, nearly all remaining family members and descendants of `Abdu'l-Bahá were expelled by him as covenant-breakers when they didn't abide by Shoghi Effendi's request to cut contact with covenant-breakers, as specified by `Abdu'l-Bahá. Other branches of Bahá'u'lláh's family had already been declared Covenant-breakers in `Abdu'l-Bahá's Will and Testament. At the time of his death, there were no living descendants of Bahá'u'lláh that remained loyal to him.
Unexpected death.
Shoghi Effendi's death came unexpectedly in London, on 4 November 1957, as he was travelling to Britain and caught the Asian Flu, during the pandemic which killed two million worldwide, and he is buried there in New Southgate Cemetery. His wife sent the following cable:
Shoghi Effendi beloved of all hearts sacred trust given believers by Master passed away sudden heart attack in sleep following Asiatic flu. Urge believers remain steadfast cling institution Hands lovingly reared recently reinforced emphasized by beloved Guardian. Only oneness heart oneness purpose can befittingly testify loyalty all National Assemblies believers departed Guardian who sacrificed self utterly for service Faith.——Ruhiyyih
According to the framework of the Will and Testament of `Abdu'l-Bahá, it was not possible to appoint a successor, and the legislative body "possessing the exclusive right to legislate on matters not explicitly revealed" was not yet established in the world. Furthermore, Shoghi Effendi had left no will as attested to by the Hands of the Cause, who were required to ratify his selection. All of the 27 living Hands of the Cause unanimously signed a statement shortly after the death of Shoghi Effendi stating that he had died "without having appointed his successor..." 
Ministry of the Custodians.
In Shoghi Effendi's final message to the Baha'i World, dated October 1957, he named the Hands of the Cause of God, "the Chief Stewards of Bahá'u'lláh's embryonic World Commonwealth." Consequently, following the death of Shoghi Effendi, the Bahá'í Faith was temporarily stewarded by the Hands of the Cause, who elected among themselves 9 "Custodians" to serve in Haifa as the head of the Faith. They reserved to the "entire body of the Hands of the Cause" the responsibility to determine the transition of the International Bahá'í Council into the Universal House of Justice, and that the Custodians reserved to themselves the authority to determine and expel Covenant-breakers.
This stewardship oversaw the execution of the final years of Shoghi Effendi's ordinances of the ten year crusade (which lasted until 1963) culminating and transitioning to the election and establishment of the Universal House of Justice, at the first Baha'i World Congress in 1963.
Election of the Universal House of Justice.
At the end of the Ten Year Crusade, planned by Shoghi Effendi and concluding in 1963, the Universal House of Justice was first elected. As its first order of business, the Universal House of Justice evaluated the situation caused by the fact that the Guardian had not appointed a successor. It determined that under the circumstances, given the criteria for succession described in the Will and Testament of `Abdu'l-Bahá, there was no legitimate way for another Guardian to be appointed. Therefore, although the Will and Testament of `Abdu'l-Bahá leaves provisions for a succession of Guardians, Shoghi Effendi remains the first and last occupant of this office.

</doc>
<doc id="29368" url="http://en.wikipedia.org/wiki?curid=29368" title="Slope">
Slope

In mathematics, the slope or gradient of a line is a number that describes both the "direction" and the "steepness" of the line. Slope is often denoted by the letter "m".
Slope is calculated by finding the ratio of the "vertical change" to the "horizontal change" between (any) two distinct points on a line. Sometimes the ratio is expressed as a quotient ("rise over run"), giving the same number for every two distinct points on the same line. A line that is decreasing has a negative "rise". The line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan.
The rise of a road between two points is the difference between the altitude of the road at those two points, say "y"1 and "y"2, or in other words, the rise is ("y"2 − "y"1) = Δ"y". For relatively short distances - where the earth's curvature may be neglected, the run is the difference in distance from a fixed point measured along a level, horizontal line, or in other words, the run is ("x"2 − "x"1) = Δ"x". Here the slope of the road between the two points is simply described as the ratio of the altitude change to the horizontal distance between any two points on the line.
In mathematical language, the slope "m" of the line is
The concept of slope applies directly to grades or gradients in geography and civil engineering. Through trigonometry, the grade "m" of a road is related to its angle of incline "θ" by the tangent function
Thus, a 45° rising line has a slope of +1 and a 45° falling line has a slope of −1.
As a generalization of this practical description, the mathematics of differential calculus defines the slope of a curve at a point as the slope of the tangent line at that point. When the curve given by a series of points in a diagram or in a list of the coordinates of points, the slope may be calculated not at a point but between any two given points. When the curve is given as a continuous function, perhaps as an algebraic formula, then the differential calculus provides rules giving a formula for the slope of the curve at any point in the middle of the curve.
This generalization of the concept of slope allows very complex constructions to be planned and built that go well beyond static structures that are either horizontals or verticals, but can change in time, move in curves, and change depending on the rate of change of other factors. Thereby, the simple idea of slope becomes one of the main basis of the modern world in terms of both technology and the built environment.
Definition.
The slope of a line in the plane containing the "x" and "y" axes is generally represented by the letter "m", and is defined as the change in the "y" coordinate divided by the corresponding change in the "x" coordinate, between two distinct points on the line. This is described by the following equation:
Given two points ("x"1,"y"1) and ("x"2,"y"2), the change in "x" from one to the other is "x"2 − "x"1 ("run"), while the change in "y" is "y"2 − "y"1 ("rise"). Substituting both quantities into the above equation generates the formula:
The formula fails for a vertical line, parallel to the "y" axis (see Division by zero), where the slope can be taken as infinite, so the slope of a vertical line is considered undefined.
Examples.
Suppose a line runs through two points: "P" = (1, 2) and "Q" = (13, 8). By dividing the difference in "y"-coordinates by the difference in "x"-coordinates, one can obtain the slope of the line:
As another example, consider a line which runs through the points (4, 15) and (3, 21). Then, the slope of the line is 
Algebra and geometry.
Examples.
For example, consider a line running through the points (2,8) and (3,20). This line has a slope, "m", of 
Consider the two lines: "y" = -3"x" + 1 and "y" = -3 "x" - 2. Both lines have slope "m" = -3. They are not the same line. So they are parallel lines.
Consider the two lines "y" = -3"x" + 1 and "y" = "x"/3 - 2. The slope of the first line is "m"1 = -3. The slope of the second line is "m"2 = 1/3. The product of these two slopes is -1. So these two lines are perpendicular.
Slope of a road or railway.
There are two common ways to describe the steepness of a road or railroad. One is by the angle between 0° and 90° (in degrees), and the other is by the slope in a percentage. See also steep grade railway and rack railway.
The formulae for converting a slope given as a percentage into an angle in degrees and vice versa are: 
where "angle" is in degrees and the trigonometric functions operate in degrees. For example, a slope of 100% or 1000‰ is an angle of 45°.
A third way is to give one unit of rise in say 10, 20, 50 or 100 horizontal units, e.g. 1:10. 1:20, 1:50 or 1:100 (or "1 in 10", "1 in 20" etc.) Note that 1:10 is steeper than 1:20. For example, steepness of 20% means 1:5 or an incline with angle 11,3°.
Calculus.
The concept of a slope is central to differential calculus. For non-linear functions, the rate of change varies along the curve. The derivative of the function at a point is the slope of the line tangent to the curve at the point, and is thus equal to the rate of change of the function at that point.
If we let Δ"x" and Δ"y" be the distances (along the "x" and "y" axes, respectively) between two points on a curve, then the slope given by the above definition,
is the slope of a secant line to the curve. For a line, the secant between any two points is the line itself, but this is not the case for any other type of curve.
For example, the slope of the secant intersecting "y" = "x"2 at (0,0) and (3,9) is 3. (The slope of the tangent at x = 3⁄2 is also 3—"a" consequence of the mean value theorem.)
By moving the two points closer together so that Δ"y" and Δ"x" decrease, the secant line more closely approximates a tangent line to the curve, and as such the slope of the secant approaches that of the tangent. Using differential calculus, we can determine the limit, or the value that Δ"y"/Δ"x" approaches as Δ"y" and Δ"x" get closer to zero; it follows that this limit is the exact slope of the tangent. If "y" is dependent on "x", then it is sufficient to take the limit where only Δ"x" approaches zero. Therefore, the slope of the tangent is the limit of Δ"y"/Δ"x" as Δ"x" approaches zero, or "dy"/"dx". We call this limit the derivative.
Its value at a point on the function gives us the slope of the tangent at that point. For example, let "y"="x"2. A point on this function is (-2,4). The derivative of this function is d"y"/d"x"=2"x". So the slope of the line tangent to "y" at (-2,4) is 2·(-2) = -4. The equation of this tangent line is: "y"-4=(-4)("x"-(-2)) or "y" = -4"x" - 4.
Other generalizations.
The concept of slope can be generalized to functions of more than one variable and is more often referred to as gradient.

</doc>
<doc id="29369" url="http://en.wikipedia.org/wiki?curid=29369" title="Sex organ">
Sex organ

A sex organ or primary sexual characteristic, as narrowly defined, is any anatomical part of the body involved in sexual reproduction and constituting the reproductive system in a complex organism, especially the external sex organs; the external sex organs are also commonly referred to as the genitalia or genitals.
Mosses, ferns, and some similar plants have gametangia for reproductive organs, which are part of the gametophyte. The flowers of flowering plants produce pollen and egg cells, but the sex organs themselves are inside the gametophytes within the pollen and the ovule. Coniferous plants likewise produce their sexually reproductive structures within the gametophytes contained within the cones and pollen. The cones and pollen are not themselves sexual organs.
Terminology.
The Latin term "genitalia", sometimes anglicized as "genitals", is used to describe the externally visible sex organs, known as "primary genitalia" or "external genitalia": in male mammals, the penis and scrotum; and in female mammals, the clitoris and vulva.
The other, hidden sex organs are referred to as the "secondary genitalia" or "internal genitalia." The most important of these are the gonads, a pair of sex organs, specifically the testes in the male or the ovaries in the female. Gonads are the true sex organs, generating reproductive gametes containing inheritable DNA. They also produce most of the primary hormones that affect sexual development, and regulate other sexual organs and sexually differentiated behaviors.
A more ambiguously defined term is "erogenous zone", subjectively, any portion of the body that when stimulated produces erotic sensation, but always prominently including the genitalia.
In general zoology, given the great variety in organs, physiologies, and behaviors involved in copulation, male genitalia are more strictly defined as "all male structures that are inserted in the female or that hold her near her gonopore during sperm transfer"; female genitalia are defined as "those parts of the female reproductive tract that make direct contact with male genitalia or male products (sperm, spermatophores) during or immediately after copulation".
Mammals.
External and internal organs.
The visible portion of the mammalian genitals for males consists of the scrotum and penis; for females, it consists of the labia, clitoris, and vagina.
In placental mammals, females have two genital orifices, the vagina and urethra, while males have only one, the urethra. Male and female genitals have many nerve endings, resulting in pleasurable and highly sensitive touch. In most human societies, particularly in conservative ones, genitals are considered a public indecency and sometimes even illegal if left uncovered in public.
In mammals, sex organs include:
Development.
In typical prenatal development, sexual organs originate from a common anlage anatomy during early gestation and differentiate into male or female variations. The SRY gene, usually located on the Y chromosome and encoding the testis determining factor, determines the direction of this differentiation. The absence of it allows the gonads to continue to develop into ovaries.
Thereafter, the development of the internal reproductive organs and the external genitalia is determined by hormones produced by certain fetal gonads (ovaries or testes) and the cells' response to them. The initial appearance of the fetal genitalia (a few weeks after conception) looks basically feminine: a pair of "urogenital folds" with a small protuberance in the middle, and the urethra behind the protuberance. If the fetus has testes, and if the testes produce testosterone, and if the cells of the genitals respond to the testosterone, the outer urogenital folds swell and fuse in the midline to produce the scrotum; the protuberance grows larger and straighter to form the penis; the inner urogenital swellings grow, wrap around the penis, and fuse in the midline to form the penile urethra.
Each sexual organ in one sex has a homologous counterpart in the other one. See a list of homologues of the human reproductive system. In a larger perspective, the whole process of sexual differentiation also includes development of secondary sexual characteristics such as patterns of pubic and facial hair and female breasts that emerge at puberty. Furthermore, differences in brain structure arise, affecting, but not absolutely determining, behavior.
Intersex is the development of genitalia somewhere between typical male and female genitalia. Once the child is born, the parents are faced with decisions that are often difficult to make, such as whether or not to modify the genitalia, assign the child as male or female, or leave the genitalia as is. Some parents allow their doctors to choose. If they do decide to modify the genitalia, they have approximately a 50% chance of getting genitalia that will match the child's gender identity. If they pick the wrong one, their child may begin to show symptoms of transsexualism, which can lead them to a life of discomfort until they are able to remedy the issue.
Because of the strong sexual selection affecting the structure and function of genitalia, they form an organ system that evolves faster that any other. A great variety of genital form and function may therefore be found among animals.
Plants.
The life cycle of land plants involves alternation of generations between a sporophyte and a haploid gametophyte. The gametophyte produces sperm and/or egg cells by mitosis. The sporophyte produces spores by meiosis which in turn develop into gametophytes. Any sex organs that are produced by the plant will develop on the gametophyte. The seed plants, which include conifers and flowering plants have small gametophytes that develop inside the pollen grains (male) and the ovule (female).
Flowering plants.
Sexual reproduction in flowering plants involves the union of the male and female germ cells, sperm and egg cells respectively. Pollen is produced in stamens, and is carried to the pistil, which has the ovary at its base where fertilization can take place. Within each pollen grain is a male gametophyte which consists of only three cells. In most flowering plants the female gametophyte within the ovule consists of only seven cells. Thus there are no sex organs as such.

</doc>
<doc id="29370" url="http://en.wikipedia.org/wiki?curid=29370" title="Snake">
Snake

Snakes are elongated, legless, carnivorous reptiles of the suborder Serpentes that can be distinguished from legless lizards by their lack of eyelids and external ears. Like all squamates, snakes are ectothermic, amniote vertebrates covered in overlapping scales. Many species of snakes have skulls with several more joints than their lizard ancestors, enabling them to swallow prey much larger than their heads with their highly mobile jaws. To accommodate their narrow bodies, snakes' paired organs (such as kidneys) appear one in front of the other instead of side by side, and most have only one functional lung. Some species retain a pelvic girdle with a pair of vestigial claws on either side of the cloaca.
Living snakes are found on every continent except Antarctica, and on most smaller land masses — exceptions include some large islands, such as Ireland and New Zealand, and many small islands of the Atlantic and central Pacific. Additionally, sea snakes are widespread throughout the Indian and Pacific Oceans. More than 20 families are currently recognized, comprising about 500 genera and about 3,400 species. They range in size from the tiny, 10 cm-long thread snake to the Reticulated python of up to 6.95 m in length. The fossil species "Titanoboa cerrejonensis" was 13 m long. Snakes are thought to have evolved from either burrowing or aquatic lizards, perhaps during the Jurassic period, with the earliest known fossils dating to between 143 and 167 Ma ago. The diversity of modern snakes appeared during the Paleocene period ("c" 66 to 56 Ma ago). The oldest preserved descriptions of snakes can be found in the Brooklyn Papyrus.
Most species are nonvenomous and those that have venom use it primarily to kill and subdue prey rather than for self-defense. Some possess venom potent enough to cause painful injury or death to humans. Nonvenomous snakes either swallow prey alive or kill by constriction.
Etymology.
The English word "snake" comes from Old English "snaca", itself from Proto-Germanic "*snak-an-" (cf. Germanic "Schnake" "ring snake", Swedish "snok" "grass snake"), from Proto-Indo-European root "*(s)nēg-o-" "to crawl", "to creep", which also gave "sneak" as well as Sanskrit "nāgá" "snake". The word ousted "adder", as "adder" went on to narrow in meaning, though in Old English "næddre" was the general word for snake. The other term, "serpent", is from French, ultimately from Indo-European "*serp-" (to creep), which also gave Ancient Greek "hérpō" (ἕρπω) "I crawl".
Evolution.
The fossil record of snakes is relatively poor because snake skeletons are typically small and fragile making fossilization uncommon. Fossils readily identifiable as snakes (though often retaining hind limbs) first appear in the fossil record during the Cretaceous period. The earliest known snake fossils come from sites in Utah and Algeria, represented by the genera "Coniophis" and "Lapparentophis", respectively. These fossil sites have been tentatively dated to the Albian or Cenomanian age of the late Cretaceous, between 112 and 94 Ma ago. However, an even greater age has been suggested for one of the Algerian sites, which may be as old as the Aptian, 125 to 112 Ma ago.
Based on comparative anatomy, there is consensus that snakes descended from lizards.:11 Pythons and boas—primitive groups among modern snakes—have vestigial hind limbs: tiny, clawed digits known as anal spurs, which are used to grasp during mating.:11 The families Leptotyphlopidae and Typhlopidae also possess remnants of the pelvic girdle, appearing as horny projections when visible.
The oldest snake is Eophis, discovered in 2015. 
Front limbs are nonexistent in all known snakes. This is caused by the evolution of Hox genes, controlling limb morphogenesis. The axial skeleton of the snakes’ common ancestor, like most other tetrapods, had regional specializations consisting of cervical (neck), thoracic (chest), lumbar (lower back), sacral (pelvic), and caudal (tail) vertebrae. Early in snake evolution, the Hox gene expression in the axial skeleton responsible for the development of the thorax became dominant. As a result, the vertebrae anterior to the hindlimb buds (when present) all have the same thoracic-like identity (except from the atlas, axis, and 1–3 neck vertebrae). In other words, most of a snake's skeleton is an extremely extended thorax. Ribs are found exclusively on the thoracic vertebrae. Neck, lumbar and pelvic vertebrae are very reduced in number (only 2–10 lumbar and pelvic vertebrae are present), while only a short tail remains of the caudal vertebrae. However, the tail is still long enough to be of important use in many species, and is modified in some aquatic and tree-dwelling species.
Modern snakes greatly diversified during the Paleocene. This occurred alongside the adaptive radiation of mammals, following the extinction of (non-avian) dinosaurs. The colubrids, one of the more common snake groups, became particularly diverse due to preying on rodents, an especially successful mammal group.
Origins.
The origin of snakes remains an unresolved issue. There are two main hypotheses competing for acceptance.
There is fossil evidence to suggest that snakes may have evolved from burrowing lizards, such as the varanids (or a similar group) during the Cretaceous Period. An early fossil snake relative, "Najash rionegrina", was a two-legged burrowing animal with a sacrum, and was fully terrestrial. One extant analog of these putative ancestors is the earless monitor "Lanthanotus" of Borneo (though it also is semiaquatic). Subterranean species evolved bodies streamlined for burrowing, and eventually lost their limbs. According to this hypothesis, features such as the transparent, fused eyelids (brille) and loss of external ears evolved to cope with fossorial difficulties, such as scratched corneas and dirt in the ears. Some primitive snakes are known to have possessed hindlimbs, but their pelvic bones lacked a direct connection to the vertebrae. These include fossil species like "Haasiophis", "Pachyrhachis" and "Eupodophis", which are slightly older than "Najash".
An alternative hypothesis, based on morphology, suggests the ancestors of snakes were related to mosasaurs—extinct aquatic reptiles from the Cretaceous—which in turn are thought to have derived from varanid lizards. According to this hypothesis, the fused, transparent eyelids of snakes are thought to have evolved to combat marine conditions (corneal water loss through osmosis), and the external ears were lost through disuse in an aquatic environment. This ultimately lead to an animal similar to today's sea snakes. In the Late Cretaceous, snakes recolonized land, and continued to diversify into today's snakes. Fossilized snake remains are known from early Late Cretaceous marine sediments, which is consistent with this hypothesis; particularly so, as they are older than the terrestrial "Najash rionegrina". Similar skull structure, reduced or absent limbs, and other anatomical features found in both mosasaurs and snakes lead to a positive cladistical correlation, although some of these features are shared with varanids.
Genetic studies in recent years have indicated snakes are not as closely related to monitor lizards as was once believed—and therefore not to mosasaurs, the proposed ancestor in the aquatic scenario of their evolution. However, more evidence links mosasaurs to snakes than to varanids. Fragmented remains found from the Jurassic and Early Cretaceous indicate deeper fossil records for these groups, which may potentially refute either hypothesis.
Distribution.
There are over 2,900 species of snakes ranging as far northward as the Arctic Circle in Scandinavia and southward through Australia. Snakes can be found on every continent except Antarctica, in the sea, and as high as 16000 ft in the Himalayan Mountains of Asia.:143 There are numerous islands from which snakes are absent, such as Ireland, Iceland, and New Zealand (although New Zealand's waters are infrequently visited by the yellow-bellied sea snake and the banded sea krait).
Taxonomy.
All modern snakes are grouped within the suborder Serpentes in Linnean taxonomy, part of the order Squamata, though their precise placement within squamates remains controversial.
The two infraorders of Serpentes are: Alethinophidia and Scolecophidia. This separation is based on morphological characteristics and mitochondrial DNA sequence similarity. Alethinophidia is sometimes split into Henophidia and Caenophidia, with the latter consisting of "colubroid" snakes (colubrids, vipers, elapids, hydrophiids, and atractaspids) and acrochordids, while the other alethinophidian families comprise Henophidia. While not extant today, the Madtsoiidae, a family of giant, primitive, python-like snakes, was around until 50,000 years ago in Australia, represented by genera such as "Wonambi".
There are numerous debates in the systematics within the group. For instance, many sources classify Boidae and Pythonidae as one family, while some keep the Elapidae and Hydrophiidae (sea snakes) separate for practical reasons despite their extremely close relation.
Recent molecular studies support the monophyly of the clades of modern snakes, scolecophidians, typhlopids + anomalepidids, alethinophidians, core alethinophidians, uropeltids ("Cylindrophis", "Anomochilus", uropeltines), macrostomatans, booids, boids, pythonids and caenophidians.
Legless lizards.
While snakes are limbless reptiles which evolved from (and are grouped within) lizards, there are may other species of lizards which have lost their limbs independently and superficially look similar to snakes. These include the slow worm and glass snake.
Biology.
Size.
The now extinct "Titanoboa cerrejonensis" snakes found were 12.8 – in length. By comparison, the largest extant snakes are the reticulated python, the longest recorded specimen measured about 6.95 m long, and the anaconda, which measures about 5.21 m long and is considered the heaviest snake on Earth.
At the other end of the scale, the smallest extant snake is "Leptotyphlops carlae", with a length of about 10 cm. Most snakes are fairly small animals, approximately 1 m in length.
Skin.
The skin of a snake is covered in scales. Contrary to the popular notion of snakes being slimy because of possible confusion of snakes with worms, snakeskin has a smooth, dry texture. Most snakes use specialized belly scales to travel, gripping surfaces. The body scales may be smooth, keeled, or granular. The eyelids of a snake are transparent "spectacle" scales, which remain permanently closed, also known as brille.
The shedding of scales is called "ecdysis" (or in normal usage, "molting" or "sloughing"). In the case of snakes, the complete outer layer of skin is shed in one layer. Snake scales are not discrete, but extensions of the epidermis—hence they are not shed separately but as a complete outer layer during each molt, akin to a sock being turned inside out.
The shape and number of scales on the head, back, and belly are often characteristic and used for taxonomic purposes. Scales are named mainly according to their positions on the body. In "advanced" (Caenophidian) snakes, the broad belly scales and rows of dorsal scales correspond to the vertebrae, allowing scientists to count the vertebrae without dissection.
Snakes' eyes are covered by their clear scales (the brille) rather than movable eyelids. Their eyes are always open, and for sleeping, the retina can be closed or the face buried among the folds of the body.
Moulting.
Moulting serves a number of functions. Firstly, the old and worn skin is replaced; secondly, it helps get rid of parasites such as mites and ticks. Renewal of the skin by moulting is supposed to allow growth in some animals such as insects; however, this has been disputed in the case of snakes.
Molting occurs periodically throughout the snake's life. Before a molt, the snake stops eating and often hides or moves to a safe place. Just before shedding, the skin becomes dull and dry looking and the eyes become cloudy or blue-colored. The inner surface of the old skin liquefies. This causes the old skin to separate from the new skin beneath it. After a few days, the eyes clear and the snake "crawls" out of its old skin. The old skin breaks near the mouth and the snake wriggles out, aided by rubbing against rough surfaces. In many cases, the cast skin peels backward over the body from head to tail in one piece, like pulling a sock off inside-out. A new, larger, brighter layer of skin has formed underneath.
An older snake may shed its skin only once or twice a year. But a younger snake, still growing, may shed up to four times a year. The discarded skin gives a perfect imprint of the scale pattern, and it is usually possible to identify the snake if the discarded skin is reasonably intact. This periodic renewal has led to the snake being a symbol of healing and medicine, as pictured in the Rod of Asclepius.
Skeleton.
The skeleton of most snakes consists solely of the skull, hyoid, vertebral column, and ribs, though henophidian snakes retain vestiges of the pelvis and rear limbs.
The skull of the snake consists of a solid and complete neurocranium, to which many of the other bones are only loosely attached, particularly the highly mobile jaw bones, which facilitate manipulation and ingestion of large prey items. The left and right sides of the lower jaw are joined only by a flexible ligament at the anterior tips, allowing them to separate widely, while the posterior end of the lower jaw bones articulate with a quadrate bone, allowing further mobility. The bones of the mandible and quadrate bones can also pick up ground borne vibrations. Because the sides of the jaw can move independently of one another, snakes resting their jaws on a surface have sensitive stereo hearing which can detect the position of prey. The jaw-quadrate-stapes pathway is capable of detecting vibrations on the angstrom scale, despite the absence of an outer ear and the ossicle mechanism of impedance matching used in other vertebrates to receive vibrations from the air.
The hyoid is a small bone located posterior and ventral to the skull, in the 'neck' region, which serves as an attachment for muscles of the snake's tongue, as it does in all other tetrapods.
The vertebral column consists of anywhere between 200 to 400 (or more) vertebrae. Tail vertebrae are comparatively few in number (often less than 20% of the total) and lack ribs, while body vertebrae each have two ribs articulating with them. The vertebrae have projections that allow for strong muscle attachment enabling locomotion without limbs.
Autotomy of the tail, a feature found in some lizards is absent in most snakes. Caudal autotomy in snakes is rare and is intervertebral, unlike that in lizards, which is intravertebral—that is, the break happens along a predefined fracture plane present on a vertebra.
In some snakes, most notably boas and pythons, there are vestiges of the hindlimbs in the form of a pair of pelvic spurs. These small, claw-like protrusions on each side of the cloaca are the external portion of the vestigial hindlimb skeleton, which includes the remains of an ilium and femur.
Internal organs.
The snake's heart is encased in a sac, called the "pericardium", located at the bifurcation of the bronchi. The heart is able to move around, however, owing to the lack of a diaphragm. This adjustment protects the heart from potential damage when large ingested prey is passed through the esophagus. The spleen is attached to the gall bladder and pancreas and filters the blood. The thymus gland is located in fatty tissue above the heart and is responsible for the generation of immune cells in the blood. The cardiovascular system of snakes is also unique for the presence of a renal portal system in which the blood from the snake's tail passes through the kidneys before returning to the heart.
The vestigial left lung is often small or sometimes even absent, as snakes' tubular bodies require all of their organs to be long and thin. In the majority of species, only one lung is functional. This lung contains a vascularized anterior portion and a posterior portion that does not function in gas exchange. This 'saccular lung' is used for hydrostatic purposes to adjust buoyancy in some aquatic snakes and its function remains unknown in terrestrial species. Many organs that are paired, such as kidneys or reproductive organs, are staggered within the body, with one located ahead of the other.
Snakes have no lymph nodes.
Teeth.
Snakes are polyphyodonts with teeth that are continuously replaced.
Venom.
Cobras, vipers, and closely related species use venom to immobilize or kill their prey. The venom is modified saliva, delivered through fangs.:243 The fangs of 'advanced' venomous snakes like viperids and elapids are hollow to inject venom more effectively, while the fangs of rear-fanged snakes such as the boomslang merely have a groove on the posterior edge to channel venom into the wound. Snake venoms are often prey specific—their role in self-defense is secondary.:243
Venom, like all salivary secretions, is a predigestant that initiates the breakdown of food into soluble compounds, facilitating proper digestion. Even nonvenomous snake bites (like any animal bite) will cause tissue damage.:209
Certain birds, mammals, and other snakes (such as kingsnakes) that prey on venomous snakes have developed resistance and even immunity to certain venoms.:243 Venomous snakes include three families of snakes, and do not constitute a formal classification group used in taxonomy.
The term "poisonous snake" is mostly incorrect. Poison is inhaled or ingested, whereas venom is injected. There are, however, two exceptions: "Rhabdophis" sequesters toxins from the toads it eats, then secretes them from nuchal glands to ward off predators, and a small population of garter snakes in Oregon retains enough toxin in their liver from the newts they eat to be effectively poisonous to small local predators (such as crows and foxes).
Snake venoms are complex mixtures of proteins, and are stored in venom glands at the back of the head. In all venomous snakes, these glands open through ducts into grooved or hollow teeth in the upper jaw.:243 These proteins can potentially be a mix of neurotoxins (which attack the nervous system), hemotoxins (which attack the circulatory system), cytotoxins, bungarotoxins and many other toxins that affect the body in different ways. Almost all snake venom contains "hyaluronidase", an enzyme that ensures rapid diffusion of the venom.:243
Venomous snakes that use hemotoxins usually have fangs in the front of their mouths, making it easier for them to inject the venom into their victims. Some snakes that use neurotoxins (such as the mangrove snake) have fangs in the back of their mouths, with the fangs curled backwards. This makes it difficult both for the snake to use its venom and for scientists to milk them. "Elapids", however, such as cobras and kraits are "proteroglyphous"—they possess hollow fangs that cannot be erected toward the front of their mouths, and cannot "stab" like a viper. They must actually bite the victim.:242
It has recently been suggested that all snakes may be venomous to a certain degree, with harmless snakes having weak venom and no fangs. Most snakes currently labelled "nonvenomous" would still be considered harmless according to this theory, as they either lack a venom delivery method or are incapable of delivering enough to endanger a human. This theory postulates that snakes may have evolved from a common lizard ancestor that was venomous—and that venomous lizards like the gila monster, beaded lizard, monitor lizards, and the now-extinct mosasaurs may also have derived from it. They share this venom clade with various other saurian species.
Venomous snakes are classified in two taxonomic families:
There is a third family containing the "opistoglyphous" (rear-fanged) snakes (as well as the majority of other snake species):
Reproduction.
Although a wide range of reproductive modes are used by snakes, all snakes employ internal fertilization. This is accomplished by means of paired, forked hemipenes, which are stored, inverted, in the male's tail. The hemipenes are often grooved, hooked, or spined in order to grip the walls of the female's cloaca.
Most species of snakes lay eggs, but most snakes abandon the eggs shortly after laying. However, a few species (such as the king cobra) actually construct nests and stay in the vicinity of the hatchlings after incubation. Most pythons coil around their egg-clutches and remain with them until they hatch. A female python will not leave the eggs, except to occasionally bask in the sun or drink water. She will even "shiver" to generate heat to incubate the eggs.
Some species of snake are ovoviviparous and retain the eggs within their bodies until they are almost ready to hatch. Recently, it has been confirmed that several species of snake are fully viviparous, such as the boa constrictor and green anaconda, nourishing their young through a placenta as well as a yolk sac, which is highly unusual among reptiles, or anything else outside of requiem sharks or placental mammals. Retention of eggs and live birth are most often associated with colder environments.
Behavior.
Winter dormancy.
In regions where winters are colder than snakes can tolerate while remaining active, local species will brumate. Unlike hibernation, in which mammals are actually asleep, brumating reptiles are awake but inactive. Individual snakes may brumate in burrows, under rock piles, or inside fallen trees, or snakes may aggregate in large numbers at hibernacula.
Feeding and diet.
All snakes are strictly carnivorous, eating small animals including lizards, frogs, other snakes, small mammals, birds, eggs, fish, snails or insects.[#endnote_] Because snakes cannot bite or tear their food to pieces, they must swallow prey whole. The body size of a snake has a major influence on its eating habits. Smaller snakes eat smaller prey. Juvenile pythons might start out feeding on lizards or mice and graduate to small deer or antelope as an adult, for example.
The snake's jaw is a complex structure. Contrary to the popular belief that snakes can dislocate their jaws, snakes have a very flexible lower jaw, the two halves of which are not rigidly attached, and numerous other joints in their skull (see snake skull), allowing them to open their mouths wide enough to swallow their prey whole, even if it is larger in diameter than the snake itself. For example, the African egg-eating snake has flexible jaws adapted for eating eggs much larger than the diameter of its head.:81 This snake has no teeth, but does have bony protrusions on the inside edge of its spine, which it uses to break shells when it eats eggs.:81
While the majority of snakes eat a variety of prey animals, there is some specialization by some species. King cobras and the Australian bandy-bandy consume other snakes. "Pareas iwesakii" and other snail-eating colubrids of subfamily Pareatinae have more teeth on the right side of their mouths than on the left, as the shells of their prey usually spiral clockwise:184
Some snakes have a venomous bite, which they use to kill their prey before eating it. Other snakes kill their prey by constriction. Still others swallow their prey whole and alive.:81
After eating, snakes become dormant while the process of digestion takes place. Digestion is an intense activity, especially after consumption of large prey. In species that feed only sporadically, the entire intestine enters a reduced state between meals to conserve energy. The digestive system is then 'up-regulated' to full capacity within 48 hours of prey consumption. Being ectothermic ("cold-blooded"), the surrounding temperature plays a large role in snake digestion. The ideal temperature for snakes to digest is 30 °C. So much metabolic energy is involved in a snake's digestion that in the Mexican rattlesnake ("Crotalus durissus"), surface body temperature increases by as much as 1.2 C-change during the digestive process. Because of this, a snake disturbed after having eaten recently will often regurgitate its prey to be able to escape the perceived threat. When undisturbed, the digestive process is highly efficient, with the snake's digestive enzymes dissolving and absorbing everything but the prey's hair (or feathers) and claws, which are excreted along with waste.
Locomotion.
The lack of limbs does not impede the movement of snakes. They have developed several different modes of locomotion to deal with particular environments. Unlike the gaits of limbed animals, which form a continuum, each mode of snake locomotion is discrete and distinct from the others; transitions between modes are abrupt.
Lateral undulation.
Lateral undulation is the sole mode of aquatic locomotion, and the most common mode of terrestrial locomotion. In this mode, the body of the snake alternately flexes to the left and right, resulting in a series of rearward-moving "waves". While this movement appears rapid, snakes have rarely been documented moving faster than two body-lengths per second, often much less. This mode of movement has the same net cost of transport (calories burned per meter moved) as running in lizards of the same mass.
Terrestrial.
Terrestrial lateral undulation is the most common mode of terrestrial locomotion for most snake species. In this mode, the posteriorly moving waves push against contact points in the environment, such as rocks, twigs, irregularities in the soil, etc. Each of these environmental objects, in turn, generates a reaction force directed forward and towards the midline of the snake, resulting in forward thrust while the lateral components cancel out. The speed of this movement depends upon the density of push-points in the environment, with a medium density of about 8 along the snake's length being ideal. The wave speed is precisely the same as the snake speed, and as a result, every point on the snake's body follows the path of the point ahead of it, allowing snakes to move through very dense vegetation and small openings.
Aquatic.
When swimming, the waves become larger as they move down the snake's body, and the wave travels backwards faster than the snake moves forwards. Thrust is generated by pushing their body against the water, resulting in the observed slip. In spite of overall similarities, studies show that the pattern of muscle activation is different in aquatic versus terrestrial lateral undulation, which justifies calling them separate modes. All snakes can laterally undulate forward (with backward-moving waves), but only sea snakes have been observed reversing the motion (moving backwards with forward-moving waves).
Sidewinding.
Most often employed by colubroid snakes (colubrids, elapids, and vipers) when the snake must move in an environment that lacks irregularities to push against (rendering lateral undulation impossible), such as a slick mud flat, or a sand dune, sidewinding is a modified form of lateral undulation in which all of the body segments oriented in one direction remain in contact with the ground, while the other segments are lifted up, resulting in a peculiar "rolling" motion. This mode of locomotion overcomes the slippery nature of sand or mud by pushing off with only static portions on the body, thereby minimizing slipping. The static nature of the contact points can be shown from the tracks of a sidewinding snake, which show each belly scale imprint, without any smearing. This mode of locomotion has very low caloric cost, less than ⅓ of the cost for a lizard or snake to move the same distance. Contrary to popular belief, there is no evidence that sidewinding is associated with the sand being hot.
Concertina.
When push-points are absent, but there is not enough space to use sidewinding because of lateral constraints, such as in tunnels, snakes rely on concertina locomotion. In this mode, the snake braces the posterior portion of its body against the tunnel wall while the front of the snake extends and straightens. The front portion then flexes and forms an anchor point, and the posterior is straightened and pulled forwards. This mode of locomotion is slow and very demanding, up to seven times the cost of laterally undulating over the same distance. This high cost is due to the repeated stops and starts of portions of the body as well as the necessity of using active muscular effort to brace against the tunnel walls.
Rectilinear.
The slowest mode of snake locomotion is rectilinear locomotion, which is also the only one where the snake does not need to bend its body laterally, though it may do so when turning. In this mode, the belly scales are lifted and pulled forward before being placed down and the body pulled over them. Waves of movement and stasis pass posteriorly, resulting in a series of ripples in the skin. The ribs of the snake do not move in this mode of locomotion and this method is most often used by large pythons, boas, and vipers when stalking prey across open ground as the snake's movements are subtle and harder to detect by their prey in this manner.
Other.
The movement of snakes in arboreal habitats has only recently been studied. While on tree branches, snakes use several modes of locomotion depending on species and bark texture. In general, snakes will use a modified form of concertina locomotion on smooth branches, but will laterally undulate if contact points are available. Snakes move faster on small branches and when contact points are present, in contrast to limbed animals, which do better on large branches with little 'clutter'.
Gliding snakes ("Chrysopelea") of Southeast Asia launch themselves from branch tips, spreading their ribs and laterally undulating as they glide between trees. These snakes can perform a controlled glide for hundreds of feet depending upon launch altitude and can even turn in midair.
Interactions with humans.
Bite.
Snakes do not ordinarily prey on humans. Unless startled or injured, most snakes prefer to avoid contact and will not attack humans. With the exception of large constrictors, nonvenomous snakes are not a threat to humans. The bite of a nonvenomous snake is usually harmless; their teeth are not designed for tearing or inflicting a deep puncture wound, but rather grabbing and holding. Although the possibility of infection and tissue damage is present in the bite of a nonvenomous snake, venomous snakes present far greater hazard to humans.:209 The World Health Organisation lists snakebite under the "other neglected conditions" category.
Documented deaths resulting from snake bites are uncommon. Nonfatal bites from venomous snakes may result in the need for amputation of a limb or part thereof. Of the roughly 725 species of venomous snakes worldwide, only 250 are able to kill a human with one bite. Australia averages only one fatal snake bite per year. In India, 250,000 snakebites are recorded in a single year, with as many as 50,000 recorded initial deaths.
The treatment for a snakebite is as variable as the bite itself. The most common and effective method is through antivenom (or antivenin), a serum made from the venom of the snake. Some antivenom is species-specific (monovalent) while some is made for use with multiple species in mind (polyvalent). In the United States for example, all species of venomous snakes are pit vipers, with the exception of the coral snake. To produce antivenom, a mixture of the venoms of the different species of rattlesnakes, copperheads, and cottonmouths is injected into the body of a horse in ever-increasing dosages until the horse is immunized. Blood is then extracted from the immunized horse. The serum is separated and further purified and freeze-dried. It is reconstituted with sterile water and becomes antivenom. For this reason, people who are allergic to horses are more likely to suffer an allergic reaction to antivenom. Antivenom for the more dangerous species (such as mambas, taipans, and cobras) is made in a similar manner in India, South Africa, and Australia, although these antivenoms are species-specific.
Snake charmers.
In some parts of the world, especially in India, snake charming is a roadside show performed by a charmer. In such a show, the snake charmer carries a basket that contains a snake that he seemingly charms by playing tunes from his flutelike musical instrument, to which the snake responds. Snakes lack external ears, though they do have internal ears, and respond to the movement of the flute, not the actual noise.
The Wildlife Protection Act of 1972 in India technically proscribes snake charming on grounds of reducing animal cruelty. Other snake charmers also have a snake and mongoose show, where both the animals have a mock fight; however, this is not very common, as the snakes, as well as the mongooses, may be seriously injured or killed. Snake charming as a profession is dying out in India because of competition from modern forms of entertainment and environment laws proscribing the practice.
Trapping.
The "Irulas" tribe of Andhra Pradesh and Tamil Nadu in India have been hunter-gatherers in the hot, dry plains forests, and have practiced the art of snake catching for generations. They have a vast knowledge of snakes in the field. They generally catch the snakes with the help of a simple stick. Earlier, the "Irulas" caught thousands of snakes for the snake-skin industry. After the complete ban of the snake-skin industry in India and protection of all snakes under the Indian Wildlife (Protection) Act 1972, they formed the Irula Snake Catcher's Cooperative and switched to catching snakes for removal of venom, releasing them in the wild after four extractions. The venom so collected is used for producing life-saving antivenom, biomedical research and for other medicinal products. The "Irulas" are also known to eat some of the snakes they catch and are very useful in rat extermination in the villages.
Despite the existence of snake charmers, there have also been professional snake catchers or wranglers. Modern-day snake trapping involves a herpetologist using a long stick with a V- shaped end. Some television show hosts, like Bill Haast, Austin Stevens, Steve Irwin, and Jeff Corwin, prefer to catch them using bare hands.
Consumption.
While not commonly thought of as food in most cultures, in some cultures, the consumption of snakes is acceptable, or even considered a delicacy, prized for its alleged pharmaceutical effect of warming the heart. Snake soup of Cantonese cuisine is consumed by local people in autumn, to warm up their body. Western cultures document the consumption of snakes under extreme circumstances of hunger. Cooked rattlesnake meat is an exception, which is commonly consumed in parts of the Midwestern United States. In Asian countries such as China, Taiwan, Thailand, Indonesia, Vietnam and Cambodia, drinking the blood of snakes—particularly the cobra—is believed to increase sexual virility. The blood is drained while the cobra is still alive when possible, and is usually mixed with some form of liquor to improve the taste.
In some Asian countries, the use of snakes in alcohol is also accepted. In such cases, the body of a snake or several snakes is left to steep in a jar or container of liquor. It is claimed that this makes the liquor stronger (as well as more expensive). One example of this is the Habu snake sometimes placed in the Okinawan liquor Awamori also known as "Habu Sake".
U.S. Army Special Forces trainees are taught to catch, kill, and eat snakes during their survival course; this has earned them the nickname "snake eaters", which the video game "" may be inferred to draw from.
Snake wine (蛇酒) is an alcoholic beverage produced by infusing whole snakes in rice wine or grain alcohol. The drink was first recorded to have been consumed in China during the Western Zhou dynasty and considered an important curative and believed to reinvigorate a person according to Traditional Chinese medicine.
Pets.
In the Western world, some snakes (especially docile species such as the ball python and corn snake) are kept as pets. To meet this demand a captive breeding industry has developed. Snakes bred in captivity tend to make better pets and are considered preferable to wild caught specimens. Snakes can be very low maintenance pets, especially compared to more traditional species. They require minimal space, as most common species do not exceed five feet (1.5 m) in length. Pet snakes can be fed relatively infrequently, usually once every 5 to 14 days. Certain snakes have a lifespan of more than 40 years if given proper care.
Symbolism.
In Egyptian history, the snake occupies a primary role with the Nile cobra adorning the crown of the pharaoh in ancient times. It was worshipped as one of the gods and was also used for sinister purposes: murder of an adversary and ritual suicide (Cleopatra).
 
In Greek mythology snakes are often associated with deadly and dangerous antagonists, but this is not to say that snakes are symbolic of evil; in fact, snakes are a chthonic symbol, roughly translated as 'earthbound'. The nine-headed Lernaean Hydra that Hercules defeated and the three Gorgon sisters are children of Gaia, the earth. Medusa was one of the three Gorgon sisters who Perseus defeated. Medusa is described as a hideous mortal, with snakes instead of hair and the power to turn men to stone with her gaze. After killing her, Perseus gave her head to Athena who fixed it to her shield called the Aegis. The Titans are also depicted in art with snakes instead of legs and feet for the same reason—they are children of Gaia and Uranus, so they are bound to the earth.
The legendary account of the foundation of Thebes mentioned a monster snake guarding the spring from which the new settlement was to draw its water. In fighting and killing the snake, the companions of the founder Cadmus all perished - leading to the term "Cadmean victory" (i.e. a victory involving one's own ruin).
Three medical symbols involving snakes that are still used today are Bowl of Hygieia, symbolizing pharmacy, and the Caduceus and Rod of Asclepius, which are symbols denoting medicine in general.
India is often called the land of snakes and is steeped in tradition regarding snakes. Snakes are worshipped as gods even today with many women pouring milk on snake pits (despite snakes' aversion for milk). The cobra is seen on the neck of Shiva and Vishnu is depicted often as sleeping on a seven-headed snake or within the coils of a serpent. There are also several temples in India solely for cobras sometimes called "Nagraj" (King of Snakes) and it is believed that snakes are symbols of fertility. There is a Hindu festival called Nag Panchami each year on which day snakes are venerated and prayed to. See also "Nāga".
In India there is another mythology about snakes. Commonly known in Hindi as "Ichchhadhari" snakes. Such snakes can take the form of any living creature, but prefer human form. These mythical snakes possess a valuable gem called "Mani", which is more brilliant than diamond. There are many stories in India about greedy people trying to possess this gem and ending up getting killed.
The ouroboros is a symbol associated with many different religions and customs, and is claimed to be related to alchemy. The ouroboros or uroboros is a snake eating its own tail in a clock-wise direction (from the head to the tail) in the shape of a circle, representing the cycle of life, death and rebirth, leading to immortality.
The snake is one of the 12 celestial animals of Chinese Zodiac, in the Chinese calendar.
Many ancient Peruvian cultures worshipped nature. They emphasized animals and often depicted snakes in their art.
Religion.
Snakes are a part of Hindu worship. A festival, Nag Panchami, in which participants worship either images of or live Nāgas (cobras) is celebrated every year. Most images of Lord Shiva depict snake around his neck. Puranas have various stories associated with snakes. In the Puranas, Shesha is said to hold all the planets of the Universe on his hoods and to constantly sing the glories of Vishnu from all his mouths. He is sometimes referred to as "Ananta-Shesha", which means "Endless Shesha". Other notable snakes in Hinduism are Ananta, Vasuki, Taxak, Karkotaka and Pingala. The term Nāga is used to refer to entities that take the form of large snakes in Hinduism and Buddhism.
Snakes have also been widely revered, such as in ancient Greece, where the serpent was seen as a healer. Asclepius carried a serpent wound around his wand, a symbol seen today on many ambulances.
In religious terms, the snake and jaguar are arguably the most important animals in ancient Mesoamerica. "In states of ecstasy, lords dance a serpent dance; great descending snakes adorn and support buildings from Chichen Itza to Tenochtitlan, and the Nahuatl word "coatl" meaning serpent or twin, forms part of primary deities such as Mixcoatl, Quetzalcoatl, and Coatlicue." In both Maya and Aztec calendars, the fifth day of the week was known as Snake Day.
In Judaism, the snake of brass is also a symbol of healing, of one's life being saved from imminent death (Book of Numbers 21:6–9).
In some parts of Christianity, Christ's redemptive work is compared to saving one's life through beholding the Nehushtan (serpent of brass) (Gospel of John 3:14). Snake handlers use snakes as an integral part of church worship in order to exhibit their faith in divine protection. However, more commonly in Christianity, the serpent has been seen as a representative of evil and sly plotting, which can be seen in the description in Genesis chapter 3 of a snake in the Garden of Eden tempting Eve. Saint Patrick is reputed to have expelled all snakes from Ireland while Christianising the country in the 5th century, thus explaining the absence of snakes there.
In Christianity and Judaism, the snake makes its infamous appearance in the first book (Genesis 3:1) of the Bible when a serpent appears before the first couple Adam and Eve and tempts them with the forbidden fruit from the Tree of Knowledge. The snake returns in Exodus when Moses, as a sign of God's power, turns his staff into a snake and when Moses made the Nehushtan, a bronze snake on a pole that when looked at cured the people of bites from the snakes that plagued them in the desert. The serpent makes its final appearance symbolizing Satan in the Book of Revelation: "And he laid hold on the dragon the old serpent, which is the devil and Satan, and bound him for a thousand years." (Revelation 20:2)
In Neo-Paganism and Wicca, the snake is seen as a symbol of wisdom and knowledge.
Medicine.
The cyto-toxic effect of snake venom is being researched as a potential treatment for cancers.
Further reading.
</dl>

</doc>
<doc id="29374" url="http://en.wikipedia.org/wiki?curid=29374" title="Steam turbine">
Steam turbine

A steam turbine is a device that extracts thermal energy from pressurized steam and uses it to do mechanical work on a rotating output shaft. Its modern manifestation was invented by Sir Charles Parsons in 1884.
Because the turbine generates rotary motion, it is particularly suited to be used to drive an electrical generator – about 90% of all electricity generation in the United States (1996) is by use of steam turbines. The steam turbine is a form of heat engine that derives much of its improvement in thermodynamic efficiency from the use of multiple stages in the expansion of the steam, which results in a closer approach to the ideal reversible expansion process.
History.
The first device that may be classified as a reaction steam turbine was little more than a toy, the classic Aeolipile, described in the 1st century by Greek mathematician Hero of Alexandria in Roman Egypt. In 1551, Taqi al-Din in Ottoman Egypt described a steam turbine with the practical application of rotating a spit. Steam turbines were also described by the Italian Giovanni Branca (1629) and John Wilkins in England (1648). The devices described by Taqi al-Din and Wilkins are today known as steam jacks.
The modern steam turbine was invented in 1884 by Sir Charles Parsons, whose first model was connected to a dynamo that generated 7.5 kW (10 hp) of electricity. The invention of Parsons' steam turbine made cheap and plentiful electricity possible and revolutionized marine transport and naval warfare. Parsons' design was a reaction type. His patent was licensed and the turbine scaled-up shortly after by an American, George Westinghouse. The Parsons turbine also turned out to be easy to scale up. Parsons had the satisfaction of seeing his invention adopted for all major world power stations, and the size of generators had increased from his first 7.5 kW set up to units of 50,000 kW capacity. Within Parson's lifetime, the generating capacity of a unit was scaled up by about 10,000 times, and the total output from turbo-generators constructed by his firm C. A. Parsons and Company and by their licensees, for land purposes alone, had exceeded thirty million horse-power.
A number of other variations of turbines have been developed that work effectively with steam. The "de Laval turbine" (invented by Gustaf de Laval) accelerated the steam to full speed before running it against a turbine blade. De Laval's impulse turbine is simpler, less expensive and does not need to be pressure-proof. It can operate with any pressure of steam, but is considerably less efficient. developed a pressure compounded impulse turbine using the de Laval principle as early as 1900, obtained a US patent in 1903, and applied the turbine to a French torpedo boat in 1904. He taught at the École des mines de Saint-Étienne for a decade until 1897, and later founded a successful company that was incorporated into the Alstom firm after his death. One of the founders of the modern theory of steam and gas turbines was Aurel Stodola, a Slovak physicist and engineer and professor at the Swiss Polytechnical Institute (now ETH) in Zurich. His work "Die Dampfturbinen und ihre Aussichten als Wärmekraftmaschinen" (English: The Steam Turbine and its prospective use as a Mechanical Engine) was published in Berlin in 1903. A further book "Dampf und Gas-Turbinen" (English: Steam and Gas Turbines) was published in 1922.
The "Brown-Curtis turbine", an impulse type, which had been originally developed and patented by the U.S. company International Curtis Marine Turbine Company, was developed in the 1900s in conjunction with John Brown & Company. It was used in John Brown-engined merchant ships and warships, including liners and Royal Navy warships.
Types.
Steam turbines are made in a variety of sizes ranging from small <0.75 kW (<1 hp) units (rare) used as mechanical drives for pumps, compressors and other shaft driven equipment, to 1 500 000 kW (1.5 GW; 2 000 000 hp) turbines used to generate electricity. There are several classifications for modern steam turbines.
Blade and stage design.
Turbine blades are of two basic types, blades and nozzles. Blades move entirely due to the impact of steam on them and their profiles do not converge. This results in a steam velocity drop and essentially no pressure drop as steam moves through the blades. A turbine composed of blades alternating with fixed nozzles is called an impulse turbine, Curtis turbine, Rateau turbine, or Brown-Curtis turbine. Nozzles appear similar to blades, but their profiles converge near the exit. This results in a steam pressure drop and velocity increase as steam moves through the nozzles. Nozzles move due to both the impact of steam on them and the reaction due to the high-velocity steam at the exit. A turbine composed of moving nozzles alternating with fixed nozzles is called a reaction turbine or Parsons turbine.
Except for low-power applications, turbine blades are arranged in multiple stages in series, called compounding, which greatly improves efficiency at low speeds. A reaction stage is a row of fixed nozzles followed by a row of moving nozzles. Multiple reaction stages divide the pressure drop between the steam inlet and exhaust into numerous small drops, resulting in a pressure-compounded turbine. Impulse stages may be either pressure-compounded, velocity-compounded, or pressure-velocity compounded. A pressure-compounded impulse stage is a row of fixed nozzles followed by a row of moving blades, with multiple stages for compounding. This is also known as a Rateau turbine, after its inventor. A velocity-compounded impulse stage (invented by Curtis and also called a "Curtis wheel") is a row of fixed nozzles followed by two or more rows of moving blades alternating with rows of fixed blades. This divides the velocity drop across the stage into several smaller drops. A series of velocity-compounded impulse stages is called a pressure-velocity compounded turbine.
By 1905, when steam turbines were coming into use on fast ships (such as HMS "Dreadnought") and in land-based power applications, it had been determined that it was desirable to use one or more Curtis wheels at the beginning of a multi-stage turbine (where the steam pressure is highest), followed by reaction stages. This was more efficient with high-pressure steam due to reduced leakage between the turbine rotor and the casing. This is illustrated in the drawing of the German 1905 AEG marine steam turbine. The steam from the boilers enters from the right at high pressure through a throttle, controlled manually by an operator (in this case a sailor known as the throttleman). It passes through five Curtis wheels and numerous reaction stages (the small blades at the edges of the two large rotors in the middle) before exiting at low pressure, almost certainly to a condenser. The condenser provides a vacuum that maximizes the energy extracted from the steam, and condenses the steam into feedwater to be returned to the boilers. On the left are several additional reaction stages (on two large rotors) that rotate the turbine in reverse for astern operation, with steam admitted by a separate throttle. Since ships are rarely operated in reverse, efficiency is not a priority in astern turbines, so only a few stages are used to save cost.
Blade Design Challenges.
A major challenge facing turbine design is reducing the creep experienced by the blades. Because of the high temperatures and high stresses of operation, steam turbine materials become damaged through these mechanisms. As temperatures are increased in an effort to improve turbine efficiency, creep becomes more significant. To limit creep, thermal coatings and superalloys with solid-solution strengthening and grain boundary strengthening are used in blade designs. 
Protective coatings are used in to reduce the thermal damage and to limit oxidation. These coatings are often stabilized zirconium oxide-based ceramics. Using a thermal protective coating limits the temperature exposure of the nickel superalloy. This reduces the creep mechanisms experienced in the blade. Oxidation coatings limit efficiency losses caused by a buildup on the outside of the blades, which is especially important in the high-temperature environment.
The nickel-based blades are alloyed with aluminum and titanium to improve strength and creep resistance. The microstructure of these alloys is composed of different regions of composition. A uniform dispersion of the gamma-prime phase – a combination of nickel, aluminum, and titanium – promotes the strength and creep resistance of the blade due to the microstructure.
Refractory elements such as rhenium and ruthenium can be added to the alloy to improve creep strength. The addition of these elements reduces the diffusion of the gamma prime phase, thus preserving the fatigue resistance, strength, and creep resistance.
Steam supply and exhaust conditions.
These types include condensing, non-condensing, reheat, extraction and induction.
Condensing turbines are most commonly found in electrical power plants. These turbines exhaust steam from a boiler in a partially condensed state, typically of a quality near 90%, at a pressure well below atmospheric to a condenser.
Non-condensing or back pressure turbines are most widely used for process steam applications. The exhaust pressure is controlled by a regulating valve to suit the needs of the process steam pressure. These are commonly found at refineries, district heating units, pulp and paper plants, and desalination facilities where large amounts of low pressure process steam are needed.
Reheat turbines are also used almost exclusively in electrical power plants. In a reheat turbine, steam flow exits from a high pressure section of the turbine and is returned to the boiler where additional superheat is added. The steam then goes back into an intermediate pressure section of the turbine and continues its expansion. Using reheat in a cycle increases the work output from the turbine and also the expansion reaches conclusion before the steam condenses, there by minimizing the erosion of the blades in last rows. In most of the cases, maximum number of reheats employed in a cycle is 2 as the cost of super-heating the steam negates the increase in the work output from turbine.
Extracting type turbines are common in all applications. In an extracting type turbine, steam is released from various stages of the turbine, and used for industrial process needs or sent to boiler feedwater heaters to improve overall cycle efficiency. Extraction flows may be controlled with a valve, or left uncontrolled.
Induction turbines introduce low pressure steam at an intermediate stage to produce additional power.
Casing or shaft arrangements.
These arrangements include single casing, tandem compound and cross compound turbines. Single casing units are the most basic style where a single casing and shaft are coupled to a generator. Tandem compound are used where two or more casings are directly coupled together to drive a single generator. A cross compound turbine arrangement features two or more shafts not in line driving two or more generators that often operate at different speeds. A cross compound turbine is typically used for many large applications.
Two-flow rotors.
The moving steam imparts both a tangential and axial thrust on the turbine shaft, but the axial thrust in a simple turbine is unopposed. To maintain the correct rotor position and balancing, this force must be counteracted by an opposing force. Thrust bearings can be used for the shaft bearings, the rotor can use dummy pistons, it can be double flow- the steam enters in the middle of the shaft and exits at both ends, or a combination of any of these. In a double flow rotor, the blades in each half face opposite ways, so that the axial forces negate each other but the tangential forces act together. This design of rotor is also called two-flow, double-axial-flow, or double-exhaust. This arrangement is common in low-pressure casings of a compound turbine.
Principle of operation and design.
An ideal steam turbine is considered to be an isentropic process, or constant entropy process, in which the entropy of the steam entering the turbine is equal to the entropy of the steam leaving the turbine. No steam turbine is truly isentropic, however, with typical isentropic efficiencies ranging from 20–90% based on the application of the turbine. The interior of a turbine comprises several sets of blades or "buckets". One set of stationary blades is connected to the casing and one set of rotating blades is connected to the shaft. The sets intermesh with certain minimum clearances, with the size and configuration of sets varying to efficiently exploit the expansion of steam at each stage.
Turbine efficiency.
To maximize turbine efficiency the steam is expanded, doing work, in a number of stages. These stages are characterized by how the energy is extracted from them and are known as either impulse or reaction turbines. Most steam turbines use a mixture of the reaction and impulse designs: each stage behaves as either one or the other, but the overall turbine uses both. Typically, higher pressure sections are reaction type and lower pressure stages are impulse type.
Impulse turbines.
An impulse turbine has fixed nozzles that orient the steam flow into high speed jets. These jets contain significant kinetic energy, which is converted into shaft rotation by the bucket-like shaped rotor blades, as the steam jet changes direction. A pressure drop occurs across only the stationary blades, with a net increase in steam velocity across the stage.
As the steam flows through the nozzle its pressure falls from inlet pressure to the exit pressure (atmospheric pressure, or more usually, the condenser vacuum). Due to this high ratio of expansion of steam, the steam leaves the nozzle with a very high velocity. The steam leaving the moving blades has a large portion of the maximum velocity of the steam when leaving the nozzle. The loss of energy due to this higher exit velocity is commonly called the carry over velocity or leaving loss.
The law of moment of momentum states that the sum of the moments of external forces acting on a fluid which is temporarily occupying the control volume is equal to the net time change of angular momentum flux through the control volume.
The swirling fluid enters the control volume at radius formula_1 with tangential velocity formula_2 and leaves at radius formula_3 with tangential velocity formula_4.
A velocity triangle paves the way for a better understanding of the relationship between the various velocities. In the adjacent figure we have:
Then by the law of moment of momentum, the torque on the fluid is given by:
formula_17
For an impulse steam turbine: formula_18. Therefore, the tangential force on the blades is formula_19. The work done per unit time or power developed: formula_20.
When ω is the angular velocity of the turbine, then the blade speed is formula_21. The power developed is then formula_22.
Blade efficiency
Blade efficiency (formula_23) can be defined as the ratio of the work done on the blades to kinetic energy supplied to the fluid, and is given by
formula_24
Stage efficiency
A stage of an impulse turbine consists of a nozzle set and a moving wheel. The stage efficiency defines a relationship between enthalpy drop in the nozzle and work done in the stage.
formula_25
Where formula_26 is the specific enthalpy drop of steam in the nozzle.
By the first law of thermodynamics: formula_27
Assuming that formula_5 is appreciably less than formula_6, we get formula_30 ≈ formula_31
Furthermore, stage efficiency is the product of blade efficiency and nozzle efficiency, or formula_32
Nozzle efficiency is given by formula_33 = formula_34, where the enthalpy (in J/Kg) of steam at the entrance of the nozzle is formula_35 and the enthalpy of steam at the exit of the nozzle is formula_36.
formula_37
formula_38
formula_39
formula_40
The ratio of the cosines of the blade angles at the outlet and inlet can be taken and denoted formula_41.
The ratio of steam velocities relative to the rotor speed at the outlet to the inlet of the blade is defined by the friction coefficient formula_42.
formula_43 and depicts the loss in the relative velocity due to friction as the steam flows around the blades (formula_44 for smooth blades).
formula_45
The ratio of the blade speed to the absolute steam velocity at the inlet is termed the blade speed ratio formula_46 = formula_47
formula_23 is maximum when formula_49 or, formula_50. That implies formula_51 and therefore formula_52. Now formula_53 (for a single stage impulse turbine)
Therefore the maximum value of stage efficiency is obtained by putting the value of formula_52 in the expression of formula_23/
We get: formula_56.
For equiangular blades, formula_57, therefore formula_58, and we get formula_59. If the friction due to the blade surface is neglected then formula_60.
Conclusions on maximum efficiency
formula_60
1. For a given steam velocity work done per kg of steam would be maximum when formula_62 or formula_63.
2. As formula_64 increases, the work done on the blades reduces, but at the same time surface area of the blade reduces, therefore there are less frictional losses.
Reaction turbines.
In the "reaction turbine", the rotor blades themselves are arranged to form convergent nozzles. This type of turbine makes use of the reaction force produced as the steam accelerates through the nozzles formed by the rotor. Steam is directed onto the rotor by the fixed vanes of the stator. It leaves the stator as a jet that fills the entire circumference of the rotor. The steam then changes direction and increases its speed relative to the speed of the blades. A pressure drop occurs across both the stator and the rotor, with steam accelerating through the stator and decelerating through the rotor, with no net change in steam velocity across the stage but with a decrease in both pressure and temperature, reflecting the work performed in the driving of the rotor.
Blade efficiency
Energy input to the blades in a stage:
formula_65 is equal to the kinetic energy supplied to the fixed blades (f) + the kinetic energy supplied to the moving blades (m).
Or, formula_66 = enthalpy drop over the fixed blades, formula_67 + enthalpy drop over the moving blades, formula_68.
The effect of expansion of steam over the moving blades is to increase the relative velocity at the exit. Therefore the relative velocity at the exit formula_12 is always greater than the relative velocity at the inlet formula_11.
In terms of velocities, the enthalpy drop over the moving blades is given by:
formula_71 
The enthalpy drop in the fixed blades, with the assumption that the velocity of steam entering the fixed blades is equal to the velocity of steam leaving the previously moving blades is given by:
formula_67 = formula_73 where V0 is the inlet velocity of steam in the nozzle
formula_74 is very small and hence can be neglected
Therefore, formula_67 = formula_76
formula_77
formula_78
A very widely used design has half degree of reaction or 50% reaction and this is known as Parson’s turbine. This consists of symmetrical rotor and stator blades.
For this turbine the velocity triangle is similar and we have:
formula_79, formula_80
formula_81, formula_82
Assuming "Parson’s turbine" and obtaining all the expressions we get
formula_83
From the inlet velocity triangle we have formula_84
formula_85
formula_86
Work done (for unit mass flow per second): formula_87
Therefore the blade efficiency is given by
formula_88
Condition of maximum blade efficiency
If formula_89, then
formula_90
For maximum efficiency formula_91, we get
formula_92
and this finally gives formula_93
Therefore formula_94 is found by putting the value of formula_95 in the expression of blade efficiency
formula_96
formula_97
Operation and maintenance.
Because of the high pressures used in the steam circuits and the materials used, steam turbines and their casings have high thermal inertia. When warming up a steam turbine for use, the main steam stop valves (after the boiler) have a bypass line to allow superheated steam to slowly bypass the valve and proceed to heat up the lines in the system along with the steam turbine. Also, a turning gear is engaged when there is no steam to slowly rotate the turbine to ensure even heating to prevent uneven expansion. After first rotating the turbine by the turning gear, allowing time for the rotor to assume a straight plane (no bowing), then the turning gear is disengaged and steam is admitted to the turbine, first to the astern blades then to the ahead blades slowly rotating the turbine at 10–15 RPM (0.17–0.25 Hz) to slowly warm the turbine. The warm up procedure for large steam turbines may exceed ten hours.
During normal operation, rotor imbalance can lead to vibration, which, because of the high rotation velocities, could lead to a blade breaking away from the rotor and through the casing. To reduce this risk, considerable efforts are spent to balance the turbine. Also, turbines are run with high quality steam: either superheated (dry) steam, or saturated steam with a high dryness fraction. This prevents the rapid impingement and erosion of the blades which occurs when condensed water is blasted onto the blades (moisture carry over). Also, liquid water entering the blades may damage the thrust bearings for the turbine shaft. To prevent this, along with controls and baffles in the boilers to ensure high quality steam, condensate drains are installed in the steam piping leading to the turbine.
Maintenance requirements of modern steam turbines are simple and incur low costs (typically around $0.005 per kWh); their operational life often exceeds 50 years.
Speed regulation.
The control of a turbine with a governor is essential, as turbines need to be run up slowly to prevent damage and some applications (such as the generation of alternating current electricity) require precise speed control. Uncontrolled acceleration of the turbine rotor can lead to an overspeed trip, which causes the nozzle valves that control the flow of steam to the turbine to close. If this fails then the turbine may continue accelerating until it breaks apart, often catastrophically. Turbines are expensive to make, requiring precision manufacture and special quality materials.
During normal operation in synchronization with the electricity network, power plants are governed with a five percent droop speed control. This means the full load speed is 100% and the no-load speed is 105%. This is required for the stable operation of the network without hunting and drop-outs of power plants. Normally the changes in speed are minor. Adjustments in power output are made by slowly raising the droop curve by increasing the spring pressure on a centrifugal governor. Generally this is a basic system requirement for all power plants because the older and newer plants have to be compatible in response to the instantaneous changes in frequency without depending on outside communication.
Thermodynamics of steam turbines.
The steam turbine operates on basic principles of thermodynamics using the part 3-4 of the Rankine cycle shown in the adjoining diagram. Superheated vapor (or dry saturated vapor, depending on application) enters the turbine, after it having exited the boiler, at high temperature and high pressure. The high heat/pressure steam is converted into kinetic energy using a nozzle (a fixed nozzle in an impulse type turbine or the fixed blades in a reaction type turbine). Once the steam has exited the nozzle it is moving at high velocity and is sent to the blades of the turbine. A force is created on the blades due to the pressure of the vapor on the blades causing them to move. A generator or other such device can be placed on the shaft, and the energy that was in the vapor can now be stored and used. The gas exits the turbine as a saturated vapor (or liquid-vapor mix depending on application) at a lower temperature and pressure than it entered with and is sent to the condenser to be cooled. If we look at the first law we can find an equation comparing the rate at which work is developed per unit mass. Assuming there is no heat transfer to the surrounding environment and that the change in kinetic and potential energy is negligible when compared to the change in specific enthalpy we come up with the following equation
where
Isentropic efficiency.
To measure how well a turbine is performing we can look at its isentropic efficiency. This compares the actual performance of the turbine with the performance that would be achieved by an ideal, isentropic, turbine. When calculating this efficiency, heat lost to the surroundings is assumed to be zero. The starting pressure and temperature is the same for both the actual and the ideal turbines, but at turbine exit the energy content ('specific enthalpy') for the actual turbine is greater than that for the ideal turbine because of irreversibility in the actual turbine. The specific enthalpy is evaluated at the same pressure for the actual and ideal turbines in order to give a good comparison between the two.
The isentropic efficiency is found by dividing the actual work by the ideal work.
where
Direct drive.
Electrical power stations use large steam turbines driving electric generators to produce most (about 80%) of the world's electricity. The advent of large steam turbines made central-station electricity generation practical, since reciprocating steam engines of large rating became very bulky, and operated at slow speeds. Most central stations are fossil fuel power plants and nuclear power plants; some installations use geothermal steam, or use concentrated solar power (CSP) to create the steam. Steam turbines can also be used directly to drive large centrifugal pumps, such as feedwater pumps at a thermal power plant.
The turbines used for electric power generation are most often directly coupled to their generators. As the generators must rotate at constant synchronous speeds according to the frequency of the electric power system, the most common speeds are 3,000 RPM for 50 Hz systems, and 3,600 RPM for 60 Hz systems. Since nuclear reactors have lower temperature limits than fossil-fired plants, with lower steam quality, the turbine generator sets may be arranged to operate at half these speeds, but with four-pole generators, to reduce erosion of turbine blades.
Marine propulsion.
In steam-powered ships, compelling advantages of steam turbines over reciprocating engines are smaller size, lower maintenance, lighter weight, and lower vibration. A steam turbine is only efficient when operating in the thousands of RPM, while the most effective propeller designs are for speeds less than 300 RPM; consequently, precise (thus expensive) reduction gears are usually required, although numerous early ships through World War I, such as "Turbinia", had direct drive from the steam turbines to the propeller shafts. Another alternative is turbo-electric transmission, in which an electrical generator run by the high-speed turbine is used to run one or more slow-speed electric motors connected to the propeller shafts; precision gear cutting may be a production bottleneck during wartime. Turbo-electric drive was most used in large US warships designed during World War I and in some fast liners, and was used in some troop transports and mass-production destroyer escorts in World War II. The purchase cost of turbines is offset by much lower fuel and maintenance requirements and the small size of a turbine when compared to a reciprocating engine having an equivalent power. However, from the 1950s diesel engines were capable of greater reliability and higher efficiencies: propulsion steam turbine cycle efficiencies have yet to break 50%, yet diesel engines today routinely exceed 50%, especially in marine applications. Diesel power plants also have lower operating costs since fewer operators are required. Thus, conventional steam power is used in very few new ships. An exception is LNG carriers which often find it more efficient to use boil-off gas with a steam turbine than to re-liquify it.
Nuclear-powered ships and submarines use a nuclear reactor to create steam for turbines. Nuclear power is often chosen where diesel power would be impractical (as in submarine applications) or the logistics of refuelling pose significant problems (for example, icebreakers). It has been estimated that the reactor fuel for the Royal Navy's "Vanguard"-class submarine is sufficient to last 40 circumnavigations of the globe – potentially sufficient for the vessel's entire service life. Nuclear propulsion has only been applied to a very few commercial vessels due to the expense of maintenance and the regulatory controls required on nuclear systems and fuel cycles.
Early development.
The development of steam turbine marine propulsion from 1894-1935 was dominated by the need to reconcile the high efficient speed of the turbine with the low efficient speed (less than 300 rpm) of the ship's propeller at an overall cost competitive with reciprocating engines. In 1894, efficient reduction gears were not available for the high powers required by ships, so direct drive was necessary. In the "Turbinia", which has direct drive to each propeller shaft, the efficient speed of the turbine was reduced after initial trials by directing the steam flow through all three direct drive turbines (one on each shaft) in series, probably totaling around 200 turbine stages operating in series. Also, there were three propellers on each shaft for operation at high speeds. The high shaft speeds of the era are represented by one of the first US turbine-powered destroyers, USS "Smith", launched in 1909, which had direct drive turbines and whose three shafts turned at 724 rpm at 28.35 knots. The use of turbines in several casings exhausting steam to each other in series became standard in most subsequent marine propulsion applications, and is a form of cross-compounding. The first turbine was called the high pressure (HP) turbine, the last turbine was the low pressure (LP) turbine, and any turbine in between was an intermediate pressure (IP) turbine. A much later arrangement than "Turbinia" can be seen on the RMS "Queen Mary" in Long Beach, California, launched in 1934, in which each shaft is powered by four turbines in series connected to the ends of the two input shafts of a single-reduction gearbox. They are the HP, 1st IP, 2nd IP, and LP turbines.
Cruising machinery and gearing.
The quest for economy was even more important when cruising speeds were considered. Cruising speed is roughly 50% of a warship's maximum speed and 20-25% of its maximum power level. This would be a speed used on long voyages when fuel economy is desired. Although this brought the propeller speeds down to an efficient range, turbine efficiency was greatly reduced, and early turbine ships had poor cruising ranges. A solution that proved useful through most of the steam turbine propulsion era was the cruising turbine. This was an extra turbine to add even more stages, at first attached directly to one or more shafts, exhausting to a stage partway along the HP turbine, and not used at high speeds. As reduction gears became available around 1911, some ships, notably the USS "Nevada", had them on cruising turbines while retaining direct drive main turbines. Reduction gears allowed turbines to operate in their efficient range at a much higher speed than the shaft, but were expensive to manufacture.
Cruising turbines competed at first with reciprocating engines for fuel economy. An example of the retention of reciprocating engines on fast ships was the famous RMS "Titanic" of 1911, which along with her sisters RMS "Olympic" and HMHS "Britannic" had triple-expansion engines on the two outboard shafts, both exhausting to an LP turbine on the center shaft. After adopting turbines with the "Delaware"-class battleships launched in 1909, the United States Navy reverted to reciprocating machinery on the "New York"-class battleships of 1912, then went back to turbines on "Nevada" in 1914. The lingering fondness for reciprocating machinery was because the US Navy had no plans for capital ships exceeding 21 knots until after World War I, so top speed was less important than economical cruising. The United States had acquired the Philippines and Hawaii as territories in 1898, and lacked the British Royal Navy's worldwide network of coaling stations. Thus, the US Navy in 1900-1940 had the greatest need of any nation for fuel economy, especially as the prospect of war with Japan arose following World War I. This need was compounded by the US not launching any cruisers 1908-1920, so destroyers were required to perform long-range missions usually assigned to cruisers. So, various cruising solutions were fitted on US destroyers launched 1908-1916. These included small reciprocating engines and geared or ungeared cruising turbines on one or two shafts. However, once fully geared turbines proved economical in initial cost and fuel they were rapidly adopted, with cruising turbines also included on most ships. Beginning in 1915 all new Royal Navy destroyers had fully geared turbines, and the United States followed in 1917.
In the Royal Navy, speed was a priority until the Battle of Jutland in mid-1916 showed that in the battlecruisers too much armour had been sacrificed in its pursuit. The British used exclusively turbine-powered warships from 1906. Because they recognized that a significant cruising range would be desirable given their world-wide empire, some warships, notably the "Queen Elizabeth"-class battleships, were fitted with cruising turbines from 1912 onwards following earlier experimental installations.
In the US Navy, the "Mahan"-class destroyers, launched 1935-36, introduced double-reduction gearing. This further increased the turbine speed above the shaft speed, allowing smaller turbines than single-reduction gearing. Steam pressures and temperatures were also increasing progressively, from 300 psi/425 F (2.07 MPa/218 C)(saturation temperature) on the World War I-era "Wickes"-class to 615 psi/850 F (4.25 MPa/454 C) superheated steam on some World War II "Fletcher"-class destroyers and later ships. A standard configuration emerged of an axial-flow high pressure turbine (sometimes with a cruising turbine attached) and a double-axial-flow low pressure turbine connected to a double-reduction gearbox. This arrangement continued throughout the steam era in the US Navy and was also used in some Royal Navy designs. Machinery of this configuration can be seen on many preserved World War II-era warships in several countries. When US Navy warship construction resumed in the early 1950s, most surface combatants and aircraft carriers used 1,200 psi/950 F (8.28 MPa/510 C) steam. This continued until the end of the US Navy steam-powered warship era with the Knox-class frigates of the early 1970s. Amphibious and auxiliary ships continued to use 600 psi (4.14 MPa) steam post-World War II, with the USS "Iwo Jima", launched in 2001, possibly being the last non-nuclear steam-powered ship built for the US Navy. Except for nuclear-powered ships and submarines and LNG carriers, steam turbines have been replaced by gas turbines on fast ships and by diesel engines on other ships.
Turbo-electric drive.
Turbo-electric drive was introduced on the USS "New Mexico", launched in 1917. Over the next eight years the US Navy launched five additional turbo-electric-powered battleships and two aircraft carriers (initially ordered as "Lexington"-class battlecruisers). Ten more turbo-electric capital ships were planned, but cancelled due to the limits imposed by the Washington Naval Treaty. Although "New Mexico" was refitted with geared turbines in a 1931-33 refit, the remaining turbo-electric ships retained the system throughout their careers. This system used two large steam turbine generators to drive an electric motor on each of four shafts. The system was less costly initially than reduction gears and made the ships more maneuverable in port, with the shafts able to reverse rapidly and deliver more reverse power than with most geared systems. Some ocean liners were also built with turbo-electric drive, as were some troop transports and mass-production destroyer escorts in World War II. However, when the US designed the "treaty cruisers", beginning with the USS "Pensacola" launched in 1927, geared turbines were used for all fast steam-powered ships thereafter.
Locomotives.
A steam turbine locomotive engine is a steam locomotive driven by a steam turbine.
The main advantages of a steam turbine locomotive are better rotational balance and reduced hammer blow on the track. However, a disadvantage is less flexible output power so that turbine locomotives were best suited for long-haul operations at a constant output power.
The first steam turbine rail locomotive was built in 1908 for the Officine Meccaniche Miani Silvestri Grodona Comi, Milan, Italy. In 1924 Krupp built the steam turbine locomotive T18 001, operational in 1929, for Deutsche Reichsbahn.
Testing.
British, German, other national and international test codes are used to standardize the procedures and definitions used to test steam turbines. Selection of the test code to be used is an agreement between the purchaser and the manufacturer, and has some significance to the design of the turbine and associated systems. In the United States, ASME has produced several performance test codes on steam turbines. These include ASME PTC 6-2004, Steam Turbines, ASME PTC 6.2-2011, Steam Turbines in Combined Cycles, PTC 6S-1988, Procedures for Routine Performance Test of Steam Turbines. These ASME performance test codes have gained international recognition and acceptance for testing steam turbines. The single most important and differentiating characteristic of ASME performance test codes, including PTC 6, is that the test uncertainty of the measurement indicates the quality of the test and is not to be used as a commercial tolerance.

</doc>
<doc id="29376" url="http://en.wikipedia.org/wiki?curid=29376" title="Sardinia">
Sardinia

Sardinia (, Italian: "Sardegna" ], Sardinian: "Sardìgna, Sardìnnia" ]/], Sassarese: "Sardhigna", Gallurese: "Saldigna", Algherese: "Saldegna", Tabarchino: "Sardegna") is the second largest island in the Mediterranean Sea (after Sicily and before Cyprus) and an autonomous region of Italy, which goes by the official name of "Regione Autonoma della Sardegna" / "Regione Autònoma de Sardigna" (Autonomous Region of Sardinia).
The nearest land masses are (clockwise from north) the island of Corsica, the Italian Peninsula, Sicily, Tunisia, the Balearic Islands, and Provence. The Tyrrhenian Sea portion of the Mediterranean Sea is directly to the east of Sardinia between the Sardinian east coast and the west coast of the Italian mainland peninsula. The Strait of Bonifacio is directly north of Sardinia and separates Sardinia from the French island of Corsica.
The region has its capital in its largest city, Cagliari, and is divided into eight provinces. Its indigenous language and the other minority languages (Sassarese, Gallurese, Catalan Algherese and Tabarchino) enjoy "equal dignity" with Italian each in the concerned territory by a regional law.
Etymology.
The name Sardinia is from the pre-Roman noun *"sard-", romanised as "sardus" (feminine "sarda"); that the name had a religious connotation is suggested from its use also as the adjective for the ancient Sardinian mythological hero-god Sardus Pater "Sardinian Father" (in modern times misunderstood as being "Father Sardus"), as well as being the stem of the adjective "sardonic". Sardinia was called "Ichnusa" (the Latinised form of the Greek "Hyknousa", Υκνούσσα), "Sandàlion" (Σανδάλιον in Greek, meaning ‘sandal’), "Sardinia" and "Sardó" (Σαρδώ) by the Romans and the ancient Greeks.
Geography.
Sardinia is the second largest island in the Mediterranean Sea, with an area of 23821 km². It is situated between 38° 51' and 41° 15' latitude north and 8° 8' and 9° 50' east longitude. To the west of Sardinia is the Sea of Sardinia, a unit of the Mediterranean Sea; to Sardinia's east is the Tyrrhenian Sea, which is also an element of the Mediterranean Sea.
The coasts of Sardinia (1849 km long) are generally high and rocky, with long, relatively straight stretches of coastline, many outstanding headlands, a few wide, deep bays, rias, many inlets and with various smaller islands off the coast.
The island has an ancient geoformation and, unlike Sicily and mainland Italy, is not earthquake-prone. Its rocks date from the Palaeozoic Era (up to 500 million years old). Due to long erosion processes, the island's highlands, formed of granite, schist, trachyte, basalt (called "jaras" or "gollei"), sandstone and dolomite limestone (called "tonneri" or "heels"), average at between 300 to. The highest peak is Punta La Marmora (1834 m), part of the Gennargentu Ranges in the centre of the island. Other mountain chains are Monte Limbara (1362 m) in the northeast, the Chain of Marghine and Goceano (1259 m) running crosswise for 40 km towards the north, the Monte Albo (1057 m), the Sette Fratelli Range in the southeast, and the Sulcis Mountains and the Monte Linas (1236 m). The island's ranges and plateaux are separated by wide alluvial valleys and flatlands, the main ones being the Campidano in the southwest between Oristano and Cagliari and the Nurra in the northwest.
Sardinia has few major rivers, the largest being the Tirso, 151 km long, which flows into the Sea of Sardinia, the Coghinas (115 km) and the Flumendosa (127 km). There are 54 artificial lakes and dams that supply water and electricity. The main ones are Lake Omodeo and Lake Coghinas. The only natural freshwater lake is Lago di Baratz. A number of large, shallow, salt-water lagoons and pools are located along the 1850 km of the coastline.
Climate.
The island has a Mediterranean climate (Köppen: Csa) along the coasts, plains and low hills and a continental climate on the interior plateaus, valleys and mountain ranges. During the year there are approximately 135 days of sunshine, with a major concentration of rainfall in the winter and autumn, some heavy showers in the spring and snowfalls in the highlands. The average temperature is between 11 to, with mild winters and hot summers on the coasts ( 9 to in January, 23 to in July), and cold winters and cool summers on the mountains ( -2 to in January, 16 to in July). Rainfall has a Mediterranean distribution all over the island, with almost totally rainless summers and wet autumns, winters and springs. However, in summer, the rare rainfalls can be characterized by short but severe thunderstorms, which can cause flash floods. The climate is also heavily influenced by the vicinity of the Gulf of Genoa (barometric low) and the relative proximity of the Atlantic Ocean. Low pressures in autumn can generate the formation of the so-called "Medicanes", extratropical cyclones which affected the Mediterranean basin. In 2013, the island was hit by several cyclones, included the Cyclone Cleopatra, which dumped almost 18 inches (450 mm) of rainfall within an hour and a half. Sardinia being relatively large and hilly, weather is not uniform; in particular the East is drier, but paradoxically it suffers the worst rainstorms : in Autumn 2009, it rained more than 200 mm in a single day in Siniscola, and 19 November 2013, locations in Sardinia were reported to have received more than 431 mm (17 inches) within two hours. The Western coast has a higher distribution of rainfalls even for modest elevations (for instance Iglesias, elevation 200 m, average annual precipitation 815 mm). The driest part of the island is the coast of Cagliari gulf, with less than 450 mm per year, the minimum is at Capo Carbonara at the extreme south-east of the island 381 mm, and the wettest is the top of the Gennargentu mountain with almost 1500 mm per year. The average for the entire island is about 800 mm per year, which is more than enough for the needs of the population and vegetation.
The Mistral from the northwest is the dominant wind on and off throughout the year, though it is most prevalent in winter and spring. It can blow quite strongly, but it is usually dry and cool and makes for a sailor's paradise.
History.
Sardinia is one of the least known of the larger islands of Europe, but it is also one of the most interesting.—Thomas Ashby (1874-1931), Director of the British School at Rome 1906-1925
Prehistory.
Sardinia is one of the most geologically ancient bodies of land in Europe.
The island was populated in various waves of emigration from prehistory until recent times.
The first people to settle in Sardinia during the Upper Paleolithic and the Mesolithic came from the Iberian and the Italian peninsula; some populations, particularly from the ancient region of Etruria (present-day Tuscany), managed to move to northern Sardinia via Corsica; however, there is some evidence in Oliena's "Corbeddu Cave" of a previous Paleolithic colonization of the island. In the mid Neolithic period, the Ozieri culture, probably of Aegean origin, flourished on the island.
During the early Bronze Age, the so-called Beaker culture, coming from Continental Europe, appeared in Sardinia. These new people predominantly settled on the west coast, where the majority of the sites attributed to them had been found.
Evidence of trade with Aegean (Eastern Mediterranean) centres is present in the period from 1600 BC onwards. As time passed, the different Sardinian populations appear to have become united in customs, yet remained politically divided into various small, tribal groupings, at times banding together, and at others waging war against each other. Habitations consisted of round thatched stone huts.
Nuragic civilization.
From about 1500 BC onwards, villages were built around round tower-fortresses called nuraghi (singular form "Nuraghe", usually pluralized in English as "Nuraghes"). These towers were often reinforced and enlarged with battlements. Tribal boundaries were guarded by smaller lookout Nuraghes erected on strategic hills commanding a view of other territories.
Today, some 7,000 Nuraghes dot the Sardinian landscape. While initially these Nuraghes had a relatively simple structure, with time they became extremely complex and monumental (see for example Su Nuraxi near Barumini or Nuraghe Arrubiu near Orroli). The scale, complexity and territorial spread of these buildings attest to the level of wealth accumulated by the Nuragic people, their advances in technology and the complexity of their society, which was able to coordinate large numbers of people with different roles for the purpose of building the monumental Nuraghes.
The Nuraghes are not the only Nuragic buildings that survive, as there are several sacred wells around Sardinia and other buildings that had religious purposes such as the Giants' grave (monumental collective tombs) and collections of religious buildings that probably served as destinations for pilgrimage and mass religious rites (e.g. Su Romanzesu near Bitti).
Sardinia was at the time at the centre of several commercial routes and it was an important provider of raw materials such as copper and lead, which were pivotal for the manufacture of the time. By controlling the extraction of these raw materials and by commercing them with other countries, the Nuragic civilisation was able to accumulate wealth and reach a level of sophistication that is not only reflected in the complexity of its surviving buildings, but also in its artworks (e.g. the votive bronze statuettes found across Sardinia).
According to some scholars, the Nuragic people(s) are identifiable with the Shardana, a tribe of the "Sea Peoples".
The Nuragic civilization was linked with other contemporaneous megalithic civilization of the western Mediterranean, such as the Talaiotic culture of the Balearic islands and the Torrean civilization of southern Corsica. Several artefacts (e.g. pots) have been found in Nuragic sites that came from as far as Anatolia, Greece as well as from Italy, which testifies the scope of commercial relations between the Nuragic people and other people in Europe and beyond.
Ancient history.
Around 1000 BC the Phoenicians began visiting Sardinia with increasing frequency, presumably initially needing safe over-night and/or all-weather anchorages along their trade routes from the coast of modern-day Lebanon as far afield as the African and European Atlantic coasts and beyond. The most common ports of call were Caralis, Nora, Bithia, Sulcis, Tharros, Bosa and Olbia.
While the Phoenicians stuck to the coastline, their relationship with the Sardinians was peaceful. However, after a few hundred years of habitation, they began expanding inward. They took over valuable natural resources, such as silver and lead mines, and established a military presence in the form of a fortress on Monte Sira in 650 BC. The Sardinians resented these intrusions, and in 509 BC they mounted a series of attacks against Phoenician settlements. The Phoenician settlers called on Carthage for help, and when it arrived they successfully took control of part of the southern part of the island.
In 238 BC, the Carthaginians, as a result of their defeat by the Romans in the First Punic War, surrendered Corsica and Sardinia to Rome, and together they became a Roman province. The existing coastal cities were enlarged and embellished, while Coloniae such as Turris Lybissonis and Feronia were founded. These were populated by Roman immigrants. The Roman military occupation brought the Nuragic civilization to an end, except for the mountainous interior of the island, which the Romans called Barbaria, meaning "Land populated by Barbarians". Roman domination of Sardinia lasted 694 years, during which it was an important source of grain for the capital. Latin came to be the dominant spoken language of Sardinia during this period, though Roman culture was slower to take hold, and Roman rule was often contested by the inhabitants of Sardinia's mountainous central regions.
Vandal conquest.
The east Germanic tribe of the Vandals conquered Sardinia in 456. Their rule lasted for 78 years up to 534, when eastern Roman troops under Cyrillus retook the island. It is known that the Vandal government continued the forms of the existing Roman Imperial structure. The governor of Sardinia continued to be called the "praeses" and apparently continued to manage military, judicial, and civil governmental functions via imperial procedures. (This continuity was not novel to Sardinia; like the Visigoths, the Vandals generally maintained the pretence of the empire, nominally acknowledging Constantinople and declaring themselves its deputies.) The only Vandal governor of Sardinia about whom there is substantial record is the last, Godas, a Visigoth noble. In AD 530, a coup d'état in Carthage removed King Hilderic, a convert to Nicene Christianity, in favor of his cousin Gelimer, an Arian Christian like most of the élite in his kingdom. Godas was sent to take charge and ensure the loyalty of Sardinia. He did the exact opposite, declaring the island's independence from Carthage and opening negotiations with Emperor Justinian I, who had declared war on Hilderic's behalf. In AD 533 Gelimer sent the bulk of his army to Sardinia to subdue Godas, with the catastrophic result that the Vandal Kingdom was overwhelmed when Justinian's own army under Belisarius arrived in their absence. The Vandal Kingdom ended and Sardinia was returned to Roman rule.
Byzantine era.
In AD 533, Sardinia returned to the rule of the Eastern Roman Empire (in this period sometimes referred to as the Byzantine Empire) when the Vandals were defeated by the armies of Justinian I under the General Belisarius in the Battle of Tricamarum, in their African kingdom; Belisarius sent his general Cyrillus to Sardinia to retake the island. Sardinia remained in Byzantine hands for the next 300 years aside from a short period in which it was invaded by the Ostrogoths in 551.
Under Byzantine rule, the island was divided into districts called "merèie" (μερείαι in Byzantine Greek), which were governed by a judge residing in Caralis (Cagliari) and garrisoned by an army stationed in "Forum Traiani" (today Fordongianus) under the command of a "dux". During this time, Christianity took deeper root on the island, supplanting the Paganism which had survived into the early Medieval era in the culturally conservative hinterlands. Along with lay Christianity, the followers of monastic figures such as St. Basil became established in Sardinia. While Christianity penetrated the majority of the population, the region of Barbagia remained largely pagan. Towards the end of the 6th century, a short-lived independent principality established itself in Barbagia and returned to the local traditional religions. One of its princes, Ospitone, conducted raids on the neighbouring Christian communities controlled by the Byzantine dux Zabarda. He was later reprimanded by Pope Gregory I within a letter for "Living, all like irrational animals, ignorant of the true God and worshipping wood and stone" In 594, Ospitone was convinced by Gregory the Great to convert to Christianity after receiving the papal letter. His followers, however, were not immediately convinced and ostracised their prince for a short time before they themselves converted.
The dates and circumstances of the end of Byzantine rule in Sardinia are not known. Direct central control was maintained at least through "c." 650, after which local legates were empowered in the face of the rebellion of Gregory the Patrician, Exarch of Africa and the first invasion of the Umayyads in North Africa. There is some evidence that senior Byzantine administration in the Exarchate of Africa retreated to Cagliari following the final fall of Carthage to the Arabs in 697. The loss of imperial control in Africa led to escalating Moorish and Berber raids on the island, the first of which is documented in 705, forcing increased military self-reliance in the province. Communication with the central government became daunting if not impossible during and after the Muslim conquest of Sicily between 827 and 902. A letter by Pope Nicholas I as early as 864 mentions the "Sardinian judges", without reference to the empire and a letter by Pope John VIII (reigned 872-882) refers to them as "principes" ("princes"). By the time of "De Administrando Imperio", completed in 952, the Byzantine authorities no longer listed Sardinia as an imperial province, suggesting they considered it lost.
Whether this final transformation from imperial civil servant to independent sovereign resulted from imperial abandonment or local assertion, by the 10th century, the giudici (Sardinian: "judikes" / Latin: "iudices", literally ‘judges’, a Byzantine administrative title) had emerged as the autonomous rulers of Sardinia. The title of "iudice" changed with the language and local understanding of the position, becoming the Sardinian "judike", essentially a king or sovereign, while "giudicato" (Sardinian: "judicadu"), literally ‘judgeship’ or ‘judicature’, came to mean both ‘State’ and ‘palace’ or ‘capital’.
Medieval history.
Early medieval Sardinian political institutions evolved from the millennium old Roman imperial structures with relatively little Germanic influence.
Although the Giudicati were hereditary lordships, the old Roman/Byzantine imperial notion that personal title or honor was separate from the state still remained, so the Giudicato was not regarded as the personal property of the monarch as was common in later European feudalism. Like the imperial systems, the new order also preserved "semi-democratic" forms, with national assemblies called "Corona de Logu". Each Giudicato saw to its own defense, maintained its own laws and administration, and looked after its own foreign and trading affairs.
In the 10th century there were five known Giudicati on Sardinia, but the annexation of the Giudicato of Agugliastra by that of Cagliari sometime in the 10th or 11th century stabilized the number at four; they would remain in place until the Aragonese invasion of the 14th century. The history of the four Giudicati would be defined by the contest for influence between the rival rising sea powers of Genoa and Pisa, and later the ambitions of the Kingdom of Aragon.
The Giudicato of Cagliari was allied to the Republic of Genoa. It was brought to an end in 1258, when its capital, "Santa Igia", was stormed and destroyed by an alliance of Sardinian and Pisan forces. The territory then briefly became a colony of Pisa.
The Giudicato of Logudoro (sometimes called Torres) was also allied to the Republic of Genoa and came to an end in 1259 on the death of the "judikessa" (queen) Adelasia. The territory was divided up between the Doria family of Genoa and the Basserra family of Arborea, while the city of Sassari became a small republic, like the Italian city-states ("comuni").
The Giudicato of Gallura ended in the year 1288, when the last giudice, Nino Visconti (a friend of Dante Alighieri), was driven out by the Pisans, who occupied the territory.
The Giudicato of Arborea, having Oristano as its capital, had a longer life compared to the other kingdoms. Its later history is entwined with the attempt to unify the island into a single Sardinian state ("Republica Sardisca") against their relatives and former Aragonese allies.
In 1297, Pope Boniface VIII established on his own initiative ("motu proprio") a hypothetical "regnum Sardiniae et Corsicae" ("Kingdom of Sardinia and Corsica") in order to settle the War of the Vespers diplomatically. This had broken out in 1282 between the Angevins and Aragonese over the possession of Sicily. Despite the existence of the indigenous states, the Pope offered this newly created crown to James II of Aragon, promising him support should he wish to conquer Pisan Sardinia in exchange for Sicily.
In 1324, in alliance with the Kingdom of Arborea and following a military campaign that lasted a year or so, led by the Catalan-Aragonese Crown Prince Alfons, the Catalan-Aragonese army occupied the Pisan territories of Cagliari and Gallura along with the allied city of Sassari, naming them "The Kingdom of Sardinia and Corsica". The kingdom was to remain a dominion of the Crown of Aragon (under the Kings of Spain) until the Treaty of Utrecht.
During this period, the Giudicato of Arborea promulgated the legal code of the kingdom in the "Carta de Logu" (‘Charter of the Land’). The Carta de Logu was originally compiled by Mariano IV of Arborea, and was amended and updated by Mariano's daughter, Queen Eleanor of Arborea. The legal code was written in Sardinian and established a whole range of citizens' rights. Among the revolutionary concepts in this Carta de Logu was the right of women to refuse marriage and to own property. In terms of civil liberties, the code made provincial 14th century Sardinia one of the most developed societies in all of Europe.
In 1353, Peter IV of Aragon, following Aragonese customs, granted a parliament to the kingdom of Sardinia and Corsica, which was followed by some degree of self-government under a viceroy and judicial independence. This parliament, however, had limited powers. It consisted of high-ranking military commanders, the clergy and the nobility. The kingdom of Aragon also introduced the feudal system into the areas of Sardinia that it ruled.
The Sardinian kingdoms never adopted feudalism, and Arborea maintained its parliament called the "Corona de Logu". In this parliament, apart from the nobles and military commanders, also sat the representatives of each township and village. The Corona de Logu exercised some control over the king: under the rule of the "bannus consensus" the king could be deposed or even killed if he did not follow the rules of the kingdom.
From 1365 to 1409, the Arborean giudici Mariano IV, Ugone III, Mariano V (assisted by his mother Eleanor of Arborea), and William III (the French grandson of Eleonora) succeeded in occupying all of Sardinia except the heavily fortified towns of the Castle of Cagliari and Alghero, which for years remained as the only Aragonese dominions in Sardinia.
In 1409, Martin I of Sicily, king of Sicily and heir to the crown of Aragon, defeated the Sardinians at the Battle of Sanluri. The battle was fought by about 20,000 Sardinian, Genoese and French knights, enrolled from their kingdom at a time when the population of Sardinia had been greatly depleted by the plague. Despite the Sardinian army outnumbering the Aragonese army, they were defeated.
The kingdom of Arborea disappeared in 1420, when its rights were sold by the last king for 100,000 gold florins, and after some of its most notable men switched sides in exchange for privileges. For example, Leonardo Cubello, with some claim to the crown being from a family related to the Kings of Arborea, was granted the title of Marquis of Oristano and feudal rights on a territory that partly overlapped with the original extension of the Kingdom of Arborea in exchange for his subjection to the King of Aragon.
The conquest of Sardinia by the Kingdom of Aragon meant the introduction of the feudal system throughout Sardinia. Thus Sardinia is probably the only European country where feudalism was introduced in the transition period from the Medieval to the Modern Era, at a time when feudalism had already been abandoned by many other European countries.
Modern history.
In 1479, the reigning monarch of Sardinia, King Ferdinand II of Aragon, married Isabel of Castile, and the "Kingdom of Sardinia" (which was separated from Corsica) was to be inherited by their Habsburg grandson, Charles I of Spain, with the state symbol of the Four Moors. Following the failure of the military ventures against the Muslims of Tunis (1535) and Algiers (1541), Charles I of Spain, in order to defend his Mediterranean territories from pirate raids by the African Berbers, fortified the Sardinian shores with a system of coastal lookout towers.
The Kingdom of Sardinia remained Spanish for approximately 400 years, from 1323 to 1720, assimilating a number of Spanish traditions, customs and linguistic expressions, nowadays vividly portrayed in the folklore parades of Saint Efisio in Cagliari (1 May), the Cavalcade on Sassari (last but one Sunday in May), and the Redeemer in Nuoro (28 August). To this day Catalan is spoken in the western city of Alghero (l'Alguer).
Many famines have been reported in Sardinia. According to Stephen L. Dyson and Robert J. Rowland, "The Jesuits of Cagliari recorded years during the late 16th century "of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds" ... During the terrible famine of 1680, some 80,000 people, out of a total population of 250,000, are said to have died, and entire villages were devastated..."
In 1708, as a consequence of the Spanish War of Succession, the rule of the Kingdom of Sardinia passed from King Philip V of Spain into the hands of the Austrians, who occupied the island. The Treaty of Utrecht granted Sardinia to the Austrians, but in 1717, Cardinal Giulio Alberoni, minister of Philip V of Spain, reoccupied Sardinia. In 1718, with the Treaty of London, Sardinia was handed over to the House of Savoy, that would impose the Italian language on the island in 1760.
In 1793, Sardinians repelled two French invasions. On 23 February 1793, Domenico Millelire, commanding the Sardinian fleet, defeated the fleets of the French Republic near the Maddalena archipelago, of which then-lieutenant Napoleon Bonaparte was a leader. Millelire became the first recipient of the Gold Medal of Military Valor of the Italian Armed Forces. In the same month, Sardinians stopped the attempted French landing on the beach of Quartu Sant'Elena, near the Capital of Cagliari. Because of these successes, the representatives of nobility and clergy ("Stamenti") formulated five requests addressed to the King Victor Amadeus III of Sardinia, but they got refused. Because of this discontent, on 28 April 1794, during an uprising in Cagliari, two Savoyard officials were killed. That was the start of a revolt (called the "Moti rivoluzionari sardi" or ""Vespri sardi") in the island, which culminated in the expulsion of the Piedmontese officers for a few days from the Capital Cagliari. On 28 December 1795 in Sassari insurgents demonstrating against feudalism, mainly from the region of Logudoro, occupied the city. On 13 February 1796, in order to prevent the spread of the revolt, the viceroy Filippo Vivalda gave the Sardinian magistrate Giovanni Maria Angioy the role of Alternos, which meant a substitute of the viceroy himself. Angioy moved from Cagliari to Sassari, and during his journey almost all the villages joined the uprising, demanding an end to feudalism and aiming to declare the island to be a republic, but once he was outnumbered by loyalist forces he fled to Paris and sought support for a French annexation of the island.
In 1798 the islet near Sardinia was attacked by the Tunisians and over 900 inhabitants were taken away as slaves. The final Muslim attack on the island was on Sant'Antioco on 16 October 1815, over a millennium since the first.
In 1799, as a consequence of the Napoleonic Wars in Italy, the Dukes of Savoy left Turin and took refuge in Cagliari for some fifteen years. In 1847, the Sardinian parliaments ("Stamenti"), in order to get the Piedmontese liberal reforms they could not afford due to their separated legal system, renounced their state autonomy and agreed to form a union with Piedmont, Savoy, Nice and Liguria in order to have a single parliament, a single magistracy and a single government in Turin; most of the pro-union supporters, including its leader Giovanni Siotto Pintor, would later regret it.
In 1848, the confederation of states powered by the Savoyard kings of Sardinia became a unitarian and constitutional state and moved to the Italian Wars of Independence for the Unification of Italy, that were led for thirteen years. In 1861, being Italy united by a debated war campaign, the parliament of the Kingdom of Sardinia decided by law to change its name and the title of its king in Kingdom of Italy and King of Italy.
During the First World War, the Sardinian soldiers of the Brigata Sassari distinguished themselves, being the most decorated with gold medals among the Italian soldiers. It was the first and only Italian military unit constituted mainly from Sardinian soldiers. The Sardinian writer Grazia Deledda won the Nobel Prize for Literature in 1926.
During the Fascist period, and implementation of the policy of autarky, several swamps around the island were reclaimed and agrarian communities founded. The main communities were in the area of Oristano, where the village of Mussolinia (now called Arborea) was located, and in the area adjacent the city of Alghero, within the region of Nurra, Fertilia was founded. Also established during that time was the city of Carbonia, which became the main centre of mining activity. Works to dry the numerous waste lands and the reprise of mining activities favored the arrival of many settlers from the Italian mainland; at first, they were mostly from the region of Veneto, but after World War II they were followed by a notable number of Istrian Italians and Dalmatian Italians, hailing from territories lost to Yugoslavia.
During the Second World War, Sardinia was an important air and naval base and was heavily bombed by the Allies, especially the city of Cagliari. German troops left the island on 8 September 1943, a few days after the Armistice of Cassibile, and retired to Corsica without fighting and bloodshed, after a bilateral agreement between the general Antonio Basso (Commander of the Armed Forces of Sardinia) and the German Karl Hans Lungerhausen, general of the 90th Panzergrenadier Division.
Post-World War II period.
In 1946, by popular referendum, Italy became a republic, with Sardinia being administered since 1948 by a special statute of autonomy. By 1951, malaria was successfully eliminated by the ERLAAS, Anti-malaric Regional Authority, and the support of the Rockefeller Foundation, which facilitated the commencement of the Sardinian tourist boom. With the increase in tourism, coal decreased in importance but Sardinia followed the Italian economic miracle.
In the early 1960s, an industrialisation effort was commenced, the so-called "Piani di Rinascita" (rebirth plans), with the initiation of major infrastructure projects on the island. These included the construction of new dams and roads, reforestation, agricultural zones on reclaimed marshland, and large industrial complexes (primarily oil refineries and related petrochemical operations). With the creation of petrochemical industries, thousands of ex-farmers became industrial workers. The 1973 oil crisis caused the termination of employment for thousands of workers employed in the petrochemical industries, which aggravated the emigration already present in the 1950s and 1960s.
Sardinia faced the creation of military bases on the island, like Decimomannu Air Base and Salto di Quirra (the biggest scientific military base in Europe) in the same decades. Even now, around 60% of all Italian and NATO military installations in Italy are on Sardinia, whose area is less than one-tenth of all the Italian territory and whose population is little more than the 2,5%; furthermore, they comprise over 35.000 hectares used for experimental weapons testing.
Sardinian nationalism and local protest movements became stronger in the 1970s, and a number of bandits ("bandìdos") started a series of kidnappings, which ended only in the 1990s, for political reasons. This also gave rise to various militant groups that blended separatist and communist ideas, the most famous being "Barbagia Rossa" and the "Movimento Armato Sardo", which perpetrated several terrorist actions between the 1970s and the early 1980s.
In 1983 a prominent activist of a separatist party, the Sardinian Action Party ("Partidu Sardu - Partito Sardo d'Azione"), was elected president of the regional parliament, and in the 1980s several other movements calling for independence from Italy were born; in the 1990s some of them became political parties, even if in a rather disjointed manner. It was not until 1999 that the island's languages (Sardinian, Sassarese, Gallurese, Algherese and Tabarchino) were recognised, even if just formally, together with Italian. The 35th G8 summit was planned by Prodi II Cabinet to be held in Sardinia, on the island of La Maddalena, in July 2009; however, in April 2009, the Italian Prime Minister, Silvio Berlusconi, decided, without convoking the Italian parliament or consulting the Sardinian governor of his own party, to move the summit, even though the works were almost completed, to L'Aquila, provoking heavy protests.
Today Sardinia is phasing in as an EU region, with a diversified economy focused on tourism and the tertiary sector. The economic efforts of the last twenty years have reduced the handicap of insularity, especially in the fields of low-cost air travel and advanced information technology. For example, the CRS4 (Center for Advanced Studies, Research and Development in Sardinia) developed the second European website and 1st in Italy in 1991 and webmail in 1995. CRS4 allowed several telecommunication companies and internet service providers based on the island to flourish, such as Videonline in 1994, Tiscali in 1998 and Andala Umts in 1999.
Education.
According to the ISTAT census of 2001, the literacy rate in Sardinia among people below 64 years old is 99.5 percent. Total literacy rate (including people over 65) is 98.2 percent.
Illiteracy rate among males below 65 years old is 0.24 percent and among women 0.25 percent; the number of women that annually graduate at secondary high schools and universities is about 10-20 percent higher than men. However, in spite of these findings, Sardinia has actually the highest rate of school and university drop-out in Italy.
Sardinia has two public universities: the University of Sassari and the University of Cagliari, founded between the 16th and 17th centuries. 48,979 students were enrolled at universities in 2007-08.
Economy.
Taken as a whole, Sardinia's economic conditions are such that the island is in the best position among Italian regions located south of Rome. The greatest economic development had taken place inland, in the provinces of Cagliari and Sassari, characterized by a certain amount of enterprise. According to Eurostat, the 2011 GDP was 33,075 millions €, 32,377 in Purchasing power parity resulting in €19,300 GDP per capita that is the 77% of the European Union one. The per capita income in Sardinia is the highest of the southern half of Italy
The most populated provincial chief towns have higher incomes: in Cagliari the income per capita is €27,545, in Sassari €24,006, in Oristano €23,887, in Nuoro is €23,316 and in Olbia is €20.827.
The Sardinian economy is, however, constrained due to the high costs of the transportation of goods and electricity, which is twice that of the continental Italian regions, and triple that of the EU average. Sardinia is the only Italian region that produces a surplus of electricity, and exports electricity to Corsica and the Italian mainland: in 2009, the new submarine power cable Sapei entered into operation, it links the Fiume Santo Power Station, in Sardinia, to the converter stations in Latina, in the Italian peninsula, the SACOI is another submarine power cable that links Sardinia to Italy, crossing Corsica, from 1965.
The submarine gas pipeline GALSI would have brought Algerian gas to the Italian mainland through the island.
Three main banks are headquartered in Sardinia: the Banco di Sardegna and the Banca di Sassari, both based in Sassari, and the Banca di Credito Sardo, based in Cagliari.
The unemployment rate for the fourth quarter of 2008 was 8.6%; by 2012, the unemployment rate had increased to 14,6%. Its rise was due to the global financial crisis that hit Sardinian exports, mainly focused on refined oil, chemical products, and also mining and metallurgical products.
There are chances for Sardinia to become a tax haven, the whole island territory being free by custom duties, vat and excise taxes on fuel; since February 2013, the town of Porto Scuso has become the first free trade zone. According to the article 12 of the Sardinian Statute modified by the regional parliament in October 2013: "The Territory of the Autonomous Region of Sardinia is located off the customs line and constitutes a Free Trade Zone enclosed by the surrounding sea; the access points consist of the seaports and the airports. The Sardinian Free Trade Zone is regulated by the laws of the European Union and Italy that are in force also in Livigno, Campione D'Italia, Gorizia, Savogna d'Isonzo and the Region of Aosta Valley".
Economic sectors.
This table shows the sectors of the Sardinian economy in 2011:
Primary.
The soil of Sardinia is exploited to 60% for breeding, 20% for agriculture and the rest is occupied by closed forests, urban areas and areas that are not exploitable. Sicily practically has reversed percentages and, with the same extension, almost three times the inhabitants of Sardinia. Sardinia is home to nearly 4 million sheep, almost half of the entire Italian assets and that makes the island one of the areas of the world with the highest density of sheep along with some parts of UK and New Zealand (135 sheep every square kilometer versus 129 in UK and 116 in New Zealand). The soils of Sardinia are largely underpowered, shallow and therefore not very productive for agriculture. Sardinia has been for thousands of years specializing in sheep breeding, and, to a lesser extent, goats and cattle that is less productive of agriculture in relation to land use. It is probably in breeding and cattle ownership the economic base of the early proto-historic and monumental Sardinian civilization from Neolithic to the Iron Age.
Even agriculture has played a very important role in the economic history of the island, especially in the great plain of Campidano, particularly suitable for wheat farming. The Sardinian soils, even those plains are slightly permeable, with aquifers of lacking and sometimes brackish water and very small natural reserves. Water scarcity was the first problem that was faced for the modernization of the sector, with the construction of a great barrier system of dams, which today contains nearly 2 billion cubic meters of water. The Sardinian agriculture is now linked to specific products such as cheese, wine, olive oil, artichoke, tomato for a growing product export. The reclamations have helped to extend the crops and to introduce other ones such as vegetables and fruit, next to the historical ones, olive and grapes that are present in the hilly areas. The Campidano plain, the largest lowland Sardinian produces oats, barley and durum, of which is one of the most important Italian producers. Among the vegetables, as well as artichokes, has a certain weight the production of oranges, and, before the reform of the sugar sector from the European Union, the cultivation of sugar beet. In the forests there is the cork oak, which grows naturally; Sardinia produces about 80% of Italian cork. In fresh food, as well as artichokes, the production of tomatoes (including Camoni tomato) and citrus fruit are of a certain weight. Sardinia is the 5th Italian region for rice production, the main paddy fields are located in the Arborea Plain.
In addition to meat, Sardinia produces a wide variety of cheese, considering that half of the sheep milk produced in Italy is produced in Sardinia, and is largely worked by the cooperatives of the shepherds and small industries. Sardinia also produces most of the pecorino romano, a non-original product of the island, much of which is traditionally addressed to the Italian overseas communities. Sardinia boasts a centuries-old tradition of horse breeding since the Aragonese domination, whose cavalry drew from equine heritage of the island to strengthen their own army or to make a gift to the other sovereigns of Europe. Today the Island boasts the highest number of horse herds in Italy.
There is little fishing (and no real maritime tradition), Portoscuso tunas are exported worldwide, but primarily to Japan. The cork district, in the northern part of the Gallura region, around Calangianus and Tempio Pausania, is composed of 130 companies. Every year in Sardinia 200,000 quintals of cork are carved, and 40% of the end products are exported.
Industry and handicraft.
The once prosperous mining industry is still active though restricted to coal (Nuraxi Figus, hamlet of Gonnesa), antimony (Villasalto), gold (Furtei), bauxite (Olmedo) and lead and zinc (Iglesiente, Nurra). The granite extraction represents one of the most flourishing industries in the northern part of the island. The Gallura granite district is composed of 260 companies that work in 60 quarries, where 75% of the Italian granite is extracted.
The principal industries are chemicals (Porto Torres, Cagliari, Villacidro, Ottana), petrochemicals (Porto Torres, Sarroch), metalworking (Porto Scuso, Porto Vesme, Villacidro), cement (Cagliari), pharmaceutical (Sassari), shipbuilding (Arbatax, Olbia, Porto Torres), oil rig construction (Arbatax), rail industry (Villacidro) and food (sugar refineries at Villasor and Oristano, dairy at Arborea, Macomer and Thiesi, fish factory at Olbia.
In Sardinia is located the DASS ("Distretto Aerospaziale della Sardegna"), a consortium of companies, research centers and universities focused on aerospace industry and research.
Plans related to industrial conversion are is in progress in the main industrial sites, like in Porto Torres, where seven research centres are developing the transformation from traditional fossil fuel related industry to an integrated production chain from vegetable oil using oleaginous seeds to bio-plastics.
Sardinia is involved in the industrial production of the AirPod, an innovative car powered by compressed air, with the first factory being built in Bolotana.
Craft industries include rugs, jewelry, textile, lacework, basket making and coral.
Tertiary.
The Sardinian economy is today focused on the overdeveloped tertiary sector (67.8% of employment), with commerce, services, information technology, public administration and especially on tourism, which represents the main industry of the island with 2,721 active companies and 189,239 rooms. In 2008 there were 2,363,496 arrivals (up 1.4% on 2007). In the same year, the airports of the island registered 11,896,674 passengers (up 1.24% on 2007).
Communications.
On the island are headquartered some telecommunication companies and internet service providers, such as Tiscali and the Mediterranean Skylogic Teleport, a ground station controlled by satellite provider Eutelsat. Sardinia is after Valle D'Aosta the Italian region with the highest e-intensity index (Index that measures the relative maturity of Internet economies on the basis of three factors: enablement, engagement, and expenditure) and the region with the highest internet performances, such as fastest broadband connection in Italy.
Sardinia has become Europe’s first region to fully adopt the new Digital Terrestrial Television broadcasting standard. From 1 November 2008 TV channels are broadcast only in digital.
Transportation.
Airports.
Sardinia has three international airports (Alghero Airport, Olbia - Costa Smeralda Airport, and Cagliari-Elmas Airport) connected with the principal Italian cities and many European destinations, mainly in the United Kingdom, Scandinavia, Spain, and Germany, and two regional airports (Oristano-Fenosu Airport and Tortolì Airport). Internal air connections between Sardinian airports are limited to a daily Cagliari-Olbia flight, and Tortolì-Olbia flight. Sardinian citizens benefit from special sales on plane tickets, and several low-cost air companies operate on the island.
Meridiana is an airline based in the airport of Olbia; it was founded as Alisarda in 1963 by the Aga Khan IV. The development of Alisarda followed the development of Costa Smeralda in the north east part of the island, a well known vacation spot among billionaires and movie stars worldwide.
Seaports.
The ferry companies operating on the island are Tirrenia di Navigazione, Moby Lines, Corsica Ferries, Grandi Navi Veloci, SNAV, SNCM and CMN; they link the Sardinian seaports of Porto Torres, Olbia, Golfo Aranci, Arbatax, Santa Teresa Gallura, Palau and Cagliari with Civitavecchia, Genoa, Livorno, Naples, Palermo, Trapani, Piombino in Italy, Marseille, Toulon, Bonifacio, Propriano and Ajaccio in France, and Barcelona in Spain.
A regional ferry company, the Saremar, links the main island to the islands of La Maddalena and San Pietro, and from 2011, also the port of Olbia with Civitavecchia, and Porto Torres with Savona.
About 40 tourist harbors are located along the Sardinian coasts.
Roads.
Sardinia is the only Italian region without Autostrade, but the road network is well developed with a system of no-toll roads with dual carriageway, called "superstrade" (en: super roads), that connect the principal towns and the main airports and seaports; the speed limit is 90 km/h/110 km/h. The principal road is the SS131 "Carlo Felice", linking the north with the south of the island, crossing the most populated regions of Sassari and Cagliari; it is part of European route E25. The SS 131 d.c.n links Oristano with Olbia, crossing the hinterland Nuoro region. Other roads designed for high-capacity traffic link Sassari with Alghero, Sassari with Tempio Pausania, Sassari - Olbia, Cagliari - Tortolì, Cagliari - Iglesias, Nuoro - Lanusei. A work in progress is converting the main routes to highway standards, with the elimination of all intersections. The secondary inland and mountain roads are generally narrow with many hairpin turns, so the speed limits are very low.
Public transport buses reach every town and village at least once a day; however, due to the low density of population, the smallest territories are reachable only by car. The Azienda Regionale Sarda Trasporti (Arst) is the public regional bus transport agency. Networks of city buses serve the main towns.
In Sardinia 1.295.462 vehicles circulate, equal to 613 per 1000 inhabitants.
Railways.
The Sardinian railway system was developed starting from the 19th century by the English engineer Benjamin Piercy.
Today there are two different railway operators:
The "Trenino Verde" ("Little Green Train") is a rail tourism service operated by ARST Gestione FdS. Vintage railcars and steam locomotives run through the wildest parts of the island. They allow the traveller to have scenic views impossible to see from the main roads.
Demographics.
With a population density of 69/km2, slightly more than a third of the national average, Sardinia is the fourth least populated region in Italy. The population distribution is anomalous compared to that of other Italian regions lying on the sea. In fact, contrary to the general trend, urban settlement has not taken place primarily along the coast but towards the centre of the island. Historical reasons for this include repeated Saracen raids during the Middle Ages (making the coast unsafe), widespread pastoral activities inland, and the swampy nature of the coastal plains (reclaimed only in the 20th century). The situation has been reversed with the expansion of seaside tourism; today all Sardinia's major urban centres are located near the coasts, while the island's interior is very sparsely populated.
It is the Italian region with the lowest total fertility rate (1.087 births per woman), and the region with the second-lowest birth rate.
Combined with the aging of population going rather fast (in 2009, people older than 65 were 18,7%), depopulation is quite a big issue: between 1991 and 2001, 71,4% of Sardinian villages have lost population (32 more than 20% and 115 between 10% and 20%), with over 30 of them being at risk to become ghost towns. Nonetheless, the population has been increasing in recent years because of massive immigration, mainly from the Italian mainland, Eastern Europe (esp. Romania), Africa and China.
Average life expectancy is 81 years (85 for women and 78 for men). Sardinia shares with the Japanese island of Okinawa the highest rate of centenarians in the world (22 centenarians/100,000 inhabitants).
Sardinia is the first discovered Blue Zone, a demographic and/or geographic area in the world with an oversize concentration of centenarians and supercentenarians.
Foreign immigration.
At the end of 2010 there were 37,853 foreign national residents, forming 2.3% of the total Sardinian population. The most represented nationalities were :
Main cities and towns.
Sardinia most populated cities are Cagliari and Sassari. The Cagliari metropolitan area has about 460,000 inhabitants, that is the 28% of the population of the entire island.
Government and politics.
Sardinia is one of the five Italian autonomous regions, along with Valle d'Aosta, Trentino-Alto Adige/Südtirol, Friuli Venezia Giulia and Sicily. Its statute, which is a constitutional law, gives the region the right to create its own laws in a wide number of domains and to carry out regional administrative functions.
The regional administration is constituted by three authorities:
Administrative divisions.
Until 2005, Sardinia had been divided into four provinces: Cagliari, Nuoro, Oristano and Sassari. In 2005 the Regional Council decided to create four new provinces becoming operative with the provincial elections for the Presidents and the Councils held in 2006. The four additional provinces are as follows: Carbonia-Iglesias, Medio Campidano, Ogliastra, Olbia-Tempio. A popular referendum, in 2012, has supported the abolition of the provinces and the halving of the members of regional council.
Culture.
Sardinia is the only autonomous region where its special Statute uses the term "popolo" ("people") to describe its citizens. While this formula applies even to Veneto, which unlike Sardinia is an ordinary region within the Italian Republic, the Sardinian Statute is adopted with a constitutional law. In both cases, this term has no recognized legal meaning of any differences with other Italian citizens.
Languages.
Alongside Italian ("Italiano"), the official language throughout Italy, Sardinian ("Sardu") is the most widely spoken language on the island. Sardinian is a distinct branch of the Romance language family that is virtually incomprehensible to Italian speakers: it is not, by any means, an Italian dialect and it has been formally recognized as a minority language in Italy since 1997, by regional and Italian law. The language has been influenced by Catalan, Spanish and recently Italian, while the once spoken Nuragic contributes many features to it in many ancient remnants. In 2006 the regional administration has approved the use of a standardised writing system, the so-called "Limba Sarda Comuna", in official acts. As a literary language, Sardinian is gaining importance, despite heated debate about the lack of a commonly acknowledged standard orthography and controversial proposed solutions to this problem. The two most widely spoken forms of the language are Campidanese ("Sardu Campidanesu"), spoken throughout the southern half of the island, and Logudorese ("Sardu Logudoresu"), from the northern-central region, extending almost to the suburbs of Sassari.
Sassarese ("Sassaresu") and Gallurese ("Gadduresu") are classified as Corso-Sardinian languages, therefore more akin to the Italo-Dalmatian branch than to the Sardinian one, and are spoken in the north.
In Sardinia there are examples of language islands: Algherese ("Alguerés") is a dialect of Catalan spoken in the city of Alghero; on the islands of San Pietro and Sant'Antioco, located in the extreme south west of Sardinia, the local population speaks a variant of Ligurian called Tabarchino ("Tabarchin"); fewer and fewer people speak Venetian, Friulian and Istriot in Arborea and Fertilia, since these villages have been populated in the 1920s and 1930s by colonists who mainly came from north-eastern Italy, and families from Istria and Dalmatia immediately after World War II.
Due to the Italian assimilation policies carried out since 1760 and the ongoing absortion into the Italian culture, over the course of time the once prevalent indigenous languages have been increasingly losing ground to Italian and the process of the currently happening language shift may eventually lead to their extinction.
World Heritage Sites.
Megalithic building structures called nuraghes are scattered in great numbers throughout Sardinia. Su Nuraxi di Barumini is a UNESCO World Heritage Site.
Music.
Sardinia is home to one of the oldest forms of vocal polyphony, generally known as cantu a tenore. In 2005, Unesco classed the "cantu a tenore" among intangible world heritage. Several famous musicians have found it irresistible, including Frank Zappa, Ornette Coleman, and Peter Gabriel. The latter travelled to the town of Bitti in the central mountainous region and recorded the now world-famous Tenores di Bitti CD on his Real World label. The guttural sounds produced in this form make a remarkable sound, similar to Tuvan throat singing. Another polyphonic style of singing, more like the Corsican "paghjella" and liturgic in nature, is found in Sardinia and is known as "cantu a cuncordu".
Another unique instrument is the launeddas. Three reed-canes (two of them glued together with beeswax) produce distinctive harmonies, which have their roots many thousands of years ago, as demonstrated by the bronze statuettes from Ittiri, of a man playing the three reed canes, dated to 2000 BC.
Beyond this, the tradition of "cantu a chiterra" (guitar songs) has its origins in town squares, when artists would compete against one another. The most famous singer of this genre are Maria Carta and Elena Ledda.
Sardinian culture is alive and well, and young people are actively involved in their own music and dancing. In 2004, BBC presenter Andy Kershaw travelled to the island with Sardinian music specialist Pablo Farba and interviewed many artists. His programme can be heard on . Sardinia has produced a number of notable jazz musicians such as Antonello Salis, Marcello Melis, and Paolo Fresu.
The main opera houses of the island are the Teatro Lirico in Cagliari and the Teatro Verdi in Sassari (soon to be replaced by the new Teatro Auditorium Comunale).
Cuisine.
Meat, dairy products, grains and vegetables constitute the most basic elements of the traditional diet,
to a lesser extent Rock lobster ("aligusta"), scampi, bottarga ("butàriga"), squid, tuna and other seafood figure in Sardinian cuisine.
Suckling pig ("porcheddu") and wild boar ("sirbone") are roasted on the spit or boiled in stews of beans and vegetables, thickened with bread.
Herbs such as mint and myrtle are used. Much Sardinian bread is made dry, which keeps longer than high-moisture breads.
Those are baked as well, including "civraxiu", "coccoi pintau", a highly decorative bread and "pistoccu" made with flour and water only, originally meant for herders, but often served at home with tomatoes, basil, oregano, garlic and a strong cheese. Traditional cheeses include pecorino sardo, pecorino romano, casizolu, ricotta and the casu marzu (notable for containing live insect larvae).
Sardinia boasts the highest consumption of beer per capita in Italy, 60 liters per person per year, almost double the national average. The Province of Nuoro has the highest consumption with an average of 100 liters per capita.
The discovery of jars containing hops, in some archaeological sites, evidence that beer was produced since the copper age.
Sports.
Soccer.
Cagliari is home to Cagliari Calcio, which was founded in 1920 and plays in the Serie A, the Italian top division. It won the Italian Championship in the 1969–70 Serie A season, becoming the first club in Southern Italy to achieve such a result. Home matches are played at the Stadio Sant'Elia.
Basketball.
Sassari is home to Dinamo Basket Sassari, the only Sardinian professional basketball club playing in the Italiana serie A (Lega A), the highest level club competition in Italian professional basketball.
It was founded in 1960, and is also known as Dinamo Banco di Sardegna thanks to a long sponsorship deal with the Sardinian bank. Since its promotion in Lega A in 2010, it has been enjoying the support of fans from Sassari and all over Sardinia with full-house matches on every game played at home.
Auto racing.
In the Province of Sassari is the Mores Raceway, the only FIA Circuit homologated by CSAI (Cars) and the IMF (Motorcycles), in Sardinia.
Cagliari hosted a Formula 3000 race in 2002 and 2003 on a 2.414-km street circuit around Sant'Elia stadium. In 2003, Renault F1's Jarno Trulli and former Ferrari driver Jean Alesi did a spectacular exhibition. At the Grand Prix BMW-F1 driver Robert Kubica took part in a F3 car, as did BMW WTCC Augusto Farfus, GP2's Fairuz Fauzy and Vitaly Petrov. Since 2004 Sardinia has hosted the Rally d'Italia Sardegna, a rally competition in the FIA World Rally Championship schedule. The rally is held on narrow, twisty, sandy and bumpy mountainous roads in the north of the island.
Water sports.
On the island of Caprera is located the "Centro Velico Caprera", that is considered the largest school of sailing in the Mediterranean Sea, founded in 1967.
The Yacht Club Costa Smeralda located in Porto Cervo and founded in 1967 is the main yachting club in the island.
Annually the island hosts the Loro Piana Super Yacht Regatta and the Maxy Yacht Rolex Cup.
Part of the Louis Vuitton Trophy was held in the Maddalena archipelago in 2010.
Cagliari hosts regular international regattas, such RC44 championship, Farr 40 World championship and Audi MedCup; all series which boast current America's Cup contenders like BMW Oracle Racing, Mascalzone Latino and Emirates Team New Zealand as contenders.
"Vento di Sardegna" (en: Wind of Sardinia)is a sailboat sponsorized by Autonomous Region of Sardinia. It skipped by Andrea Mura won the Single-Handed Trans-Atlantic Race in 2013, the Two Handed Transatlantic Race (Twostar) regatta in 2012 and the Route du Rhum.
Porto Pollo, north of Palau, is a bay often used by windsurfers and kitesurfers. The bay is divided by a thin tongue of land that separates it in an area for advanced and beginning/intermediate windsurfers. There is also a restricted area for kitesurfers. Many Italian freestyle surfers come to Porto Pollo for training and 2007 saw the finale of the freestyle pro kids Europe 2007 contest. Because of a Venturi effect between Sardinia and Corsica, western wind accelerates between the islands and creates the wind that makes Porto Pollo popular among windsurfing enthusiasts. In 2005, Aglientu, hosted the Kitesurf World Cup in the Vignola's beach.
Winter sports.
Four ski resorts are located on the Gennargentu Range at Separadorgiu, Monte Spada, S'Arena and Bruncu Spina, they are equipped with ski schools, skilifts and ski equipment hire.
Traditional sports.
"S'Istrumpa", also known as Sardinian Wrestling, is a traditional Sardinian sport, officially recognized by the Italian National Olympic Committee (C.O.N.I.) and the International Federation of Celtic Wrestling (I.F.C.W.).
Sardinia boasts ancient equestrian traditions and is the Italian region with the highest number of horse riders (29% of population) and boasts also fine darts tradition, which many believe originated in the Sassari region of the country towards the end of the 15th century. In those days, the darts were carved from Beech ("Fagus") wood and the flights were feathers drawn from the indigenous "pollo sultano" (‘sultana bird’), famed for its spectacular violet-blue plumage.
Environment.
The island has some environmental laws. Following an enormous reforestation plan it has become the Italian region with the largest forest extension. The 50% of the territory is covered by forested areas, 1,213,250 hectares (12,132 km2). The "Corpo forestale e di vigilanza ambientale della Regione Sarda" is the Sardinian Forestry Corps. Sardinia is the Italian region most affected by forest fires during the summer.
The Regional Landscape Plan prohibits new building activities on the coast (except in urban centers), next to forests, lakes or other environmental or cultural sites and the Coastal conservation agency ensures the protection of natural areas on the Sardinian coast.
Renewable energies have increased noticeably in recent years, mainly wind power, favoured by the windy climate, but also solar power (Carlo Rubbia, Nobelist in physics, is creating an experimental solar thermal energy central) and biofuel, based on jatropha oil and colza oil. 586.8 megawatts of wind power capacity were installed on the island at the end of 2009.
Wildlife / Fauna.
Sardinia is home to a wide variety of rare or uncommon animals, such as several species of mammals, many of them belonging to an own subspecies: the Mediterranean monk seal, the Sarcidano horse, the Giara horse, the albino donkey, the Sardinian wild cat ("Felis lybica sarda"), the mouflon, the Sardinian long-eared bat, the Sardinian deer, the fallow deer, the Sardinian fox ("Vulpes vulpes ichnusae"), the Sardinian hare ("Lepus capensis mediterraneus"), the wild boar ("Sus scrofa meridionalis"), the edible dormouse and the European pine marten.
Rare amphibias, found only on the island, are the Sardinian brook salamander, the brown cave salamander, the imperial cave salamander, the Monte Albo cave salamander, the Supramonte cave salamander and the Sarrabus cave salamander ("Speleomantes sarrabusensis"); the Sardinian tree frog instead is found also in Corsica and in Tuscan Archipelago. Among the reptiles worthy of note is the Bedriaga's rock lizard, the Tyrrhenian wall lizard and the Fitzinger's algyroides, endemic species of Sardinia and Corsica. The island is inhabited by terrestrial tortoises and sea turtles like the Hermann's tortoise, the spur-thighed tortoise, the marginated tortoise ("Testudo marginata sarda"), the Nabeul tortoise, the loggerhead sea turtle and the green sea turtle.
Sardinia has four endemic subspecies of birds found nowhere else in the world: its great spotted woodpecker (ssp "harterti"), great tit (ssp "ecki"), common chaffinch (ssp "sarda"), and Eurasian jay (ssp "ichnusae"). It also shares a further 10 endemic subspecies of bird with Corsica. In some cases Sardinia is a delimited part of the species range. For example, the subspecies of hooded crow, "Corvus cornix" ssp "cornix" occurs in Sardinia and Corsica, but no further south.
Birds of prey found are the griffon vulture, the common buzzard, the golden eagle, the long-eared owl, the western marsh harrier, the peregrine falcon, the European honey buzzard, the Sardinian goshawk ("Accipiter gentilis arrigonii"), the Bonelli's eagle and the Eleonora's falcon, whose name comes from Eleonor of Arborea, national heroine of Sardinia, expert in falconry.
The hundreds of lagoons and coastal lakes that dot the island are home for many species of wading birds, such as the greater flamingo.
Conversely, Sardinia lacks many common species such as the viper, the wolf, the bear and the marmot, which are found on the European continent.
The island has also long been used for grazing flocks of indigenous Sardinian sheep. The Sardinian Anglo-Arab is a horse breed that was established in Sardinia, where it has been selectively bred for more than one hundred years.
Three different breeds of dogs are peculiar to Sardinia: the Pastore Fonnese, the Dogo Sardo and the Levriero Sardo.
Natural parks and reserves.
Over 600,000 hectares of Sardinian territory is environmentally preserved (about 25% of the island's territory).
The island has three national parks:
Ten regional parks:
There are 60 wildlife reserves, 5 W.W.F oases, 25 natural monuments and one Geomineral Park, preserved by UNESCO.
Northern Sardinian Coasts are included in the Pelagos Sanctuary for Mediterranean Marine Mammals, a Marine Protected Area, that covers a surface of approximately 84000 km², aimed at the protection of marine mammals.

</doc>
<doc id="29378" url="http://en.wikipedia.org/wiki?curid=29378" title="Scrooge McDuck">
Scrooge McDuck

Scrooge McDuck is a cartoon character created in 1947 as a work-for-hire by Carl Barks for The Walt Disney Company. Scrooge is an elderly Scottish anthropomorphic Pekin Duck with a yellow-orange bill, legs, and feet. He typically wears a red or blue frock coat, top hat, pince-nez glasses, and spats and is portrayed in animations as speaking with a slight Scottish accent, also sometimes known as a Scottish burr. His dominant character trait is his thrift, and within the context of the fictional Disney universe, he is the world's richest person.
Named after Ebenezer Scrooge from the 1843 novel "A Christmas Carol", Scrooge is a wealthy Scottish business magnate and tycoon. He was in his first few appearances characterized as a greedy miser and antihero (as Charles Dickens' original Scrooge was), but in later comics and animated shorts and the modern day he is more often portrayed as a charitable and thrifty hero, adventurer, explorer and philanthropist. Scrooge was created by Barks as a comic book character originally as an antagonist for Donald Duck, first appearing in the 1947 "Four Color" story "Christmas on Bear Mountain" (#178). The character soon became so popular that McDuck became a major figure of the Duck universe. In 1952 he was given his own comic book series, called "Uncle Scrooge", which still runs today. Scrooge was most famously drawn by his creator Carl Barks, and later by Don Rosa. Comics have remained Scrooge's primary medium, although he has also appeared in animated cartoons, most extensively in the television series "DuckTales" (1987–1990).
Along with several other characters in the Disney franchise, Scrooge has enjoyed international popularity, particularly in Europe, and books about him are frequently translated into other languages. He is the maternal uncle of Donald Duck, the grand-uncle of Huey, Dewey and Louie, and a usual financial backer of Gyro Gearloose. His "money bin" and indeed Scrooge himself are often used as humorous metonyms for great wealth in popular culture around the world.
One possible inspiration is an unnamed character in the 1943 "Donald Duck" short film "The Spirit of '43" who was a representation of Donald's thrifty conscience. This anonymous character had many of Scrooge's characteristics including sideburns, pince-nez glasses, and a Scottish accent.
Comics history.
First appearance.
Scrooge McDuck, maternal uncle of previously established character Donald Duck, made his first named appearance in the story "Christmas on Bear Mountain" which was published in Dell's Four Color Comics #178, December 1947, written and drawn by artist Carl Barks. His appearance may have been based on a similar-looking, nameless Scottish character from the 1943 propaganda short "The Spirit of '43".
In "Christmas on Bear Mountain", Scrooge was a bearded, bespectacled, reasonably wealthy old duck, visibly leaning on his cane, and living in isolation in a "huge mansion". Scrooge's misanthropic thoughts in this first story are quite pronounced: "Here I sit in this big lonely dump, waiting for Christmas to pass! Bah! That silly season when everybody loves everybody else! A curse on it! Me—I'm different! Everybody hates me, and I hate everybody!"
Barks later reflected, "Scrooge in 'Christmas on Bear Mountain' was only my first idea of a rich, old uncle. I had made him too old and too weak. I discovered later on that I had to make him more active. I could not make an old guy like that do the things I wanted him to do."
Recurring character.
Barks would later claim that he originally only intended to use Scrooge as a one-shot character, but then decided Scrooge (and his fortune) could prove useful for motivating further stories. Barks continued to experiment with Scrooge's appearance and personality over the next four years.
Scrooge's second appearance, in "The Old Castle's Secret" (first published in June 1948), had Scrooge recruiting his nephews to search for a family treasure hidden in Dismal Downs, the McDuck family's ancestral castle, built in the middle of Rannoch Moor in Scotland. "Foxy Relations" (first published in November 1948) was the first story where Scrooge is called by his title and catchphrase "The Richest Duck in the World".
First hints of Scrooge's past.
The story, "Voodoo Hoodoo", first published in Dell's Four Color Comics #238, August 1949, was the first story to hint at Scrooge's past with the introduction of two figures from it. The first was Foola Zoola, an old African sorcerer and chief of the Voodoo tribe who had cursed Scrooge, seeking revenge for the destruction of his village and the taking of his tribe's lands by Scrooge decades ago.
Scrooge privately admitted to his nephews that he had used an army of "cutthroats" to get the tribe to abandon their lands, in order to establish a rubber plantation. The event was placed by Carl Barks in 1879 during the story, but it would later be retconned by Don Rosa to 1909 to fit with Scrooge's later-established personal history. 
The second figure was Bombie the Zombie, the organ of the sorcerer's curse and revenge. He had reportedly sought Scrooge for decades before reaching Duckburg, mistaking Donald for Scrooge.
Barks, with a note of skepticism often found in his stories, explained the zombie as a living person who has never died, but has somehow gotten under the influence of a sorcerer. Although some scenes of the story were intended as a parody of Bela Lugosi's "White Zombie", the story is the first to not only focus on Scrooge's past but also touch on the darkest aspects of his personality.
Later stories.
"Trail of the Unicorn", first published in February 1950, introduced Scrooge's private zoo. One of his pilots had managed to photograph the last living unicorn, which lived in the Indian part of the Himalayas. Scrooge offered a reward to competing cousins Donald Duck and Gladstone Gander, which would go to the one who captured the unicorn for Scrooge's collection of animals.
This was also the story that introduced Scrooge's private airplane. Barks would later establish Scrooge as an experienced aviator. Donald had previously been shown as a skilled aviator, as was Flintheart Glomgold in later stories. In comparison, Huey, Dewey, and Louie were depicted as only having taken flying lessons in the story "Frozen Gold" (published in January 1945).
"The Pixilated Parrot", first published in July 1950, introduced the precursor to Scrooge's money bin; in this story, Scrooge's central office building is said to contain "three cubic acres of money." Two nameless burglars who briefly appear during the story are considered to be the precursors of the Beagle Boys.
Scrooge as a major character.
"The Magic Hourglass", first published in September 1950, was arguably the first story to change the focus of the Duck stories from Donald to Scrooge. During the story, several themes were introduced for Scrooge.
Donald first mentions in this story that his uncle practically owns Duckburg, a statement that Scrooge's rival John D. Rockerduck would later put in dispute. Scrooge first hints that he was not born into wealth, as he remembers buying the Hourglass in Morocco when he was a member of a ship's crew as a cabin boy. It is also the first story in which Scrooge mentions speaking another language besides his native English and reading other alphabets besides the Latin alphabet, as during the story, he speaks Arabic and reads the Arabic alphabet.
The latter theme would be developed further in later stories. Barks and current Scrooge writer Don Rosa have depicted Scrooge as being fluent in Arabic, Dutch, German, Mongolian, Spanish, Mayan, Bengali, Finnish, and various dialects of Chinese. Scrooge acquired this knowledge from years of living or traveling to the various regions of the world where those languages are spoken. Later writers would depict Scrooge having at least working knowledge of several other languages.
Scrooge was shown in "The Magic Hourglass" in a more positive light than in previous stories, but his more villainous side is present too. Scrooge is seen in this story attempting to reacquire a magic hourglass that he gave to Donald, before finding out that it acted as a protective charm for him. Scrooge starts losing one billion dollars each minute, and comments that he will go bankrupt within 600 years. This line is a parody of Orson Welles's line in "Citizen Kane" "You know, Mr. Thatcher, at the rate of a million dollars a year, I'll have to close this place in... 60 years". To convince his nephews to return it, he pursues them throughout Morocco, where they had headed to earlier in the story. Memorably during the story, Scrooge interrogates Donald by having him tied up and tickled with a feather in an attempt to get Donald to reveal the hourglass's location. Scrooge finally manages to retrieve it, exchanging it for a flask of water, as he had found his nephews exhausted and left in the desert with no supplies. As Scrooge explains, he intended to give them a higher offer, but he just could not resist having somebody at his mercy without taking advantage of it.
Final developments.
"A Financial Fable", first published in March 1951, had Scrooge teaching Donald some lessons in productivity as the source of wealth, along with the laws of supply and demand. Perhaps more importantly, it was also the first story where Scrooge observes how diligent and industrious Huey, Louie and Dewey are, making them more similar to himself rather than to Donald. Donald in Barks's stories is depicted as working hard on occasion, but given the choice often proves to be a shirker. The three younger nephews first side with Scrooge rather than Donald in this story, with the bond between granduncle and grandnephews strengthening in later stories. However, there have been rare instances where Donald proved invaluable to Scrooge, such as when the group traveled back in time to Ancient Egypt to retrieve a pharaoh's papyrus. Donald cautions against taking it with him, as no one would believe the story unless it was unearthed. Donald then buries it and makes a marking point from the Nile River, making Scrooge think to himself admiringly "Donald must have swallowed the Encyclopedia Britannica!"
"Terror of the Beagle Boys", first published in November 1951, introduced the readers to the Beagle Boys, although Scrooge in this story seems to be already familiar with them. "The Big Bin on Killmotor Hill" introduced Scrooge's money bin, built on Killmotor Hill in the center of Duckburg.
By this point, Scrooge had become familiar to readers in the United States and Europe. Other Disney writers and artists besides Barks began using Scrooge in their own stories, including Italian writer Romano Scarpa. Western Publishing, the then-publisher of the Disney crafty comics, started thinking about using Scrooge as a protagonist rather than a supporting character, and then decided to launch Scrooge in his own self-titled comic. "Uncle Scrooge" #1, featuring the story "Only a Poor Old Man", was published in March 1952 – 1953. This story along with "Back to the Klondike", first published a year later in March 1953, became the biggest influences in how Scrooge's character, past, and beliefs would become defined.
After this point, Barks produced most of his longer stories in "Uncle Scrooge", with a focus mainly on adventure, while his ten-page stories for Walt Disney's Comics and Stories continued to feature Donald as the star and focused on comedy. In Scrooge's stories, Donald and his nephews were cast as Scrooge's assistants, who accompanied Scrooge in his adventures around the world. This change of focus from Donald to Scrooge was also reflected in stories by other contemporary writers. Since then, Scrooge remains a central figure of the Duck comics' universe, thus the coining of the term "Scrooge McDuck Universe".
The modern era.
After Barks's retirement, the character continued under other artists. In 1972, Barks was persuaded to write more stories for Disney. He wrote Junior Woodchuck stories where Scrooge often plays the part of the villain, closer to the role he had before he acquired his own series. Under Barks, Scrooge always was a malleable character who would take on whatever persona was convenient to the plot.
The Italian writer and artist Romano Scarpa made several additions to Scrooge McDuck's universe, including characters such as Brigitta McBridge, Scrooge's self-styled fiancée, and Gideon McDuck, a newspaper editor who is Scrooge's brother. Those characters have appeared mostly in European comics. So is also the case for Scrooge's rival John D. Rockerduck (created by Barks for just one story) and Donald's cousin Fethry Duck, who sometimes works as a reporter for Scrooge's newspaper.
Another major development was the arrival of writer and artist Don Rosa in 1986 with his story "The Son of the Sun", released by Gladstone Publishing and nominated for a Harvey Award, one of the comics industry's highest honors. Rosa has said in interviews that he considers Scrooge to be his favorite Disney character. Unlike most other Disney writers, Don Rosa considered Scrooge as a historical character whose Disney adventures had occurred in the fifties and sixties and ended (in his undepicted death) in 1967 when Barks retired. He considered only Barks' stories canonical, and fleshed out a timeline as well as a family tree based on Barks' stories. Eventually he wrote and drew "The Life and Times of Scrooge McDuck", a full history in twelve chapters which received an Eisner Award in 1995. Later editions included additional chapters. Under Rosa, Scrooge became more ethical; while he never cheats, he ruthlessly exploits any loopholes. He owes his fortune to his hard work and his money bin is "full of souvenirs" since every coin reminds him of a specific circumstance. Rosa remains the foremost contemporary duck artist and has been nominated for five 2007 Eisner Awards. His work is regularly reprinted by itself as well as along with Barks stories for which he created a sequel.
Daan Jippes, who can mimic Barks's art to a close extent, repenciled all of Barks's 1970s Junior Woodchucks stories, as well as Barks' final Uncle Scrooge stories, from the 1990s to the early 2000s. Other notable Disney artists who have worked with the Scrooge character include Marco Rota, William Van Horn, and Tony Strobl.
In an interview with the Norwegian "Aftenposten" from 1992 Don Rosa says that "in the beginning Scrooge [owed] his existence to his nephew Donald, but that has changed and today it's Donald that [owes] his existence to Scrooge" and he also says that this is one of the reasons why he is so interested in Scrooge.
Characterization.
Wealth.
Scrooge has worked his way up the financial ladder from humble immigrant roots. As a young boy, he took up a job polishing and shining boots in his native Glasgow. His turning point was when a ditchdigger paid him with an 1875 US dime, which was useless as currency in 19th century Glasgow. Enraged, Scrooge vowed to never be taken advantage of again. He takes a position as cabin boy on a Clyde cattle ship to the United States to make his fortune at the age of 13. In 1898, after many adventures he finally ends up in Klondike, where he finds a golden rock the size of a goose's egg. By the following year he had made his first $1,000,000 and bought the deed for Killmule Hill from Casey Coot, the son of Clinton Coot and grandson of Cornelius Coot. He finally ends up in Duckburg in 1902. After some dramatic events where he faces both the Beagle Boys and president Roosevelt and his "Rough Riders" at the same time, he tears down the rest of the old fort Duckburg and builds his famous Money Bin at the site. In the years to follow, Uncle Scrooge travels all around the world in order to increase his fortune, while his family remained behind to manage the Money Bin. When Scrooge finally returns to Duckburg, he is the richest duck in the world, rivaled only by Flintheart Glomgold, John D. Rockerduck and, less prominently, the maharaja of the fictional country Howdoyoustan (play on Hindustan). His experiences, however, had changed him into a hostile miser, and he made his own family leave. 12 years later, he closed his empire down, but eventually returned to a public life 5 years later and started his business.
He keeps the majority of his wealth in a massive Money Bin overlooking the city of Duckburg. In the short "Scrooge McDuck and Money", he remarks to his nephews that this money is "just petty cash". In the Dutch and Italian version he regularly forces Donald and his nephews to polish the coins one by one in order to pay off Donald's debts—Scrooge will not even pay them much for this lengthy, tedious, hand-breaking work. As far as he is concerned, even 5 cents an hour is too much expenditure.
A shrewd businessduck and noted tightwad, he is fond of diving into and swimming in his money, without injury. He is also the richest member of The Billionaires Club of Duckburg, a society which includes the most successful businessmen of the world and allows them to keep connections with each other. Glomgold and Rockerduck are also influential members of the Club. His most famous prized possession is his Number One Dime.
The sum of Scrooge's wealth is unclear. According to Barks' "The Second Richest Duck" as noted by a "TIME" article, Scrooge is worth "one multiplujillion, nine obsquatumatillion, six hundred twenty-three dollars and sixty-two cents." In the "DuckTales" episode "Liquid Assets", Fenton Crackshell (Scrooge's accountant) notes that McDuck's money bin contains "607 tillion 386 zillion 947 trillion 522 billion dollars and 36 cents". Don Rosa's "Life and Times of Scrooge McDuck" notes that Scrooge amounts to "five multiplujillion, nine impossibidillion, seven fantasticatrillion dollars and sixteen cents". A thought bubble from Scrooge McDuck sitting in his car with his Chauffeur in "Walt Disney's Christmas Parade" No.1 (Published in 1949) that takes place in the story "Letter to Santa" clearly states "What's the use of having "eleven octillion dollars" if I don't make a big noise about it?" "Forbes" has occasionally tried to estimate McDuck's wealth in real terms; in 2007, Forbes estimated his wealth at $28.8 billion; in 2011, it rose to $44.1 billion due to the rise in gold prices. One website used the size of Scrooge's Money Bin as a basis and calculated that it could contain over $27 trillion. Barks himself has said that the fortune is five billion quintiplitilion unptuplatillion multuplatillion impossibidillion fantasticatrillion dollars. Whatever the amount, Scrooge never considers it to be enough; he believes that he has to continue to earn money by any means possible. A running gag is Scrooge always making profit on any business deal.
Education.
Scrooge never completed a formal education, as he left school at an early age. However, he has a sharp mind and is always ready to learn new skills. Because of his secondary occupation as a treasure hunter, Scrooge has become something of a scholar and an amateur archaeologist. Starting with Barks, several writers have explained how Scrooge becomes aware of the treasures he decides to pursue. This often involves periods of research consulting various written sources in search of passages that might lead him to a treasure. Often Scrooge decides to search for the possible truth behind old legends, or discovers obscure references to the activities of ancient conquerors, explorers and military leaders that he considers interesting enough to begin a new expedition.
As a result of his research, Scrooge has built up an extensive personal library, which includes many rare tomes. In Barks's and Rosa's stories, among the prized pieces of this library is an almost complete collection of Spanish and Dutch naval logs of the 16th and 17th centuries. Their references to the fates of other ships have often allowed Scrooge to locate sunken ships and recover their treasures from their watery graves. Mostly self-taught as he is, Scrooge is a firm believer in the saying "knowledge is power". Scrooge is also an accomplished linguist and entrepreneur, having learned to speak several different languages during his business trips around the world, selling fridges to eskimos, wind to windmill manufacturers in the Netherlands etc.
Morality and beliefs.
Both as a businessman and as a treasure hunter, Scrooge is noted for his drive to set new goals and face new challenges. As Carl Barks described his character, for Scrooge there is "always another rainbow". The phrase later provided the title for one of Barks's better-known paintings depicting Scrooge. Periods of inactivity between adventures and lack of serious challenges tend to be depressing for Scrooge after a while; some stories see these phases take a toll on his health. Scrooge's other motto is "Work smarter, not harder."
As a businessman, Scrooge often resorts to aggressive tactics and deception. He seems to have gained significant experience in manipulating people and events towards his own ends. As often seen in stories by writer Guido Martina and occasionally by others, Scrooge is noted for his cynicism, especially towards ideals of morality when it comes to business and the pursuit of set goals. This has been noted by some as not being part of Barks's original profile of the character, but has since come to be accepted as one valid interpretation of Scrooge's way of thinking.
Scrooge seems to have a personal code of honesty that offers him an amount of self-control. He can often be seen contemplating the next course of action, divided between adopting a ruthless pursuit of his current goal against those tactics he considers more honest. At times, he can sacrifice his goal in order to remain within the limits of this sense of honesty. Several fans of the character have come to consider these depictions as adding to the depth of his personality, because based on the decisions he takes Scrooge can be both the hero and the villain of his stories. This is one thing he has in common with his nephew Donald. Scrooge's sense of honesty also distinguishes him from his rival Flintheart Glomgold, who places no such self-limitations. During the cartoon series "DuckTales", at times he would be heard saying to Glomgold, "You're a cheater, and cheaters "never" prosper!"
Scrooge has a nasty temper and rarely hesitates to use violence against those who provoke his ire (often his nephew Donald, but also bill and tax collectors as well as door-to-door salesmen); however, he seems to be against the use of lethal force. On occasion, he has even saved the lives of enemies who had threatened his own life but were in danger of losing their own. According to Scrooge's own explanation, this is to save himself from feelings of guilt over their deaths; he generally awaits no gratitude from them. Scrooge has also opined that only in fairy tales do bad people turn good, and that he is old enough to not believe in fairy tales. Scrooge believes in keeping his word—never breaking a promise once given. In Italian-produced stories of the 1950s to 1970s, however, particularly those written by Guido Martina, Scrooge often acts differently from in American or Danish comics productions.
Carl Barks gave Scrooge a definite set of ethics which were in tone with the time he was supposed to have made his fortune. The robber barons and industrialists of the 1890–1920s era were McDuck's competition as he earned his fortune. Scrooge proudly asserts "I made it by being tougher than the toughies and smarter than the smarties! And I made it square!" It is obvious that Barks's creation is averse to dishonesty in the pursuit of wealth. When Disney filmmakers first contemplated a Scrooge feature cartoon in the fifties, the animators had no understanding of the Scrooge McDuck character and merely envisioned Scrooge as a duck version of Ebenezer Scrooge—a very unsympathetic character. In the end they shelved the idea because a duck who gets all excited about money just was not funny enough.
In an interview, Barks summed up his beliefs about Scrooge and capitalism:
I've always looked at the ducks as caricatured human beings. In rereading the stories, I realized that I had gotten kind of deep in some of them: there was philosophy in there that I hadn't realized I was putting in. It was an added feature that went along with the stories. I think a lot of the philosophy in my stories is conservative—conservative in the sense that I feel our civilization peaked around 1910. Since then we've been going downhill. Much of the older culture had basic qualities that the new stuff we keep hatching can never match.
Look at the magnificent cathedrals and palaces that were built. Nobody can build that sort of thing nowadays. Also, I believe that we should preserve many old ideals and methods of working: honor, honesty, allowing other people to believe in their own ideas, not trying to force everyone into one form. The thing I have against the present political system is that it tries to make everybody exactly alike. We should have a million different patterns.
They say that wealthy people like the Vanderbilts and Rockefellers are sinful because they accumulated fortunes by exploiting the poor. I feel that everybody should be able to rise as high as they can or want to, provided they don't kill anybody or actually oppress other people on the way up. A little exploitation is something you come by in nature. We see it in the pecking order of animals—everybody has to be exploited or to exploit someone else to a certain extent. I don't resent those things.
"DuckTales".
In the "DuckTales" series, Scrooge has adopted the nephews (as Donald has joined the Navy and is away on his tour of duty), and as a result his rougher edges are smoothed out somewhat. While most of his traits remain from the comics, he is notably more jovial and less irritable in the cartoon. In an early episode, Scrooge credits his improved temperament to the nephews and Webby (his housekeeper's granddaughter, who comes to live in Scrooge's mansion), saying that "for the first time since I left Scotland, I have a family." Though Scrooge is far from heartless in the comics, he is rarely so openly sentimental. While he still hunts for treasure in Ducktales, many episodes focus on him attempting to thwart villains. He remains, however, just as tightfisted with money as he has always been. Scrooge displays a strict code of honor, insisting that the only valid way to acquire wealth is to "earn it square", and he goes to great lengths to thwart those (sometimes even his own nephews) who gain money dishonestly. This code also prevents him from ever being dishonest himself, saying that "Scrooge McDuck's word is as good as gold." He also expresses great disgust at being viewed by others as a greedy liar and cheater. The show fleshed out his upbringing depicting his life as an individual who worked hard his entire life to earn his keep and fiercely defend it against those who were truly dishonest: a value he teaches his nephews. Also it was shown that money is no longer the most important thing in his life. For one episode he was under a love spell, which caused him to lavish his time on a goddess, over everything else. The nephews find out that the only way to break the spell, is make the person realize that the object of their love will cost them something they truly love. The boys make it appear that Scrooge's love is allergic to money; however, he simply decides to give up his wealth so he can be with her. Later, when he realizes he will have to give up his nephews to be with her, the spell is immediately broken, showing that family is the most important thing to him. On occasion he demonstrates physical fitness by single-handedly beating bigger foes. He credits his strength to "lifting money bags."
Europe.
Many of the European comics based on the Disney Universe have created their own version of Scrooge McDuck, usually involving him in slapstick adventures. This is particularly true of the Italian comics which were very popular in the 1960s, 70s and 80s in most parts of Western continental Europe. In these, Scrooge is mainly an anti-hero dragging his long-suffering nephews into treasure hunts and shady business deals. Donald is a reluctant participant in these travels, only agreeing to go along when his uncle reminds him of the debts and back-rent Donald owes him, threatens him with a sword or blunderbuss or offers a share of the loot. When he promises Donald a share of the treasure, Scrooge will add a little loophole in the terms which may seem obscure at first but which he brings up at the end of the adventure to deny Donald his share, keeping the whole for himself. After Donald risks life and limb – something which Scrooge shows little concern for – he tends to end up with nothing.
Another running joke is Scrooge reminiscing on his adventures while gold prospecting in the Klondike much to Donald and the nephews' chagrin at hearing the never-ending and tiresome stories.
Age.
Scrooge's age has never been specified, although according to Barks, Scrooge was born in Scotland in 1867, and earned his Number One Dime exactly ten years later. Although Rosa stated that Scrooge died at the age of 100, that has never been published in the comics. The "DuckTales" episodes show a Scrooge who hailed from Scotland in the 19th Century, yet was clearly familiar with all the technology and amenities of the 1980s. Despite this case of extreme old age, Scrooge has not appeared to be on dotage's door, and has been strong enough to keep up with his nephews in adventures, with rare exception there appears to be no sign of him slowing down. Barks explained to some fan letters asking about Scrooge's Adamic age that in the story "That's No Fable!", where Scrooge drank water from a Fountain of Youth for several days, rather than making him young again (bodily contact with the water was required for that), ingesting the water rejuvenated his body and cured him of his rheumatia, which arguably allowed Scrooge to live beyond his expected years with no sign of slowdown or senility.
Impact.
Scrooge McDuck Universe.
The popularity of Scrooge McDuck comics spawned an entire mythology around the character, including new supporting characters, adventures, and life experiences as told by numerous authors. The popularity of the Duck universe – the fandom term for the associated intellectual properties that have developed from Scrooge's stories over the years, including the city of Duckburg – has led Don Rosa to claim that "in the beginning Scrooge [owed] his existence to his nephew Donald, but that has changed and today it's Donald that [owes] his existence to Scrooge."
In addition to the many original and existing characters in stories about Scrooge McDuck, authors have frequently led historical figures to meet Scrooge over the course of his life. Most notably, Scrooge has met U.S. president Theodore Roosevelt. Roosevelt and Scrooge would meet each other at least three times: in the Dakotas in 1883, in Duckburg in 1902, and in Panama in 1906. "See Historical Figures in Scrooge McDuck stories".
Based on writer Don Rosa's "The Life and Times of Scrooge McDuck", a popular timeline chronicling Scrooge's adventures was created consisting of the most important "facts" about Scrooge's life. "See Scrooge McDuck Timeline according to Don Rosa".
In 2014, composer Tuomas Holopainen of Nightwish released a conceptual album based on the book, "The Life and Times of Scrooge McDuck". The album is titled "Music Inspired by the Life and Times of Scrooge". Don Rosa illustrated the cover artwork for the album.
In other media.
The character of Scrooge has appeared in various mediums aside from comic books. Scrooge's first appearance in animated form (save for a brief "Mickey Mouse Club" television series cameo) was in Disney's 1967 theatrical short "Scrooge McDuck and Money" (voiced by Bill Thompson), in which he teaches his nephews basic financial tips.
He later appeared as Ebenezer Scrooge in "Mickey's Christmas Carol" (1983), an animated version of the Dickens classic. In this adaptation Scrooge's character is voiced by co-writer Alan Young (Known for playing Wilbur Post on Mister Ed from 1961 to 1965). He also appeared as himself in the television special "Sport Goofy in Soccermania" (1987) (the only time when he was voiced by Will Ryan).
Scrooge's biggest role outside comics would come in the 1987 animated series "DuckTales", a series loosely based on Carl Barks's comics, and where Alan Young returned to voice his character. In this series, premiered over two-hours on September 18, 1987, while the regular episodes began three days later, Scrooge becomes caretaker of Huey, Dewey and Louie when Donald joins the United States Navy. Scrooge's "DuckTales" persona is considerably softer than in most previous appearances; his ruthlessness is played down considerably and his often abrasive personality is reduced in many episodes to that of a crotchety but lovable old uncle. Still, there are flashes of Barks' Scrooge to be seen, particularly in early episodes of the first season. After the series Scrooge also appeared in "". He was mentioned in the "Darkwing Duck" episode "Tiff of the Titans", but never really seen.
He has appeared in some episodes of "Raw Toonage", two shorts of "Mickey Mouse Works" and some episodes (specially "House of Scrooge") of "Disney's House of Mouse", as well as the direct-to-video films "Mickey's Once Upon a Christmas" and "Mickey's Twice Upon a Christmas". His video game appearances include the three "DuckTales" releases ("DuckTales", "DuckTales 2", and "DuckTales - the Quest for Gold"), and in "Toontown Online" as the accidental creator of the Cogs. Additionally, he is a secret playable character in 2008 quiz game, "Disney TH!NK Fast". In the 2012 Nintendo 3DS game "", he is one of the first characters Mickey rescues, running a shop in the fortress selling upgrades and serving as a Sketch summon in which he uses his cane pogostick from the Ducktales NES games.
In 1961 a 45rpm single record was released entitled "Donald Duck and Uncle Scrooge's Money Rocket" (a.k.a. Uncle Scrooge's Rocket to the Moon,) a story of how Scrooge builds a rocket to send all his money to the moon to protect it from the "Beagle Boys."
Scrooge also makes an appearance in Disney's and Square Enix's Kingdom Hearts series, in a role where he helps Mickey Mouse set up a world transit system. He first appears in "Kingdom Hearts II" as a minor non-playable character in Hollow Bastion, where he is trying to recreate his favorite ice cream flavor – sea-salt. Scrooge later appears in the prequel, "", this time with a speaking role. He's working on establishing an ice-cream business in Radiant Garden and gives Ventus three passes to the Dream Festival in Disney Town. Young reprises the role in the English version of "Birth by Sleep".
Scrooge has appeared in the Boom! Studios "Darkwing Duck" comic, playing a key role at the end of its initial story, "The Duck Knight Returns". Later he would also play a key role on the final story arc entitled "Dangerous Currency", where he teams up with Darkwing Duck in order to stop the Phantom Blot and Magica De Spell from taking over St. Canard and Duckburg.
In 2015, Scrooge was seen in the Mickey Mouse short Goofy's First Love, where Mickey and Donald are trying to help Goofy find his love. Donald suggests money, and they head over to Scrooge's mansion where Donald tells his uncle that Goofy needs a million dollars. Scrooge then has his butler kick them out.
In popular culture.
"Forbes" magazine routinely lists Scrooge McDuck on its annual "Fictional 15" list of the richest fictional characters by net worth:
In tribute to its famous native, Glasgow City Council added Scrooge to its list of "Famous Glaswegians" in 2007, alongside the likes of Billy Connolly and Charles Rennie Mackintosh.
In 2008 "The Weekly Standard" parodied the bailout of the financial markets by publishing a memo where Scrooge applies to the TARP program.
An extortionist named Arno Funke targeted German department store chain Karstadt from 1992 until his capture in 1994, under the alias "Dagobert", the German (first) name for Scrooge McDuck.
In the "Family Guy" episode "Lottery Fever", Peter injures himself trying to dive into a pile of coins like Scrooge McDuck.
Dagobertducktaks ("Dagobert Duck" is the Dutch name for Scrooge McDuck), a tax for the wealthy, was elected Dutch word of the year 2014 in a poll by Van Dale.

</doc>
<doc id="29379" url="http://en.wikipedia.org/wiki?curid=29379" title="Shiva (Judaism)">
Shiva (Judaism)

Shiva (Hebrew: שבעה‎) (literally "seven") is the week-long mourning period in Judaism for first-degree relatives: father, mother, son, daughter, brother, sister, and spouse. The ritual is referred to as "sitting shiva." Immediately after burial, people assume the "halakhic" status of "avel" (Hebrew: אבל ; "mourner"). This state lasts for seven days, during which family members traditionally gather in one home (preferably the home of the deceased) and receive visitors. At the funeral, mourners traditionally tear an outer garment, a ritual known as "keriah". This garment is worn throughout shiva.
Etymology.
The word Shiva comes from the Hebrew word "shiv'ah", which literally means "seven". The tradition was developed in response to the story in Genesis 50:1-14 in which Joseph mourns the death of his father Jacob (Israel) for seven days.
Length of shiva.
The Hebrew word "shiva" means "seven", and the official shiva period is seven days. The day of the funeral is counted as the first day of shiva, even though the practice does not begin until after the mourner(s) arrive at the designated location following the funeral. On day seven, shiva generally ends in the morning, following services. On Shabbat during the week of shiva, no formal mourning takes place, but the day is counted as one of the seven. Sometimes, a minyan with a Torah reading will take place at the mourner's house.
If the first day of a "Yom Tov" (holy days which includes Rosh Hashanah, Yom Kippur, Sukkot, Passover, and Shavuot) occurs during shiva, the shiva ends, regardless of the number of days that have already been observed. Even if a Yom Tov begins at nightfall on the day of the funeral, the remainder of shiva is cancelled.
If the death occurs during "Yom Tov", shiva does not begin until the burial is completed. Burial may not take place on "Yom Tov", but can on "Chol HaMoed" (the intermediate days of Sukkot or Passover). Burial can also take place on the second day of "Yom Tov" in the Diaspora. In addition, it is also permitted to delegate the burial to gentiles even on the first day, though such is not usually done.
If a burial occurs on "Chol HaMoed" of Passover, shiva does not begin until after the Yom Tov is completed. In the Diaspora, where most "Yom Tovim" are observed for two days, mourning does not take place on the second day, but the day is still counted as one of the days of shiva.
Shiva customs.
Traditionally, the first meal after the funeral, the "seudat havra'ah" (Hebrew: סעודת הבראה ; "meal of comforting"), is supplied by neighbors and friends. The mourners do not bathe or shower for pleasure, they do not wear leather shoes or jewelry, men do not shave, and in many communities household mirrors are covered. The prohibition of bathing includes bathing or showering the whole body, or using hot water. It is permitted to wash separately various parts of the body in cool water. Marital relations and Torah study are not permitted. (It is permitted to study the laws of mourning, as well as that material which may be studied on "Tisha B'Av", including Job, Lamentations, portions of Jeremiah and the third chapter of Talmud tractate "Moed Katan".) No public mourning may occur on Shabbat, nor may the burial take place on Shabbat; "private" mourning restrictions continue during the Shabbat. It is customary for the mourners to sit on low stools, or even the floor, symbolic of the emotional reality of being "brought low" by the grief. Typically, mourners do not return to work until the end of the week of mourning.
Many communities have an arrangement where members of the "chevra kadisha" (local Jewish burial society) organise the meals for the mourners, and serve refreshments for visitors. If prayer services are organized in the house of mourning, it is customary for an adult mourner to lead the prayers.
Visiting a shiva home.
It is considered a great "mitzvah" (literally "commandment" but usually interpreted as "good deed") of kindness and compassion to pay a home visit ("make" or "pay a shiva call") to the mourners, a practice known as "Nichum Aveilim". Traditionally, no greetings are exchanged and visitors wait for the mourners to initiate conversation, or remain silent if the mourners do not do so, out of respect for their bereavement. Once engaged in conversation by the mourners, it is appropriate for visitors to talk about the deceased, sharing stories of their life. Some mourners use the "shiva" as a distraction from their loss, other mourners prefer to openly experience their grief together with friends and family.
Upon leaving an Ashkenazic shiva house, visitors recite a traditional blessing: "May God comfort you among the other mourners of Zion and Jerusalem" (המקום ינחם אתכם בתוך שאר אבלי ציון וירושלים, transliterated "HaMakom yenachem etchem betoch sha'ar aveylei Tziyon viYerushalayim"). At a Sephardic shiva house, visitors say, "May Heaven comfort you" (מן השמים תנוחמו – "min haShamayim tenuchamu").
It is considered a mitzvah for visitors to bring prepared food for the mourners. Among Sephardic Jews, food is served to the visitors and it is considered a mitzvah to make a blessing on the food in the merit of the deceased. Sephardim believe that every beracha (blessing) said elevates the "neshama" (soul) of the deceased, and so one should eat a variety of foods to be able to say more than one beracha. In an Ashkenazic home of mourning, food is not served except for the possibility of a light breakfast as a courtesy to those attending "Shaharit" (morning prayer), since they generally go straight to work after the service. The mourner is not allowed to serve food to the visitors and it is family and friends who take care of the guests and everyday issues.
Leaving the shiva house.
Leaving the shiva house is permitted when traveling between two locations where shiva is being observed by different members of the family, in cases of pikuach nefesh, e.g., a human life is in danger, whether that of the mourner or someone else; when something must be done to prevent another person from suffering and no one else can do it, such as caring for a child or an elderly or sick person; to feed or care for one's animals if there is no one else to do so; if another relative for whom the mourner is required to sit shiva dies, the mourner may attend the funeral. Leaving the house is also permitted on Shabbat.
Generally, one does not work or conduct business during shiva, although an exception may be made for those whose duties involve pikuach nefesh (doctors, nurses and emergency medical technicians). The same is true for mourners who are liable to suffer serious economic loss. If a mourner shares a business with a partner, and the partner can operate the business alone, the partner shall run the business. The partner is entitled to keep all profits made during the time, but if the partner does not exert additional effort, and the mourner will suffer economic loss, the partner is encouraged to donate the profits to the mourner, considering it "tzedaka". A mourner may do the minimal amount of work necessary in order to assure the survival of a business, or if his position is important in meeting the needs of the public and no substitute can be found. This includes elected officials whose work is necessary for the citizens. During the shiva period, the mourner is permitted to give instructions on how to handle business in his absence.
Shiva minyan.
During shiva, a "minyan" (a quorum of ten or more adult Jews) traditionally gather at the shiva home for services. The services held are like those at a synagogue, except that certain prayers or verses are either added or omitted. On days that the Torah is read in a synagogue, it is likewise read at the shiva home. An effort is made by the community to lend a Torah scroll to the mourner for this purpose. Kaddish is recited during the services; the mourner, if eligible, may recite kaddish.
Keriah.
The torn garment, usually a shirt, jacket or vest that "covers the heart," is worn throughout the shiva period (a practice known as "keriah"; alternative spellings "keriyah", "kria"), except on Shabbat. Conservative and Reform Jews will usually wear a torn piece of black ribbon instead of a torn garment. The torn garment symbolizes and expresses the grief of the mourner.

</doc>
<doc id="29381" url="http://en.wikipedia.org/wiki?curid=29381" title="Semi-trailer truck">
Semi-trailer truck

A semi-trailer truck is the combination of a tractor unit and one or more semi-trailers to carry freight. It is variously known as a "transport (truck)" in Canada; "semi" or "single" in Australia; "semi", "tractor-trailer", "big rig", or "eighteen-wheeler" in the United States; "articulated lorry", abbreviated "artic", in Britain and Ireland.
A semi-trailer attaches to the tractor with a fifth wheel hitch, with much of its weight borne by the tractor. The result is that both tractor and semi-trailer will have a distinctly different design than a rigid truck and trailer.
Regional configurations.
North America.
In North America, the combination vehicles made up of a powered truck and one or more detachable trailers, are known as "semi-tractor-trailers", "tractor-trailers", "semis", "big rigs", "semi trucks" or "eighteen-wheelers".
Trucks.
The "tractors", or powered trucks, typically have two or three axles; those built for hauling heavy-duty commercial-construction machinery may have as many as four or five axles, some often being lift axles.
The most common tractor-cab layout has a forward engine, one steering axle, and two drive axles.
The fifth-wheel trailer coupling on most tractor trucks is movable fore and aft, to allow adjustment in the weight distribution over its rear axle(s).
Ubiquitous in Europe, but less common in North America since the 1990s, is the cabover configuration, where the driver sits next to, or over the engine. With changes in the US to the maximum length of the combined vehicle, the cabover was mostly phased out of North American "over-the-road" or "long-haul" service by 2007. Cabovers were notorious for being difficult to service, as the cab could not be lifted on its hinges to a full 90-degree forward tilt, and this severely limited access to the front part of the engine.
Trucks average between 4 and, with fuel economy standards requiring more than 7 mpgus efficiency by 2014.
Trailers.
The cargo trailer usually has a tandem axle pair at the rear, each of which has dual wheels, or eight wheels on the trailer, four per axle. The combination of eight tires on the trailer and ten tires on the tractor is what led to the moniker "eighteen wheeler", although this term is considered by some truckers to be a misnomer. Many trailers are equipped with movable tandem axles to allow adjusting the weight distribution.
The United States also allows two-axle tractors to pull two single-axle 28.5 ft semi-trailers, known officially as "STAA doubles", and colloquially as "doubles", "a set", or "a set of joints", on all highways that are part of the national network.
To connect the second of a set of doubles to the first trailer, and to support the front half of the second trailer, a "converter gear", also known as a "con-gear" or "dolly" is used. This apparatus has one or two axles, a fifth-wheel coupling for the rear trailer, and a tongue with a ring-hitch coupling for the forward trailer. Individual states may further allow longer vehicles, known as "longer combination vehicles" (or LCVs), and may allow them to operate on roads other than those that are part of the national network.
LCV types include:
Future LCV's under consideration and study for the U.S. MAP-21 transportation bill are container doubles. These combinations are under study for potential recommendation in November 2014:
Regulations on LCVs vary widely from one state or province to another. None allows more than three trailers without a special permit. Reasons for limiting the legal trailer configurations include both safety concerns and the impracticality of designing and constructing roads that can accommodate the larger wheelbase of these vehicles and the larger minimum turning radii associated with them.
Most states restrict operation of larger tandem trailer setups such as triple units, "turnpike doubles" and "Rocky-Mountain doubles". In general, these configurations are restricted to turnpikes. Except for these units, tandem setups are not restricted to certain roads any more than a single setup. They are also not restricted by weather conditions or "difficulty of operation". The Canadian province of Ontario, however, does have .
In the United States, 80000 lb is the maximum allowable legal gross vehicle weight without a permit.
The axle-weight breakdown is:
Over-length and overweight permits are issued by each individual state whose roads will be traveled. The permits are usually issued in advance, for a specific period of time, over a specific route, with a specific load. Most over-length loads require one or more escort vehicles. An escort is an accompanying automobile and its driver, who communicates with the driver of the payload vehicle regarding the position of the load in relation to the road and shoulder, and about other situational considerations.
A trailer's dimensions can vary greatly, depending on the amount and type of cargo it is designed to haul. In the United States, they are normally limited to 8.5 ft in width. (See types of trailers under Construction, below.)
Europe.
The noticeable difference between tractor units in the North American and Europe is that almost all European models are "cab over engine" (COE or "forward control"), while the majority of North American trucks are "conventional" (or "normal control"). For repairs, the entire cab hinges forward to allow maintenance access. European trucks, whether rigid or fully articulated, have a sheer face on the front. This allows for shorter trucks with longer trailers (with larger freight capacity) within the legal maximum total length. Furthermore, it offers greater maneuverability and better overview for the driver. Conversely, "conventional" cab tractors offer the driver a more comfortable driving environment and better protection in a collision as well as eliminating the need to empty the driver's personal effects from the tractor whenever the engine requires service.
In Europe usually only the rear tractor axle has twin wheels, while larger size single wheels are used for the cargo trailer. The most common combination used in Europe is a semi tractor with two axles and a cargo trailer with three axles, giving five axles and 12 wheels in total. Lesser used (common in Scandinavia) are tractors with three axles, which feature twin wheels either on one or both rear axles. In addition to the most common three axles variant, cargo trailers with only two or only one axle are in use, again usually with larger single wheels.
In Sweden, lumber and long distance freight is run on seven or eight axle combinations up to 60000 kg in weight and 25.25 m long. Semi-trailers are used for short distance freight. In Finland, since 1 October 2013, law N 407/2013 allows 76 tonnes maximum if the truck has nine axles, and 68 tonnes maximum if the truck has eight axles. The truck can be 25.25 m long and 4.4 m high in both cases. The forest sector plans to submit to the Finnish Transport Safety Agency an application for testing semi-trailer trucks weighing 90 tonnes and 30 m long on Finnish roads.
United Kingdom.
In the United Kingdom, the maximum permitted gross weight of a semi-trailer truck, without the use of a Special Type General Order (STGO), is 44000 kg, which is the second heaviest permitted legal weight for a single semi-trailer truck in the world (50000 kg is allowed in the Netherlands). In order for a 44 tonne semi-trailer truck to be permitted on UK roads the tractor and semi-trailer must have three or more axles each. Lower weight semi-trailer trucks can mean some tractors and trailer having fewer axles. In practice, like double decker buses and coaches in the UK, there is no legal height limit for semi-trailer trucks; however, bridges over 5.03 m do not have the height marked on them. Semi-trailer trucks on Continental Europe have a height limit of 4.0 m.
Vehicles heavier than 44,000 kg are permitted on UK roads but are indivisible loads, which would be classed as abnormal (or oversize). Such vehicles are required to display an STGO (Special Types General Order) plate on the front of the tractor unit and, under certain circumstances, are required to travel by an authorized route and have an escort.
Most UK trailers are 13.7 m long and, dependent on the position of the fifth wheel and kingpin, a coupled tractor unit and trailer will have a combined length of between 15.25 and. Although the Construction and Use Regulations allow a maximum rigid length of 18.2 m, this, combined with a shallow kingpin and fifth wheel set close to the rear of the tractor unit, can give an overall length of around 22.75 m, although combinations of this length are usually used only to carry steel or concrete beams. Providing certain requirements are fulfilled, a Special Types General Order (STGO) allows for vehicles of any size or weight to travel on UK roads. However, in practice any such vehicle has to travel by a route authorized by the Department of Transport and move under escort. The escort of abnormal loads in the UK is now predominantly carried out by private companies, but extremely large or heavy loads that require road closures must still be escorted by the police.
In the UK, some articulated trucks have eight tyres on three axles on the tractor; these are known as six-wheelers or "six leggers", with either the center or rear axle having single wheels which normally steer as well as the front axle and can be raised when not needed (i.e. when unloaded or only a light load is being carried; an arrangement known as a TAG axle when it is the rear axle, or mid-lift when it is the center axle). Some trailers have two axles which have twin wheels on each axle; other trailers have three axles, of which one axle can be a lift axle which has super-single wheels. In the UK, two wheels bolted to the same hub are classed as a single wheel, therefore a standard six-axle articulated truck is considered to have twelve wheels, even though it has twenty tyres. The UK also allows articulated truck tractors which have six tires on two axles; these are known as four-wheelers.
Denby Eco-Link B-Train.
In 2009, the operator Denby Transport designed and built a 25.25-metre-long B-Train (or B-Double) semi-trailer truck called the Denby Eco-Link to show the benefits of such a vehicle, which were a reduction in road accidents and result in less road deaths, a reduction in emissions due to the one tractor unit still being used and no further highway investment being required. Furthermore, Denby Transport asserted that two Eco-Links would replace three standard semi-trailer trucks while, if limited to the current UK weight limit of 44 tonnes, it was claimed the Eco-Link would reduce carbon emissions by 16% and could still halve the number of trips needed for the same amount of cargo carried in conventional semi-trailer trucks. This is based on the fact that for light but bulky goods such as toilet paper, plastic bottles, cereals and aluminum cans, conventional semi-trailer trucks run out of cargo space before they reach the weight limit. At 44 tonnes, as opposed to 60 tonnes usually associated with B-Trains, the Eco-Link also exerts less weight per axle on the road compared to the standard six-axle 44 tonne articulated combination.
The vehicle was built after Denby Transport believed they had found a legal-loophole in the present UK law to allow the Eco-Link to be used on the public roads. The relevant legislation concerned the 1986 Road Vehicles Construction and Use Regulations. The 1986 regulations state that "certain vehicles" may be permitted to draw more than one trailer and can be up to 85 ft. The point of law reportedly hinged on the definition of a "towing implement", with Denby prepared to argue that the second trailer on the Eco-Link was one. The Department for Transport were of the opinion that this refers to recovering a vehicle after an accident or breakdown, but the regulation does not explicitly state this.
During BTAC performance testing the Eco-Link was given an "excellent" rating for its performance in maneuverability, productivity, safety and emissions tests, superseding ordinary semi-trailer trucks in many respects. Private trials had also reportedly shown the Denby vehicle had a 20% shorter stopping distance than conventional semi-trailer trucks of the same weight, due to having extra axles. The active steer system meant that the Eco-Link had a turning circle of 41 ft, the same as a conventional semi-trailer truck.
Although the Department for Transport advised that the Eco-Link was not permissible on public roads, Denby Transport gave the Police prior warning of the timing and route of the test drive on the public highway, as well as outlining their position in writing to the Eastern Traffic Area Office. On 1 December 2009 Denby Transport were preparing to drive the Eco-Link on public roads, but this was cut short because the Police pulled the semi-trailer truck over as it left the gates in order to test it for its legality "to investigate any... offences which may be found". The Police said the vehicle was unlawful due to its length and Denby Transport was served with a notice by the Vehicle and Operator Services Agency (VOSA) inspector to remove the vehicle from the road for inspection. Having returned to the yard, Denby Transport was formally notified by Police and VOSA that the semi-trailer truck could not be used. Neither the Eco-Link, nor any other B-Train, have since been permitted on UK roads. However, this prompted the Department for Transport to undertake a desk study in to semi-trailer trucks, which has resulted in the longer semi-trailer trial which commenced in 2012.
Longer semi-trailers.
Starting in January 2012 the Department for Transport is conducting a trial of longer semi-trailers. The trial involves 900 semi-trailers of 14.6 m in length (i.e. 1 m longer than the current maximum), and a further 900 semi-trailers of 15.65 m in length (i.e. 2.05 m longer). This will result in the total maximum length of the semi-trailer truck being 17.5 m for trailers 14.6 m in length, and 18.55 m for trailers 15.65 m in length. The increase in length will not result in the 44,000 kg weight limit being exceeded and will allow some operators to approach the weight limit which may not have been previously possible due to the previous length of trailers. The trial will run for a maximum of 10 years.
Continental Europe.
The maximum overall length applying in the EU and EEA member states is 18.75 m with a maximum weight of 40 tonnes, or 44 tonnes if carrying an ISO container. However, rules limiting the semi-trailers to 16.5 m and 18.75 m are met with trucks carrying a standardized 7.82 m body with one additional 7.82 m body on tow as a trailer. Since 1996, when Sweden and Finland formally won a final exemption from the European Economic Area rules with 60 tonne and 25.25 m combinations, all other member states gained the ability to adopt the same rules. In Italy the maximum permitted weight (unless exceptional transport is authorized) is 44 tonnes for any kind of combination with five axles or more.
Maximum lengths.
The 25.25 metre truck combinations were developed under the branding of "EcoCombi" which influenced the name of "EuroCombi" for an ongoing standardization effort where such truck combinations shall be legal to operate in all jurisdictions of the European Economic Area. With the 50% increase in cargo weight, the fuel efficiency increases with an average of 20% with a corresponding relative decrease in carbon emissions and with the added benefit of one third fewer trucks on the road. The 1996 EU regulation defines a Europe Module System (EMS) as it was implemented in Sweden. The wording of EMS combinations and EuroCombi are now used interchangeably to point to truck combinations as specified in the EU document; however apart from Sweden and Finland the EuroCombi is only allowed to operate on specific tracks in other EU member states. 
From 2006, 25.25 m truck trailer combinations are to be allowed on restricted routes within Germany, following a similar (on-going) trial in The Netherlands. Similarly, Denmark have allowed 25.25 m combinations on select routes. Like in Sweden and Finland, these vehicles in continental Europe will run a 60 tonne weight limit. Two types are to be used: 1) a 26 tonne truck pulling a dolly and semi-trailer, or 2) an articulated tractor unit pulling a B-double. The UK government has so far decided not to have its own trial of these 60 tonne vehicles, but to keep an eye on the other countries' trials.
When using a dolly, which generally has to be equipped with lights and a license plate, rigid trucks can be used to pull semi-trailers. The dolly is equipped with a fifth wheel to which the trailer is coupled. Because the dolly attaches to a pintle hitch on the truck, manoeuvrings a trailer hooked to a dolly is different from maneuvering a fifth wheel trailer. Backing the vehicle requires same technique as backing an ordinary truck/full trailer combination, though the dolly/semi setup is probably longer, thus requiring more space for maneuvering. The tractor/semi-trailer configuration is rarely used on timber trucks, since these will use the two big advantages of having the weight of the load on the drive wheels, and the loader crane used to lift the logs from the ground can be mounted on the rear of the truck behind the load, allowing a short (lightweight) crane to reach both ends of the vehicle without uncoupling. Also construction trucks are more often seen in a rigid + midaxle trailer configuration instead of the tractor/semi-trailer setup.
Scandinavia, Finland, and the Netherlands.
Denmark, the Netherlands and Norway all allow 25.25 m trucks (the Netherlands from 2000, Denmark from 2008, and Norway from 2008 on selected routes).
In Sweden, the allowed length has been 24 m since 1967. Before that, the maximum length was unlimited; the only limitations were on axle load. What stopped Sweden from adopting the same rules as the rest of Europe, when securing road safety, was the national importance of a competitive forestry industry. Finland, with the same road safety issues and equally important forestry industry, followed suit. The change made trucks able to carry three stacks of cut-to-length logs instead of two, as it would be in a short combination. They have one on stack together with a crane on the 6×4 truck, and two additional stacks on a four axle trailer. The allowed gross weight in both countries is up to 60 tonnes depending on the distance between the first and last axle.
In the negotiations starting in the late 1980s preceding the two countries' entries to the European Economic Area and later the European Union, they insisted on exemptions from the EU rules citing environmental concerns and the transportation needs of the logging industry. In 1995, after Sweden and Finland's entry to the union, the rules changed again, this time to allow trucks carrying a standard CEN unit of 7.82 m to draw a 13.6 m standard semi-trailer on a dolly, a total overall length of 25.25 m. Later, B-double combinations came into use, often with one 20 ft container on the B-link and a 40 ft container (or two 20 ft containers) on a semi-trailer bed. In allowing the longer truck combinations, what would take two 16.5 m semi-trailer trucks and one 18.75 m truck and trailer to haul on the continent now could be handled by just two 25.25 m trucks - greatly reducing overall costs and emissions. Prepared since late 2012 and effective on January 2013, Finland has changed its regulations to allow total maximum legal weight of a combination to be 76 tonnes. At the same time the maximum allowed height would be increased by 20 cm; from current maximum of 4.2 m to 4.4 m. The effect this major maximum weight increase would cause to the roads and bridges in Finland over time is strongly debated.
However, longer and heavier combinations are regularly seen on public roads; special permits are issued for special cargo. The mining company Boliden AB have a standing special permit for 80 tonne combinations on select routes between mines in the inland and the processing plant in Boliden, taking a 50 tonne load of ore. Volvo has a special permit for a 32 m, steering B-trailer-trailer combination carrying two 40 ft containers to and from Gothenburg harbour and the Volvo Trucks factory, all on the island of Hisingen. Another example is the ongoing project "En Trave Till" (lit. "One more pile/stack") started in December 2008. It will allow even longer vehicles to further rationalize the logging transports. As the name of the project points out, it will be able to carry four stacks of timber, instead of the usual three. The test is limited to Norrbotten county and the European route E4 between the timber terminal in Överkalix and the sawmill in Munksund (outside Piteå). The vehicle is a 30 m long truck trailer combination with a gross weight exceeding 90 tonnes. It is estimated that this will give a 20% lower cost and 20-25% CO2 emissions reduction compared to the regular 60 tonne truck combinations. As the combinations spreads its weight over more axles, braking distance, road wear and traffic safety is believed to be either the same or improved with the 90 tonne truck-trailer. In the same program two types of 74 tonne combinations will be tested in Dalsland and Bohuslän counties in western Sweden: an enhanced truck and trailer combination for use in the forest and a b-double for plain highway transportation to the mill in Skoghall. In 2012, the Northland Mining company received permission for 90 tonne combinations with normal axle load (an extra dolly) for use on the 150 km Kaunisvaara-Svappavaara route, carrying iron ore.
Australia.
Australian road transport has a reputation for using very large trucks and road trains. This is reflected in the most popular configurations of trucks generally having dual drive axles and three axles on the trailers, with four tires on each axle. This means that Australian single semi-trailer trucks will usually have 22 wheels, which is generally more than their counterparts in other countries. Long haul transport usually operates as B-doubles with two trailers (each with three axles), for a total of nine axles (including steering). In some lighter duty applications only one of the rear axles of the truck is driven, and the trailer may have only two axles.
From July 2007, the Australian Federal and State Governments allowed the introduction of B-triple trucks on a specified network of roads. B-Triples are set up differently from conventional road trains. The front of their first trailer is supported by the turntable on the prime mover. The second and third trailers are supported by turntables on the trailers in front of them. As a result, B-Triples are much more stable than road trains and handle exceptionally well.
True road trains only operate in remote areas, regulated by each state or territory government.
In total, the maximum length that any articulated vehicle may be (without a special permit and escort) is 53.5 m, its maximum load may be up to 164 tonnes gross, and may have up to four trailers. However, heavy restrictions apply to the areas where such a vehicle may travel in most states. In remote areas such as the Northern Territory great care must be taken when sharing the road with longer articulated vehicles that often travel during the day time, especially four trailer road trains.
Articulated trucks towing a single trailer or two trailers (commonly known as "short doubles") with maximum overall length of 19 m are referred to as "General access heavy vehicles" and are permitted in all areas, including metropolitan. B-doubles are limited to a maximum total weight of 62.5 tonnes and overall length of 25 m, or 26 m if they are fitted with approved FUPS (Front Underrun Protection System) devices. B-doubles may only operate on designated roads, which includes most highways and some major metropolitan roads. B-doubles are very common in all parts of Australia including state capitals and on major routes they outnumber single trailer configurations.
Maximum width of any vehicle is 2.5 m and a height of 4.3 m. In the past few years, allowance has been made by several states to allow certain designs of heavy vehicles up to 4.6 m high but they are also restricted to designated routes. In effect, a 4.6 meter high B-double will have to follow two sets of rules: they may access only those roads that are permitted for B-doubles "and" for 4.6 meter high vehicles.
In Australia, both conventional tractor units and cabovers are common, however cabovers are most often seen on B-doubles on the eastern seaboard where the reduction in total length allows the vehicle to pull longer trailers and thus more cargo than it would otherwise.
Super single tires are sometimes used on tri-axle trailers. The suspension is designed with travel limiting, which will hold the rim off the road for one blown or deflated tire for each side of the trailer, so a trailer can be driven at reduced speed to a safe place for repair. Super singles are also often used on the steer axle in Australia to allow greater loading over the steer axle. The increase in loading of steer tires requires a permit.
Semi-truck manufacturers.
Current semi-truck manufacturers include:
Construction.
Types of trailers.
There are many types of semi-trailers in use, designed to haul a wide range of products.
Coupling and uncoupling.
The cargo trailer is, by means of a king pin, hooked to a horseshoe-shaped quick-release coupling device called a fifth wheel or a turntable hitch at the rear of the towing engine that allows easy hook up and release. The truck trailer cannot move by itself because it only has wheels at the rear end: it requires a forward axle, provided by the towing engine, to carry half the load weight. When braking hard at high speeds, the vehicle has a tendency to fold at the pivot point between the towing vehicle and the trailer. Such a truck accident is called a "trailer swing", although it is also commonly described as a "jackknife". Jackknifing is a condition where the tractive unit swings round against the trailer, and not vice versa.
Braking.
Semi trucks use air pressure, rather than hydraulic fluid, to actuate the brakes mainly due to the much larger braking forces required. The use of air hoses allows for ease of coupling and uncoupling of trailers from the tractor unit, as well as reducing the potential for problems common to hydraulic systems, such as leakage or brake failure caused when overheated brake fluid vaporizes in the hydraulic lines. The most common failure is "brake fade", usually caused when the drums or discs and the linings of the brakes overheat from excessive use.
The parking brake of the tractor unit and the emergency brake of the trailer are spring brakes that require air pressure in order to be released. They are applied when air pressure is released from the system, and disengaged when air pressure is supplied. This is a fail-safe design feature which ensures that if air pressure to either unit is lost, the vehicle will stop to a grinding halt, instead of continuing without brakes and becoming uncontrollable. The trailer controls are coupled to the tractor through two "gladhand connectors", which provide air pressure, and an electrical cable, which provides power to the lights and any specialized features of the trailer.
"Glad-hand connectors" (also known as "palm couplings") are air hose connectors, each of which has a flat engaging face and retaining tabs. The faces are placed together, and the units are rotated so that the tabs engage each other to hold the connectors together. This arrangement provides a secure connection, but allows the couplers to break away without damaging the equipment if they are pulled, as may happen when the tractor and trailer are separated without first uncoupling the air lines. These connectors are similar in design to the ones used for a similar purpose between railroad cars. Two air lines typically connect to the trailer unit. An "emergency" or "main" air supply line pressurizes the trailer's air tank and disengages the emergency brake, and a second "service" line controls the brake application during normal operation.
In the UK, male/female quick release connectors ("red line" or emergency), have a female on the truck and male on the trailer, but a "yellow line" or service has a male on the truck and female on the trailer. This avoids coupling errors (causing no brakes) plus the connections will not come apart if pulled by accident. The three electrical lines will fit one way around a primary black, a secondary green, and an ABS lead, all of which are collectively known as "suzies" or "suzie coils".
Another braking feature of semi-trucks is engine braking, which could be either a compression brake (usually shortened to "Jake brake") or exhaust brake or combination of both. However, the use of compression brake alone produces a loud and distinctive noise, and to control noise pollution, some local municipalities have prohibited or restricted the use of engine brake systems inside their jurisdictions, particularly in residential areas. The advantage to using engine braking instead of conventional brakes is that a truck can descend a long grade without overheating its wheel brakes. Some vehicles can also be equipped with hydraulic or electric retarders which have an advantage of near silent operation.
Transmission.
Because of the wide variety of loads the semi may carry, they usually have a manual transmission to allow the driver to have as much control as possible. However, all truck manufacturers now offer semi-automatic transmissions (manual gearboxes with automated gear change), as well as automatic transmissions.
Semi-truck transmissions can have as few as three forward speeds or as many as 18 forward speeds (plus 2 reverse speeds). A large number of transmission ratios means the driver can operate the engine more efficiently. Modern on-highway diesel engines are designed to provide maximum torque in a narrow RPM range (usually 1200-1500 RPM); having more gear ratios means the driver can hold the engine in its optimum range regardless of road speed (drive axle ratio must also be considered).
A ten-speed manual transmission, for example is controlled via a six-slot H-box pattern, similar to that in five-speed cars — five forward and one reverse gear. Gears six to ten (and high speed reverse) are accessed by a Lo/High range splitter; gears one to five are Lo range; gears six to ten are High range using the same shift pattern. A Super-10 transmission, by contrast, has no range splitter; it uses alternating "stick and button" shifting (stick shifts 1-3-5-7-9, button shifts 2-4-6-8-10). The 13-, 15-, and 18-speed transmissions have the same basic shift pattern, but include a splitter button to enable additional ratios found in each range. Some transmissions may have 12 speeds.
Another difference between semi-trucks and cars is the way the clutch is set up. On an automobile, the clutch pedal is depressed full stroke to the floor for every gear shift, to ensure the gearbox is disengaged from the engine. On a semi-truck with constant mesh transmission (non synchronized), such as by the Eaton Roadranger series, not only is double clutching required, but a clutch brake is required as well. The clutch brake stops the rotation of the gears, and allows the truck to be put into gear without grinding when stationary. The clutch is pressed to the floor only to allow smooth engagement of low gears when starting from a full stop; when the truck is moving, the clutch pedal is pressed only far enough to break torque for gear changes.
Lights.
An electrical connection is made between the tractor and the trailer through a cable often referred to as a "pigtail". This cable is a bundle of wires in a single casing. Each wire controls one of the electrical circuits on the trailer, such as running lights, brake lights, turn signals, etc. A straight cable would break when the rig went around corners, so a coiled cable is used which retracts these coils when not under tension. It is these coils that cause the cable to look like a pigtail.
In most countries a trailer or semi-trailer must have minimum
Wheels and tires.
Although dual wheels are the most common, use of two single, wider tires, known as "super singles", on each axle is becoming popular among bulk cargo carriers and other weight-sensitive operators. With increased efforts to reduce greenhouse gas emissions, the use of the super-single tire is gaining popularity. There are several advantages to this configuration. The first of these is that super singles reduce fuel consumption. In 1999, tests on an oval track showed a 10% fuel savings when super singles were used. These savings are realized because less energy is wasted flexing fewer tire sidewalls. Second, the lighter overall tire weight allows a truck to be loaded with more freight. The third advantage is that the single wheel encloses less of the brake unit, which allows faster cooling and reduces brake fade.
One of the major disadvantages of the super singles is that they are currently not as widely available as a standard tire. In addition, if a tire should become deflated or be destroyed, there is not another tire attached to the same hub to maintain the dynamic stability of the vehicle, as would be the case with dual wheels. With dual wheels, the remaining tire may be overloaded, but it will typically allow the vehicle to be safely stopped or driven to a repair facility.
Skirted trailers.
An innovation rapidly growing in popularity is the skirted trailer. The space between the road and the bottom of the trailer frame was traditionally left open, until it was realized that the turbulent air swirling under the trailer is a major source of aerodynamic drag. Three split skirt concepts were verified by the United States Environmental Protection Agency (EPA) to provide fuel savings greater than 5%, and four split skirt concepts had EPA-verified fuel savings between 4% and 5%.
Skirted trailers are often combined with Underrun Protection Systems ("underride guards"), greatly improving safety for passenger vehicles sharing the road.
Underride guard.
Technically called a Rear Underrun Protection System (RUPS), this is a rigid assembly hanging down from the bottom rear of the trailer, which is intended to provide some protection for passenger cars which collide with the rear of the trailer. Public awareness of this safeguard was increased in the aftermath of the accident that killed actress Jayne Mansfield on 29 June 1967, when the car she was in hit the rear of a tractor-trailer, causing fatal head trauma. After her death, the NHTSA recommended requiring a rear underride guard, also known as a "Mansfield bar", an "ICC bar", or a DOT (Department of Transportation) bumper, but the trucking industry has been slow to upgrade this safety feature.
The bottom rear of the trailer is near head level for an adult seated in a car, and without the underride guard, the only protection for such an adult's head in a rear-end collision would be the car's windshield. Because of the height mismatch between a passenger car bumper and the much-higher height of the platform of a trailer, the car's protective crush zone becomes irrelevant and air bags are ineffective in protecting the car passengers, if the underride guard is missing or inadequate.
In addition to rear underride guards, truck tractor cabs may be equipped with a Front Underrun Protection System (FUPS) at the front bumper of the truck. The safest tractor-trailers are also equipped with side underride guards, also called Side Underrun Protection System (SUPS). These additional barriers prevent passenger cars from skidding underneath the trailer from the side, such as in an oblique or side collision, or if the trailer jackknifes across the road. In addition to safety benefits, these underride guards may improve fuel mileage by reducing air turbulence under the trailer at highway speeds.
Another benefit of having a sturdy underride guard is that it may be secured to a loading dock with a hook to prevent "trailer creep", a movement of the trailer away from the dock, which opens up a dangerous gap during loading or unloading operations.
Driver's license.
A special driver's license is required to operate various commercial vehicles.
Canada.
Regulations vary by province. A license to operate a vehicle with air brakes is required (i.e., normally a Class I, II, or III commercial license with an "A" or "S" endorsement in provinces other than Ontario). In Ontario, a "Z" endorsement is required to drive any vehicle using air brakes; in provinces other than Ontario, the "A" endorsement is for air brake operation only, and an "S" endorsement is for both operation and adjustment of air brakes. Anyone holding a valid Ontario driver's license (i.e., excluding a motorcycle license) with a "Z" endorsement can legally drive any air-brake-equipped truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicle) up to 11 tonnes, that includes one trailer weighing no more than 4.6 tonnes if the license falls under the following three classes: Class E (school bus—maximum 24-passenger capacity or ambulance), F (regular bus—maximum 24-passenger capacity or ambulance) or G (car, van, or small-truck).
A Class B (any school bus), C (any urban-transit-vehicle or highway-coach), or D (heavy trucks other than tractor-trailers) license enables its holder to drive any truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicle) greater than 11 tonnes, that includes one trailer weighing no more than 4.6 tonnes. Anyone holding an Ontario Class A license (or its equivalent) can drive any truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicles) greater than 11 tonnes, that includes one or more trailers weighing more than 4.6 tonnes.
United States.
Drivers of semi-trailer trucks generally require a Class A commercial driver's license (CDL) to operate any combination vehicles with a combined Gross Vehicle Weight Rating (or CGVWR) in excess of 26000 lb if the gross vehicle weight rating (GVWR) of the towed vehicle(s) is in excess of 10000 lb. Some states (such as North Dakota) provide exemptions for farmers, allowing non-commercial license holders to operate semis within a certain air-mile radius of their reporting location. State exemptions, however, are only applicable in intrastate commerce; stipulations of the Code of Federal Regulations (CFR) may be applied in interstate commerce. Also a person under the age of 21 cannot operate a commercial vehicle outside the state where the commercial license was issued. This restriction may also be mirrored by certain states in their intrastate regulations. A person must be at least 18 in order to be issued a commercial license.
In addition, "Endorsements" are necessary for certain cargo and vehicle arrangements and types;
Taiwan.
The Road Traffic Security Rules () require a combination vehicle driver license () to drive a combination vehicle (). These rules define a combination vehicle as a motor vehicle towing a heavy trailer, i.e., a trailer with a gross weight of more than 750 kg.
Europe.
A category CE driving licence is required to drive a tractor-trailer in Europe. Category C(Γ in Greece) is required for vehicles over 7500 kg, while category E is for heavy trailers, which in the case of trucks and buses means any trailer over 750 kg. Vehicles over 3500 kg—which is the maximum limit of B license—but under 7,500 kg can be driven with a C1 license. Buses require a D(Δ in Greece) license. A bus that is registered for no more than 16 passengers, excluding the driver, can be driven with a D1 license.
Australia.
Truck drivers in Australia require an endorsed license. These endorsements are gained through training and experience. The minimum age to hold an endorsed license is 18 years, and/or must have held open (full) driver's license for minimum 12 months.
The following are the heavy vehicle license classes in Australia:
In order to obtain a HC License the driver must have held an MR or HR license for at least 12 months. To upgrade to an MC License the driver must have held a HR or HC license for at least 12 months. From licenses MR and upward there is also a B Condition which may apply to the license if testing in a synchromesh or automatic transmission vehicle. The B Condition may be removed upon the driver proving the ability to drive a constant mesh transmission using the clutch. "Constant mesh transmission refers to "crash box" transmissions, predominantly Road Ranger eighteen-speed transmissions in Australia."
New Zealand.
In New Zealand drivers of heavy vehicles require specific licenses, termed as classes. A Class 1 license ("car license") will allow the driving of any vehicle with Gross Laden Weight (GLW) or Gross Combination Weight (GCW) of 4500 kg or less. For other types of vehicles the classes are separately licensed as follows:
Further information on the New Zealand licensing system for heavy vehicles can be found at .
Role in trade.
Modern day semi-trailer trucks often operate as a part of a domestic or international transport infrastructure to support containerized cargo shipment.
Various types of rail flat bed train cars are modified to hold the cargo trailer or container with wheels or without. This is called "Intermodal" or "piggyback". The system allows the cargo to switch from highway to railway or vice versa with relative ease by using gantry cranes.
The large trailers pulled by a tractor unit come in many styles, lengths, and shapes. Some common types are: vans, reefers, flatbeds, sidelifts and tankers. These trailers may be refrigerated, heated, ventilated, or pressurized, depending on climate and cargo. Some trailers have movable wheel axles that can be adjusted by moving them on a track underneath the trailer body and securing them in place with large pins. The purpose of this is to help adjust weight distribution over the various axles, to comply with local laws.

</doc>
<doc id="29383" url="http://en.wikipedia.org/wiki?curid=29383" title="Stonewall riots">
Stonewall riots

The Stonewall riots were a series of spontaneous, violent demonstrations by members of the gay community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn, located in the Greenwich Village neighborhood of Manhattan, New York City. They are widely considered to constitute the single most important event leading to the gay liberation movement and the modern fight for LGBT rights in the United States.
Gay Americans in the 1950s and 1960s faced an anti-homosexual legal system. Early homophile groups in the U.S. sought to prove that gay people could be assimilated into society, and they favored non-confrontational education for homosexuals and heterosexuals alike. The last years of the 1960s, however, were very contentious, as many social movements were active, including the African American Civil Rights Movement, the Counterculture of the 1960s, and antiwar demonstrations. These influences, along with the liberal environment of Greenwich Village, served as catalysts for the Stonewall riots.
Very few establishments welcomed openly gay people in the 1950s and 1960s. Those that did were often bars, although bar owners and managers were rarely gay. At the time, the Stonewall Inn was owned by the Mafia. It catered to an assortment of patrons and was known to be popular among the poorest and most marginalized people in the gay community: drag queens, representatives of the transgender community, effeminate young men, male prostitutes, and homeless youth. Police raids on gay bars were routine in the 1960s, but officers quickly lost control of the situation at the Stonewall Inn. They attracted a crowd that was incited to riot. Tensions between New York City police and gay residents of Greenwich Village erupted into more protests the next evening, and again several nights later. Within weeks, Village residents quickly organized into activist groups to concentrate efforts on establishing places for gays and lesbians to be open about their sexual orientation without fear of being arrested.
After the Stonewall riots, gays and lesbians in New York City faced gender, race, class, and generational obstacles to becoming a cohesive community. Within six months, two gay activist organizations were formed in New York, concentrating on confrontational tactics, and three newspapers were established to promote rights for gays and lesbians. Within a few years, gay rights organizations were founded across the U.S. and the world. On June 28, 1970, the first Gay Pride marches took place in New York, Los Angeles, San Francisco and Chicago commemorating the anniversary of the riots. Similar marches were organized in other cities. Today, Gay Pride events are held annually throughout the world toward the end of June to mark the Stonewall riots.
Background.
Homosexuality in 20th-century United States.
Following the social upheaval of World War II, many people in the United States felt a fervent desire to "restore the prewar social order and hold off the forces of change", according to historian Barry Adam. Spurred by the national emphasis on anti-communism, Senator Joseph McCarthy conducted hearings searching for communists in the U.S. government, the U.S. Army, and other government-funded agencies and institutions, leading to a national paranoia. Anarchists, communists, and other people deemed un-American and subversive were considered security risks. Homosexuals were included in this list by the U.S. State Department in 1950, on the theory that they were susceptible to blackmail. Under Secretary of State James E. Webb noted in a report, "It is generally believed that those who engage in overt acts of perversion lack the emotional stability of normal persons." Between 1947 and 1950, 1,700 federal job applications were denied, 4,380 people were discharged from the military, and 420 were fired from their government jobs for being suspected homosexuals.
Throughout the 1950s and 1960s, the Federal Bureau of Investigation (FBI) and police departments kept lists of known homosexuals, their favored establishments, and friends; the U.S. Post Office kept track of addresses where material pertaining to homosexuality was mailed. State and local governments followed suit: bars catering to homosexuals were shut down, and their customers were arrested and exposed in newspapers. Cities performed "sweeps" to rid neighborhoods, parks, bars, and beaches of gay people. They outlawed the wearing of opposite gender clothes, and universities expelled instructors suspected of being homosexual. Thousands of gay men and women were publicly humiliated, physically harassed, fired, jailed, or institutionalized in mental hospitals. Many lived double lives, keeping their private lives secret from their professional ones.
In 1952, the American Psychiatric Association listed homosexuality in the "Diagnostic and Statistical Manual" (DSM) as a mental disorder. A large-scale study of homosexuality in 1962 was used to justify inclusion of the disorder as a supposed pathological hidden fear of the opposite sex caused by traumatic parent–child relationships. This view was widely influential in the medical profession. In 1956, however, the psychologist Evelyn Hooker performed a study that compared the happiness and well-adjusted nature of self-identified homosexual men with heterosexual men and found no difference. Her study stunned the medical community and made her a hero to many gay men and lesbians, but homosexuality remained in the "DSM" until 1973.
Homophile activism.
In response to this trend, two organizations formed independently of each other to advance the cause of homosexuals and provide social opportunities where gays and lesbians could socialize without fear of being arrested. Los Angeles area homosexuals created the Mattachine Society in 1950, in the home of communist activist Harry Hay. Their objectives were to unify homosexuals, educate them, provide leadership, and assist "sexual deviants" with legal troubles. Facing enormous opposition to its radical approach, in 1953 the Mattachine shifted their focus to assimilation and respectability. They reasoned that they would change more minds about homosexuality by proving that gays and lesbians were normal people, no different from heterosexuals. Soon after, several women in San Francisco met in their living rooms to form the Daughters of Bilitis (DOB) for lesbians. Although the eight women who created the DOB initially came together to be able to have a safe place to dance, as the DOB grew they developed similar goals to the Mattachine, and urged their members to assimilate into general society.
One of the first challenges to government repression came in 1953. An organization named ONE, Inc. published a magazine called "ONE". The U.S. Postal Service refused to mail its August issue, which concerned homosexuals in heterosexual marriages, on the grounds that the material was obscene despite it being covered in brown paper wrapping. The case eventually went to the Supreme Court, which in 1958 ruled that ONE, Inc. could mail its materials through the Postal Service.
Homophile organizations—as homosexual groups were called—grew in number and spread to the East Coast. Gradually, members of these organizations grew bolder. Frank Kameny founded the Mattachine of Washington, D.C. He had been fired from the U.S. Army Map Service for being a homosexual, and sued unsuccessfully to be reinstated. Kameny wrote that homosexuals were no different from heterosexuals, often aiming his efforts at mental health professionals, some of whom attended Mattachine and DOB meetings telling members they were abnormal. In 1965, Kameny, inspired by the Civil Rights Movement, organized a picket of the White House and other government buildings to protest employment discrimination. The pickets shocked many gay people, and upset some of the leadership of Mattachine and the DOB. At the same time, demonstrations in the Civil Rights movement and opposition to the Vietnam War all grew in prominence, frequency, and severity throughout the 1960s, as did their confrontations with police forces.
Compton's Cafeteria riot.
On the outer fringes of the few small gay communities were people who challenged gender expectations. They were effeminate men and masculine women, or people assigned male at birth who dressed and lived as women and people assigned female at birth who dressed as men, respectively, either part or full-time. Contemporary nomenclature classified them as transvestites, and they were the most visible representatives of sexual minorities. They belied the carefully crafted image portrayed by the Mattachine Society and DOB that asserted homosexuals were respectable, normal people. The Mattachine and DOB considered the trials of being arrested for wearing clothing of the opposite gender as a parallel to the struggles of homophile organizations: similar but distinctly separate. Gay and transgender people staged a small riot in Los Angeles in 1959 in response to police harassment.
In a larger event in 1966 in San Francisco, drag queens, hustlers, and transvestites were sitting in Compton's Cafeteria when the police arrived to arrest men dressed as women. A riot ensued, with the patrons of the cafeteria slinging cups, plates, and saucers, and breaking the plexiglass windows in the front of the restaurant, and returning several days later to smash the windows again after they were replaced. Professor Susan Stryker classifies the Compton's Cafeteria riot as an "act of anti-transgender discrimination, rather than an act of discrimination against sexual orientation" and connects the uprising to the issues of gender, race, and class that were being downplayed by homophile organizations. It marked the beginning of transgender activism in San Francisco.
Greenwich Village.
The Manhattan neighborhoods of Greenwich Village and Harlem were home to a sizable homosexual population after World War I, when men and women who had served in the military took advantage of the opportunity to settle in larger cities. The enclaves of gays and lesbians, described by a newspaper story as "short-haired women and long-haired men", developed a distinct subculture through the following two decades. Prohibition inadvertently benefited gay establishments, as drinking alcohol was pushed underground along with other behaviors considered immoral. New York City passed laws against homosexuality in public and private businesses, but because alcohol was in high demand, speakeasies and impromptu drinking establishments were so numerous and temporary that authorities were unable to police them all.
The social repression of the 1950s resulted in a cultural revolution in Greenwich Village. A cohort of poets, later named the Beat poets, wrote about the evils of the social organization at the time, glorifying anarchy, drugs, and hedonistic pleasures over unquestioning social compliance, consumerism, and closed mindedness. Of them, Allen Ginsberg and William S. Burroughs—both Greenwich Village residents—also wrote bluntly and honestly about homosexuality. Their writings attracted sympathetic liberal-minded people, as well as homosexuals looking for a community.
By the early 1960s, a campaign to rid New York City of gay bars was in full effect by order of Mayor Robert F. Wagner, Jr., who was concerned about the image of the city in preparation for the 1964 World's Fair. The city revoked the liquor licenses of the bars, and undercover police officers worked to entrap as many homosexual men as possible. Entrapment usually consisted of an undercover officer who found a man in a bar or public park, engaged him in conversation; if the conversation headed toward the possibility that they might leave together—or the officer bought the man a drink—he was arrested for solicitation. One story in the "New York Post" described an arrest in a gym locker room, where the officer grabbed his crotch, moaning, and a man who asked him if he was all right was arrested. Few lawyers would defend cases as undesirable as these, and some of those lawyers kicked back their fees to the arresting officer.
The Mattachine Society succeeded in getting newly elected Mayor John Lindsay to end the campaign of police entrapment in New York City. They had a more difficult time with the New York State Liquor Authority (SLA). While no laws prohibited serving homosexuals, courts allowed the SLA discretion in approving and revoking liquor licenses for businesses that might become "disorderly". Despite the high population of gays and lesbians who called Greenwich Village home, very few places existed, other than bars, where they were able to congregate openly without being harassed or arrested. In 1966 the New York Mattachine held a "sip-in" at a Greenwich Village bar named Julius, which was frequented by gay men, to illustrate the discrimination homosexuals faced.
None of the bars frequented by gays and lesbians were owned by gay people. Almost all of them were owned and controlled by organized crime, who treated the regulars poorly, watered down the liquor, and overcharged for drinks. However, they also paid off police to prevent frequent raids.
Stonewall Inn.
The Stonewall Inn, located at 51 and 53 Christopher Street, along with several other establishments in the city, was owned by the Genovese family. In 1966, three members of the Mafia invested $3,500 to turn the Stonewall Inn into a gay bar, after it had been a restaurant and a nightclub for heterosexuals. Once a week a police officer would collect envelopes of cash as a payoff; the Stonewall Inn had no liquor license. It had no running water behind the bar—used glasses were run through tubs of water and immediately reused. There were no fire exits, and the toilets overran consistently. Though the bar was not used for prostitution, drug sales and other "cash transactions" took place. It was the only bar for gay men in New York City where dancing was allowed; dancing was its main draw since its re-opening as a gay club.
Visitors to the Stonewall Inn in 1969 were greeted by a bouncer who inspected them through a peephole in the door. The legal drinking age was 18, and to avoid unwittingly letting in undercover police (who were called "Lily Law", "Alice Blue Gown", or "Betty Badge"), visitors would have to be known by the doorman, or look gay. The entrance fee on weekends was $3, for which the customer received two tickets that could be exchanged for two drinks. Patrons were required to sign their names in a book to prove that the bar was a private "bottle club", but rarely signed their real names. There were two dance floors in the Stonewall; the interior was painted black, making it very dark inside, with pulsing gel lights or black lights. If police were spotted, regular white lights were turned on, signaling that everyone should stop dancing or touching. In the rear of the bar was a smaller room frequented by "queens;" it was one of two bars where effeminate men who wore makeup and teased their hair (though dressed in men's clothing) could go. Only a few transvestites, or men in full drag, were allowed in by the bouncers. The customers were "98 percent male" but a few lesbians sometimes came to the bar. Younger homeless adolescent males, who slept in nearby Christopher Park, would often try to get in so customers would buy them drinks. The age clientele ranged between the upper teens and early thirties, and the racial mix was evenly distributed among white, black, and Hispanic patrons. Because of its even mix of people, its location, and the attraction of dancing, the Stonewall Inn was known by many as ""the" gay bar in the city".
Police raids on gay bars were frequent—occurring on average once a month for each bar. Many bars kept extra liquor in a secret panel behind the bar, or in a car down the block, to facilitate resuming business as quickly as possible if alcohol was seized. Bar management usually knew about raids beforehand due to police tip-offs, and raids occurred early enough in the evening that business could commence after the police had finished. During a typical raid, the lights were turned on, and customers were lined up and their identification cards checked. Those without identification or dressed in full drag were arrested; others were allowed to leave. Some of the men, including those in drag, used their draft cards as identification. Women were required to wear three pieces of feminine clothing, and would be arrested if found not wearing them. Employees and management of the bars were also typically arrested. The period immediately before June 28, 1969, was marked by frequent raids of local bars—including a raid at the Stonewall Inn on the Tuesday before the riots—and the closing of the Checkerboard, the Tele-Star, and two other clubs in Greenwich Village.
Riots.
Police raid.
At 1:20 AM on Saturday, June 28, 1969, four plainclothes policemen in dark suits, two patrol officers in uniform, and Detective Charles Smythe and Deputy Inspector Seymour Pine arrived at the Stonewall Inn's double doors and announced "Police! We're taking the place!" Stonewall employees do not recall being tipped off that a raid was to occur that night, as was the custom. According to Duberman (p. 194), there was a rumor that one might happen, but since it was much later than raids generally took place, Stonewall management thought the tip was inaccurate. Days after the raid, one of the bar owners complained that the tipoff had never come, and that the raid was ordered by the Bureau of Alcohol, Tobacco, and Firearms, who objected that there were no stamps on the liquor bottles, indicating the alcohol was bootlegged.
David Carter presents information indicating that the Mafia owners of the Stonewall and the manager were blackmailing wealthier customers, particularly those who worked in the Financial District. They appeared to be making more money from extortion than they were from liquor sales in the bar. Carter deduces that when the police were unable to receive kickbacks from blackmail and the theft of negotiable bonds (facilitated by pressuring gay Wall Street customers), they decided to close the Stonewall Inn permanently. Two undercover policewomen and two undercover policemen had entered the bar earlier that evening to gather visual evidence, as the Public Morals Squad waited outside for the signal. Once inside, they called for backup from the Sixth Precinct using the bar's pay telephone. The music was turned off and the main lights were turned on. Approximately 205 people were in the bar that night. Patrons who had never experienced a police raid were confused. A few who realized what was happening began to run for doors and windows in the bathrooms, but police barred the doors. Michael Fader remembered, Things happened so fast you kind of got caught not knowing. All of a sudden there were police there and we were told to all get in lines and to have our identification ready to be led out of the bar."
The raid did not go as planned. Standard procedure was to line up the patrons, check their identification, and have female police officers take customers dressed as women to the bathroom to verify their sex, upon which any men dressed as women would be arrested. Those dressed as women that night refused to go with the officers. Men in line began to refuse to produce their identification. The police decided to take everyone present to the police station, after separating those cross-dressing in a room in the back of the bar. Maria Ritter, then known as Steve to her family, recalled, "My biggest fear was that I would get arrested. My second biggest fear was that my picture would be in a newspaper or on a television report in my mother's dress!" Both patrons and police recalled that a sense of discomfort spread very quickly, spurred by police who began to assault some of the lesbians by "feeling some of them up inappropriately" while frisking them.
"When did you ever see a fag fight back?... Now, times were a-changin'. Tuesday night was the last night for bullshit... Predominantly, the theme [w]as, "this shit has got to stop!""
—anonymous Stonewall riots participant
The police were to transport the bar's alcohol in patrol wagons. Twenty-eight cases of beer and nineteen bottles of hard liquor were seized, but the patrol wagons had not yet arrived, so patrons were required to wait in line for about 15 minutes. Those who were not arrested were released from the front door, but they did not leave quickly as usual. Instead, they stopped outside and a crowd began to grow and watch. Within minutes, between 100 and 150 people had congregated outside, some after they were released from inside the Stonewall, and some after noticing the police cars and the crowd. Although the police forcefully pushed or kicked some patrons out of the bar, some customers released by the police performed for the crowd by posing and saluting the police in an exaggerated fashion. The crowd's applause encouraged them further: "Wrists were limp, hair was primped, and reactions to the applause were classic."
When the first patrol wagon arrived, Inspector Pine recalled that the crowd—most of whom were homosexual—had grown to at least ten times the number of people who were arrested, and they all became very quiet. Confusion over radio communication delayed the arrival of a second wagon. The police began escorting Mafia members into the first wagon, to the cheers of the bystanders. Next, regular employees were loaded into the wagon. A bystander shouted, "Gay power!", someone began singing "We Shall Overcome", and the crowd reacted with amusement and general good humor mixed with "growing and intensive hostility". An officer shoved a transvestite, who responded by hitting him on the head with her purse as the crowd began to boo. Author Edmund White, who had been passing by, recalled, "Everyone's restless, angry, and high-spirited. No one has a slogan, no one even has an attitude, but something's brewing." Pennies, then beer bottles, were thrown at the wagon as a rumor spread through the crowd that patrons still inside the bar were being beaten.
A scuffle broke out when a woman in handcuffs was escorted from the door of the bar to the waiting police wagon several times. She escaped repeatedly and fought with four of the police, swearing and shouting, for about ten minutes. Described as "a typical New York butch" and "a dyke–stone butch", she had been hit on the head by an officer with a baton for, as one witness claimed, complaining that her handcuffs were too tight. Bystanders recalled that the woman, whose identity remains unknown (Stormé DeLarverie has been identified by some, including herself, as the woman, but accounts vary ), sparked the crowd to fight when she looked at bystanders and shouted, "Why don't you guys do something?" After an officer picked her up and heaved her into the back of the wagon, the crowd became a mob and went "berserk": "It was at that moment that the scene became explosive."
The last straw.
The police tried to restrain some of the crowd, and knocked a few people down, which incited bystanders even more. Some of those handcuffed in the wagon escaped when police left them unattended (deliberately, according to some witnesses). As the crowd tried to overturn the police wagon, two police cars and the wagon—with a few slashed tires—left immediately, with Inspector Pine urging them to return as soon as possible. The commotion attracted more people who learned what was happening. Someone in the crowd declared that the bar had been raided because "they didn't pay off the cops", to which someone else yelled "Let's pay them off!" Coins sailed through the air towards the police as the crowd shouted "Pigs!" and "Faggot cops!" Beer cans were thrown and the police lashed out, dispersing some of the crowd, who found a construction site nearby with stacks of bricks. The police, outnumbered by between 500 and 600 people, grabbed several people, including folk singer Dave Van Ronk—who had been attracted to the revolt from a bar two doors away from the Stonewall. Though Van Ronk was not gay, he had experienced police violence when he participated in antiwar demonstrations: "As far as I was concerned, anybody who'd stand against the cops was all right with me, and that's why I stayed in... Every time you turned around the cops were pulling some outrage or another." Ten police officers—including two policewomen—barricaded themselves, Van Ronk, Howard Smith (a writer for "The Village Voice"), and several handcuffed detainees inside the Stonewall Inn for their own safety.
Multiple accounts of the riot assert that there was no pre-existing organization or apparent cause for the demonstration; what ensued was spontaneous. Michael Fader explained,
We all had a collective feeling like we'd had enough of this kind of shit. It wasn't anything tangible anybody said to anyone else, it was just kind of like everything over the years had come to a head on that one particular night in the one particular place, and it was not an organized demonstration... Everyone in the crowd felt that we were never going to go back. It was like the last straw. It was time to reclaim something that had always been taken from us... All kinds of people, all different reasons, but mostly it was total outrage, anger, sorrow, everything combined, and everything just kind of ran its course. It was the police who were doing most of the destruction. We were really trying to get back in and break free. And we felt that we had freedom at last, or freedom to at least show that we demanded freedom. We weren't going to be walking meekly in the night and letting them shove us around—it's like standing your ground for the first time and in a really strong way, and that's what caught the police by surprise. There was something in the air, freedom a long time overdue, and we're going to fight for it. It took different forms, but the bottom line was, we weren't going to go away. And we didn't.
The only photograph taken during the first night of the riots shows the homeless youth that slept in nearby Christopher Park, scuffling with police. The Mattachine Society newsletter a month later offered its explanation of why the riots occurred: "It catered largely to a group of people who are not welcome in, or cannot afford, other places of homosexual social gathering... The Stonewall became home to these kids. When it was raided, they fought for it. That, and the fact that they had nothing to lose other than the most tolerant and broadminded gay place in town, explains why."
Garbage cans, garbage, bottles, rocks, and bricks were hurled at the building, breaking the windows. Witnesses attest that "flame queens", hustlers, and gay "street kids"—the most outcast people in the gay community—were responsible for the first volley of projectiles, as well as the uprooting of a parking meter used as a battering ram on the doors of the Stonewall Inn. Sylvia Rivera, who was in full drag and had been in the Stonewall during the raid, remembered: 
You've been treating us like shit all these years? Uh-uh. Now it's our turn!... It was one of the greatest moments in my life. 
The mob lit garbage on fire and stuffed it through the broken windows as the police grabbed a fire hose. Because it had no water pressure, the hose was ineffective in dispersing the crowd, and seemed only to encourage them. When demonstrators broke through the windows—which had been covered by plywood by the bar owners to deter the police from raiding the bar—the police inside unholstered their pistols. The doors flew open and officers pointed their weapons at the angry crowd, threatening to shoot. "The Village Voice" writer Howard Smith, in the bar with the police, took a wrench from the bar and stuffed it in his pants, unsure if he might have to use it against the mob or the police. He watched someone squirt lighter fluid into the bar; as it was lit and the police took aim, sirens were heard and fire trucks arrived. The onslaught had lasted 45 minutes.
Escalation.
The Tactical Police Force (TPF) of the New York City Police Department arrived to free the police trapped inside the Stonewall. One officer's eye was cut, and a few others were bruised from being struck by flying debris. Bob Kohler, who was walking his dog by the Stonewall that night, saw the TPF arrive: "I had been in enough riots to know the fun was over... The cops were totally humiliated. This never, ever happened. They were angrier than I guess they had ever been, because everybody else had rioted... but the fairies were not supposed to riot... no group had ever forced cops to retreat before, so the anger was just enormous. I mean, they wanted to kill." With larger numbers, police detained anyone they could and put them in patrol wagons to go to jail, though Inspector Pine recalled, "Fights erupted with the transvestites, who wouldn't go into the patrol wagon." His recollection was corroborated by another witness across the street who said, "All I could see about who was fighting was that it was transvestites and they were fighting furiously."
The TPF formed a phalanx and attempted to clear the streets by marching slowly and pushing the crowd back. The mob openly mocked the police. The crowd cheered, started impromptu kick lines, and sang to the tune of "The Howdy Doody Show" theme song: "We are the Stonewall girls/ We wear our hair in curls/ We don't wear underwear/ We show our pubic hairs." Lucian Truscott reported in "The Village Voice": "A stagnant situation there brought on some gay tomfoolery in the form of a chorus line facing the line of helmeted and club-carrying cops. Just as the line got into a full kick routine, the TPF advanced again and cleared the crowd of screaming gay power[-]ites down Christopher to Seventh Avenue." One participant who had been in the Stonewall during the raid recalled, "The police rushed us, and that's when I realized this is not a good thing to do, because they got me in the back with a nightstick." Another account stated, "I just can't ever get that one sight out of my mind. The cops with the [nightsticks] and the kick line on the other side. It was the most amazing thing... And all the sudden that kick line, which I guess was a spoof on the machismo... I think that's when I felt rage. Because people were getting smashed with bats. And for what? A kick line."
Craig Rodwell, owner of the Oscar Wilde Memorial Bookshop, reported watching police chase participants through the crooked streets, only to see them appear around the next corner behind the police. Members of the mob stopped cars, overturning one of them to block Christopher Street. Jack Nichols and Lige Clarke, in their column printed in "Screw", declared that "massive crowds of angry protesters chased [the police] for blocks screaming, 'Catch them!' "
By 4:00 in the morning the streets had nearly been cleared. Many people sat on stoops or gathered nearby in Christopher Park throughout the morning, dazed in disbelief at what had transpired. Many witnesses remembered the surreal and eerie quiet that descended upon Christopher Street, though there continued to be "electricity in the air". One commented: "There was a certain beauty in the aftermath of the riot... It was obvious, at least to me, that a lot of people really were gay and, you know, this was our street." Thirteen people had been arrested. Some in the crowd were hospitalized, and four police officers were injured. Almost everything in the Stonewall Inn was broken. Inspector Pine had intended to close and dismantle the Stonewall Inn that night. Pay phones, toilets, mirrors, jukeboxes, and cigarette machines were all smashed, possibly in the riot and possibly by the police.
Open rebellion.
During the siege of the Stonewall, Craig Rodwell called "The New York Times", the "New York Post", and the "Daily News" to inform them what was happening. All three papers covered the riots; "The New York Daily News" placed coverage on the front page. News of the riot spread quickly throughout Greenwich Village, fueled by rumors that it had been organized by the Students for a Democratic Society, the Black Panthers, or triggered by "a homosexual police officer whose roommate went dancing at the Stonewall against the officer's wishes". All day Saturday, June 28, people came to stare at the burned and blackened Stonewall Inn. Graffiti appeared on the walls of the bar, declaring "Drag power", "They invaded our rights", "Support gay power", and "Legalize gay bars", along with accusations of police looting, and—regarding the status of the bar—"We are open."
The next night, rioting again surrounded Christopher Street; participants remember differently which night was more frantic or violent. Many of the same people returned from the previous evening—hustlers, street youths, and "queens"—but they were joined by "police provocateurs", curious bystanders, and even tourists. Remarkable to many was the sudden exhibition of homosexual affection in public, as described by one witness: "From going to places where you had to knock on a door and speak to someone through a peephole in order to get in. We were just out. We were in the streets."
"You know, the guys there were so beautiful—they've lost that wounded look that fags all had 10 years ago " – Allen Ginsberg
 
Thousands of people had gathered in front of the Stonewall, which had opened again, choking Christopher Street until the crowd spilled into adjoining blocks. The throng surrounded buses and cars, harassing the occupants unless they either admitted they were gay or indicated their support for the demonstrators. Sylvia Rivera saw a friend of hers jump on a nearby car trying to drive through; the crowd rocked the car back and forth, terrifying its occupants. Another of Rivera's friends, Marsha P. Johnson, climbed a lamppost and dropped a heavy bag onto the hood of a police car, shattering the windshield. As on the previous evening, fires were started in garbage cans throughout the neighborhood. More than a hundred police were present from the Fourth, Fifth, Sixth, and Ninth Precincts, but after 2:00 a.m. the TPF arrived again. Kick lines and police chases waxed and waned; when police captured demonstrators, whom the majority of witnesses described as "sissies" or "swishes", the crowd surged to recapture them. Street battling ensued again until 4:00 a.m.
Beat poet and longtime Greenwich Village resident Allen Ginsberg lived on Christopher Street, and happened upon the jubilant chaos. After he learned of the riot that had occurred the previous evening, he stated, "Gay power! Isn't that great!... It's about time we did something to assert ourselves", and visited the open Stonewall Inn for the first time. While walking home, he declared to Lucian Truscott, "You know, the guys there were so beautiful—they've lost that wounded look that fags all had 10 years ago."
"Intolerable situation".
Activity in Greenwich Village was sporadic on Monday and Tuesday, partly due to rain. Police and Village residents had a few altercations, as both groups antagonized each other. Craig Rodwell and his partner Fred Sargeant took the opportunity the morning after the first riot to print and distribute 5,000 leaflets, one of them reading: "Get the Mafia and the Cops out of Gay Bars." The leaflets called for gays to own their own establishments, for a boycott of the Stonewall and other Mafia-owned bars, and for public pressure on the mayor's office to investigate the "intolerable situation".
Not everyone in the gay community considered the revolt a positive development. To many older homosexuals and many members of the Mattachine Society who had worked throughout the 1960s to promote homosexuals as no different from heterosexuals, the display of violence and effeminate behavior was embarrassing. Randy Wicker, who had marched in the first gay picket lines before the White House in 1965, said the "screaming queens forming chorus lines and kicking went against everything that I wanted people to think about homosexuals... that we were a bunch of drag queens in the Village acting disorderly and tacky and cheap." Others found the closing of the Stonewall Inn, termed a "sleaze joint", as advantageous to the Village.
On Wednesday, however, "The Village Voice" ran reports of the riots, written by Howard Smith and Lucian Truscott, that included unflattering descriptions of the events and its participants: "forces of faggotry", "limp wrists", and "Sunday fag follies". A mob descended upon Christopher Street once again and threatened to burn down the offices of "The Village Voice". Also in the mob of between 500 and 1,000 were other groups that had had unsuccessful confrontations with the police, and were curious how the police were defeated in this situation. Another explosive street battle took place, with injuries to demonstrators and police alike, looting in local shops, and arrests of five people. The incidents on Wednesday night lasted about an hour, and were summarized by one witness: "The word is out. Christopher Street shall be liberated. The fags have had it with oppression."
Aftermath.
The feeling of urgency spread throughout Greenwich Village, even to people who had not witnessed the riots. Many who were moved by the rebellion attended organizational meetings, sensing an opportunity to take action. On July 4, 1969, the Mattachine Society performed its annual picketing in front of Independence Hall in Philadelphia, called the Annual Reminder. Organizers Craig Rodwell, Frank Kameny, Randy Wicker, Barbara Gittings, and Kay Lahusen, who had all participated for several years, took a bus along with other picketers from New York City to Philadelphia. Since 1965, the pickets had been very controlled: women wore skirts and men wore suits and ties, and all marched quietly in organized lines. This year Rodwell remembered feeling restricted by the rules Kameny had set. When two women spontaneously held hands, Kameny broke them apart, saying, "None of that! None of that!" Rodwell, however, convinced about ten couples to hold hands. The hand-holding couples made Kameny furious, but they earned more press attention than all of the previous marches. Participant Lilli Vincenz remembered, "It was clear that things were changing. People who had felt oppressed now felt empowered." Rodwell returned to New York City determined to change the established quiet, meek ways of trying to get attention. One of his first priorities was planning Christopher Street Liberation Day.
Gay Liberation Front.
Although the Mattachine Society had existed since the 1950s, many of their methods now seemed too mild for people who had witnessed or been inspired by the riots. Mattachine recognized the shift in attitudes in a story from their newsletter entitled, "The Hairpin Drop Heard Around the World." When a Mattachine officer suggested an "amicable and sweet" candlelight vigil demonstration, a man in the audience fumed and shouted, "Sweet! "Bullshit!" That's the role society has been forcing these queens to play." With a flyer announcing: "Do You Think Homosexuals Are Revolting? You Bet Your Sweet Ass We Are!", the Gay Liberation Front (GLF) was soon formed, the first gay organization to use "gay" in its name. Previous organizations such as the Mattachine Society, the Daughters of Bilitis, and various homophile groups had masked their purpose by deliberately choosing obscure names.
The rise of militancy became apparent to Frank Kameny and Barbara Gittings—who had worked in homophile organizations for years and were both very public about their roles—when they attended a GLF meeting to see the new group. A young GLF member demanded to know who they were and what their credentials were. Gittings, nonplussed, stammered, "I'm gay. That's why I'm here." The GLF borrowed tactics from and aligned themselves with black and antiwar demonstrators with the ideal that they "could work to restructure American society". They took on causes of the Black Panthers, marching to the Women's House of Detention in support of Afeni Shakur, and other radical New Left causes. Four months after they formed, however, the group disbanded when members were unable to agree on operating procedure.
Gay Activists Alliance.
Within six months of the Stonewall riots, activists started a city-wide newspaper called "Gay"; they considered it necessary because the most liberal publication in the city—"The Village Voice"—refused to print the word "gay" in GLF advertisements seeking new members and volunteers. Two other newspapers were initiated within a six-week period: "Come Out!" and "Gay Power"; the readership of these three periodicals quickly climbed to between 20,000 and 25,000.
GLF members organized several same-sex dances, but GLF meetings were chaotic. When Bob Kohler asked for clothes and money to help the homeless youth who had participated in the riots, many of whom slept in Christopher Park or Sheridan Square, the response was a discussion on the downfall of capitalism. In late December 1969, several people who had visited GLF meetings and left out of frustration formed the Gay Activists Alliance (GAA). The GAA was to be entirely focused on gay issues, and more orderly. Their constitution started, "We as liberated homosexual activists demand the freedom for expression of our dignity and value as human beings." The GAA developed and perfected a confrontational tactic called a zap, where they would catch a politician off guard during a public relations opportunity, and force him or her to acknowledge gay and lesbian rights. City councilmen were zapped, and Mayor John Lindsay was zapped several times—once on television when GAA members made up the majority of the audience.
Raids on gay bars had not stopped after the Stonewall riots. In March 1970, Deputy Inspector Seymour Pine raided the Zodiac and 17 Barrow Street. An after-hours gay club with no liquor or occupancy licenses called The Snake Pit was soon raided, and 167 people were arrested. One of them was Diego Vinales, an Argentinian national so frightened that he might be deported as a homosexual that he tried to escape the police precinct by jumping out of a two-story window, impaling himself on a 14 in spike fence. "The New York Daily News" printed a graphic photo of the young man's impalement on the front page. GAA members organized a march from Christopher Park to the Sixth Precinct in which hundreds of gays, lesbians, and liberal sympathizers peacefully confronted the TPF. They also sponsored a letter-writing campaign to Mayor Lindsay in which the Greenwich Village Democratic Party and Congressman Ed Koch sent pleas to end raids on gay bars in the city.
The Stonewall Inn lasted only a few weeks after the riot. By October 1969 it was up for rent. Village residents surmised it was too notorious a location, and Rodwell's boycott discouraged business.
Gay Pride.
Christopher Street Liberation Day on June 28, 1970 marked the first anniversary of the Stonewall riots with an assembly on Christopher Street; with simultaneous Gay Pride marches in Los Angeles and Chicago, these mark the first Gay Pride marches in U.S. history. The next year, Gay Pride marches took place in Boston, Dallas, Milwaukee, London, Paris, West Berlin, and Stockholm. The New York Gay Pride march covered 51 blocks, from Christopher Street to Central Park. The march took less than half the scheduled time due to excitement, but also due to wariness about walking through the city with gay banners and signs. Although the parade permit was delivered only two hours before the start of the march, the marchers encountered little resistance from onlookers. "The New York Times" reported (on the front page) that the marchers took up the entire street for about 15 city blocks. Reporting by "The Village Voice" was positive, describing "the out-front resistance that grew out of the police raid on the Stonewall Inn one year ago".
"There was little open animosity, and some bystanders applauded when a tall, pretty girl carrying a sign "I am a Lesbian" walked by." – "The New York Times" coverage of Gay Liberation Day, 1970
 
By 1972 the participating cities included Atlanta, Buffalo, Detroit, Washington D.C., Miami, and Philadelphia, as well as San Francisco.
Frank Kameny soon realized the pivotal change brought by the Stonewall riots. An organizer of gay activism in the 1950s, he was used to persuasion, trying to convince heterosexuals that gay people were no different than they were. When he and other people marched in front of the White House, the State Department, and Independence Hall only five years earlier, their objective was to look as if they could work for the U.S. government. Ten people marched with Kameny then, and they alerted no press to their intentions. Although he was stunned by the upheaval by participants in the Annual Reminder in 1969, he later observed, "By the time of Stonewall, we had fifty to sixty gay groups in the country. A year later there was at least fifteen hundred. By two years later, to the extent that a count could be made, it was twenty-five hundred."
Similar to Kameny's regret at his own reaction to the shift in attitudes after the riots, Randy Wicker came to describe his embarrassment as "one of the greatest mistakes of his life". The image of gays retaliating against police, after so many years of allowing such treatment to go unchallenged, "stirred an unexpected spirit among many homosexuals". Kay Lahusen, who photographed the marches in 1965, stated, "Up to 1969, this movement was generally called the homosexual or homophile movement... Many new activists consider the Stonewall uprising the birth of the gay liberation movement. Certainly it was the birth of gay pride on a massive scale." David Carter, in his article "What made Stonewall different", explained that even though there were several uprisings before Stonewall, the reason Stonewall was so historical was that thousands of people were involved, the riot lasted a long time (six days), it was the first to get major media coverage, and it sparked the formation of many gay rights groups.
Legacy.
Unlikely community.
Within two years of the Stonewall riots there were gay rights groups in every major American city, as well as Canada, Australia, and Western Europe. People who joined activist organizations after the riots had very little in common other than their same-sex attraction. Many who arrived at GLF or GAA meetings were taken aback by the number of gay people in one place. Race, class, ideology, and gender became frequent obstacles in the years after the riots. This was illustrated during the 1973 Stonewall rally when, moments after Barbara Gittings exuberantly praised the diversity of the crowd, feminist activist Jean O'Leary protested what she perceived as the mocking of women by cross-dressers and drag queens in attendance. During a speech by O'Leary, in which she claimed that drag queens made fun of women for entertainment value and profit, Sylvia Rivera and Lee Brewster jumped on the stage and shouted "You go to bars because of what drag queens did for you, and "these bitches" tell us to quit being ourselves!" Both the drag queens and lesbian feminists in attendance left in disgust.
O'Leary also worked in the early 1970s to exclude trans people from gay rights issues because she felt that rights for trans people would be too difficult to attain. Sylvia Rivera left gay activism in the 1970s to work on issues for transgender people and cross-dressers. The initial disagreements between participants in the movements, however, often evolved after further reflection. O'Leary later regretted her stance against the drag queens attending in 1973: "Looking back, I find this so embarrassing because my views have changed so much since then. I would never pick on a transvestite now." "It was horrible. How could I work to exclude transvestites and at the same time criticize the feminists who were doing their best back in those days to exclude lesbians?"
O'Leary was referring to the Lavender Menace, a description by second wave feminist Betty Friedan for attempts by members of the National Organization for Women (NOW) to distance themselves from the perception of NOW as a haven for lesbians. As part of this process, Rita Mae Brown and other lesbians who had been active in NOW were forced out. They staged a protest in 1970 at the Second Congress to Unite Women, and earned the support of many NOW members, finally gaining full acceptance in 1971.
The growth of lesbian feminism in the 1970s at times so conflicted with the gay liberation movement that some lesbians refused to work with gay men. Many lesbians found men's attitudes patriarchal and chauvinistic, and saw in gay men the same misguided notions about women as they saw in heterosexual men. The issues most important to gay men—entrapment and public solicitation—were not shared by lesbians. In 1977 a Lesbian Pride Rally was organized as an alternative to sharing gay men's issues, especially what Adrienne Rich termed "the violent, self-destructive world of the gay bars". Veteran gay activist Barbara Gittings chose to work in the gay rights movement, rationalizing "It's a matter of where does it hurt the most? For me it hurts the most not in the female arena, but the gay arena."
Throughout the 1970s gay activism had significant successes. One of the first and most important was the "zap" in May 1970 by the Los Angeles GLF at a convention of the American Psychiatric Association (APA). At a conference on behavior modification, during a film demonstrating the use of electroshock therapy to decrease same-sex attraction, Morris Kight and GLF members in the audience interrupted the film with shouts of "Torture!" and "Barbarism!" They took over the microphone to announce that medical professionals who prescribed such therapy for their homosexual patients were complicit in torturing them. Although 20 psychiatrists in attendance left, the GLF spent the hour following the zap with those remaining, trying to convince them that homosexuals were not mentally ill. When the APA invited gay activists to speak to the group in 1972, activists brought John E. Fryer, a gay psychiatrist who wore a mask, because he felt his practice was in danger. In December 1973—in large part due to the efforts of gay activists—the APA voted unanimously to remove homosexuality from the "Diagnostic and Statistical Manual".
Gay men and lesbians came together to work in grassroots political organizations responding to organized resistance in 1977. A coalition of conservatives named Save Our Children staged a campaign to repeal a civil rights ordinance in Dade County, Florida. Save Our Children was successful enough to influence similar repeals in several American cities in 1978. However, the same year a campaign in California called the Briggs Initiative, designed to force the dismissal of homosexual public school employees, was defeated. Reaction to the influence of Save Our Children and the Briggs Initiative in the gay community was so significant that it has been called the second Stonewall for many activists, marking their initiation into political participation.
Rejection of gay subculture.
The Stonewall riots marked such a significant turning point that many aspects of prior gay and lesbian culture, such as bar culture formed from decades of shame and secrecy, were forcefully ignored and denied. Historian Martin Duberman writes, "The decades preceding Stonewall... continue to be regarded by most gays and lesbians as some vast neolithic wasteland." Historian Barry Adam notes, "Every social movement must choose at some point what to retain and what to reject out of its past. What traits are the results of oppression and what are healthy and authentic?" In conjunction with the growing feminist movement of the early 1970s, roles of butch and femme that developed in lesbian bars in the 1950s and 1960s were rejected, because as one writer put it: "all role playing is sick." Lesbian feminists considered the butch roles as archaic imitations of masculine behavior. Some women, according to Lillian Faderman, were eager to shed the roles they felt forced into playing. The roles returned for some women in the 1980s, although they allowed for more flexibility than before Stonewall.
Author Michael Bronski highlights the "attack on pre-Stonewall culture", particularly gay pulp fiction for men, where the themes often reflected self-hatred or ambivalence about being gay. Many books ended unsatisfactorily and drastically, often with suicide, and writers portrayed their gay characters as alcoholics or deeply unhappy. These books, which he describes as "an enormous and cohesive literature by and for gay men", have not been reissued and are lost to later generations. Dismissing the reason simply as political correctness, Bronski writes, "gay liberation was a youth movement whose sense of history was defined to a large degree by rejection of the past."
Lasting impact.
The riots spawned from a bar raid became a literal example of gays and lesbians fighting back, and a symbolic call to arms for many people. Historian David Carter remarks in his book about the Stonewall riots that the bar itself was a complex business that represented a community center, an opportunity for the Mafia to blackmail its own customers, a home, and a place of "exploitation and degradation". The true legacy of the Stonewall riots, Carter insists, is the "ongoing struggle for lesbian, gay, bisexual, and transgender equality". Historian Nicholas Edsall writes, Stonewall has been compared to any number of acts of radical protest and defiance in American history from the Boston Tea Party on. But the best and certainly a more nearly contemporary analogy is with Rosa Parks' refusal to move to the back of the bus in Montgomery, Alabama, in December 1955, which sparked the modern civil rights movement. Within months after Stonewall radical gay liberation groups and newsletters sprang up in cities and on college campuses across America and then across all of northern Europe as well.
Before the rebellion at the Stonewall Inn, homosexuals were, as historians Dudley Clendinen and Adam Nagourney write, a secret legion of people, known of but discounted, ignored, laughed at or despised. And like the holders of a secret, they had an advantage which was a disadvantage, too, and which was true of no other minority group in the United States. They were invisible. Unlike African Americans, women, Native Americans, Jews, the Irish, Italians, Asians, Hispanics, or any other cultural group which struggled for respect and equal rights, homosexuals had no physical or cultural markings, no language or dialect which could identify them to each other, or to anyone else... But that night, for the first time, the usual acquiescence turned into violent resistance... From that night the lives of millions of gay men and lesbians, and the attitude toward them of the larger culture in which they lived, began to change rapidly. People began to appear in public as homosexuals, demanding respect.
Historian Lillian Faderman calls the riots the "shot heard round the world", explaining, "The Stonewall Rebellion was crucial because it sounded the rally for that movement. It became an emblem of gay and lesbian power. By calling on the dramatic tactic of violent protest that was being used by other oppressed groups, the events at the Stonewall implied that homosexuals had as much reason to be disaffected as they."
Joan Nestle co-founded the Lesbian Herstory Archives in 1974, and credits "its creation to that night and the courage that found its voice in the streets." Cautious, however, not to attribute the start of gay activism to the Stonewall riots, Nestle writes,
I certainly don't see gay and lesbian history starting with Stonewall... and I don't see resistance starting with Stonewall. What I do see is a historical coming together of forces, and the sixties changed how human beings endured things in this society and what they refused to endure... Certainly something special happened on that night in 1969, and we've made it more special in our need to have what I call a point of origin... it's more complex than saying that it all started with Stonewall.
The events of the early morning of June 28, 1969 were not the first instances of homosexuals fighting back against police in New York City and elsewhere. Not only had the Mattachine Society been active in major cities such as Los Angeles and Chicago, but similarly marginalized people started the riot at Compton's Cafeteria in 1966, and another riot responded to a raid on Los Angeles' Black Cat Tavern in 1967. However, several circumstances were in place that made the Stonewall riots memorable. The location of the raid was a factor: it was across the street from "The Village Voice" offices, and the narrow crooked streets gave the rioters advantage over the police. Many of the participants and residents of Greenwich Village were involved in political organizations that were effectively able to mobilize a large and cohesive gay community in the weeks and months after the rebellion. The most significant facet of the Stonewall riots, however, was the commemoration of them in Christopher Street Liberation Day, which grew into the annual Gay Pride events around the world.
The middle of the 1990s was marked by the inclusion of bisexuals as a represented group within the gay community, when they successfully sought to be included on the platform of the 1993 March on Washington for Lesbian, Gay and Bi Equal Rights and Liberation. Transgender people also asked to be included, but were not, though trans-inclusive language was added to the march's list of demands. The transgender community continued to find itself simultaneously welcome and at odds with the gay community as attitudes about binary and fluid sexual orientation and gender developed and came increasingly into conflict. In 1994, New York City celebrated "Stonewall 25" with a march that went past the United Nations Headquarters and into Central Park. Estimates put the attendance at 1.1 million people. Sylvia Rivera led an alternate march in New York City in 1994 to protest the exclusion of transgender people from the events. Attendance at Gay Pride events has grown substantially over the decades. Most large cities around the world now have some kind of Pride demonstration. Pride events in some cities mark the largest annual celebration of any kind. The growing trend towards commercializing marches into parades—with events receiving corporate sponsorship—has caused concern about taking away the autonomy of the original grassroots demonstrations that put inexpensive activism in the hands of individuals.
In June 1999 the U.S. Department of the Interior designated 51 and 53 Christopher Street, the street itself, and the surrounding streets as a National Historic Landmark, the first of significance to the lesbian, gay, bisexual and transgender community. In a dedication ceremony, Assistant Secretary of the Department of the Interior John Berry stated, "Let it forever be remembered that here—on this spot—men and women stood proud, they stood fast, so that we may be who we are, we may work where we will, live where we choose and love whom our hearts desire."
On June 1, 2009, President Barack Obama declared June 2009 Lesbian, Gay, Bisexual, and Transgender Pride Month, citing the riots as a reason to "commit to achieving equal justice under law for LGBT Americans". The year marked the 40th anniversary of the riots, giving journalists and activists cause to reflect on progress made since 1969. Frank Rich in "The New York Times" noted that no federal legislation exists to protect the rights of gay Americans. An editorial in the "Washington Blade" compared the scruffy, violent activism during and following the Stonewall riots to the lackluster response to failed promises given by President Obama; for being ignored, wealthy LGBT activists reacted by promising to give less money to Democratic causes. Two years later, the Stonewall Inn served as a rallying point for celebrations after the New York Senate voted to pass same-sex marriage. The act was signed into law by Governor Andrew Cuomo on June 24, 2011.
Obama also referenced the Stonewall riots in a call for full equality during his second inaugural address on January 21, 2013:"We, the people, declare today that the most evident of truths—that all of us are created equal—is the star that guides us still; just as it guided our forebears through Seneca Falls, and Selma, and Stonewall... Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law—for if we are truly created equal, then surely the love we commit to one another must be equal as well."This was a historic moment, being the first time that a president mentioned gay rights or the word "gay" in an inaugural address.
In 2014 a marker dedicated to the Stonewall riots was included in the Legacy Walk, an outdoor public display in Chicago celebrating LGBT history and people.
See also.
Films
References.
Bibliography.
</dl>

</doc>
<doc id="29388" url="http://en.wikipedia.org/wiki?curid=29388" title="Sheffer stroke">
Sheffer stroke

In Boolean functions and propositional calculus, the Sheffer stroke, named after Henry M. Sheffer, written "|" (see vertical bar, not to be confused with "||" which is often used to represent disjunction), "D"pq"", or "↑" (an upwards arrow), denotes a logical operation that is equivalent to the negation of the conjunction operation, expressed in ordinary language as "not both". It is also called nand ("not and") or the alternative denial, since it says in effect that at least one of its operands is false. In Boolean algebra and digital electronics it is known as the NAND operation.
Like its dual, the NOR operator (also known as the Peirce arrow or Quine dagger), NAND can be used by itself, without any other logical operator, to constitute a logical formal system (making NAND functionally complete). This property makes the NAND gate crucial to modern digital electronics, including its use in NAND flash memory and computer processor design.
Definition.
The NAND operation is a logical operation on two logical values. It produces a value of true, if — and only if — at least one of the propositions is false.
Truth table.
The truth table of A NAND B (also written as A | B, Dpq, or A ↑ B) is as follows:
History.
The stroke is named after Henry M. Sheffer, who in 1913 published a paper in the "Transactions of the American Mathematical Society" (Sheffer 1913) providing an axiomatization of Boolean algebras using the stroke, and proved its equivalence to a standard formulation thereof by Huntington employing the familiar operators of propositional logic (and, or, not). Because of self-duality of Boolean algebras, Sheffer's axioms are equally valid for either of the NAND or NOR operations in place of the stroke. Sheffer interpreted the stroke as a sign for non-disjunction (NOR) in his paper, mentioning non-conjunction only in a footnote and without a special sign for it. It was Jean Nicod who first used the stroke as a sign for non-conjunction (NAND) in a paper of 1917 and which has since become current practice. Russell and Whitehead used the Sheffer stroke in the 1927 second edition of Principia Mathematica and suggested it as a replacement for the "or" and "not" operations of the first edition. 
Charles Sanders Peirce (1880) had discovered the functional completeness of NAND or NOR more than 30 years earlier, using the term "ampheck" (for 'cutting both ways'), but he never published his finding.
Properties.
NAND does not possess any of the following five properties, each of which is required to be absent from, and the absence of all of which is sufficient for, at least one member of a set of functionally complete operators: truth-preservation, falsity-preservation, linearity, monotonicity, self-duality. (An operator is truth- (falsity-) preserving if its value is truth (falsity) whenever all of its arguments are truth (falsity).) Therefore {NAND} is a functionally complete set.
This can also be realized as follows: All three elements of the functionally complete set {AND, OR, NOT} can be constructed using only NAND. Thus the set {NAND} must be functionally complete as well.
Introduction, elimination, and equivalencies.
The Sheffer stroke formula_1 is the negation of the conjunction:
Expressed in terms of NAND formula_1, the usual operators of propositional logic are:
Formal system based on the Sheffer stroke.
The following is an example of a formal system based entirely on the Sheffer stroke, yet having the functional expressiveness of the propositional logic:
Symbols.
"pn" for natural numbers "n" <br>
The Sheffer stroke commutes but does not associate (e.g., (T|T)|F = T, but T|(T|F) = F). Hence any formal system including the Sheffer stroke must also include a means of indicating grouping. We shall employ '(' and ')' to this effect.
We also write "p", "q", "r", … instead of "p"0, "p"1, "p"2.
Syntax.
Construction Rule I: For each natural number "n", the symbol "pn" is a well-formed formula (wff), called an atom.
Construction Rule II: If "X" and "Y" are wffs, then ("X"|"Y") is a wff.
Closure Rule: Any formulae which cannot be constructed by means of the first two Construction Rules are not wffs.
The letters "U", "V", "W", "X", and "Y" are metavariables standing for wffs.
A decision procedure for determining whether a formula is well-formed goes as follows: "deconstruct" the formula by applying the Construction Rules backwards, thereby breaking the formula into smaller subformulae. Then repeat this recursive deconstruction process to each of the subformulae. Eventually the formula should be reduced to its atoms, but if some subformula cannot be so reduced, then the formula is not a wff.
Calculus.
All wffs of the form
are axioms. Instances of
are inference rules.
Simplification.
Since the only connective of this logic is |, the symbol | could be discarded altogether, leaving only the parentheses to group the letters. A pair of parentheses must always enclose a pair of "wff"s. Examples of theorems in this simplified notation are
The notation can be simplified further, by letting
for any "U". This simplification causes the need to change some rules:
The result is a parenthetical version of the Peirce existential graphs.
Another way to simplify the notation is to eliminate parenthesis by using Polish Notation. For example, the earlier examples with only parenthesis could be rewritten using only strokes as follows
This follows the same rules as the parenthesis version, with opening parenthesis replaced with a Sheffer stroke and the (redundant) closing parenthesis removed.

</doc>
<doc id="29390" url="http://en.wikipedia.org/wiki?curid=29390" title="Stalactite">
Stalactite

A stalactite (, ; from the Greek "stalasso", (σταλάσσω), "to drip", and meaning "that which drips") is a type of formation that hangs from the ceiling of caves, hot springs, or manmade structures such as bridges and mines. Any material which is soluble, can be deposited as a colloid, or is in suspension, or is capable of being melted, may form a stalactite. Stalactites may be composed of amberat, lava, minerals, mud, peat, pitch, sand, and sinter. A stalactite is not necessarily a speleothem, though speleothems are the most common form of stalactite because of the abundance of limestone caves.
The corresponding formation on the floor of the cave is known as a stalagmite.
Formation and type.
Limestone stalactites.
The most common stalactites are speleothems, which occur in limestone caves. They form through deposition of calcium carbonate and other minerals, which is precipitated from mineralized water solutions. Limestone is the chief form of calcium carbonate rock which is dissolved by water that contains carbon dioxide, forming a calcium bicarbonate solution in underground caverns. The chemical formula for this reaction is:
This solution travels through the rock until it reaches an edge and if this is on the roof of a cave it will drip down. When the solution comes into contact with air the chemical reaction that created it is reversed and particles of calcium carbonate are deposited. The reversed reaction is:
An average growth rate is 0.13 mm a year. The quickest growing stalactites are those formed by fast-flowing water rich in calcium carbonate and carbon dioxide, these can grow at 3 mm per year.
All limestone stalactites begin with a single mineral-laden drop of water. When the drop falls, it deposits the thinnest ring of calcite. Each subsequent drop that forms and falls deposits another calcite ring. Eventually, these rings form a very narrow (0.5 mm), hollow tube commonly known as a "soda straw" stalactite. Soda straws can grow quite long, but are very fragile. If they become plugged by debris, water begins flowing over the outside, depositing more calcite and creating the more familiar cone-shaped stalactite. The same water drops that fall from the tip of a stalactite deposit more calcite on the floor below, eventually resulting in a rounded or cone-shaped stalagmite. Unlike stalactites, stalagmites never start out as hollow "soda straws." Given enough time, these formations can meet and fuse to create pillars of calcium carbonate.
Lava stalactites.
Another type of stalactite is formed in lava tubes while lava is still active inside. The mechanism of formation is similar to that of limestone stalactites. Essentially, it is still the deposition of material on the ceilings of caves, however with lava stalactites formation happens very quickly in only a matter of hours, days, or weeks, whereas limestone stalactites may take up to thousands of years. A key difference with lava stalactites is that once the lava has ceased flowing, so too will the stalactites cease to grow. This means that if the stalactite were to be broken it would never grow back.
The generic term "lavacicle" has been applied to lava stalactites and stalagmites indiscriminately and evolved from the word icicle.
Like limestone stalactites, they can leave lava drips on the floor that turn into lava stalagmites and may eventually fuse with the corresponding stalactite to form a column.
Shark tooth stalactites The shark tooth stalactite is broad and tapering in appearance. It may begin as a small driblet of lava from a semi-solid ceiling, but then grows by accreting layers as successive flows of lava rise and fall in the lava tube, coating and recoating the stalactite with more material. They can vary from a few millimeters to over a meter in length. 
Splash stalactites
As lava flows through a tube, material will be splashed up on the ceiling and ooze back down, hardening into a stalactite. This type of formation results in a very irregularly shaped stalactite, looking somewhat like stretched taffy. Often they may be of a different color than the original lava that formed the cave.
Tubular lava stalactites
When the roof of a lava tube is cooling, a skin will form that traps semi-molten material inside. Trapped gases force lava to extrude out through small openings that result in hollow, tubular stalactites analogous to the soda straws formed as depositional speleothems in solution caves, The longest known is almost 2 meters in length. These are common in Hawaiian lava tubes and are often associated with a drip stalagmite that forms below as material is carried through the tubular stalactite and piles up on the floor beneath. Sometimes the tubular form collapses near the distal end, most likely when the pressure of escaping gases decreased and still-molten portions of the stalactites deflated and cooled.
Often these tubular stalactites will acquire a twisted, vermiform appearance as bits of lava crystallize and force the flow in different directions. These tubular lava helictites may also be influenced by air currents through a tube and point downwind.
Ice stalactites.
A common stalactite found seasonally or year round in many caves is the ice stalactite, commonly referred to as icicles, especially on the surface. Water seepage from the surface will penetrate into a cave and if temperatures are below freezing the water will form stalactites. Creation may also be done by the freezing of water vapor. Similar to lava stalactites, ice stalactites form very quickly within hours or days. Unlike lava stalactites however, they may grow back as long as water and temperatures are suitable.
Ice stalactites can also form under sea ice when saline water is introduced to ocean water. These specific stalactites are referred to as brinicles.
Ice stalactites may also form corresponding stalagmites below them and given time may grow together to form an ice column.
Concrete stalactites.
Stalactites can also form on concrete, and on plumbing where there is a slow leak and limestone (or other minerals) in the water supply, although they form much more rapidly there than in the natural cave environment (description and experiments see literature).
The way stalactites form on concrete is due to different chemistry than those that form naturally in limestone caves and is the result of the presence of calcium oxide in concrete. This calcium oxide reacts with any rainwater that penetrates the concrete and forms a solution of calcium hydroxide. The chemical formula for this is:
Over time this calcium hydroxide solution reaches the edge of the concrete and, if the concrete is suspended in the air, for example, in a ceiling or a beam, then this will drip down from the edge. When this happens the solution comes into contact with air and another chemical reaction takes place. The solution reacts with carbon dioxide in the air and precipitates calcium carbonate.
When this solution drops down it leaves behind particles of calcium carbonate and over time these form into a stalactite. They are normally a few centimeters long and with a diameter of approximately 5 mm.
Records.
The White Chamber in the Jeita Grotto's upper cavern in Lebanon contains an 8.2 m limestone stalactite which is accessible to visitors and is claimed to be the longest stalactite in the world. Another such claim is made for a 20 m limestone stalactite that hangs in the Chamber of Rarities in the Gruta Rei do Mato (Sete Lagoas, Minas Gerais, Brazil). However, vertical cavers have often encountered longer stalactites while exploring. One of the longest stalactites viewable by the general public is in Pol an Ionain (Doolin Cave), County Clare, Ireland, in a karst region known as The Burren; what makes it more impressive is the fact that the stalactite is held on by a section of calcite less than 0.3 sqm.
Origin of the term.
Stalactites are first mentioned (though not by name) by the Roman natural historian Pliny in a text which also mentions stalagmites and columns and refers to their creation by the dripping of water. The term "stalactite" was coined in the 17th century by the Danish Physician Ole Worm, who coined the Latin word from the Greek word σταλακτός (stalaktos, "dripping") and the Greek suffix -ίτης (-ites, connected with or belonging to)

</doc>
<doc id="29391" url="http://en.wikipedia.org/wiki?curid=29391" title="Strangers in Paradise">
Strangers in Paradise

Strangers in Paradise is a long-running, mostly self-published black-and-white comic book, written and drawn by Terry Moore. The first issue was published January 1, 1993. The series has reached its planned conclusion, finishing off in 2007 with issue #90 of volume 3. However, it was announced at Comic-Con International 2012 there will be another story in the form of a novel.
Origins.
Terry Moore stated that "I started out wanting to do a newspaper strip, and tried one idea after another before I realised I hated the gag-a-day life and really wanted to try a story instead." The story he chose to tell turned out to be "Strangers in Paradise", or "this story about 2 girls and a guy who gets to know them" (from Moore's introduction to "The Collected Strangers in Paradise, Volume One"), which used characters he had developed during his time on the gag-a-day circuit. For example, Katchoo appears as a "happy-go-lucky wood nymph" in an early strip by Moore about an enchanted forest. These strips were collected into two trade paperbacks, but they did not include three issues. Because of this, the entire run was later published in one large paperback edition entitled "The Complete Paradise Too". This volume can be considered the true origin of Katchoo, Francine and the "SiP" universe.
Plot.
The story primarily concerns the difficult relationship between two women, Helen Francine Peters (known simply as Francine) and Katina Marie ("Katchoo") Choovanski, and their friend David Qin. Francine considers Katchoo her best friend; Katchoo is in love with Francine. David is in love with Katchoo (a relationship which Katchoo herself is deeply confused about).
This plays itself out over a second plot element, a thriller style story concerning the shadowy 'Big Six' organization. Its leader is a woman named Darcy Parker, who uses highly trained women (of whom Katchoo was one) to infiltrate—and to an extent control—the American political system. It uses a non-linear approach to storytelling, putting in sequence elements of the story often years apart, and going back in time frequently.
"SiP", as it is commonly known, began as a three-issue mini-series published by Antarctic Press in 1993, which focused entirely on the relationship between the three main characters and Francine's unfaithful boyfriend. This is now known as "Volume 1". Thirteen issues were published under Moore's own "Abstract Studio" imprint, and these make up "Volume 2". This is where the "thriller" plot was introduced. The series moved to Image Comics' Homage imprint for the start of "Volume 3", but after eight issues moved back to Abstract Studio, where it continued with the same numbering. Volume 3, and with it the series itself, has concluded, with Moore ending at issue #90 June 6. 2007.
Awards.
The series received the Eisner Award for Best Serialized Story in 1996 for "I Dream of You" as well as the National Cartoonists Society Reuben Award for Best Comic Book in 2003. It also won the GLAAD Award for Best Comic Book in 2001.
Collected editions.
It has been collected into a series of full-size trade paperbacks, hardback collections, and smaller format paperback collections. These reprints collect the issues into different sets.
The full-size paperback collections to date are:
The hardback collections to date are:
The "pocket book" collections to date are:
Other books to date are:
Merchandise.
Two limited edition statuettes of Katchoo were produced by Clayburn Moore as the first in a planned series of three statues based around the series. In the first she is standing in a skimpy black dress, and in the second she is reclining in a bath wearing her leather jacket and holding a drink and a gun.
In 2009 Shocker Toys released a Katchoo figure as part of the first series of its "Indie Spotlight" line.
In 1996 a series of trading cards was released by Comic Images, consisting of a 90-card base set plus extra collector's cards, such as the 500 'autograph cards' that featured Terry Moore's signature and information on the creation of "SiP". These extra cards were inserted randomly into packs. Also produced was a matching "SiP" binder, which came with 12 9-pocket sleeves to hold the cards.
Advertised on the official "SiP" website are character pin badges representing Francine, Katchoo and David. There is also a black tote bag featuring the "Strangers in Paradise" logo and a tumbler decorated with colour panels from the series, in addition to a postcard set and two T shirts, although several of these items are listed as 'sold out', and are hard to come by elsewhere.

</doc>
<doc id="29392" url="http://en.wikipedia.org/wiki?curid=29392" title="Summer">
Summer

Summer is the hottest of the four temperate seasons, falling between spring and autumn. At the summer solstice, the days are longest and the nights are shortest, with day-length decreasing as the season progresses after the solstice. The date of the beginning of summer varies according to climate, tradition and culture, but when it is summer in the Northern Hemisphere it is winter in the Southern Hemisphere, and vice versa.
Timing.
From an astronomical view, the equinoxes and solstices would be the middle of the respective seasons, but a variable seasonal lag means that the meteorological start of the season, which is based on average temperature patterns, occurs several weeks later than the start of the astronomical season. According to meteorologists, summer extends for the whole months of June, July, and August in the northern hemisphere and the whole months of December, January, and February in the southern hemisphere. Under meteorological definitions, all seasons are arbitrarily set to start at the beginning of a calendar month and end at the end of a month. This meteorological definition of summer also aligns with the commonly viewed notion of summer as the season with the longest (and warmest) days of the year, in which daylight predominates. The meteorological reckoning of seasons is used in Austria, Denmark, the former Soviet Union and Japan. It is also used by many in the United Kingdom. In Ireland, the summer months according to the national meteorological service, Met Éireann, are June, July and August. However, according to the Irish Calendar summer begins on 1 May and ends on 1 August. School textbooks in Ireland follow the cultural norm of summer commencing on 1 May rather than the meteorological definition of 1 June.
Days continue to lengthen from equinox to solstice and summer days progressively shorten after the solstice, so meteorological summer encompasses the build-up to the longest day and a diminishing thereafter, with summer having many more hours of daylight than spring. Reckoning by hours of daylight alone, summer solstice marks the midpoint, not the beginning, of the seasons. Midsummer takes place over the shortest night of the year, which is the summer solstice, or on a nearby date that varies with tradition.
Where a seasonal lag of half a season or more is common, reckoning based on astronomical markers is shifted half a season. By this method, in North America, summer is the period from the summer solstice (usually 20 or 21 June in the Northern Hemisphere) to the autumn equinox.
Reckoning by cultural festivals, the summer season in the United States is commonly regarded as beginning on Memorial Day weekend (the last weekend in May) and ending on Labor Day weekend (the first weekend in September), more closely in line with the meteorological definition for the parts of the country that have four-season weather. The similar Canadian tradition starts summer on Victoria Day one week prior (although summer conditions vary widely across Canada's expansive territory) and ends, as in the United States, on Labour Day.
In Chinese astronomy, summer starts on or around 5 May, with the "jiéqì" (solar term) known as lìxià (立夏), i.e. "establishment of summer", and it ends on or around 6 August.
In southern and southeast Asia, where the monsoon occurs, summer is more generally defined as lasting from March, April, May and June, the warmest time of the year, ending with the onset of the monsoon rains.
Because the temperature lag is shorter in the oceanic temperate southern hemisphere, most countries in this region use the meteorological definition with summer starting on 1 December and ending on the last day of February.
In Australia and New Zealand, Summer officially begins on 1 December and ends on 28 and 29 February.
Weather.
Summer is traditionally associated with hot or warm weather. In the Mediterranean regions, it is also associated with dry weather, while in other places (particularly in Eastern Asia because of the Monsoon) it is associated with rainy weather. The wet season is the main period of vegetation growth within the savanna climate regime. Where the wet season is associated with a seasonal shift in the prevailing winds, it is known as a monsoon.
In the northern Atlantic Ocean, a distinct tropical cyclone season occurs from 1 June to 30 November. The statistical peak of the Atlantic hurricane season is 10 September. The Northeast Pacific Ocean has a broader period of activity, but in a similar time frame to the Atlantic. The Northwest Pacific sees tropical cyclones year-round, with a minimum in February and March and a peak in early September. In the North Indian basin, storms are most common from April to December, with peaks in May and November. In the Southern Hemisphere, the tropical cyclone season runs from 1 November until the end of April with peaks in mid-February to early March.
Thunderstorm season in the USA and Canada runs in the spring through summer. These storms can produce hail, strong winds and tornadoes, usually during the afternoon and evening.
Holidays.
School breaks.
Schools and universities typically have a summer break to take advantage of the warmer weather and longer days. In almost all countries, children are out of school during this time of year for summer break, although dates vary. In the United States, public schools usually end in early June while colleges get out in early May. In India, school ends in April end and resumes in late June or early July. In England and Wales, school ends in mid-July and resumes again in early September; in Scotland, the summer holiday begins in late June and ends in mid- to late-August. Similarly, in Canada the summer holidays start in late June and end at the very start of September. In Pakistan, school usually ends in early June and resumes in mid-September. In the Philippines, the months of April and May make up the summer break In the Southern Hemisphere, school summer holiday dates include the major holidays of Christmas and New Year's Day. School summer holidays in Australia, New Zealand and South Africa begin in mid-December and end in late January, with the dates varying between states. In Cameroon and Nigeria, Schools usually go for summer vacation in mid-July and resume back in the later weeks of September or first week of October.
Public holidays.
A wide range of public holidays fall during summer, including:
Activities.
People take advantage of the warmer temperatures by spending more time outdoors during the summer. Activities such as traveling to the beach and picnics occur during summer months. Sports such as basketball, American football, volleyball, skateboarding, baseball, softball, cricket, tennis, and water polo are played. Water sports also occur. These include water skiing, wake boarding, swimming, surfing, and tubing. The modern Olympics have been held during the summer months every four years since 1896. The 2000 Summer Olympics, in Sydney, however, were held during the Australian spring.
Summer is usually a low point in television viewing, and television schedules generally reflect this by not scheduling new episodes of their most popular shows between the end of May sweeps and the beginning of the television season in September, instead scheduling low-cost reality television shows and burning off commitments to already-canceled series. There is an exception to this with children's television. Many television shows made for children and are popular with children are released during the summer months, especially on children's cable channels such as the Disney Channel in the United States, as children are off school. Disney Channel for example ends its preschool programming earlier in the day for older school age children in the summer months while it reverts back to the original scheduling as the school year begins. Conversely, the music and film industries generally experience higher returns during the summer than other times of the year and market their summer hits accordingly. The summer season is also most popular for animated movies to be released theatrically in movie theaters.
With most school-age children and college students (except those attending summer school) on summer vacation during the summer months, especially in the United States, travel and vacationing traditionally peaks during the summer, with the volume of travel in a typical summer weekend rivaled only by Thanksgiving. Teenagers and college students often take summer jobs in industries that cater to recreation. Business activity for the recreation, tourism, restaurant, and retail industries peak during the Summer months as well as the holiday season.

</doc>
<doc id="29393" url="http://en.wikipedia.org/wiki?curid=29393" title="Spring">
Spring

Spring(s) may refer to:

</doc>
<doc id="29394" url="http://en.wikipedia.org/wiki?curid=29394" title="Shrike">
Shrike

Shrikes are passerine birds of the family Laniidae. The family is composed of thirty-one species in three genera. The family name, and that of the largest genus, "Lanius", is derived from the Latin word for "butcher", and some shrikes are also known as "butcher birds" because of their feeding habits. They are fairly closely related to the bush-shrike family Malaconotidae.
Distribution, migration and habitat.
Most shrike species have a Eurasian and African distribution, with just two breeding in North America (the loggerhead and great grey shrikes). There are no members of this family in South America or Australia, although one species reaches New Guinea. The shrikes vary in the extent of their ranges, with some species like the great grey shrike ranging across the northern hemisphere to the Newton's fiscal which is restricted to the island of São Tomé. 
They inhabit open habitats, especially steppe and savannah. A few species of shrike are forest dwellers, seldom occurring in open habitats. Some species breed in northern latitudes during the summer, then migrate to warmer climes for the winter.
Description.
Shrikes are medium-sized birds, up to 50 cm in length, with grey, brown, or black and white plumage. Their beaks are hooked, like that of a bird of prey, reflecting their predatory nature, and their calls are strident.
Behaviour.
Shrikes are known for their habit of catching insects and small vertebrates and impaling their bodies on thorns, the spikes on barbed-wire fences or any available sharp point. This helps them to tear the flesh into smaller, more conveniently-sized fragments, and serves as a cache so that the shrike can return to the uneaten portions at a later time. This same behavior of impaling insects serves as an adaptation to eating the toxic lubber grasshopper, "Romalea guttata". The bird waits for 1–2 days for the toxins within the grasshopper to degrade, and then can eat it.
Shrikes are territorial, and these territories are defended from other pairs. In migratory species a breeding territory is defended in the breeding grounds and a smaller feeding territory is established during migration and in the wintering grounds. Where several species of shrike exist together competition for territories can be intense. 
Shrikes make regular use of exposed perch sites, where they adopt a conspicuous upright stance. These sites are used in order to watch for prey items and to advertise their presence to rivals.
Breeding.
The shrikes are generally monogamous breeders, although polygyny has been recorded in some species. Co-operative breeding, where younger birds help their parents raise the next generation of young, has been recorded in both species in the genera "Eurocephalus" and "Corvinella" as well as one species of "Lanius". Males attract females to their territory with well stocked caches, which may include inedible but brightly coloured items. During courtship the male will perform a ritualised dance which includes actions that mimic the skewering of prey on thorns and will feed the female. Shrikes make simple, cup-shaped nests from twigs and grasses, in bushes and the lower branches of trees.
Species in taxonomic order.
FAMILY: LANIIDAE
Birds with similar names.
Other species, popularly called "shrikes", are in the families: 
The Prionopidae and Malaconotidae are quite closely related to the Laniidae, and were formerly included in the shrike family. The cuckoo-shrikes are not closely related to the true shrikes.
The Australasian butcherbirds are not shrikes, although they occupy a similar ecological niche.

</doc>
<doc id="29396" url="http://en.wikipedia.org/wiki?curid=29396" title="Screwdriver (cocktail)">
Screwdriver (cocktail)

A screwdriver is a popular alcoholic highball drink made with orange juice and vodka. While the basic drink is simply the two ingredients, there are many variations; the most common one is made with one part vodka, one part of any kind of orange soda, and one part of orange juice. Many of the variations have different names in different parts of the world. The International Bartender Association has designated this cocktail as an IBA Official Cocktail.
History.
This drink appears in literature as early as 1938 "And answered it "The famous Smirnoff Screwdriver", Just pour a jigger of smirnoff vodka over ice cubes, fill glass with orange juice and serve" Then later it is claimed that this drink was invented by American aviators "A Screwdriver —a half-orange-juice and half-vodka drink popularized by interned American aviators—costs a dollar including the customary barman's tip." 
A written reference to the screwdriver is from the October 24, 1949 issue of "Time":
In the dimly lighted bar of the sleek Park Hotel, Turkish intelligence agents mingle with American engineers and Balkan refugees, drinking the latest Yankee concoction of vodka and orange juice, called a 'screwdriver'.
Variations.
A screwdriver with equal parts vanilla vodka and Blue Curaçao topped with lemon-lime soda is a "Sonic Screwdriver".
A screwdriver with equal parts vodka and Mountain Dew is a "Dew Driver".
A screwdriver with one part of Tequila and two parts of orange juice is a "Mexican Screw".
A screwdriver with one part of Sloe Gin and two parts of orange juice is a "Sloe Screw".
A screwdriver with one part of Southern Comfort and two parts of orange juice is a "Comfortable Screw".
A screwdriver with two parts of Sloe Gin, one part of Southern Comfort and filled with orange juice is a "Slow Comfortable Screw".
A screwdriver with three parts vodka, six parts orange juice and one part Galliano is a "Harvey Wallbanger".
A screwdriver with one part of Sloe Gin, one part of Southern Comfort and one part Galliano and filled with orange juice is a "Slow Comfortable Screw Up Against The Wall".
A screwdriver with one part of Bourbon and two parts of orange juice is a "American Screw".
A screwdriver with one part of Cognac and two parts of orange juice is a "French Screw".
A screwdriver with one part of Galliano and two parts of orange juice is a "Italian Screw".
A screwdriver with one part of Gin and two parts of orange juice is a "Left Handed Screwdriver".
A screwdriver with one part of Rum and two parts of orange juice is a "Cuban Screw" or "Scurvy Medic".
A screwdriver with one part of Schnapps and two parts of orange juice is a "German Screw".
A screwdriver with one part of Chambord and two parts of orange juice is a "Royal Screw".
A screwdriver with one part of Brandy and two parts orange juice is a "Rusty Screw".
A screwdriver with one part of Cointreau and two parts orange juice is a "Double Screw".
A shot of Vodka with an orange wedge either dipped or covered with sugar is a "Cordless Screwdriver"

</doc>
<doc id="29398" url="http://en.wikipedia.org/wiki?curid=29398" title="Single-stage-to-orbit">
Single-stage-to-orbit

A single-stage-to-orbit (or SSTO) vehicle reaches orbit from the surface of a body without jettisoning hardware, expending only propellants and fluids. The term usually, but not exclusively, refers to reusable vehicles.
No Earth-launched SSTO launch vehicles have ever been constructed. To date, orbital launches have been performed either by multi-stage fully or partially expendable rockets, or by the Space Shuttle which was multi-stage and partially reusable.
Launch costs for Low Earth Orbit (LEO) range from $4500 to $8500 per pound of payload ($10000 - $19000 / kg). Reusable SSTO vehicles offer the promise of reduced launch expenses by eliminating recurring costs associated with hardware replacement inherent in expendable launch systems. However, the nonrecurring costs associated with design, development, research and engineering (DDR&E) of reusable SSTO systems are much higher than expendable systems due to the substantial technical challenges of SSTO.
It is considered to be marginally possible to launch a single stage to orbit spacecraft from Earth. The principal complicating factors for SSTO from Earth are: high orbital velocity of over 7400 m/s; the need to overcome the earth's gravity, especially in the early stages of flight; and flight within the Earth's atmosphere, which limits speed in the early stages of flight and influences engine performance. The marginality of SSTO can be seen in the launch of the space shuttle. The shuttle and main tank combination successfully orbits after booster separation from an altitude of 45 km and a speed of 4828 kph. This is approximately 12% of the gravitational potential energy and just 3% of the kinetic energy needed for orbital velocity (4% of total energy required).
Notable single stage to orbit research spacecraft include Skylon, the DC-X, the Lockheed Martin X-33, and the Roton SSTO. However, despite showing some promise, none of them has come close to achieving orbit yet due to problems with finding the most efficient propulsion system.
Single-stage-to-orbit has been achieved from the Moon by both the Apollo program's Lunar Module and several robotic spacecraft of the Soviet Luna program; the lower lunar gravity and absence of any significant atmosphere makes this much easier than from Earth.
Approaches.
There have been various approaches to SSTO, including pure rockets that are launched and land vertically, air-breathing scramjet-powered vehicles that are launched and land horizontally, nuclear-powered vehicles, and even jet-engine-powered vehicles that can fly into orbit and return landing like an airliner, completely intact.
For rocket-powered SSTO, the main challenge is achieving a high enough mass-ratio to carry sufficient propellant to achieve orbit, plus a meaningful payload weight. One possibility is to give the rocket an initial speed with a space gun, as planned in the Quicklaunch project.
For air-breathing SSTO, the main challenge is system complexity and associated research and development costs, material science, and construction techniques necessary for surviving sustained high-speed flight within the atmosphere, "and" achieving a high enough mass-ratio to carry sufficient propellant to achieve orbit, plus a meaningful payload weight. Air-breathing designs typically fly at supersonic or hypersonic speeds, and usually include a rocket engine for the final burn for orbit.
Whether rocket-powered or air-breathing, a reusable vehicle must be rugged enough to survive multiple round trips into space without adding excessive weight or maintenance. In addition a reusable vehicle must be able to reenter without damage, and land safely.
While single-stage rockets were once thought to be beyond reach, advances in materials technology and construction techniques have shown them to be possible. For example, calculations show that the Titan II first stage, launched on its own, would have a 25-to-1 ratio of fuel to vehicle hardware.
It has a sufficiently efficient engine to achieve orbit, but without carrying much payload.
Design challenges inherent in SSTO.
The design space constraints of SSTO vehicles were described by rocket design engineer Robert Truax:
Using similar technologies (i.e., the same propellants and structural fraction), a two-stage-to-orbit vehicle will always have a better payload-to-weight ratio than a single stage designed for the same mission, in most cases, a very much better [payload-to-weight ratio]. Only when the structural factor approaches zero [very little vehicle structure weight] does the payload/weight ratio of a single-stage rocket approach that of a two-stage. A slight miscalculation and the single-stage rocket winds up with no payload. To get any at all, technology needs to be stretched to the limit. Squeezing out the last drop of specific impulse, and shaving off the last pound, costs money and/or reduces reliability.
 
The Tsiolkovsky rocket equation expresses the maximum change in velocity any single rocket stage can achieve:
where:
The mass ratio of a vehicle is defined as a ratio the initial vehicle mass when fully loaded with propellants formula_7 to the final vehicle mass formula_8 after the burn:
where:
The propellant mass fraction (formula_16) of a vehicle can be expressed solely as a function of the mass ratio:
The structural coefficient (formula_18) is a critical parameter in SSTO vehicle design. Structural efficiency of a vehicle is maximized as the structural coefficient approaches zero. The structural coefficient is defined as:
The overall structural mass fraction formula_20 can be expressed in terms of the structural coefficient:
An additional expression for the overall structural mass fraction can be found by noting that the payload mass fraction formula_22, propellant mass fraction and structural mass fraction sum to one:
Equating the expressions for structural mass fraction and solving for the initial vehicle mass yields:
This expression shows how the size of a SSTO vehicle is dependent on its structural efficiency. Given a mission profile formula_26 and propellant type formula_27,the size of a vehicle increases with an increasing structural coefficient. This growth factor sensitivity is shown parametrically for both SSTO and two-stage-to-orbit (TSTO) vehicles for a standard LEO mission. The curves vertically asymptote at the maximum structural coefficient limit where mission criteria can no longer be met:
In comparison to a non-optimized TSTO vehicle using restricted staging, a SSTO rocket launching an identical payload mass and using the same propellants will always require a substantially smaller structural coefficient to achieve the same Delta-v. Given that current materials technology places a lower limit of approximately 0.1 on the smallest structural coefficients attainable, reusable SSTO vehicles are typically an impractical choice even when using the highest performance propellants available.
Dense versus hydrogen fuels.
Hydrogen might seem the obvious fuel for SSTO vehicles. When burned with oxygen, hydrogen gives the highest specific impulse of any commonly used fuel: around 450 seconds, compared with up to 350 seconds for kerosene.
Hydrogen has the following advantages:
However, hydrogen also has these disadvantages:
These issues can be dealt with, but at extra cost.
While kerosene tanks can be 1% of the weight of their contents, hydrogen tanks often must weigh 10% of their contents. This is because of both the low density and the additional insulation required to minimize boiloff (a problem which does not occur with kerosene and many other fuels). The low density of hydrogen further affects the design of the rest of the vehicle — pumps and pipework need to be much larger in order to pump the fuel to the engine. The end result is the thrust/weight ratio of hydrogen-fueled engines is 30–50% lower than comparable engines using denser fuels.
This inefficiency indirectly affects gravity losses as well; the vehicle has to hold itself up on rocket power until it reaches orbit. The lower excess thrust of the hydrogen engines due to the lower thrust/weight ratio means that the vehicle must ascend more steeply, and so less thrust acts horizontally. Less horizontal thrust results in taking longer to reach orbit, and gravity losses are increased by at least 300 m/s. While not appearing large, the mass ratio to delta-v curve is very steep to reach orbit in a single stage, and this makes a 10% difference to the mass ratio on top of the tankage and pump savings.
The overall effect is that there is surprisingly little difference in overall performance between SSTOs that use hydrogen and those that use denser fuels, except that hydrogen vehicles may be rather more expensive to develop and buy. Careful studies have shown that some dense fuels (for example liquid propane) exceed the performance of hydrogen fuel when used in an SSTO launch vehicle by 10% for the same dry weight.
In the 1960s Philip Bono investigated single-stage, VTVL tripropellant rockets, and showed that it could improve payload size by around 30%.
Operational experience with the DC/X experimental rocket has caused a number of SSTO advocates to reconsider hydrogen as a satisfactory fuel. The late Max Hunter, while employing hydrogen fuel in the DC/X, often said that he thought the first successful orbital SSTO would more likely be fueled by propane.
One engine for all altitudes.
Some SSTO vehicles use the same engine for all altitudes, which is a problem for traditional engines with a bell-shaped nozzle. Depending on the atmospheric pressure, different bell shapes are optimal. Engines operating in the lower atmosphere have shorter bells than those designed to work in vacuum. Having a bell not optimized for the height makes the engine less efficient.
One possible solution would be to use an aerospike engine, which can be effective in a wide range of ambient pressures. In fact, a linear aerospike engine was used in the X-33 design.
Other solutions involve using multiple engines and other altitude adapting designs such as double-mu bells or extensible bell sections.
Still, at very high altitudes, the extremely large engine bells tend to expand the exhaust gases down to near vacuum pressures. As a result, these engine bells are counterproductive due to their excess weight. Some SSTO vehicles simply use very high pressure engines which permit high ratios to be used from ground level. This gives good performance, negating the need for more complex solutions.
Airbreathing SSTO.
Some designs for SSTO attempt to use airbreathing jet engines that collect oxidizer and reaction mass from the atmosphere to reduce the take-off weight of the vehicle.
Some of the issues with this approach are:
Thus with for example scramjet designs (e.g. X-43) the mass budgets do not seem to close for orbital launch.
Similar issues occur with single-stage vehicles attempting to carry conventional jet engines to orbit- the weight of the jet engines is not compensated by the reduction in propellant sufficiently.
On the other hand LACE-like precooled airbreathing designs such as the Skylon spaceplane (and ATREX) which transition to rocket thrust at rather lower speeds (Mach 5.5) do seem to give, on paper at least, an improved orbital mass fraction over pure rockets (even multistage rockets) sufficiently to hold out the possibility of full reusability with better payload fraction.
It is important to note that mass fraction is an important concept in the engineering of a rocket. However, mass fraction may have little to do with the costs of a rocket, as the costs of fuel are very small when compared to the costs of the engineering program as a whole. As a result, a cheap rocket with a poor mass fraction may be able to deliver more payload to orbit with a given amount of money than a more complicated, more efficient rocket.
Launch assists.
Many vehicles are only narrowly suborbital, so practically anything that gives a relatively small delta-v increase can be helpful, and outside assistance for a vehicle is therefore desirable.
Proposed launch assists include:
And on-orbit resources such as:
Nuclear propulsion.
Due to weight issues such as shielding, many nuclear propulsion systems are unable to lift their own weight, and hence are unsuitable for launching to orbit. However some designs such as the Orion project and some nuclear thermal designs do have a thrust to weight ratio in excess of 1, enabling them to lift off. Clearly one of the main issues with nuclear propulsion would be safety, both during a launch for the passengers, but also in case of a failure during launch. No current program is attempting nuclear propulsion from Earth's surface.
Beam-powered propulsion.
Because they can be more energetic than the potential energy that chemical fuel allows for, some laser or microwave powered rocket concepts have the potential to launch vehicles into orbit, single stage. In practice, this area is relatively undeveloped, and current technology falls far short of this.
Comparison with the Shuttle.
The high cost per launch of the Space Shuttle sparked interest throughout the 1980s in designing a cheaper successor vehicle. Several official design studies were done, but most were basically smaller versions of the existing Shuttle concept.
Most cost analysis studies of the Space Shuttle have shown that workforce is by far the single greatest expense. Early shuttle discussions speculated airliner-type operation, with a two-week turnaround. However, senior NASA planners envisioned no more than 10 to 12 flights per year for the entire shuttle fleet. The absolute maximum flights per year for the entire fleet was limited by external tank manufacturing capacity to 24 per year.
Very efficient (hence complex and sophisticated) main engines were required to fit within the available vehicle space. Likewise the only known suitable lightweight thermal protection was delicate, maintenance-intensive silica tiles. These and other design decisions resulted in a vehicle that requires great maintenance after every mission. The engines are removed and inspected, and prior to the new "block II" main engines, the turbopumps were removed, disassembled and rebuilt. While Space Shuttle Atlantis was refurbished and relaunched in 53 days between missions STS-51-J and STS-61-B, generally months were required to repair an orbiter for a new mission.
Many in the aerospace community concluded that an entirely self-contained, reusable single-stage vehicle could solve these problems. The idea behind such a vehicle is to reduce the processing requirements from those of the Shuttle.
Examples.
It is easier to achieve SSTO from a body with lower gravitational pull than Earth, such as the Moon or Mars. The Apollo Lunar Module achieved deorbit to a soft landing, and return to lunar orbit, each with a single stage for descent and ascent.
A detailed study into SSTO vehicles was prepared by Chrysler Corporation's Space Division in 1970–1971 under NASA contract NAS8-26341. Their proposal (Shuttle SERV) was an enormous vehicle with more than 50000 kg of payload, utilizing jet engines for (vertical) landing. While the technical problems seemed to be solvable, the USAF required a winged design that led to the Shuttle as we know it today.
The unmanned DC-X technology demonstrator, originally developed by McDonnell Douglas for the Strategic Defense Initiative (SDI) program office, was an attempt to build a vehicle that could lead to an SSTO vehicle. The one-third-size test craft was operated and maintained by a small team of three people based out of a trailer, and the craft was once relaunched less than 24 hours after landing. Although the test program was not without mishap (including a minor explosion), the DC-X demonstrated that the maintenance aspects of the concept were sound. That project was cancelled when it crashed on the fourth flight after transferring management from the Strategic Defense Initiative Organization to NASA.
The Aquarius Launch Vehicle was designed to bring bulk materials to orbit as cheaply as possible.
Current development.
Current private SSTO projects include the Japanese Kankoh-maru project, the Skylon, and the Indian Avatar spaceplane.
Skylon.
The British Government partnered with the ESA in 2010 to promote a single-stage to orbit spaceplane concept called Skylon. This design was pioneered by Reaction Engines Limited (REL), a company founded by Alan Bond after HOTOL was canceled. The Skylon spaceplane has been positively received by the British government, and the British Interplanetary Society. Following a successful propulsion system test that was audited by ESA's propulsion division in mid-2012, REL announced that it would begin a three-and-a-half-year project to develop and build a test jig of the Sabre engine to prove the engines performance across its air-breathing and rocket modes. In November 2012, it was announced that a key test of the engine precooler had been successfully completed, and that ESA had verified the precooler's design. The project's development is now allowed to advance to its next phase, which involves the construction and testing of a full-scale prototype engine.
Haas 2C.
On 1 June 2012, Romanian organization ARCA announced that they are constructing an expendable rocket, named Haas 2C that will attempt to reach orbit in one stage.
The rocket has 520 kg empty weight and can carry 15.5 tons of fuel. It will use kerosene as fuel and liquid oxygen as oxidizer. In Spring 2012 they have successfully tested a lightweight composite kerosene fuel tank. The liquid oxygen tank is being designed and it will also be made of composite materials. The launch is expected to take place in Spring 2013.
Alternative approaches to inexpensive spaceflight.
Many studies have shown that regardless of selected technology, the most effective cost reduction technique is economies of scale. Merely launching a large total quantity reduces the manufacturing costs per vehicle, similar to how the mass production of automobiles brought about great increases in affordability.
Using this concept, some aerospace analysts believe the way to lower launch costs is the exact opposite of SSTO. Whereas reusable SSTOs would reduce per launch costs by making a reusable high-tech vehicle that launches frequently with low maintenance, the "mass production" approach views the technical advances as a source of the cost problem in the first place. By simply building and launching large quantities of rockets, and hence launching a large volume of payload, costs can be brought down. This approach was attempted in the late 1970s, early 1980s in West Germany with the Democratic Republic of the Congo-based OTRAG rocket.
A related idea is to obtain economies of scale from building simple, massive, multi-stage rockets using cheap, off-the-shelf parts. The vehicles would be dumped into the ocean after use. This strategy is known as the "big dumb booster" approach.
This is somewhat similar to the approach some previous systems have taken, using simple engine systems with "low-tech" fuels, as the Russian and Chinese space programs still do. These nations' launches are significantly cheaper than their Western counterparts.
An alternative to scale is to make the discarded stages practically reusable: this is the goal of the SpaceX reusable launch system development program and its Grasshopper demonstrator.

</doc>
<doc id="29400" url="http://en.wikipedia.org/wiki?curid=29400" title="Structural biology">
Structural biology

Structural biology is a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules, especially proteins and nucleic acids, how they acquire the structures they have, and how alterations in their structures affect their function. This subject is of great interest to biologists because macromolecules carry out most of the functions of cells, and because it is only by coiling into specific three-dimensional shapes that they are able to perform these functions. This architecture, the "tertiary structure" of molecules, depends in a complicated way on the molecules' basic composition, or "primary structures." 
Biomolecules are too small to see in detail even with the most advanced light microscopes. The methods that structural biologists use to determine their structures generally involve measurements on vast numbers of identical molecules at the same time. These methods include:
Most often researchers use them to study the "native states" of macromolecules. But variations on these methods are also used to watch nascent or denatured molecules assume or reassume their native states. See protein folding. 
A third approach that structural biologists take to understanding structure is bioinformatics to look for patterns among the diverse sequences that give rise to particular shapes. Researchers often can deduce aspects of the structure of integral membrane proteins based on the membrane topology predicted by hydrophobicity analysis. See protein structure prediction. 
In the past few years it has become possible for highly accurate physical molecular models to complement the "in silico" study of biological structures. Examples of these models can be found in the PDB.

</doc>
<doc id="29402" url="http://en.wikipedia.org/wiki?curid=29402" title="Sunni Islam">
Sunni Islam

Sunni Islam ( or ) is the largest branch of Islam; s of 2009[ [update]] Sunni Muslims constituted 87-90% of the world's Muslim population. Its adherents are referred to in Arabic as "ahl as-sunnah wa l-jamāʻah" (Arabic: أهل السنة والجماعة‎), "people of the tradition of Muhammad and the consensus of the Ummah" or "ahl as-sunnah" (أهل السنة) for short. In English, its theological study or doctrine is called "Sunnism", while adherents are known as Sunni Muslims, Sunnis, and Sunnites. Sunni Islam is the world's second largest religious body (after Christianity) and the largest religious denomination for any religion in the world. Sunni Islam is sometimes referred to as the orthodox version of the religion. The word "Sunni" is believed to come from the term "Sunnah" (Arabic: سنة‎), which refers to the sayings and actions of the Islamic prophet Muhammad as recorded in the "hadith".
The primary collections consisting of Kutub al-Sittah accepted by Sunni orthodoxy, in conjunction with the Quran and binding consensus, form the basis of all jurisprudence within Sunni Islam. Laws are derived from these basic sources; in addition, Sunni Islam's juristic schools recognize differing methods to derive legal verdicts such as analogical reason, consideration of public welfare and juristic discretion.
Lexicology.
Sunnī (Classical Arabic: سُنِّي /ˈsunniː/) also commonly referred to as Sunnīism is a broad term derived from "sunnah" (سُنَّة /ˈsunna/, plural سُنَن "sunan" /ˈsunan/) meaning "habit", "usual practice", "custom", "tradition". The Muslim use of this term refers to the sayings and living habits of the prophet Muhammad. In its full form, this branch of Islam is referred to as "Ahl al-Sunnah wal Jamaah" (literally, "People of the "Sunnah" and the Community"). People claiming to follow the Sunnah (tradition of the prophet) who can demonstrate that they have no action or belief against the prophetic Sunnah can consider themselves to be Sunni Muslims. One who espouses political Sunnite beliefs or specialises in Sunnism is sometimes called a Sunnist. Some regions that are heavily Sunni-populated have been coined with derived neological terms such as "Sunnistan" or "Sunni belt".
History.
After the death of Prophet Muhammad, Muslims who accepted Abu Bakr as the first Caliph became known as "Ahl al-Sunnah wa al-Jama'ah" or "the people of tradition and unification" in order to differentiate them from the Shi'a, who rejected Abu Bakr's authority in favor of Ali, whom Sunnis accepted as the fourth Caliph rather than the first.
The first four caliphs are known among Sunnis as the Rashidun or "Rightly-Guided Ones". Sunni recognition includes the aforementioned Abu Bakr as the first, Umar as the second, Uthman as the third, and Ali as the fourth.
After the first four caliphs, the Caliphate was upheld as a political system by dynasties such as the Abbasids, the Ottomans, and the Mughal Empire of South Asia. It was also upheld for relatively short periods of time by other competing dynasties in Spain, North Africa and Egypt.
Mustafa Kemal Atatürk abolished the system of the Ottoman caliphate after Abdülmecid II was deposed and expelled from what was once the Ottoman Empire, whereby the Republic of Turkey was founded in 1923 upon secular principles.
Adherents.
Sunnis believe that the companions of Muhammad were the best of Muslims. This belief is based upon prophetic traditions such as one narrated by Abdullah, son of Masud, in which Muhammad said: "The best of the people are my generation, then those who come after them, then those who come after them." Support for this view is also found in the Quran, according to Sunnis. Sunnis also believe that the companions were true believers since it was the companions who were given the task of compiling the Quran. Furthermore, narrations that were narrated by the companions (ahadith) are considered by Sunnis to be a second source of knowledge of the Muslim faith. A study conducted by the "Pew Research Center" in 2010 and released January 2011 found that there are 1.62 billion Muslims around the world, and it is estimated over 75–90% are Sunni.
Organizational structure.
Islam does not have a formal hierarchy or clergy. Leaders are informal, and gain influence through study to become a scholar of Islamic law, called "sharia". According to the Islamic Center of Columbia, South Carolina, anyone with the intelligence and will can become an Islamic scholar. During Midday Mosque services on Fridays, the congregation will choose a well educated person to lead the service, known as an imam (one who leads).
Schools of law.
There are several intellectual traditions within the field of Islamic law, often referred to as legal schools. These varied traditions reflect differing viewpoints on some laws and obligations within Islamic law. While one school may see a certain act as a religious obligation, another may see the same act as optional. Historically, the schools were often engaged in violent conflict with one another, though today these schools aren't regarded as sects; rather, they represent differing viewpoints on issues that are not considered the core of Islamic belief.
Historians have differed regarding the exact delineation of the schools based on the underlying principles they follow. Many traditional scholars saw Sunni Islam in two groups: Ahl al-Ra'i, or "people of reason," due to their emphasis on scholarly judgment and discourse; and Ahl al-Hadith, or "people of traditions," due to their emphasis on restricting juristic thought to only what is found in scripture. Ibn Khaldun defined the Sunni schools as three: the Hanafi school representing reason, the Ẓāhirīte school representing tradition, and a broader, middle school encompassing the Shafi'ite, Malikite and Hanbalite schools.
During the Middle Ages, the Mamluk Sultanate in Egypt delineated the acceptable Sunni schools as only Hanafi, Maliki, Shafi'i and Hanbali, excluding the Ẓāhirī school. The Ottoman Empire later reaffirmed the official status of four schools as a reaction to the Shiite character of their ideological and political arch rival, the Persian Safavids, though former Prime Minister of Sudan Al-Sadiq al-Mahdi, as well as the Amman Message issued by King Abdullah II of Jordan, recognize the Ẓāhirī and keep the number of Sunni schools at five.
Differences in the schools.
Interpreting Islamic law by deriving specific rulings – such as how to pray – is commonly known as Islamic jurisprudence. The schools of law all have their own particular tradition of interpreting this jurisprudence. As these schools represent clearly spelled out methodologies for interpreting Islamic law, there has been little change in the methodology with regard to each school. While conflict between the schools was often violent in the past, today the schools recognize one another as viable legal methods rather than sources of error or heresy in contrast to one another. Each school has its evidences, and differences of opinion are generally respected.
The six pillars of "iman".
Sunni Islam has six articles of faith known as the six pillars of "iman" that all Sunni Muslims are united upon in belief, along with the 105 key points of creed mentioned in "Aṭ-Ṭaḥāwī's Islamic Theology".
Theological traditions.
Some Islamic scholars faced questions that they felt were not explicitly answered in the "Quran" and the "Sunnah", especially questions with regard to philosophical conundra such as the nature of God, the existence of human free will, or the eternal existence of the "Quran." Various schools of theology and philosophy developed to answer these questions, each claiming to be true to the "Quran" and the Muslim tradition ("sunnah"). Among Sunni Muslims, various schools of thought in theology began to be born out of the sciences of kalam in opposition to the textualists who stood by affirming texts without delving into philosophical speculation as they saw it as an innovation in Islam. The following were the three dominant schools of theology that grew. All three of these are accepted by Muslims around the globe, and are considered within "Islamic orthodoxy". The key beliefs of Sunni Islam are all agreed upon (being the six pillars of Iman) and codified in the treatise on Aqeedah by Imam Ahmad ibn Muhammad al-Tahawi in his Aqeedat Tahawiyyah.
Maturidi.
Founded by Abu Mansur al-Maturidi (died 944). Maturidiyyah was a minority tradition until it was accepted by the Turkish tribes of Central Asia (previously they had been Ash'ari and followers of the Shafi'i school, it was only later on migration into Anatolia that they became Hanafi and followers of the Maturidi creed.) One of the tribes, the Seljuk Turks, migrated to Turkey, where later the Ottoman Empire was established. Their preferred school of law achieved a new prominence throughout their whole empire although it continued to be followed almost exclusively by followers of the Hanafi school while followers of the Shafi and Maliki schools within the empire followed the Ash'ari and Athari schools of thought. Thus, wherever can be found Hanafi followers, there can be found the Maturidi creed.["discuss"]
Ash'ari.
Founded by Abu al-Hasan al-Ash'ari (873–935). This theological school of Aqeedah was embraced by many Muslim scholars and developed in parts of the Islamic world throughout history; the Imam al-Ghazali wrote on the creed discussing it and agreeing upon some of its principles.
Ash'ari theology stresses divine revelation over human reason. Contrary to the Mu'tazilites, they say that ethics cannot be derived from human reason, but that God's commands, as revealed in the "Quran" and the "Sunnah" (the practices of Muhammad and his companions as recorded in the traditions, or hadith), are the sole source of all morality and ethics.
Regarding the nature of God and the divine attributes, the Ash'ari rejected the Mu'tazili position that all Quranic references to God as having real attributes were metaphorical. The Ash'aris insisted that these attributes were as they "best befit His Majesty". The Arabic language is a wide language in which one word can have 15 different meanings, so the Ash'aris endeavor to find the meaning that best befits God and is not contradicted by the Quran. Therefore when God states in the Quran, "He who does not resemble any of His creation," this clearly means that God cannot be attributed with body parts because He created body parts. Ash'aris tend to stress divine omnipotence over human free will and they believe that the Quran is eternal and uncreated.
Athari.
Athari (Classical Arabic: أثري), or "textualism", is derived from the Arabic word "athar", literally meaning "remnant", and also referring to "narrations". Their disciples are called the Atharis or "al-Atharia". The Atharis are considered to be one of three Sunni schools of Aqidah.
The Athari methodology of textual or literal interpretation is to avoid delving into any extensive theological speculation. They believe in God and his attributes in the exact fashion that they were mentioned in the Quran, the Sunnah, and by the Sahabah. They do not attempt to further interpret the aforementioned texts by giving an altered meaning like the Tashbih (simile or likening), nor through tahrif (distortion), nor ta`weel (allegory or metaphor), nor ta'teel (denial). They avoid entering into deep rational philosophical discussions of matters relating to Islamic beliefs that are not supported by the Quran, the Sunnah or the understanding of the Sahabah with specific wording; rather, their discussion and presentation of beliefs revolves entirely around textual evidences found in these three main sources, while remaining cautious to avoid taking the path of non-Atharis either. The Atharis believe this to be the methodology adhered to by the first three generations of Muslims (i.e. the Salaf), therefore making it the school of Sunni Aqeedah that they believe is adhering to the truth and keeping to the balanced middle path of Islam.
The codifier of the Athari Aqeedah was the great classical Islamic scholar Ahmad ibn Hanbal who today is perhaps better known for his School of Jurisprudence than his school of Aqeedah. Prominent proponents of Classical Atharism today include, among others, Yusuf al-Qaradawi and Mauritanian scholar Sheikh Muhammad Al-Hassan Ad-Dedew.
Sunni view of "hadith".
The Quran as it exists today in book form was compiled by Muhammad's companions ("Sahabah") within a handful of months of his death, and is accepted by all sects of Islam. However, there were many matters of belief and daily life that were not directly prescribed in the Quran, but were actions that were observed by Muhammad and the early Muslim community. Later generations sought out oral traditions regarding the early history of Islam, and the practices of Muhammad and his first followers, and wrote them down so that they might be preserved. These recorded oral traditions are called hadith. Muslim scholars have through the ages sifted through the hadith and evaluated the chain of narrations of each tradition, scrutinizing the trustworthiness of the narrators and judging the strength of each hadith accordingly.
"Kutub al-Sittah".
"Kutub al-Sittah" are six books containing collections of hadiths. Sunni Muslims accept the hadith collections of Bukhari and Muslim as the most authentic ("sahih," or correct), and while accepting all hadiths verified as authentic, grant a slightly lesser status to the collections of other recorders. There are, however, four other collections of hadith that are also held in particular reverence by Sunni Muslims, making a total of six:
There are also other collections of hadith which also contain many authentic hadith and are frequently used by scholars and specialists. Examples of these collections include:

</doc>
<doc id="29403" url="http://en.wikipedia.org/wiki?curid=29403" title="Sour mix">
Sour mix

Sour mix (also known as sweet and sour mix) is a mixer that is yellow-green in color and is used in many cocktails. It is made from approximately equal parts lemon and/or lime juice and simple syrup and shaken vigorously with ice. This produces a pearly-white liquid with a pronounced flavor.
Sour mix can be mixed with liquor(s) to make a sour drink; most common are vodka sour (vodka) and whiskey sour (whiskey).
Pre-mixed versions are available, and are in use in many bars. These typically consist of a powder which must be rehydrated by adding water prior to use.

</doc>
<doc id="29405" url="http://en.wikipedia.org/wiki?curid=29405" title="Sikh">
Sikh

A Sikh (; Punjabi: ਸਿੱਖ "sikkh" ]) is a follower of Sikhism, a monotheistic religion which originated during the 15th century in the Punjab region of South Asia. The term "Sikh" has its origin in the Sanskrit words शिष्य ("śiṣya"; disciple, student) or शिक्ष ("śikṣa"; instruction). A Sikh, according to Article I of the "Sikh Rehat Maryada" (the Sikh code of conduct), is "any human being who faithfully believes in One Immortal Being; ten Gurus, from Guru Nanak to Guru Gobind Singh; Guru Granth Sahib; the teachings of the ten Gurus and the baptism bequeathed by the tenth Guru". "Sikh" properly refers to adherents of Sikhism as a religion, not an ethnic group. However, because Sikhism has seldom sought converts, most Sikhs share strong ethno-religious ties. Many countries, such as the U.K., therefore recognize Sikh as a designated ethnicity on their censuses. The American non-profit organization United Sikhs has fought to have Sikh included on the U.S. census as well, arguing that Sikhs "self-identify as an 'ethnic minority'" and believe "that they are more than just a religion".
Male Sikhs usually have "Singh" (Lion), and female Sikhs have "Kaur" (prince) as their middle or last name. Sikhs who have undergone the "khanḍe-kī-pahul" (the Sikh initiation ceremony) may also be recognised by the five Ks: Kesh uncut hair which is kept covered, usually by a turban to protect the Dasam Duwar ("god head"); an iron or steel bracelet ("kara"); a "kirpan" (a sword tucked into a "gatra" strap or a "kamal kasar" belt); "kachehra", a cotton undergarment, and "kanga", a small wooden comb. Initiated male Sikhs must cover their hair with a turban. The greater Punjab region is the historic homeland of the Sikhs, although significant communities exist around the world.
History.
Guru Nanak (1469–1538), founder of Sikhism, was born to Mehta Kalu and Mata Tripta, in a Hindu family in the village of Talwandi, now called Nankana Sahib, near Lahore. Guru Nanak was a religious leader and social reformer. However, Sikh political history may be said to begin with the death of the fifth Sikh guru, Guru Arjan Dev, in 1606. Religious practices were formalised by Guru Gobind Singh on 30 March 1699. Singh initiated five people from a variety of social backgrounds, known as the "Panj Piare" (the five beloved ones) to form the Khalsa, or collective body of initiated Sikhs. The Sikh faith has generally had amicable relations with other religions, except for the period of Mughal rule in India (1556–1707). Several Sikh gurus were killed by the Mughals for opposing their persecution of minority religious communities including Sikhs. Sikhs subsequently militarised to oppose Mughal rule.
The emergence of the Sikh Confederacy under Ranjit Singh was characterised by religious tolerance and pluralism, with Christians, Muslims and Hindus in positions of power. The confederacy is considered the zenith of political Sikhism, encompassing Kashmir, Ladakh and Peshawar. Hari Singh Nalwa, the commander-in-chief of the Sikh army in the North West Frontier, expanded the confederacy to the Khyber Pass. Its secular administration implemented military, economic and governmental reforms.
After the annexation of the Sikh kingdom by the British, the latter recognized the Martial qualities of the Sikhs and Punjabis in general and started recruiting from that area. During the 1857 Indian mutiny, the Sikhs stayed loyal to the British. This resulted in heavy recruiting from Punjab to the colonial army for the next 90 years of the British Raj. The distinct turban that differentiates a Sikh from other turban wearers is a relic of the rules of the British Indian Army.
The months leading up to the partition of India in 1947 were marked by conflict in the Punjab between Sikhs and Muslims. This caused the religious migration of Punjabi Sikhs and Hindus from West Punjab, mirroring a similar religious migration of Punjabi Muslims from East Punjab.
The 1960s saw growing animosity between Sikhs and Hindus in India, with the Sikhs demanding the creation of a Punjab state on a linguistic basis similar to other states in India. This was promised to Sikh leader Master Tara Singh by Jawaharlal Nehru, in return for Sikh political support during negotiations for Indian independence. Although the Sikhs obtained the Punjab, they lost Hindi-speaking areas to Himachal Pradesh, Haryana and Rajasthan. Chandigarh was made a union territory and the capital of Haryana and Punjab on 1 November 1966.
Tensions arose again during the late 1970s, fueled by Sikh claims of discrimination and marginalisation by the Hindu-dominated Indian National Congress party and tactics adopted by the Prime Minister Indira Gandhi. According to Katherine Frank, Indira Gandhi's assumption of emergency powers in 1975 resulted in the weakening of the "legitimate and impartial machinery of government", and her increasing "paranoia" about opposing political groups led her to institute a "despotic policy of playing castes, religions and political groups against each other for political advantage". Sikh leader Jarnail Singh Bhindranwale articulated Sikh demands for justice, and this triggered violence in the Punjab. The prime minister's 1984 defeat of Bhindranwale led to an attack on the Golden Temple in Operation Blue Star and to her assassination by her Sikh bodyguards. Gandhi's assassination resulted in an explosion of violence against Sikh communities and the killing of thousands of Sikhs throughout India. Khushwant Singh described the riots as a Sikh pogrom; he "felt like a refugee in my country. In fact, I felt like a Jew in Nazi Germany". Since 1984, relations between Sikhs and Hindus have moved toward a rapprochement aided by economic prosperity. However, a 2002 claim by the Hindu right-wing Rashtriya Swayamsevak Sangh (RSS) that "Sikhs are Hindus" disturbed Sikh sensibilities. The Khalistan movement campaigns for justice for the victims of the violence, and for the political and economic needs of the Punjab.
During the 1999 Vaisakhi, Sikhs worldwide celebrated the 300th anniversary of the creation of the Khalsa. Canada Post honoured Sikh Canadians with a commemorative stamp in conjunction with the 300th anniversary of Vaisakhi. On April 9, 1999, Indian president K.R. Narayanan issued a stamp commemorating the 300th anniversary of the Khalsa.
Sikh Culture and Religious Observations.
Five Ks.
The five Ks ("panj kakaar") are five articles of faith which all baptized Sikhs (Amritdhari Sikhs) are obliged to wear. The symbols represent the ideals of Sikhism: honesty, equality, fidelity, meditating on God and never bowing to tyranny.
The five symbols are:
Music and instruments.
The Sikhs have a number of musical instruments: the rabab, dilruba, taus, jori and saranda. Playing the sarangi was encouraged in "Guru Har Gobind". The rabab was first played by Bhai Mardana as he accompanied Guru Nanak on his journeys. The jori and saranda were designed by Guru Arjan. The taus was made by Guru Gobind Singh Ji, who supposedly heard a peacock singing and wanted to create an instrument mimicking its sounds ("taus" is the Persian word for peacock). The dilruba was made by Guru Gobind Singh at the request of his followers, who wanted a smaller instrument than the taus. After "Japji Sahib", all of the shabad in the "Guru Granth Sahib" were composed as raags. This type of singing is known as Gurmat Sangeet.
When they marched into battle, the Sikhs would play a "Ranjit Nagara" (victory drum) to boost morale. Nagaras (usually two to three feet in diameter, although some were up to five feet in diameter) are played with two sticks. The beat of the large drums, and the raising of the Nishan Sahib, meant that the "singhs" were on their way.
Demographics.
Numbering about 27 million worldwide, Sikhs make up 0.39 percent of the world population; approximately 83 percent live in India. About 76 percent of all Sikhs live in the north Indian State of Punjab, where they form a majority (about two-thirds) of the population. Substantial communities of Sikhs (more than 200,000) live in the Indian states or union territories of Haryana (more than 1.1 million), Rajasthan, West Bengal, Uttar Pradesh, Delhi, Maharashtra, Uttarakhand, Madhya Pradesh, Assam and Jammu and Kashmir.
Sikh migration from British India began in earnest during the second half of the 19th century, when the British completed their annexation of the Punjab. The British Raj recruited Sikhs for the Indian Civil Service (particularly the British Indian Army), which led to Sikh migration throughout India and the British Empire. During the Raj, semiskilled Sikh artisans were transported from the Punjab to British East Africa to help build railroads. Sikhs emigrated from India and Pakistan after World War II, most going to the United Kingdom but many to North America. Some Sikhs who had settled in eastern Africa were expelled by Ugandan dictator Idi Amin in 1972. Economics is a major factor in Sikh migration, and significant communities exist in the United Kingdom, Canada, the United States, Malaysia, East Africa, Australia and Thailand.
Although the rate of Sikh migration from the Punjab has remained high, traditional patterns of Sikh migration favouring English-speaking countries (particularly the United Kingdom) have changed during the past decade due to stricter immigration laws. Moliner (2006) wrote that as a consequence of Sikh migration to the UK "becom[ing] virtually impossible since the late 1970s", migration patterns evolved to continental Europe. Italy is a rapidly growing destination for Sikh migration, with Reggio Emilia and Vicenza having significant Sikh population clusters. Italian Sikhs are generally involved in agriculture, agricultural processing, the manufacture of machine tools and horticulture.
Primarily for socio-economic reasons, Indian Sikhs have the lowest adjusted growth rate of any major religious group in India, at 16.9 percent per decade (estimated from 1991 to 2001). Johnson and Barrett (2004) estimate that the global Sikh population increases annually by 392,633 (1.7 percent per year, based on 2004 figures); this percentage includes births, deaths and conversions.
Sikh castes.
Sikh Gurus criticised the hierarchy of the caste system, however, one does exist in the Sikh community. Over 60% of Sikhs belong to the Jat caste, which is a rural caste. Despite being very small in numbers, the mercantile Khatri and Arora castes wield considerable influence within the Sikh community. Other Sikhs castes include the Ramgarhias (artisans), the Ahluwalias (formerly Kalals (brewers)) and the two Dalit castes, known in Sikh terminology as the Mazhabis and the Ramdasias .
According to Sunrinder S, Jodhka, the Sikh religion does not advocate discrimination against any caste or creed, however, in practice, Sikhs belonging to the landowning dominant castes have not shed all their prejudices against the dalit castes. While dalits would be allowed entry into the village gurudwaras they would not be permitted to cook or serve langar (Communal meal). Therefore, wherever they could mobilise resources, the Sikh dalits of Punjab have tried to construct their own gurudwara and other local level institutions in order to attain a certain degree of cultural autonomy.
In 1953, the government of India acceded to the demands of the Sikh leader, Master Tara Singh, to include Sikh castes of the converted untouchables in the list of scheduled castes.
In the Shiromani Gurdwara Prabandhak Committee, 20 of the 140 seats are reserved for low-caste Sikhs.
Representation.
Sikhs have been represented in Indian politics by former Indian prime minister Manmohan Singh and the deputy chairman of the Indian Planning Commission, Montek Singh Ahluwalia. Punjab Chief Minister Parkash Singh Badal is also a Sikh. Past Sikh politicians in India include former president Giani Zail Singh, Sardar Swaran Singh (India's first foreign minister), Speaker of Parliament Gurdial Singh Dhillon and former Chief Minister of Punjab Pratap Singh Kairon.
Politicians from the Sikh diaspora include the first Asian American member of the United States Congress, Dalip Singh Saund, British MPs Piara Khabra, Parmjit Dhanda and Paul Uppal, the first couple to sit together in a Commonwealth parliament (Gurmant Grewal and Nina Grewal, who requested a Canadian government apology for the Komagata Maru incident), former Canadian Shadow Social Development Minister Ruby Dhalla, Canadian Minister of State for Sport Baljit Singh Gosal and Legislative Assembly of Ontario members Vic Dhillon and Jagmeet Singh. Ujjal Dosanjh was the New Democratic Party Premier of British Columbia from July 2004 to February 2005, and was later a Liberal frontbench MP in Ottawa. In Malaysia, two Sikhs were elected MPs in the 2008 general elections: Karpal Singh (Bukit Gelugor) and his son, Gobind Singh Deo (Puchong). Two Sikhs were elected assemblymen: Jagdeep Singh Deo (Datuk Keramat) and Keshvinder Singh (Malim Nawar).
According to a 1994 estimate, Punjabis (Sikhs and non-Sikhs) comprised 10 to 15 percent of all ranks in the Indian Army, although the state contained less than 3% of the country's population. The Indian government does not release religious or ethnic origins of the military personnel, but a 1991 report by Tim McGirk estimated that 20 percent of Indian Army officers were Sikhs. Apart from the Gurkhas recruited from Nepal, the Sikhs remain the only community to have exclusive regiments in the Indian Army. The Sikh Regiment is one of the most-decorated regiments in the army, with 73 Battle Honours, 14 Victoria Crosses, 21 first-class Indian Orders of Merit (equivalent to the Victoria Cross), 15 Theatre Honours, five COAS Unit Citations, two Param Vir Chakras, 14 Maha Vir Chakras, five Kirti Chakras, 67 Vir Chakras and 1,596 other awards. The highest-ranking general in the history of the Indian Air Force is a Punjabi Sikh, Marshal of the Air Force Arjan Singh. Plans by the United Kingdom Ministry of Defence for a Sikh infantry regiment were scrapped in June 2007.
Historically, most Indians have been farmers and 66 percent of the Indian population are engaged in agriculture. Indian Sikhs are employed in agriculture to a lesser extent; India's 2001 census found 39 percent of the working population of the Punjab employed in this sector. The success of the 1960s Green Revolution, in which India went from "famine to plenty, from humiliation to dignity", was based in the Punjab (which became known as "the breadbasket of India"). The Punjab is the wealthiest Indian state per capita, with the average Punjabi income three times the national average. The Green Revolution centred on Indian farmers adopting more intensive and mechanised agricultural methods, aided by the electrification of the Punjab, cooperative credit, consolidation of small holdings and the existing, British Raj-developed canal system. According to Swedish political scientist Ishtiaq Ahmad, a factor in the success of the Indian green revolution was the "Sikh cultivator, often the Jat, whose courage, perseverance, spirit of enterprise and muscle prowess proved crucial". However, not all aspects of the green revolution were beneficial. Indian physicist Vandana Shiva wrote that the green revolution made the "negative and destructive impacts of science [i.e. the green revolution] on nature and society" invisible, and was a catalyst for Punjabi Sikh and Hindu tensions despite a growth in material wealth.
Punjabi Sikhs are engaged in a number of professions which include science, engineering and medicine. Notable examples are nuclear scientist Piara Singh Gill (who worked on the Manhattan Project), fibre-optics pioneer Narinder Singh Kapany and physicist, science writer and broadcaster Simon Singh.
In business, the UK-based clothing retailers New Look and the Thai-based Jaspal were founded by Sikhs. India's largest pharmaceutical company, Ranbaxy Laboratories, is headed by Sikhs. UK Sikhs have the highest percentage of home ownership (82 percent) of any religious community. UK Sikhs are the second-wealthiest (after the Jewish community) religious group in the UK, with a median total household wealth of £229,000.
In Singapore Kartar Singh Thakral expanded his family's trading business, Thakral Holdings, into total assets of almost $1.4 billion and is Singapore's 25th-richest person. Sikh Bob Singh Dhillon is the first Indo-Canadian billionaire. The Sikh diaspora has been most successful in North America, especially in California’s fertile Central Valley. American Sikh farmers such as Harbhajan Singh Samra and Didar Singh Bains dominate California agriculture, with Samra specialising in okra and Bains in peaches.
Sikh intellectuals, sportsmen and artists include writer Khushwant Singh, England cricketer Monty Panesar, former 400m runner Milkha Singh, Indian wrestler and actor Dara Singh, former Indian hockey team captains Ajitpal Singh and Balbir Singh Sr., former Indian cricket captain Bishen Singh Bedi, Harbhajan Singh (India's most successful off spin cricket bowler), Navjot Singh Sidhu (former Indian cricketer turned politician). Bollywood actresses include Neetu Singh, Poonam Dhillon, Mahi Gill, Esha Deol, Parminder Nagra, Gul Panag, Mona Singh, Sunny Leone, Namrata Singh Gujral and director Gurinder Chadha.
Sikhs have migrated worldwide, with a variety of occupations. The Sikh Gurus preached ethnic and social harmony, and Sikhs comprise a number of ethnic groups. Those with over 1,000 members include the Ahluwalia, Arain, Arora, Bhatra, Bairagi, Bania, Basith, Bawaria, Bazigar, Bhabra, Chamar, Chhimba, Darzi, Dhobi, Gujar, Jatt, Jhinwar, Kahar, Kalal, Kamboj, Khatri, Kumhar, Labana, Lohar, Mahtam, Mazhabi, Megh, Mirasi, Mochi, Mohyal, Nai, Rajput, Ramgarhia, Saini, Sudh, Tarkhan.
An order of Punjabi Sikhs, the Nihang or the Akalis, was formed during Ranjit Singh's time. Under their leader, Akali Phula Singh, they won many battles for the Sikh Confederacy during the early 19th century.
In the Indian and British armies.
Sikhs supported the British during the Indian Rebellion of 1857. By the beginning of World War I, Sikhs in the British Indian Army totaled over 100,000 (20 percent of the force). Until 1945 fourteen Victoria Crosses were awarded to Sikhs, a per-capita regimental record. In 2002 the names of all Sikh VC and George Cross recipients were inscribed on the monument of the Memorial Gates on Constitution Hill, next to Buckingham Palace. Chanan Singh Dhillon was instrumental in campaigning for the memorial.
During World War I, Sikh battalions fought in Egypt, Palestine, Mesopotamia, Gallipoli and France. Six battalions of the Sikh Regiment were raised during World War II, serving in the Second Battle of El Alamein, the Burma and Italian campaigns and in Iraq and receiving 27 battle honours. Around the world, Sikhs are commemorated in Commonwealth cemeteries.
In the last two world wars 83,005 turban wearing Sikh soldiers were killed and 109,045 were wounded fighting for the British Empire. During shell fire, they had no other head protection but the turban, the symbol of their faith.
—General Sir Frank Messervy
British people are highly indebted and obliged to Sikhs for a long time. I know that within this century we needed their help twice [in two world wars] and they did help us very well. As a result of their timely help, we are today able to live with honour, dignity, and independence. In the war, they fought and died for us, wearing the turbans.—Sir Winston Churchill
Sikh Diaspora.
During the late 19th and early 20th centuries, Sikhs began to emigrate to East Africa, the Far East, Canada, the United States and the United Kingdom. In 1907 the Khalsa Diwan Society was established in Vancouver, and four years later the first gurdwara was established in London. In 1912 the first gurdwara in the United States was founded in Stockton, California.
Since Sikhs (like Middle Eastern men) wear turbans, some in Western countries have been mistaken for Muslim or Arabic men since the September 11 attacks and the Iraq War. Several days after the 9/11 attacks Sikh Balbir Singh Sodhi was murdered by Frank Roque, who thought Sodhi was connected with al-Qaeda. CNN suggested an increase in hate crimes against Sikh men in the United States and the UK after the 9/11 attacks.
Since Sikhism has never actively sought converts, the Sikhs have remained a relatively homogeneous ethnic group. The Kundalini Yoga-based activities of Harbhajan Singh Yogi in his 3HO (Happy, Healthy, Holy) organisation claim to have inspired a moderate growth in non-Indian adherents of Sikhism. In 1998 an estimated 7,800 3HO Sikhs, known colloquially as ‘gora’ (ਗੋਰਾ) or ‘white’ Sikhs, were mainly centred around Española, New Mexico and Los Angeles, California. Sikhs and the Sikh American Legal Defense and Education Fund overturned a 1925 Oregon law banning the wearing of turbans by teachers and government officials.
In an attempt to foster Sikh leaders in the Western world, youth initiatives by a number of organisations have begun. The Sikh Youth Alliance of North America sponsors an annual Sikh Youth Symposium, a public-speaking and debate competition held in gurdwaras throughout the U.S. and Canada. There are a number of sikhs office holders in Canada. In the United States, the current governor of South Carolina, Nikki Haley was born and raised as a Sikh but has converted to Christianity after her marriage.
Art and culture.
Sikh art and culture are nearly synonymous with that of the Punjab, and Sikhs are easily recognised by their distinctive turban (Dastar). The Punjab has been called India’s melting pot, due to the confluence of invading cultures from the rivers from which the region gets its name. Sikh culture is therefore a synthesis of cultures. Sikhism has forged a unique architecture, which S. S. Bhatti described as "inspired by Guru Nanak’s creative mysticism" and "is a mute harbinger of holistic humanism based on pragmatic spirituality".
During the Mughal and Afghan persecution of the Sikhs during the 17th and 18th centuries, the latter were concerned with preserving their religion and gave little thought to art and culture. With the rise of Ranjit Singh and the Sikh Raj in Lahore and Delhi, there was a change in the landscape of art and culture in the Punjab; Hindus and Sikhs could build decorated shrines without the fear of destruction or looting.
The Sikh Confederacy was the catalyst for a uniquely Sikh form of expression, with Ranjit Singh commissioning forts, palaces, bungas (residential places) and colleges in a Sikh style. Sikh architecture is characterised by gilded fluted domes, cupolas, kiosks, stone lanterns, ornate balusters and square roofs. A pinnacle of Sikh style is Harmandir Sahib (also known as the Golden Temple) in Amritsar.
Sikh culture is influenced by militaristic motifs (with the Khanda the most obvious), and most Sikh artifacts—except for the relics of the Gurus—have a military theme. This theme is evident in the Sikh festivals of Hola Mohalla and Vaisakhi, which feature marching and displays of valor.
Although the art and culture of the Sikh diaspora have merged with that of other Indo-immigrant groups into categories like "British Asian", "Indo-Canadian" and "Desi-Culture", a minor cultural phenomenon which can be described as "political Sikh" has arisen. The art of diaspora Sikhs like Amarjeet Kaur Nandhra and Amrit and Rabindra Kaur Singh (the "Singh Twins") is influenced by their Sikhism and current affairs in the Punjab.
Bhangra and Giddha are two forms of Punjabi folk dancing which have been adapted and pioneered by Sikhs. Punjabi Sikhs have championed these forms of expression worldwide, resulting in Sikh culture becoming linked to Bhangra (although "Bhangra is not a Sikh institution but a Punjabi one").
Painting.
Sikh painting is a direct offshoot of the Kangra school of painting. In 1810, Ranjeet Singh (1780–1839) occupied Kangra Fort and appointed Sardar Desa Singh Majithia his governor of the Punjab hills. In 1813 the Sikh army occupied Guler State, and Raja Bhup Singh became a vassal of the Sikhs. With the Sikh kingdom of Lahore becoming the paramount power, some of the Pahari painters from Guler migrated to Lahore for the patronage of Maharaja Ranjeet Singh and his Sardars.
The Sikh school adapted Kangra painting to Sikh needs and ideals. Its main subjects are the ten Sikh gurus and stories from Guru Nanak's Janamsakhis. The tenth Guru, Gobind Singh, left a deep impression on the followers of the new faith because of his courage and sacrifices. Hunting scenes and portraits are also common in Sikh painting.

</doc>
<doc id="29407" url="http://en.wikipedia.org/wiki?curid=29407" title="Superworld">
Superworld

Superworld is a superhero-themed role-playing game published by Chaosium in 1983. Written by "Basic Role-Playing" and "RuneQuest" author Steve Perrin, "Superworld" began as one third of the "Worlds of Wonder" product, which also included a generic fantasy setting, "Magic World", and a generic science fiction setting, "Future World", all using the same core "Basic Role-Playing" rules. Only "Superworld" became a game in its own right.
Game system.
"Superworld" is based on the traditional Chaosium "Basic Role-Playing" system, here augmented by super-powers. 
Seven characteristics (Strength, Constitution, Size, Intelligence, Power, Dexterity, Appearance) are rolled with dice (2D6+6, rather the 3d6 used for many other "Basic Role-Playing" games.) The sum of these characteristics gives a total of Hero Points used to buy super powers. 
The super powers system follows the "Champions" model of powers that are described by their effects. For example, one does not buy "Laser Vision", but the effect "Energy Blast", and specifies that it is a laser emitted by the hero's eyes. Each effect can be modified by Advantages (less energy expenditure, for example) or Disadvantages (reduced number of uses, for example) which increase or reduce the cost of a power. 
Hero Points can also be used to buy skills, or increase characteristics, for a super-strong character, for example. It is possible to get more Hero Points for character creation by choosing Disabilities for the character, such as Public Identity, Vulnerability to a Substance, Psychological Problems, etc. More Hero Points would be awarded for experience at the end of a game session.
The system functions in the same way as the other "Basic Role-Playing" games, by rolling percentile dice against skills. Lower rolls than needed can cause increased effect from Specials (equivalent to Impales in "RuneQuest"), or Criticals, and high rolls can cause critical failures (Fumbles). Combat rules have many options and take into account three types of energy for damage: Kinetic, Electric, and Radiation.
Game materials.
The game box contains three rules booklets, a booklet of character sheets, one of tables for the Gamemaster, a page of cardboard figure silhouettes to be cut out, and a set 6, 8, and 20-sided dice. 1984 printings also contains a 4 page errata booklet.
Supplements.
"Bad Medicine for Dr. Drugs".
(1984) Scenario. Author: Ken Rolston. Set in a high school, and designed for teenage characters. It comes with six young pregenerated heroes, or lets players use their own. Beginning with the funeral of one of their friends, it sets the heroes on the track of a drug distribution network in their school, directed by the aforementioned Dr. Drugs.
It also includes rules for the creation and management of adolescent characters that have just discovered their powers, and a plan of Warren G. Harding High School, though the scenario recommends substituting the school in which the GM and the players studied.
"Superworld Companion".
(1985) Rules supplement. 
Many authors: Stephen R. Marsh, Stephen Perrin, Ian Lee Starcher, Anthony Affronti, Jimmy Akin II, William A Barton, Norman Doege, Bruce Dresselhaus, Ray Greer, Zoran Kovacich, George MacDonald, Steve Maurer, Sandy Petersen, Wayne Shaw, John Sullivan—most are listed because they provided one or more optional rules. 
Includes:
"Trouble for HAVOC".
(1984) Scenario / Campaign. 
Authors: Stephen Perrin, Yurek Chodak, Donald Harrington, Charles Huber.
A linked collection of three scenarios based on the members of the criminal organization HAVOC. All the characters are presented with characteristics for with three different systems, "Superworld", "Champions" and "Villains & Vigilantes". Each may be played separately, or as part of a campaign.
"Wild Cards".
The "Wild Cards" series of science fiction books came from an Albuquerque, New Mexico campaign gamemastered by George R. R. Martin, and played in by other science fiction writers.

</doc>
<doc id="29408" url="http://en.wikipedia.org/wiki?curid=29408" title="Samuel Taylor Coleridge">
Samuel Taylor Coleridge

Samuel Taylor Coleridge (; 21 October 1772 – 25 July 1834) was an English poet, literary critic and philosopher who, with his friend William Wordsworth, was a founder of the Romantic Movement in England and a member of the Lake Poets. He wrote the poems "The Rime of the Ancient Mariner" and "Kubla Khan", as well as the major prose work "Biographia Literaria". His critical work, especially on Shakespeare, was highly influential, and he helped introduce German idealist philosophy to English-speaking culture. Coleridge coined many familiar words and phrases, including suspension of disbelief. He was a major influence on Emerson, and American transcendentalism.
Throughout his adult life, Coleridge suffered from crippling bouts of anxiety and depression; it has been speculated that he suffered from bipolar disorder, a condition not identified during his lifetime. He also suffered from poor physical health that may have stemmed from a bout of rheumatic fever and other childhood illnesses. He was treated for these concerns with laudanum, which fostered a lifelong opium addiction.
Early life.
Coleridge was born on 21 October 1772 in the country town of Ottery St Mary, Devonshire, England. Samuel's father, the Reverend John Coleridge (1718–1781), was a well-respected vicar of the parish and headmaster of Henry VIII's Free Grammar School at Ottery. He had three children by his first wife. Samuel was the youngest of ten by Reverend Coleridge's second wife, Anne Bowden (1726–1809). Coleridge suggests that he "took no pleasure in boyish sports" but instead read "incessantly" and played by himself. After John Coleridge died in 1781, 8-year-old Samuel was sent to Christ's Hospital, a charity school which was founded in the 16th century in Greyfriars, London, where he remained throughout his childhood, studying and writing poetry. At that school Coleridge became friends with Charles Lamb, a schoolmate, and studied the works of Virgil and William Lisle Bowles.
In one of a series of autobiographical letters written to Thomas Poole, Coleridge wrote: "At six years old I remember to have read "Belisarius", "Robinson Crusoe", and "Philip Quarll" – and then I found the "Arabian Nights' Entertainments" – one tale of which (the tale of a man who was compelled to seek for a pure virgin) made so deep an impression on me (I had read it in the evening while my mother was mending stockings) that I was haunted by spectres whenever I was in the dark – and I distinctly remember the anxious and fearful eagerness with which I used to watch the window in which the books lay – and whenever the sun lay upon them, I would seize it, carry it by the wall, and bask, and read."
However, Coleridge seems to have appreciated his teacher, as he wrote in recollections of his schooldays in "Biographia Literaria": 
I enjoyed the inestimable advantage of a very sensible, though at the same time, a very severe master [...] At the same time that we were studying the Greek Tragic Poets, he made us read Shakespeare and Milton as lessons: and they were the lessons too, which required most time and trouble to bring up, so as to escape his censure. I learnt from him, that Poetry, even that of the loftiest, and, seemingly, that of the wildest odes, had a logic of its own, as severe as that of science; and more difficult, because more subtle, more complex, and dependent on more, and more fugitive causes. [...] In our own English compositions (at least for the last three years of our school education) he showed no mercy to phrase, metaphor, or image, unsupported by a sound sense, or where the same sense might have been conveyed with equal force and dignity in plainer words... In fancy I can almost hear him now, exclaiming "Harp? Harp? Lyre? Pen and ink, boy, you mean! Muse, boy, Muse? your Nurse's daughter, you mean! Pierian spring? Oh aye! the cloister-pump, I suppose!" [...] Be this as it may, there was one custom of our master's, which I cannot pass over in silence, because I think it ... worthy of imitation. He would often permit our theme exercises, ... to accumulate, till each lad had four or five to be looked over. Then placing the whole number abreast on his desk, he would ask the writer, why this or that sentence might not have found as appropriate a place under this or that other thesis: and if no satisfying answer could be returned, and two faults of the same kind were found in one exercise, the irrevocable verdict followed, the exercise was torn up, and another on the same subject to be produced, in addition to the tasks of the day. 
Throughout his life, Coleridge idealised his father as pious and innocent, while his relationship with his mother was more problematic. His childhood was characterised by attention seeking, which has been linked to his dependent personality as an adult. He was rarely allowed to return home during the school term, and this distance from his family at such a turbulent time proved emotionally damaging. He later wrote of his loneliness at school in the poem "Frost at Midnight":
"With unclosed lids, already had I dreamt/Of my sweet birthplace."
From 1791 until 1794, Coleridge attended Jesus College, Cambridge. In 1792, he won the Browne Gold Medal for an ode that he wrote on the slave trade. In December 1793, he left the college and enlisted in the Royal Dragoons using the false name "Silas Tomkyn Comberbache", perhaps because of debt or because the girl that he loved, Mary Evans, had rejected him. Afterwards, he was rumoured to have had a bout of severe depression. His brothers arranged for his discharge a few months later under the reason of "insanity" and he was readmitted to Jesus College, though he would never receive a degree from Cambridge.
Pantisocracy and marriage.
Cambridge and Somerset.
At the university, he was introduced to political and theological ideas then considered radical, including those of the poet Robert Southey. Coleridge joined Southey in a plan, soon abandoned, to found a utopian commune-like society, called Pantisocracy, in the wilderness of Pennsylvania. In 1795, the two friends married sisters Sarah and Edith Fricker, in St Mary Redcliffe, Bristol, but Coleridge's marriage with Sarah proved unhappy. He grew to detest his wife, whom he only married because of social constraints. He eventually separated from her. Coleridge made plans to establish a journal, "The Watchman", to be printed every eight days to avoid a weekly newspaper tax. The first issue of the short-lived journal was published in March 1796; it had ceased publication by May of that year.
The years 1797 and 1798, during which he lived in what is now known as Coleridge Cottage, in Nether Stowey, Somerset, were among the most fruitful of Coleridge's life. In 1795, Coleridge met poet William Wordsworth and his sister Dorothy. (Wordsworth, having visited him and being enchanted by the surroundings, rented Alfoxton Park, a little over three miles [5 km] away.) Besides the "Rime of The Ancient Mariner", he composed the symbolic poem "Kubla Khan", written—Coleridge himself claimed—as a result of an opium dream, in "a kind of a reverie"; and the first part of the narrative poem "Christabel". The writing of "Kubla Khan", written about the Mongol emperor Kublai Khan and his legendary palace at Xanadu, was said to have been interrupted by the arrival of a "Person from Porlock" – an event that has been embellished upon in such varied contexts as science fiction and Nabokov's "Lolita". During this period, he also produced his much-praised "conversation" poems "This Lime-Tree Bower My Prison", "Frost at Midnight", and "".
In 1798, Coleridge and Wordsworth published a joint volume of poetry, "Lyrical Ballads", which proved to be the starting point for the English romantic age. Wordsworth may have contributed more poems, but the real star of the collection was Coleridge's first version of "The Rime of the Ancient Mariner". It was the longest work and drew more praise and attention than anything else in the volume. In the spring Coleridge temporarily took over for Rev. Joshua Toulmin at Taunton's Mary Street Unitarian Chapel while Rev. Toulmin grieved over the drowning death of his daughter Jane. Poetically commenting on Toulmin's strength, Coleridge wrote in a 1798 letter to John Prior Estlin, "I walked into Taunton (eleven miles) and back again, and performed the divine services for Dr. Toulmin. I suppose you must have heard that his daughter, (Jane, on 15 April 1798) in a melancholy derangement, suffered herself to be swallowed up by the tide on the sea-coast between Sidmouth and Bere ["sic"] (Beer). These events cut cruelly into the hearts of old men: but the good Dr. Toulmin bears it like the true practical Christian, – there is indeed a tear in his eye, but that eye is lifted up to the Heavenly Father."
The West Midlands and the North.
Coleridge also worked briefly in Shropshire, where he came in December 1797 as locum to its local Unitarian minister, Dr Rowe, in their church in the High Street at Shrewsbury. He is said to have read his "Rime of the Ancient Mariner" at a literary evening in Mardol. He was then contemplating a career in the ministry, and gave a probationary sermon in High Street church on Sunday, 14 January 1798. William Hazlitt, a Unitarian minister's son, was in the congregation, having walked from Wem to hear him. Coleridge later visited Hazlitt and his father at Wem but within a day or two of preaching he received a letter from Josiah Wedgwood II, who had offered to help him out of financial difficulties with an annuity of £150 (approximately £13,000 in today's money) per year on condition he give up his ministerial career. Coleridge accepted this, to the disappointment of Hazlitt who hoped to have him as a neighbour in Shropshire.
In the autumn of 1798, Coleridge and Wordsworth left for a stay in Germany; Coleridge soon went his own way and spent much of his time in university towns. During this period, he became interested in German philosophy, especially the transcendental idealism and critical philosophy of Immanuel Kant, and in the literary criticism of the 18th century dramatist Gotthold Lessing. Coleridge studied German and, after his return to England, translated the dramatic trilogy "Wallenstein" by the German Classical poet Friedrich Schiller into English. He continued to pioneer these ideas through his own critical writings for the rest of his life (sometimes without attribution), although they were unfamiliar and difficult for a culture dominated by empiricism.
In 1799, Coleridge and Wordsworth stayed at Thomas Hutchinson's farm on the River Tees at Sockburn, near Darlington.
It was at Sockburn that Coleridge wrote his ballad-poem "Love", addressed to Sara Hutchinson. The knight mentioned is the mailed figure on the Conyers tomb in ruined Sockburn church. The figure has a wyvern at his feet, a reference to the Sockburn Worm slain by Sir John Conyers (and a possible source for Lewis Carroll's "Jabberwocky"). The worm was supposedly buried under the rock in the nearby pasture; this was the 'greystone' of Coleridge's first draft, later transformed into a 'mount'. The poem was a direct inspiration for John Keats' famous poem "La Belle Dame Sans Merci".
Coleridge's early intellectual debts, besides German idealists like Kant and critics like Lessing, were first to William Godwin's "Political Justice", especially during his Pantisocratic period, and to David Hartley's "Observations on Man", which is the source of the psychology which is found in "Frost at Midnight". Hartley argued that one becomes aware of sensory events as impressions, and that "ideas" are derived by noticing similarities and differences between impressions and then by naming them. Connections resulting from the coincidence of impressions create linkages, so that the occurrence of one impression triggers those links and calls up the memory of those ideas with which it is associated (See Dorothy Emmet, "Coleridge and Philosophy").
Coleridge was critical of the literary taste of his contemporaries, and a literary conservative insofar as he was afraid that the lack of taste in the ever growing masses of literate people would mean a continued desecration of literature itself.
In 1800, he returned to England and shortly thereafter settled with his family and friends at Keswick in the Lake District of Cumberland to be near Grasmere, where Wordsworth had moved. Soon, however, he was beset by marital problems, nightmares, illnesses, increased opium dependency, tensions with Wordsworth, and a lack of confidence in his poetic powers, all of which fuelled the composition of "Dejection: An Ode" and an intensification of his philosophical studies. He also sprinkled cayenne pepper over his eggs, which he ate from a teacup.
In 1802, Coleridge took a nine-day walking holiday in the fells of the Lake District. Coleridge is credited with the first recorded decent of Scafell to Mickledore via Broad Stand, although this was more due to his getting lost than a keenness for mountaineering.
Later life and increasing drug use.
Travel and "The Friend".
In 1804, he travelled to Sicily and Malta, working for a time as Acting Public Secretary of Malta under the Commissioner, Alexander Ball, a task he performed quite successfully. He lived in St Antons' Palace in the village of Attard. However, he gave this up and returned to England in 1806. Dorothy Wordsworth was shocked at his condition upon his return. From 1807 to 1808, Coleridge returned to Malta and then travelled in Sicily and Italy, in the hope that leaving Britain's damp climate would improve his health and thus enable him to reduce his consumption of opium. Thomas de Quincey alleges in his "Recollections of the Lakes and the Lake Poets" that it was during this period that Coleridge became a full-blown opium addict, using the drug as a substitute for the lost vigour and creativity of his youth. It has been suggested, however, that this reflects de Quincey's own experiences more than Coleridge's.
His opium addiction (he was using as much as two quarts of laudanum a week) now began to take over his life: he separated from his wife Sarah in 1808, quarrelled with Wordsworth in 1810, lost part of his annuity in 1811, and put himself under the care of Dr. Daniel in 1814. His addiction caused severe constipation, which required regular and humiliating enemas.
In 1809, Coleridge made his second attempt to become a newspaper publisher with the publication of the journal entitled "The Friend". It was a weekly publication that, in Coleridge’s typically ambitious style, was written, edited, and published almost entirely single-handedly. Given that Coleridge tended to be highly disorganised and had no head for business, the publication was probably doomed from the start. Coleridge financed the journal by selling over five hundred subscriptions, over two dozen of which were sold to members of Parliament, but in late 1809, publication was crippled by a financial crisis and Coleridge was obliged to approach "Conversation Sharp", Tom Poole and one or two other wealthy friends for an emergency loan to continue. "The Friend" was an eclectic publication that drew upon every corner of Coleridge's remarkably diverse knowledge of law, philosophy, morals, politics, history, and literary criticism. Although it was often turgid, rambling, and inaccessible to most readers, it ran for 25 issues and was republished in book form a number of times. Years after its initial publication, "The Friend" became a highly influential work and its effect was felt on writers and philosophers from J.S. Mill to Emerson.
London: final years and death.
Between 1810 and 1820, Coleridge gave a series of lectures in London and Bristol – those on Shakespeare renewed interest in the playwright as a model for contemporary writers. Much of Coleridge's reputation as a literary critic is founded on the lectures that he undertook in the winter of 1810–11, which were sponsored by the Philosophical Institution and given at Scot's Corporation Hall off Fetter Lane, Fleet Street. These lectures were heralded in the prospectus as "A Course of Lectures on Shakespeare and Milton, in Illustration of the Principles of Poetry." Coleridge's ill-health, opium-addiction problems, and somewhat unstable personality meant that all his lectures were plagued with problems of delays and a general irregularity of quality from one lecture to the next. As a result of these factors, Coleridge often failed to prepare anything but the loosest set of notes for his lectures and regularly entered into extremely long digressions which his audiences found difficult to follow. However, it was the lecture on "Hamlet" given on 2 January 1812 that was considered the best and has influenced "Hamlet" studies ever since. Before Coleridge, "Hamlet" was often denigrated and belittled by critics from Voltaire to Dr. Johnson. Coleridge rescued the play's reputation, and his thoughts on it are often still published as supplements to the text.
In August 1814, Coleridge was approached by Lord Byron's publisher, John Murray, about the possibility of translating Goethe's classic "Faust" (1808). Coleridge was regarded by many as the greatest living writer on the demonic and he accepted the commission, only to abandon work on it after six weeks. Until recently, scholars were in agreement that Coleridge never returned to the project, despite Goethe's own belief in the 1820s that he had in fact completed a long translation of the work. In September 2007, Oxford University Press sparked a heated scholarly controversy by publishing an English translation of Goethe's work that purported to be Coleridge's long-lost masterpiece (the text in question first appeared anonymously in 1821).
In 1817, Coleridge, with his addiction worsening, his spirits depressed, and his family alienated, took residence in the Highgate homes, then just north of London, of the physician James Gillman, first at South Grove and later at the nearby 3 The Grove. Gillman was partially successful in controlling the poet's addiction. Coleridge remained in Highgate for the rest of his life, and the house became a place of literary pilgrimage for writers including Carlyle and Emerson.
In Gillman's home, Coleridge finished his major prose work, the "Biographia Literaria" (1817), a volume composed of 23 chapters of autobiographical notes and dissertations on various subjects, including some incisive literary theory and criticism. He composed much poetry here and had many inspirations – a few of them from opium overdose. Perhaps because he conceived such grand projects, he had difficulty carrying them through to completion, and he berated himself for his "indolence". It is unclear whether his growing use of opium (and the brandy in which it was dissolved) was a symptom or a cause of his growing depression.
He published other writings while he was living at the Gillman homes, notably "Sibylline Leaves" (1817), "Hush" (1820), "Aids to Reflection" (1825), and "On the Constitution of the Church and State" (1830). He also published essays, such as, "Essay on Faith" (1838) and "Confessions Of An Inquiring Spirit" (1840). He died in Highgate, London on 25 July 1834 as a result of heart failure compounded by an unknown lung disorder, possibly linked to his use of opium. Coleridge had spent 18 years under the roof of the Gillman family, who built an addition onto their home to accommodate the poet.Faith may be defined as fidelity to our own being, so far as such being is not and cannot become an object of the senses; and hence, by clear inference or implication to being generally, as far as the same is not the object of the senses; and again to whatever is affirmed or understood as the condition, or concomitant, or consequence of the same. This will be best explained by an instance or example. That I am conscious of something within me peremptorily commanding me to do unto others as I would they should do unto me; in other words a categorical (that is, primary and unconditional) imperative; that the maxim ("regula maxima", or supreme rule) of my actions, both inward and outward, should be such as I could, without any contradiction arising therefrom, will to be the law of all moral and rational beings. "Essay On Faith"
Carlyle described him at Highgate: "Coleridge sat on the brow of Highgate Hill, in those years, looking down on London and its smoke-tumult, like a sage escaped from the inanity of life's battle ... The practical intellects of the world did not much heed him, or carelessly reckoned him a metaphysical dreamer: but to the rising spirits of the young generation he had this dusky sublime character; and sat there as a kind of "Magus", girt in mystery and enigma; his Dodona oak-grove (Mr. Gilman's house at Highgate) whispering strange things, uncertain whether oracles or jargon." 
Poetry.
Coleridge is one of the most important figures in English poetry. His poems directly and deeply influenced all the major poets of the age. He was known by his contemporaries as a meticulous craftsman who was more rigorous in his careful reworking of his poems than any other poet, and Southey and Wordsworth were dependent on his professional advice. His influence on Wordsworth is particularly important because many critics have credited Coleridge with the very idea of "Conversational Poetry". The idea of utilising common, everyday language to express profound poetic images and ideas for which Wordsworth became so famous may have originated almost entirely in Coleridge’s mind. It is difficult to imagine Wordsworth’s great poems, "The Excursion" or "The Prelude", ever having been written without the direct influence of Coleridge’s originality.
As important as Coleridge was to poetry as a poet, he was equally important to poetry as a critic. His philosophy of poetry, which he developed over many years, has been deeply influential in the field of literary criticism. This influence can be seen in such critics as A.O. Lovejoy and I.A. Richards.
"The Rime of the Ancient Mariner", "Christabel", and "Kubla Khan".
Coleridge is probably best known for his long poems, "The Rime of the Ancient Mariner" and "Christabel". Even those who have never read the "Rime" have come under its influence: its words have given the English language the metaphor of an albatross around one's neck, the quotation of "water, water everywhere, nor any drop to drink" (almost always rendered as "but not a drop to drink"), and the phrase "a sadder and a wiser man" (again, usually rendered as "a sadder but wiser man"). The phrase "All creatures great and small" may have been inspired by "The Rime": "He prayeth best, who loveth best;/ All things both great and small;/ For the dear God who loveth us;/ He made and loveth all." "Christabel" is known for its musical rhythm, language, and its Gothic tale.
"Kubla Khan", or, "A Vision in a Dream, A Fragment", although shorter, is also widely known. Both "Kubla Khan" and "Christabel" have an additional "Romantic" aura because they were never finished. Stopford Brooke characterised both poems as having no rival due to their "exquisite metrical movement" and "imaginative phrasing."
The Conversation poems.
The eight of Coleridge's poems listed above are now often discussed as a group entitled "Conversation poems". The term itself was coined in 1928 by George McLean Harper, who borrowed the subtitle of "The Nightingale: A Conversation Poem" (1798) to describe the seven other poems as well. The poems are considered by many critics to be among Coleridge's finest verses; thus Harold Bloom has written, "With "Dejection", "The Ancient Mariner", and "Kubla Khan", "Frost at Midnight" shows Coleridge at his most impressive." They are also among his most influential poems, as discussed further below.
Harper himself considered that the eight poems represented a form of blank verse that is "...more fluent and easy than Milton's, or any that had been written since Milton". In 2006 Robert Koelzer wrote about another aspect of this apparent "easiness", noting that Conversation poems such as "... Coleridge's "The Eolian Harp" and "The Nightingale" maintain a middle register of speech, employing an idiomatic language that is capable of being construed as un-symbolic and un-musical: language that lets itself be taken as 'merely talk' rather than rapturous 'song'."
The last ten lines of "Frost at Midnight" were chosen by Harper as the "best example of the peculiar kind of blank verse Coleridge had evolved, as natural-seeming as prose, but as exquisitely artistic as the most complicated sonnet." The speaker of the poem is addressing his infant son, asleep by his side:
In 1965, M. H. Abrams wrote a broad description that applies to the Conversation poems: "The speaker begins with a description of the landscape; an aspect or change of aspect in the landscape evokes a varied by integral process of memory, thought, anticipation, and feeling which remains closely intervolved with the outer scene. In the course of this meditation the lyric speaker achieves an insight, faces up to a tragic loss, comes to a moral decision, or resolves an emotional problem. Often the poem rounds itself to end where it began, at the outer scene, but with an altered mood and deepened understanding which is the result of the intervening meditation." In fact, Abrams was describing both the Conversation poems and later poems influenced by them. Abrams' essay has been called a "touchstone of literary criticism". As Paul Magnuson described it in 2002, "Abrams credited Coleridge with originating what Abrams called the 'greater Romantic lyric', a genre that began with Coleridge's 'Conversation' poems, and included Wordsworth's "Tintern Abbey", Shelley's "Stanzas Written in Dejection" and Keats's "Ode to a Nightingale", and was a major influence on more modern lyrics by Matthew Arnold, Walt Whitman, Wallace Stevens, and W. H. Auden."
Literary criticism.
Biographia Literaria.
In addition to his poetry, Coleridge also wrote influential pieces of literary criticism including "Biographia Literaria", a collection of his thoughts and opinions on literature which he published in 1817. The work delivered both biographical explanations of the author's life as well as his impressions on literature. The collection also contained an analysis of a broad range of philosophical principles of literature ranging from Aristotle to Immanuel Kant and Schelling and applied them to the poetry of peers such as William Wordsworth. Coleridge's explanation of metaphysical principles were popular topics of discourse in academic communities throughout the 19th and 20th centuries, and T.S. Eliot stated that he believed that Coleridge was "perhaps the greatest of English critics, and in a sense the last." Eliot suggests that Coleridge displayed "natural abilities" far greater than his contemporaries, dissecting literature and applying philosophical principles of metaphysics in a way that brought the subject of his criticisms away from the text and into a world of logical analysis that mixed logical analysis and emotion. However, Eliot also criticises Coleridge for allowing his emotion to play a role in the metaphysical process, believing that critics should not have emotions that are not provoked by the work being studied. Hugh Kenner in "Historical Fictions", discusses Norman Fruman's "Coleridge, the Damaged Archangel" and suggests that the term "criticism" is too often applied to "Biographia Literaria", which both he and Fruman describe as having failed to explain or help the reader understand works of art. To Kenner, Coleridge's attempt to discuss complex philosophical concepts without describing the rational process behind them displays a lack of critical thinking that makes the volume more of a biography than a work of criticism.
In "Biographia Literaria" and his poetry, symbols are not merely "objective correlatives" to Coleridge, but instruments for making the universe and personal experience intelligible and spiritually covalent. To Coleridge, the "cinque spotted spider," making its way upstream "by fits and starts," [Biographia Literaria] is not merely a comment on the intermittent nature of creativity, imagination, or spiritual progress, but the journey and destination of his life. The spider's five legs represent the central problem that Coleridge lived to resolve, the conflict between Aristotelian logic and Christian philosophy. Two legs of the spider represent the "me-not me" of thesis and antithesis, the idea that a thing cannot be itself and its opposite simultaneously, the basis of the clockwork Newtonian world view that Coleridge rejected. The remaining three legs—exothesis, mesothesis and synthesis or the Holy trinity—represent the idea that things can diverge without being contradictory. Taken together, the five legs—with synthesis in the center, form the Holy Cross of Ramist logic. The cinque-spotted spider is Coleridge's emblem of holism, the quest and substance of Coleridge's thought and spiritual life.
Coleridge and the influence of the Gothic.
Coleridge wrote reviews of Ann Radcliffe's books and "The Mad Monk", among others. He comments in his reviews: "Situations of torment, and images of naked horror, are easily conceived; and a writer in whose works they abound, deserves our gratitude almost equally with him who should drag us by way of sport through a military hospital, or force us to sit at the dissecting-table of a natural philosopher. To trace the nice boundaries, beyond which terror and sympathy are deserted by the pleasurable emotions, – to reach those limits, yet never to pass them, hic labor, hic opus est." and "The horrible and the preternatural have usually seized on the popular taste, at the rise and decline of literature. Most powerful stimulants, they can never be required except by the torpor of an unawakened, or the languor of an exhausted, appetite... We trust, however, that satiety will banish what good sense should have prevented; and that, wearied with fiends, incomprehensible characters, with shrieks, murders, and subterraneous dungeons, the public will learn, by the multitude of the manufacturers, with how little expense of thought or imagination this species of composition is manufactured."
However, Coleridge used these elements in poems such as "The Rime of the Ancient Mariner" (1798), "Christabel" and "Kubla Khan" (published in 1816, but known in manuscript form before then) and certainly influenced other poets and writers of the time. Poems like these both drew inspiration from and helped to inflame the craze for Gothic romance. Coleridge also made considerable use of Gothic elements in his commercially successful play "Remorse".
Mary Shelley, who knew Coleridge well, mentions "The Rime of the Ancient Mariner" twice directly in "Frankenstein", and some of the descriptions in the novel echo it indirectly. Although William Godwin, her father, disagreed with Coleridge on some important issues, he respected his opinions and Coleridge often visited the Godwins. Mary Shelley later recalled hiding behind the sofa and hearing his voice chanting "The Rime of the Ancient Mariner".
Further reading.
 incorporates text from a publication now in the public domain: 
Collected Works.
A current standard edition is"The Collected Works of Samuel Taylor Coleridge," edited by Kathleen Coburn and many other editors (1969-2002), which appeared (from Princeton University Press and Routledge and Kegan Paul) in Bollingen Series 75, in 16 volumes, broken down as follows into further volumes and parts, to a total of 34 separate printed volumes: 1. "Lectures 1795 on Politics and Religion" (1971); 2. "The Watchman" (1970); 3. "Essays on his Times in the Morning Post and the Courier" (1978) in 3 vols; 4. "The Friend" (1969) in 2 vols; 5. "Lectures, 1808-1819, on Literature" (1987) in 2 vols; 6. "Lay Sermons" (1972); 7. "Biographia Literaria" (1983) in 2 vols; 8. "Lectures 1818-1819 on the History of Philosophy" (2000) in 2 vols; 9. "Aids to Reflection" (1993); 10. "On the Constitution of the Church and State" (1976); 11. "Shorter Works and Fragments" (1995) in 2 vols; 12. "Marginalia" (1980 and following) in 6 vols; 13. "Logic" (1981); 14. "Table Talk" (1990) in 2 vols; 15. "Opus Maximum" (2002); 16. "Poetical Works" (2001) in 6 vols (part 1 Reading Edition in 2 vols; part 2 Variorum Text in 2 vols; part 3 Plays in 2 vols).
External links.
 #if: 
 #if: A Short Biographical Dictionary of English Literature
 |, 
 #if: 
 }}{{
 #if: 
 #if: 
 | ()
 |{{
 #if: 
 #if: 
 | {{#if:||}}{{
 #if: 
 #if: 
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 
 #if: A Short Biographical Dictionary of English Literature
 |{{
 #if: 
 }} {{Citation/make link
 | 1={{
 #if: 
 #if: 
 #if: 
 |{{
 #if: 
 | 2=" A Short Biographical Dictionary of English Literature
 #if:| []
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 }}{{
 #if: 
 | ( ed.)
 #if: 
 }}{{
 #if: 
 #if: 
 |,
 #if: 
 |{{
 #if: 1910
 |, 1910{{
 #if:
}}{{
 #if: 
 #ifeq: | 1910
 |{{
 #if: 
 #if: 
 | (published )
 |{{
 #if: 
 | (published )
}}{{
 #if: 
 |{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |, {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
 #if: 
}}{{
 #if:
 | , {{#ifeq: | no
 | {{#if:
 |{{Citation/make link||{{#ifeq:|.|A|a}}rchived}} from the original
 |{{#ifeq:|.|A|a}}rchived
 | {{#ifeq:|.|A|a}}rchived{{#if:
 }}{{#if:| on }}{{
 |. {{citation error|nocat=
 #if: A Short Biographical Dictionary of English Literature
 |, {{
 #if: 
 |
 |, {{
 #if: 
 |
 }}{{
 #if: 
 | {{#ifeq:|,|, r|. R}}etrieved 
}}{{#if:
}}{{#if:
}}{{#if:
}}<span
 class="Z3988"
 title="ctx_ver=Z39.88-2004&rft_val_fmt={{urlencode:info:ofi/fmt:kev:mtx:}}{{
 #if: 
 |journal&rft.genre=article&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |book{{
 #if: 
 |&rft.genre=bookitem&rft.btitle={{urlencode:}}&rft.atitle={{urlencode: A Short Biographical Dictionary of English Literature
 |&rft.genre=book&rft.btitle={{urlencode: A Short Biographical Dictionary of English Literature
 #if: |&rft.aulast={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: |&rft.au={{urlencode:}}{{
 }}{{
 #if: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 |&rft.pages={{urlencode: {{
 #if: A Short Biographical Dictionary of English Literature
 #if: 
 #if: Coleridge, Samuel Taylor
 #switch: 
 #if: 
 #if: 
 |{{
 #switch: 
 #if: 
 #if: 
 }}{{
 }}&rfr_id=info:sid/en.wikipedia.org:{{FULLPAGENAMEE}}"> 
 |IncludedWorkTitle =  
 |IncludedWorkURL = 
 |Other = 
 |Edition = 
 |Place = 
 |PublicationPlace = 
 |Publisher = Wikisource
 |PublicationDate = 
 |EditorSurname1 = 
 |EditorSurname2 = 
 |EditorSurname3 = 
 |EditorSurname4 = 
 |EditorGiven1 = 
 |EditorGiven2=
 |EditorGiven3=
 |EditorGiven4=
 |Editorlink1=
 |Editorlink2=
 |Editorlink3=
 |Editorlink4=
 |language = 
 |format = 
 |ARXIV=
 |ASIN=
 |BIBCODE=
 |DOI=
 |DoiBroken=
 |ISBN=
 |ISSN=
 |JFM=
 |JSTOR=
 |LCCN=
 |MR=
 |OCLC=
 |OL=
 |OSTI=
 |PMC=
 |Embargo=1010-10-10
 |PMID=
 |RFC=
 |SSRN=
 |ZBL=
 |ID=
 |AccessDate=
 |DateFormat=none
 |quote = 
 |laysummary = 
 |laydate = 
 |Ref=
 |Sep = .
 |PS = 
 |AuthorSep = ; 
 |NameSep = , 
 |Trunc = 8
 |amp = 

</doc>
<doc id="29409" url="http://en.wikipedia.org/wiki?curid=29409" title="Spica">
Spica

Spica (; α Vir, α Virginis, Alpha Virginis) is the brightest star in the constellation Virgo, and the 15th brightest star in the night sky. It is a blue giant and a variable star of the Beta Cephei type, 250 light years from Earth.
Observation history.
Spica is believed to be the star that gave Hipparchus the data that led him to discover the precession of the equinoxes. A temple to Menat (an early Hathor) at Thebes was oriented with reference to Spica when it was built in 3200 BC, and, over time, precession slowly but noticeably changed Spica's location relative to the temple. Nicolaus Copernicus made many observations of Spica with his home-made triquetrum for his researches on precession.
As one of the nearest massive binary star systems to the Sun, Spica has been the subject of many observational studies.
Characteristics.
Spica is a close binary star whose components orbit about each other every four days. They stay close enough together that they cannot be resolved as two stars through a telescope. The changes in the orbital motion of this pair results in a Doppler shift in the absorption lines of their respective spectra, making them a double-lined spectroscopic binary. Initially, the orbital parameters for this system were inferred using spectroscopic measurements. Between 1966 and 1970, the Narrabri Stellar Intensity Interferometer was used to observe the pair and to directly measure the orbital characteristics and the angular diameter of the primary, which was found to be (0.90 ± 0.04) × 10−3 arcseconds, and the angular size of the semi-major axis of the orbit was found to be only slightly larger at (1.54 ± 0.05) × 10−3 arcseconds.
The primary star has a stellar classification of B1 III-IV. The luminosity class matches the spectrum of a star that is midway between a subgiant and a giant star, and it is no longer a B-type main-sequence star. This is a massive star with more than 10 times the mass of the Sun and seven times the Sun's radius. The total luminosity of this star is about 12,100 times that of the Sun, and eight times the luminosity of its companion. The primary is one of the nearest stars to the Sun that has enough mass to end its life in a Type II supernova explosion.
The primary is classified as a Beta Cephei-type variable star that varies in brightness over a 0.1738-day period. The spectrum shows a radial velocity variation with the same period, indicating that the surface of the star is regularly pulsating outward and then contracting. This star is rotating rapidly, with a rotational velocity of 199 km/s along the equator.
The secondary member of this system is one of the few stars to display the Struve–Sahade effect. This is an anomalous change in the strength of the spectral lines over the course of an orbit, where the lines become weaker as the star is moving away from the observer. It may be caused by a strong stellar wind from the primary scattering the light from secondary when it is receding. This star is smaller than the primary, with about 7 times the mass of the Sun and 3.6 times the Sun's radius. Its stellar classification is B2 V, making this a main-sequence star.
Spica is a rotating ellipsoidal variable, which is a non-eclipsing close binary star system where the stars are mutually distorted through their gravitational interaction. This effect causes the apparent magnitude of the star system to vary by 0.03 over an interval that matches the orbital period. This slight dip in magnitude is barely noticeable visually. Both stars rotate faster than their mutual orbital period. This lack of synchronization and the high ellipticity of their orbit may indicate that this is a young star system. Over time, the mutual tidal interaction of the pair may lead to rotational synchronization and orbit circularization.
Visibility.
Spica is 2.05 degrees from the ecliptic and can be occulted by the Moon and sometimes by the planets. The last planetary occultation of Spica occurred when Venus passed in front of the star (as seen from Earth) on November 10, 1783. The next occultation will occur on September 2, 2197, when Venus again passes in front of Spica. The Sun passes a little more than 2° north of Spica around October 16 every year, and the star's heliacal rising occurs about two weeks later. Every 8 years, Venus passes Spica around the time of the star's heliacal rising, as in 2009 when it passed 3.5° north of the star on November 3.
A method of finding Spica is to follow the arc of the handle of the Big Dipper to Arcturus, and then continue on the same angular distance to Spica. This can be recalled by the mnemonic phrase, "arc to Arcturus and spike to Spica."
Names.
The name "Spica" derives from Latin "spīca virginis" "the virgin's ear of [wheat] grain". It was also anglicized as "Virgin's Spike".
Johann Bayer cited the name "Arista". 
Another alternative name is "Azimech", from Arabic السماك الأعزل "al-simāk al-a‘zal" 'the Undefended', and Alarph, Arabic for 'the Grape Gatherer'. 
Sumbalet (Sombalet, Sembalet and variants) is from an Arabic "sunbulah" "corn ear".
In medieval astrology, it was a Behenian fixed star, associated with the emerald and sage. 
In his "Three Books of Occult Philosophy", Cornelius Agrippa attributes its kabbalistic symbol to Hermes Trismegistus.
In Chinese astronomy, the star is known as "Jiao Xiu 1" (角宿一), i.e. the first star of the "Jiao Xiu" asterism.
In Hindu astronomy, Spica corresponds to the Nakshatra "Chitra".
A blue star represents Spica on the of the Brazilian state of Pará. Spica is also the star representing Pará on the Brazilian flag.
Namesakes.
Both USS "Spica" (AK-16) and USNS "Spica" (T-AFS-9) were named after this star whilst USS "Azimech" (AK-124), a "Crater"-class cargo ship, was given one of the star's medieval names.
External links.
Coordinates: 

</doc>
<doc id="29414" url="http://en.wikipedia.org/wiki?curid=29414" title="Stuart Little">
Stuart Little

Stuart Little is a 1945 children's novel by E. B. White, his first book for children, and is widely recognized as a classic in children's literature. "Stuart Little" was illustrated by the subsequently award-winning artist Garth Williams, also his first work for children. It is a realistic fantasy about a talking mouse, Stuart Little, born to human parents in New York City.
Background.
In a letter White wrote in response to inquiries from readers, he described how he came to conceive of Stuart Little: "many years ago I went to bed one night in a railway sleeping car, and during the night I dreamed about a tiny boy who acted rather like a mouse. That's how the story of Stuart Little got started". He had the dream in the spring of 1926, while sleeping on a train on his way back to New York from a visit to the Shenandoah Valley. Biographer Michael Sims wrote that Stuart "arrived in [White's] mind in a direct shipment from the subconscious." White typed up a few stories about Stuart, which he told to his 18 nieces and nephews when they asked him to tell them a story. In 1935, White's wife Katharine showed these stories to Clarence Day, then a regular contributor to "The New Yorker". Day liked the stories and encouraged White not to neglect them, but neither Oxford University Press nor Viking Press were interested in the stories, and White did not immediately develop them further.
In the fall of 1938, as his wife wrote her annual collection of children's book reviews for "The New Yorker", White wrote a few paragraphs in his "One Man's Meat" column in "Harper's Magazine" about writing children's books. Anne Carroll Moore, the head children's librarian at the New York Public Library, read this column and responded by encouraging him to write a children's book that would "make the library lions roar". White's editor at Harper, who had heard about the Stuart stories from Katherine, asked to see them, and by March 1939 was intent on publishing them. Around that time, White wrote to James Thurber that he was "about half done" with the book; however, he made little progress with it until the winter of 1944-1945.
Plot.
First we learn of Stuart's birth to a family in New York City and how the family adapts, socially and structurally, to having such a small son. He has an adventure in which he gets caught in a window-blind while exercising; Snowbell, the family cat, then places Stuart's hat and cane outside a rat hole, panicking the family. He was accidentally released by his brother George. Then two chapters describe Stuart's participation in a model sailboat race in Central Park. A bird named Margalo is adopted by the Little family, and Stuart protects her from Snowbell, their malevolent cat. The bird repays his kindness by saving Stuart when he is trapped in a garbage can and shipped out for disposal at sea.
Margalo flees when she is warned that one of Snowbell's friends intends to eat her, and Stuart strikes out to find her. A friendly dentist, who is also the owner of the boat Stuart had raced in Central Park, gives him use of a gasoline-powered model car, and Stuart departs to see the country. He works for a while as a substitute teacher and comes to the town of Ames Crossing, where he meets a girl named Harriet Ames who is no taller than he is. They go on one date, and then Stuart leaves town. As the book ends, he has not yet found Margalo, but feels confident he will do so.
Reception.
Lucien Agosta, in his overview of the critical reception of the book, notes that "Critical reactions to "Stuart Little" have varied from disapprobation to unqualified admiration since the book was published in 1945, though generally it has been well received." Anne Carroll Moore, who had initially encouraged White to write the book, was critical of it when she read a proof of it. She wrote letters to White; his wife, Katharine; and Ursula Nordstrom, the children's editors at Harper's, advising that the book not be published.
Malcolm Cowley, who reviewed the book for "The New York Times", wrote, "Mr. White has a tendency to write amusing scenes instead of telling a story. To say that "Stuart Little" is one of the best children's books published this year is very modest praise for a writer of his talent." The book has become a children's classic, and is widely read by children and used by teachers. White received the Laura Ingalls Wilder Medal in 1970 for "Stuart Little" and "Charlotte's Web".
Adaptations.
Film.
The book was very loosely adapted into a 1999 film of the same name, which combined live-action with computer animation. A 2002 sequel to the first film, "Stuart Little 2", was truer to the book. A third film, "" was released direct-to-video in 2006. This film was entirely computer-animated, and its plot was not derived from the book. The voice for Stuart Little was provided by Michael J. Fox in all three films.
Television.
"The World of Stuart Little," a 1966 episode of NBC's "Children's Theater", narrated by Johnny Carson, won a Peabody Award and was nominated for an Emmy. An animated television series, "", (based on the film adaptations) was produced for HBO Family and aired for 13 episodes in 2003.
Video games.
Three video games based on Stuart Little were produced, but were mostly based off the film adaptations of the same name. "Stuart Little: the Journey Home" based on the 1999 film, was released only for the Game Boy Color in 2001. A game based on Stuart Little 2 was released for the PlayStation, Game Boy Advance, Windows 2000, Windows ME, Windows XP and Windows Little in 2002. And a third game entitled "Stuart Little 3: Big Photo Adventure" was released only for the PlayStation 2 in 2005. Although it bears no resembleance to the film .

</doc>
<doc id="29417" url="http://en.wikipedia.org/wiki?curid=29417" title="Statite">
Statite

A statite (a portmanteau of "static" and "satellite") is a hypothetical type of artificial satellite that employs a solar sail to continuously modify its orbit in ways that gravity alone would not allow. Typically, a statite would use the solar sail to "hover" in a location that would not otherwise be available as a stable geosynchronous orbit. Statites have been proposed that would remain in fixed locations high over Earth's poles, using reflected sunlight to counteract the gravity pulling them down. Statites might also employ their sails to change the shape or velocity of more conventional orbits, depending upon the purpose of the particular statite.
The concept of the statite was invented independently (and approximately simultaneously) by Robert L. Forward (who coined the term "statite"), and by Colin McInnes, who used the term "halo orbit" (not to be confused with the type of halo orbit invented by Robert Farquhar). Subsequently the terms "non-Keplerian orbit" and "artificial Lagrange point" have been used as a generalization of the above terms.
No statites have been deployed to date, as solar sail technology is still in its infancy. NASA's Sunjammer solar sail mission has the stated objective of flying to an artificial Lagrange point near the Earth/Sun L1 point, to demonstrate the feasibility of the geomagnetic storm warning mission concept proposed by NOAA's Patricia Mulligan.

</doc>
<doc id="29419" url="http://en.wikipedia.org/wiki?curid=29419" title="Stanford (disambiguation)">
Stanford (disambiguation)

Stanford may refer to: 

</doc>
<doc id="29420" url="http://en.wikipedia.org/wiki?curid=29420" title="Solar sail">
Solar sail

Solar sails (also called light sails or photon sails) are a form of spacecraft propulsion using the radiation pressure (also called solar pressure) from stars to push large ultra-thin mirrors to high speeds. Light sails could also be driven by energy beams to extend their range of operations, which is strictly beam sailing rather than solar sailing.
Solar sail craft offer the possibility of low-cost operations combined with long operating lifetimes. Since they have few moving parts and use no propellant, they can potentially be used numerous times for delivery of payloads.
Solar sails use a phenomenon that has a proven, measured effect on spacecraft. Solar pressure affects all spacecraft, whether in interplanetary space or in orbit around a planet or small body. A typical spacecraft going to Mars, for example, will be displaced by thousands of kilometres by solar pressure, so the effects must be accounted for in trajectory planning, which has been done since the time of the earliest interplanetary spacecraft of the 1960s. Solar pressure also affects the attitude of a craft, a factor that must be included in spacecraft design.
The total force exerted on an 800 by 800 meter solar sail, for example, is about 5 N at Earth's distance from the Sun, 
making it a low-thrust propulsion system, similar to spacecraft propelled by electric engines.
History of concept.
Johannes Kepler observed that comet tails point away from the Sun and suggested that the Sun caused the effect. In a letter to Galileo in 1610, he wrote, "Provide ships or sails adapted to the heavenly breezes, and there will be some who will brave even that void." He might have had the comet tail phenomenon in mind when he wrote those words, although his publications on comet tails came several years later.
James Clerk Maxwell, in 1861–64, published his theory of electromagnetic fields and radiation, which shows that light has momentum and thus can exert pressure on objects. Maxwell's equations provide the theoretical foundation for sailing with light pressure. So by 1864, the physics community and beyond knew sunlight carried momentum that would exert a pressure on objects.
Jules Verne, in "From the Earth to the Moon", published in 1865, wrote "there will some day appear velocities far greater than these [of the planets and the projectile], of which light or electricity will probably be the mechanical agent ... we shall one day travel to the moon, the planets, and the stars." This is possibly the first published recognition that light could move ships through space. Given the date of his publication and the widespread, permanent distribution of his work, it appears that he should be regarded as the originator of the concept of space sailing by light pressure, although he did not develop the concept further. Verne probably got the idea directly and immediately from Maxwell's 1864 theory (although it cannot be ruled out that Maxwell or an intermediary recognized the sailing potential and became the source for Verne).
Pyotr Lebedev was first to successfully demonstrate light pressure, which he did in 1899 with a torsional balance; Ernest Nichols and Gordon Hull conducted a similar independent experiment in 1901 using a Nichols radiometer.
Albert Einstein provided a different formalism by his recognizing the equivalence of mass and energy. He simply wrote p = E/c as the relationship between the momentum, the energy, and the speed of light.
Svante Arrhenius predicted in 1908 the possibility of solar radiation pressure distributing life spores across interstellar distances, the concept of panspermia. He apparently was the first scientist to state that light could move objects between stars.
Friedrich Zander (Tsander) published a technical paper that included technical analysis of solar sailing. Zander wrote of "using tremendous mirrors of very thin sheets" and "using the pressure of sunlight to attain cosmic velocities".
JBS Haldane speculated in 1927 the invention of tubular spaceships that would take humanity to space and how "wings of metallic foil of a square kilometre or more in area are spread out to catch the Sun's radiation pressure".
J.D. Bernal wrote in 1929, "A form of space sailing might be developed which used the repulsive effect of the Sun's rays instead of wind. A space vessel spreading its large, metallic wings, acres in extent, to the full, might be blown to the limit of Neptune's orbit. Then, to increase its speed, it would tack, close-hauled, down the gravitational field, spreading full sail again as it rushed past the Sun."
The first formal technology and design effort for a solar sail began in 1976 at Jet Propulsion Laboratory for a proposed mission to rendezvous with Halley's Comet.
Physical principles.
Solar radiation pressure.
Solar radiation exerts a pressure on the sail due to reflection and a small fraction that is absorbed. The absorbed energy heats the sail, which re-radiates that energy from the front and rear surfaces.
The momentum of a photon or an entire flux is given by p = E/c, where E is the photon or flux energy, p is the momentum, and c is the speed of light. Solar radiation pressure is calculated on an irradiance (solar constant) value of 1361 W/m2 at 1 AU (Earth-Sun distance), as revised in 2011:
perfect absorbance: F = 4.54 μN per square metre (4.54 μPa)
perfect reflectance: F = 9.08 μN per square metre (9.08 μPa)  (normal to surface)
A perfect sail is flat and has 100% specular reflection. An actual sail will have an overall efficiency of about 90%, about 8.25 μN/m2, due to curvature (billow), wrinkles, absorbance, re-radiation from front and back, non-specular effects, and other factors.
The force on a sail and the actual acceleration of the craft vary by the inverse square of distance from the Sun (unless close to the Sun), and by the square of the cosine of the angle between the sail force vector and the radial from the Sun, so
F = F0 cos2 θ / R2 (ideal sail)
where R is distance from the Sun in AU. An actual square sail can be modeled as:
F = F0 (0.349 + 0.662 cos 2θ − 0.011 cos 4θ) / R2
Note that the force and acceleration approach zero generally around θ = 60° rather than 90° as one might expect with an ideal sail.
Solar wind, the flux of charged particles blown out from the Sun, exerts a nominal dynamic pressure of about 3 to 4 nPa, three orders of magnitude less than solar radiation pressure on a reflective sail.
Sail parameters.
Sail loading (areal density) is an important parameter, which is the total mass divided by the sail area, expressed in g/m2. It is represented by the Greek letter σ.
A sail craft has a characteristic acceleration, ac, which it would experience at 1 AU when facing the Sun. It is related to areal density by:
ac = 8.25 / σ, in mm/s2 (assuming 90% efficiency)
The lightness number, λ, is the dimensionless ratio of maximum vehicle acceleration divided by the Sun's local gravity; using the values at 1 AU:
λ = ac / 5.93
The table presents some example values. Payloads are not included. The first two are from the detailed design effort at JPL in the 1970s. The third, the lattice sailer, might represent about the best possible performance level. The dimensions for square and lattice sails are edges. The dimension for heliogyro is blade tip to blade tip.
Attitude control.
An active attitude control system (ACS) is essential for a sail craft to achieve and maintain a desired orientation. The required sail orientation changes slowly (often less than 1 degree per day) in interplanetary space, but much more rapidly in a planetary orbit. The ACS must be capable of meeting these orientation requirements.
Attitude control is achieved by a relative shift between the craft's center of pressure and its center of mass. This can be achieved with control vanes, movement of individual sails, movement of a control mass, or altering reflectivity.
Holding a constant attitude requires that the ACS maintain a net torque of zero on the craft. The total force and torque on a sail, or set of sails, is not constant along a trajectory. The force changes with solar distance and sail angle, which changes the billow in the sail and deflects some elements of the supporting structure, resulting in changes in the sail force and torque.
Sail temperature also changes with solar distance and sail angle, which changes sail dimensions. The radiant heat from the sail changes the temperature of the supporting structure. Both factors affect total force and torque.
To hold the desired attitude the ACS must compensate for all of these changes.
Constraints.
In Earth orbit, solar pressure and drag pressure are typically equal at an altitude of about 800 km, which means that a sail craft would have to operate above that altitude. Sail craft must operate in orbits where their turn rates are compatible with the orbits, which is generally a concern only for spinning disk configurations.
Sail operating temperatures are a function of solar distance, sail angle, reflectivity, and front and back emissivities. A sail can be used only where its temperature is kept within its material limits. Generally, a sail can be used rather close to the Sun, around 0.25 AU, or even closer if carefully designed for those conditions.
Applications.
Potential applications for sail craft range throughout the Solar System, from near the Sun to the comet clouds beyond Neptune. The craft can make outbound voyages to deliver loads or to take up station keeping at the destination. They can be used to haul cargo and possibly also used for human travel.
Inner planets.
For trips within the inner Solar System, they can deliver loads and then return to Earth for subsequent voyages, operating as an interplanetary shuttle. For Mars in particular, the craft could provide economical means of routinely supplying operations on the planet.
Solar sail craft can approach the Sun to deliver observation payloads or to take up station keeping orbits. They can operate at 0.25 AU or closer. They can reach high orbital inclinations, including polar.
Solar sails can travel to and from all of the inner planets. Trips to Mercury and Venus are for rendezvous and orbit entry for the payload. Trips to Mars could be either for rendezvous or swing-by with release of the payload for aerodynamic braking.
Outer planets.
Minimum transfer times to the outer planets benefit from using an indirect transfer (solar swing-by). However, this method results in high arrival speeds. Slower transfers have lower arrival speeds.
The minimum transfer time to Jupiter for "ac" of 1 mm/s2 with no departure velocity relative to Earth is 2 years when using an indirect transfer (solar swing-by). The arrival speed ("V"∞) is close to 17 km/s. For Saturn, the minimum trip time is 3.3 years, with an arrival speed of nearly 19 km/s.
Oort Cloud.
A small team had initially proposed a beryllium inflated sail that would go down to 0.05 AU from the Sun in order to get an acceleration peaking at 36.4 m/s2, reaching a speed of 0.00264c (about 950 km/s) in less than a day. Such proximity to the Sun could prove to be impractical in the near term due to the structural degradation of beryllium at high temperatures, diffusion of Hydrogen at high temperatures as well as an electrostatic gradient, generated by the ionization of beryllium due to the solar wind, posing a burst risk; thus a revised perihelion of 0.1 AU was proposed to reduce the aforementioned temperature and solar flux exposure.
Such a sail would take "Two and a half years to reach the heliopause, six and a half years to get to the Sun’s inner gravitational focus (at about 550 AU, the point at which light is focused by gravity as it passes the Sun,) with arrival at the inner Oort Cloud in no more than thirty years." "Such a mission could perform useful astrophysical observations en route, explore gravitational focusing techniques, and image Oort Cloud objects while exploring particles and fields in that region that are of galactic rather than solar origin."
Satellites.
Robert L. Forward pointed out that a solar sail could be used to modify the orbit of a satellite around the Earth. In the limit, a sail could be used to "hover" a satellite above one pole of the Earth. Spacecraft fitted with solar sails could also be placed in close orbits about the Sun that are stationary with respect to either the Sun or the Earth, a type of satellite named by Forward a statite. This is possible because the propulsion provided by the sail offsets the gravitational potential of the Sun. Such an orbit could be useful for studying the properties of the Sun over long durations. Likewise a solar sail-equipped spacecraft could also remain on station nearly above the polar terminator of a planet such as the Earth by tilting the sail at the appropriate angle needed to just counteract the planet's gravity.
In his book "The Case for Mars", Robert Zubrin points out that the reflected sunlight from a large statite placed near the polar terminator of the planet Mars could be focused on one of the Martian polar ice caps to significantly warm the planet's atmosphere. Such a statite could be made from asteroid material.
Trajectory corrections.
The MESSENGER probe orbiting Mercury, used light pressure on its solar panels to perform fine trajectory corrections on the way to Mercury. By changing the angle of the solar panels relative to the Sun, the amount of solar radiation pressure was varied to adjust the spacecraft trajectory more delicately than possible with thrusters. Minor errors are greatly amplified by gravity assist maneuvers, so using radiation pressure to make very small corrections saved large amounts of propellant.
Interstellar flight.
In the 1970s, Robert Forward proposed two beam-powered propulsion schemes using either lasers or masers to push giant sails to a significant fraction of the speed of light.
In "The Flight of the Dragonfly", Forward described a light sail propelled by super lasers. As the starship neared its destination, the outer portion of the sail would detach. The outer sail would then refocus and reflect the lasers back onto a smaller, inner sail. This would provide braking thrust to stop the ship in the destination star system.
Both methods pose monumental engineering challenges. The lasers would have to operate for years continuously at gigawatt strength. Forward's solution to this requires enormous solar panel arrays to be built at or near the planet Mercury. A planet-sized mirror or fresnel lens would be needed several dozen astronomical units from the Sun to keep the lasers focused on the sail. The giant braking sail would have to act as a precision mirror to focus the braking beam onto the inner "deceleration" sail.
A potentially easier approach would be to use a maser to drive a "solar sail" composed of a mesh of wires with the same spacing as the wavelength of the microwaves, since the manipulation of microwave radiation is somewhat easier than the manipulation of visible light. The hypothetical "Starwisp" interstellar probe design would use microwaves, rather than visible light, to push it. Masers spread out more rapidly than optical lasers owing to their longer wavelength, and so would not have as long an effective range.
Masers could also be used to power a painted solar sail, a conventional sail coated with a layer of chemicals designed to evaporate when struck by microwave radiation. The momentum generated by this evaporation could significantly increase the thrust generated by solar sails, as a form of lightweight ablative laser propulsion.
To further focus the energy on a distant solar sail, designs have considered the use of a large zone plate. This would be placed at a location between the laser or maser and the spacecraft.
Another more physically realistic approach would be to use the light from the Sun to accelerate. The ship would first drop into an orbit making a close pass to the Sun, to maximize the solar energy input on the sail, then the ship would begin to accelerate away from the system using the light from the Sun to keep accelerating. Acceleration will drop approximately as the inverse square of the distance from the Sun, and beyond some distance, the ship would no longer receive enough light to accelerate it significantly, but would maintain its course due to inertia. When nearing the target star, the ship could turn its sails toward it and begin to use the outward acceleration to decelerate. Additional forward and reverse thrust could be achieved with more conventional means of propulsion such as rockets.
Similar solar sailing launch and capture were suggested for directed panspermia to expand life in other solar system. Velocities of 0.0005 c could be obtained by solar sails carrying 10 kg payloads, using thin solar sail vehicles with effective areal densities of 0.1 g/m2 with thin sails of 0.1 µm thickness and sizes on the order of one square kilometer. Alternatively, swarms of 1 mm capsules can be launched on solar sails with radii of 42 cm, each carrying 10,000 capsules of a hundred million extremophile microorganisms to seed life in diverse target environments.
Deorbiting artificial satellites.
Small solar sails have been proposed to accelerate the deorbiting of small artificial satellites from Earth orbits. Satellites in low Earth orbit can use a combination of solar pressure on the sail and increased atmospheric drag to accelerate satellite reentry.
Sail configurations.
IKAROS, launched in 2010, has been the first practical solar sail vehicle. In 2012, it was still under thrust, proving the practicality of a solar sail for long-duration missions. It is spin-deployed, with tip-masses in the corners of its square sail. The sail is made of thin polyimide film, with evaporated aluminium on it. It steers with electrically-controlled liquid crystal panels. The sail slowly spins, and these panels turn on and off to control the attitude of the vehicle. When on, they diffuse light, reducing the momentum transfer to that part of the sail. When off, the sail reflects more light, transferring more momentum. In that way, they turn the sail. Thin-film solar cells are also integrated into the sail, powering the spacecraft. The design is very reliable, because spin deployment simplified the mechanisms to unfold the sail and the LCD panels have no moving parts.
Parachutes have very low mass, but a parachute is not a workable configuration for a solar sail. Analysis shows that a parachute configuration would collapse from the forces exerted by shroud lines, since radiation pressure does not behave like aerodynamic pressure, and would not act to keep the parachute open.
Eric Drexler proposed very high thrust-to-mass solar sails, and made prototypes of the sail material. His sail would use panels of thin aluminium film (30 to 100 nanometres thick) supported by a tensile structure. The sail would rotate and would have to be continually under thrust. He made and handled samples of the film in the laboratory, but the material was too delicate to survive folding, launch, and deployment. The design planned to rely on space-based production of the film panels, joining them to a deploy-able tension structure. Sails in this class would offer high area per unit mass and hence accelerations up to "fifty times higher" than designs based on deploy-able plastic films.
The highest thrust-to-mass designs for ground-assembled deploy-able structures are square sails with the masts and guy lines on the dark side of the sail. Usually there are four masts that spread the corners of the sail, and a mast in the center to hold guy-wires. One of the largest advantages is that there are no hot spots in the rigging from wrinkling or bagging, and the sail protects the structure from the Sun. This form can, therefore, go close to the Sun for maximum thrust. Most designs steer with small moving sails on the ends of the spars.
In the 1970s JPL studied many rotating blade and ring sails for a mission to rendezvous with Halley's Comet. The intention was to stiffen the structures using angular momentum, eliminating the need for struts, and saving mass. In all cases, surprisingly large amounts of tensile strength were needed to cope with dynamic loads. Weaker sails would ripple or oscillate when the sail's attitude changed, and the oscillations would add and cause structural failure. The difference in the thrust-to-mass ratio between practical designs was almost nil, and the static designs were easier to control.
JPL's reference design was called the "heliogyro". It had plastic-film blades deployed from rollers and held out by centrifugal forces as it rotated. The spacecraft's attitude and direction were to be completely controlled by changing the angle of the blades in various ways, similar to the cyclic and collective pitch of a helicopter. Although the design had no mass advantage over a square sail, it remained attractive because the method of deploying the sail was simpler than a strut-based design.
JPL also investigated "ring sails" (Spinning Disk Sail in the above diagram), panels attached to the edge of a rotating spacecraft. The panels would have slight gaps, about one to five percent of the total area. Lines would connect the edge of one sail to the other. Masses in the middles of these lines would pull the sails taut against the coning caused by the radiation pressure. JPL researchers said that this might be an attractive sail design for large manned structures. The inner ring, in particular, might be made to have artificial gravity roughly equal to the gravity on the surface of Mars.
A solar sail can serve a dual function as a high-gain antenna. Designs differ, but most modify the metalization pattern to create a holographic monochromatic lens or mirror in the radio frequencies of interest, including visible light.
Electric solar wind sail.
Pekka Janhunen from FMI has invented a type of solar sail called the electric solar wind sail. Mechanically it has little in common with the traditional solar sail design. The sails are replaced with straightened conducting tethers (wires) placed radially around the host ship. The wires are electrically charged to create an electric field around the wires. The electric field extends a few tens of metres into the plasma of the surrounding solar wind. The solar electrons are reflected by the electric field (like the photons on a traditional solar sail). The radius of the sail is from the electric field rather than the actual wire itself, making the sail lighter. The craft can also be steered by regulating the electric charge of the wires. A practical electric sail would have 50–100 straightened wires with a length of about 20 km each.
Electric solar wind sails can adjust their electrostatic fields and sail attitudes.
Magnetic sail.
A magnetic sail would also employ the solar wind. However, the magnetic field deflects the electrically charged particles in the wind. It uses wire loops, and runs a static current through them instead of applying a static voltage.
All these designs maneuver, though the mechanisms are different.
Magnetic sails bend the path of the charged protons that are in the solar wind. By changing the sails' attitudes, and the size of the magnetic fields, they can change the amount and direction of the thrust.
Sail making.
Materials.
The material developed for the Drexler solar sail was a thin aluminium film with a baseline thickness of 0.1 µm, to be fabricated by vapor deposition in a space-based system. Drexler used a similar process to prepare films on the ground. As anticipated, these films demonstrated adequate strength and robustness for handling in the laboratory and for use in space, but not for folding, launch, and deployment.
The most common material in current designs is aluminized 2 µm Kapton film. It resists the heat of a pass close to the Sun and still remains reasonably strong. The aluminium reflecting film is on the Sun side. The sails of "Cosmos 1" were made of aluminized PET film (Mylar).
Research by Geoffrey Landis in 1998–1999, funded by the NASA Institute for Advanced Concepts, showed that various materials such as alumina for laser lightsails and carbon fiber for microwave pushed lightsails were superior sail materials to the previously standard aluminium or Kapton films.
In 2000, Energy Science Laboratories developed a new carbon fiber material that might be useful for solar sails. The material is over 200 times thicker than conventional solar sail designs, but it is so porous that it has the same mass. The rigidity and durability of this material could make solar sails that are significantly sturdier than plastic films. The material could self-deploy and should withstand higher temperatures.
There has been some theoretical speculation about using molecular manufacturing techniques to create advanced, strong, hyper-light sail material, based on nanotube mesh weaves, where the weave "spaces" are less than half the wavelength of light impinging on the sail. While such materials have so far only been produced in laboratory conditions, and the means for manufacturing such material on an industrial scale are not yet available, such materials could mass less than 0.1 g/m2, making them lighter than any current sail material by a factor of at least 30. For comparison, 5 micrometre thick Mylar sail material mass 7 g/m2, aluminized Kapton films have a mass as much as 12 g/m2, and Energy Science Laboratories' new carbon fiber material masses 3 g/m2.
The least dense metal is lithium, about 5 times less dense than aluminium. Fresh, unoxidized surfaces are reflective. At a thickness of 20 nm, lithium has an areal density of 0.011 g/m2. A high-performance sail could be made of lithium alone at 20 nm (no emission layer). It would have to be fabricated in space and not used to approach the Sun. In the limit, a sail craft might be constructed with a total areal density of around 0.02 g/m2, giving it a lightness number of 67 and ac of about 400 mm/s2. Magnesium and beryllium are also potential materials for high-performance sails. These 3 metals can be alloyed with each other and with aluminium.
Reflection and emissivity layers.
Aluminium is the common choice for the reflection layer. It typically has a thickness of at least 20 nm, with a reflectivity of 0.88 to 0.90. Chromium is a good choice for the emission layer on the face away from the Sun. It can readily provide emissivity values of 0.63 to 0.73 for thicknesses from 5 to 20 nm on plastic film. Usable emissivity values are empirical because thin-film effects dominate; bulk emissivity values do not hold up in these cases because material thickness is much thinner than the emitted wavelengths.
Fabrication.
Sails are fabricated on Earth on long tables where ribbons are unrolled and joined to create the sails. These sails are packed, launched, and unfurled in space.
In the future, fabrication could take place in orbit inside large frames that support the sail. This would result in lower mass sails and elimination of the risk of deployment failure.
Operations.
Changing orbits.
Sailing operations are simplest in interplanetary orbits, where attitude changes are done at low rates. For outward bound trajectories, the sail force vector is oriented forward of the Sun line, which increases orbital energy and angular momentum, resulting in the craft moving farther from the Sun. For inward trajectories, the sail force vector is oriented behind the Sun line, which decreases orbital energy and angular momentum, resulting in the craft moving in toward the Sun. It is worth noting that only the Sun's gravity pulls the craft toward the Sun—there is no analog to a sailboat's tacking to windward. To change orbital inclination, the force vector is turned out of the plane of the velocity vector.
In orbits around planets or other bodies, the sail is oriented so that its force vector has a component along the velocity vector, either in the direction of motion for an outward spiral, or against the direction of motion for an inward spiral.
Trajectory optimizations can often require intervals of reduced or zero thrust. This can be achieved by rolling the craft around the Sun line with the sail set at an appropriate angle to reduce or remove the thrust.
Swing-by Maneuvers.
A close solar passage can be used to increase a craft's energy. The increased radiation pressure combines with the efficacy of being deep in the Sun's gravity well to substantially increase the energy for runs to the outer Solar System. The optimal approach to the Sun is done by increasing the orbital eccentricity while keeping the energy level as high as practical. The minimum approach distance is a function of sail angle, thermal properties of the sail and other structure, load effects on structure, and sail optical characteristics (reflectivity and emissivity). A close passage can result in substantial optical degradation. Required turn rates can increase substantially for a close passage. A sail craft arriving at a star can use a close passage to reduce energy, which also applies to a sail craft on a return trip from the outer Solar System.
A lunar swing-by can have important benefits for trajectories leaving from or arriving at Earth. This can reduce trip times, especially in cases where the sail is heavily loaded. A swing-by can also be used to obtain favorable departure or arrival directions relative to Earth.
A planetary swing-by could also be employed similar to what is done with coasting spacecraft, but good alignments might not exist due to the requirements for overall optimization of the trajectory.
Smart lines.
A smart line could be a critical element of sailing operations. As with maritime ships, lines are essential for a wide range of uses. One difference is that some lines may be very long and need to be self-guiding. The lines could extend from and retract into the sail craft.
A maneuverable grappling device can be used at the end of a line to place or pick up payload containers, to secure a ship to a structure such as a station, to pick up samples from an asteroid or comet, or to engage in towing. The maneuvering unit is like a small spacecraft, with many of the same sensors and control systems. It could draw power from and communicate with the sail craft through the line. These operations could be done autonomously.
Lines a few hundred kilometers long may be used to move a ship from a space station to an orbit farther out where it could begin sailing.
Towing.
Smart lines can enable towing operations by being able to attach to or release objects at the remote end of the line. Attached objects might be pulled in to the body of the sailer or remain at the end of the deployed line. Objects to be towed may have attachment points that allow multiple sail craft to engage in the towing. Towing operations can include deflecting large bodies that pose a hazard to Earth, bringing natural bodies to Earth or other sites for resource recovery, and transporting disabled spacecraft or other structures.
To tow or deflect a large body, poles can be inserted on the spin axis of the body. Sail craft can attach to the embedded poles using smart lines. Slip rings enable the craft to tow without the lines getting wrapped up as a result of rotation of the body.
Projects operating or completed.
IKAROS 2010.
On 21 May 2010, Japan Aerospace Exploration Agency (Jaxa) launched the world's first interplanetary solar sail spacecraft "IKAROS" ("Interplanetary Kite-craft Accelerated by Radiation Of the Sun") to Venus.
Japan's JAXA successfully tested IKAROS in 2010. The goal was to deploy and control the sail and for the first time determining the minute orbit perturbations caused by light pressure. Orbit determination was done by the nearby AKATSUKI probe from which IKAROS detached after both had been brought into a transfer orbit to Venus. The total effect over the six month' flight was 100 m/s.
Until 2010, no solar sails had been successfully used in space as primary propulsion systems. On 21 May 2010, the Japan Aerospace Exploration Agency (JAXA) launched the IKAROS (Interplanetary Kite-craft Accelerated by Radiation Of the Sun) spacecraft, which deployed a 200 m2 polyimide experimental solar sail on June 10.<ref name=jaxa.jp/press/2010/06/20100611_ikaros_e></ref> In July, the next phase for the demonstration of acceleration by radiation began. On 9 July 2010, it was verified that IKAROS collected radiation from the Sun and began photon acceleration by the orbit determination of IKAROS by range-and-range-rate (RARR) that is newly calculated in addition to the data of the relativization accelerating speed of IKAROS between IKAROS and the Earth that has been taken since before the Doppler effect was utilized.<ref name=http://www.jaxa.jp/press/2010/07/20100709_ikaros_j.html></ref> The data showed that IKAROS appears to have been solar-sailing since 3 June when it deployed the sail.
IKAROS has a diagonal spinning square sail 20 m made of a 7.5 μm thick sheet of polyimide. The polyimide sheet had a mass of about 10 grams per square metre. A thin-film solar array is embedded in the sail. Eight LCD panels are embedded in the sail, whose reflectance can be adjusted for attitude control. IKAROS spent six months traveling to Venus, and then began a three-year journey to the far side of the Sun.
Attitude (orientation) control.
Both the Mariner 10 mission, which flew by the planets Mercury and Venus, and the MESSENGER mission to Mercury demonstrated the use of solar pressure as a method of attitude control in order to conserve attitude-control propellant.
Hayabusa also used solar pressure as a method of attitude control to compensate for broken reaction wheels and chemical thruster.
Sail deployment tests.
NASA has successfully tested deployment technologies on small scale sails in vacuum chambers.
On February 4, 1993, the Znamya 2, a 20-meter wide aluminized-mylar reflector, was successfully deployed from the Russian Mir space station. Although the deployment succeeded, propulsion was not demonstrated. A second test, Znamya 2.5, failed to deploy properly.
In 1999, a full-scale deployment of a solar sail was tested on the ground at DLR/ESA in Cologne.
On August 9, 2004, the Japanese ISAS successfully deployed two prototype solar sails from a sounding rocket. A clover-shaped sail was deployed at 122 km altitude and a fan-shaped sail was deployed at 169 km altitude. Both sails used 7.5-micrometer film. The experiment purely tested the deployment mechanisms, not propulsion.
Solar sail propulsion attempts.
A joint private project between Planetary Society, Cosmos Studios and Russian Academy of Science made two sail testing attempts: in 2001 a suborbital prototype test failed because of rocket failure; and in June 21, 2005, "Cosmos 1" launched from a submarine in the Barents Sea, but the Volna rocket failed, and the spacecraft failed to reach orbit. They intended to use the sail to gradually raise the spacecraft to a higher Earth orbit over a mission duration of one month. On Carl Sagan's 75th birthday (November 9, 2009) the same group announced plans to make three further attempts, dubbed LightSail-1, -2, and -3. The new design will use a 32-square-meter Mylar sail, deployed in four triangular segments like NanoSail-D. The launch configuration is that of three adjacent CubeSats, and as of 2011 was waiting for a piggyback launch opportunity.
A 15-meter-diameter solar sail (SSP, solar sail sub payload, "soraseiru sabupeiro-do") was launched together with ASTRO-F on a M-V rocket on February 21, 2006, and made it to orbit. It deployed from the stage, but opened incompletely.
NanoSail-D 2010.
A team from the NASA Marshall Space Flight Center (Marshall), along with a team from the NASA Ames Research Center, developed a solar sail mission called NanoSail-D, which was lost in a launch failure aboard a Falcon 1 rocket on 3 August 2008. The second backup version, NanoSail-D2, also sometimes called simply NanoSail-D, was launched with FASTSAT on a Minotaur IV on November 19, 2010, becoming NASA's first solar sail deployed in low earth orbit. The objectives of the mission were to test sail deployment technologies, and to gather data about the use of solar sails as a simple, "passive" means of de-orbiting dead satellites and space debris. The NanoSail-D structure was made of aluminium and plastic, with the spacecraft massing less than 10 lb. The sail has about 100 sqft of light-catching surface. After some initial problems with deployment, the solar sail was deployed and over the course of its 240 day mission reportedly produced a "wealth of data" concerning the use of solar sails as passive deorbit devices.
NASA launched the second NanoSail-D unit stowed inside the FASTSAT satellite on the Minotaur IV on November 19, 2010. The ejection date from the FASTSAT microsatellite was planned for December 6, 2010, but deployment only occurred on January 20, 2011.
Projects in development or proposed.
Despite the losses of "Cosmos 1" and NanoSail-D (which were due to failure of their launchers), scientists and engineers around the world remain encouraged and continue to work on solar sails. While most direct applications created so far intend to use the sails as inexpensive modes of cargo transport, some scientists are investigating the possibility of using solar sails as a means of transporting humans. This goal is strongly related to the management of very large (i.e. well above 1 km2) surfaces in space and the sail making advancements. Manned space flight utilizing solar sails is still in the development state of infancy.
Sunjammer 2015.
A technology demonstration sail craft, dubbed "Sunjammer", was in development with the intent to prove the viability and value of sailing technology. "Sunjammer" had a square sail, 124 feet (38 meters) wide on each side (total area 13,000 sq ft or 1,208 sq m). It would have traveled from the Sun-Earth L1 Lagrangian point 900,000 miles from Earth (1.5 million km) to a distance of 1,864,114 miles (3 million kilometers). The demonstration was expected to launch on a Falcon 9 in January 2015. It would have been a secondary payload, released after the placement of the DSCOVR climate satellite at the L1 point. Citing a lack of confidence in its contractor's ability to deliver, the mission was cancelled in October 2014.
LightSail-1.
The Planetary Society of the United States plans to do a short test of an artificial satellite "LightSail-1", to be launched on 20 May 2015. This launch would deliver the satellite to an orbit low enough that atmospheric drag exceeds the thrust available from the light sail, but will allow a full checkout of the satellite's systems in advance of the main 2016 mission.
Gossamer deorbit sail.
, the European Space Agency (ESA) has a proposed deorbit sail, named "Gossamer", that would be intended to be used to accelerate the deorbiting of small (less than 700 kg) artificial satellites from low-Earth orbits. The launch mass is 2 kg with a launch volume of only 15 *. Once deployed, the sail would expand to 5 x and would use a combination of solar pressure on the sail and increased atmospheric drag to accelerate satellite reentry.
NEA Scout.
The Near-Earth Asteroid Scout (NEA Scout) is a proposed mission concept by NASA to develop a controllable low-cost CubeSat solar sail spacecraft capable of encountering near-Earth asteroids (NEA). Four 7-m booms would deploy, unfurling the 83 m2 aluminized polyimide solar sail.
In science fiction.
The earliest reference to solar sailing was in Jules Verne's 1865 novel "From the Earth to the Moon", coming only a year after Maxwell's equations were published. The next known publication came more than 20 years later when Georges Le Faure and Henri De Graffigny published a four-volume science fiction novel in 1889, "The Extraordinary Adventures of a Russian Scientist", which included a spacecraft propelled by solar pressure. B. Krasnogorskii published "On the Waves of the Ether" in 1913. In his story backed by technical calculations, a small, bullet-shaped capsule is surrounded by a circular mirror 35 meters in diameter. It travels through space by means of solar pressure on the mirror.
One of the earliest American stories about light sails is "The Lady Who Sailed the Soul" by Cordwainer Smith, which was published in 1960. In it, a tragedy results from the slowness of interstellar travel by this method. Another example is the 1962 story "Gateway to Strangeness" (also known as "Sail 25") by Jack Vance, in which the outward direction of propulsion poses a life-threatening dilemma. Also in early 20th century literature, Pierre Boulle's "Planet of the Apes" novel starts with a couple floating in space on a ship propelled and maneuvered by light sails. In Larry Niven and Jerry Pournelle's "The Mote in God's Eye", a sail is used as a brake and a weapon. Author and scientist Arthur C. Clarke depicted a "yacht race" between solar sail spacecraft in the 1964 short story "Sunjammer". In "The Flight of the Dragonfly", Robert Forward (who also proposed the microwave-pushed Starwisp design) described an interstellar journey using a light driven propulsion system, wherein a part of the sail was broken off and used as a reflector to slow the main spacecraft as it approached its destination. Forward's ideas were developed further in Charles Stross's novel "Accelerando". In the 1982 film "Tron", a "Solar Sailer" was an inner spacecraft with butterfly like sails moved along focused beam of light. The 1983 episode "Enlightenment" of "Doctor Who" featured sailing ships in space that used solar wind to fly. In the episode "" of ' that aired in 1995, a reconstructed, "ancient" Bajoran "light ship" was featured. It was designed to use solar wind to fly out of a Solar System with no engine. In the film ' one is used by Count Dooku to propel himself across space. A solar sail was also used in James Cameron's "Avatar". In the Disney film "Treasure Planet", solar sails are used literally as sails for interstellar travel as well as serving for the photovoltaic gathering of energy for the jet propulsion of a steampunk-styled masted sailing ship capable of traveling through space.

</doc>
<doc id="29425" url="http://en.wikipedia.org/wiki?curid=29425" title="Sabellianism">
Sabellianism

In Christianity, Sabellianism in the Eastern church or Patripassianism in the Western church (also known as modalism, modalistic monarchianism, or modal monarchism) is the nontrinitarian or anti-trinitarian belief that the Heavenly Father, Resurrected Son, and Holy Spirit are three different "modes" or "aspects" of one monadic God, as perceived by "the believer", rather than three distinct persons within "the Godhead"--that there are no real or substantial differences among the three, such that there is no substantial identity for the Spirit or the Son.
The term Sabellianism comes from Sabellius, who was a theologian and priest from the 3rd century.
Meaning and origins.
God was said to have three "faces" or "masks" (Greek πρόσωπα "prosopa"; Latin "personae"). Modalists note that the only number ascribed to God in the Holy Bible is "One" and that there is no inherent threeness ascribed to God explicitly in scripture. The number three is never mentioned in relation to God in scripture, which of course is the number that is central to the word "Trinity". The only possible exceptions to this are the Great Commission Matthew 28:16-20, 2 Corinthians 13:14, and the Comma Johanneum, which many regard as a spurious text passage in First John (1 John 5:7) known primarily from the King James Version and some versions of the Textus Receptus but not included in modern critical texts. It is also suggested by some modern "Oneness Pentecostal" critics, that Matthew 28:19 is not part of the original text, because Eusebius of Caesarea quoted it by saying "In my name", and there is no mention of baptism in the verse. Eusebius did, however, quote the trinitarian formula in his later writings. (Conybeare ("Hibbert Journal" i (1902-3), page 102). Matthew 28:19 is quoted also in the Didache (Didache 7:1, which dates to the late 1st Century, early 2nd Century, and in Diatesseron, a mid 2nd Century harmony of the Synoptic Gospels (Diatesseron 55:5-7). The "Shem-Tob's Hebrew Gospel of Matthew" (George Howard), written during the 14th century, also has no reference of baptism or a trinitarian formula in Matthew 28:19. However, it is also true that no Greek manuscript of the Gospel of Matthew has ever been found which does not contain Matthew 28:19. The earliest extant copies of Matthew's Gospel date to the 3rd Century, and they contain Matthew 28:19. Therefore, scholars generally agree that Matthew 28:19 is likely part of the original Gospel of Matthew, though a minority disputes this.
Trinitarians believe that all three members of the Trinity were present as seemingly distinct persons at Jesus' baptism, and believe there is other scriptural evidence for Trinitarianism (see main page for details). Modalism has been mainly associated with Sabellius, who taught a form of it in Rome in the 3rd century. This had come to him via the teachings of Noetus and Praxeas.
Hippolytus of Rome knew Sabellius personally and mentioned him in the "Philosophumena". He knew Sabellius disliked Trinitarian theology, yet he called Modal Monarchism the heresy of Noetus, not that of Sabellius. Sabellianism was embraced by Christians in Cyrenaica, to whom Demetrius, Patriarch of Alexandria, wrote letters arguing against this belief.
Modalism teaches that the Heavenly Father, Resurrected Son and Holy Spirit identified by the Trinity Doctrine are different "modes" or "aspects" of the One God, as perceived by "the believer", rather than "three coeternal persons" within "the Godhead". In passages of scripture such as where the Son, Father, and Holy Spirit are separated in the text, they view this phenomenon as confirming God's omnipresence, and His ability to manifest himself as he pleases. Oneness Pentecostals and Modalists dispute the traditional Trinitarian doctrine, while affirming the Christian doctrine of God taking on flesh as Jesus Christ. Like Trinitarians, Oneness adherents believe that Jesus Christ is fully God and fully man. However, whereas Trinitarians believe that "God the Son", the eternal second person of the Trinity, became man, Oneness adherents hold that the one and only true God—who manifests himself in any way he chooses, including as Father, Son and Holy Spirit—became man. Oneness Pentecostals and other modalists are regarded by Catholic, Orthodox, and some other mainstream Christians as heretical for rejecting the Trinity Doctrine, which they regard as equivalent to Unitarianism. Modalists differentiate themselves from Unitarians by affirming Christ's Godhead. Oneness teaches that there is only one being, revealing himself in different ways. Explains the Oneness view of God, as opposed to the Trinitarian viewpoint. Modalists cite passages in the New Testament that refer to God in the singular, and note the lack of the word "Trinity" in any canonical scripture. They claim that refers to Christ's relationship with the Father in a similar sense:
He is the image of the invisible God, the firstborn of all creation. For by him all things were created, in heaven and on earth, visible and invisible, whether thrones or dominions or rulers or authorities; all things were created through him and for him. And he is before all things, and in him all things hold together. And he is the head of the body, the church. He is the beginning, the firstborn from the dead, that in everything he might be preeminent. For in him all the fullness of God was pleased to dwell, and through him to reconcile to himself all things, whether on earth or in heaven, making peace by the blood of his cross.
They also cite Christ's response to Philip's query on who the Father was in :
 Jesus answered: "Don't you know me, Philip, even after I have been among you such a long time? Anyone who has seen me has seen the Father. How can you say, 'Show us the Father'? 
A notable modern adherent of Modalism is T.D. Jakes.
Ancient opposition.
The chief critic of Sabellianism was Tertullian, who labelled the movement "Patripassianism", from the Latin words "pater" for "father", and "passus" from the verb "to suffer" because it implied that the Father suffered on the Cross. It was coined by Tertullian in his work "Adversus Praxeas", Chapter I, "By this Praxeas did a twofold service for the devil at Rome: he drove away prophecy, and he brought in heresy; he put to flight the Paraclete, and he crucified the Father."
It is important to note that our only sources extant for our understanding of Sabellianism are from their detractors. Scholars today are not in agreement as to what exactly Sabellius or Praxeas taught. It is easy to suppose Tertullian and Hippolytus misrepresented the opinions of their opponents.
Tertullian seems to suggest that most of the unwise and unlearned believers at that time favoured the Sabellian view of the oneness of God. Epiphanius(Haeres 62) about 375 notes that the adherents of Sabellius were still to be found in great numbers, both in Mesopotamia and at Rome. The first general council at Constantinople in 381 in canon VII and the third general council at Constantinople in 680 in canon XCV declared the baptism of Sabellius to be invalid, which indicates that Sabellianism was still extant. 
Historic Sabellianism taught that God the Father was the only true existence of the Godhead, a belief known as Monarchianism. One author has described Sabellius' teaching thus: "The true question, therefore, turns on this, viz., what is it which constitutes what we name ‘person’ in the Godhead? Is it original, substantial, essential to divinity itself? Or does it belong to and arise from the exhibitions and developments which the divine Being has made of himself to his creatures? The former Sabellius denied; the latter he fully admitted."
It has been noted that the Greek term "homoousian" or "con-substantial", which Athanasius of Alexandria favoured, was actually a term reported to be put forth by Sabellius, and was a term that many followers of Athanasius were uneasy about. Their objection to the term "homoousian" was that it was considered to be un-Scriptural, suspicious, and "of a Sabellian tendency." This was because Sabellius also considered the Father and the Son to be "one substance." Meaning that, to Sabellius, the Father and Son were one essential person, though operating as different manifestations or faces.
Sabellianism has been rejected by the majority of Christian churches in favour of Trinitarianism, which was eventually defined as three distinct, co-equal, co-eternal persons by the Athanasian Creed late in the 4th century.
Eastern Orthodox view.
The Greek Orthodox teach that God is not of a substance that is comprehensible since God the Father has no origin and is eternal and infinite. That it is improper to speak of things as physical and metaphysical but rather it is Christian to speak of things as created and uncreated. God the Father is the origin, source of the Trinity not God in substance or essence. Therefore the consciousness of God is not obtainable to created beings either in this life or the next (see apophatism), though through co-operation with God (called theosis) Mankind can become good (God-like) and from such a perspective reconcile himself to the Knowledge of Good and the Knowledge of Evil he consumed in the Garden of Eden (see the Fall of Man). Thus returning himself to the proper relationship with his creator and source of being.
Current adherents.
At the Arroyo Seco World Wide Camp Meeting, near Los Angeles, in 1913, Canadian evangelist R. E. McAlister stated at a baptismal service that the apostles had baptized in the name of Jesus only and note in the triune Name of Father, Son, and Holy Spirit. Later that night, John G. Schaeppe, a German immigrant, had a vision of Jesus and woke up the camp shouting that the name of Jesus needed to be glorified. From that point, Frank J. Ewart began requiring that anyone baptized using the Trinitarian formula needed to be rebaptized in the name of Jesus “only.” Support for this position began to spread, a long a belief in one Person in the Godhead, acting in different modes or offices.
The General Council of the Assemblies of God convened in St. Louis in October 1916, to confirm their belief in Trinitarian orthodoxy. The Oneness camp was faced by a majority who required acceptance of the Trinitarian baptismal formula and the orthodox doctrine of the Trinity or removem themselves from the denomination. In the end, about a quarter of the ministers withdrew.
Oneness Pentecostalism teaches that God is one Person, and that the Father (a spirit) is united with Jesus (a man) as the Son of God. However, Oneness Pentecostalism differs somewhat by rejecting sequential modalism, and by the full acceptance of the begotten humanity of the Son, not eternally begotten, who was the man Jesus and was born, crucified, and risen, and not the deity. This directly opposes Patripassianism and the pre-existence of the Son, which Sabellianism does not.
Oneness Pentecostals believe that Jesus was "Son" only when he became flesh on earth, but was the Father before being made man. They refer to the Father as the "Spirit" and the Son as the "Flesh". But they believe that Jesus and the Father are one essential Person. Though operating as different "manifestations" or "modes". Oneness Pentecostals reject the Trinity doctrine, viewing it as pagan and un-Scriptural, and hold to the Jesus' Name doctrine with respect to baptisms. They are often referred to as "Modalists" or "Sabellians" or "Jesus Only". Oneness Pentecostalism can be compared to Sabellianism, or can be described as holding to a form of Sabellianism, as both are Nontrinitarian, and as both believe that Jesus was "Almighty God in the Flesh", but they do not totally identify each other.
However, it cannot be certain whether Sabellius taught Modalism as it is taught today as Oneness doctrine, since only a few fragments of his writings are extant and, therefore, all we have of his teachings comes through the writing of his detractors.
The following excerpts which demonstrate some of the known doctrinal characteristics of ancient Sabellians may be seen to compare with the doctrines in the modern Oneness movement:

</doc>
<doc id="29426" url="http://en.wikipedia.org/wiki?curid=29426" title="Sino-Indian War">
Sino-Indian War

The Sino-Indian War (Hindi: भारत-चीन युद्ध "Bhārat-Chīn Yuddh"), also known as the Sino-Indian Border Conflict (), was a war between China and India that occurred in 1962. A disputed Himalayan border was the main pretext for war, but other issues played a role. There had been a series of violent border incidents after the 1959 Tibetan uprising, when India had granted asylum to the Dalai Lama. India initiated a Forward Policy in which it placed outposts along the border, including several north of the McMahon Line, the eastern portion of a Line of Actual Control proclaimed by Chinese Premier Zhou Enlai in 1959.
Unable to reach political accommodation on disputed territory along the 3,225-kilometre-long Himalayan border, the Chinese launched simultaneous offensives in Ladakh and across the McMahon Line on 20 October 1962. Chinese troops advanced over Indian forces in both theatres, capturing Rezang la in Chushul in the western theatre, as well as Tawang in the eastern theatre. The war ended when the Chinese declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal from the disputed area.
The Sino-Indian War is notable for the harsh mountain conditions under which much of the fighting took place, entailing large-scale combat at altitudes of over 4,000 metres (14,000 feet). The Sino-Indian War was also noted for the non-deployment of the navy or air force by either the Chinese or Indian side.
Location.
China and India shared a long border, sectioned into three stretches by Nepal, Sikkim (then an Indian protectorate), and Bhutan, which follows the Himalayas between Burma and what was then West Pakistan. A number of disputed regions lie along this border. At its western end is the Aksai Chin region, an area the size of Switzerland, that sits between the Chinese autonomous region of Xinjiang and Tibet (which China declared as an autonomous region in 1965). The eastern border, between Burma and Bhutan, comprises the present Indian state of Arunachal Pradesh (formerly the North East Frontier Agency). Both of these regions were overrun by China in the 1962 conflict.
Most combat took place at high altitudes. The Aksai Chin region is a desert of salt flats around 5,000 metres above sea level, and Arunachal Pradesh is mountainous with a number of peaks exceeding 7000 metres. The Chinese Army had possession of one of the highest ridges in the regions. The high altitude and freezing conditions also cause logistical and welfare difficulties; in past similar conflicts (such as the Italian Campaign of World War I) more casualties have been caused by the harsh conditions than enemy action. The Sino-Indian War was no different, with many troops on both sides dying in the freezing cold.
Background.
The cause of the war was a dispute over the sovereignty of the widely separated Aksai Chin and Arunachal Pradesh border regions. Aksai Chin, claimed by India to belong to Kashmir and by China to be part of Xinjiang, contains an important road link that connects the Chinese regions of Tibet and Xinjiang. China's construction of this road was one of the triggers of the conflict.
Aksai Chin.
The western portion of the Sino-Indian boundary originated in 1834, with the Sikh Confederacy's conquest of Ladakh. In 1842, the Sikh Confederacy, which at the time ruled over much of Northern India (including the frontier regions of Jammu and Kashmir), signed a treaty which guaranteed the integrity of its existing borders with its neighbours. The British defeat of the Sikhs in 1846 resulted in transfer of sovereignty over Ladakh, part of the Jammu and Kashmir region, to the British, and British commissioners contacted Chinese officials to negotiate the border. The boundaries at its two extremities, Pangong Lake and Karakoram Pass, were well defined, but the Aksai Chin area in between lay undefined.
W. H. Johnson, a civil servant with the Survey of India, proposed the "Johnson Line" in 1865, which put Aksai Chin in Kashmir. Johnson presented this line to the Maharaja of Kashmir, who then claimed the 18,000 square kilometres contained within. Johnson's work was severely criticized as inaccurate. His boundary line was described as "patently absurd", and extending further north than the Indian claim. Johnson was reprimanded by the British Government for crossing into Khotan without permission and resigned from the Survey. According to Francis Younghusband, who explored the region in the late 1880s, there was only an abandoned fort and not one inhabited house at Shahidulla when he was there - it was just a convenient staging post for the nomadic Kirghiz. The abandoned fort had apparently been built a few years earlier by the Kashmiris. In 1878 the Chinese had reconquered Xinjiang, and by 1890 they already had Shahidulla before the issue was decided. By 1892, China had erected boundary markers at Karakoram Pass.
In 1893, Hung Ta-chen, a senior Chinese official at Kashgar, handed a map of the boundary proposed by China to George Macartney, the British consul-general at Kashgar. This boundary placed the Lingzi Tang plains, which are south of the Laktsang range, in India, and Aksai Chin proper, which is north of the Laktsang range, in China. Macartney agreed with the proposal and forwarded it to the British Indian government. The British presented this line, known as the Macartney-MacDonald line, to the Chinese in 1899 in a note by Sir Claude MacDonald. In 1911 the Xinhai Revolution resulted in power shifts in China, and by the end of World War I, the British officially used the Johnson Line. However they took no steps to establish outposts or assert actual control on the ground. According to Neville Maxwell, the British had used as many as 11 different boundary lines in the region, as their claims shifted with the political situation. From 1917 to 1933, the "Postal Atlas of China", published by the Government of China in Peking had shown the boundary in Aksai Chin as per the Johnson line, which runs along the Kunlun mountains. The "Peking University Atlas", published in 1925, also put the Aksai Chin in India.:101 Upon independence in 1947, the government of India used the Johnson Line as the basis for its official boundary in the west, which included the Aksai Chin. On 1 July 1954, India's first Prime Minister Jawaharlal Nehru definitively stated the Indian position, claiming that Aksai Chin had been part of the Indian Ladakh region for centuries, and that the border (as defined by the Johnson Line) was non-negotiable. According to George N. Patterson, when the Indian government finally produced a report detailing the alleged proof of India's claims to the disputed area, "the quality of the Indian evidence was very poor, including some very dubious sources indeed".:275
In 1956–57, China constructed a road through Aksai Chin, connecting Xinjiang and Tibet, which ran south of the Johnson Line in many places. Aksai Chin was easily accessible to the Chinese, but access from India, which meant negotiating the Karakoram mountains, was much more difficult. The road came on Chinese maps published in 1958.
The McMahon Line.
In 1826, British India gained a common border with China after the British wrested control of Manipur and Assam from the Burmese, following the First Anglo-Burmese War of 1824–1826. In 1847, Major J. Jenkins, agent for the North East Frontier, reported that the Tawang was part of Tibet. In 1872, four monastic officials from Tibet arrived in Tawang and supervised a boundary settlement with Major R. Graham, NEFA official, which included the Tawang Tract as part of Tibet. Thus, in the last half of the 19th century, it was clear that the British treated the Tawang Tract as part of Tibet. This boundary was confirmed in a 1 June 1912 note from the British General Staff in India, stating that the "present boundary (demarcated) is south of Tawang, running westwards along the foothills from near Ugalguri to the southern Bhutanese border." A 1908 map of The Province of Eastern Bengal and Assam prepared for the Foreign Department of the Government of India, showed the international boundary from Bhutan continuing to the Baroi River, following the Himalayas foothill alignment. In 1913, representatives of Great Britain, China and Tibet attended a conference in Simla regarding the borders between Tibet, China and British India. Whilst all three representatives initialed the agreement, Beijing later objected to the proposed boundary between the regions of Outer Tibet and Inner Tibet, and did not ratify it. The details of the Indo-Tibetan boundary was not revealed to China at the time. The foreign secretary of the British Indian government, Henry McMahon, who had drawn up the proposal, decided to bypass the Chinese (although instructed not to by his superiors) and settle the border bilaterally by negotiating directly with Tibet. According to later Indian claims, this border was intended to run through the highest ridges of the Himalayas, as the areas south of the Himalayas were traditionally Indian. However, the McMahon Line lay south of the boundary India claims. India's government held the view that the Himalayas were the ancient boundaries of the Indian subcontinent, and thus should be the modern boundaries of India, while it is the position of the Chinese government that the disputed area in the Himalayas have been geographically and culturally part of Tibet since ancient times.
Months after the Simla agreement, China set up boundary markers south of the McMahon Line. T. O'Callaghan, an official in the Eastern Sector of the North East Frontier, relocated all these markers to a location slightly south of the McMahon Line, and then visited Rima to confirm with Tibetan officials that there was no Chinese influence in the area. The British-run Government of India initially rejected the Simla Agreement as incompatible with the Anglo-Russian Convention of 1907, which stipulated that neither party was to negotiate with Tibet "except through the intermediary of the Chinese government". The British and Russians cancelled the 1907 agreement by joint consent in 1921. It was not until the late 1930s that the British started to use the McMahon Line on official maps of the region.
China took the position that the Tibetan government should not have been allowed to make a such a treaty, rejecting Tibet's claims of independent rule. For its part, Tibet did not object to any section of the McMahon Line excepting the demarcation of the trading town of Tawang, which the Line placed under British-Indian jurisdiction. However, up until World War II, Tibetan officials were allowed to administer Tawang with complete authority. Due to the increased threat of Japanese and Chinese expansion during this period, British Indian troops secured the town as part of the defence of India's eastern border.
In the 1950s, India began actively patrolling the region. It found that, at multiple locations, the highest ridges actually fell north of the McMahon Line. Given India's historic position that the original intent of the line was to separate the two nations by the highest mountains in the world, in these locations India extended its forward posts northward to the ridges, regarding this move as compliant with the original border proposal, although the Simla Convention did not explicitly state this intention.
Events leading up to war.
Tibet and the border dispute.
The 1940s saw huge change in South Asia with the Partition of India in 1947 (resulting in the establishment of the two new states of India and Pakistan), and the establishment of the People's Republic of China (PRC) in 1949. One of the most basic policies for the new Indian government was that of maintaining cordial relations with China, reviving its ancient friendly ties. India was among the first nations to grant diplomatic recognition to the newly created PRC.
At the time, Chinese officials issued no condemnation of Nehru's claims or made any opposition to Nehru's open declarations of control over Aksai Chin. In 1956, Chinese Premier Zhou Enlai stated that he had no claims over Indian controlled territory. He later argued that Aksai Chin was already under Chinese jurisdiction and that the McCartney MacDonald Line was the line China could accept. Zhou later argued that as the boundary was undemarcated and had never been defined by treaty between any Chinese or Indian government, the Indian government could not unilaterally define Aksai Chin's borders.
In 1950, the Chinese People's Liberation Army annexed Tibet and later the Chinese extended their influence by building a road in 1956–67 and placing border posts in Aksai Chin. India found out after the road was completed, protested against these moves and decided to look for a diplomatic solution to ensure a stable Sino-Indian border. To resolve any doubts about the Indian position, Prime Minister Jawaharlal Nehru declared in parliament that India regarded the McMahon Line as its official border. The Chinese expressed no concern at this statement, and in 1951 and 1952, the government of China asserted that there were no frontier issues to be taken up with India.
In 1954, Prime Minister Nehru wrote a memo calling for India's borders to be clearly defined and demarcated; in line with previous Indian philosophy, Indian maps showed a border that, in some places, lay north of the McMahon Line. Chinese Premier Zhou Enlai, in November 1956, again repeated Chinese assurances that the People's Republic had no claims on Indian territory, although official Chinese maps showed 120000 km2 of territory claimed by India as Chinese. CIA documents created at the time revealed that Nehru had ignored Burmese premier Ba Swe when he warned Nehru to be cautious when dealing with Zhou. They also allege that Zhou purposefully told Nehru that there were no border issues with India.
In 1954, China and India negotiated the Five Principles of Peaceful Coexistence, by which the two nations agreed to abide in settling their disputes. India presented a frontier map which was accepted by China, and the slogan "Hindi-Chini bhai-bhai" (Indians and Chinese are brothers) was popular then. However, Nehru in 1958, had privately told G. Parthasarathi, the Indian envoy to China not to trust the Chinese at all and send all communications directly to him, bypassing the Defence Minister VK Krishna Menon since his communist background clouded his thinking about China. According to Georgia Tech political analyst John W Garver, Nehru's policy on Tibet was to create a strong Sino-Indian partnership which would be catalysed through agreement and compromise on Tibet. Garver believes that Nehru's previous actions had given him confidence that China would be ready to form an "Asian Axis" with India.
This apparent progress in relations suffered a major setback when, in 1959, Nehru accommodated the Tibetan religious leader at the time, the 14th Dalai Lama, who fled Lhasa after a failed Tibetan uprising against Chinese rule. The Chairman of the Chinese Communist Party, Mao Zedong, was enraged and asked the Xinhua News Agency to produce reports on Indian expansionists operating in Tibet.
Border incidents continued through this period. In August 1959, the People's Liberation Army took an Indian prisoner at Longju, which had an ambiguous position in the McMahon Line, and two months later in Aksai Chin, a clash led to the death of nine Indian frontier policemen.
On 2 October, Soviet Premier Nikita Khrushchev defended Nehru in a meeting with Mao. This action reinforced China's impression that the Soviet Union, the United States and India all had expansionist designs on China. The People's Liberation Army went so far as to prepare a self-defence counterattack plan. Negotiations were restarted between the nations, but no progress was made.
As a consequence of their non-recognition of the McMahon Line, China's maps showed both the North East Frontier Area (NEFA) and Aksai Chin to be Chinese territory. In 1960, Zhou Enlai unofficially suggested that India drop its claims to Aksai Chin in return for a Chinese withdrawal of claims over NEFA. Adhering to his stated position, Nehru believed that China did not have a legitimate claim over either of these territories, and thus was not ready to concede them. This adamant stance was perceived in China as Indian opposition to Chinese rule in Tibet. Nehru declined to conduct any negotiations on the boundary until Chinese troops withdrew from Aksai Chin, a position supported by the international community. India produced numerous reports on the negotiations, and translated Chinese reports into English to help inform the international debate. China believed that India was simply securing its claim lines in order to continue its "grand plans in Tibet". India's stance that China withdraw from Aksai Chin caused continual deterioration of the diplomatic situation to the point that internal forces were pressuring Nehru to take a military stance against China.
1960 meetings to resolve the boundary question.
In 1960, based on an agreement between Nehru and Chou En-Lai, officials from India and China held discussions in order to settle the boundary dispute.:91 China and India disagreed on the major watershed that defined the boundary in the western sector.:96 The Chinese statements with respect to their border claims often misrepresented the cited sources.:99
The Forward Policy.
At the beginning of 1961, Nehru appointed General B. M. Kaul as army Chief of General Staff, but he refused to increase military spending and prepare for a possible war. According to James Barnard Calvin of the U.S. Navy, in 1959, India started sending Indian troops and border patrols into disputed areas. This program created both skirmishes and deteriorating relations between India and China. The aim of this policy was to create outposts behind advancing Chinese troops to interdict their supplies, forcing them north of the disputed line. There were eventually 60 such outposts, including 43 north of the McMahon Line, to which India claimed sovereignty. China viewed this as further confirmation of Indian expansionist plans directed towards Tibet. According to the Indian official history, implementation of the Forward Policy was intended to provide evidence of Indian occupation in the previously unoccupied region through which Chinese troops had been advancing. Kaul was confident, through contact with Indian Intelligence and CIA information, that China would not react with force. Indeed, at first the PLA simply withdrew, but eventually Chinese forces began to counter-encircle the Indian positions which clearly encroached into the north of McMahon Line. This led to a tit-for-tat Indian reaction, with each force attempting to outmanoeuver the other. However, despite the escalating nature of the dispute, the two forces withheld from engaging each other directly.
Chinese attention was diverted for a time by the military activity of the Nationalists on Taiwan, but on 23 June the U.S. assured China that a Nationalist invasion would not be permitted. China's heavy artillery facing Taiwan could then be moved to Tibet. It took China six to eight months to gather the resources needed for the war, according to Anil Athale, author of the official Indian history. The Chinese sent a large quantity of non-military supplies to Tibet through the Indian port of Calcutta.
Early incidents.
Various border conflicts and "military incidents" between India and China flared up throughout the summer and autumn of 1962. In May, the Indian Air Force was told not to plan for close air support, although it was assessed as being a feasible way to counter the unfavourable ratio of Chinese to Indian troops. In June, a skirmish caused the deaths of dozens of Chinese troops. The Indian Intelligence Bureau received information about a Chinese buildup along the border which could be a precursor to war.
During June–July 1962, Indian military planners began advocating "probing actions" against the Chinese, and accordingly, moved mountain troops forward to cut off Chinese supply lines. According to Patterson, the Indian motives were threefold:
On 10 July 1962, 350 Chinese troops surrounded an Indian occupied post in Chushul (north of the McMahon Line) but withdrew after a heated argument via loudspeaker. On 22 July, the Forward Policy was extended to allow Indian troops to push back Chinese troops already established in disputed territory. Whereas Indian troops were previously ordered to fire only in self-defence, all post commanders were now given discretion to open fire upon Chinese forces if threatened. In August, the Chinese military improved its combat readiness along the McMahon Line and began stockpiling ammunition, weapons and gasoline.
Given his foreknowledge of the coming Cuban Missile Crisis, Mao Zedong was able to persuade Nikita Khrushchev to reverse the Russian policy of backing India, at least temporarily. In mid-October, the Communist organ "Pravda" encouraged peace between India and China. When the Cuban Missile Crisis ended and Mao's rhetoric changed, however, Russia reversed course.
Confrontation at Thag La.
In June 1962, Indian forces established an outpost at Dhola, on the southern slopes of the Thag La Ridge. Dhola lay north of the McMahon Line but south of the ridges along which India interpreted the McMahon Line to run. In August, China issued diplomatic protests and began occupying positions at the top of Thag La. On 8 September, a 60-strong PLA unit descended to the south side of the ridge and occupied positions that dominated one of the Indian posts at Dhola. Fire was not exchanged, but Nehru said to the media that the Indian Army had instructions to "free our territory" and the troops had been given discretion to use force. On 11 September, it was decided that "all forward posts and patrols were given permission to fire on any armed Chinese who entered Indian territory".
However, the operation to occupy Thag La was flawed in that Nehru's directives were unclear and it got underway very slowly because of this. In addition to this, each man had to carry 35 kg over the long trek and this severely slowed down the reaction. By the time the Indian battalion reached the point of conflict, Chinese units controlled both banks of the Namka Chu River. On 20 September, Chinese troops threw grenades at Indian troops and a firefight developed, triggering a long series of skirmishes for the rest of September.
Some Indian troops, including Brigadier Dalvi who commanded the forces at Thag La, were also concerned that the territory they were fighting for was not strictly territory that "we should have been convinced was ours". According to Neville Maxwell, even members of the Indian defence ministry were categorically concerned with the validity of the fighting in Thag La.
On 3 October, a week before the start of the war, Zhou Enlai visited Nehru in New Delhi promising there would be no war. On 4 October, Kaul assigned some troops to secure regions south of the Thag La Ridge. Kaul decided to first secure Yumtso La, a strategically important position, before re-entering the lost Dhola post. Kaul had then realised that the attack would be desperate and the Indian government tried to stop an escalation into all-out war. Indian troops marching to Thag La had suffered in the previously unexperienced conditions; two Gurkha soldiers died of pulmonary edema.
On 10 October, an Indian Punjabi patrol of 50 troops to Yumtso La were met by an emplaced Chinese position of some 1,000 soldiers. Indian troops were in no position for battle, as Yumtso La was 16,000 feet (4,900 m) above sea level and Kaul did not plan on having artillery support for the troops. The Chinese troops opened fire on the Indians under their belief that they were north of the McMahon Line. The Indians were surrounded by Chinese positions which used mortar fire. However, they managed to hold off the first Chinese assault, inflicting heavy casualties.
At this point, the Indian troops were in a position to push the Chinese back with mortar and machine gun fire. However, Brigadier Dalvi opted not to fire, as it would mean decimating the Rajput who were still in the area of the Chinese regrouping. They helplessly watched the Chinese ready themselves for a second assault. In the second Chinese assault, the Indians began their retreat, realising the situation was hopeless. The Indian patrol suffered 25 casualties, and the Chinese 33. The Chinese troops held their fire as the Indians retreated, and then buried the Indian dead with military honours, as witnessed by the retreating soldiers. This was the first occurrence of heavy fighting in the war.
This attack had grave implications for India and Nehru tried to solve the issue, but by 18 October, it was clear that the Chinese were preparing for an attack on India, with massive troop buildups on the border. A long line of mules and porters had also been observed supporting the buildup and reinforcement of positions south of the Thag La Ridge.
Chinese and Indian preparations.
Motives.
Two of the major factors leading up to China's eventual conflicts with Indian troops were India's stance on the disputed borders and perceived Indian subversion in Tibet. There was "a perceived need to punish and end perceived Indian efforts to undermine Chinese control of Tibet, Indian efforts which were perceived as having the objective of restoring the pre-1949 status quo ante of Tibet". The other was "a perceived need to punish and end perceived Indian aggression against Chinese territory along the border". John W. Garver argues that the first perception was incorrect based on the state of the Indian military and polity in the 1960s. It was, nevertheless a major reason for China's going to war. However, he argues the Chinese perception of Indian aggression to be "substantially accurate".
The CIA's recently declassified POLO documents reveal contemporary American analysis of Chinese motives during the war. According to this document, "Chinese apparently were motivated to attack by one primary consideration — their determination to retain the ground on which PLA forces stood in 1962 and to punish the Indians for trying to take that ground". In general terms, they tried to show the Indians once and for all that China would not acquiesce in a military "reoccupation" policy. The secondary reasons for the attack, which had made it desirable but not necessary, included a desire :
Another factor which might have affected China's decision for war with India was a perceived need to stop a Soviet-U.S.-India encirclement and isolation of China. India's relations with the Soviet Union and United States were both strong at this time, but the Soviets (and Americans) were preoccupied by the Cuban Missile Crisis and would not interfere with the Sino-Indian War. P. B. Sinha suggests that China waited until October to attack because the timing of the war was exactly in parallel with American actions so as to avoid any chance of American or Soviet involvement. Although American buildup of forces around Cuba occurred on the same day as the first major clash at Dhola, and China's buildup between 10 and 20 October appeared to coincide exactly with the United States establishment of a blockade against Cuba which began 20 October, the Chinese probably prepared for this before they could anticipate what would happen in Cuba. Another explanation is that the confrontation in the Taiwan Strait had eased by then.
Garver argues that the Chinese correctly assessed Indian border policies, particularly the Forward Policy, as attempts for incremental seizure of Chinese-controlled territory. On Tibet, Garver argues that one of the major factors leading to China's decision for war with India was a common tendency of humans "to attribute others behavior to interior motivations, while attributing their own behavior to situational factors". Studies from China published in the 1990s confirmed that the root cause for China going to war with India was the perceived Indian aggression in Tibet, with the forward policy simply catalysing the Chinese reaction.
Neville Maxwell and Allen Whiting argue that the Chinese leadership believed they were defending territory that was legitimately Chinese, and which was already under de facto Chinese occupation prior to Indian advances, and regarded the Forward Policy as an Indian attempt at creeping annexation. Mao Zedong himself compared the Forward Policy to a strategic advance in Chinese chess:
Their [India's] continually pushing forward is like crossing the Chu Han boundary. What should we do? We can also set out a few pawns, on our side of the river. If they don't then cross over, that’s great. If they do cross, we'll eat them up [chess metaphor meaning to take the opponent's pieces]. Of course, we cannot blindly eat them. Lack of forbearance in small matters upsets great plans. We must pay attention to the situation.
India claims that the motive for the Forward Policy was to cut off the supply routes for Chinese troops posted in NEFA and Aksai Chin. According to the official Indian history, the forward policy was continued because of its initial success, as it claimed that Chinese troops withdrew when they encountered areas already occupied by Indian troops. It also claimed that the Forward Policy was having success in cutting out supply lines of Chinese troops who had advanced South of the McMahon Line, though there was no evidence of such advance before the 1962 war. However, the Forward Policy rested on the assumption that Chinese forces "were not likely to use force against any of our posts, even if they were in a position to do so". No serious re-appraisal of this policy took place even when Chinese forces ceased withdrawing. Nehru's confidence was probably justified given the difficulty for China to supply the area over the high altitude terrain over 5000 km from the more populated areas of China.
The Chinese leadership initially held a sympathetic view towards India as the latter had been ruled by British colonial masters for centuries. However, Nehru's forward policy convinced PRC leadership that the independent Indian leadership was a reincarnation of British imperialism. Mao Zedong stated: "Rather than being constantly accused of aggression, it's better to show the world what really happens when China indeed moves its muscles."
Chinese policy toward India, therefore, operated on two contradictory assumptions in the first half of 1961. On the one hand, the Chinese leaders continued to entertain a hope, although a shrinking one, that some opening for talks.would appear. On, the other hand, they read Indian statements and actions as clear signs that Nehru wanted to talk only about a Chinese withdrawal. Regarding the hope, they were willing to negotiate and tried to prod Nehru into a similar attitude. Regarding Indian intentions, they began to act politically and to build a rationale based on the assumption that Nehru already had become a lackey of imperialism; for this reason he opposed border talks.
Krishna Menon is reported to have said that when he arrived in Geneva on 6 June 1961 for an international conference in Laos, Chinese officials in Chen Yi's delegation indicated that Chen might be interested in discussing the border dispute with him. At several private
meetings with Menon, Chen avoided any discussion of the dispute and Menon surmised that the Chinese wanted him to broach the matter first. He did not, as he was under instructions from Nehru to avoid taking the initiative, leaving the Chinese with the impression
that Nehru was unwilling to show any flexibility.
In September, the Chinese took a step toward criticizing Nehru openly in their commentary. After citing Indonesian and Burmese press criticism of Nehru by name, the Chinese critiqued his moderate remarks on colonialism (People's Daily Editorial, 9 September) - Somebody at the Non-Aligned Nations Conference advanced the argument that the era of classical colonialism is gone and dead...contrary to facts." This was a distortion of Nehru's remarks but appeared close enough to be credible. On the same day, Chen Yi referred to Nehru by implication at the Bulgarian embassy reception: 'Those who attempted to' deny history, ignore reality, and distort the truth and who attempted to divert the Conference from its important object have failed to gain support and were isolated." On 10 September, they dropped all circumlocutions and criticized him by name in a China Youth article and NCNA report—the first time in almost two years that they had commented extensively on the Prime Minister.
By early 1962, the Chinese leadership began to believe that India's intentions were to launch a massive attack against Chinese troops, and that the Indian leadership wanted a war. In 1961, the Indian army had been sent into Goa, a small region without any other international borders apart from the Indian one, after Portugal refused to surrender the exclave colony to the Indian Union. Although this action met little to no international protest or opposition, China saw it as an example of India's expansionist nature, especially in light of heated rhetoric from Indian politicians. India's Home Minister declared, "If the Chinese will not vacate the areas occupied by it, India will have to repeat what it did in Goa. India will certainly drive out the Chinese forces", while another member of the Indian Congress Party pronounced, "India will take steps to end [Chinese] aggression on Indian soil just as it ended Portuguese aggression in Goa". By mid-1962, it was apparent to the Chinese leadership that negotiations had failed to make any progress, and the Forward Policy was increasingly perceived as a grave threat as Delhi increasingly sent probes deeper into border areas and cut off Chinese supply lines. Foreign Minister Marshal Chen Yi commented at one high-level meeting, "Nehru's forward policy is a knife. He wants to put it in our heart. We cannot close our eyes and await death." The Chinese leadership believed that their restraint on the issue was being perceived by India as weakness, leading to continued provocations, and that a major counterblow was needed to stop perceived Indian aggression.
Xu Yan, prominent Chinese military historian and professor at the PLA's National Defense University, gives an account of the Chinese leadership's decision to go to war. By late September 1962, the Chinese leadership had begun to reconsider their policy of "armed coexistence", which had failed to address their concerns with the forward policy and Tibet, and consider a large, decisive strike. On 22 September 1962, the "People's Daily" published an article which claimed that "the Chinese people were burning with 'great indignation' over the Indian actions on the border and that New Delhi could not 'now say that warning was not served in advance'."
Military planning.
The Indian side was confident war would not be triggered and made little preparations. India had only two divisions of troops in the region of the conflict. In August 1962, Brigadier D. K. Palit claimed that a war with China in the near future could be ruled out. Even in September 1962, when Indian troops were ordered to "expel the Chinese" from Thag La, Maj. General J. S. Dhillon expressed the opinion that "experience in Ladakh had shown that a few rounds fired at the Chinese would cause them to run away." Because of this, the Indian army was completely unprepared when the attack at Yumtso La occurred.
Recently declassified CIA documents which were compiled at the time reveal that India's estimates of Chinese capabilities made them neglect their military in favour of economic growth. It is claimed that if a more military-minded man had been in place instead of Nehru, India would have been more likely to have been ready for the threat of a counter-attack from China.
On 6 October 1962, the Chinese leadership convened. Lin Biao reported that PLA intelligence units had determined that Indian units might assault Chinese positions at Thag La on 10 October (Operation Leghorn). The Chinese leadership and the Central Military Council decided upon war to launch a large-scale attack to punish perceived military aggression from India. In Beijing, a larger meeting of Chinese military was convened in order to plan for the coming conflict.
Mao and the Chinese leadership issued a directive laying out the objectives for the war. A main assault would be launched in the eastern sector, which would be coordinated with a smaller assault in the western sector. All Indian troops within China's claimed territories in the eastern sector would be expelled, and the war would be ended with a unilateral Chinese ceasefire and withdrawal to prewar positions, followed by a return to the negotiating table. India led the Non-Aligned Movement, Nehru enjoyed international prestige, and China, with a larger military, would be portrayed as an aggressor. However, he said that a well-fought war "will guarantee at least thirty years of peace" with India, and determined the benefits to offset the costs.
China also reportedly bought significant amount of Indian Rupee currency notes from Hong Kong, supposedly to distribute amongst its soldiers in preparation for the war.
On 8 October, additional veteran and elite divisions were ordered to prepare to move into Tibet from the Chengdu and Lanzhou military regions.
On 12 October, Nehru declared that he had ordered the Indian army to "clear Indian territory in the NEFA of Chinese invaders" and personally met with Kaul, issuing instructions to him.
On 14 October, an editorial on "People's Daily" issued China's final warning to India: "So it seems that Mr. Nehru has made up his mind to attack the Chinese frontier guards on an even bigger scale.  ... It is high time to shout to Mr. Nehru that the heroic Chinese troops, with the glorious tradition of resisting foreign aggression, can never be cleared by anyone from their own territory ... If there are still some maniacs who are reckless enough to ignore our well-intentioned advice and insist on having another try, well, let them do so. History will pronounce its inexorable verdict ... At this critical moment ... we still want to appeal once more to Mr. Nehru: better rein in at the edge of the precipice and do not use the lives of Indian troops as stakes in your gamble."
Marshal Liu Bocheng headed a group to determine the strategy for the war. He concluded that the opposing Indian troops were among India's best, and to achieve victory would require deploying crack troops and relying on force concentration to achieve decisive victory. On 16 October, this war plan was approved, and on the 18th, the final approval was given by the Politburo for a "self-defensive counter-attack", scheduled for 20 October.
Chinese offensive.
On 20 October 1962, the Chinese People's Liberation Army launched two attacks, 1000 kilometres apart. In the western theatre, the PLA sought to expel Indian forces from the Chip Chap valley in Aksai Chin while in the eastern theatre, the PLA sought to capture both banks of the Namka Chu river. Some skirmishes also took place at the Nathula Pass, which is in the Indian state of Sikkim (an Indian protectorate at that time). Gurkha rifles travelling north were targeted by Chinese artillery fire. After four days of fierce fighting, the three regiments of Chinese troops succeeded in securing a substantial portion of the disputed territory.
Eastern theatre.
Chinese troops launched an attack on the southern banks of the Namka Chu River on 20 October. The Indian forces were undermanned, with only an understrength battalion to support them, while the Chinese troops had three regiments positioned on the north side of the river. The Indians expected Chinese forces to cross via one of five bridges over the river and defended those crossings. However, the PLA bypassed the defenders by crossing the shallow October river instead. They formed up into battalions on the Indian-held south side of the river under cover of darkness, with each battalion assigned against a separate group of Rajputs.
At 5:14 am, Chinese mortar fire began attacking the Indian positions. Simultaneously, the Chinese cut the Indian telephone lines, preventing the defenders from making contact with their headquarters. At about 6:30 am, the Chinese infantry launched a surprise attack from the rear and forced the Indians to leave their trenches.
The Chinese troops overwhelmed the Indians in a series of flanking manoeuvres south of the McMahon Line and prompted their withdrawal from Namka Chu. Fearful of continued losses, Indian troops escaped into Bhutan. Chinese forces respected the border and did not pursue. Chinese forces now held all of the territory that was under dispute at the time of the Thag La confrontation, but they continued to advance into the rest of NEFA.
On 22 October, at 12:15 am, PLA mortars fired on Walong, on the McMahon line. Flares launched by Indian troops the next day revealed numerous Chinese milling around the valley. The Indians tried to use their mortars against the Chinese but the PLA responded by lighting a bushfire, causing confusion amongst the Indians. Some 400 Chinese troops attacked the Indian position. The initial Chinese assault was halted by accurate Indian mortar fire. The Chinese were then reinforced and launched a second assault. The Indians managed to hold them back for four hours, but the Chinese used sheer weight of numbers to break through. Most Indian forces to withdraw to established positions in Walong, while a company supported by mortars and medium machine guns remained to cover the retreat.
On the morning 23 October, the Indians discovered a Chinese force gathered in a cramped pass and opened fire with mortars and machine guns, leading to heavy fighting. About 200 Chinese soldiers were killed and wounded in this action. Nine Indian soldiers were also killed. The fighting continued well into the afternoon, until the company was ordered to withdraw. Meanwhile, the 4th Sikhs made contact with the Chinese and subjected them to withering mortar and machine gun fire as the Chinese set off a brushfire and attempted to sneak forward. Sepoy Piara Singh tried to douse the fire while fighting the enemy, but died after he was wounded and refused to be evacuated.
Elsewhere, Chinese troops were launched a three-pronged attack on Tawang, which the Indians evacuated without any resistance.
Over the following days, there were clashes between Indian and Chinese patrols at Walong as the Chinese rushed in reinforcements. On 25 October, the Chinese made a probe, which was met with resistance from the 4th Sikhs. As some Chinese soldiers began to close in, Sepoy Kewal Singh charged them with his bayonet and killed a few of them in hand-to-hand combat, but he himself was killed. The following day, a patrol from the 4th Sikhs was encircled, and after being unable to break the encirclement, an Indian unit sneaked in and attacked the Chinese flank, allowing the Sikhs to break free.
Western theatre.
On the Aksai Chin front, China already controlled most of the disputed territory. Chinese forces quickly swept the region of any remaining Indian troops. Late on 19 October, Chinese troops launched a number of attacks throughout the western theatre. By 22 October, all posts north of Chushul had been cleared.
On 20 October, the Chinese easily took the Chip Chap Valley, Galwan Valley, and Pangong Lake. Many outposts and garrisons along the Western front were unable to defend against the surrounding Chinese troops. Most Indian troops positioned in these posts offered resistance but were either killed or taken prisoner. Indian support for these outposts was not forthcoming, as evidenced by the Galwan post, which had been surrounded by enemy forces in August, but no attempt made to relieve the besieged garrison. Following the 20 October attack, nothing was heard from Galwan.
On 24 October, Indian forces fought hard hold the Rezang La Ridge, in order to prevent a nearby airstrip from falling to the Chinese.
After realising the magnitude of the attack, the Indian Western Command withdrew many of the isolated outposts to the south-east. Daulet Beg Oldi was also evacuated, but it was south of the Chinese claim line and was not approached by Chinese forces. Indian troops were withdrawn in order to consolidate and regroup in the event that China probed south of their claim line.
Lull in the fighting.
By 24 October, the PLA had entered territory previously administered by India to give the PRC a diplomatically strong position over India. The majority of Chinese forces had advanced sixteen kilometres south of the control line prior to the conflict. Four days of fighting were followed by a three-week lull. Zhou ordered the troops to stop advancing as he attempted to negotiate with Nehru. The Indian forces had retreated into more heavily fortified positions around Se La and Bombdi La which would be difficult to assault. Zhou sent Nehru a letter, proposing
Nehru's 27 October reply expressed interest in the restoration of peace and friendly relations and suggested a return to the "boundary prior to 8 September 1962". He was categorically concerned about a mutual twenty kilometre withdrawal after "40 or 60 kilometres of blatant military aggression". He wanted the creation of a larger immediate buffer zone and thus resist the possibility of a repeat offensive. Zhou's 4 November reply repeated his 1959 offer to return to the McMahon Line in NEFA and the Chinese traditionally claimed MacDonald Line in Aksai Chin. Facing Chinese forces maintaining themselves on Indian soil and trying to avoid political pressure, the Indian parliament announced a national emergency and passed a resolution which stated their intent to "drive out the aggressors from the sacred soil of India". The United States and the United Kingdom supported India's response. However, the Soviet Union was preoccupied with the Cuban Missile Crisis and did not offer the support it had provided in previous years. With the backing of other great powers, a 14 November letter by Nehru to Zhou once again rejected his proposal.
Neither side declared war, used their air force, or fully broke off diplomatic relations; however, the conflict is commonly referred to as a war. This war coincided with the Cuban Missile Crisis and was viewed by the western nations at the time as another act of aggression by the Communist bloc.
According to Calvin, the Chinese side evidently wanted a diplomatic resolution and discontinuation of the conflict.
Continuation of war.
After Zhou received Nehru's letter (rejecting Zhou's proposal), the fighting resumed on the eastern theatre on 14 November (Nehru's birthday), with an Indian attack on Walong, claimed by China, launched from the defensive position of Se La and inflicting heavy casualties on the Chinese. The Chinese resumed military activity on Aksai Chin and NEFA hours after the Walong battle.
Eastern theatre.
On the eastern theatre, the PLA attacked Indian forces near Se La and Bomdi La on 17 November. These positions were defended by the Indian 4th Infantry Division. Instead of attacking by road as expected, PLA forces approached via a mountain trail, and their attack cut off a main road and isolated 10,000 Indian troops.
Se La occupied high ground, and rather than assault this commanding position, the Chinese captured Thembang, which was a supply route to Se La.
Western theatre.
On the western theatre, PLA forces launched a heavy infantry attack on 18 November near Chushul. Their attack started at 4:35 am, despite a mist surrounding most of the areas in the region. At 5:45 the Chinese troops advanced to attack 2 platoons of Indian troops at Gurung Hill.
The Indians did not know what was happening, as communications were dead. As a patrol was sent, China attacked with greater numbers. Indian artillery could not hold off against superior Chinese forces. By 9:00 am, Chinese forces attacked Gurung Hill directly and Indian commanders withdrew from the area and also from the connecting Spangur Gap.
The Chinese had been simultaneously attacking Rezang La which was held by 123 Indian troops. At 5:05 am, Chinese troops launched their attack audaciously. Chinese medium machine gun fire pierced through the Indian tactical defences.
At 6:55 am the sun rose and the Chinese attack on the 8th platoon began in waves. Fighting continued for the next hour, until the Chinese signaled that they had destroyed the 7th platoon. Indians tried to use light machine guns on the medium machine guns from the Chinese but after 10 minutes the battle was over. Logistical inadequacy once again hurt the Indian troops. The Chinese gave the Indian troops a respectful military funeral. The battles also saw the death of Major Shaitan Singh of the Kumaon Regiment, who had been instrumental in the first battle of Rezang La. The Indian troops were forced to withdraw to high mountain positions. Indian sources believed that their troops were just coming to grips with the mountain combat and finally called for more troops. However, the Chinese declared a ceasefire, ending the bloodshed.
Indians suffered heavy casualties, with dead Indian troops' bodies being found in the ice, frozen with weapons in hand. Chinese forces also suffered heavy casualties, especially at Rezang La. This signalled the end of the war in Aksai Chin as China had reached their claim line – many Indian troops were ordered to withdraw from the area. China claimed that the Indian troops wanted to fight on until the bitter end. However, the war ended with their withdrawal, so as to limit the amount of casualties.
The PLA penetrated close to the outskirts of Tezpur, Assam, a major frontier town nearly fifty kilometres from the Assam-North-East Frontier Agency border. The local government ordered the evacuation of the civilians in Tezpur to the south of the Brahmaputra River, all prisons were thrown open, and government officials who stayed behind destroyed Tezpur's currency reserves in anticipation of a Chinese advance.
Ceasefire.
China had reached its claim lines so the PLA did not advance farther, and on 19 November, it declared a unilateral cease-fire. Zhou Enlai declared a unilateral ceasefire to start on midnight, 21 November. Zhou's ceasefire declaration stated,
Beginning from 21 November 1962, the Chinese frontier guards will cease fire along the entire Sino-Indian border. Beginning from 1 December 1962, the Chinese frontier guards will withdraw to positions 20 kilometres behind the line of actual control which existed between China and India on 7 November 1959. In the eastern sector, although the Chinese frontier guards have so far been fighting on Chinese territory north of the traditional customary line, they are prepared to withdraw from their present positions to the north of the illegal McMahon Line, and to withdraw twenty kilometres back from that line. In the middle and western sectors, the Chinese frontier guards will withdraw twenty kilometres from the line of actual control.
Zhou had first given the ceasefire announcement to Indian chargé d'affaires on 19 November (before India's request for United States air support), but New Delhi did not receive it until 24 hours later. The aircraft carrier was ordered back after the ceasefire, and thus, American intervention on India's side in the war was avoided. Retreating Indian troops, who hadn't come into contact with anyone knowing of the ceasefire, and Chinese troops in NEFA and Aksai Chin, were involved in some minor battles, but for the most part, the ceasefire signalled an end to the fighting. The United States Air Force flew in supplies to India in November 1962, but neither side wished to continue hostilities.
Toward the end of the war India increased its support for Tibetan refugees and revolutionaries, some of them having settled in India, as they were fighting the same common enemy in the region. The Nehru administration ordered the raising of an elite Indian-trained "Tibetan Armed Force" composed of Tibetan refugees. The CIA had already begun operations in bringing about change in Tibet.
World opinion.
The Chinese military action has been viewed by the United States as part of the PRC's policy of making use of aggressive wars to settle its border disputes and to distract from its internal issues. According to James Calvin from the United States Marine Corps, western nations at the time viewed China as an aggressor during the China–India border war, and the war was part of a monolithic communist objective for a world dictatorship of the proletariat. This was further triggered by Mao Zedong's views that: "The way to world conquest lies through Havana, Accra, and Calcutta". Calvin believes that Chinese actions show a "pattern of conservative aims and limited objectives, rather than expansionism" and blames this particular conflict on India's provocations towards China. However, Calvin also expresses that China, in the past, has been adamant to gain control over regions to which it has a "traditional claim", which triggered the dispute over NEFA and Aksai Chin and indeed Tibet. Calvin's assumption, based on the history of the Cold War and the Domino Effect, assumed that China might ultimately try to regain control of everything that it considers as "traditionally Chinese" which in its view includes the entirety of South East Asia.
The Kennedy administration was disturbed by what they considered "blatant Chinese communist aggression against India". In a May 1963 National Security Council meeting, contingency planning on the part of the United States in the event of another Chinese attack on India was discussed. Defense Secretary Robert McNamara and General Maxwell Taylor advised the president to use nuclear weapons should the Americans intervene in such a situation. McNamara stated "Before any substantial commitment to defend India against China is given, we should recognize that in order to carry out that commitment against any substantial Chinese attack, we would have to use nuclear weapons. Any large Chinese Communist attack on any part of that area would require the use of nuclear weapons by the U.S., and this is to be preferred over the introduction of large numbers of U.S. soldiers." After hearing this and listening to two other advisers, Kennedy stated "We should defend India, and therefore we will defend India." It remains unclear if his aides were trying to dissuade the President of considering any measure with regard to India by immediately raising the stakes to an unacceptable level, nor is it clear if Kennedy was thinking of conventional or nuclear means when he gave his reply. By 1964 China had developed its own nuclear weapon which would have likely caused any American nuclear policy in defense of India to be reviewed. The Johnson Administration considered and then rejected giving nuclear weapons technology to the Indians. However India developed its own nuclear weapon by 1974, within 10 years of the Chinese.
The non-aligned nations remained mostly uninvolved, and only the United Arab Republic openly supported India. Of the non-aligned nations, six, Egypt, Burma, Cambodia, Sri Lanka, Ghana and Indonesia, met in Colombo on 10 December 1962. The proposals stipulated a Chinese withdrawal of 20 km from the customary lines without any reciprocal withdrawal on India's behalf. The failure of these six nations to unequivocally condemn China deeply disappointed India.
In 1972, Chinese Premier Zhou explained the Chinese point of view to President Nixon of the US. As for the causes of the war, Zhou asserted that China did not try to expel Indian troops from south of the McMahon line and that three open warning telegrams were sent to Nehru before the war. However, Indian patrols south of the McMahon line were expelled and suffered casualties in the Chinese attack. Zhou also told Nixon that Chairman Mao ordered the troops to return to show good faith. The Indian government maintains that the Chinese military could not advance further south due to logistical problems and the cut-off of resource supplies.
While Western nations did not view Chinese actions favourably because of fear of the Chinese and competitiveness, Pakistan, which had had a turbulent relationship with India ever since the Indian partition, improved its relations with China after the war. Prior to the war, Pakistan also shared a disputed boundary with China, and had proposed to India that the two countries adopt a common defence against "northern" enemies (i.e. China), which was rejected by India. However, China and Pakistan took steps to peacefully negotiate their shared boundaries, beginning on 13 October 1962, and concluding in December of that year. Pakistan also expressed fear that the huge amounts of western military aid directed to India would allow it to threaten Pakistan's security in future conflicts. Mohammed Ali, External Affairs Minister of Pakistan, declared that massive Western aid to India in the Sino-Indian dispute would be considered an unfriendly act towards Pakistan. As a result Pakistan made efforts to improve its relations with China. The following year, China and Pakistan peacefully settled disputes on their shared border, and negotiated the China-Pakistan Border Treaty in 1963, as well as trade, commercial, and barter treaties. On 2 March 1963, Pakistan conceded its northern claim line in Pakistani-controlled Kashmir to China in favor of a more southerly boundary along the Karakoram Range. The border treaty largely set the border along the MacCartney-Macdonald Line. India's military failure against China would embolden Pakistan to initiate the Second Kashmir War with India. However, it effectively ended in a stalemate as Calvin states that the Sino-Indian War had caused the previously passive government to take a stand on actively modernising India's military. China offered diplomatic support to Pakistan in this war but did not offer military support. In January 1966, China condemned the Tashkent Agreement between India and Pakistan as a Soviet-US plot in the region. In the Indo-Pakistani War of 1971, Pakistan expected China to provide military support, but it was left alone as India successfully helped the rebels in East Pakistan to found the new nation-state of Bangladesh.
Involvement of other nations.
During the conflict, Nehru wrote two desperate letters to U.S. President John F. Kennedy, requesting 12 squadrons of fighter jets and a modern radar system. These jets were seen as necessary to beef up Indian air strength so that air-to-air combat could be initiated safely from the Indian perspective (bombing troops was seen as unwise for fear of Chinese retaliatory action). Nehru also asked that these aircraft be manned by American pilots until Indian airmen were trained to replace them. These requests were rejected by the Kennedy Administration (which was involved in the Cuban Missile Crisis during most of the Sino-Indian War). According to former Indian diplomat G Parthasarathy, "only after we got nothing from the US did arms supplies from the Soviet Union to India commence." In 1962, President of Pakistan Ayub Khan made clear to India that Indian troops could safely be transferred from the Pakistan frontier to the Himalayas.
Aftermath.
China.
According to the China's official military history, the war achieved China's policy objectives of securing borders in its western sector, as China retained de facto control of the Aksai Chin. After the war, India abandoned the Forward Policy, and the de facto borders stabilised along the Line of Actual Control.
According to James Calvin of Marine Corps Command and Staff College, even though China won a military victory it lost in terms of its international image. China's first nuclear weapon test in October 1964 and it support of Pakistan in the 1965 India Pakistan War tended to confirm the American view of communist world objectives, including Chinese influence over Pakistan.
Lora Saalman opined in a study of Chinese military publications, that while the war led to much blame, debates and ultimately acted as causation of military modernization of India but the war is now treated as basic reportage of facts with relatively diminished interest by Chinese analysts.
India.
The aftermath of the war saw sweeping changes in the Indian military to prepare it for similar conflicts in the future, and placed pressure on Indian prime minister Jawaharlal Nehru, who was seen as responsible for failing to anticipate the Chinese attack on India. Indians reacted with a surge in patriotism and memorials were erected for many of the Indian troops who died in the war. Arguably, the main lesson India learned from the war was the need to strengthen its own defences and a shift from Nehru's foreign policy with China based on his stated concept of "brotherhood". Because of India's inability to anticipate Chinese aggression, Prime Minister Nehru faced harsh criticism from government officials, for having promoted pacifist relations with China. Indian President Radhakrishnan said that Nehru's government was naive and negligent about preparations, and Nehru admitted his failings. According to Inder Malhotra, a former editor of "The Times of India" and a commentator for "The Indian Express", Indian politicians invested more effort in removing Defence Minister Krishna Menon than in actually waging war. Krishna Menon's favoritism weakened the Indian Army, and national morale dimmed. The public saw the war as political and military debacle. Under American advice (by American envoy John Kenneth Galbraith who made and ran American policy on the war as all other top policy makers in USA were absorbed in coincident Cuban Missile Crisis) Indians refrained, not according to the best choices available, from using the Indian air force to beat back the Chinese advances. The CIA later revealed that at that time the Chinese had neither the fuel nor runways long enough for using their air force effectively in Tibet. Indians in general became highly sceptical of China and its military. Many Indians view the war as a betrayal of India's attempts at establishing a long-standing peace with China and started to question the once popular "Hindi-Chini bhai-bhai" (meaning "Indians and Chinese are brothers"). The war also put an end to Nehru's earlier hopes that India and China would form a strong Asian Axis to counteract the increasing influence of the Cold War bloc superpowers.
The unpreparedness of the army was blamed on Defence Minister Menon, who resigned his government post to allow for someone who might modernise India's military further. India's policy of weaponisation via indigenous sources and self-sufficiency was thus cemented. Sensing a weakened army, Pakistan, a close ally of China, began a policy of provocation against India by infiltrating Jammu and Kashmir and ultimately triggering the Second Kashmir War with India in 1965 and Indo-Pakistani war of 1971. The Attack of 1965 was successfully stopped and ceasefire was negotiated under international pressure. In the Indo-Pakistani war of 1971 India won a clear victory, resulting in liberation of Bangladesh (formerly East-Pakistan).
As a result of the war, the Indian government commissioned an investigation, resulting in the classified Henderson-Brooks-Bhagat Report on the causes of the war and the reasons for failure. India's performance in high-altitude combat in 1962 led to an overhaul of the Indian Army in terms of doctrine, training, organisation and equipment. Neville Maxwell claimed that the Indian role in international affairs after the border war was also greatly reduced after the war and India's standing in the non-aligned movement suffered. The Indian government has attempted to keep the Hendersen-Brooks-Bhagat Report secret for decades, although portions of it have recently been leaked by Neville Maxwell.
According to James Calvin, an analyst from the U.S. Navy, India gained many benefits from the 1962 conflict. This war united the country as never before. India got 32,000 square miles (8.3 million hectares, 83,000 km2) of disputed territory even if it felt that NEFA was hers all along. The new Indian republic had avoided international alignments; by asking for help during the war, India demonstrated its willingness to accept military aid from several sectors. And, finally, India recognised the serious weaknesses in its army. It would more than double its military manpower in the next two years and it would work hard to resolve the military's training and logistic problems to later become the third-largest army in the world. India's efforts to improve its military posture significantly enhanced its army's capabilities and preparedness. This played a role in subsequent wars against Pakistan.
Internment and deportation of Chinese Indians.
Soon after the end of the war, the Indian government passed the Defence of India Act in December 1962, permitting the "apprehension and detention in custody of any person [suspected] of being of hostile origin." The broad language of the act allowed for the arrest of any person simply for having a Chinese surname, Chinese ancestry or a Chinese spouse. The Indian government incarcerated thousands of Chinese-Indians in an internment camp in Deoli, Rajasthan, where they were held for years without trial. The last internees were not released until 1967. Thousands more Chinese-Indians were forcibly deported or coerced to leave India. Nearly all internees had their properties sold off or looted. Even after their release, the Chinese Indians faced many restrictions in their freedom. They could not travel freely until the mid-1990s.
Later conflicts.
India also reported a series of military conflicts after the 1962 war. One report provided by India shows that in late 1967, there were two incidents in which both countries exchanged fire in Sikkim. The first one was dubbed the "Nathu La incident", and the other being "Chola incident".
Diplomatic process.
In 1993 and 1996, the two sides signed the Sino-Indian Bilateral Peace and Tranquility Accords, agreements to maintain peace and tranquility along the Line of Actual Control (LoAC). Ten meetings of a Sino-Indian Joint Working Group (SIJWG) and five of an expert group have taken place to determine where the LoAC lies, but little progress has occurred.
On 20 November 2006 Indian politicians from Arunachal Pradesh expressed their concern over Chinese military modernization and appealed to parliament to take a harder stance on the PRC following a military buildup on the border similar to that in 1962. Additionally, China's military aid to Pakistan as well is a matter of concern to the Indian public, as the two sides have engaged in various wars.
On 6 July 2006, the historic Silk Road passing through this territory via the Nathu La pass was reopened. Both sides have agreed to resolve the issues by peaceful means.
In Oct 2011, it was stated that India and China will formulate a border mechanism to handle different perceptions as to the LAC and resume the bilateral army exercises between Indian and Chinese army from early 2012.

</doc>
<doc id="29430" url="http://en.wikipedia.org/wiki?curid=29430" title="Simple module">
Simple module

In mathematics, specifically in ring theory, the simple modules over a ring "R" are the (left or right) modules over "R" that have no non-zero proper submodules. Equivalently, a module "M" is simple if and only if every cyclic submodule generated by a non-zero element of "M" equals "M". Simple modules form building blocks for the modules of finite length, and they are analogous to the simple groups in group theory.
In this article, all modules will be assumed to be right unital modules over a ring "R".
Examples.
Z-modules are the same as abelian groups, so a simple Z-module is an abelian group which has no non-zero proper subgroups. These are the cyclic groups of prime order.
If "I" is a right ideal of "R", then "I" is simple as a right module if and only if "I" is a minimal non-zero right ideal: If "M" is a non-zero proper submodule of "I", then it is also a right ideal, so "I" is not minimal. Conversely, if "I" is not minimal, then there is a non-zero right ideal "J" properly contained in "I". "J" is a right submodule of "I", so "I" is not simple.
If "I" is a right ideal of "R", then "R"/"I" is simple if and only if "I" is a maximal right ideal: If "M" is a non-zero proper submodule of "R"/"I", then the preimage of "M" under the quotient map "R" → "R"/"I" is a right ideal which is not equal to "R" and which properly contains "I". Therefore "I" is not maximal. Conversely, if "I" is not maximal, then there is a right ideal "J" properly containing "I". The quotient map "R"/"I" → "R"/"J" has a non-zero kernel which is not equal to "R"/"I", and therefore "R"/"I" is not simple.
Every simple "R"-module is isomorphic to a quotient "R"/"m" where "m" is a maximal right ideal of "R". By the above paragraph, any quotient "R"/"m" is a simple module. Conversely, suppose that "M" is a simple "R"-module. Then, for any non-zero element "x" of "M", the cyclic submodule "xR" must equal "M". Fix such an "x". The statement that "xR" = "M" is equivalent to the surjectivity of the homomorphism "R" → "M" that sends "r" to "xr". The kernel of this homomorphism is a right ideal "I" of "R", and a standard theorem states that "M" is isomorphic to "R"/"I". By the above paragraph, we find that "I" is a maximal right ideal. Therefore "M" is isomorphic to a quotient of "R" by a maximal right ideal.
If "k" is a field and "G" is a group, then a group representation of "G" is a left module over the group ring "k[G]". The simple "k[G]" modules are also known as irreducible representations. A major aim of representation theory is to understand the irreducible representations of groups.
Basic properties of simple modules.
The simple modules are precisely the modules of length 1; this is a reformulation of the definition.
Every simple module is indecomposable, but the converse is in general not true.
Every simple module is cyclic, that is it is generated by one element.
Not every module has a simple submodule; consider for instance the Z-module Z in light of the first example above.
Let "M" and "N" be (left or right) modules over the same ring, and let "f" : "M" → "N" be a module homomorphism. If "M" is simple, then "f" is either the zero homomorphism or injective because the kernel of "f" is a submodule of "M". If "N" is simple, then "f" is either the zero homomorphism or surjective because the image of "f" is a submodule of "N". If "M" = "N", then "f" is an endomorphism of "M", and if "M" is simple, then the prior two statements imply that "f" is either the zero homomorphism or an isomorphism. Consequently the endomorphism ring of any simple module is a division ring. This result is known as Schur's lemma.
The converse of Schur's lemma is not true in general. For example, the Z-module Q is not simple, but its endomorphism ring is isomorphic to the field Q.
Simple modules and composition series.
If "M" is a module which has a non-zero proper submodule "N", then there is a short exact sequence
A common approach to proving a fact about "M" is to show that the fact is true for the center term of a short exact sequence when it is true for the left and right terms, then to prove the fact for "N" and "M"/"N". If "N" has a non-zero proper submodule, then this process can be repeated. This produces a chain of submodules
In order to prove the fact this way, one needs conditions on this sequence and on the modules "M""i"/"M""i" + 1. One particularly useful condition is that the length of the sequence is finite and each quotient module "M""i"/"M""i" + 1 is simple. In this case the sequence is called a composition series for "M". In order to prove a statement inductively using composition series, the statement is first proved for simple modules, which form the base case of the induction, and then the statement is proved to remain true under an extension of a module by a simple module. For example, the Fitting lemma shows that the endomorphism ring of a finite length indecomposable module is a local ring, so that the strong Krull-Schmidt theorem holds and the category of finite length modules is a Krull-Schmidt category.
The Jordan–Hölder theorem and the Schreier refinement theorem describe the relationships amongst all composition series of a single module. The Grothendieck group ignores the order in a composition series and views every finite length module as a formal sum of simple modules. Over semisimple rings, this is no loss as every module is a semisimple module and so a direct sum of simple modules. Ordinary character theory provides better arithmetic control, and uses simple C"G" modules to understand the structure of finite groups "G". Modular representation theory uses Brauer characters to view modules as formal sums of simple modules, but is also interested in how those simple modules are joined together within composition series. This is formalized by studying the Ext functor and describing the module category in various ways including quivers (whose nodes are the simple modules and whose edges are composition series of non-semisimple modules of length 2) and Auslander–Reiten theory where the associated graph has a vertex for every indecomposable module.
The Jacobson density theorem.
An important advance in the theory of simple modules was the Jacobson density theorem. The Jacobson density theorem states:
In particular, any primitive ring may be viewed as (that is, isomorphic to) a ring of "D"-linear operators on some "D"-space.
A consequence of the Jacobson density theorem is Wedderburn's theorem; namely that any right artinian simple ring is isomorphic to a full matrix ring of "n" by "n" matrices over a division ring for some "n". This can also be established as a corollary of the Artin–Wedderburn theorem.

</doc>
<doc id="29438" url="http://en.wikipedia.org/wiki?curid=29438" title="Sonar">
Sonar

Sonar (originally an acronym for SOund Navigation And Ranging) is a technique that uses sound propagation (usually underwater, as in submarine navigation) to navigate, communicate with or detect objects on or under the surface of the water, such as other vessels. Two types of technology share the name "sonar": "passive" sonar is essentially listening for the sound made by vessels; "active" sonar is emitting pulses of sounds and listening for echoes. Sonar may be used as a means of acoustic location and of measurement of the echo characteristics of "targets" in the water. Acoustic location in air was used before the introduction of radar. Sonar may also be used in air for robot navigation, and SODAR (an upward looking in-air sonar) is used for atmospheric investigations. The term "sonar" is also used for the equipment used to generate and receive the sound. The acoustic frequencies used in sonar systems vary from very low (infrasonic) to extremely high (ultrasonic). The study of underwater sound is known as underwater acoustics or hydroacoustics.
History.
Although some animals (dolphins and bats) have used sound for communication and object detection for millions of years, use by humans in the water is initially recorded by Leonardo da Vinci in 1490: a tube inserted into the water was said to be used to detect vessels by placing an ear to the tube.
In the 19th century an underwater bell was used as an ancillary to lighthouses to provide warning of hazards.
The use of sound to 'echo locate' underwater in the same way as bats use sound for aerial navigation seems to have been prompted by the "Titanic" disaster of 1912. The world's first patent for an underwater echo ranging device was filed at the British Patent Office by English meteorologist Lewis Richardson a month after the sinking of the Titanic, and a German physicist Alexander Behm obtained a patent for an echo sounder in 1913.
The Canadian engineer Reginald Fessenden, while working for the Submarine Signal Company in Boston, built an experimental system beginning in 1912, a system later tested in Boston Harbor, and finally in 1914 from the U.S. Revenue (now Coast Guard) Cutter Miami on the Grand Banks off Newfoundland Canada. In that test, Fessenden demonstrated depth sounding, underwater communications (Morse code) and echo ranging (detecting an iceberg at two miles (3 km) range). The so-called Fessenden oscillator, at ca. 500 Hz frequency, was unable to determine the bearing of the berg due to the 3 metre wavelength and the small dimension of the transducer's radiating face (less than 1 metre in diameter). The ten Montreal-built British H class submarines launched in 1915 were equipped with a Fessenden oscillator.
During World War I the need to detect submarines prompted more research into the use of sound. The British made early use of underwater listening devices called hydrophones, while the French physicist Paul Langevin, working with a Russian immigrant electrical engineer, Constantin Chilowsky, worked on the development of active sound devices for detecting submarines in 1915. Although piezoelectric and magnetostrictive transducers later superseded the electrostatic transducers they used, this work influenced future designs. Lightweight sound-sensitive plastic film and fibre optics have been used for hydrophones (acousto-electric transducers for in-water use), while Terfenol-D and PMN (lead magnesium niobate) have been developed for projectors.
ASDIC.
In 1916, under the British Board of Invention and Research, Canadian physicist Robert William Boyle took on the active sound detection project with A B Wood, producing a prototype for testing in mid-1917. This work, for the Anti-Submarine Division of the British Naval Staff, was undertaken in utmost secrecy, and used quartz piezoelectric crystals to produce the world's first practical underwater active sound detection apparatus. To maintain secrecy no mention of sound experimentation or quartz was made - the word used to describe the early work ('supersonics') was changed to 'ASD'ics, and the quartz material to 'ASD'ivite: hence the British acronym "ASDIC". In 1939, in response to a question from the Oxford English Dictionary, the Admiralty made up the story that it stood for 'Allied Submarine Detection Investigation Committee', and this is still widely believed, though no committee bearing this name has been found in the Admiralty archives.
By 1918, both France and Britain had built prototype active systems. The British tested their ASDIC on HMS "Antrim" in 1920, and started production in 1922. The 6th Destroyer Flotilla had ASDIC-equipped vessels in 1923. An anti-submarine school, HMS "Osprey", and a training flotilla of four vessels were established on Portland in 1924. The US Sonar QB set arrived in 1931.
By the outbreak of World War II, the Royal Navy had five sets for different surface ship classes, and others for submarines, incorporated into a complete anti-submarine attack system. The effectiveness of early ASDIC was hamstrung by the use of the depth charge as an anti-submarine weapon. This required an attacking vessel to pass over a submerged contact before dropping charges over the stern, resulting in a loss of ASDIC contact in the moments leading up to attack. The hunter was effectively firing blind, during which time a submarine commander could take evasive action. This situation was remedied by using several ships cooperating and by the adoption of "ahead throwing weapons", such as Hedgehog and later Squid, which projected warheads at a target ahead of the attacker and thus still in ASDIC contact. Developments during the war resulted in British ASDIC sets which used several different shapes of beam, continuously covering blind spots. Later, acoustic torpedoes were used.
At the start of World War II, British ASDIC technology was transferred for free to the United States. Research on ASDIC and underwater sound was expanded in the UK and in the US. Many new types of military sound detection were developed. These included sonobuoys, first developed by the British in 1944 under the codename "High Tea", dipping/dunking sonar and mine detection sonar. This work formed the basis for post war developments related to countering the nuclear submarine. Work on sonar had also been carried out in the Axis countries, notably in Germany, which included countermeasures. At the end of World War II this German work was assimilated by Britain and the US. Sonars have continued to be developed by many countries, including Russia, for both military and civil uses. In recent years the major military development has been the increasing interest in low frequency active systems.
SONAR.
During the 1930s American engineers developed their own underwater sound detection technology and important discoveries were made, such as thermoclines, that would help future development. After technical information was exchanged between the two countries during the Second World War, Americans began to use the term "SONAR" for their systems, coined as the equivalent of RADAR.
Materials and designs.
There was little progress in development from 1915 to 1940. In 1940, the US sonars typically consisted of a magnetostrictive transducer and an array of nickel tubes connected to a 1-foot-diameter steel plate attached back to back to a Rochelle salt crystal in a spherical housing. This assembly penetrated the ship hull and was manually rotated to the desired angle. The piezoelectric Rochelle salt crystal had better parameters, but the magnetostrictive unit was much more reliable. Early WW2 losses prompted rapid research in the field, pursuing both improvements in magnetostrictive transducer parameters and Rochelle salt reliability. Ammonium dihydrogen phosphate (ADP), a superior alternative, was found as a replacement for Rochelle salt; the first application was a replacement of the 24 kHz Rochelle salt transducers. Within nine months, Rochelle salt was obsolete. The ADP manufacturing facility grew from few dozen personnel in early 1940 to several thousands in 1942.
One of the earliest application of ADP crystals were hydrophones for acoustic mines; the crystals were specified for low frequency cutoff at 5 Hz, withstanding mechanical shock for deployment from aircraft from 10,000 ft, and ability to survive neighbouring mine explosions. One of key features of ADP reliability is its zero aging characteristics; the crystal keeps its parameters even over prolonged storage.
Another application was for acoustic homing torpedoes. Two pairs of directional hydrophones were mounted on the torpedo nose, in the horizontal and vertical plane; the difference signals from the pairs were used to steer the torpedo left-right and up-down. A countermeasure was developed: the targeted submarine discharged an effervescent chemical, and the torpedo went after the noisier fizzy decoy. The counter-countermeasure was a torpedo with active sonar – a transducer was added to the torpedo nose, and the microphones were listening for its reflected periodic tone bursts. The transducers comprised identical rectangular crystal plates arranged to diamond-shaped areas in staggered rows.
Passive sonar arrays for submarines were developed from ADP crystals. Several crystal assemblies were arranged in a steel tube, vacuum-filled with castor oil, and sealed. The tubes then were mounted in parallel arrays.
The standard US Navy scanning sonar at the end of the World War II operated at 18 kHz, using an array of ADP crystals. Desired longer range however required use of lower frequencies. The required dimensions were too big for ADP crystals, so in the early 1950s magnetostrictive and barium titanate piezoelectric systems were developed, but these had problems achieving uniform impedance characteristics and the beam pattern suffered. Barium titanate was then replaced with more stable lead zirconate titanate (PZT), and the frequency was lowered to 5 kHz. The US fleet used this material in the AN/SQS-23 sonar for several decades. The SQS-23 sonar first used magnetostrictive nickel transducers, but these weighed several tons and nickel was expensive and considered a critical material; piezoelectric transducers were therefore substituted. The sonar was a large array of 432 individual transducers. At first the transducers were unreliable, showing mechanical and electrical failures and deteriorating soon after installation; they were also produced by several vendors, had different designs, and their characteristics were different enough to impair the array's performance. The policy to allow repair of individual transducers was then sacrificed, and "expendable modular design", sealed non-repairable modules, was chosen instead, eliminating the problem with seals and other extraneous mechanical parts.
The Imperial Japanese Navy at the onset of WW2 used projectors based on quartz. These were big and heavy, especially if designed for lower frequencies; the one for Type 91 set, operating at 9 kHz, had a diameter of 30 inches and was driven by an oscillator with 5 kW power and 7000 volts of output amplitude. The Type 93 projectors consisted of solid sandwiches of quartz, assembled into spherical cast iron bodies. The Type 93 sonars were later replaced with Type 3, which followed German design and used magnetostrictive projectors; the projectors consisted of two rectangular identical independent units in a cast iron rectangular body about 16×9 inches. The exposed area was half the wavelength wide and three wavelengths high. The magnetostrictive cores were made from 4 mm stampings of nickel, and later of an iron-aluminium alloy with aluminium content between 12.7 and 12.9%. The power was provided from a 2 kW at 3800 volts, with polarization from a 20 V/8 A DC source.
The passive hydrophones of the Imperial Japanese Navy were based on moving coil design, Rochelle salt piezo transducers, and carbon microphones.
Magnetostrictive transducers were pursued after WW2 as an alternative to piezoelectric ones. Nickel scroll-wound ring transducers were used for high-power low-frequency operations, with size up to 13 feet in diameter, probably the largest individual sonar transducers ever. The advantage of metals is their high tensile strength and low input electrical impedance, but they have electrical losses and lower coupling coefficient than PZT, whose tensile strength can be increased by prestressing. Other materials were also tried; nonmetallic ferrites were promising for their low electrical conductivity resulting in low eddy current losses, Metglas offered high coupling coefficient, but they were inferior to PZT overall. In the 1970s, compounds of rare earths and iron were discovered with superior magnetomechanic properties, namely the Terfenol-D alloy. This made possible new designs, e.g. a hybrid magnetostrictive-piezoelectric transducer. The most recent sch material is Galfenol.
Other types of transducers include variable reluctance (or moving armature, or electromagnetic) transducers, where magnetic force acts on the surfaces of gaps, and moving coil (or electrodynamic) transducers, similar to conventional speakers; the latter are used in underwater sound calibration, due to their very low resonance frequencies and flat broadband characteristics above them.
Active sonar.
Active sonar uses a sound transmitter and a receiver. When the two are in the same place it is monostatic operation. When the transmitter and receiver are separated it is bistatic operation. When more transmitters (or more receivers) are used, again spatially separated, it is multistatic operation. Most sonars are used monostatically with the same array often being used for transmission and reception. Active sonobuoy fields may be operated multistatically.
Active sonar creates a pulse of sound, often called a "ping", and then listens for reflections (echo) of the pulse. This pulse of sound is generally created electronically using a sonar projector consisting of a signal generator, power amplifier and electro-acoustic transducer/array. A beamformer is usually employed to concentrate the acoustic power into a beam, which may be swept to cover the required search angles. Generally, the electro-acoustic transducers are of the Tonpilz type and their design may be optimised to achieve maximum efficiency over the widest bandwidth, in order to optimise performance of the overall system. Occasionally, the acoustic pulse may be created by other means, e.g. (1) chemically using explosives, or (2) airguns or (3) plasma sound sources.
To measure the distance to an object, the time from transmission of a pulse to reception is measured and converted into a range by knowing the speed of sound. To measure the bearing, several hydrophones are used, and the set measures the relative arrival time to each, or with an array of hydrophones, by measuring the relative amplitude in beams formed through a process called beamforming. Use of an array reduces the spatial response so that to provide wide cover multibeam systems are used. The target signal (if present) together with noise is then passed through various forms of signal processing, which for simple sonars may be just energy measurement. It is then presented to some form of decision device that calls the output either the required signal or noise. This decision device may be an operator with headphones or a display, or in more sophisticated sonars this function may be carried out by software. Further processes may be carried out to classify the target and localise it, as well as measuring its velocity.
The pulse may be at constant frequency or a chirp of changing frequency (to allow pulse compression on reception). Simple sonars generally use the former with a filter wide enough to cover possible Doppler changes due to target movement, while more complex ones generally include the latter technique. Since digital processing became available pulse compression has usually been implemented using digital correlation techniques. Military sonars often have multiple beams to provide all-round cover while simple ones only cover a narrow arc, although the beam may be rotated, relatively slowly, by mechanical scanning.
Particularly when single frequency transmissions are used, the Doppler effect can be used to measure the radial speed of a target. The difference in frequency between the transmitted and received signal is measured and converted into a velocity. Since Doppler shifts can be introduced by either receiver or target motion, allowance has to be made for the radial speed of the searching platform.
One useful small sonar is similar in appearance to a waterproof flashlight. The head is pointed into the water, a button is pressed, and the device displays the distance to the target. Another variant is a "fishfinder" that shows a small display with shoals of fish. Some civilian sonars (which are not designed for stealth) approach active military sonars in capability, with quite exotic three-dimensional displays of the area near the boat.
When active sonar is used to measure the distance from the transducer to the bottom, it is known as echo sounding. Similar methods may be used looking upward for wave measurement.
Active sonar is also used to measure distance through water between two sonar transducers or a combination of a hydrophone (underwater acoustic microphone) and projector (underwater acoustic speaker). A transducer is a device that can transmit and receive acoustic signals ("pings"). When a hydrophone/transducer receives a specific interrogation signal it responds by transmitting a specific reply signal. To measure distance, one transducer/projector transmits an interrogation signal and measures the time between this transmission and the receipt of the other transducer/hydrophone reply. The time difference, scaled by the speed of sound through water and divided by two, is the distance between the two platforms. This technique, when used with multiple transducers/hydrophones/projectors, can calculate the relative positions of static and moving objects in water.
In combat situations, an active pulse can be detected by an opponent and will reveal a submarine's position.
A very directional, but low-efficiency, type of sonar (used by fisheries, military, and for port security) makes use of a complex nonlinear feature of water known as non-linear sonar, the virtual transducer being known as a "parametric array".
Project Artemis.
Project Artemis was a one-of-a-kind low-frequency sonar for surveillance that was deployed off Bermuda for several years in the early 1960s. The active portion was deployed from a World War II tanker, and the receiving array was a built into a fixed position on an offshore bank.
Transponder.
This is an active sonar device that receives a stimulus and immediately (or with a delay) retransmits the received signal or a predetermined one.
Performance prediction.
A sonar target is small relative to the sphere, centred around the emitter, on which it is located. Therefore, the power of the reflected signal is very low, several orders of magnitude less than the original signal. Even if the reflected signal was of the same power, the following example (using hypothetical values) shows the problem: Suppose a sonar system is capable of emitting a 10,000 W/m² signal at 1 m, and detecting a 0.001 W/m² signal. At 100 m the signal will be 1 W/m² (due to the inverse-square law). If the entire signal is reflected from a 10 m² target, it will be at 0.001 W/m² when it reaches the emitter, i.e. just detectable. However, the original signal will remain above 0.001 W/m² until 300 m. Any 10 m² target between 100 and 300 m using a similar or better system would be able to detect the pulse but would not be detected by the emitter. The detectors must be very sensitive to pick up the echoes. Since the original signal is much more powerful, it can be detected many times further than twice the range of the sonar (as in the example).
In active sonar there are two performance limitations, due to noise and reverberation. In general one or other of these will dominate so that the two effects can be initially considered separately.
In noise limited conditions at initial detection:
where SL is the source level, TL is the transmission loss (or propagation loss), TS is the target strength, NL is the noise level, DI is the directivity index of the array (an approximation to the array gain) and DT is the detection threshold.
In reverberation limited conditions at initial detection (neglecting array gain):
where RL is the reverberation level and the other factors are as before.
Passive sonar.
Passive sonar listens without transmitting. It is often employed in military settings, although it is also used in science applications, "e.g.", detecting fish for presence/absence studies in various aquatic environments - see also passive acoustics and passive radar. In the very broadest usage, this term can encompass virtually any analytical technique involving remotely generated sound, though it is usually restricted to techniques applied in an aquatic environment.
Identifying sound sources.
Passive sonar has a wide variety of techniques for identifying the source of a detected sound. For example, U.S. vessels usually operate 60 Hz alternating current power systems. If transformers or generators are mounted without proper vibration insulation from the hull or become flooded, the 60 Hz sound from the windings can be emitted from the submarine or ship. This can help to identify its nationality, as all European submarines and nearly every other nation's submarine have 50 Hz power systems. Intermittent sound sources (such as a wrench being dropped) may also be detectable to passive sonar. Until fairly recently, an experienced, trained operator identified signals, but now computers may do this.
Passive sonar systems may have large sonic databases, but the sonar operator usually finally classifies the signals manually. A computer system frequently uses these databases to identify classes of ships, actions (i.e. the speed of a ship, or the type of weapon released), and even particular ships. Publications for classification of sounds are provided by and continually updated by the US Office of Naval Intelligence.
Noise limitations.
Passive sonar on vehicles is usually severely limited because of noise generated by the vehicle. For this reason, many submarines operate nuclear reactors that can be cooled without pumps, using silent convection, or fuel cells or batteries, which can also run silently. Vehicles' propellers are also designed and precisely machined to emit minimal noise. High-speed propellers often create tiny bubbles in the water, and this cavitation has a distinct sound.
The sonar hydrophones may be towed behind the ship or submarine in order to reduce the effect of noise generated by the watercraft itself. Towed units also combat the thermocline, as the unit may be towed above or below the thermocline.
The display of most passive sonars used to be a two-dimensional waterfall display. The horizontal direction of the display is bearing. The vertical is frequency, or sometimes time. Another display technique is to color-code frequency-time information for bearing. More recent displays are generated by the computers, and mimic radar-type plan position indicator displays.
Performance prediction.
Unlike active sonar, only one way propagation is involved. Because of the different signal processing used, the minimum detectable signal to noise ratio will be different. The equation for determining the performance of a passive sonar is:
where SL is the source level, TL is the transmission loss, NL is the noise level, DI is the directivity index of the array (an approximation to the array gain) and DT is the detection threshold. The figure of merit of a passive sonar is:
Performance factors.
The detection, classification and localisation performance of a sonar depends on the environment and the receiving equipment, as well as the transmitting equipment in an active sonar or the target radiated noise in a passive sonar.
Sound propagation.
Sonar operation is affected by variations in sound speed, particularly in the vertical plane. Sound travels more slowly in fresh water than in sea water, though the difference is small. The speed is determined by the water's bulk modulus and mass density. The bulk modulus is affected by temperature, dissolved impurities (usually salinity), and pressure. The density effect is small. The speed of sound (in feet per second) is approximately:
This empirically derived approximation equation is reasonably accurate for normal temperatures, concentrations of salinity and the range of most ocean depths. Ocean temperature varies with depth, but at between 30 and 100 meters there is often a marked change, called the thermocline, dividing the warmer surface water from the cold, still waters that make up the rest of the ocean. This can frustrate sonar, because a sound originating on one side of the thermocline tends to be bent, or refracted, through the thermocline. The thermocline may be present in shallower coastal waters. However, wave action will often mix the water column and eliminate the thermocline. Water pressure also affects sound propagation: higher pressure increases the sound speed, which causes the sound waves to refract away from the area of higher sound speed. The mathematical model of refraction is called Snell's law.
If the sound source is deep and the conditions are right, propagation may occur in the 'deep sound channel'. This provides extremely low propagation loss to a receiver in the channel. This is because of sound trapping in the channel with no losses at the boundaries. Similar propagation can occur in the 'surface duct' under suitable conditions. However in this case there are reflection losses at the surface.
In shallow water propagation is generally by repeated reflection at the surface and bottom, where considerable losses can occur.
Sound propagation is affected by absorption in the water itself as well as at the surface and bottom. This absorption depends upon frequency, with several different mechanisms in sea water. Long-range sonar uses low frequencies to minimise absorption effects.
The sea contains many sources of noise that interfere with the desired target echo or signature. The main noise sources are waves and shipping. The motion of the receiver through the water can also cause speed-dependent low frequency noise.
Scattering.
When active sonar is used, scattering occurs from small objects in the sea as well as from the bottom and surface. This can be a major source of interference. This acoustic scattering is analogous to the scattering of the light from a car's headlights in fog: a high-intensity pencil beam will penetrate the fog to some extent, but broader-beam headlights emit much light in unwanted directions, much of which is scattered back to the observer, overwhelming that reflected from the target ("white-out"). For analogous reasons active sonar needs to transmit in a narrow beam to minimise scattering.
Target characteristics.
The sound "reflection" characteristics of the target of an active sonar, such as a submarine, are known as its target strength. A complication is that echoes are also obtained from other objects in the sea such as whales, wakes, schools of fish and rocks.
Passive sonar detects the target's "radiated" noise characteristics. The radiated spectrum comprises a continuous spectrum of noise with peaks at certain frequencies which can be used for classification.
Countermeasures.
"Active" (powered) countermeasures may be launched by a submarine under attack to raise the noise level, provide a large false target, and obscure the signature of the submarine itself.
"Passive" (i.e., non-powered) countermeasures include:
Military applications.
Modern naval warfare makes extensive use of both passive and active sonar from water-borne vessels, aircraft and fixed installations. Although active sonar was used by surface craft in World War II, submarines avoided the use of active sonar due to the potential for revealing their presence and position to enemy forces. However, the advent of modern signal-processing enabled the use of passive sonar as a primary means for search and detection operations. In 1987 a division of Japanese company Toshiba reportedly sold machinery to the Soviet Union that allowed their submarine propeller blades to be milled so that they became radically quieter, making the newer generation of submarines more difficult to detect.
The use of active sonar by a submarine to determine bearing is extremely rare and will not necessarily give high quality bearing or range information to the submarines fire control team;however, use of active sonar on surface ships is very common. Active sonar is used by submarines when if the tactical situation dictates it is more important to determine the position of a hostile submarine than conceal their own position. With surface ships it might be assumed that the threat is already tracking the ship with satellite data. Any vessel around the emitting sonar will detect the emission. Having heard the signal, it is easy to identify the sonar equipment used (usually with its frequency) and its position (with the sound wave's energy). Active sonar is similar to radar in that, while it allows detection of targets at a certain range, it also enables the emitter to be detected at a far greater range, which is undesirable.
Since active sonar reveals the presence and position of the operator, and does not allow exact classification of targets, it is used by fast (planes, helicopters) and by noisy platforms (most surface ships) but rarely by submarines. When active sonar is used by surface ships or submarines, it is typically activated very briefly at intermittent periods to minimize the risk of detection. Consequently active sonar is normally considered a backup to passive sonar. In aircraft, active sonar is used in the form of disposable sonobuoys that are dropped in the aircraft's patrol area or in the vicinity of possible enemy sonar contacts.
Passive sonar has several advantages. Most importantly, it is silent. If the target radiated noise level is high enough, it can have a greater range than active sonar, and allows the target to be identified. Since any motorized object makes some noise, it may in principle be detected, depending on the level of noise emitted and the ambient noise level in the area, as well as the technology used. To simplify, passive sonar "sees" around the ship using it. On a submarine, nose-mounted passive sonar detects in directions of about 270°, centered on the ship's alignment, the hull-mounted array of about 160° on each side, and the towed array of a full 360°. The invisible areas are due to the ship's own interference. Once a signal is detected in a certain direction (which means that something makes sound in that direction, this is called broadband detection) it is possible to zoom in and analyze the signal received (narrowband analysis). This is generally done using a Fourier transform to show the different frequencies making up the sound. Since every engine makes a specific sound, it is straightforward to identify the object. Databases of unique engine sounds are part of what is known as "acoustic intelligence" or ACINT.
Another use of passive sonar is to determine the target's trajectory. This process is called Target Motion Analysis (TMA), and the resultant "solution" is the target's range, course, and speed. TMA is done by marking from which direction the sound comes at different times, and comparing the motion with that of the operator's own ship. Changes in relative motion are analyzed using standard geometrical techniques along with some assumptions about limiting cases.
Passive sonar is stealthy and very useful. However, it requires high-tech electronic components and is costly. It is generally deployed on expensive ships in the form of arrays to enhance detection. Surface ships use it to good effect; it is even better used by submarines, and it is also used by airplanes and helicopters, mostly to a "surprise effect", since submarines can hide under thermal layers. If a submarine's commander believes he is alone, he may bring his boat closer to the surface and be easier to detect, or go deeper and faster, and thus make more sound.
Examples of sonar applications in military use are given below. Many of the civil uses given in the following section may also be applicable to naval use.
Anti-submarine warfare.
Until recently, ship sonars were usually with hull mounted arrays, either amidships or at the bow. It was soon found after their initial use that a means of reducing flow noise was required. The first were made of canvas on a framework, then steel ones were used. Now domes are usually made of reinforced plastic or pressurized rubber. Such sonars are primarily active in operation. An example of a conventional hull mounted sonar is the SQS-56.
Because of the problems of ship noise, towed sonars are also used. These also have the advantage of being able to be placed deeper in the water. However, there are limitations on their use in shallow water. These are called towed arrays (linear) or variable depth sonars (VDS) with 2/3D arrays. A problem is that the winches required to deploy/recover these are large and expensive. VDS sets are primarily active in operation while towed arrays are passive.
An example of a modern active/passive ship towed sonar is Sonar 2087 made by Thales Underwater Systems.
Torpedoes.
Modern torpedoes are generally fitted with an active/passive sonar. This may be used to home directly on the target, but wake following torpedoes are also used. An early example of an acoustic homer was the Mark 37 torpedo.
Torpedo countermeasures can be towed or free. An early example was the German "Sieglinde" device while the "Bold" was a chemical device. A widely used US device was the towed AN/SLQ-25 Nixie while Mobile submarine simulator (MOSS) was a free device. A modern alternative to the Nixie system is the UK Royal Navy S2170 Surface Ship Torpedo Defence system.
Mines.
Mines may be fitted with a sonar to detect, localize and recognize the required target. Further information is given in acoustic mine and an example is the CAPTOR mine.
Mine countermeasures.
Mine Countermeasure (MCM) Sonar, sometimes called "Mine and Obstacle Avoidance Sonar (MOAS)", is a specialized type of sonar used for detecting small objects. Most MCM sonars are hull mounted but a few types are VDS design. An example of a hull mounted MCM sonar is the Type 2193 while the SQQ-32 Mine-hunting sonar and Type 2093 systems are VDS designs. See also Minesweeper (ship)
Submarine navigation.
Submarines rely on sonar to a greater extent than surface ships as they cannot use radar at depth. The sonar arrays may be hull mounted or towed. Information fitted on typical fits is given in Oyashio class submarine and Swiftsure class submarine.
Aircraft.
Helicopters can be used for antisubmarine warfare by deploying fields of active/passive sonobuoys or can operate dipping sonar, such as the AQS-13. Fixed wing aircraft can also deploy sonobuoys and have greater endurance and capacity to deploy them. Processing from the sonobuoys or Dipping Sonar can be on the aircraft or on ship. Dipping sonar has the advantage of being deployable to depths appropriate to daily conditions Helicopters have also been used for mine countermeasure missions using towed sonars such as the AQS-20A.
Underwater communications.
Dedicated sonars can be fitted to ships and submarines for underwater communication. See also the section on the underwater acoustics page.
Ocean surveillance.
For many years, the United States operated a large set of passive sonar arrays at various points in the world's oceans, collectively called Sound Surveillance System (SOSUS) and later Integrated Undersea Surveillance System (IUSS). A similar system is believed to have been operated by the Soviet Union. As permanently mounted arrays in the deep ocean were utilised, they were in very quiet conditions so long ranges could be achieved. Signal processing was carried out using powerful computers ashore. With the ending of the Cold War a SOSUS array has been turned over to scientific use.
In the United States Navy, a special badge known as the Integrated Undersea Surveillance System Badge is awarded to those who have been trained and qualified in its operation.
Underwater security.
Sonar can be used to detect frogmen and other scuba divers. This can be applicable around ships or at entrances to ports. Active sonar can also be used as a deterrent and/or disablement mechanism. One such device is the Cerberus system.
See Underwater Port Security System and Anti-frogman techniques#Ultrasound detection.
Hand-held sonar.
Limpet Mine Imaging Sonar (LIMIS) is a hand-held or ROV-mounted imaging sonar designed for patrol divers (combat frogmen or clearance divers) to look for limpet mines in low visibility water.
The LUIS is another imaging sonar for use by a diver.
Integrated Navigation Sonar System (INSS) is a small flashlight-shaped handheld sonar for divers that displays range.
Intercept sonar.
This is a sonar designed to detect and locate the transmissions from hostile active sonars. An example of this is the Type 2082 fitted on the British Vanguard class submarines.
Civilian applications.
Fisheries.
Fishing is an important industry that is seeing growing demand, but world catch tonnage is falling as a result of serious resource problems. The industry faces a future of continuing worldwide consolidation until a point of sustainability can be reached. However, the consolidation of the fishing fleets are driving increased demands for sophisticated fish finding electronics such as sensors, sounders and sonars. Historically, fishermen have used many different techniques to find and harvest fish. However, acoustic technology has been one of the most important driving forces behind the development of the modern commercial fisheries.
Sound waves travel differently through fish than through water because a fish's air-filled swim bladder has a different density than seawater. This density difference allows the detection of schools of fish by using reflected sound. Acoustic technology is especially well suited for underwater applications since sound travels farther and faster underwater than in air. Today, commercial fishing vessels rely almost completely on acoustic sonar and sounders to detect fish. Fishermen also use active sonar and echo sounder technology to determine water depth, bottom contour, and bottom composition.
Companies such as eSonar, Raymarine UK, Marport Canada, Wesmar, Furuno, Krupp, and Simrad make a variety of sonar and acoustic instruments for the deep sea commercial fishing industry. For example, net sensors take various underwater measurements and transmit the information back to a receiver on board a vessel. Each sensor is equipped with one or more acoustic transducers depending on its specific function. Data is transmitted from the sensors using wireless acoustic telemetry and is received by a hull mounted hydrophone. The analog signals are decoded and converted by a digital acoustic receiver into data which is transmitted to a bridge computer for graphical display on a high resolution monitor.
Echo sounding.
Echo sounding is a process used to determine the depth of water beneath ships and boats. A type of active sonar, echo sounding is the transmission of an acoustic pulse directly downwards to the seabed, measuring the time between transmission and echo return, after having hit the bottom and bouncing back to its ship of origin. The acoustic pulse is emitted by a transducer which receives the return echo as well. The depth measurement is calculated by multiplying the speed of sound in water(averaging 1,500 meters per second) by the time between emission and echo return.
The value of underwater acoustics to the fishing industry has led to the development of other acoustic instruments that operate in a similar fashion to echo-sounders but, because their function is slightly different from the initial model of the echo-sounder, have been given different terms.
Net location.
The net sounder is an echo sounder with a transducer mounted on the headline of the net rather than on the bottom of the vessel. Nevertheless, to accommodate the distance from the transducer to the display unit, which is much greater than in a normal echo-sounder, several refinements have to be made. Two main types are available. The first is the cable type in which the signals are sent along a cable. In this case there has to be the provision of a cable drum on which to haul, shoot and stow the cable during the different phases of the operation. The second type is the cable less net-sounder – such as Marport’s Trawl Explorer - in which the signals are sent acoustically between the net and hull mounted receiver/hydrophone on the vessel. In this case no cable drum is required but sophisticated electronics are needed at the transducer and receiver.
The display on a net sounder shows the distance of the net from the bottom (or the surface), rather than the depth of water as with the echo-sounder's hull-mounted transducer. Fixed to the headline of the net, the footrope can usually be seen which gives an indication of the net performance. Any fish passing into the net can also be seen, allowing fine adjustments to be made to catch the most fish possible. In other fisheries, where the amount of fish in the net is important, catch sensor transducers are mounted at various positions on the cod-end of the net. As the cod-end fills up these catch sensor transducers are triggered one by one and this information is transmitted acoustically to display monitors on the bridge of the vessel. The skipper can then decide when to haul the net.
Modern versions of the net sounder, using multiple element transducers, function more like a sonar than an echo sounder and show slices of the area in front of the net and not merely the vertical view that the initial net sounders used.
The sonar is an echo-sounder with a directional capability that can show fish or other objects around the vessel.
ROV and UUV.
Small sonars have been fitted to Remotely Operated Vehicles (ROV) and Unmanned Underwater Vehicles (UUV) to allow their operation in murky conditions. These sonars are used for looking ahead of the vehicle. The Long-Term Mine Reconnaissance System is an UUV for MCM purposes.
Vehicle location.
Sonars which act as beacons are fitted to aircraft to allow their location in the event of a crash in the sea. Short and Long Baseline sonars may be used for caring out the location, such as LBL.
Prosthesis for the visually impaired.
In 2013 an inventor in the United States unveiled a "spider-sense" bodysuit, equipped with ultrasonic sensors and haptic feedback systems, which alerts the wearer of incoming threats; allowing them to respond to attackers even when blindfolded.
Scientific applications.
Biomass estimation.
Detection of fish, and other marine and aquatic life, and estimation their individual sizes or total biomass using active sonar techniques. As the sound pulse travels through water it encounters objects that are of different density or acoustic characteristics than the surrounding medium, such as fish, that reflect sound back toward the sound source. These echoes provide information on fish size, location, abundance and behavior. Data is usually processed and analysed using a variety of software such as Echoview.
See Also: Hydroacoustics and Fisheries Acoustics.
Wave measurement.
An upward looking echo sounder mounted on the bottom or on a platform may be used to make measurements of wave height and period. From this statistics of the surface conditions at a location can be derived.
Water velocity measurement.
Special short range sonars have been developed to allow measurements of water velocity.
Bottom type assessment.
Sonars have been developed that can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Relatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings. Advanced substrate classification analysis can be achieved using calibrated (scientific) echosounders and parametric or fuzzy-logic analysis of the acoustic data (See: Acoustic Seabed Classification)
Bathymetric mapping.
Side-scan sonars can be used to derive maps of seafloor topography (bathymetry) by moving the sonar across it just above the bottom. Low frequency sonars such as GLORIA have been used for continental shelf wide surveys while high frequency sonars are used for more detailed surveys of smaller areas.
Sub-bottom profiling.
Powerful low frequency echo-sounders have been developed for providing profiles of the upper layers of the ocean bottom.
Synthetic aperture sonar.
Various synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.
Parametric sonar.
Parametric sources use the non-linearity of water to generate the difference frequency between two high frequencies. A virtual end-fire array is formed. Such a projector has advantages of broad bandwidth, narrow beamwidth, and when fully developed and carefully measured it has no obvious sidelobes: see Parametric array. Its major disadvantage is very low efficiency of only a few percent. P.J. Westervelt's seminal 1963 JASA paper summarizes the trends involved.
Sonar in space.
Use of sonar has been proposed for determining the depth of hydrocarbon seas on Titan.
Effect of sonar on marine life.
Effect on marine mammals.
Research has shown that use of active sonar can lead to mass strandings of marine mammals. Beaked whales, the most common casualty of the strandings, have been shown to be highly sensitive to mid-frequency active sonar. Other marine mammals such as the blue whale also flee away from the source of the sonar, while naval activity was suggested to be the most probable cause of a mass stranding of dolphins. The US Navy, which part-funded some of studies, said the findings only showed behavioural responses to sonar, not actual harm, but "will evaluate the effectiveness of [their] marine mammal protective measures in light of new research findings."
Some marine animals, such as whales and dolphins, use echolocation systems, sometimes called "biosonar" to locate predators and prey. It is conjectured that active sonar transmitters could confuse these animals and interfere with basic biological functions such as feeding and mating.
Effect on fish.
High intensity sonar sounds can create a small temporary shift in the hearing threshold of some fish. 
Frequencies and resolutions.
The frequencies of sonars range from infrasonic to above a megahertz. Generally, the lower frequencies have longer range, while the higher frequencies offer better resolution, and smaller size for a given directionality.
To achieve reasonable directionality, frequencies below 1 kHz generally require large size, usually achieved as towed arrays.
Low frequency sonars are loosely defined as 1–5 kHz, albeit some navies regard 5–7 kHz also as low frequency. Medium frequency is defined as 5–15 kHz. Another style of division considers low frequency to be under 1 kHz, and medium frequency at between 1–10 kHz.
American World War II era sonars operated at a relatively high frequency of 20–30 kHz, to achieve directionality with reasonably small transducers, with typical maximum operational range of 2500 yd. Postwar sonars used lower frequencies to achieve longer range; e.g. SQS-4 operated at 10 kHz with range up to 5000 yd. SQS-26 and SQS-53 operated at 3 kHz with range up to 20,000 yd; their domes had size of approx. a 60-ft personnel boat, an upper size limit for conventional hull sonars. Achieving larger sizes by conformal sonar array spread over the hull has not been effective so far, for lower frequencies linear or towed arrays are therefore used.
Japanese WW2 sonars operated at a range of frequencies. The Type 91, with 30 inch quartz projector, worked at 9 kHz. The Type 93, with smaller quartz projectors, operated at 17.5 kHz (model 5 at 16 or 19 kHz magnetostrictive) at powers between 1.7 and 2.5 kilowatts, with range of up to 6 km. The later Type 3, with German-design magnetostrictive transducers, operated at 13, 14.5, 16, or 20 kHz (by model), using twin transducers (except model 1 which had three single ones), at 0.2 to 2.5 kilowatts. The Simple type used 14.5 kHz magnetostrictive transducers at 0.25 kW, driven by capacitive discharge instead of oscillators, with range up to 2.5 km.
The sonar's resolution is angular; objects further apart will be imaged with lower resolutions than nearby ones.
Another source lists ranges and resolutions vs frequencies for sidescan sonars. 30 kHz provides low resolution with range of 1000–6000 m, 100 kHz gives medium resolution at 500–1000 m, 300 kHz gives high resolution at 150–500 m, and 600 kHz gives high resolution at 75–150 m. Longer range sonars are more adversely affected by nonhomogenities of water. Some environments, typically shallow waters near the coasts, have complicated terrain with many features; higher frequencies become necessary there.
As a specific example, the Sonar 2094 Digital, a towed fish capable of reaching depth of 1000 or 2000 meters, performs side-scanning at 114 kHz (600m range at each side, 50 by 1 degree beamwidth) and 410 kHz (150m range, 40 by 0.3 degree beamwidth), with 3 kW pulse power.
A JW Fishers system offers side-scanning at 1200 kHz with very high spatial resolution, optionally coupled with longer-range 600 kHz (range 200 ft at each side) or 100 kHz (up to 2000 ft per side, suitable for scanning large areas for big targets).
Bibliography.
Fisheries Acoustics References
</dl>

</doc>
<doc id="29440" url="http://en.wikipedia.org/wiki?curid=29440" title="Slavs">
Slavs

The Slavs are an Indo-European ethno-linguistic group living in Central Europe, Eastern Europe, Southeast Europe, North Asia and Central Asia, who speak the Indo-European Slavic languages, and share, to varying degrees, certain cultural traits and historical backgrounds. From the early 6th century they spread to inhabit most of Central and Eastern Europe and Southeast Europe. Over half of Europe's territory is inhabited by Slavic-speaking communities. Present-day Slavic people are classified into West Slavs, East Slavs and South Slavs.
Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.
Overview.
The Slavs are an Indo-European ethno-linguistic group living in Central Europe, Eastern Europe, Southeast Europe, North Asia and Central Asia, who speak the Indo-European Slavic languages, and share, to varying degrees, certain cultural traits and historical backgrounds. From the early 6th century they spread to inhabit most of Central and Eastern Europe and Southeast Europe; while at the other geographic extreme, Slavic mercenaries fighting for the Byzantines and Arabs settled Asia Minor and even as far as Syria. Later, East Slavs colonized Siberia and Central Asia. Every Slavic ethnicity has emigrated to other parts of the world. Over half of Europe's territory is inhabited by Slavic-speaking communities.
Modern nations and ethnic groups called by the ethnonym "Slavs" are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.
Present-day Slavic people are classified into West Slavic (chiefly Poles, Czechs and Slovaks), East Slavic (chiefly Russians, Belarusians, and Ukrainians), and South Slavic (chiefly Serbs, Bulgarians, Croats, Bosniaks, Macedonians, Slovenes, and Montenegrins). For a more comprehensive list, see the ethnocultural subdivisions.
Ethnonym.
The Slavic autonym is reconstructed in Proto-Slavic as "*Slověninъ", plural "*Slověně". The oldest documents written in Old Church Slavonic and dating from the 9th century attest Словѣне "Slověne" to describe the Slavs. Other early Slavic attestations include Old East Slavic Словѣнѣ "Slověně" for "an East Slavic group near Novgorod." However, the earliest written references to the Slavs under this name are in other languages. In the 6th century AD Procopius, writing in Byzantine Greek, refers to the Σκλάβοι "Sklaboi", Σκλαβηνοί "Sklabēnoi", Σκλαυηνοί "Sklauenoi", Σθλαβηνοί "Sthlauenoi", or Σκλαβῖνοι "Sklabinoi", while his contemporary Jordanes refers to the "Sclaveni" in Latin.
The Slavic autonym "*Slověninъ" is usually considered a derivation from "slovo" "word", originally denoting "people who speak (the same language)," i.e. people who understand each other, in contrast to the Slavic word denoting "foreign people" – "němci", meaning "mumbling, murmuring people" (from Slavic "*němъ" – "mumbling, mute"). The latter word may be the derivation of words to denote German/Germanic people in many later Slavic languages: e.g., Czech "Němec", Slovak "Nemec", Slovene "Nemec", Belarusian, Russian and Bulgarian "Немец", Serbian "Немац, "Serbian", "Bosnian" and "Croatian" Nijemac", Polish "Niemiec", Ukrainian "Німець", etc., but another theory states that rather these words are derived from the name of the Nemetes tribe, which is derived from the Celtic root "nemeto-".
The English word Slav could be derived from the Middle English word "sclave", which was borrowed from Medieval Latin "sclavus" or "slavus", itself a borrowing and Byzantine Greek σκλάβος "sklábos" "slave," which was in turn apparently derived from a misunderstanding of the Slavic autonym (denoting a speaker of their own languages). The Byzantine term "Sklavinoi" was loaned into Arabic as "Saqaliba صقالبة" (sing. "Saqlabi صقلبي") by medieval Arab historiographers. However, the origin of this word is disputed.
Alternative proposals for the etymology of "*Slověninъ" propounded by some scholars have much less support. Lozinski argues that the word "*slava" once had the meaning of "worshipper," in this context meaning "practicer of a common Slavic religion," and from that evolved into an ethnonym. S.B. Bernstein speculates that it derives from a reconstructed Proto-Indo-European "*(s)lawos", cognate to Ancient Greek λαός "laós" "population, people," which itself has no commonly accepted etymology. Meanwhile others have pointed out that the suffix -enin indicates a man from a certain place, which in this case should be a place called Slova or Slava, possibly a river name. The Old East Slavic "Slavuta" for the Dnieper River was argued by Henrich Bartek (1907–1986) to be derived from "slova" and also the origin of Slovene.
Last scientific opinions about the earliest mentions of Slavic raids across the lower River Danube show that they may be dated to the first half of the 6th century, yet no archaeological evidence of a Slavic settlement in the Balkans could be securely dated before c. 600 AD.
Early history.
Discourse on the early Slavs.
The meaning of the term Slav depends upon the context in which it is used. This term refers to a culture (or cultures) living north of the River Danube, east of the River Elbe, and west of the River Vistula during the 530s CE. In addition, Slav is an identifier for the common ethnic group. Furthermore, Slav denotes any language with linguistic ties to the modern Slavic language family (which has no connection to a common culture or shared ethnicity). Despite the various notions of Slav, it is unclear whether any of these descriptions add to an accurate representation of that group's history, since historians, such as George Vernadsky, Florin Curta, and Michael Karpovich have called into question how, why, and to what degree the Slavs were cohesive as a society between the 6th and 9th centuries. When discussing the evidence that specialists use to construct a plausible history of the Slavs, the information tends to fall into three avenues of research: the archeological, the historiographic, and the linguistic.
Archaeologically, myriad physical evidence from that time period pertains to the Slavs. This evidence ranges from hill forts, to ceramic pots and fragments, to abodes. However, there are three major problems in studying the spread of early Slavic groups by purely archaeological methods. Archaeologists face difficulties in distinguishing which finds are truly Slavic and which are not. In addition, many of these findings are either inaccurately carbon-dated or so isolated that they do not reflect organized Slavic settlement. The combination of these facts makes it difficult to create a reliable chronology of ceramic materials, hill forts, houses, brooches, and other small artifacts. As a result, using archaeological finds without other forms of evidence is not wholly reliable for historical debates about this group. The lack of grave sites also diminishes archaeologists' abilities to assess how the Slavs changed as a people, both in terms of their social behavior and their migratory patterns. Consequently, discerning where in northern Europe Slavic groups lived during the 6th to 9th centuries represents a challenge. The cumulative effects of these difficulties prevents the construction of a thorough history of Slavic development in Northern Europe during this period through archaeological evidence alone.
Historiographically, a number of sources describe the Slavs. However there are several problems using these texts to build upon the available knowledge of the early Slavs, even when used in a multidisciplinary fashion. The useful historical information about the Slavs from these texts is either cryptic or lacks any mention of their sources. Moreover, these works tend to discuss the Slavs only in terms of their effects on surrounding empires, particularly the Byzantines and the Franks. The variety of names from historiographic texts that refer to the Slavs, such as the Antes, Sclaveni and Venethi, in addition to the locales and regions which they at one point or another occupied, makes it laborious to establish a geographical boundary for major Slavic settlement. This is a troublesome task when the names of these places have not always remained the same or even survived. Most importantly, the majority of the texts utilized to describe the Slavs during this period are either second-hand accounts or describe an encounter with these groups years, decades, or centuries after it occurred. While earlier texts contextualize the Slavs' early history and later development, texts written about an event long after it had occurred make the relevant information less reliable. Unfortunately, neither earlier nor later texts directly aid understanding of the Slavs during the 6th to 9th centuries.
Linguistically, the pursuit of a Slavic history is also problematic. This pursuit has focused on three main areas of study: Slavic geographical names, names of flora and fauna, and "lexical and structural similarities and differences between Slavic and other languages. " The use of ethnic identifiers in written texts during and after the 500s, such as the description of the Slavs as Antes, Sclaveni, and Venethi by their immediate neighbors, produces problems. Moreover, the concept of ethnicity during this period was so fluid that different ethnicities would be ascribed to the same group depending upon the situation of the encounter, such as in Michal Parczewski's map. This map, a conglomeration of different written fragments about the Slavs' homeland, selectively draws upon these fragments. In order to validate his preconceived theories about Slavic migration, Parczewski omitted information from his sources which directly contradicted his conclusions, thus making the map of Slavic settlement in relation to their neighbors during the 6th century extremely suspect. Moreover, the association of particular styles of pots and burials with specific ethnonyms by archaeologists, and extremely selective use of historiographic materials, presumes a direct connection between language and ethnicity. These facts reinforce how subjective ethnic identification can be, especially in a region where many tribal groups existed and identified themselves as distinct from one another.
The history of the early Slavs is inseparable from the political agenda behind much 19th- and 20th-century archaeological, linguistic, and historiographic research. Florin Curta, an expert on the history of the early Slavs, contends that the process of creating such a history "was a function of both ethnic formation and ethnic identification". However, this process became extremely blurred by a myriad of interests. These agendas ranged from Pan-Slavic researchers in Central and Eastern Europe during the 18th and 19th centuries, to post-World War Two European nations strengthening their newfound legitimacy, to contemporary politicization of historical, archaeological, and linguistic discourse.
Origins.
Homeland.
The location of the Slavic homeland has been the subject of significant debate. The Prague-Penkov-Kolochin complex of cultures of the 6th to 7th centuries AD are generally accepted to reflect the expansion of Slavic-speakers at that time. Serious candidates for the core from which they expanded are cultures within the territories of modern Belarus, Poland, and Ukraine. The proposed frameworks are:
Ethnogenesis scenarios.
The Globular Amphora culture stretches from the middle Dnieper to the Elbe in the late 4th and early 3rd millennia BCE. It has been suggested as the locus of a Germano-Balto-Slavic continuum (compare Germanic substrate hypothesis), but the identification of its bearers as Indo-Europeans is uncertain. The area of this culture contains numerous tumuli – typical for IE originators.
The Chernoles culture (8th to 3rd centuries BC, sometimes associated with the "Scythian farmers" of Herodotus) is "sometimes portrayed as either a state in the development of the Slavic languages or at least some form of late Indo-European ancestral to the evolution of the Slavic stock." The Milograd culture (700 BCE – 100 CE), centered roughly on present-day Belarus, north of the contemporaneous Chernoles culture, has also been proposed as ancestral to either Slavs or Balts.
The ethnic composition of the bearers of the Przeworsk culture (2nd century BC to the 4th century AD, associated with the Lugii) of central and southern Poland, northern Slovakia and Ukraine, including the Zarubintsy culture (2nd century BCE to the 2nd century CE, also connected with the Bastarnae tribe) and the Oksywie culture are other candidates.
The area of southern Ukraine is known to have been inhabited by Scythian and Sarmatian tribes prior to the foundation of the Gothic kingdom. Early Slavic stone stelae found in the middle Dniester region are markedly different from the Scythian and Sarmatian stelae found in the Crimea.
The Wielbark Culture displaced the eastern Oksywie part of the Przeworsk culture from the 1st century AD. While the Chernyakhov culture; 2nd to 5th centuries CE leads to the decline of the late Sarmatian culture in the 2nd to 4th centuries, the western part of the Przeworsk culture remains intact until the 4th century, and the Kiev culture flourishes during the same time, in the 2nd to 5th centuries AD. This latter culture is recognized as the direct predecessor of the Prague-Korchak and Pen'kovo cultures (6th–7th centuries AD), the first archaeological cultures the bearers of which are indisputably identified as Slavic.
Proto-Slavic is thus likely to have reached its final stage in the Kiev area; there is, however, substantial disagreement in the scientific community over the identity of the Kiev culture's predecessors, with some scholars tracing it from the Ruthenian Milograd culture, others from the "Ukrainian" Chernoles and Zarubintsy cultures and still others from the "Polish" Przeworsk culture.
Research history.
The starting point in the autochthonous/allochthonous debate was the year 1745, when Johann Christoph de Jordan published "De Originibus Slavicis." The works of Slovak philologist and poet Pavel Jozef Šafárik (1795–1861) has influenced generations of scholars. The foundation of his theory was the work of Jordanes, the "Getica". Jordanes had equated the Sclavenes and the Antes to the Venethi (or Venedi) also known from much earlier sources, such as Pliny the Elder, Tacitus and Ptolemy. Šafárik bequeathed to posterity not only his vision of a Slavic history, but also a powerful methodology for exploring its Dark Ages: language. The Polish scholar Tadeusz Wojciechowski (1839–1919) was the first to use place names to write Slavic history. He was followed by A. L. Pogodin and the Polish botanist, J. Rostafinski.
The first to introduce archaeological data into the scholarly discourse about the early Slavs, Lubor Niederle (1865–1944), endorsed Rostafinski's theory in his multi-volume work "The Antiquities of the Slavs". Vykentyi V.Khvoika (1850–1914), a Ukrainian archaeologist of Czech origin, linked the Slavs with Neolithic Cucuteni culture. A. A. Spicyn (1858–1931) assigned to the Antes the finds of silver and bronze in central and southern Ukraine. Czech archaeologist Ivan Borkovsky (1897–1976) postulated the existence of a pottery "Prague type" which was a national, exclusively Slavic, pottery. Boris Rybakov, has issued a theory that made a link between both Spicyn's "Antian antiquities" and the remains excavated by Khvoika from Chernyakhov culture and that those should be attributed to the Slavs.
From the 19th century onwards, the debate became politically charged, particularly in connection with the history of the Partitions of Poland and German imperialism known as Drang nach Osten. The question whether Germanic or Slavic peoples were indigenous on the land east of the River Oder was used by factions to pursue their respective German and Polish political claims to governance of those lands.
Geneticists entered the debate in the 21st century. See the Genetics section.
Earliest accounts.
The relationship between the Slavs and a tribe called the Veneti east of the River Vistula in the Roman period is uncertain. The name may refer both to Balts and Slavs. 
The Slavs under name of the "Antes" and the "Sclaveni" make their first appearance in Byzantine records in the early 6th century. Byzantine historiographers under Justinian I (527–565), such as Procopius of Caesarea, Jordanes and Theophylact Simocatta describe tribes of these names emerging from the area of the Carpathian Mountains, the lower Danube and the Black Sea, invading the Danubian provinces of the Eastern Empire.
Procopius wrote in 545 that "the Sclaveni and the Antae actually had a single name in the remote past; for they were both called Spori in olden times." He describes their social structure and beliefs:
For these nations, the Sclaveni and the Antae, are not ruled by one man, but they have lived from of old under a democracy, and consequently everything which involves their welfare, whether for good or for ill, is referred to the people. It is also true that in all other matters, practically speaking, these two barbarian peoples have had from ancient times the same institutions and customs. For they believe that one god, the maker of lightning, is alone lord of all things, and they sacrifice to him cattle and all other victims.
He mentions that they were tall and hardy:
"They live in pitiful hovels which they set up far apart from one another, but, as a general thing, every man is constantly changing his place of abode. When they enter battle, the majority of them go against their enemy on foot carrying little shields and javelins in their hands, but they never wear corselets. Indeed, some of them do not wear even a shirt or a cloak, but gathering their trews up as far as to their private parts they enter into battle with their opponents. And both the two peoples have also the same language, an utterly barbarous tongue. Nay further, they do not differ at all from one another in appearance. For they are all exceptionally tall and stalwart men, while their bodies and hair are neither very fair or blond, nor indeed do they incline entirely to the dark type, but they are all slightly ruddy in color. And they live a hard life, giving no heed to bodily comforts ...".
Jordanes tells us that the Sclaveni had swamps and forests for their cities. Another 6th-century source refers to them living among nearly impenetrable forests, rivers, lakes, and marshes.
Menander Protector mentions a Daurentius (577–579) that slew an Avar envoy of Khagan Bayan I. The Avars asked the Slavs to accept the suzerainty of the Avars, he however declined and is reported as saying: "Others do not conquer our land, we conquer theirs – so it shall always be for us".
Migrations.
According to eastern homeland theory, prior to becoming known to the Roman world, Slavic-speaking tribes were part of the many multi-ethnic confederacies of Eurasia – such as the Sarmatian, Hun and Gothic empires. The Slavs emerged from obscurity when the westward movement of Germans in the 5th and 6th centuries CE (thought to be in conjunction with the movement of peoples from Siberia and Eastern Europe: Huns, and later Avars and Bulgars) started the great migration of the Slavs, who settled the lands abandoned by Germanic tribes fleeing the Huns and their allies: westward into the country between the Oder and the Elbe-Saale line; southward into Bohemia, Moravia, much of present day Austria, the Pannonian plain and the Balkans; and northward along the upper Dnieper river. Perhaps some Slavs migrated with the movement of the Vandals to Iberia and north Africa.
Around the 6th century, Slavs appeared on Byzantine borders in great numbers. The Byzantine records note that grass would not regrow in places where the Slavs had marched through, so great were their numbers. After a military movement even the Peloponnese and Asia Minor were reported to have Slavic settlements. This southern movement has traditionally been seen as an invasive expansion. By the end of the 6th century, Slavs had settled the Eastern Alps regions.
Early Slavic states.
When their migratory movements ended, there appeared among the Slavs the first rudiments of state organizations, each headed by a prince with a treasury and a defense force. Moreover, it was the beginnings of class differentiation, and nobles pledged allegiance either to the Frankish/ Holy Roman Emperors or the Byzantine Emperors.
In the 7th century, the Frankish merchant Samo, who supported the Slavs fighting their Avar rulers, became the ruler of the first known Slav state in Central Europe, which, however, most probably did not outlive its founder and ruler. This provided the foundation for subsequent Slavic states to arise on the former territory of this realm with Carantania being the oldest of them. Very old also are the Principality of Nitra and the Moravian principality (see under Great Moravia). In this period, there existed central Slavic groups and states such as the Balaton Principality, but the subsequent expansion of the Magyars, as well as the Germanisation of Austria, separated the northern and southern Slavs. The First Bulgarian Empire was founded in 681, the Slavic language Old Bulgarian became the main and official of the empire in 864. Bulgaria was instrumental in the spread of Slavic literacy and Christianity to the rest of the Slavic world.
Assimilation.
 
Throughout their history, Slavs came into contact with non-Slavic groups. In the postulated homeland region (present-day Ukraine), they had contacts with the Iranic Sarmatians and the Germanic Goths. After their subsequent spread, they began assimilating non-Slavic peoples. For example, in the Balkans, there were Paleo-Balkan peoples, such as Romanized and Hellenized (Jireček Line) Illyrians, Thracians and Dacians, as well as Greeks and Celtic Scordisci. Over time, due to the larger number of Slavs, most descendants of the indigenous populations of the Balkans were Slavicized. The Thracians and Illyrians vanished from the population during this period – although the modern Albanian nation claims descent from the Illyrians. Exceptions are Greece, where the lesser numbered Slavs scattered there came to be Hellenized (aided in time by more Greeks returning to Greece in the 9th century and the role of the church and administration) and Romania where Slavic people settled en route for present-day Greece, Republic of Macedonia, Bulgaria and East Thrace whereby the Slavic population had come to assimilate. Bulgars were also assimilated by local Slavs but their ruling status and subsequent land cast the nominal legacy of "Bulgarian country and people" onto all future generations. The Romance speakers within the fortified Dalmatian cities managed to retain their culture and language for a long time, as Dalmatian Romance was spoken until the high Middle Ages. However, they too were eventually assimilated into the body of Slavs.
In the Western Balkans, South Slavs and Germanic Gepids intermarried with Avar invaders, eventually producing a Slavicized population. In Central Europe, the Slavs intermixed with Germanic and Celtic, while the eastern Slavs encountered Uralic and Scandinavian peoples. Scandinavians (Varangians) and Finnic peoples were involved in the early formation of the Rus state but were completely Slavicized after a century. Some Finno-Ugric tribes in the north were also absorbed into the expanding Rus population. At the time of the Magyar migration, the present-day Hungary was inhabited by Slavs, numbering about 200,000, and by Romano-Dacians who were either assimilated or enslaved by the Magyars. In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchaks and the Pechenegs, caused a massive migration of East Slavic populations to the safer, heavily forested regions of the north. In the Middle Ages, groups of Saxon ore miners settled in medieval Bosnia, Serbia and Bulgaria where they were Slavicized.
Polabian Slavs (Wends) settled in parts of England (Danelaw), apparently as Danish allies. Polabian-Pomeranian Slavs are also known to have even settled on Norse age Iceland. "Saqaliba" refers to the Slavic mercenaries and slaves in the medieval Arab world in North Africa, Sicily and Al-Andalus. Saqaliba served as caliph's guards. In the 12th century, there was intensification of Slavic piracy in the Baltics. The Wendish Crusade was started against the Polabian Slavs in 1147, as a part of the Northern Crusades. Niklot, pagan chief of the Slavic Obodrites, began his open resistance when Lothar III, Holy Roman Emperor, invaded Slavic lands. In August 1160 Niklot was killed and German colonization (Ostsiedlung) of the Elbe-Oder region began. In Hanoverian Wendland, Mecklenburg-Vorpommern and Lusatia invaders started germanization. Early forms of germanization were described by German monks: Helmold in the manuscript Chronicon Slavorum and Adam of Bremen in Gesta Hammaburgensis ecclesiae pontificum. The Polabian language survived until the beginning of the 19th century in what is now the German state of Lower Saxony.
Cossacks, although Slavic-speaking and Orthodox Christians, came from a mix of ethnic backgrounds, including Tatars and other Turks. Many early members of the Terek Cossacks were Ossetians.
The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descend of this population.
Conversely, some Slavs were assimilated into other populations. Although the majority continued south, attracted by the riches of the territory which would become Bulgaria, a few remained in the Carpathian basin and were ultimately assimilated into the Magyar or Romanian population. There is a large number of river names and other placenames of Slavic origin in Romania. Similarly, the populations of the respective southern and eastern parts of Austria and eastern parts of Germany are to some degree made up of people with Slavic ancestry.
Modern history.
As of 1878, there were only three free Slavic states in the world: the Russian Empire, Serbia and Montenegro. Bulgaria was also free but was "de jure" vassal to the Ottoman Empire until official independence was declared in 1908. In the entire Austro-Hungarian Empire of approximately 50 million people, about 23 million were Slavs. The Slavic peoples who were, for the most part, denied a voice in the affairs of the Austro-Hungarian Empire, were calling for national self-determination. During World War I, representatives of the Czechs, Slovaks, Poles, Serbs, Croats, and Slovenes set up organizations in the Allied countries to gain sympathy and recognition. In 1918, after World War I ended, the Slavs established such independent states as Czechoslovakia, the Second Polish Republic, and the State of Slovenes, Croats and Serbs.
During World War II, Hitler's "Generalplan Ost" (general plan for the East) entailed killing, deporting, or enslaving the Slavic and Jewish population of occupied Eastern Europe to create "Lebensraum" (living space) for German settlers. The Nazi Hunger Plan and "Generalplan Ost" would have led to the starvation of 80 million people in the Soviet Union. These partially fulfilled plans resulted in the deaths of an estimated 19.3 million civilians and prisoners of war.
The first half of the 20th century in Russia and the Soviet Union was marked by a succession of wars, famines and other disasters, each accompanied by large-scale population losses. Stephen J. Lee estimates that, by the end of World War II in 1945, the Russian population was about 90 million fewer than it could have been otherwise.
Because of the vastness and diversity of the territory occupied by Slavic people, there were several centers of Slavic consolidation. In the 19th century, Pan-Slavism developed as a movement among intellectuals, scholars, and poets, but it rarely influenced practical politics and did not find support in some nations that had Slavic origins. Pan-Slavism became compromised when the Russian Empire started to use it as an ideology justifying its territorial conquests in Central Europe as well as subjugation of other ethnic groups of Slavic origins such as Poles and Ukrainians, and the ideology became associated with Russian imperialism. The common Slavic experience of communism combined with the repeated usage of the ideology by Soviet propaganda after World War II within the Eastern bloc (Warsaw Pact) was a forced high-level political and economic hegemony of the USSR dominated by Russians. A notable political union of the 20th century that covered most South Slavs was Yugoslavia, but it ultimately broke apart in the 1990s along with the Soviet Union.
The word "Slavs" was used in the national anthem of the Slovak Republic (1939–1945), Yugoslavia (1943–1992) and the Federal Republic of Yugoslavia (1992–2003), later Serbia and Montenegro (2003–2006).
Former Soviet states such as Kazakhstan, have very large minority Slavic populations with most being Russians. Also former satellite states and Warsaw Pact territories also have large minority Slavic populations also being Russian or Ukrainian and those from the three Slavic states in the Soviet Union. As of now, Kazakhstan have the largest Slavic minority population, with most being Russians.
Pan-Slavism.
Pan-Slavism, a movement which came into prominence in the mid-19th century, emphasized the common heritage and unity of all the Slavic peoples. The main focus was in the Balkans where the South Slavs had been ruled for centuries by other empires: the Byzantine Empire, Austria-Hungary, the Ottoman Empire, and Venice. The Russian Empire used Pan-Slavism as a political tool; as did the Soviet Union, which gained political-military influence and control over most Slavic-majority nations between 1945 and 1948 and retained a hegemonic role until the period 1989–1991.
Culture.
Language.
Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.
Slavic standard languages which are official in at least one country: Belarusian, Bosnian, Bulgarian, Croatian, Czech, Macedonian, Montenegrin, Polish, Russian, Serbian, Slovak, Slovene, and Ukrainian.
Proto-Slavic language.
Proto-Slavic, the supposed ancestor language of all Slavic languages, is a descendant of common Proto-Indo-European, via a Balto-Slavic stage in which it developed numerous lexical and morphophonological isoglosses with the Baltic languages. In the framework of the Kurgan hypothesis, "the Indo-Europeans who remained after the migrations [from the steppe] became speakers of Balto-Slavic".
Proto-Slavic, sometimes referred to as "Common Slavic" or "Late Proto-Slavic", is defined as the last stage of the language preceding the geographical split of the historical Slavic languages. That language was uniform, and on the basis of borrowings from foreign languages and Slavic borrowings into other languages, cannot be said to have any recognizable dialects, suggesting a comparatively compact homeland. Slavic linguistic unity was to some extent visible as late as Old Church Slavonic manuscripts which, though based on local Slavic speech of Thessaloniki, could still serve the purpose of the first common Slavic literary language.
Alphabet.
The alphabet depends on what religion is usual for the respective Slavic ethnic groups. The Orthodox use the Cyrillic alphabet and the Roman Catholics use Latin alphabet, the Bosniaks who are Muslims also use the Latin. Few Greek Roman and Roman Catholics use the Cyrillic alphabet however. The Serbian language and Montenegrin language uses both Cyrillic and Latin alphabets. There is also a Latin script to write in Belarusian, called the Lacinka alphabet.
Religion.
Most Slavic populations gradually adopted Christianity (Orthodox Christianity for the East Slavs and Roman Catholicism for the West Slavs, with South Slavs split by the two churches) between the 6th and 10th centuries, and consequently their old pagan beliefs declined. See also Slavic Neopaganism.
The majority of contemporary Slavs who profess a religion are Orthodox, followed by Roman Catholic. A very small minority are Protestant. Bosniaks, Gorani, Torbeshi and Pomaks are Muslims, and many other Slavic ethnic groups have lived in close contact with the Muslim world for centuries. Religious delineations by nationality can be very sharp; usually in the Slavic ethnic groups the vast majority of religious people share the same religion. Some Slavs are atheist or agnostic: only 19% of Czechs professed belief in god/s in the 2005 Eurobarometer survey.
The main Slavic ethnic groups by religion:
Ethnocultural subdivisions.
Slavs are customarily divided along geographical lines into three major subgroups: East Slavs, West Slavs, and South Slavs, each with a different and a diverse background based on unique history, religion and culture of particular Slavic group within them. Apart from prehistorical archaeological cultures, the subgroups have had notable cultural contact with non-Slavic Bronze- and Iron Age civilisations.
South Slavs.
Western group.
 Extinct
 Also considered part of Rusyns
 Considered transitional between Ukrainians and Belarusians
 The ethnic affiliation of the Lemkos has become an ideological conflict. It has been alleged that among the Lemkos the idea of "Carpatho-Ruthenian" nation is supported only by Lemkos residing in Transcarpathia and abroad
 Most inhabitants of historic Moravia considered themselves as Czechs but significant amount declared their Moravian nationality, different from that Czech (although people from Bohemia and Moravia use the same official language).
 Also considered Poles.
There are sources that show Silesians as part of the Poles. Parts of the southmost population of Upper Silesia is sometimes considered Czech (controversial).
 A census category recognized as an ethnic group. Most Slavic Muslims (especially in Bosnia, Croatia, Montenegro and Serbia) now opt for Bosniak ethnicity, but some still use the "Muslim" designation. "Bosniak" and "Muslim" are considered two ethnonyms for a single ethnicity and the terms may even be used interchangeably. However, a small number of people within Bosnia and Herzegovina declare Bosniak but are not necessarily Muslim by faith.
 This identity continues to be used by a minority throughout the former Yugoslav republics. The nationality is also declared by diasporans living in the USA and Canada. There are a multitude of reasons as to why people prefer this affiliation, some published on the article.
 Sub-groups of Croats include Bunjevci (in Bačka), Šokci (in Slavonia and Vojvodina), Janjevci (in Kosovo), Burgenland Croats (in Austria), Bosniaks (in Hungary), Molise Croats (in Italy), Krashovans (in Romania), Moravian Croats (in the Czech Republic)
 Sub-groups of Slovenes include Prekmurians, Hungarian Slovenes, Carinthian Slovenes, Venetian Slovenes, Resians, and the extinct Carantanians and Somogy Slovenes.
Note: Besides ethnic groups, Slavs often identify themselves with the local geographical region in which they live. Some of the major regional South Slavic groups include: Zagorci in northern Croatia, Istrijani in westernmost Croatia, Dalmatinci in southern Croatia, Boduli in Adriatic islands, Vlaji in hinterland of Dalmatia, Slavonci in eastern Croatia, Bosanci in Bosnia, Hercegovci in Herzegovina, Krajišnici in western Bosnia, Semberci in northeast Bosnia, Srbijanci in Serbia proper, Šumadinci in central Serbia, Vojvođani in northern Serbia, Sremci in Syrmia, Bačvani in northwest Vojvodina, Banaćani in Banat, Sandžaklije (Muslims in Serbia/Montenegro border), Kosovci in Kosovo, Bokelji in southwest Montenegro, Trakiytsi in Upper Thracian Lowlands, Dobrudzhantsi in north-east Bulgarian region, Balkandzhii in Central Balkan Mountains, Miziytsi in north Bulgarian region, Warmiaks and Masurians in north-east Polish regions Warmia and Mazuria, Pirintsi in Blagoevgrad Province, Ruptsi in the Rhodopes etc.
Another interesting note is that the very term Slavic itself was registered in the US census of 2000 by more than 127,000 residents.
Genetics.
The modern Slavic peoples carry a variety of Mitochondrial DNA haplogroups and Y-chromosome DNA haplogroups. Yet two paternal haplogroups predominate: R1a1a [M17] and I2a2a [L69.2=T/S163.2]. The frequency of Haplogroup R1a ranges from 63.39% in the Sorbs, through 56.4% in Poland, 54% in Ukraine, 52% in Russia, Belarus, to 15.2% in Republic of Macedonia, 14.7% in Bulgaria and 12.1% in Herzegovina. The correlation between R1a1a [M17] and the speakers of Indo-European languages, particularly those of Eastern Europe (Russian) and Central and Southern Asia, was noticed in the late 1990s. From this Spencer Wells and colleagues, following the Kurgan hypothesis, deduced that R1a1a arose on the Pontic-Caspian steppe.
Specific studies of Slavic genetics followed. In 2007 Rębała and colleagues studied several Slavic populations with the aim of localizing the Proto-Slavic homeland. The significant findings of this study are that:
Marcin Woźniak and colleagues (2010) searched for specifically Slavic sub-group of R1a1a [M17]. Working with haplotypes, they found a pattern among Western Slavs which turned out to correspond to a newly discovered marker, M458, which defines subclade R1a1a7. This marker correlates remarkably well with the distribution of Slavic-speakers today. The team led by Peter Underhill, which discovered M458, did not consider the possibility that this was a Slavic marker, since they used the "evolutionary effective" mutation rate, which gave a date far too old to be Slavic. Woźniak and colleagues pointed out that the pedigree mutation rate, giving a later date, is more consistent with the archaeological record.
Pomors are distinguished by the presence of Y Haplogroup N among them. Postulated to originate from southeast Asia, it is found at high rates in Uralic peoples. Its presence in Pomors (called "Northern Russians" in the report) attests to the non-Slavic tribes (mixing with Finnic tribes of northern Eurasia). Autosomally, Russians are generally similar to populations in central-eastern Europe but some northern Russians are intermediate to Finno-Ugric groups.
On the other hand I2a1b1 (P41.2) is typical of the South Slavic populations, being highest in Bosnia-Herzegovina (>50%). Haplogroup I2a2 is also commonly found in north-eastern Italians. There is also a high concentration of I2a2a in the Moldavian region of Romania, Moldova and western Ukraine. According to original studies, Hg I2a2 was believed to have arisen in the west Balkans sometime after the LGM, subsequently spreading from the Balkans through Central Russian Plain. Recently, Ken Nordtvedt has split I2a2 into two clades – N (northern) and S (southern), in relation where they arose compared to Danube river. He proposes that N is slightly older than S. He recalculated the age of I2a2 to be ~ 2550 years and proposed that the current distribution is explained by a Slavic expansion from the area north-east of the Carpathians.
In 2008, biochemist Boris Abramovich Malyarchuk (Russian: Борис Абрамович Малярчук) et al. of the Institute of Biological Problems of the North, Russian Academy of Sciences, Magadan, Russia, used a sample (n=279) of Czech individuals to determine the frequency of "Mongoloid" "mtDNA lineages". Malyarchuk found Czech mtDNA lineages were typical of "Slavic populations" with "1.8%" Mongoloid mtDNA lineage. Malyarchuk added that "Slavic populations" "almost always" contain Mongoloid mtDNA lineage. Malyarchuk said the Mongoloid component of Slavic people was partially added before the split of "Balto-Slavics" in 2,000–3,000 BC with additional Mongoloid mixture occurring among Slavics in the last 4,000 years. Malyarchuk said the "Russian population" was developed by the "assimilation of the indigenous pre-Slavic population of Eastern Europe by true Slavs" with additional "assimilation of Finno-Ugric populations" and "long-lasting" interactions with the populations of "Siberia" and "Central Asia". Malyarchuk said that other Slavs "Mongoloid component" was increased during the waves of migration from "steppe populations (Huns, Avars, Bulgars and Mongols)", especially the decay of the "Avar Khaganate".
DNA samples from 1228 Russians show that the Y chromosomes analyzed, all except 20 (1.6%) fall into seven major haplogroups all characteristic to West Eurasian populations. Taken together, they account for 95% of the total Russian Y chromosomal pool. Only (0.7%) fell into haplogroups that are specific to East and South Asian populations.
Mitochondrial DNA (mtDNA) examined in Poles and Russians revealed the presence of all major European haplogroups, which were characterized by similar patterns of distribution in Poles and Russians. An analysis of the DNA did not reveal any specific combinations of unique mtDNA haplotypes and their subclusters. The DNA clearly shows that both Poles and Russians are not different from the neighbouring European populations.
Sources.
http://www.academia.edu/227792/The_Slavic_lingua_franca_Linguistic_notes_of_an_archaeologist_turned_historian_

</doc>
<doc id="29441" url="http://en.wikipedia.org/wiki?curid=29441" title="Skylab">
Skylab

Skylab was a space station launched and operated by NASA and was the United States' first space station. Skylab orbited the Earth from 1973 to 1979, and included a workshop, a solar observatory, and other systems. It was launched unmanned by a modified Saturn V rocket, with a weight of 169,950 lb. Three manned missions to the station, conducted between 1973 and 1974 using the Apollo Command/Service Module (CSM) atop the smaller Saturn IB, each delivered a three-astronaut crew. On the last two manned missions, an additional Apollo / Saturn IB stood by ready to rescue the crew in orbit if it was needed.
The station was damaged during launch when the micrometeoroid shield separated from the workshop and tore away, taking one of two main solar panel arrays with it and jamming the other one so that it could not deploy. This deprived Skylab of most of its electrical power, and also removed protection from intense solar heating, threatening to make it unusable. The first crew was able to save it in the first in-space major repair, by deploying a replacement heat shade and freeing the jammed solar panels.
Skylab included the Apollo Telescope Mount, which was a multi-spectral solar observatory, Multiple Docking Adapter (with two docking ports), Airlock Module with EVA hatches, and the Orbital Workshop, the main habitable volume. Electrical power came from solar arrays, as well as fuel cells in the docked Apollo CSM. The rear of the station included a large waste tank, propellant tanks for maneuvering jets, and a heat radiator.
Numerous scientific experiments were conducted aboard Skylab during its operational life, and crews were able to confirm the existence of coronal holes in the Sun. The Earth Resources Experiment Package (EREP) was used to view the Earth with sensors that recorded data in the visible, infrared, and microwave spectral regions. Thousands of photographs of Earth were taken, and records for human time spent in orbit were extended. Plans were made to refurbish and reuse Skylab, using the Space Shuttle to boost its orbit and repair it. However, development of the Shuttle was delayed, and Skylab reentered Earth's atmosphere and disintegrated in 1979, with debris striking portions of Western Australia. Post-Skylab NASA space laboratory projects included Spacelab, Shuttle-Mir, and Space Station "Freedom" (later merged into the International Space Station).
Background.
Rocket engineer Wernher von Braun, science fiction writer Arthur C. Clarke, and other early advocates of manned space travel, expected until the 1960s that a space station would be an important early step in space exploration. Von Braun participated in the publishing of a series of influential articles in "Collier's" magazine from 1952 to 1954, titled "Man Will Conquer Space Soon!". He envisioned a large, circular station 250 feet (75m) in diameter that would rotate to generate artificial gravity and require a fleet of 7,000-ton (6,500-metric ton) space shuttles for construction in orbit. The 80 men aboard the station would include astronomers operating a telescope, meteorologists to forecast the weather, and soldiers to conduct surveillance. Von Braun expected that future expeditions to the Moon and Mars would leave from the station.:2–5
The development of the transistor, the solar cell, and telemetry, led in the 1950s and early 1960s to unmanned satellites that could take photographs of weather patterns or enemy nuclear weapons and send them to Earth. A large station was no longer necessary for such purposes, and the United States Apollo program to send men to the Moon chose a mission mode that would not need in-orbit assembly. A smaller station that a single rocket could launch retained value, however, for scientific purposes.:55–60
Early studies.
In 1959, von Braun, head of the Development Operations Division at the Army Ballistic Missile Agency, submitted his final Project Horizon plans to the U.S. Army. The overall goal of Horizon was to place men on the Moon, a mission that would soon be taken over by the rapidly forming NASA. Although concentrating on the Moon missions, von Braun also detailed an orbiting laboratory built out of a Horizon upper stage,:23 an idea used for Skylab.:9 A number of NASA centers studied various space station designs in the early 1960s. Studies generally looked at platforms launched by the Saturn V, followed up by crews launched on Saturn IB using an Apollo Command/Service Module,:10 or a Gemini capsule:14 on a Titan II-C, the latter being much less expensive in the case where cargo was not needed. Proposals ranged from an Apollo-based station with two to three men, or a small "canister" for four men with Gemini capsules resupplying it, to a large, rotating station with 24 men and an operating lifetime of about five years.:13–14 A proposal to study the use of a Saturn S-IVB as a manned space laboratory was documented in 1962 by the Douglas Aircraft Company.
Air Force plans.
The Department of Defense (DoD) and NASA cooperated closely in many areas of space.:198–202 In September 1963, NASA and the DoD agreed to cooperate in building a space station.:17 The DoD wanted its own manned facility, however,:203 and in December it announced Manned Orbital Laboratory (MOL), a small space station primarily intended for photo reconnaissance using large telescopes directed by a two-man crew. The station was the same diameter as a Titan II upper stage, and would be launched with the crew riding atop in a modified Gemini capsule with a hatch cut into the heat shield on the bottom of the capsule.:17–19 MOL competed for funding with a NASA station for the next five years:15 and politicians and other officials often suggested that NASA participate in MOL or use the DoD design.:203 The military project led to changes to the NASA plans so that they would resemble MOL less.:17
Development.
Apollo Applications Program.
NASA management was concerned about losing the 400,000 workers involved in Apollo after landing on the moon in 1969.:20,22 A reason Von Braun, head of NASA's Marshall Space Flight Center during the 1960s, advocated for a smaller station after his large one was not built was that he wished to provide his employees with work beyond developing the Saturn rockets, which would be completed relatively early during Project Apollo.:61 NASA set up the "Apollo Logistic Support System Office", originally intended to study various ways to modify the Apollo hardware for scientific missions. The office initially proposed a number of projects for direct scientific study, including an extended-stay lunar mission which required two Saturn V launchers, a "lunar truck" based on the Lunar Module (LEM), a large manned solar telescope using a LEM as its crew quarters, and small space stations using a variety of LEM or CSM-based hardware. Although it did not look at the space station specifically, over the next two years the office would become increasingly dedicated to this role. In August 1965, the office was renamed, becoming the "Apollo Applications Program" (AAP).:20
As part of their general work, in August 1964 the Manned Spacecraft Center (MSC) presented studies on an expendable lab known as "Apollo "X"", short for "Apollo Extension System". "Apollo X" would have replaced the LEM carried on the top of the S-IVB stage with a small space station slightly larger than the CSM's service area, containing supplies and experiments for missions between 15 and 45 days' duration. Using this study as a baseline, a number of different mission profiles were looked at over the next six months.
Wet workshop.
In November 1964, von Braun proposed a more ambitious plan to build a much larger station built from the S-II second stage of a Saturn V. His design replaced the S-IVB third stage with an aeroshell, primarily as an adapter for the CSM on top. Inside the shell was a 10 ft cylindrical equipment section. On reaching orbit, the S-II second stage would be vented to remove any remaining hydrogen fuel, then the equipment section would be slid into it via a large inspection hatch. This became known as a "wet workshop" concept, because of the conversion of an active fuel tank. The station filled the entire interior of the S-II stage's hydrogen tank, with the equipment section forming a "spine" and living quarters located between it and the walls of the booster. This would have resulted in a very large 33 by living area. Power was to be provided by solar cells lining the outside of the S-II stage.:22
One problem with this proposal was that it required a dedicated Saturn V launch to fly the station. At the time the design was being proposed, it was not known how many of the then-contracted Saturn Vs would be required to achieve a successful Moon landing. However, several planned Earth-orbit test missions for the LEM and CSM had been canceled, leaving a number of Saturn IBs free for use. Further work led to the idea of building a smaller "wet workshop" based on the S-IVB, launched as the second stage of a Saturn IB.
A number of S-IVB-based stations were studied at MSC from mid-1965, which had much in common with the Skylab design that eventually flew. An airlock would be attached to the hydrogen tank, in the area designed to hold the LEM, and a minimum amount of equipment would be installed in the tank itself in order to avoid taking up too much fuel volume. Floors of the station would be made from an open metal framework that allowed the fuel to flow through it. After launch, a follow-up mission launched by a Saturn IB would launch additional equipment, including solar panels, an equipment section and docking adapter, and various experiments. Douglas Aircraft, builder of the S-IVB stage, was asked to prepare proposals along these lines. The company had for several years been proposing stations based on the S-IV stage, before it was replaced by the S-IVB.:25
On April 1, 1966, MSC sent out contracts to Douglas, Grumman, and McDonnell for the conversion of a S-IVB spent stage, under the name "Saturn S-IVB spent-stage experiment support module" (SSESM).:30 In May, astronauts voiced concerns over the purging of the stage's hydrogen tank in space. Nevertheless, in late July it was announced that the Orbital Workshop would be launched as a part of Apollo mission AS-209, originally one of the Earth-orbit CSM test launches, followed by two Saturn I/CSM crew launches, AAP-1 and AAP-2.
MOL remained AAP's chief competitor for funds, although the two programs cooperated on technology. NASA considered flying experiments on MOL, or using its Titan IIIC booster instead of the much more expensive Saturn IB. The agency decided that the Air Force station was not large enough, and that converting Apollo hardware for use with Titan would be too slow and too expensive.:45–48 The DoD later canceled MOL in June 1969.:109
Dry workshop.
Design work continued over the next two years, in an era of shrinking budgets. (NASA sought $450 million for Apollo Applications in fiscal year 1967, for example, but received $42 million.):64–65 In August 1967, the agency announced that the lunar mapping and base construction missions examined by the AAP were being canceled. Only the Earth-orbiting missions remained, namely the Orbital Workshop and Apollo Telescope Mount solar observatory.
The success of Apollo 8 in December 1968, launched on the third flight of a Saturn V, made it likely that one would be available to launch a dry workshop.:66 Later, several Moon missions were canceled as well, originally to be Apollo missions 18 through 20. The cancellation of these missions freed up three Saturn V boosters for the AAP program. Although this would have allowed them to develop von Braun's original S-II based mission, by this time so much work had been done on the S-IV based design that work continued on this baseline. With the extra power available, the wet workshop was no longer needed;:109–110 the S-IC and S-II lower stages could launch a "dry workshop", with its interior already prepared, directly into orbit.
Habitability.
A dry workshop simplified plans for the interior of the station.:130 Industrial design firm Raymond Loewy/William Snaith recommended emphasizing habitability and comfort for the astronauts by, for example, providing a wardroom for meals and relaxation,:133–134 and a window to view the Earth and space, although astronauts who participated in Skylab planning were dubious about the designers' focus on areas such as color schemes.:137 Habitability had not previously been an area of concern when building spacecraft, due to their small volume and brief mission durations, but the Skylab missions would last for months.:133 NASA sent a scientist on Jacques Piccard's "Ben Franklin" submarine in the Gulf Stream in July and August 1969, to learn how six people would live in an enclosed space for four weeks.:139–140
Astronauts were uninterested in watching movies on a proposed entertainment center or playing games, but did want books and individual music choices.:137 Food was also important; early Apollo crews complained about its quality, and a NASA volunteer found living on the Apollo food for four days on Earth to be intolerable; its taste and composition, in the form of cubes and squeeze tubes, were unpleasant. Skylab food significantly improved on its predecessors by prioritizing edibility over scientific needs.:141–142
Each astronaut had a private sleeping area the size of a small walk-in closet, with a curtain, sleeping bag, and locker.:82 Designers also added a shower:139:80 and a toilet;:152–158:30 the latter was both for comfort and to obtain precise urine and feces samples for examination on Earth.:165
Rescue.
Rescuing astronauts from Skylab was possible in the most likely emergency circumstances. The crew could use the CSM to quickly return to Earth if the station suffered serious damage. If the CSM failed, the spacecraft and Saturn IB for the next Skylab mission would have been launched with two astronauts to retrieve the crew; given Skylab's ample supplies, its residents would have been able to wait up to several weeks for the rescue mission.
Operational history.
Completion and launch.
On August 8, 1969, the McDonnell Douglas Corporation received a contract for the conversion of two existing S-IVB stages to the Orbital Workshop configuration. One of the S-IV test stages was shipped to McDonnell Douglas for the construction of a mock-up in January 1970. The Orbital Workshop was renamed "Skylab" in February 1970 as a result of a NASA contest.:115 The actual stage that flew was the upper stage of the AS-212 rocket (the S-IVB stage - S-IVB 212). The mission computer used aboard Skylab was the IBM System/4Pi TC-1, a relative of the AP-101 Space Shuttle computers. A Saturn V originally produced for the Apollo program – before the cancellation of Apollo 18, 19, and 20 – was repurposed and redesigned to launch Skylab. The Saturn V's upper stage removed, but with the avionics remaining in the same position.
Skylab was launched on May 14, 1973 by the modified Saturn V. The launch is sometimes referred to as Skylab 1, or SL-1. Severe damage was sustained during launch and deployment, including the loss of the station's micrometeoroid shield/sun shade and one of its main solar panels. Debris from the lost micrometeoroid shield further complicated matters by pinning the remaining solar panel to the side of the station, preventing its deployment and thus leaving the station with a huge power deficit.:253–255
Immediately following Skylab's launch, Pad A at Kennedy Space Center Launch Complex 39 was deactivated, and construction proceeded to modify it for the Space Shuttle program, originally targeting a maiden launch in March 1979. The manned missions to Skylab would occur from Launch Pad 39B.
Manned missions.
Three manned missions, designated SL-2, SL-3 and SL-4, were made to Skylab. The first manned mission, SL-2, launched on May 25, 1973 atop a Saturn IB and involved extensive repairs to the station. The crew deployed a parasol-like sunshade through a small instrument port from the inside of the station bringing station temperatures down to acceptable levels and preventing overheating that would have melted the plastic insulation inside the station and released poisonous gases. This solution was designed by NASA's "Mr. Fix It" Jack Kinzler, who won the NASA Distinguished Service Medal for his efforts. The crew conducted further repairs via two spacewalks (extra-vehicular activity, or EVA). The crew stayed in orbit with Skylab for 28 days. Two additional missions followed, with the launch dates of July 28, 1973 (SL-3) and November 16, 1973 (SL-4), and mission durations of 59 and 84 days, respectively. The last Skylab crew returned to the Earth on February 8, 1974.
Orbital operations.
Skylab orbited Earth 2,476 times during the 171 days and 13 hours of its occupation during the three manned Skylab missions. Astronauts performed ten spacewalks, totaling 42 hours and 16 minutes. Skylab logged about 2,000 hours of scientific and medical experiments, 127,000 frames of film of the Sun and 46,000 of the Earth.:340 Solar experiments included photographs of eight solar flares, and produced valuable results:155 that scientists stated would have been impossible to obtain with unmanned spacecraft.:342–344 The existence of the Sun's coronal holes were confirmed because of these efforts.:357 Many of the experiments conducted investigated the astronauts' adaptation to extended periods of microgravity.
A typical day began at 6 AM Central Time Zone.:307–308 Although the toilet was small and noisy, both veteran astronauts—who had endured earlier missions' rudimentary waste-collection systems—and rookies complimented it.:165,307:80 The first crew enjoyed taking a shower once a week, but found drying themselves in weightlessness and vacuuming excess water difficult; later crews usually cleaned themselves daily with wet washcloths instead of using the shower. Astronauts also found that bending over in weightlessness to put on socks or tie shoelaces strained their stomach muscles.:306–308
Breakfast began at 7 AM. Astronauts usually stood to eat, as sitting in microgravity also strained their stomach muscles. They reported that their food—although greatly improved from Apollo—was bland and repetitive, and weightlessness caused utensils, food containers, and bits of food to float away; also, gas in their drinking water contributed to flatulence. After breakfast and preparation for lunch, experiments, tests and repairs of spacecraft systems and, if possible, 90 minutes of physical exercise followed; the station had a bicycle and other equipment, and astronauts could jog around the water tank. After dinner, which was scheduled for 6 PM, crews performed household chores and prepared for the next day's experiments. Following lengthy daily instructions (some of which were up to 15 meters long) sent via teleprinter, the crews were often busy enough to postpone sleep.:309,334:2–7
Each Skylab mission set a record for the amount of time astronauts spent in space. The station offered what a later study called "a highly satisfactory living and working environment for crews", with enough room for personal privacy.:2–4 Although it had a dart set, playing cards, and other recreational equipment in addition to books and music players, the window with its view of Earth became the most popular way to relax in orbit.:79–80,134–135
Experiments.
Overview of most major experiments: Skylab 3 carried several more experiments, such as to observe Comet Kohoutek.
Post SL-4 plans.
Skylab was abandoned after the end of the SL-4 mission in February 1974, but to welcome visitors the crew left a bag filled with supplies and left the hatch unlocked. NASA discouraged any discussion of additional visits due to the station's age,:335,361 but in 1977 and 1978, when the agency still believed the Space Shuttle would be ready by 1979, it completed two studies on reusing the station.:3-1 By September 1978, the agency believed Skylab was safe for crews, with all major systems intact and operational.:3-2 It still had 180 man-days of water and 420 man-days of oxygen, and astronauts could refill both; the station could hold up to about 600 to 700 man-days of drinkable water and 420 man-days of food.:2–7
The studies cited several benefits from reusing Skylab, which one called a resource worth "hundreds of millions of dollars":1–13 with "unique habitability provisions for long duration space flight.":3–11 Since no more operational Saturn V rockets were available after the Apollo program, four to five shuttle flights and extensive space architecture would have been needed to build another station as large as Skylab's 12,400 cuft volume.:1-12 to 1-13 Its ample size—much greater than that of the shuttle alone, or even the shuttle plus Spacelab:2–8—was enough, with some modifications, for up to seven astronauts:2–31 of both sexes,:3–14 and experiments needing a long duration in space;:1–13 even a movie projector for recreation was possible.:3–11
Proponents of Skylab's reuse also said repairing and upgrading Skylab would provide information on the results of long-duration exposure to space for future stations. The most serious issue for reactivation was stationkeeping, as one of the station's gyroscopes had failed:361 and the attitude control system needed refueling; these issues would need EVA to fix or replace. The station had not been designed for extensive resupply. However, while plans had originally called for Skylab crews to perform only limited maintenance:34 they successfully made major repairs during EVA, such as the SL-2 crew's deploying of the solar panel:73–75 and the SL-4 crew's repair of the primary coolant loop.:317:130:3–21 The SL-2 crew fixed one item during EVA by, reportedly, "hit[ting] it with [a] hammer.":89
Some studies also said, beyond the opportunity for space construction and maintenance experience, reactivating the station would free up shuttle flights for other uses,:1–13 and reduce the need to modify the shuttle for long-duration missions.:2-9 to 2-10 Even if the station were not manned again, went one argument, it would serve as a useful experimental platform.:2–61
Shuttle mission plans.
The reactivation would likely have occurred in four phases:
The first three phases would have required about $60 million in 1980s dollars, not including launch costs.
After departure.
After a boost of 6.8 mi by SL-4's Apollo CSM before its departure in 1974, Skylab was left in a parking orbit of 269 mi by 283 mi:361 that was expected to last until at least the early 1980s, based on estimates of the 11-year sunspot cycle that began in 1976.:361 NASA began considering the potential risks of a space station reentry as early as 1962, but decided to not incorporate a retrorocket system in Skylab due to cost and acceptable risk.:127–129
The spent 49-ton Saturn V S-II stage which had launched Skylab in 1973 remained in orbit for almost two years, and made an uncontrolled reentry on January 11, 1975. Some debris, most prominently the five heavy J-2 engines, likely survived to impact in the North Atlantic Ocean. Although this event did not receive heavy media or public attention, it was followed closely by NASA and the Air Force, and helped emphasize the need for improved planning and public awareness for Skylab's eventual reentry.
Solar activity.
Greater-than-expected solar activity:362 heated the outer layers of the Earth's atmosphere and thereby increased drag on Skylab. By late 1977, NORAD accurately forecast a reentry in mid-1979; a National Oceanic and Atmospheric Administration (NOAA) scientist criticized NASA for using an inaccurate model for the second most-intense sunspot cycle in a century, and for ignoring NOAA predictions published in 1976.:362–363
The reentry of the USSR's nuclear powered Cosmos 954 in January 1978, and the resulting radioactive debris fall in northern Canada, drew more attention to Skylab's orbit. Although Skylab did not contain radioactive materials, the State Department warned NASA about the potential diplomatic repercussions of station debris.:363 Battelle Memorial Institute forecast that up to 25 tons of metal debris could land in 500 pieces over an area 4,000 miles long and 1,000 miles wide. The lead-lined film vault, for example, might land intact at 400 feet per second. Ground controllers re-established contact with Skylab in March 1978 and recharged its batteries. Although NASA worked on plans to reboost Skylab with the Space Shuttle through 1978 and the TRS was almost complete, the agency gave up in December when it became clear that the shuttle would not be ready in time;:363–367 its first flight, STS-1, did not occur until April 1981. Also rejected were proposals to launch the TRS using one or two unmanned rockets or to attempt to destroy the station with missiles.
Re-entry.
Skylab's demise was an international media event, with merchandising of T-shirts and hats with bullseyes, wagering on the time and place of re-entry, and nightly news reports. The "San Francisco Examiner" offered a $10,000 prize for the first piece of Skylab delivered to its offices; the competing "Chronicle" offered $200,000 if a subscriber suffered personal or property damage. NASA calculated that the odds of station re-entry debris hitting a human were 1 to 152 and the odds of any particular person being hit were 1 in 600 billion, although the odds of debris hitting a city of 100,000 or more were 1 to 7 and special teams were readied to head to any country hit by debris and requesting help.
We assume that Skylab is on the planet Earth, somewhere.
Charles S. Harlan, Skylab mission controller
In the hours before re-entry, ground controllers adjusted Skylab's orientation to try to minimize the risk of re-entry on a populated area. They aimed the station at a spot 810 mi south southeast of Cape Town, South Africa, and re-entry began at approximately 16:37 UTC, July 11, 1979.:371 The Air Force provided data from a secret tracking system able to monitor the reentry. The station did not burn up as fast as NASA expected, however. Due to a 4% calculation error, debris landed southeast of Perth, Western Australia,:371 and was found between Esperance and Rawlinna, from 31° to 34°S and 122° to 126°E, about 130–150 km radius around Balladonia. Residents and an airline pilot saw dozens of colorful fireworks-like flares as large pieces broke up in the atmosphere. The Shire of Esperance facetiously fined NASA A$400 for littering, a fine which remained unpaid for 30 years. The fine was paid in April 2009, when radio show host Scott Barley of Highway Radio raised the funds from his morning show listeners and paid the fine on behalf of NASA.
Seventeen-year-old Stan Thornton found 24 pieces of Skylab at his home in Esperance. A Philadelphia businessman flew him, his parents, and girlfriend to San Francisco, where he collected the "Examiner" prize.:371 In a coincidence for the organizers, the annual Miss Universe pageant was scheduled to be held a few days later, on July 20, 1979 in Perth. A large piece of Skylab debris was displayed on the stage. Analysis of the debris showed that the station had not disintegrated until 10 miles above the Earth, much lower than expected.
After the demise of Skylab, NASA focused on the reusable Spacelab module, an orbital workshop that could be deployed with the Space Shuttle and returned to Earth. The next American major space station project was Space Station "Freedom", which was merged into the International Space Station in 1993, and launched starting in 1998. Shuttle-"Mir" was another project, and led to the U.S. funding Spektr, Priroda, and the "Mir" Docking Module in the 1990s.
Planned, but unflown missions.
Skylab 5.
Skylab 5 would have been a short 20-day mission to conduct scientific experiments and boost Skylab into a higher orbit. Vance Brand (commander), William B. Lenoir (science pilot), and Don Lind (pilot) would have been the crew for this mission, with Brand and Lind being the prime crew for the Skylab Rescue flights. Brand and Lind also trained for a mission that would have aimed Skylab for a controlled deorbit.
Skylab B.
In addition to the flown Skylab space station, a second flight-quality backup Skylab space station had been built during the program. NASA considered using it for a second station in May 1973 or later, to be called Skylab B (S-IVB 515), but decided against it. Launching another Skylab with another Saturn V rocket would have been very costly, and it was decided to spend this money on the development of the Space Shuttle instead. The backup is on display at the National Air and Space Museum in Washington, D.C.
Engineering mock-ups.
A full-size training mock-up once used for astronaut training is located at the Lyndon B. Johnson Space Center visitor's center in Houston, Texas. Another full-size training mock-up is at the U.S. Space & Rocket Center in Huntsville, Alabama. Originally displayed indoors, it was subsequently stored outdoors for several years to make room for other exhibits. To mark the 40th anniversary of the Skylab program, the Orbital Workshop portion of the trainer was restored and moved into the Davidson Center in 2013. NASA transferred the backup Skylab to the National Air and Space Museum in 1975. On display in the Museum's Space Hall since 1976, the orbital workshop has been slightly modified to permit viewers to walk through the living quarters.
Mission designations.
The numerical identification of the manned Skylab missions was the cause of some confusion. Originally, the unmanned launch of Skylab and the three manned missions to the station were numbered "SL-1" through "SL-4". During the preparations for the manned missions, some documentation was created with a different scheme -- "SLM-1" through "SLM-3"—for those missions only. William Pogue credits Pete Conrad with asking the Skylab program director which scheme should be used for the mission patches, and the astronauts were told to use 1-2-3, not 2-3-4. By the time NASA administrators tried to reverse this decision, it was too late, as all the in-flight clothing had already been manufactured and shipped with the 1-2-3 mission patches.
Program cost.
From 1966 to 1974, the Skylab program cost a total of $2.2 billion or $10 billion in 2010 dollars with inflation. As its three three-man crews spent 510 total man-days in space, each man-day cost approximately $20 million in 2010 dollars, compared to $7.5 million for the International Space Station.
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="29442" url="http://en.wikipedia.org/wiki?curid=29442" title="Sacramento (disambiguation)">
Sacramento (disambiguation)

Sacramento is the capital of the U.S. state of California.
Sacramento may also refer to:

</doc>
<doc id="29445" url="http://en.wikipedia.org/wiki?curid=29445" title="StrongARM">
StrongARM

The StrongARM is a family of computer microprocessors developed by Digital Equipment Corporation and manufactured in the late 1990s which implemented the ARM v4 instruction set architecture. It was later sold to Intel in 1997, who continued to manufacture it before replacing it with the XScale in the early 2000s.
History.
The StrongARM was a collaborative project between DEC and Advanced RISC Machines to create a faster ARM microprocessor. The StrongARM was designed to address the upper-end of the low-power embedded market, where users needed more performance than the ARM could deliver while being able to accept more external support. Targets were devices such as newer personal digital assistants and set-top boxes.
Traditionally, the semiconductor division of DEC was located in Massachusetts. In order to gain access to the design talent in Silicon Valley, DEC opened a design center in Palo Alto, California. This design center was led by Dan Dobberpuhl and was the main design site for the StrongARM project. Another design site which worked on the project was in Austin, Texas that was created by some ex-DEC designers returning from Apple Computer and Motorola. The project was set up in 1995, and quickly delivered their first design, the SA-110.
DEC agreed to sell StrongARM to Intel as part of a lawsuit settlement in 1997. Intel used the StrongARM to replace their ailing line of RISC processors, the i860 and i960.
When the semiconductor division of DEC was sold to Intel, many engineers from the Palo Alto design group moved to SiByte, a start-up company designing MIPS system-on-a-chip (SoC) products for the networking market. The Austin design group spun off to become Alchemy Semiconductor, another start-up company designing MIPS SoCs for the hand-held market. A new StrongARM core was developed by Intel and introduced in 2000 as the XScale.
SA-110.
The SA-110 was the first microprocessor in the StrongARM family. The first versions, operating at 100, 160, and 200 MHz, were announced on 5 February 1996. When announced, samples of these versions were available, with volume production slated for mid-1996. Faster 166 and 233 MHz versions were announced on 12 September 1996. Samples of these versions were available at announcement, with volume production slated for December 1996. Throughout 1996, the SA-110 was the highest performing microprocessor for portable devices. Towards the end of 1996 it was a leading CPU for internet/intranet appliances and thin client systems. The SA-110's first design win was the Apple MessagePad 2000. It was also used in a number of products including the Acorn Computers Risc PC and Eidos Optima video editing system. The SA-110's lead designers were Daniel W. Dobberpuhl, Gregory W. Hoeppner, Liam Madden, and Richard T. Witek.
Description.
The SA-110 had a simple microarchitecture. It was a scalar design that executed instructions in-order with a five-stage classic RISC pipeline. The microprocessor was partitioned into several blocks, the IBOX, EBOX, IMMU, DMMU, BIU, WB and PLL. The IBOX contained hardware that operated in the first two stages of the pipeline such as the program counter. It fetched, decoded and issued instructions. Instruction fetch occurs during the first stage, decode and issue during the second. The IBOX decodes the more complex instructions in the ARM instruction set by translating them into sequences of simpler instructions. The IBOX also handled branch instructions. The SA-110 did not have branch prediction hardware, but had mechanisms for their speedy processing.
Execution starts at stage three. The hardware that operates during this stage is contained in the EBOX, which comprises the register file, arithmetic logic unit (ALU), barrel shifter, multiplier and condition code logic. The register file had three read ports and two write ports. The ALU and barrel shifter executed instructions in a single cycle. The multiplier is not pipelined and has a latency of multiple cycles.
The IMMU and DMMU are memory management units for instructions and data, respectively. Each MMU contained a 32-entry fully associative translation lookaside buffer (TLB) that can map 4 KB, 64 KB or 1 MB pages. The write buffer (WB) has eight 16-byte entries. It enables the pipelining of stores. The bus interface unit (BIU) provided the SA-110 with an external interface.
The PLL generates the internal clock signal from an external 3.68 MHz clock signal. It was not designed by DEC, but was contracted to the Centre Suisse d'Electronique et de Microtechnique (CSEM) located in Neuchâtel, Switzerland.
The instruction cache and data cache each have a capacity of 16 KB and are 32-way set-associative and virtually addressed. The SA-110 was designed to be used with slow (and therefore low-cost) memory and therefore the high set associativity allows a higher hit rate than competing designs, and the use of virtual addresses allows memory to be simultaneously cached and uncached. The caches are responsible for most of the transistor count and they take up half the die area.
The SA-110 contained 2.5 million transistors and is 7.8 mm by 6.4 mm large (49.92 mm2). It was fabricated by DEC in its proprietary CMOS-6 process at its Fab 6 fab in Hudson, Massachusetts. CMOS-6 was DEC's sixth-generation complementary metal–oxide–semiconductor (CMOS) process. CMOS-6 has a 0.35 µm feature size, a 0.25 µm effective channel length but for use with the SA-110, only three levels of aluminium interconnect. It used a power supply with a variable voltage of 1.2 to 2.2 volts (V) to enable designs to find a balance between power consumption and performance (higher voltages enable higher clock rates). The SA-110 was packaged in a 144-pin thin quad flat pack (TQFP).
SA-1100.
The SA-1100 was a derivative of the SA-110 developed by DEC. Announced in 1997, the SA-1100 was targeted for portable applications such as PDAs and differs from the SA-110 by providing a number of features that are desirable for such applications. To accommodate these features, the data cache was reduced in size to 8 KB. 
The extra features are integrated memory, PCMCIA, and color LCD controllers connected to an on-die system bus, and five serial I/O channels that are connected to a peripheral bus attached to the system bus. The memory controller supported FPM and EDO DRAM, SRAM, flash, and ROM. The PCMCIA controller supports two slots. The memory address and data bus is shared with the PCMCIA interface. Glue logic is required. The serial I/O channels implement a slave USB interface, a SDLC, two UARTs, an IrDA interface, a MCP, and a synchronous serial port.
The SA-1100 had a companion chip, the SA-1101. It was introduced by Intel on 7 October 1998. The SA-1101 provided additional peripherals to complement those integrated on the SA-1100 such as a video output port, two PS/2 ports, a USB controller and a PCMCIA controller that replaces that on the SA-1100. Design of the device started by DEC, but was only partially complete when acquired by Intel, who had to finish the design. It was fabricated at DEC's former Hudson, Massachusetts fabrication plant, which was also sold to Intel.
The SA-1100 contained 2.5 million transistors and measured 8.24 mm by 9.12 mm (75.15 mm2). It was fabricated in a 0.35 μm CMOS process with three levels of aluminium interconnect and was packaged in a 208-pin TQFP.
One of the early recipients of this processor was-ill fated Psion netBook and its more consumer oriented sibling Psion Series 7.
SA-1110.
The SA-1110 was a derivative of the SA-110 developed by Intel. It was announced on 31 March 1999, positioned as an alternative to the SA-1100. At announcement, samples were set for June 1999 and volume later that year. Intel discontinued the SA-1110 in early 2003. The SA-1110 was available in 133 or 206 MHz versions. It differed from the SA-1100 by featuring support for 66 MHz (133 MHz version only) or 103 MHz (206 MHz version only) SDRAM. Its companion chip, which provided additional support for peripherals, was the SA-1111. The SA-1110 was packaged in a 256-pin micro ball grid array. It was used in mobile phones, personal data assistants (PDAs) such as the Compaq (later HP) iPAQ and HP Jornada, the Sharp SL-5x00 Linux Based Platforms and the Simputer. It was also used to run the Intel Web Tablet, a tablet device that is considered potentially the first to introduce large screen, portable web browsing. Intel (who also owned ARM) dropped the product just prior to launch in 2001.
SA-1500.
The SA-1500 was a derivative of the SA-110 developed by DEC initially targeted for set-top boxes. It was designed and manufactured in low volumes by DEC but was never put into production by Intel. The SA-1500 was available at 200 to 300 MHz. The SA-1500 featured an enhanced SA-110 core, an on-chip coprocessor called the "Attached Media Processor" (AMP), and an on-chip SDRAM and I/O bus controller. The SDRAM controller supported 100 MHz SDRAM, and the I/O controller implemented a 32-bit I/O bus that may run at frequencies up to 50 MHz for connecting to peripherals and the SA-1501 companion chip.
The AMP implemented a long instruction word instruction set containing instructions designed for multimedia, such as integer and floating-point multiply–accumulate and SIMD arithmetic. Each long instruction word is 64 bits wide and specifies an arithmetic operation and a branch or a load/store. Instructions operate on operands from a 64-entry 36-bit register file, and on a set of control registers. The AMP communicates with the SA-110 core via an on-chip bus and it shares the data cache with the SA-110. The AMP contained an ALU with a shifter, a branch unit, a load/store unit, a multiply–accumulate unit, and a single-precision floating-point unit. The AMP supported user-defined instructions via a 512-entry writable control store.
The SA-1501 companion chip provided additional video and audio processing capabilities and various I/O functions such as PS/2 ports, a parallel port, and interfaces for various peripherals.
The SA-1500 contains 3.3 million transistors and measures 60 mm2. It was fabricated in a 0.28 µm CMOS process. It used a 1.5 to 2.0 V internal power supply and 3.3 V I/O, consuming less than 0.5 W at 100 MHz and 2.5 W at 300 MHz. It was packaged in a 240-pin metal quad flat package or a 256-ball plastic ball grid array.

</doc>
<doc id="29450" url="http://en.wikipedia.org/wiki?curid=29450" title="Shaul Mofaz">
Shaul Mofaz

Lt. General Shaul Mofaz (Hebrew: שאול מופז‎; Persian: شهرام مفضض‌کار‎, "Shahrām Mofazzazkār", born 4 November 1948) is an Israeli former soldier and politician. He joined the Israel Defense Forces in 1966 and served in the Paratroopers Brigade. He fought in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit. In 1998 he became the sixteenth IDF's Chief of the General Staff, serving until 2002.
After leaving the army, he entered politics. He was appointed Minister of Defense in 2002, holding the position until 2006 when he was elected to the Knesset on the Kadima list. He then served as Deputy Prime Minister and Minister of Transportation and Road Safety until 2009. After becoming Kadima leader in March 2012 he became Leader of the Opposition, before returning to the cabinet during a 70-day spell in which he served as Acting Prime Minister, Vice Prime Minister and Minister without Portfolio. Kadima was reduced to just two seats in the 2013 elections, and Mofaz retired from politics shortly before the 2015 elections.
Biography.
Early life and military career.
Born Shahrām Mofazzazkār in Tehran (although his parents came from Isfahan), Mofaz immigrated to Israel with his parents in 1957. Upon graduating from high school he joined the Israel Defense Forces in 1966 and served in the Paratroopers Brigade. He served in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit.
Mofaz was then appointed an infantry brigade commander for the 1982 Lebanon War. Afterwards he attended the US Marine Corps Command and Staff College in Quantico, Virginia, USA. On his return he was briefly appointed commander of the Officers School, before returning to active service as commander of the Paratroop Brigade in 1986.
Mofaz served in a series of senior military posts, having been promoted to the rank of Brigadier General (1988). In 1993 he was made commander of the IDF forces in the West Bank. In 1994, he was promoted to Major General, commanding the Southern Corps. His rapid rise continued; in 1997 Mofaz was appointed Deputy Chief of the General Staff and in 1998 he was appointed Chief of the General Staff.
His term of Chief of Staff was noted for financial and structural reforms of the Israeli Army. But the most significant event in his tenure was the eruption of the Second Intifada in September, 2000. The tough tactics undertaken by Mofaz drew widespread concern from the international community but were broadly supported by the Israeli public. Controversy erupted over the offensive in Jenin, intermittent raids in the Gaza Strip, and the continued isolation of Yasser Arafat.
Mofaz foresaw the wave of violence coming early as 1999 and prepared the IDF for intense guerrilla warfare in the territories. He fortified posts at the Gaza Strip and kept Israel Defense Forces casualties low. While he was known for claiming, "Israel has the most moral army in the world," he drew criticism from both Israeli and international human rights monitoring groups because of the methods he had undertaken, including using armored bulldozers to demolish 2,500 Palestinian civilian homes, displacing thousands, in order to create a security "buffer zone" along the Rafah border.
Political career.
Following a government crisis in 2002, Shaul Mofaz was appointed Defense Minister by Ariel Sharon. Although he supported an agreement with the Palestinians, he was willing to make no compromise in the war against militant groups such as Hamas, Islamic Jihad, Tanzim, and Al-Aqsa Martyrs Brigades.
The fact that he had only recently left his position as IDF Chief of Staff prevented him from participating in the 2003 election (by which time Mofaz had joined Sharon's Likud). Nevertheless, Sharon reappointed him as Defense Minister in the new government.
On 21 November 2005, Mofaz rejected Sharon's invitation to join his new party, Kadima, and instead announced his candidacy for the leadership of Likud. But, on 11 December 2005, one day after he promised he would never leave the Likud, he withdrew from both the leadership race and the Likud to join Kadima.
Following the elections in late March 2006, Mofaz was moved from the position of Defense Minister and received the Transport ministry in the new Cabinet installed on 4 May 2006.
In 2008, with Israel's then prime minister, Ehud Olmert, being pressured to resign due to corruption charges, Mofaz announced that he would run for the leadership of the Kadima party.
On 5 August 2008, Mofaz officially entered the race to be leader of Kadima. That same day he received a blessing by Shas spiritual leader Rabbi Ovadia Yosef. On 17 September 2008, he lost the Kadima party election, losing to Tzipi Livni for the spot of the Prime Minister and leader of Kadima. Livni's narrow margin of 431 votes was 43.1% to Shaul Mofaz's 42.0%, a huge difference from the 10 to 12-point exit polls margins. She said the "national responsibility (bestowed) by the public brings me to approach this job with great reverence". Mofaz accepted the Kadima primary's result, despite his lawyer, Yehuda Weinstein's appeal advice, and telephoned Livni congratulating her. Livni got 16,936 votes, with 16,505 votes, for Mofaz. Public Security Minister Avi Dichter and Interior Minister Meir Sheetrit had 6.5% and 8.5% respectively.
Placed second on the Kadima list, Mofaz retained his seat in the 2009 elections, but lost his cabinet position after Likud formed the government.
On 27 March 2012, Shaul Mofaz won the Kadima party leadership primaries by a landslide, defeating party chairwoman Tzipi Livni. Mofaz became Vice Prime Minister as part of a deal reached for a government of national unity with Binyamin Netanyahu. Mofaz said during the Kadima primaries that he would not join a government led by Netanyahu.
Mofaz left over Netanyahu's indecision over a draft reform law and warned that the prime minister was trying to patch together a majority for a vote to plunge the region into war.
In 2013 Kadima, just 4 years prior the ruling party, received 2% of the votes, barely passing to the Knesset.
In the buildup to the 2015 elections Kadima was not expect to pass the threshold, as it was raised to 3.25%. Mofaz negotiated with the Zionist Union alliance to bring Kadima onto their slate, but ended negotiations when it became clear he would not be their candidate for Defense Minister. Immediately after Mofaz announced he was not joining the Zionist Union slate, it was announced the former Military Intelligence Directorate (Israel) head Amos Yadlin was appointed to the Zionist Union slate and would be their candidate for Defense Minister. Within a week of his announcement that he was not running with the Zionist Union, Mofaz announced his retirement from politics.
In popular culture.
A fictionalized version of Mofaz appeared in the 2008 drama film "Lemon Tree".

</doc>
<doc id="29452" url="http://en.wikipedia.org/wiki?curid=29452" title="Stasi">
Stasi

The Ministry for State Security (German: "Ministerium für Staatssicherheit", MfS), commonly known as the Stasi (]) (abbreviation German: "Staatssicherheit", literally State Security), also State Security Service (German "Staatssicherheitsdienst", SSD), was the official state security service of the German Democratic Republic (GDR), colloquially known as East Germany. It has been described as one of the most effective and repressive intelligence and secret police agencies to have ever existed. The Stasi was headquartered in East Berlin, with an extensive complex in Berlin-Lichtenberg and several smaller facilities throughout the city. The Stasi motto was "Schild und Schwert der Partei" (Shield and Sword of the Party), referring to the ruling Socialist Unity Party of Germany (SED).
One of its main tasks was spying on the population, mainly through a vast network of citizens turned informants, and fighting any opposition by overt and covert measures, including hidden psychological destruction of dissidents ("Zersetzung", literally meaning decomposition). Its "Main Directorate for Reconnaissance" (German: "Hauptverwaltung Aufklärung") was responsible for both espionage and for conducting covert operations in foreign countries. Under its long-time head Markus Wolf, it gained a reputation as one of the most effective intelligence agencies of the Cold War. Numerous Stasi officials were prosecuted for their crimes after 1990. After German reunification, the surveillance files that the Stasi had maintained on millions of East Germans were laid open, so that any citizen could inspect their personal file on request; these files are now maintained by the Federal Commissioner for the Stasi Records.
Creation.
The Stasi was founded on 8 February 1950. Wilhelm Zaisser was the first Minister of State Security of the GDR, and Erich Mielke was his deputy. Zaisser tried to depose SED General Secretary Walter Ulbricht after the June 1953 uprising, but was instead removed by Ulbricht and replaced with Ernst Wollweber thereafter. Wollweber resigned in 1957 after clashes with Ulbricht and Erich Honecker, and was succeeded by his deputy, Erich Mielke.
In 1957, Markus Wolf became head of the Hauptverwaltung Aufklärung (HVA) (General Reconnaissance Administration), the foreign intelligence section of the Stasi. As intelligence chief, Wolf achieved great success in penetrating the government, political and business circles of West Germany with spies. The most influential case was that of Günter Guillaume, which led to the downfall of West German Chancellor Willy Brandt in May 1974. In 1986, Wolf retired and was succeeded by Werner Grossmann.
Relationship with the KGB.
Although Mielke's Stasi was superficially granted independence in 1957, until 1990 the KGB continued to maintain liaison officers in all eight main Stasi directorates, each with his own office inside the Stasi's Berlin compound, and in each of the fifteen Stasi district headquarters around East Germany. Collaboration was so close that the KGB invited the Stasi to establish operational bases in Moscow and Leningrad to monitor visiting East German tourists and Mielke referred to the Stasi officers as "Chekists of the Soviet Union". In 1978, Mielke formally granted KGB officers in East Germany the same rights and powers that they enjoyed in the Soviet Union.
Organization.
The Ministry for State Security also included the following entities:
Operations.
Personnel and recruitment.
Between 1950 and 1989, the Stasi employed a total of 274,000 people in an effort to root out the class enemy. In 1989, the Stasi employed 91,015 people full-time, including 2,000 fully employed unofficial collaborators, 13,073 soldiers and 2,232 officers of GDR army, along with 173,081 unofficial informants inside GDR and 1,553 informants in West Germany.
Regular commissioned Stasi officers were recruited from conscripts who had been honourably discharged from their 28 months' compulsory military service, had been members of the SED, had had a high level of participation in the Party's youth wing's activities and had been Stasi informers during their service in the Military. The candidates would then have to be recommended by their military unit political officers and Stasi agents, the local chiefs of the District (Bezirke) Stasi and Volkspolizei office, of the district in which they were permanently resident, and the District Secretary of the SED. These candidates were then made to sit through several tests and exams, which identified their intellectual capacity to be an officer, and their political reliability. University graduates who had completed their military service did not need to take these tests and exams. They then attended a two-year officer training programme at the Stasi college ("Hochschule") in Potsdam. Less mentally and academically-endowed candidates were made ordinary technicians and attended a one-year technology-intensive course for non-commissioned officers.
By 1995, some 174,000 "inoffizielle Mitarbeiter"(IMs) Stasi informants had been identified, almost 2.5% of East Germany's population between the ages of 18 and 60. 10,000 IMs were under 18 years of age. From the volume of material destroyed in the final days of the regime, the office of the Federal Commissioner for the Stasi Records (BStU) believes that there could have been as many as 500,000 informers. A former Stasi colonel who served in the counterintelligence directorate estimated that the figure could be as high as 2 million if occasional informants were included.
Infiltration.
Full-time officers were posted to all major industrial plants (the extensiveness of any surveillance largely depended on how valuable a product was to the economy) and one tenant in every apartment building was designated as a watchdog reporting to an area representative of the Volkspolizei (Vopo). Spies reported every relative or friend who stayed the night at another's apartment. Tiny holes were drilled in apartment and hotel room walls through which Stasi agents filmed citizens with special video cameras. Schools, universities, and hospitals were extensively infiltrated.
The Stasi had formal categorizations of each type of informant, and had official guidelines on how to extract information from, and control, those who they came into contact with. The roles of informants ranged from those already in some way involved in state security (such as the police and the armed services) to those in the dissident movements (such as in the arts and the Protestant Church). Information gathered about the latter groups was frequently used to divide or discredit members. Informants were made to feel important, given material or social incentives, and were imbued with a sense of adventure, and only around 7.7%, according to official figures, were coerced into cooperating. A significant proportion of those informing were members of the SED; to employ some form of blackmail, however, was not uncommon. A large number of Stasi informants were trolley conductors, janitors, doctors, nurses and teachers; Mielke believed that the best informants were those whose jobs entailed frequent contact with the public.
The Stasi's ranks swelled considerably after Eastern Bloc countries signed the 1975 Helsinki accords, which GDR leader Erich Honecker viewed as a grave threat to his regime because they contained language binding signatories to respect "human and basic rights, including freedom of thought, conscience, religion, and conviction." The number of IMs peaked at around 180,000 in that year, having slowly risen from 20,000–30,000 in the early 1950s, and reaching 100,000 for the first time in 1968, in response to "Ostpolitik" and protests worldwide. The Stasi also acted as a proxy for KGB to conduct activities in other Eastern Bloc countries, such as Poland, where the Soviets were despised.
The Stasi infiltrated almost every aspect of GDR life. In the mid-1980s, a network of IMs began growing in both German states; by the time that East Germany collapsed in 1989, the Stasi employed 91,015 employees and 173,081 informants. About one out of every 63 East Germans collaborated with the Stasi. By at least one estimate, the Stasi maintained greater surveillance over its own people than any secret police force in history. The Stasi employed one full-time agent for every 166 East Germans. The ratios swelled when informers were factored in: counting part-time informers, the Stasi had one informer per 6.5 people. By comparison, the Gestapo employed one secret policeman per 2,000 people. This comparison led Nazi hunter Simon Wiesenthal to call the Stasi even more oppressive than the Gestapo. Additionally, Stasi agents infiltrated and undermined West Germany's government and spy agencies.
In some cases, spouses even spied on each other. A high-profile example of this was peace activist Vera Lengsfeld, whose husband, Knud Wollenberger, was a Stasi informant.
Zersetzung.
The Stasi perfected the technique of psychological harassment of perceived enemies known as "Zersetzung" (]) – a term borrowed from chemistry which literally means "decomposition".
By the 1970s, the Stasi had decided that the methods of overt persecution that had been employed up to that time, such as arrest and torture, were too crude and obvious. It was realised that psychological harassment was far less likely to be recognised for what it was, so its victims, and their supporters, were less likely to be provoked into active resistance, given that they would often not be aware of the source of their problems, or even its exact nature. "Zersetzung" was designed to side-track and "switch off" perceived enemies so that they would lose the will to continue any "inappropriate" activities.
Tactics employed under "Zersetzung" generally involved the disruption of the victim's private or family life. This often included psychological attacks, such as breaking into homes and subtly manipulating the contents, in a form of gaslighting – moving furniture, altering the timing of an alarm, removing pictures from walls or replacing one variety of tea with another. Other practices included property damage, sabotage of cars, purposely incorrect medical treatment, smear campaigns including sending falsified compromising photos or documents to the victim's family, denunciation, provocation, psychological warfare, psychological subversion, wiretapping, bugging, mysterious phone calls or unnecessary deliveries, even including sending a vibrator to a target's wife. Usually, victims had no idea that the Stasi were responsible. Many thought that they were losing their minds, and mental breakdowns and suicide could result.
One great advantage of the harassment perpetrated under "Zersetzung" was that its subtle nature meant that it was able to be plausibly denied. This was important given that the GDR was trying to improve its international standing during the 1970s and 80s, especially in conjunction with the "Ostpolitik" of West-German chancellor Willy Brandt massively improving relations between the two German states.
"Zersetzung" techniques were used by other East Bloc intelligence and security agencies. This includes extensive use by the hierarchically superior agency in the USSR intelligence framework, the Russian KGB. Techniques that would be described as "Zersetzung" techniques, would otherwise be described and fall under those of "active measures" as termed by the KGB. Techniques that would be classified as active measures or "Zersetzung" continue to be employed by selected security and intelligence organizations worldwide to this date. The Russian FSB, which has the present organizational responsibilities and congruent authorizations to the internal security, CI, investigatory directorates of the former KGB has been widely implicated in continued use of active measures techniques in numerous operations.
International operations.
Other files (the Rosenholz Files), which contained the names of East German spies abroad, led American spy agencies to capture them. After German reunification, revelations of Stasi's international activities were publicized, such as its military training to the West German Red Army Faction.
Directorate X was responsible for disinformation. Rolf Wagenbreth, director of disinformation operations, stated "Our friends in Moscow call it 'dezinformatsiya'. Our enemies in America call it 'active measures', and I, dear friends, call it ‘my favorite pastime'".
Fall of the Soviet Union.
Recruitment of informants became increasingly difficult towards the end of the GDR's existence, and, after 1986, there was a negative turnover rate of IMs. This had a significant impact on the Stasi's ability to survey the population, in a period of growing unrest, and knowledge of the Stasi's activities became more widespread. The Stasi had been tasked during this period with preventing the country's economic difficulties becoming a political problem, through suppression of the very worst problems the state faced, but it failed to do so.
Stasi officers reportedly had discussed re-branding East Germany as a democratic capitalist country to the West, but which would be in practice taken over by Stasi officers. The plan specified 2,587 OibE officers who would take over power ("Offiziere im besonderen Einsatz", "officers on special assignment") and it was registered as Top Secret Document 0008-6/86 of 17 March 1986. According to Ion Mihai Pacepa, the chief intelligence officer in communist Romania, other communist intelligence services had similar plans. On 12 March 1990, "Der Spiegel" reported that the Stasi was indeed attempting to implement 0008-6/86. Pacepa has noted that what happened in Russia and how KGB Colonel Vladimir Putin took over Russia resembles these plans. See Putinism.
On 7 November 1989, in response to the rapidly changing political and social situation in the GDR in late 1989, Erich Mielke resigned. On 17 November 1989, the Council of Ministers "(Ministerrat der DDR)" renamed the Stasi as the "Office for National Security" "(Amt für Nationale Sicherheit" – AfNS), which was headed by "Generalleutnant" Wolfgang Schwanitz. On 8 December 1989, GDR Prime Minister Hans Modrow directed the dissolution of the AfNS, which was confirmed by a decision of the "Ministerrat" on 14 December 1989.
As part of this decision, the "Ministerrat" originally called for the evolution of the AfNS into two separate organizations: a new foreign intelligence service "(Nachrichtendienst der DDR)" and an "Office for the Protection of the Constitution of the GDR" "(Verfassungsschutz der DDR)", along the lines of the West German "Bundesamt für Verfassungsschutz", however, the public reaction was extremely negative, and under pressure from the "Round Table" "(Runder Tisch)", the government dropped the creation of the "Verfassungsschutz der DDR" and directed the immediate dissolution of the AfNS on 13 January 1990. Certain functions of the AfNS reasonably related to law enforcement were handed over to the GDR Ministry of Internal Affairs. The same ministry also took guardianship of remaining AfNS facilities.
When the parliament of Germany investigated public funds that disappeared after the Fall of the Berlin Wall, it found out that East Germany had transferred large amounts of money to Martin Schlaff through accounts in Vaduz, the capital of Liechtenstein, in return for goods "under Western embargo".
Moreover, high-ranking Stasi officers continued their post-DDR careers in management positions in Schlaff's group of companies. For example, in 1990, Herbert Kohler, Stasi commander in Dresden, transferred 170 million marks to Schlaff for "harddisks" and months later went to work for him.
The investigations concluded that "Schlaff's empire of companies played a crucial role" in the Stasi attempts to secure the financial future of Stasi agents and keep the intelligence network alive.
The "Stern" magazine noted that KGB officer Vladimir Putin worked with his Stasi colleagues in Dresden in 1989.
In the Soviet Union, about 50 billion U.S. dollars were transferred out of the country (see FIMACO).
Recovery of the Stasi files.
During the Peaceful Revolution of 1989, Stasi offices were overrun by enraged citizens, but not before the Stasi destroyed a number of documents (approximately 5%) consisting of, by one calculation, 1 billion sheets of paper.
Storming the Stasi headquarters.
With the fall of the German Democratic Republic the Stasi was dissolved. Stasi employees began to destroy the extensive files and documents they held, by hand, fire and with the use of shredders. When these activities became known, a protest began in front of the Stasi headquarters, The evening of 15 January 1990 saw a large crowd form outside the gates calling for a stop to the destruction of sensitive files. The building contained vast records of personal files, many of which would form important evidence in convicting those who had committed atrocities for the Stasi. The protesters continued to grow in number until they were able to overcome the police and gain entry into the complex. Once inside, specific targets of the protesters anger were portraits of Erich Honecker which were trampled on or burnt. Among the protesters were former Stasi collaborators seeking to destroy incriminating documents.
Controversy of the Stasi files.
With the German Reunification on 3 October 1990, a new government agency was founded called the "Federal Commissioner for the Records of the State Security Service of the former German Democratic Republic" (German: "Der Bundesbeauftragte für die Unterlagen des Staatssicherheitsdienstes der ehemaligen Deutschen Demokratischen Republik", officially abbreviated "BStU". There was a debate about what should happen to the files, whether they should be opened to the people or kept closed.
Those who opposed opening the files cited privacy as a reason. They felt that the information in the files would lead to negative feelings about former Stasi members, and, in turn, cause violence. Pastor Rainer Eppelmann, who became Minister of Defense and Disarmament after March 1990, felt that new political freedoms for former Stasi members would be jeopardized by acts of revenge. Prime Minister Lothar de Maizière even went so far as to predict murder. They also argued against the use of the files to capture former Stasi members and prosecute them, arguing that not all former members were criminals and should not be punished solely for being a member. There were also some who believed that everyone was guilty of something. Peter Michael Diestel, the Minister of Interior, opined that these files could not be used to determine innocence and guilt, claiming that "there were only two types of individuals who were truly innocent in this system, the newborn and the alcoholic". Other opinions, such as the one of West German Interior Minister Wolfgang Schäuble, believed in putting the Stasi behind them and working on German reunification.
Others argued that everyone should have the right to see their own file, and that the files should be opened to investigate former Stasi members and prosecute them, as well as not allow them to hold office. Opening the files would also help clear up some of the rumors that were floating around. Some also believed that politicians involved with the Stasi should be investigated.
The fate of the files was finally decided under the Unification Treaty between the GDR and Federal Republic of Germany (FRG). This treaty took the Volkskammer law further and allowed more access and use of the files. Along with the decision to keep the files in a central location in the East, they also decided who could see and use the files, allowing people to see their own files.
In 1992, following a declassification ruling by the German government, the Stasi files were opened, leading people to look for their files. Timothy Garton Ash, an English historian, after reading his file, wrote "The File: A Personal History".
Between 1991 and 2011, around 2.75 million individuals, mostly GDR citizens, requested to see their own files. The ruling also gave people the ability to make duplicates of their documents. Another big issue was how the media could use and benefit from the documents. It was decided that the media could obtain files as long as they were depersonalized and not regarding an individual under the age of 18 or a former Stasi member. This ruling not only gave the media access to the files, but also gave schools access.
Tracking down former Stasi informers with the files.
Even though groups of this sort were active in the community, those who were tracking down ex-members were, as well. Many of these hunters succeeded in catching ex-Stasi; however, charges could not be made for merely being a member. The person in question would have had to participate in an illegal act, not just be a registered Stasi member. Among the high-profile individuals who were arrested and tried were Erich Mielke, Third Minister of State Security of the GDR, and Erich Honecker, head of state for the GDR. Mielke was sentenced to six years prison for the murder of two policemen in 1931. Honecker was charged with authorizing the killing of would-be escapees on the East-West frontier and the Berlin Wall. During his trial, he went through cancer treatment. Because he was nearing death, Honecker was allowed to spend his final time in freedom. He died in Chile in May 1994.
Reassembling the destroyed files.
Some of it is very easy due to the number of archives and the failure of shredding machines (in some cases "shredding" meant tearing paper in two by hand and documents could be recovered easily). In 1995, the BStU began reassembling the shredded documents; 13 years later, the three dozen archivists commissioned to the projects had only reassembled 327 bags; they are now using computer-assisted data recovery to reassemble the remaining 16,000 bags – estimated at 45 million pages. It is estimated that this task may be completed at a cost of 30 million dollars.
The CIA acquired some Stasi records during the looting of the Stasi's archives. The Federal Republic of Germany has asked for their return and received some in April 2000. See also Rosenholz files.
Museum in the old headquarters.
The Anti-Stalinist Action Normannenstraße (ASTAK), an association founded by former GDR Citizens' Committees, has transformed the former headquarters of the Stasi into a museum. It is divided into three floors:
The ground floor has been kept as it used to be. The decor is original, with many statues and flags.
Photo gallery:
Stasi officers after the reunification.
Recruitment by Russian state-owned companies.
Former Stasi agent Matthias Warnig (codename "Arthur") is currently the CEO of Nord Stream.
German investigations have revealed that some of the key Gazprom Germania managers are former Stasi agents.
Lobbying.
Former Stasi officers continue to be politically active via the "Gesellschaft zur Rechtlichen und Humanitären Unterstützung e. V." (Society for Legal and Humanitarian Support) (GRH). Former high-ranking officers and employees of the Stasi, including the last Stasi director, Wolfgang Schwanitz, make up the majority of the organization's members, and it receives support from the German Communist Party, among others.
Impetus for the establishment of the GRH was provided by the criminal charges filed against the Stasi in the early 1990s. The GRH, decrying the charges as "victor's justice", called for them to be dropped. Today the group provides an alternative if somewhat utopian voice in the public debate on the GDR legacy. It calls for the closure of the museum in Hohenschönhausen and can be a vocal presence at memorial services and public events. In March 2006 in Berlin, GRH members disrupted a museum event; a political scandal ensued when the Berlin Senator (Minister) of Culture refused to confront them.
Behind the scenes, the GRH also lobbies people and institutions promoting opposing viewpoints. For example, in March 2006, the Berlin Senator for Education received a letter from a GRH member and former Stasi officer attacking the Museum for promoting "falsehoods, anticommunist agitation and psychological terror against minors". Similar letters have also been received by schools organizing field trips to the museum.

</doc>
<doc id="29455" url="http://en.wikipedia.org/wiki?curid=29455" title="Sandra Bullock">
Sandra Bullock

Sandra Annette Bullock (; born July 26, 1964) is an American actress and film producer. She is one of Hollywood's highest-paid actresses, and is the recipient of one Academy Award from two nominations, and one Golden Globe Award from five nominations. She was named the "Most Beautiful Woman" by "People" magazine in 2015.
Bullock made her acting debut with a minor role in the 1987 thriller "Hangmen". Her breakthrough role was in "Demolition Man" (1993), after which she starred in several successful films including "Speed" (1994), "While You Were Sleeping" (1995), "Hope Floats" (1998), and "A Time to Kill" (1996). Bullock achieved further success in the 2000s and 2010s with starring roles in "Miss Congeniality" (2000), "Two Weeks Notice" (2002), "Crash" (2004), "The Proposal" (2009), and "The Heat" (2013). She was awarded the Academy Award for Best Actress and the Golden Globe Award for Best Actress in a Drama for playing Leigh Anne Tuohy in "The Blind Side" (2009). Bullock's greatest commercial success came with the science fiction film "Gravity" (2013) for which she received her second Oscar nomination.
In addition to acting, Bullock is the founder of the production company Fortis Films. She was married to Jesse G. James from 2005 to 2010.
Early life.
Bullock was born in Arlington, Virginia, a suburb of Washington, D.C. Her father, John W. Bullock (born 1925), was a United States Army employee and part-time voice coach; her mother, Helga Mathilde Meyer (1942–2000), was an opera singer and voice teacher. Bullock's father is from Birmingham, Alabama, and has English, Irish, German, and French ancestry, while Bullock's mother was German. Bullock's maternal grandfather was a rocket scientist from Nuremberg, Germany. Bullock's father, who was in charge of the Army's Military Postal Service in Europe, was stationed in Nuremberg when he met his wife. They married in Germany and moved to Arlington, where John worked with the Army Materiel Command, before becoming a contractor for The Pentagon. She has a younger sister, Gesine Bullock-Prado, who was formerly the vice-president of Bullock's production company Fortis Films.
Bullock was raised in Nuremberg, Germany for twelve years and grew up speaking German. She attended the humanistic Waldorf School. As a child, Bullock frequently accompanied her mother on European opera tours. Bullock studied ballet and vocal arts as a child, taking small parts in her mother's opera productions. She sang in the opera's children's choir at the Staatstheater Nürnberg. The scar above her left eye was caused when she fell into a creek as a child. Bullock attended Washington-Lee High School, where she was a cheerleader and performed in high school theater productions. After graduating in 1982, she attended East Carolina University in Greenville, North Carolina, where she received a degree in drama in 1986. While at ECU, she performed in multiple theater productions, including "Peter Pan" and "Three Sisters." She then moved to Manhattan and supported herself as a bartender, cocktail waitress, and coat checker while auditioning for roles.
Until the age of eighteen, Bullock held American/German dual citizenship. In 2009, she reapplied for German citizenship.
Career.
1987–99: Career breakthrough worldwide.
While in New York, Bullock took acting classes with Sanford Meisner. She appeared in several student films, and later landed a role in an Off-Broadway play "No Time Flat". Director Alan J. Levi was impressed by Bullock's performance and offered her a part in the TV movie "Bionic Showdown: The Six Million Dollar Man and the Bionic Woman" (1989). This led to her being cast in a series of small roles in several independent films as well as in the lead role of the short-lived NBC television version of the film "Working Girl" (1990). She went on to appear in several films, such as "Love Potion No. 9" (1992), "The Thing Called Love" (1993) and "Fire on the Amazon" (1993).
A prominent supporting role in the science-fiction/action film "Demolition Man" (1993) was followed by a leading role in "Speed" the following year. "Speed" was a huge hit that took in $350 million at the box office worldwide, making it her second most successful picture to date.
A string of successes during the mid-1990s included "While You Were Sleeping" (1995), for which she received her first Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy, "The Net" (1995) and "A Time to Kill" (1996). Bullock received $11 million for "" (1997), which she agreed to star in for financial backing for her own project, "Hope Floats" (1998), and has revealed she regrets making the sequel.
She was selected as one of "People" magazine's 50 Most Beautiful People in the World in 1996 and 1999, and was also ranked #58 in "Empire" magazine's Top 100 Movie Stars of All Time list.
2000–08: Financial success and producing.
In 2000, Bullock starred in "Miss Congeniality", a financial success that took in $212 million at the box office worldwide, and received another Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy. She was presented with the 2002 Raúl Juliá Award for Excellence for her efforts, as the executive producer of the sitcom "George Lopez", in helping expand career openings for Hispanic talent in the media and entertainment industry. She also made several appearances on the show as Accident Amy, an accident-prone employee at the factory Lopez's character manages. The same year, she starred opposite Hugh Grant in "Two Weeks Notice" (2002).
In 2004, Bullock had a supporting role in the film "Crash", which won the Academy Award for Best Picture. She received positive reviews for her performance, with some critics suggesting that it was the best performance of her career. She later received a $17.5-million-salary for "" (2005). The same year, she was a co-recipient of the Women in Film Crystal Award.
Although Bullock was reunited with her "Speed" co-star Keanu Reeves in the romantic drama "The Lake House", their film characters are separated throughout the film, so Bullock and Reeves were only on set together for two weeks during filming. The same year, Bullock appeared in "Infamous", playing author Harper Lee. Bullock also starred in "Premonition" with Julian McMahon, which was released in March 2007. In 2008, Bullock was announced as "the face" of the cosmetic brand Artistry.
2009–12: Critical acclaim and recognition.
The year 2009 proved to be especially good for Bullock, giving the actress two record highs in her career, as earlier in the year she released "The Proposal", with co-star Ryan Reynolds, a huge hit that took in $317 million at the box office worldwide, making it her third most successful picture to date. She received her third Golden Globe Award nomination for Best Actress role for Motion Picture Musical or Comedy.
In November 2009, Bullock starred in "The Blind Side", which opened at #2 behind "New Moon" with $34.2 million, making it her second highest opening weekend ever. "The Blind Side" is unique in that it had a 17.6% increase at the box office its second weekend, and it took the top spot of the box office in its third weekend. The film cost $29 million to make according to the Box Office Mojo. It grossed over $309 million, making it her domestic highest grossing film, her fourth highest grossing film worldwide, and the first one in history to pass the $200 million mark with only one top-billed female star.
Bullock had initially turned down the role of Leigh Anne Tuohy three times due to a discomfort with portraying a devout Christian. She was awarded the Academy Award for Best Actress, Golden Globe Award for Best Actress – Motion Picture Drama, Screen Actors Guild Award for Outstanding Performance by a Female Actor in a Leading Role and Broadcast Film Critics Association Award for Best Actress. "The Blind Side" also received an Academy Award for Best Picture nomination.
Winning the Oscar also gave Bullock another unique distinction—since she won two "Razzies" the day before, for her performance in "All About Steve" (2009), she is the only performer ever to have been named both "Best" and "Worst" for the same year.
In 2011, Bullock starred in the drama "Extremely Loud and Incredibly Close" alongside Tom Hanks, a film adaptation based on the novel of the same name. Despite mixed reviews, the film was nominated for numerous awards, including an Academy Award for Best Picture nomination. Bullock was nominated for Best Actress Drama by Teen Choice Awards.
By 2015, Bullock's films had grossed over $3.6 billion worldwide, which makes her 29th most profitable movie star. According to "The Numbers", her total domestic gross stands at $1.9 billion, placing her among the Top 100 Stars at the Box Office.
2013–present: Continued success.
In 2013, Bullock starred in the comedy film "The Heat", alongside Melissa McCarthy. It received positive reviews from critics, and took in $230 million at the box office worldwide. Bullock also starred in the science fiction film "Gravity", opposite George Clooney. The film premiered at the 70th Venice Film Festival, and was released on October 4, 2013 to coincide with the beginning of World Space Week. "Gravity" received universal acclaim among critics and a standing ovation in Venice. The film was called "the most realistic and beautifully choreographed film ever set in space". Bullock's performance was praised, with some critics saying that "Gravity" is the best work of her career. "Variety" wrote, "Bullock inhabits the role with grave dignity and hints at Stone's past scars with sensitivity and tact, and she holds the screen effortlessly once Gravity becomes a veritable one-woman show... the actress remains fully present emotionally, projecting a very appealing combo of vulnerability, intelligence and determination that not only wins us over immediately, but sustains attention all the way through the cathartic closing reels."
 "Gravity" took in $716 million at the box office worldwide, making it Bullock's most successful picture to date. For her role as Dr. Ryan Stone, Bullock was nominated for the Academy Award, Golden Globe Award, BAFTA Award, Screen Actors Guild Award and Broadcast Film Critics Association Award for Best Actress. By August 2014, Bullock was the highest earning actress in the movie business.
Public image.
Since her debut as an actress, Bullock has been dubbed in the media as "America’s sweetheart," due to her nature which has been described as "friendly and direct and so unpretentious."
On March 24, 2005, Bullock received a star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard in Hollywood.
While critics have praised her screen persona, some have been less receptive towards her films. At the 2009 release of "The Proposal", Mark Kermode said she has made only three "good" films in her career—"Speed", "While You Were Sleeping", and "Crash", and added that "she's funny, she's gorgeous, it's impossible not to love her and yet she makes rotten film after rotten film after rotten film." As of December 2009, Bullock has appeared on three "Entertainment Weekly" covers.
She was selected by "People" magazine as its "Woman of the Year" for 2010 
and ranked #12 on "People"‍ '​s Most Beautiful 2011 list.
In 2010, "Time" magazine included Bullock in its annual "TIME" 100 as one of the "Most Influential People in the World."
In 2013, "The Hollywood Reporter" named Bullock among the most powerful women in entertainment. Bullock joined Hollywood legends at the TCL Chinese Theatre on Hollywood Boulevard by attaching her hand and footprints in cement in the forecourt of the theater in September 2013.
In November 2013 it was announced that Bullock was named "Entertainment Weekly"‍ '​s "Entertainer of the Year", due to her success with "The Heat" and "Gravity", which "Entertainment Weekly" believed would earn her an Oscar nomination. Bullock shared the "Entertainer of the Year" title with other distinguished people in the industry such as the "masterminds" behind the television show "Breaking Bad", Matthew McConaughey, Jennifer Lawrence, "Grumpy Cat", and others.
In 2014, Bullock ranked No. 2 on "Forbes"‍ '​s most powerful actresses and was honored with the Decade of Hotness Award by Spike Guys' Choice Awards.
Business ventures.
Bullock runs her own production company, Fortis Films. Bullock was an executive producer of the "George Lopez" sitcom, which garnered a lucrative syndication deal that banked her some $10 million (co-produced with Robert Borden). Bullock tried to produce a film based on F.X. Toole's short story "Million-Dollar Baby" but could not interest the studios in a female boxing drama. The story was eventually adapted and directed by Clint Eastwood as the Oscar-winning film, "Million Dollar Baby" (2004). Fortis Films also produced "All About Steve" which was released in September 2009. Her father, John Bullock, is the company's CEO, and her sister, Gesine Bullock-Prado, is the former president.
In November 2006 Bullock founded an Austin, Texas restaurant, Bess Bistro, located on West 6th Street. She later opened another business, Walton's Fancy and Staple, across the street in a building she extensively renovated. Walton's is a bakery, upscale restaurant and floral shop that also offers services including event planning.
Personal life.
Bullock was once engaged to actor Tate Donovan, whom she met while filming "Love Potion No. 9"; their relationship lasted three years. She previously dated football player Troy Aikman, and actors Matthew McConaughey and Ryan Gosling.
Bullock married motorcycle builder and "Monster Garage" host Jesse James on July 16, 2005. They first met when Bullock arranged for her ten-year-old godson to meet James as a Christmas present. In November 2009, Bullock and James entered into a custody battle with James' second ex-wife, former pornographic actress Janine Lindemulder, with whom James had a child. Bullock and James subsequently won full legal custody of James' five-year-old daughter.
In March 2010, a scandal arose when several women claimed to have had affairs with James during his marriage to Bullock. Bullock cancelled European promotional appearances for "The Blind Side" citing "unforeseen personal reasons." On March 18, 2010, James responded to the rumors of infidelity by issuing a public apology to Bullock. He stated, "The vast majority of the allegations reported are untrue and unfounded" and "Beyond that, I will not dignify these private matters with any further public comment." James declared that "There is only one person to blame for this whole situation, and that is me", and asked that his wife and children one day "find it in their hearts to forgive me" for their current "pain and embarrassment." James' publicist subsequently announced on March 30, 2010, that James had checked into a rehab facility "to deal with personal issues" and "save his marriage" to Bullock. However on April 28, 2010, it was reported that Bullock had filed for divorce on April 23 in Austin. Their divorce was finalized on June 28, 2010, with "conflict of personalities" cited as the reason.
Bullock announced on April 28, 2010, that she had proceeded with plans to adopt a son born in January 2010 in New Orleans. Bullock and James had begun an initial adoption process four months earlier. Bullock's son began living with them in January 2010, but they chose to keep the news private until after the Oscars in March 2010. However, given the couple's separation and then divorce, Bullock continued the adoption of her son Louis Bardo Bullock, as a single parent.
On December 20, 2000, Bullock, another passenger, and the two crew survived the crash of a chartered business jet during an attempted night landing at Jackson Hole Airport. The pilots were unable to activate the runway lights due to having out-of-date approach plates, but continued the landing. The aircraft landed in the airport's graded safety area between the runway and parallel taxiway and hit a snowbank. The accident caused a separation of the nose cone and landing gear, partial separation of the right wing, and a bend in the left wing.
On April 18, 2008, while Bullock was in Massachusetts shooting the film "The Proposal", she and her husband were in an SUV that was hit head-on (driver's side offset) at moderate speed by a drunken driver. Vehicle damage was not major and there were no injuries.
Legal matters.
In October 2004, Bullock won a multimillion-dollar judgment against Benny Daneshjou, the builder of her Lake Austin, Texas home; the jury ruled the house was uninhabitable. It has since been torn down and rebuilt.
On April 22, 2007, Marcia Diana Valentine was found lying outside James and Bullock's Southern California home in Orange County. When James confronted the woman, she ran to her car, got behind the wheel, and tried to run over him. The woman is said to be an obsessed fan of Sandra Bullock. The woman was charged with one felony count each of aggravated assault and stalking. Bullock obtained a restraining order to bar Valentine from "contacting or coming near her home, family or work for three years". Valentine pleaded not guilty to charges of aggravated assault and stalking. Valentine was subsequently convicted of stalking and was sentenced to three years of probation.
Bullock was also stalked by Thomas James Weldon, commencing in 2002, across several states. In 2003, Bullock obtained a restraining order against Weldon, which was renewed again in 2006. After the renewed restraining order expired, and Weldon was released from a mental institution, he again traveled across several states to find Bullock, and she then obtained another restraining order.
Philanthropy.
Bullock has been a public supporter of the American Red Cross, having donated $1 million to the organization at least four times. Her first public donation of that amount was to the Red Cross's Liberty Disaster Relief Fund. Three years later, she sent money in response to the 2004 Indian Ocean earthquake and tsunamis. In 2010, she donated $1 million to relief efforts in Haiti following the Haiti earthquake, and again donated the same amount following the 2011 Japan Earthquake.
Along with other stars, Bullock did a public service announcement urging people to sign a petition for clean-up efforts of the oil spill in the Gulf of Mexico. Bullock backs the Texas non-profit organization The Kindred Life Foundation, Inc. In late 2008, she joined other top celebrities in supporting the work of CEO and Founder Amos Ramirez. She shared this at a gala that raised money for the organization, "Amos has led many efforts across our nation that have helped families that are in need. Our country needs more organizations that are committed to the service that Kindred Life is."
In 2012, Bullock was inducted into the Warren Easton Hall of Fame for her donations to charities and in 2013 was honored with the Favorite Humanitarian Award at the 2013 People's Choice Awards for her contributions to New Orleans Warren Easton Charter High School, which was severely damaged after Hurricane Katrina in 2005.

</doc>
<doc id="29458" url="http://en.wikipedia.org/wiki?curid=29458" title="Smallfilms">
Smallfilms

Smallfilms is a British company that made animated television programmes for children, from 1959 to the 1980s. In 2014 the company was operating again and producing a remake of the Clangers. It was originally a partnership between Oliver Postgate (writer, animator and narrator) and Peter Firmin (modelmaker and illustrator). Several very popular series of short films were made using stop-motion animation, including "The Clangers", "Noggin the Nog", and "Ivor the Engine". Another Smallfilms production, "Bagpuss", came top of a BBC poll to find the favourite children's programme.
Background.
In 1957 Postgate was appointed a stage manager with Associated-Rediffusion, which then held the commercial weekday television franchise for London. Attached to the children's programming section, he thought he could do better with the relatively low budgets of the then black and white television productions.
Postgate wrote "Alexander the Mouse," a story about a mouse born to be king. Using an Irish-produced magnetic system – on which animated characters were attached to a painted background, and then photographed through a 45 degree mirror – he persuaded Peter Firmin, who was then teaching at the Central School of Art, to create the background scenes. Postgate later recalled they undertook around 26 of these programmes live-to-air, which were made harder by the production problems encountered by the use and restrictions of using magnets.
After the relative success of "Alexander the Mouse", Postgate agreed a deal to make the next series on film, for a budget of £175 per programme. Making a stop motion animation table in his bedroom, he wrote the Chinese story "The Journey of Master Ho". This was intended for deaf children, a distinct advantage in that the production required no soundtrack which reduced the production costs. He engaged a painter to produce the backgrounds, but as the painter was classical Chinese-trained he produced them in three-quarters view, rather than in the conventional Egyptian full-view manner used for flat animation under a camera. This resulted in the Firmin-produced characters looking like they were short in one leg, but the success of the production provided the foundation for Postgate and Firmin to start up their own company solely producing animated children's programmes.
History.
Setting up their business in a disused cowshed at Firmin's home in Blean near Canterbury, Kent, Postgate and Firmin worked on children's animation programmes. Based on concepts which mostly originated with Postgate, Firmin did the artwork and built the models, while Postgate wrote the scripts, did the stop motion filming and many of the voices. Smallfilms was resultantly able to produce two minutes of film per day, ten times as much as a conventional animation studio, with Postgate moving the cardboard pieces himself, and working his 16mm camera frame-by-frame with a home-made clicker. As Postgate wholly voiced many of the productions, including the WereBear story tapes, his distinctive voice became familiar to generations of children.
They started in 1959 with "Ivor the Engine", a series for ITV about a Welsh steam locomotive who wanted to sing in a choir. Based on Postgate's wartime encounter with Welshman Denzyl Ellis, who used to be the fireman on the Royal Scot, it was remade in colour for the BBC in the 1970s. This was followed by "Noggin the Nog" for the BBC, which established Smallfilms as a safe and reliable pair of hands to produce children's entertainment, in the days when the number of UK television channels was restricted.
In 2000 Postgate and his friend Loaf set up a small publishing company called the Dragons Friendly Society to look after Noggin the Nog, Pogles Wood, Pingwings.
After Postgate's death in December 2008 Smallfilms was inherited by his son Daniel Postgate. Also, Universal took the distribution rights to the works of Smallfilms. Any such agreement does not include the materials Oliver published with The Dragons Friendly Society.
In 2014 Postgate's son Daniel was working with Peter Firmin on a remake of the Clangers. 
Series development and philosophy.
Postgate and Firmin recognised that their product was not to be sold to or bought by children, but by the commissioning television executives. Postgate described in a later interview the then "gentlemanly and rather innocent" business of programme commissioning thus:
Postgate had strict views on storyline development, which perhaps resultantly restricted the length of each particular series development. When asked if the "Clangers" adventures were quite surreal sometimes, Postgate replied:
The Smallfilms system was reliant on the company's only two employees – Postgate and Firmin – and was devoid of the modern considerations and essentials, as Postgate pointed out: "excused the interference of educationalists, sociologists and other pseudo-scientists, which produces eventually a confection of formulae which have no integrity. No, the mainspring of what we did was because it was fun."
Recognising their commissioning audience, Smallfilms purposefully developed storylines which were engaging for both adults and children. While the storylines and production were remembered by children, the adult jokes like those about the Welsh in "Ivor the Engine", or the fact that the Clangers swore occasionally; gave them both an instant parent engagement as well as a later revival with children who had grown up and were re-watching their favourite programmes.
Coolabi.
In October 2008, production company Coolabi acquired the merchandising and distribution rights until 2013 to a number of the Smallfilms productions. Coolabi planned to introduce Bagpuss to a new generation; the company said there was "significant potential to build on the affection in which this classic brand is held".

</doc>
<doc id="29460" url="http://en.wikipedia.org/wiki?curid=29460" title="List of mayors of Sacramento, California">
List of mayors of Sacramento, California

This is a list of mayors of Sacramento, California. The Sacramento City Council met for the first time on August 1, 1849 and the citizens approved the city charter on October 13, 1849. The City Charter was recognized by the State of California on February 27, 1850 and Sacramento was incorporated on March 18, 1850.
"See also :" Lists of incumbents
References.
http://ohp.parks.ca.gov/?page_id=21454

</doc>
<doc id="29462" url="http://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity or corporation through subversion, obstruction, disruption, or destruction. In a workplace setting, sabotage is the conscious withdrawal of efficiency generally directed at causing some change in workplace conditions. One who engages in sabotage is a saboteur. Saboteurs typically try to conceal their identities because of the consequences of their actions.
Any unexplained adverse condition might be sabotage. Sabotage is sometimes called tampering, meddling, tinkering, malicious pranks, malicious hacking, a practical joke or the like to avoid needing to invoke legal and organizational requirements for addressing sabotage.
Etymology.
Claimed explanations include:
As industrial action.
At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811-1812) used sabotage as a means of negotiation in labor disputes.
Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.
The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:
The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.
For the IWW, sabotage came to mean any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments.
One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.
As environmental action.
Certain groups turn to destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.
As war tactic.
In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.
A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.
Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.
Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element." While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.
There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":
From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."
Value of simple sabotage in wartime.
The United States Office of Strategic Services, later renamed the CIA, noted specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."
In World War I.
On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.
On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at 17 million in damages. Wozniak's involvement was not discovered until 1927. 
On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.
Post World War I.
In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.
In World War II.
Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."
The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied airpower. See also Normandy Landings for more information about sabotage on D-Day.
During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.
In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.
After World War II.
From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the Malaysian Government, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communist's sabotage efforts were so successful that they caused backlash amongst the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.
In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the Balfour Declaration in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.
In Vietnam.
The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.
During the Cold War.
On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated 3.7 million dollars in required repairs, and considerably impacting on El Salvadoran business and security. 
In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving Tegucigalpa, the capital city, for three days without power.
As crime.
Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment.
As political action.
The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign. See Watergate. The term could also describe the actions and expenditures of private entities, corporations and organizations against democratically approved or enacted laws, policies and programs.
In a coup d'etat.
Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."
Railroads, where strategically important to the regime the coup is against, are prime targets for sabotage- if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.
Derivative usages.
Sabotage radio.
A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.
Cybotage.
Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms," including "cybotage," a portmanteau from the words "sabotage" and "cyber." They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.
Counter-sabotage.
Counter-sabotage, defined by Webster's dictionary, is "counterintelligence designed to detect and counteract sabotage." The United States Department of Defense definition, found in the Dictionary of Military and Associated Terms, is "Action designed to detect and counteract sabotage. See also counterintelligence".
In World War II.
During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant for the manufacture of heavy bombers, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.
Borrowed into Japanese.
In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.

</doc>
<doc id="29463" url="http://en.wikipedia.org/wiki?curid=29463" title="Scabbard">
Scabbard

A scabbard is a sheath for holding a sword, knife, or other large blade. Scabbards have been made of many materials over the millennia, including leather, wood, and metals such as brass or steel.
Types of scabbards.
Most commonly, scabbards were worn suspended from a sword belt or shoulder belt. (baldric). (see also "Koshirae").
Ancient scabbards.
Wooden scabbards were usually covered in fabric or leather, and leather versions also usually bore metal fittings for added protection and carrying ease. Japanese blades, however, typically have their sharp cutting edge protected by a wooden scabbard called a "saya". Many scabbards like the ones the Greeks and Romans used were small and light. They were designed for holding the sword rather than protecting it. All-metal scabbards were popular items for a display of wealth among elites in the European Iron Age, and often intricately decorated. A lot is unknown on the scabbards of early Iron Age since they were made of wood.However during the Middle and late Iron Ages, the scabbard became important especially as a vehicle for decorative elaboration. After 200 BC fully decorated scabbards became rare. A number of ancient scabbards have been recovered from weapons sacrifices, a few of which had a lining of fur on the inside. The fur was probably kept oily, keeping the blade free from rust. The fur would also allow a smoother, quicker draw.
Modern scabbards.
Entirely metal scabbards became popular in Europe early in the 19th century and eventually superseded most other types. Metal was more durable than leather and could better withstand the rigors of field use, particularly among troops mounted on horseback. In addition, metal offered the ability to present a more military appearance, as well as the opportunity to display increased ornamentation. Nevertheless, leather scabbards never entirely lost favor among military users and were widely used as late as the American Civil War (1861–65).
Some military police forces, naval shore patrols, law enforcement and other groups used leather scabbards as a kind of truncheon.
Scabbards were never worn across one's back in European, Near East, or Indian military cultures and depictions of such are a modern invention and have enjoyed great popularity in fiction and fantasy, to the point that they are widely believed to be a Medieval invention. Some well-known examples of this include the back scabbard depicted in the movie "Braveheart" and the back scabbard seen in the video game series "The Legend of Zelda". There is some limited data from woodcuts and textual fragments that Mongol light horse archers and some Chinese soldiers wore a slung baldric over the shoulder, allowing longer blades to be strapped across the back.
However in "The Ancient Celts" by Barry Cunliffe, on page 94 of that book, Professor Cunliffe writes,"All these pieces of equipment [shields, spears, swords, mail armour], mentioned in the texts, are reflected in the archaeological record and in the surviving iconography, though it is sometimes possible to detect regional variations. Among the Parisii of Yorkshire, for example, "the sword was sometimes worn across the back and therefore had to be drawn over the shoulder from behind the head".""
Common terms.
The metal fitting where the blade enters the leather or metal scabbard is called the throat, which is often part of a larger scabbard mount, or locket, that bears a carrying ring or stud to facilitate wearing the sword. The blade's point in leather scabbards is usually protected by a metal tip, or chape, which on both leather and metal scabbards is often given further protection from wear by an extension called a drag, or shoe.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="29467" url="http://en.wikipedia.org/wiki?curid=29467" title="Spinel">
Spinel

Spinel is the magnesium aluminium member of the larger spinel group of minerals. It has the formula MgAl2O4. Balas ruby is an old name for a rose-tinted variety.
Properties of true spinel.
Spinel crystallizes in the isometric system; common crystal forms are octahedra, usually twinned. It has an imperfect octahedral cleavage and a conchoidal fracture. Its hardness is 8, its specific gravity is 3.5–4.1 and it is transparent to opaque with a vitreous to dull luster. It may be colorless, but is usually various shades of red, blue, green, yellow, brown, or black. There is a unique natural white spinel, now lost, that surfaced briefly in what is now Sri Lanka. Some spinels are among the most famous gemstones: Among them are the Black Prince's Ruby and the "Timur ruby" in the British Crown Jewels, and the "Côte de Bretagne", formerly from the French Crown jewels. The Samarian Spinel is the largest known spinel in the world, weighing 500 carat.
The transparent red spinels were called spinel-rubies or balas rubies. In the past, before the arrival of modern science, spinels and rubies were equally known as rubies. After the 18th century the word ruby was only used for the red gem variety of the mineral corundum and the word spinel came to be used. "Balas" is derived from Balascia, the ancient name for Badakhshan, a region in central Asia situated in the upper valley of the Kokcha River, one of the principal tributaries of the Oxus River. The Badakshan Province was for centuries the main source for red and pink spinels.
Occurrence.
Spinel has long been found in the gemstone-bearing gravel of Sri Lanka and in limestones of the Badakshan Province in modern day Afghanistan and of Mogok in Burma. Recently gem quality spinels were also found in the marbles of Luc Yen (Vietnam), Mahenge and Matombo (Tanzania), Tsavo (Kenya) and in the gravels of Tunduru (Tanzania) and Ilakaka (Madagascar). Spinel is found as a metamorphic mineral, and also as a primary mineral in rare mafic igneous rocks; in these igneous rocks, the magmas are relatively deficient in alkalis relative to aluminium, and aluminium oxide may form as the mineral corundum or may combine with magnesia to form spinel. This is why spinel and ruby are often found together.
Spinel, (Mg,Fe)(Al,Cr)2O4, is common in peridotite in the uppermost Earth's mantle, between approximately 20 km to approximately 120 km, possibly to lower depths depending on the chromium content. At significantly shallower depths, above the Moho, calcic plagioclase is the more stable aluminous mineral in peridotite, while garnet is the stable phase deeper in the mantle below the spinel stability region. 
Spinel, (Mg,Fe)Al2O4, is a common mineral in the Ca-Al-rich inclusions (CAIs) in some chondritic meteorites.
Synthetic spinel.
Synthetic spinel was accidentally produced in the middle of the 18th century, and has been more recently described in scientific publications in 2000 and 2004. By 2015, transparent spinel was being made in sheets and other shapes through sintering.

</doc>
<doc id="29468" url="http://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

In computer science and electrical engineering, speech recognition (SR) is the translation of spoken words into text. It is also known as "automatic speech recognition" (ASR), "computer speech recognition", or just "speech to text" (STT).
Some SR systems use "speaker-independent speech recognition" while others use "training" where an individual speaker reads sections of text into the SR system. These systems analyze the person's specific voice and use it to fine-tune the recognition of that person's speech, resulting in more accurate transcription. Systems that do not use training are called "speaker-independent" systems. Systems that use training are called "speaker-dependent" systems.
Speech recognition applications include voice user interfaces such as voice dialling (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed Direct Voice Input).
The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data.
History.
As early as 1932, Bell Labs researchers like Harvey Fletcher were investigating the science of speech perception. In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.
Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce's letter compared speech recognition to "schemes for turning water into gasoline, extracting gold from the sea, curing cancer, or going to the moon." Pierce defunded speech recognition research at Bell Labs.
Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess. Also around this time Soviet researchers invented the dynamic time warping algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. Achieving speaker independence was a major unsolved goal of researchers during this time period.
In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. BBN. IBM., Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter. Despite the fact that CMU's Harpy system met the goals established at the outset of the program, many of the predictions turned out to be nothing more than hype disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.
During the late 1960's Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. At CMU, Raj Reddy's student James Baker and his wife Janet Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.
Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the 1980s.
IBM had a few competitors including Dragon Systems founded by James and Janet Baker in 1982. The 1980s also saw the introduction of the n-gram language model.
Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models.
Another one of Raj Reddy's former students, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Huang went on to found the speech recognition group at Microsoft in 1993.
The 1990s saw the first introduction of commercially successful speech recognition technologies. By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. In 2000, Lernout & Hauspie acquired Dragon Systems and was an industry leader until an accounting scandal brought an end to the company in 2001. The L&H speech technology was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.
In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, BBN,
Cambridge University and a team composed of ISCI, SRI and University of Washington. The GALE program focused on Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 with the launch of GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.
The use of deep learning for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979." In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have began to use deep learning techniques for language modeling as well.
In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 80's, 90's and a few years into 2000.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
A number of key difficulties had been methodologically analyzed in 1990's, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009-2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited the renaissance of neural networks and initiated deep learning research and applications in speech recognition.
Models, methods, and algorithms.
Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.
Hidden Markov models.
Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.
Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.
Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).
Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).
A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.
Dynamic time warping (DTW)-based speech recognition.
Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.
Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.
A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.
Neural networks.
Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, and speaker adaptation.
In contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phones and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.
However, recently Recurrent Neural Networks(RNN's) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition. This however enormously increases the computational cost involved and hence makes the process of speech recognition slower. A lot of research is still going on in this field to ensure that TDNN's and RNN's can be used in a more computationally affordable way to improve the Speech Recognition Accuracy immensely.
Deep Neural Networks and Denoising Autoencoders are also being experimented with to tackle this problem in an effective manner.
Due to the inability of traditional Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.
Deep Neural Networks and Other Deep Learning Models.
A deep neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data. The DNN is the most popular type of deep learning architectures successfully used as an acoustic model for speech recognition since 2010.
The success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
 See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent . See also the related background of automatic speech recognition and the impact of various machine learning paradigms including notably deep learning in
a recent overview article.
One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.
Since the initial successful debut of DNNs for speech recognition around 2009-2011, there have been huge new progresses made. This progress (as well as future directions) has been summarized into the following eight major areas: 
Large-scale automatic speech recognition is the first and the most convincing successful case of deep learning in the recent history, embraced by both industry and academic across the board. Between 2010 and 2014, the two major conferences on signal processing and speech recognition, IEEE-ICASSP and Interspeech, have seen near exponential growth in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition. More importantly, all major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) nowadays are based on deep learning methods. See also the recent media interview with the CTO of Nuance Communications.
Applications.
In-car systems.
Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands. allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.
Health care.
Medical documentation.
In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalised. Deferred speech recognition is widely used in the industry currently.
One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary are relatively minimal for people who are sighted and who can operate a keyboard and mouse.
A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases - e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam - e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.
As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.
Therapeutic use.
Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.
Military.
High-performance fighter aircraft.
Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note is the U.S. program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), and a program in France installing speech recognition systems on Mirage aircraft, and also programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.
Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing G-loads. It was also concluded that adaptation greatly improved the results in all cases and introducing models for breathing was shown to improve recognition scores significantly. Contrary to what might be expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as could be expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.
The Eurofighter Typhoon currently in service with the UK RAF employs a speaker-dependent system, i.e. it requires each pilot to create a template. The system is not used for any safety critical or weapon critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands.
Speaker-independent systems are also being developed and are in testing for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracies in excess of 98%.
Helicopters.
The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.
As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.
Training air traffic controllers.
Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.
The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.
Telephony and other domains.
ASR in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.
The improvement of mobile processor speeds made feasible the speech-enabled Symbian and Windows Mobile smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), and SVOX.
Usage in education and daily life.
For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.
Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.
Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.
Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.
Voice Recognition Software's use, in conjunction with a digital audio recorder, a personal computer and Microsoft Word has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.
People with disabilities.
People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.
Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involved disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.
This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.
Performance.
The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
However, speech recognition (by a machine) is a very complex problem. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition vary with the following:
Accuracy.
As mentioned earlier in this article, accuracy of speech recognition varies in the following:
e.g. The 10 digits "zero" to "nine" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.
e.g. The 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: "B, C, D, E, G, P, T, V, Z");
an 8% error rate is considered good for this vocabulary.
A speaker-dependent system is intended for use by a single speaker.
A speaker-independent system is intended for use by any speaker, more difficult.
With isolated speech single words are used, therefore it becomes easier to recognize the speech.
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
e.g. Querying application may dismiss the hypothesis "The apple is red." 
e.g. Constraints may be semantic; rejecting "The apple is angry." 
e.g. Syntactic; rejecting "Red is apple the." 
Constraints are often represented by a grammar. 
When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like "uh" and "um", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. 
Environmental noise (e.g. Noise in a car or a factory) 
Acoustical distortions (e.g. echoes, room acoustics)
Speech recognition is a multi-levelled pattern recognition task.
e.g. Phonemes, Words, Phrases, and Sentences;
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level;
Speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: 
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;
Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).
The sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.
Given basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.
In 1982, Kurzweil Applied Intelligence and Dragon Systems released speech recognition products. By 1985, Kurzweil’s software had a vocabulary of 1,000 words—if uttered one word at a time. Two years later, in 1987, its lexicon reached 20,000 words, entering the realm of human vocabularies, which range from 10,000 to 150,000 words. But recognition accuracy was only 10% in 1993. Two years later, the error rate crossed below 50%. Dragon Systems released "Naturally Speaking" in 1997, which recognized normal human speech. Progress mainly came from improved computer performance and larger source text databases. The Brown Corpus was the first major database available, containing several million words. Carnegie Mellon University researchers found no significant increase in recognition accuracy.
Further information.
Conferences and Journals.
Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing --- after merging with an ACM publication), Computer Speech and Language, and Speech Communication.
Books.
Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc. More up to date are "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of "Speech and Language Processing (2008)" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniuqes as is done in speech recognition. A most recent comprehensive textbook, "Fundamentals of Speaker Recognition" by Homayoon Beigi, is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).
A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).
The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009-2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.
Software.
In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). The AT&T libraries GRM and DCD are also general software libraries for large-vocabulary speech recognition. For more recent and state-of-the-art techniques, Kaldi toolkit can be used.
For more software resources, see List of speech recognition software.

</doc>
<doc id="29469" url="http://en.wikipedia.org/wiki?curid=29469" title="Sapphire">
Sapphire

Sapphire (Greek: σάπφειρος; "sappheiros", 'blue stone', which probably referred instead at the time to lapis lazuli) is a typically blue gemstone variety of the mineral corundum, an aluminium oxide (α-Al2O3). Trace amounts of elements such as iron, titanium, chromium, copper, or magnesium can give corundum respectively blue, yellow, purple, orange, or green color. Chromium impurities in corundum yield pink or red tint, the latter being called ruby.
Commonly, sapphires are worn in jewelry. Sapphires may be found naturally, by searching through certain sediments (due to their resistance to being eroded compared to softer stones) or rock formations. They also may be manufactured for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires—9 on the Mohs scale (the third hardest mineral, right behind diamond at 10 and moissanite at 9.25)—and of aluminium oxide in general, sapphires are used in some non-ornamental applications, including infrared optical components, such as in scientific instruments; high-durability windows; wristwatch crystals and movement bearings; and very thin electronic wafers, which are used as the insulating substrates of very special-purpose solid-state electronics (especially integrated circuits and GaN-based LEDs).
Natural sapphires.
The sapphire is one of the three gem-varieties of corundum, the other two being ruby – defined as corundum in a shade of red—and padparadscha—a pinkish orange variety. Although blue is their most well-known color, sapphires may also be colorless and they are found in many colors including shades of gray and black.
The cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality – as well as their geographic origin. Significant sapphire deposits are found in Eastern Australia, Thailand, Sri Lanka, China (Shandong), Madagascar, East Africa, and in North America in a few locations, mostly in Montana. Sapphire and rubies are often found in the same geographic environment, but one of the gems is usually more abundant in any of the sites.
Blue sapphire.
Color in gemstones breaks down into three components: hue, saturation, and tone. Hue is most commonly understood as the "color" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).
Blue sapphires are evaluated based upon the purity of their primary hue. Purple, violet, and green are the most common secondary hues found in blue sapphires. Violet and purple can contribute to the overall beauty of the color, while green is considered to be distinctly negative. Blue sapphires with up to 15% violet or purple are generally said to be of fine quality. Blue sapphires with any amount of green as a secondary hue are not considered to be fine quality. Gray is the normal saturation modifier or mask found in blue sapphires. Gray reduces the saturation or brightness of the hue, and therefore has a distinctly negative effect.
The color of fine blue sapphires may be described as a vivid medium dark violet to purplish blue where the primary blue hue is at least 85% and the secondary hue no more than 15%, without the least admixture of a green secondary hue or a gray mask.
The 423 carat Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence.
Sapphires of other colors.
Yellow and green sapphires are also commonly found. Pink sapphires deepen in color as the quantity of chromium increases. The deeper the pink color the higher their monetary value, as long as the color is tending toward the red of rubies. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a "pink sapphire".
Sapphires also occur in shades of orange and brown. Colorless sapphires are sometimes used as diamond substitutes in jewelry. Natural padparadscha (pinkish orange) sapphires often draw higher prices than many of even the finest blue sapphires. Recently, more sapphires of this color have appeared on the market as a result of a new artificial treatment method called "lattice diffusion".
Padparadscha.
"Padparadscha" is a delicate light to medium toned pink-orange to orange-pink hue corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.
The name is derived from the Sanskrit "padma ranga" (padma = lotus; ranga = color), a color akin to the lotus flower (Nelumbo nucifera ‘Speciosa’).
Star sapphire.
A "star sapphire" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as "star rubies". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed "star"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut "en cabochon", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallisation, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.
The inclusions can alternatively produce a "cat's eye" effect if the 'face-up' direction of the cabochon's dome is oriented perpendicular to the crystal's c-axis rather than parallel to it. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.
The Black Star of Queensland, the largest gem-quality star sapphire in the world, weighs 733 carats. The Star of India (mined in Sri Lanka) (weighing 563.4 carats) is thought to be the second-largest star sapphire (the largest blue), and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, (mined in Sri Lanka), located in the National Museum of Natural History, in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism.
Color change sapphire.
A rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Thailand and Tanzania. The color-change effect is caused by the interaction of the sapphire, which absorbs specific wavelengths of light, and the light-source, whose spectral output varies depending upon the illuminant. Transition-metal impurities in the sapphire, such as chromium and vanadium, are responsible for the color change.
Certain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as "alexandrium" or "synthetic alexandrite". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite "simulants". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.
Source of color.
Rubies are corundum which contain chromium impurities that absorb yellow-green light and result in deeper ruby red color with increasing content. Purple sapphires contain trace amounts of vanadium and come in a variety of shades. Corundum that contains ~0.01% of titanium is colorless. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a deep-blue color.
Unlike localized ("intra-atomic") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe2+ or Fe3+, while titanium generally takes the form Ti4+. If Fe2+ and Ti4+ ions are substituted for Al3+, localized areas of charge imbalance are created. An electron transfer from Fe2+ and Ti4+ can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.
Intervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.
Treatments.
Sapphires may be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance color. This is done by heating the sapphires in furnaces to temperatures between 500 and for several hours, or by heating in a nitrogen-deficient atmosphere oven for seven days or more. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures are used, the stone loses all silk (inclusions) and it becomes clear under magnification. The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to "no evidence of heat treatment".
Yogo sapphires sometimes do not need heat treating because their cornflower blue coloring is uniform and deep, they are generally free of the characteristic inclusions, and they have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by 1982 the heat treatment became a major issue. At that time, 95% of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front page story in the "Wall Street Journal" on August 29, 1984 in an article by Bill Richards, "Carats and Schticks: Sapphire Marketer Upsets The Gem Industry".
Diffusion treatments are used to add impurities to the sapphire to enhance color. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially ("c." 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. The colored layer can be removed when stones chip or are repolished or refaceted, depending on the depth of the impurity layer. Treated padparadschas may be very difficult to detect, and many stones are certified by gemological labs ("e.g.", Gubelin, SSEF, AGTA).
According to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.
There are several ways of treating sapphire. Heat-treatment in a reducing or oxidising atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as "heating only" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as "diffusion" in the gem trade. However, despite what the terms "heating only" and "diffusion" might suggest, both of these categories of treatment actually involve diffusion processes.
Mining.
Sapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, USA, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.
Sapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir (India), Burma, or Sri Lanka that have not been subjected to heat-treatment.
The Logan sapphire, the Star of India, and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.
In North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Missoula, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.
The sapphire deposits of Kashmir are still well known in the gem industry, despite the fact that the peak production from this area mostly took place in a relatively short period at the end of the nineteenth and early twentieth centuries. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. At present, the world record price-per-carat for sapphire at auction was achieved by a sapphire from Kashmir in a ring, which sold for approximately $212,000 per carat (more than $7.45 million in total, including buyer's premium) in May 2015.
Synthetic sapphire.
In 1902 the French chemist Auguste Verneuil developed a process for producing synthetic sapphire crystals. In the Verneuil process, named after him, fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a mantle. The alumina in the flame is slowly deposited, creating a teardrop shaped "boule" of sapphire material. Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.
Synthetic sapphire is also produced industrially from agglomerated aluminium oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.
In 2003 the world's production of synthetic sapphire was 250 tons (1.25 × 109 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material:
The first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.
High quality synthetic sapphire substrates use in nanotechnology is often called Blue Glass, due to its blue color.
Common applications.
Along with zirconia and aluminium oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.
One type of xenon arc lamp (originally called the "Cermax" its first brand name), which is now known generically as the "ceramic body xenon lamp", uses sapphire crystal output windows that tolerate higher thermal loads – and thus higher output powers when compared with conventional Xe lamps with pure silica window.
Sapphire glass.
One application of synthetic sapphire is "sapphire glass" often called "blue glass" as sapphires are blue in color. Here "glass" is a layman term which refers not to the amorphous state, but to the transparency. Sapphire is not only highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the human eye can discern wavelengths from about 380 nm to 750 nm), but is also extraordinarily scratch-resistant. Sapphire has a value of 9 on the Mohs scale of mineral hardness.
The key benefits of sapphire windows are:
So-called "sapphire glass" refers to crystalline sapphire used as an optical window or cover. Some windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application. The boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.
Magical Mirror X5 manufactured by the Chinese company Desay is the first smartphone utilizing a sapphire window screen. Previously sapphire window was used for example in Apple Touch ID of the iPhone 5s, iPhone 6, and iPad mini 3 and the display of the Apple Watch. Also, sapphire covers are used for the rear camera in every iPhone 5 or newer, and every iPod Touch (5th generation) or newer.
Sapphire windows are used in high pressure chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.
It is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.
Use as substrate for semiconducting circuits.
Thin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or "SOS"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. "SOS" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.
In one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.
Wafers of single-crystal sapphire are also used in the semiconductor industry as a substrate for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).

</doc>
<doc id="29471" url="http://en.wikipedia.org/wiki?curid=29471" title="Slack voice">
Slack voice

Slack voice (or lax voice) is the pronunciation of consonant or vowels with a glottal opening slightly wider than that occurring in modal voice. Such sounds are often referred to informally as lenis or half-voiced in the case of consonants. In some Chinese languages, such as Wu, and in many Austronesian languages, the 'intermediate' phonation of slack stops confuses listeners of languages without these distinctions, so that different transcription systems may use /p/ or /b/ for the same consonant. In Xhosa, slack-voiced consonants have usually been transcribed as breathy voice. Although the IPA has no dedicated diacritic for slack voice, the voiceless diacritic (the under-ring) may be used with a voiced consonant letter, though this convention is also used for partially voiced consonants in languages such as English.
Wu Chinese "muddy" consonants are slack voice word-initially, the primary effect of which is a slightly breathy quality of the following vowel.
Javanese contrasts slack and stiff voiced bilabial, dental, retroflex, and velar stops.
Parauk contrasts slack voicing in its vowels. The contrast is between "slightly stiff" and "slightly breathy" vowels; the first are between modal and stiff voice, while the latter are captured by slack voice.

</doc>
<doc id="29472" url="http://en.wikipedia.org/wiki?curid=29472" title="SADC">
SADC

SADC may stand for:

</doc>
<doc id="29473" url="http://en.wikipedia.org/wiki?curid=29473" title="Salvation">
Salvation

Salvation (Latin "salvatio"; Greek "sōtēria"; Hebrew "yeshu'ah") is being saved or protected from harm or being saved or delivered from some dire situation. In religion, salvation is stated as the saving of the soul from sin and its consequences.
The academic study of salvation is called soteriology.
Meaning.
In religion, salvation is the saving of the soul from sin and its consequences. It may also be called "deliverance" or "redemption" from sin and its effects. Salvation is considered to be caused either by the free will and grace of a deity or by personal efforts through prayer and asceticism, or some combination of the two. Religions often emphasize the necessity of both personal effort—for example, repentance and asceticism—and divine action (e.g. grace).
Abrahamic religions.
Judaism.
In contemporary Judaism, redemption (Hebrew "ge'ulah"), refers to God redeeming the people of Israel from their various exiles. This includes the final redemption from the present exile.
Judaism holds that adherents do not need personal salvation as Christians believe. Jews do not subscribe to the doctrine of Original sin. Instead, they place a high value on individual morality as defined in the law of God — embodied in what Jews know as the Torah or The Law, given to Moses by God on Mount Sinai, the summary of which is comprised in the Ten Commandments. The Jewish sage Hillel the Elder states that The Law can be further compressed in just one line, popularly known as the Golden Rule: "That which is hateful to you, do not do unto your fellow".
In Judaism, salvation is closely related to the idea of redemption, a saving from the states or circumstances that destroy the value of human existence. God as the universal spirit and Creator of the World, is the source of all salvation for humanity, provided an individual honours God by observing his precepts. So redemption or salvation depends on the individual. Judaism stresses that salvation cannot be obtained through anyone else or by just invoking a deity or believing in any outside power or influence.
The Jewish concept of Messiah visualises the return of the prophet Elijah as the harbinger of one who will redeem the world from war and suffering, leading mankind to universal brotherhood under the fatherhood of one God. The Messiah is not considered as a future divine or supernatural being but as a dominating human influence in an age of universal peace, characterised by the spiritual regeneration of humanity.
In Judaism, salvation is open to all people and not limited to those of the Jewish faith; the only important consideration being that the people must observe and practise the ethical pattern of behaviour as summarised in the Ten Commandments. When Jews refer to themselves as the chosen people of God, they do not imply they have been chosen for special favours and privileges but rather they have taken it upon themselves to show to all peoples by precept and example the ethical way of life.
When examining Jewish intellectual sources throughout history, there is clearly a spectrum of opinions regarding death versus the Afterlife. Possibly an over-simplification, one source says salvation can be achieved in the following manner: Live a holy and righteous life dedicated to Yahweh, the God of Creation. Fast, worship, and celebrate during the appropriate holidays.
By origin and nature, Judaism is an ethnic religion. Therefore, salvation has been primarily conceived in terms of the destiny of Israel as the elect people of Yahweh (often referred to as “the Lord”), the God of Israel. In the biblical text of Psalms, there is a description of death, when people go into the earth or the "realm of the dead" and cannot praise God. The first reference to resurrection is collective in Ezekiel's vision of the dry bones, when all the Israelites in exile will be resurrected. There is a reference to individual resurrection in the Book of Daniel (165 BCE), the last book of the Hebrew Bible. It was not until the 2nd century BCE that there arose a belief in an afterlife, in which the dead would be resurrected and undergo divine judgment. Before that time, the individual had to be content that his posterity continued within the holy nation.
The salvation of the individual Jew was connected to the salvation of the entire people. This belief stemmed directly from the teachings of the Torah. In the Apostle Paul's letter to the Romans, the notion of corporate salvation of Israel is reflected. In the Torah, God taught his people sanctification of the individual. However, he also expected them to function together (spiritually) and be accountable to one another. The concept of salvation was tied to that of restoration for Israel.
During the Second Temple Period, the Sadducees, High Priests, denied any particular existence of individuals after death because it wasn't written in the Torah, while the Pharisees, ancestors of the rabbis, affirmed both bodily resurrection and immortality of the soul, most likely based on the influence of Hellenistic ideas about body and soul and the Pharisaic belief in the Oral Torah. The Pharisees maintained that after death, the soul is connected to God until the messianic era when it is rejoined with the body in the land of Israel at the time of resurrection.
Christianity.
Christianity’s primary premise is that the incarnation and death of Jesus Christ formed the climax of a divine plan for humanity’s salvation. This plan was conceived by God consequent on the Fall of Adam, the progenitor of the human race, and it would be completed at the Last Judgment, when the Second Coming of Christ would mark the catastrophic end of the world.
For Christianity, salvation is only possible through Jesus Christ. Christians believe that Jesus' death on the cross was the once-for-all sacrifice that atoned for the sin of humanity.
The Christian religion, though not the exclusive possessor of the idea of redemption, has given to it a special definiteness and a dominant position. Taken in its widest sense, as deliverance from dangers and ills in general, most religions teach some form of it. It assumes an important position, however, only when the ills in question form part of a great system against which human power is helpless.
According to Christian belief, sin as the human predicament is considered to be universal. For example, in the Apostle Paul declared everyone to be under sin—Jew and Gentile alike. Similarly, the Apostle John was explicit: "If we say that we have no sin, we deceive ourselves and the truth is not in us". Again, he said, "Should we say that we have not sinned, we make him a liar and his word is not in us". Salvation is made possible by the life, death, and resurrection of Jesus, which in the context of salvation is referred to as the "atonement". Christian soteriology ranges from exclusive salvation:p.123 to universal reconciliation concepts. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.
"At the heart of Christian faith is the reality and hope of salvation in Jesus Christ. Christian faith is faith in the God of salvation revealed in Jesus of Nazareth. The Christian tradition has always equated this salvation with the transcendent, eschatological fulfillment of human existence in a life freed from sin, finitude, and mortality and united with the triune God. This is perhaps "the" non-negotiable item of Christian faith. What has been a matter of debate is the relation between salvation and our activities in the world."—Anselm Kyongsuk Min:p.79
"The Bible presents salvation in the form of a story that describes the outworking of God's eternal plan to deal with the problem of human sin. The story is set against the background of the history of God's people and reaches its climax in the person and work of Christ. The Old Testament part of the story shows that people are sinners by nature, and describes a series of covenants by which God sets people free and makes promises to them. His plan includes the promise of blessing for all nations through Abraham and the redemption of Israel from every form of bondage. God showed his saving power throughout Israel's history, but he also spoke about a Messianic figure who would save all people from the power, guilt, and penalty of sin. This role was fulfilled by Jesus, who will ultimately destroy all the devil's work, including suffering, pain, and death."—"Macmillan Dictionary of the Bible."
Variant views on salvation are among the main fault lines dividing the various Christian denominations, both between Roman Catholicism and Protestantism and within Protestantism, notably in the Calvinist–Arminian debate, and the fault lines include conflicting definitions of depravity, predestination, atonement, but most pointedly justification.
Salvation is believed to be a process that begins when a person first becomes a Christian, continues through that person's life, and is completed when they stand before Christ in judgment. Therefore, according to Catholic apologist James Akin, the faithful Christian can say in faith and hope, "I "have been" saved; I "am being" saved; and I "will be" saved."
Christian salvation concepts are varied and complicated by certain theological concepts, traditional beliefs, and dogmas. Scripture is subject to individual and ecclesiastical interpretations. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.
The purpose of salvation is debated, but in general most Christian theologians agree that God devised and implemented his plan of salvation because he loves them and regards human beings as his children. Since human existence on Earth is said to be "[given] to sin", salvation also has connotations that deal with the liberation of human beings from sin, and the suffering associated with the punishment of sin—i.e., "the wages of sin is death."
Christians believe that salvation depends on the grace of God. Stagg writes that a fact assumed throughout the Bible is that humanity is in "serious trouble from which we need deliverance…. The fact of sin as the human predicament is implied in the mission of Jesus, and it is explicitly affirmed in that connection". By its nature, salvation must answer to the plight of humankind as it actually is. Each individual's plight as sinner is the result of a fatal choice involving the whole person in bondage, guilt, estrangement, and death. Therefore, salvation must be concerned with the total person. "It must offer redemption from bondage, forgiveness for guilt, reconciliation for estrangement, renewal for the marred image of God".
Islam.
In Islam, salvation refers to the eventual entrance to heaven. Islam teaches that people who die disbelieving in the God do not receive salvation. It also teaches that non-Muslims who die believing in the God but disbelieving in his message (Islam), are left to his will. Those who die believing in the One God and his message (Islam) receive salvation.
Narrated Anas Radeyallāhu ′Anhu that Muhammad Ṣallallāhu ′alayhe wa sallam said,
Whoever said "None has the right to be worshipped but Allah" and has in his heart good (faith) equal to the weight of a barley grain will be taken out of Hell. And whoever said, "None has the right to be worshipped but Allah" and has in his heart good (faith) equal to the weight of a wheat grain will be taken out of Hell. And whoever said, "None has the right to be worshipped but Allah" and has in his heart good (faith) equal to the weight of an atom will be taken out of Hell.—Muhammad Sahih al-Bukhari, 
Islam teaches that all who enter into Islam must remain so in order to receive salvation.
"If anyone desires a religion other than Islam (submission to Allah), never will it be accepted of him; and in the Hereafter He will be in the ranks of those who have lost (all spiritual good)."— Quran, sura 3 (Al Imran), ayat 85
For those who have not been granted Islam or to whom the message has not been brought;
Those who believe (in the Qur'an), those who follow the Jewish (scriptures), and the Sabians and the Christians,- any who believe in Allah and the Last Day, and work righteousness,- on them shall be no fear, nor shall they grieve."
Tawhid.
Belief in the “One God”, also known as the "Tawhid" (التَوْحيدْ) in Arabic, consists of two parts (or principles):
Sin and repentance.
Islam also stresses that in order to gain salvation, one must also avoid sinning along with performing good deeds. Islam acknowledges the inclination of humanity towards sin. Therefore, Muslims are constantly commanded to seek God's forgiveness and repent. Islam teaches that no one can gain salvation simply by virtue of their belief or deeds, instead it is the Mercy of God, which merits them salvation. However, this repentance must not be used to sin any further. Islam teaches that God is Merciful, but it also teaches that He is Omnipresent. The Quran states:
Allah accepts the repentance of those who do evil in ignorance and repent soon afterwards; to them will Allah turn in mercy: For Allah is full of knowledge and wisdom. Of no effect is the repentance of those who continue to do evil, until death faces one of them, and he says, "Now have I repented indeed;" nor of those who die rejecting Faith: for them have We prepared a punishment most grievous.— Qur'an, sura 4 (An-Nisa), ayat 17 
Allah forgiveth not that partners should be set up with Him; but He forgiveth anything else, to whom He pleaseth; to set up partners with Allah is to devise a sin Most heinous indeed.—Qur'an, sura 4 (An-Nisa), ayat 48 
Islam describes a true believer to have Love of God and Fear of God. Islam also teaches that every person is responsible for their own sins. The Quran states;
If ye reject (Allah), Truly Allah hath no need of you; but He liketh not ingratitude from His servants: if ye are grateful, He is pleased with you. No bearer of burdens can bear the burden of another. In the end, to your Lord is your Return, when He will tell you the truth of all that ye did (in this life). for He knoweth well all that is in (men's) hearts.— Qur'an, sura 39 (Az-Zumar), ayat 7 
Al-Agharr al-Muzani, a companion of Mohammad, reported that Ibn 'Umar stated to him that Mohammad said,
O people, seek repentance from Allah. Verily, I seek repentance from Him a hundred times a day.—Prophet Mohammad Sahih Muslim, 
Sin in Islam is not a state, but an action (a bad deed); Islam teaches that a child is born sinless, regardless of the belief of his parents, dies a Muslim; he enters heaven, and does not enter hell. Sahih al-Bukhari, 
Narrated Aisha, that Mohammad said, "Do good deeds properly, sincerely and moderately, and receive good news because one's good deeds will not make him enter Paradise." They asked, "Even you, O Allah's Apostle?" He said, "Even I, unless and until Allah bestows His pardon and Mercy on me." Sahih al-Bukhari, 
Five Pillars.
There are acts of worship that Islam teaches to be mandatory. Islam is built on five principles. Narrated Ibn 'Umar that Muhammad said,
Islam is based on (the following) five (principles):
Not performing the mandatory acts of worship may deprive Muslims of the chance of salvation.
Indian religions.
Hinduism, Buddhism, Jainism and Sikhism share certain key concepts, which are interpreted differently by different groups and individuals. In those religions one is not liberated from sin and its consequences, but from the cycle of rebirth which is perpetuated by the passions and delusions, and its resulting actions. They differ however on the exact nature of this liberation. Salvation is called "moksha" or "mukti" which mean liberation and release respectively. This state and the conditions considered necessary for its realization is described in early texts of Indian religion such as the Upanishads and the Pali Canon, and later texts such the Yoga Sutras of Patanjali and the Vedanta tradition. "Moksha" can be attained by practicing "Sādhanā", literally "a means of accomplishing something". It includes a variety of disciplines, such as yoga and meditation.
"Nirvana" is the profound peace of mind that is acquired with "moksha" (liberation). In Buddhism and Jainism, it is the state of being free from suffering. In Hindu philosophy, it is union with the Brahman (Supreme Being). The word literally means "blown out" (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired there-after.
In Theravada Buddhism the emphasis is on one's own liberation from samsara. The Mahayana traditions emphasize the Bodhisattva-path, in which "each Buddha and Bodhisattwa is a redeemer", assisting the Buddhist in seeking to achieve the redemptive state. The assistance rendered is a form of self-sacrifice on the part of the teachers, who would presumably be able to achieve total detachment from worldly concerns, but have instead chosen to remain engaged in the material world to the degree that this is necessary to assist others in achieving such detachment.
Other disciplines are not so desolate, and "each Buddha and Bodhisattwa is a redeemer", assisting the Buddhist in seeking to achieve the redemptive state.
Sources.
</dl>

</doc>
<doc id="29475" url="http://en.wikipedia.org/wiki?curid=29475" title="Lockheed S-3 Viking">
Lockheed S-3 Viking

The Lockheed S-3 Viking is a four-seat, twin-engine turbofan-powered jet aircraft that was used by the U.S. Navy to identify and track enemy submarines. In the late 1990s, the S-3B's mission focus shifted to surface warfare and aerial refueling. The Viking also provided electronic warfare and surface surveillance capabilities to the carrier battle group. A carrier-based, subsonic, all-weather, multi-mission aircraft with long range; it carried automated weapon systems, and was capable of extended missions with in-flight refueling. Because of the Viking's engines’ low-pitched sound, it was nicknamed the "Hoover" after the vacuum cleaner brand.
The S-3 was retired from front-line fleet service aboard aircraft carriers by the US Navy in January 2009, with its missions being assumed by other platforms such as the P-3C Orion, Sikorsky SH-60 Seahawk, and Boeing F/A-18E/F Super Hornet. Several examples continue to be flown by Air Test and Evaluation Squadron Thirty (VX-30) at Naval Base Ventura County / NAS Point Mugu, California for range clearance and surveillance operations on the NAVAIR Point Mugu Range, and a single example is operated by the National Aeronautics and Space Administration (NASA) at the NASA Glenn Research Center.
Development.
In the mid-1960s, the U.S. Navy developed the VSX (Heavier-than-air, Anti-submarine, Experimental) requirement for a replacement for the piston-engined Grumman S-2 Tracker as an anti-submarine aircraft to fly off the Navy's aircraft carriers. In August 1968, a team led by Lockheed and a Convair/Grumman team were asked to further develop their proposals to meet this requirement. Lockheed recognised that it had little recent experience in designing carrier based aircraft, so Ling-Temco-Vought (LTV) was brought into the team, being responsible for the folding wings and tail, the engine nacelles, and the landing gear, which was derived from LTV A-7 Corsair II (nose) and Vought F-8 Crusader (main). Sperry Univac Federal Systems was assigned the task of developing the aircraft's onboard computers which integrated input from sensors and sonobuoys.
On 4 August 1969, Lockheed's design was selected as the winner of the contest, and eight prototypes, designated YS-3A were ordered. The first prototype flew on 21 January 1972 and the S-3 entered service in 1974. During the production run from 1974 to 1978, a total of 186 S-3As were built. The majority of the surviving S-3As were later upgraded to the S-3B variant, with sixteen aircraft converted into ES-3A Shadow electronic intelligence (ELINT) collection aircraft.
ES-3A Shadow.
The ES-3A Shadow was designed as a carrier-based, subsonic, all-weather, long-range, electronic reconnaissance (ELINT) aircraft. All 16 aircraft were modified S-3 Viking airframes, which were modified with numerous additional antennas and antenna housings. The Shadow replaced the EA-3B Skywarrior, and entered fleet service in 1993.
The ES-3A carried an extensive suite of electronic sensors and communications gear, replacing the S-3’s submarine detection, armament, and maritime surveillance equipment with avionics racks accommodating the ES-3A’s sensors. These modifications had minor impact on airspeed, reducing its top rated speed from 450 KTAS to 405 KTAS but had no noticeable impact on the aircraft's range and actually increased its rated loiter time. Because these aircraft were standoff indications and warnings platforms and were never intended to be part of an ingress strike package, this new speed limitation was considered insignificant.
Design.
The S-3 is a conventional monoplane with a high-mounted cantilever wing, swept at an angle of 15°. The two GE TF-34 high-bypass turbofan engines mounted in nacelles under the wings provide excellent fuel efficiency, giving the Viking the required long range and endurance, while maintaining docile engine-out characteristics.
The aircraft can seat four crew members, three officers and one enlisted aircrewman, with the pilot and the copilot/tactical coordinator (COTAC) in the front of the cockpit and the tactical coordinator (TACCO) and sensor operator (SENSO) in the back. Entry is by an entry door / ladder which folds out of the side of the fuselage. When the aircraft's anti-submarine warfare (ASW) role ended in the late 1990s, the enlisted SENSOs were removed from the crew. In the tanking crew configuration, the S-3B typically flew with only a crew of two (pilot and co-pilot/COTAC). The wing is fitted with leading edge and Fowler flaps. Spoilers are fitted to both the upper and the lower surfaces of the wings. All control surfaces are actuated by dual hydraulically boosted irreversible systems. In the event of dual hydraulic failures, an Emergency Flight Control System (EFCS) permits manual control with greatly increased stick forces and reduced control authority.
Unlike many tactical jets which required ground service equipment, the S-3 was equipped with an auxiliary power unit (APU) and capable of unassisted starts. The aircraft's original APU could provide only minimal electric power and pressurized air for both aircraft cooling and for the engines' pneumatic starters. A newer, more powerful APU could provide full electrical service to the aircraft. The APU itself was started from a hydraulic accumulator by pulling a mechanical handle in the cockpit. The APU accumulator was fed from the primary hydraulic system, but could also be pumped up manually (with much effort) from the cockpit.
All crew members sit on forward-facing, upward-firing Douglas Escapac zero-zero ejection seats. In "group eject" mode, initiating ejection from either front seat ejects the entire crew in sequence, with the back seats ejecting 0.5 seconds before the front in order to provide safe separation. The rear seats are capable of self ejection, and the ejection sequence includes a pyrotechnic charge that stows the rear keyboard trays out of the occupants' way immediately before ejection. Safe ejection requires the seats to be weighted in pairs, and when flying with a single crewman in the back the unoccupied seat is fitted with ballast blocks.
At the time it entered the fleet, the S-3 introduced an unprecedented level of systems integration. Previous ASW aircraft like the Lockheed P-3 Orion and S-3's predecessor, the Grumman S-2 Tracker, featured separate instrumentation and controls for each sensor system. Sensor operators often monitored paper traces, using mechanical calipers to make precise measurements and annotating data by writing on the scrolling paper. Beginning with the S-3, all sensor systems were integrated through a single General Purpose Digital Computer (GPDC). Each crew station had its own display, the co-pilot/COTAC, TACCO and SENSO displays were Multi-Purpose Displays (MPD) capable of displaying data from any of a number of systems. This new level of integration allowed the crew to consult with each other by examining the same data at multiple stations simultaneously, to manage workload by assigning responsibility for a given sensor from one station to another, and to easily combine clues from each sensor to classify faint targets. Because of this, the four-man S-3 was considered roughly equivalent in capability to the much larger P-3 with a crew of 12.
The aircraft has two underwing hardpoints that can be used to carry fuel tanks, general purpose and cluster bombs, missiles, rockets, and storage pods. It also has four internal bomb bay stations that can be used to carry general purpose bombs, aerial torpedoes, and special stores (B57 and B61 nuclear weapons). Fifty-nine sonobuoy chutes are fitted, as well as a dedicated Search and Rescue (SAR) chute. The S-3 is fitted with the ALE-39 countermeasure system and can carry up to 90 rounds of chaff, flares, and expendable jammers (or a combination of all) in three dispensers. A retractable magnetic anomaly detector (MAD) Boom is fitted in the tail.
In the late 1990s, the S-3B's role was changed from anti-submarine warfare (ASW) to anti-surface warfare (ASuW). At that time, the MAD Boom was removed, along with several hundred pounds of submarine detection electronics. With no remaining sonobuoy processing capability, most of the sonobuoy chutes were faired over with a blanking plate.
Operational history.
On 20 February 1974, the S-3A officially became operational with the Air Antisubmarine Squadron FORTY-ONE (VS-41), the "Shamrocks," at NAS North Island, California, which served as the initial S-3 Fleet Replacement Squadron (FRS) for both the Atlantic and Pacific Fleets until a separate Atlantic Fleet FRS, VS-27, was established in the 1980s. The first operational cruise of the S-3A took place in 1975 with the VS-21 "Fighting Redtails" aboard USS "John F. Kennedy".
Starting in 1987, some S-3As were upgraded to S-3B standard with the addition of a number of new sensors, avionics, and weapons systems, including the capability to launch the AGM-84 Harpoon anti-ship missile. The S-3B could also be fitted with "buddy stores" external fuel tanks that allowed the Viking to refuel other aircraft. In July 1988, VS-30 became the first fleet squadron to receive the enhanced capability Harpoon/ISAR equipped S-3B, based at NAS Cecil Field in Jacksonville, FL. 16 S-3As were converted to ES-3A Shadows for carrier-based electronic intelligence (ELINT) duties. Six aircraft, designated US-3A, were converted for a specialized utility and limited cargo COD requirement. Plans were also made to develop the KS-3A carrier-based tanker aircraft to replace the retired KA-6D Intruder, but this program was ultimately cancelled after the conversion of just one early development S-3A.
With the collapse of the Soviet Union and the breakup of the Warsaw Pact, the Soviet-Russian submarine threat was perceived as much reduced, and the Vikings had the majority of their antisubmarine warfare equipment removed. The aircraft's mission subsequently changed to sea surface search, sea and ground attack, over-the-horizon targeting, and aircraft refueling. As a result, the S-3B after 1997 was typically crewed by one pilot and one copilot [NFO]; the additional seats in the S-3B could still support additional crew members for certain missions. To reflect these new missions the Viking squadrons were redesignated from "Air Antisubmarine Warfare Squadrons" to "Sea Control Squadrons."
Prior to the aircraft's retirement from front-line fleet use aboard US aircraft carriers, a number of upgrade programs were implemented. These include the Carrier Airborne Inertial Navigation System II (CAINS II) upgrade, which replaced older inertial navigation hardware with ring laser gyroscopes with a Honeywell EGI (Enhanced GPS Inertial Navigation System) and added digital electronic flight instruments (EFI). The Maverick Plus System (MPS) added the capability to employ the AGM-65E laser-guided or AGM-65F infrared-guided AGM-65 Maverick air-to-surface missile, and the AGM-84H/K Stand-off Land Attack Missile Expanded Response (SLAM/ER). The SLAM/ER is a GPS/inertial/infrared guided cruise missile derived from the AGM-84 Harpoon that can be controlled by the aircrew in the terminal phase of flight if an AWW-13 data link pod is carried by the aircraft.
The S-3B saw extensive service during the 1991 Gulf War, performing attack, tanker, and ELINT duties, and launching ADM-141 TALD decoys. The aircraft also participated in the Yugoslav wars in the 1990s and in Operation Enduring Freedom in 2001.
The first ES-3A was delivered in 1991, entering service after two years of testing. The Navy established two squadrons of eight ES-3A aircraft each in both the Atlantic and Pacific Fleets to provide detachments of typically two aircraft, ten officers, and 55 enlisted aircrew, maintenance and support personnel (which comprised/supported four complete aircrews) to deploying carrier air wings. The Pacific Fleet squadron, Fleet Air Reconnaissance Squadron FIVE (VQ-5), the "Sea Shadows," was originally based at the former NAS Agana, Guam but later relocated to NAS North Island in San Diego, California with the Pacific Fleet S-3 Viking squadrons when NAS Agana closed in 1995 as a result of a 1993 Base Realignment and Closure (BRAC) decision. The Atlantic Fleet squadron, the VQ-6 "Black Ravens," were originally based with all Atlantic Fleet S-3 Vikings at the former NAS Cecil Field in Jacksonville, Florida, but later moved to NAS Jacksonville, approximately 10 mi to the east, when NAS Cecil Field was closed in 1999 as a result of the same 1993 BRAC decision that closed NAS Agana.
The ES-3A operated primarily with carrier battle groups, providing organic ‘Indications and Warning’ support to the group and joint theater commanders. In addition to their warning and reconnaissance roles, and their extraordinarily stable handling characteristics and range, Shadows were a preferred recovery tanker (aircraft that provide refueling for returning aircraft). They averaged over 100 flight hours per month while deployed. Excessive utilization caused earlier than expected equipment replacement when Naval aviation funds were limited, making them an easy target for budget-driven decision makers. In 1999, both ES-3A squadrons and all 16 aircraft were decommissioned and the ES-3A inventory placed in Aerospace Maintenance and Regeneration Group (AMARG) storage at Davis-Monthan AFB, Arizona.
Though a proposed airframe known as the Common Support Aircraft was once advanced as a successor to the S-3, E-2 and C-2, this plan failed to materialize. As the surviving S-3 airframes were forced into sundown retirement, a Lockheed Martin full scale fatigue test was performed and extended the service life of the aircraft by approximately 11,000 flight-hours. This supported Navy plans to retire all Vikings from front-line Fleet service by 2009 so new strike fighter and multi-mission aircraft could be introduced to recapitalize the aging Fleet inventory, with former Viking missions assumed by other fixed-wing and rotary-wing aircraft.
Iraq War.
In March 2003, during Operation Iraqi Freedom, an S-3B Viking from Sea Control Squadron 38 (The "Red Griffins") piloted by Richard McGrath Jr. launched from USS "Constellation". The crew successfully executed a time sensitive strike and fired a laser-guided Maverick missile to neutralize a significant Iraqi naval and leadership target in the port city of Basra, Iraq.
This was one of the few times in its long and distinguished operational history that the S-3B Viking had been employed overland on an offensive combat air strike and the first time it launched a laser-guided Maverick missile in combat. The first time an S-3B was employed overland during an offensive air strike was during Operation Desert Storm when an aircraft from Squadron VS-24, from the , attacked an Iraqi Silkworm missile site.
On 1 May 2003, US President George W. Bush flew in the co-pilot seat of a VS-35 Viking from NAS North Island, California to USS "Abraham Lincoln" off the California coast. There, he delivered his "Mission Accomplished" speech announcing the end of major combat in the 2003 invasion of Iraq. During the flight, the aircraft used the customary presidential callsign of "Navy One". The aircraft that President Bush flew in was retired shortly thereafter and on 15 July 2003 was accepted as an exhibit at the National Museum of Naval Aviation at NAS Pensacola, Florida.
Between July and December 2008 the VS-22 Checkmates, the last sea control squadron, operated a detachment of four S-3Bs from the Al Asad Air Base in Al Anbar Province, 180 mi west of Baghdad. The planes were fitted with LANTIRN pods and they performed non-traditional intelligence, surveillance, and reconnaissance (NTISR). After more than 350 missions, the Checkmates returned to NAS Jacksonville, Florida on 15 December 2008, prior to disestablishing on 29 January 2009.
Retirement.
The final carrier based S-3B Squadron, VS-22 was decommissioned at NAS Jacksonville on 29 January 2009. Sea Control Wing Atlantic was decommissioned the following day on 30 January 2009, concurrent with the U.S. Navy retiring the last S-3B Viking from front-line Fleet service.
In June 2010 the first of three aircraft to patrol the Pacific Missile Test Center's range areas off of California was reactivated and delivered. The jet aircraft's higher speed, 10 hour endurance, modern radar, and a LANTIRN targeting pod allow it to quickly confirm the test range being clear of wayward ships and aircraft before tests commence. These S-3Bs are flown by Air Test and Evaluation Squadron Thirty(VX-30) based out of NAS Point Mugu, CA.
Also, the NASA Glenn Research Center acquired four S-3Bs in 2005. Since 2009, one of these aircraft (USN BuNo 160607) has also carried the civil registration N601NA and is used for various tests.
Potential future.
South Korea has expressed an interest in acquiring up to 18 secondhand, ex-USN S-3s in order to augment their current fleet of 16 Lockheed P-3 Orion aircraft. If the purchase goes through then South Korea would become the first foreign operator of the type.
In April 2014, Lockheed Martin announced that they would offer refurbished and remanufactured S-3s as a replacement for the Northrop Grumman C-2A Greyhound. The requirement for 35 aircraft would be met from the 91 S-3s currently in storage. In February 2015, the Navy announced that the Bell Boeing V-22 Osprey had been selected to replace the C-2 for the COD mission.
References.
Notes
Bibliography
</dl>

</doc>
<doc id="29476" url="http://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter, originally developed in the late 1950s as a fast utility helicopter for the United States Navy. In the 1970s, anti-submarine, anti-surface threat capabilities were added to the design, including over-the-horizon targeting, resulting in modifying most existing UH-2 models to the "SH-2 Seasprite".
This aircraft extends and increases shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles. It served with the U.S. Navy from the 1960s until the last SH-2G helicopters were retired in 2001.
Design and development.
Origins.
In 1956, the U.S. Navy launched a competition to meet its requirement for a compact, all-weather multipurpose naval helicopter. Kaman's K-20 model was selected as the winner. Kaman was awarded a contract for four prototype and 12 production "HU2K-1" helicopters in late 1957. Kaman's design was for a conventional helicopter powered by a single General Electric T58-8F turboshaft engine, driving a 44-foot four-bladed main rotor and a four-bladed tail rotor.
In 1960, the Royal Canadian Navy announced that the HU2K was the frontrunner for a large anti-submarine warfare contract; the Canadian Treasury Board had approved an initial procurement of 12 units for $14.5 million. Abruptly, Kaman raised the estimated price to $23 million, and there was concern that the manufacturer's weight and performance projections were overly optimistic. The Naval Board decided to wait until after the US Navy had conducted sea trials before approving the purchase. These trials revealed the HU2K to be overweight and underpowered, and thus incapable of meeting Canadian requirements. Hence, in late 1961, the Sikorsky Sea King was selected.
With no follow-on orders, Kaman ended production in the late 1960s after delivering 184 SH-2s to the U.S. Navy; production was later restarted in 1971 to manufacture an improved variant of the helicopter, the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was newer and more capable in anti-submarine operations, was too large to be operated from the small flight decks of older frigates.
Further development.
Upon enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 was redesignated "UH-2A" and the "HU2K-1U" was redesignated "UH-2B". In service, the UH-2 Seasprite would see several modifications and improvements, such as the addition of fixtures for mounting external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured, their single engines being replaced by a twin-engine arrangement.
The UH-2 was selected to be the airframe for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter in October 1970. LAMPS evolved in the late 1960s from an urgent requirement to develop a manned helicopter that would support a non-aviation ship and serve as its tactical Anti-Submarine Warfare arm. Known as LAMPS Mark I, the advanced sensors, processors, and display capabilities aboard the helicopter enabled ships to extend their situational awareness beyond the line-of-sight limitations that hamper shipboard radars and the short distances for acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. H-2s reconfigured for the LAMPS mission were redesignated "SH-2D". On 16 March 1971, the first SH-2D LAMPS prototype first flew.
The full LAMPS I system was equipped on the "SH-2F". The SH-2F was delivered to the Navy beginning in 1973. This variant had upgraded engines, longer life rotor, and higher take-off weight. In 1981, the Navy ordered 60 production SH-2Fs. Beginning in 1987, 16 SH-2Fs were upgraded with chin mounted Forward Looking Infrared Sensors (FLIR), Chaff (AIRBOC)/Flares, dual rear mounted IR scramblers, and Missile/Mine detecting equipment.
Eventually all but two H-2s then in Navy inventory were remanufactured into SH-2Fs. The final production procurement of the SH-2F was in Fiscal Year 1986. The last six orders for production SH-2Fs were switched to the SH-2G Super Seasprite variant.
Operational history.
United States.
The UH-2 began entering operational service in 1962. The Navy soon found the helicopter's capabilities to be restricted by its single engine, and ordered Kaman to retrofit all of its Seasprites with a twin-engine arrangement instead; with two engines the Seasprite was capable of reaching an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The Navy would operate a total fleet of nearly 200 Seasprites for various duties, such as anti-submarine warfare (ASW), search and rescue (SAR) and transportation. Typically, several UH-2s would be deployed upon an aircraft carrier to perform plane guard and SAR missions.
The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964; the Seasprite's principal contribution to what would become the Vietnam War was the retrieval of downed aircrews, both from the sea and from inside enemy territory, and was increasingly relied upon in this mission as the war intensified, such as during Operation Rolling Thunder in 1965. In October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were able to recover 103 men.
In the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the US Navy with its first ASW helicopter capable of operating from vessels other than its aircraft carriers. The small size of the SH-2 allowed it to be operated from flight decks that were too small for most helicopters, this being a factor in the Navy's decision to acquire the improved SH-2F in the early 1980s.
SH-2Fs were utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment on the SH-2F allowed it to conduct combat support and surface warfare missions in these hostile environments, which had an often-minimal threat from submarines. The SH-2F was retired from active service in October 1993, at roughly the same time that the Navy retired the last of its Vietnam-era Knox Class Frigates that were unable to accommodate the larger SH-60 Sea Hawk.
In 1991, the US Navy began to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.
New Zealand.
The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with four interim SH-2F Seasprites (ex-US Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G Super Seasprites were delivered. The Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai in October 2005. RNZN Seasprites have seen service in East Timor. Six additional SH-2Fs rejected by the Royal Australian Navy were purchased and are now stationed at the RNZAF Ground Training Wing (GTW) at Woodbourne near Blenheim as training helicopters. An SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.
Exports.
In the late 1990s the United States offered surplus U.S. Navy SH-2Fs as foreign aid to a number of countries, Greece which had been offered six and Turkey which had been offered 14 rejected the offer. Egypt acquired four SH-2F aircraft under the aid program mainly for spares to support a fleet of ten SH-2Gs. Poland acquired the later SH-2G.
Specifications.
UH-2A.
"Data from" "Carrier Aviation Air Power Directory"General characteristics
Performance
SH-2F.
"Data from" "The Encyclopedia of World Military Aircraft"General characteristics* Crew: 3 (Pilot, Co-pilot/Tactical Coordinator (TACCO), Sensor Operator (SENSO))
Performance
</ul>Armament
 </ul>
References.
</dl>

</doc>
<doc id="29480" url="http://en.wikipedia.org/wiki?curid=29480" title="Stop consonant">
Stop consonant

 
In phonetics, a stop, also known as a plosive, is an oral occlusive, a consonant in which the vocal tract is blocked so that all airflow ceases. The occlusion may be made with the tongue blade ([t], [d]) or body ([k], [ɡ]), lips ([p], [b]), or glottis ([ʔ]). Stops contrast with nasals, where the vocal tract is blocked but airflow continues through the nose, as in /m/ and /n/, and with fricatives, where partial occlusion impedes but does not block airflow in the vocal tract.
Terminology.
The terms "stop, occlusive," and "plosive" are often used interchangeably. Linguists who distinguish them may not agree on the distinction being made. The terms refer to different features of the consonant. "Stop" refers to the airflow that is stopped. "Occlusive" refers to the articulation, which occludes (blocks) the vocal tract. "Plosive" refers to the release burst (plosion) of the consonant.
Either "occlusive" or "stop" may be used as a general term covering the other together with nasals. That is, 'occlusive' may be defined as oral occlusives (stops/plosives) plus nasal occlusives (nasals such as [m], [n]), or 'stop' may be defined as oral stops (plosives) plus nasal stops (nasals). Ladefoged and Maddieson (1996) prefer to restrict 'stop' to oral occlusives. They say,
In addition, they use "plosive" for a pulmonic stop; "stops" in their usage include ejective and implosive consonants. 
If a term such as 'plosive' is used for oral obstruents, and nasals are not called nasal stops, then a "stop" may mean the glottal stop; 'plosive' may even mean non-glottal stop. In other cases, however, it may be the word 'plosive' that is restricted to the glottal stop. Note that, generally speaking, stops do not have plosion (a release burst). In English, for example, there are stops with no audible release, such as the /p/ in "apt". However, pulmonic stops do have plosion in other environments.
In Ancient Greek, the term for stop was ἄφωνον ("áphōnon"), which means "unpronounceable", "voiceless", or "silent", because stops could not be pronounced without a vowel. This term was calqued into Latin as "mūta", and from there borrowed into English as "mute". "Mute" was sometimes used instead for voiceless consonants, whether stops or fricatives, a usage that was later replaced with "surd", from Latin "surdus" "deaf" or "silent", a term still occasionally seen in the literature. For more information on the Ancient Greek terms, see .
Common stops.
All languages in the world have stops, and most have at least the voiceless stops [p], [t], and [k]. However, there are exceptions: Colloquial Samoan lacks the coronal [t], and several North American languages, such as the northern Iroquoian and southern Iroquoian languages (i.e., Cherokee), lack the labial [p]. In fact, the labial is the least stable of the voiceless stops in the languages of the world, as the unconditioned sound change [p] → [f] (→ [h] → Ø) is quite common in unrelated languages, having occurred in the history of Classical Japanese, Classical Arabic, and Proto-Celtic, for instance. Formal Samoan has only one word with velar [k]; colloquial Samoan conflates /t/ and /k/ to /k/. Ni‘ihau Hawaiian has [t] for /k/ to a greater extent than Standard Hawaiian, but neither distinguish a /k/ from a /t/. It may be more accurate to say that Hawaiian and colloquial Samoan do not distinguish velar and coronal stops than to say they lack one or the other.
See Common occlusives for the distribution of both stops and nasals.
Articulation.
In the articulation of the stop, three phases can be distinguished:
In many languages, such as Malay and Vietnamese, word-final stops lack a release burst, even when followed by a vowel, or have a nasal release. See no audible release.
Nasal occlusives are somewhat similar. In the catch and hold, airflow continues through the nose; in the release, there is no burst, and final nasals are typically unreleased across most languages. 
In affricates, the catch and hold are those of a stop, but the release is that of a fricative. That is, affricates are stop–fricative contours.
Classification.
Voice.
Voiced stops are pronounced with vibration of the vocal cords, voiceless stops without. Stops are commonly voiceless, and many languages, such as Mandarin Chinese and Hawaiian, have only voiceless stops. Others, such as most Australian languages, are indeterminate: stops may vary between voiced and voiceless without distinction.
Aspiration.
In aspirated stops, the vocal cords (vocal folds) are abducted at the time of release. In a prevocalic aspirated stop (a stop followed by a vowel or sonorant), the time when the vocal cords begin to vibrate will be delayed until the vocal folds come together enough for voicing to begin, and will usually start with breathy voicing. The duration between the release of the stop and the voice onset is called the "voice onset time" (VOT) or the "aspiration interval". Highly aspirated stops have a long period of aspiration, so that there is a long period of voiceless airflow (a phonetic [h]) before the onset of the vowel. In tenuis stops, the vocal cords come together for voicing immediately following the release, and there is little or no aspiration (a voice onset time close to zero). In English, there may be a brief segment of breathy voice that identifies the stop as voiceless and not voiced. In voiced stops, the vocal folds are set for voice before the release, and often vibrate during the entire hold, and in English, the voicing after release is not breathy. A stop is called "fully voiced" if it is voiced during the entire occlusion. In English, however, initial voiced stops like /#b/ or /#d/ may have no voicing during the period of occlusion, or the voicing may start shortly before the release and continue after release, though word-final stops tend to be fully voiced: In most dialects of English, the final "g" in the "bag" is likely to be fully voiced, whereas the initial "b" will only be voiced during part of its occlusion. Initial voiceless stops, like the "p" in "pie", are aspirated, with a palpable puff of air upon release, whereas a stop after an "s", as in "spy", is tenuis (unaspirated). When spoken near a candle flame, the flame will flicker more after the words "par, tar," and "car" are articulated, compared with "spar, star," and "scar". In the common pronunciation of "papa", the initial "p" is aspirated whereas the medial "p" is not.
Length.
In a geminate or long consonant, the occlusion lasts longer than in simple consonants. In languages where stops are only distinguished by length (e.g., Arabic, Ilwana, Icelandic), the long stops may be held up to three times as long as the short stops. Italian is well known for its geminate stops, as the double "t" in the name "Vittoria" takes just as long to say as the "ct" does in English "Victoria". Japanese also prominently features geminate consonants, such as in the minimal pair 来た "kita" 'came' and 切った "kitta" 'cut'.
Note that there are many languages where the features voice, aspiration, and length reinforce each other, and in such cases it may be hard to determine which of these features predominates. In such cases, the terms fortis is sometimes used for aspiration or gemination, whereas lenis is used for single, tenuous, or voiced stops. Be aware, however, that the terms "fortis" and "lenis" are poorly defined, and their meanings vary from source to source.
Nasalization.
Simple nasals are differentiated from stops only by a lowered velum that allows the air to escape through the nose during the occlusion. Nasals are acoustically sonorants, as they have a non-turbulent airflow and are nearly always voiced, but they are articulatorily obstruents, as there is complete blockage of the oral cavity. The term occlusive may be used as a cover term for both nasals and stops.
A prenasalized stop starts out with a lowered velum that raises during the occlusion. The closest examples in English are consonant clusters such as the [nd] in "candy", but many languages have prenasalized stops that function phonologically as single consonants. Swahili is well known for having words beginning with prenasalized stops, as in "ndege" 'bird', and in many languages of the South Pacific, such as Fijian, these are even spelled with single letters: "b" [mb], "d" [nd].
A postnasalized stop begins with a raised velum that lowers during the occlusion. This causes an audible nasal "release", as in English "sudden". This could also be compared to the /dn/ cluster found in Russian and other Slavic languages, which can be seen in the name of the Dnieper River.
Note that the terms "prenasalization" and "postnasalization" are normally used only in languages where these sounds are phonemic: that is, not analyzed into sequences of stop plus nasal.
Airstream mechanism.
Stops may be made with more than one airstream mechanism. The normal mechanism is pulmonic egressive, that is, with air flowing outward from the lungs. All languages have pulmonic stops. Some languages have stops made with other mechanisms as well: ejective stops (glottalic egressive), implosive stops (glottalic ingressive), or click consonants (lingual ingressive).
Tenseness.
A fortis stop (in the narrow sense) is produced with more muscular tension than a lenis stop (in the narrow sense). However, this is difficult to measure, and there is usually debate over the actual mechanism of alleged fortis or lenis consonants. 
There are a series of stops in the Korean language, sometimes written with the IPA symbol for ejectives, which are produced using "stiff voice", meaning there is increased contraction of the glottis than for normal production of voiceless stops. The indirect evidence for stiff voice is in the following vowels, which have a higher fundamental frequency than those following other stops. The higher frequency is explained as a result of the glottis being tense. Other such phonation types include breathy voice, or murmur; slack voice; and creaky voice.
Transcription.
The following stops have been given dedicated symbols in the IPA. 
Variations.
Many subclassifications of stops are transcribed by adding a diacritic or modifier letter to the IPA symbols above.

</doc>
<doc id="29482" url="http://en.wikipedia.org/wiki?curid=29482" title="Stayman convention">
Stayman convention

Stayman is a bidding convention in the card game contract bridge. It is used by a partnership to find a 4-4 or 5-3 trump fit in a major suit after making a one notrump (1NT) opening bid and it has been adapted for use after a 2NT opening, a 1NT overcall, and many other natural notrump bids.
The convention is named for Sam Stayman, who wrote the first published description in 1945, but its inventors were two other players: the British expert Jack Marx in 1939, who published it only in 1946, and Stayman's regular partner George Rapée in 1944.
Rationale.
A game contract bid and made in a major suit (i.e. 4♥ or 4 ♠) scores better than a game contract bid and made in a minor suit (i.e. 5♣ or 5 ♦) or in notrump (i.e. 3NT). Also, the success rate for a game contract in a major suit when a partnership has a combined holding of 26 points and eight cards in the major is about 80%, whereas a game contract in 3NT with 26 high card points (HCP) has a success rate of only 60%, or 50% with 25 HCP; the success rate for a minor suit game contract when holding 26 points is about 30%.
Accordingly, partnership priority is to find an eight card or better major suit fit when jointly holding sufficient values for a game contract. 5-3 and 6-2 fits are easy to find in basic methods as responder can bid 3♥ or 3♠ over 1NT, and opener will not normally have a 5 card major to bid 1NT. However, finding 4-4 fits presents a problem. The 2♥ and 2♠ bids cannot be used for this as they are weak takeouts, a sign-off bid.
Standard Stayman.
After an opening bid or an overcall of 1NT (2NT), responder or advancer bids an artificial 2♣ (3♣) to ask opener or overcaller if he holds a four or five card major suit; some partnership agreements may require the major to be headed by an honor of at least a specified rank, such as the queen. The artificial club bid typically promises four cards in at least one of the major suits (promissory Stayman) and, "in standard form", enough strength to continue bidding after partner's response (8 HCP for an invitational bid opposite a standard strong 1NT opening or overcall showing 15-17 HCP, 11 HCP opposite a weak notrump of 12-14 HCP, or 5 HCP to go to game opposite a standard 2NT showing 20-21 points). It also promises distribution that is not 4333. By invoking the Stayman convention, the responder takes control of the bidding since strength and distribution of the opener's hand is already known within a limited range. The opener responds with the following rebids.
A notrump opener should have neither a suit longer than five cards nor more than one 5-card suit since an opening notrump bid shows a balanced hand. A notrump bidder who has at least four cards in each major suit normally responds in hearts, as this still can still allow a spade fit to be found. Variant methods are to bid the longer or stronger major, with a preference given to spades, or to use 2NT to show both majors.
In the standard form of Stayman over 1NT, the responder has a number of options depending on his partner's answer:
Over these bids, the notrump bidder (1) with a maximum hand (17 HCP), goes to game over an invitational bid and (2) with four (or more) cards in each major suit, corrects to the previously unbid major suit.
In the standard form of Stayman over 2NT, the responder has only two normal rebids.
In either case, a responder who rebids notrump over a response in a major suit promises four cards of the other major suit. Thus, a notrump opener who holds at least four cards in each major suit should "correct" by bidding the other major suit at the lowest level.
Of course, once a fit is found, responder who has sufficient strength also may bid 4♣ (Gerber) or 4NT (Blackwood), or cue bid aces, depending upon partnership agreement, to explore slam in any of the above sequences. Some partnerships also admit responder's rebids of a major suit that the notrump bidder did not name.
A bid of 4♣ over an opening bid of 3NT may be either Stayman or Gerber, depending upon the partnership agreement.
If an adverse suit bid is inserted immediately after a 1NT opening, Stayman may be employed via a double (by partnership agreement) or a cue bid, depending on the strength of his hand. The cue bid, which is conventional, is completely artificial and means nothing other than invoking Stayman. For example, if South opens 1NT, and West overcalls 2♦, North, if he has adequate values, may call 3♦, invoking Stayman. South would then show his major or bid game in notrump. Alternatively, North, if his hand lacks the values for game in notrump, may double, which by partnership agreement employs Stayman. This keeps the Stayman bidding at second level.
Partnerships who have not yet learned Stayman but choose to adopt Stayman (without having yet learned or having chosen not to use Jacoby Transfers) will need to adjust their use of normal level 2 responses after a 1NT opening, because the availability of this convention changes the nature of what had been normal 1 NT responses. When the notrump bidder's partner does not invoke Stayman but instead calls 2♥ or 2♠, it is a sign of relative weakness (since if responder held 8 HCP or more, he would have invoked Stayman). These bids are commonly referred to as "drop dead bids", as the opening notrump bidder is requested to withdraw from the auction. If opener has maximum values, a fit, and strong support, he may raise to the 3-level, but under no circumstances may he take any other action. This provides the partnership with an advantage that the non-Stayman partnership doesn't enjoy. For example, a responder may have no honors at all; that is, a total of zero HCP. His partner is likely to be set if he passes. A non-Stayman responder would have to pass, because to bid would provoke a rebid. But a Stayman responder can respond to his partner's 1NT opening at level 2 if he has a 6-card non-club suit. The responder with 3 HCP and a singleton can make a similar call with a 5-card non-club suit. This gives the partnership a better than even chance of success in making the contract, whereas without a response (and without Stayman), the contract would likely be set.
Similarly, a response of 2 Diamonds indicates less than 8 HCP and should usually be passed. In rare cases, when the opener has maximum values and a fit in Diamonds with at least two of the top three honors, he may raise Diamonds, and responder may see a chance for game in notrump.
There are many variations on this basic theme, and partnership agreement may alter the details of its use. It is one of the most widely used conventions in bridge.
Non promissory Stayman and 2♠ checkback by responder.
Some partnerships play that 2♣ Stayman does not absolutely promise a four-card major (non promissory Stayman). For example, if responder has a short suit and wishes to know if opener has four-card cover in it, so as to play in notrumps. If opener shows hearts initially, 2♠ can be used to find a fit in spades when the 2♣ does not promise a four-card major.
1NT - 2♣, 2♥ -
Alternatively 2♠ can be used for all hands with four spades and not four hearts, either invitational or game values, while 3NT denies four spades.
Using Jacoby transfers with Stayman.
Today, most players use Stayman in conjunction with Jacoby transfers. With Stayman in effect, the responder practically denies having a five-card major, as otherwise he would transfer to the major immediately. The only exception is when responder has 5-4 in the majors; in that case, he could use Stayman, and in the case of a 2♦ response, bid the five-card major at the two level (weakness take-out / Garbage Stayman) or at the three level (forcing to game). However, the latter hand can also be bid by first using a transfer and then showing the second suit naturally. The Smolen convention provides an alternative method to show a five-card major and game-going values. A minor drawback of Jacoby transfers is that a 2♦ contract is not possible.
Smolen convention.
The Smolen convention is an adjunct to Stayman for situations in which the notrump opener has denied holding a four-card major and responder has a five-card major and a four-card major with game-going values.
If the notrump opener responds to the Stayman 2♣ asking bid with 2♦, denying a four-card major, responder initiates the Smolen Transfer with a jump shift to three of his four-card major. The jump shift shows which is the four-card major and promises five in the other major. The notrump opener then bids four of the other major with three cards in the suit or 3NT with fewer than three.
Smolen may also be used when responder has a six-card major and a four-card major with game-going values; after the 2♦ negative response by opener, responder double jump shifts to four in the suit just below his six-card major and the notrump opener transfers to four of his partner's six-card major.
This convention allows a partnership to find either a 5-3 fit, 6-3 and 6-2 fit while ensuring that the notrump opener, who has the stronger hand, will be declarer.
Garbage Stayman and Crawling Stayman.
"Garbage" Stayman (or "Weak Stayman") and "Crawling" Stayman are adaptations of Stayman frequently used for damage control when holding a weak hand opposite a 1NT opening bid. Suppose you hold the following hand.
Your partner opens 1NT (15-17), and your right hand opponent passes. Now, what?
In this scenario, opener has about 16 HCP and the opponents have about 24 HCP. Thus, 1NT is virtually certain to go down by at least three or four tricks. Indeed, in Notrump, this dummy will be completely worthless.
But consider what happens if you bid 2♣ Stayman rather than passing on the first round, and then "pass opener's response". If opener rebids a major suit you have found a 4-4 fit and ability to trump club losers (or, alternately, to sluff the other major on club winners and then to trump losers in the other major). Likewise, a response of 2♦ guarantees no worse than a 5-2 fit in diamonds and, with a fifth trump, a potential additional ruff. The ability to reach dummy with a couple ruffs also may allow the declarer to take a couple finesses or execute a squeeze that otherwise would not be possible, and which might yield another trick or two. The result is a contract that will go down fewer tricks or that might even make, especially with a somewhat better hand than the example, rather than a contract that is virtually certain to go down at least three or four tricks. The practice of bidding Stayman with a relatively weak hand of this (or similar) shape and then passing the Notrump bidder's reply is often called "Garbage Stayman" because it is bidding Stayman with a "garbage" hand.
"Crawling Stayman" is an optional extension of "Garbage Stayman" for situations in which the responder's diamond suit is short. In "Crawling Stayman", the responder rebids 2♥ over the Notrump bidder's 2♦ reply. This conventional bid shows a weak hand with at least four cards in each major suit, asking the Notrump bidder to choose between the major suits at the cheapest level by either passing the 2♥ bid or correcting to 2♠. The name "Crawling Stayman" comes from the fact that the bidding "crawls" at the slowest possible pace: (pass) – 1NT – (pass) – 2♣; (pass) – 2♦ – (pass) – 2♥; (pass) – 2♠; (pass) – pass – (pass).
Alternatively, responder's 2♥and 2♠ bids after the 2♦ rebid can be weak sign-offs. This allows responder to effectively bid hands which are 5-4 in the majors, by looking first for a 4-4 fit and, if none is found, signing off in his 5 card suit.
"Garbage Stayman" and "Crawling Stayman" bids over a 2NT bid work the same way, but occur at the "three" level.
Forcing and Non-Forcing Stayman.
If Jacoby transfers are not played, there are two approaches to resolve the situation when responder has a 5-card major but only invitational values. In one, more common, referred to as "non-forcing Stayman", in the sequence:
responder's simple rebid of a major suit is only invitational, showing 8-9 points and a 5-card spade suit. In the "forcing Stayman" variant, the bid is one-round forcing.
In the original Precision Club system, forcing and non-forcing Stayman are differentiated in the start: 2♣ by responder shows only invitational values (and the continuation is the same as in basic Stayman), while 2♦ is forcing to game (responder bids 2NT without majors).
Non Promissory Game Forcing Stayman.
This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2♦, 2♥, 2♠, 2NT, 3♣ allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. Disadvantage is 2♣ can't be used as a damage control bid.
 1NT – 2♣
Developed to be used in combination with following other responses to 1NT: 2♦, 2♥ Jacoby transfers to majors; 2♠ range finder/transfer to minors (opener's rebids: 2NT 12-13 HCP, 3♣ 14 HCP. Responder passes or corrects to 3♣ or 3♦ sign off if weak. After opener's 3♣ rebid responder bids 3♥ to show 4 hearts or 3♠ to show 4 spades both game forcing. Responder's rebid of 3NT denies 4 card major); 2NT invitational hand with both 4 card majors (opener's rebids: no bid no 4 card major 12-13 HCP, 3♣ 4 hearts 12-13 HCP, 3♦ 4 spades 12-13 HCP, 3♥ 4 hearts 14 HCP, 3♠ 4 spades 14 HCP, 3NT 14 HCP no 4 card major)
Non Promissory Relay Stayman.
This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2♦, 2♥, 2♠ allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. May be also used as a damage control bid.
1NT – 2♣
1NT – 3♣ weak sign off.
Opener's rebids of 2♦, 2♥, 2♠ may all be passed if responder is weak.
Developed to be used in combination with following other responses to 1NT: 2♦, 2♥ Jacoby transfers to majors; 2♠ 5 spades 4 hearts 10-11 HCP; 2NT invitational hand with 5,5 minors 10-11 HCP.
Five Card Major Stayman.
This allows responder to check for 5-3 major fits where it is possible that opener's 1NT or 2NT might include a five card major. As described by Australian Ron Klinger, it can be played with a weak or strong 1NT.
1NT - 2♣
1NT - 2♣, 2♦ OR 2NT
After a transfer, accept it with any 4333, bid 3NT with only two trumps, otherwise bid 4M.
1NT - 2♣, 2♦ OR 2NT - 3♣ = Stayman
1NT - 2♣, (2♦ OR 2NT) - 3♣, 3♦
An alternative, simpler version of 5 card Stayman is:
1NT - 2♣
This structure permits use by weak hands with 5+ diamonds and 2+ cards in each major.
After 1NT - 2♣, 2♦
If responder has a five-card major, he begins with a transfer. After completion of the transfer, bidding the other major at the three level shows four cards in it and a game forcing hand, in line with the 1NT - 2♣, 2♦ structure above (1NT - 2♦, 2♥ - 2♠ = invitational 5♥-4♠).
Similarly after 2NT - 3♣, 3♦
A drawback of Five Card Major Stayman (particularly the simpler version) is that the weaker hand may become declarer in a 4-4 major fit.
Puppet Stayman.
Puppet Stayman is similar to Five Card Stayman. It is more complex but has the major advantage that the strong hand virtually always becomes declarer.
Initially developed by Neil Silverman and refined by Kit Woolsey and Steve Robinson in 1977-78, is a variation of the Stayman convention designed to find a 5-3 fit in a major, augmenting the search for a 4-4 major fit by standard Stayman. In 1977, Woolsey wrote that Puppet Stayman has several advantages over standard Stayman:
Responder's rebids.
As in standard Stayman, Puppet Stayman begins with a 2♣ response to a 1NT opening and is at least game invitational; this asks opener to bid a 5-card major if he has one and otherwise to bid 2♦. Over a 2♦ response, rebids by responder are intended to dislose his distributional features in the majors as well as his strength. The original 1977 and 1978 revised rebids described by Woolsey are tabulated below: 
Opener and responder continue the bidding having a clearer understanding of each other's distributional features and are better positioned to select the ultimate denomination and level of the contract.
Modern applications.
Many variations to the Puppet Stayman bidding structure have been devised since Woolsey's 1978 summary; partnership review and agreement on the preferred modern treatment is required.
Some no longer advocate use of Puppet Stayman over a 1NT opening preferring to use the concept exclusively over a 2NT opening and reserving other Stayman variations and conventions such Jacoby Transfers and Smolen Transfers in search of major-suit fits after a 1NT opening.
Responses to a 2NT opening or rebid.
Puppet Stayman is more commonly used after a 2NT opening than after a 1NT opening. Responses to a 2NT opening or very strong 2NT rebid (20-22 or 23-24):
Responder bids 3♣ seeking information about opener's major suit holding. Opener replies:
By this means all 5-3 and 4-4 major suit fits can be found.
An alternative pattern frees up the 2NT-3♠ sequence as a slam try in the minors. To allow 3-5 spade fits to be found when responder holds 5 spades and 4 hearts, some of the responses change:
Checkback Stayman.
2♣ Checkback Stayman (or simply Checkback) is used after a 1NT rebid by opener rather than a 1NT opening. It is used to "check back" if opener has major suit support, saying nothing additional about the club suit. It can find 3-5 fits, 4-4 fits (in Standard American) and 5-3 fits (in Acol), and also shows whether opener was maximum or minimum strength for his notrump bid. In five-card major systems, bidding Checkback implies that the responder has five cards in his major, and may have four in the other.
1m – 1M; 1NT – 2♣
The 2♣ is "Checkback Stayman". Responses by opener shows the following:
Partnership agreement is required on how to handle the case of holding four of the other major and three of partner's suit. One could agree to bid up the line, or support partner's suit first. If partner cannot support your first suit, he will invite with 2NT or bid game with 3NT and you will then correct to your other suit.
In Acol, if the opening bid was a major, opener can rebid his major after a Checkback inquiry to show that it has five cards rather than four and find 5-3 fits. Moreover, 1M – 2m; 2NT – 3♣ can also be used as Checkback Stayman. It is useful also to include an indication of range, particularly if opener's 2NT rebid is forcing to game and shows a wide points range (15-19). This is achieved by using 3♦ for minimum hands and 3♥/3♠/3NT for maximum hands, or vice versa. After 3♦, responder can still bid 3♥/3♠ to look for a 5-3 fit.
New Minor Forcing is an alternative to Checkback Stayman where either 2♣ or 2♦ can be used as the checkback bid. It can be used by responder with invitational values or better to find three-card support for his major or to find a 4-4 heart fit if holding five spades and four hearts); it also allows a return to the minor to play.

</doc>
<doc id="29483" url="http://en.wikipedia.org/wiki?curid=29483" title="Saks Fifth Avenue">
Saks Fifth Avenue

Saks Fifth Avenue is an American department store chain owned by multinational corporation Hudson's Bay Company, which operates the flagship store and corporate headquarters in Midtown Manhattan, New York City. It competes with high-end specialty stores in Manhattan, notably Bergdorf Goodman, Barneys New York and Bloomingdale's. It also competes with luxury retailers; Neiman Marcus, Nordstrom, as well as corporate cousin Lord & Taylor.
History.
 
Saks Fifth Avenue is the successor of a business founded by Andrew Saks in 1867 and incorporated in New York in 1902 as Saks & Company. Saks died in 1912, and in 1923 Saks & Co. merged with Gimbel Brothers, Inc., which was owned by a cousin of Horace Saks, Bernard Gimbel, operating as a separate autonomous subsidiary. On September 15, 1924, Horace Saks and Bernard Gimbel opened Saks Fifth Avenue in New York City, with a full-block avenue frontage south of St Patrick's Cathedral, facing Rockefeller Center. The architects were Starrett & van Vleck, who developed a reticent, genteel Anglophile classicizing facade similar to their Gimbels Department Store, Pittsburgh (1914).
When Bernard's brother, Adam Gimbel, became President of Saks Fifth Avenue in 1926 after Bernard's sudden passing, the company assumed national aspirations, opening its branch that year in Palm Beach, Florida, as a seasonal resort store, followed by a second resort store in Southampton, New York, in 1928. The first full-line year-round Saks store opened in Chicago, in 1929, followed by another resort store in Miami Beach, Florida. In 1938, Saks expanded to the West Coast, opening in Beverly Hills, California. By the end of the 1930s, Saks Fifth Avenue had a total of 10 stores, including resort locations such as Sun Valley, Idaho, Mount Stowe, and Newport, Rhode Island. More full-line stores followed with Detroit, Michigan, in 1940 and Pittsburgh, Pennsylvania, in 1949. In Downtown Pittsburgh, the company moved to its own freestanding location approximately one block from its former home on the fourth floor in the downtown Gimbel's flagship. The San Francisco location opened in 1952. More expansion followed from the 1960s through the 1990s including the Midwest, and the South, particularly in Texas.
BATUS Inc. acquired Gimbel Bros., Inc. and its Saks Fifth Avenue subsidiary in 1973 as part of its diversification strategy. In 1990, BATUS sold Saks to Investcorp S.A., which after investing in the company and weathering the early 1990s recession took Saks public in 1996 as Saks Holdings, Inc. In 1998, Proffitt's, Inc. the parent company of Proffitt's and other department stores, acquired Saks Holdings Inc. Upon completing the acquisition, Proffitt's, Inc. changed its name to Saks Inc.
In 2005, vendors filed against Saks alleging unlawful chargebacks. The U.S. Securities and Exchange Commission (SEC) investigated the complaint and Saks settled with the SEC in 2007.
In August 2007, the United States Postal Service began an experimental program selling the plus zip code extension to businesses. The first company to do so was Saks Fifth Avenue which received the zip code of 10022-SHOE (7463) for the eighth floor shoe department in its flagship Fifth Avenue store. Today, the New York flagship store accounts for a significant amount of the entire chain's annual revenue.
Saks continued expanding its physical presence throughout much of the 1980s and 1990s, resulting in Saks being saddled with a number of underperforming locations. Since 2000, Saks has closed several locations, including White Plains, Garden City, and Southampton in New York, Minneapolis, San Diego, Portland, Oregon, Kansas City, and suburban Chicago locations. Since 2010 another 10 stores have closed or are scheduled to close, including Pittsburgh (despite local efforts to save it), Denver, Charleston, South Carolina, Stamford, Connecticut, Austin, Dallas Willow Bend and Orlando which will close in 2014. Additionally, the company closed its location in Tampa, operated since 1996, in April, 2013 and its last Dallas location in June 2013 to implement the "strategy of employing our resources in our most productive locations."
Despite scaling back its Saks Fifth Avenue presence, the company has expanded the number of "Saks Off Fifth" locations, a concept first introduced in 2000. In states where it continues to operate, the number of stores has remained constant or risen through the addition of Saks Off Fifth sites.
On July 29, 2013, the Hudson's Bay Company (HBC), owner of the competing chain Lord & Taylor, announced it would acquire Saks Fifth Avenue's parent company for US$2.9 billion. Plans call for up to seven Saks Fifth Avenues to open in major Canadian markets, using either existing Hudson's Bay locations or new construction. Expansion into Canada is expected to challenge Canadian Holt Renfrew chain and compete with Nordstrom's expansion into Canada, which began in Summer 2014 with the opening of a Nordstrom store in Calgary. In January 2014, HBC announced the first Saks store in Canada would occupy 150000 sqft in its flagship Queen Street building in downtown Toronto. The store will open in autumn 2015 with a second Toronto area location in the Sherway Gardens shopping center opening in spring 2016. Earlier, the company announced it would convert its location in the Hudson's Bay Centre at Bloor and Yonge Streets to Saks Fifth Avenue to make it part of the upscale Yorkville shopping district.
The regular price business may be undermined by a larger number of "Saks Off Fifth" to open. Some Saks Fifth Avenue stores in the United States may convert to Lord & Taylor stores as early as January 2014.
On January 6, 2014, Marigay McKee, previously Chief Merchant at Harrods, became president of Saks Fifth Avenue. She stepped down 15 months later on April 2, 2015, and was replaced by Marc Metrick, a former executive at Saks’s parent company, Hudson’s Bay.
Controversy.
In 2014 Saks fired transgender employee Leyth Jamal after she was "belittled by coworkers, forced to use the men's room and repeatedly referred to by male pronouns (he and him)". After Jamal submitted a lawsuit for unfair dismissal, the company stated in a motion to dismiss that "it is well settled that transsexuals are not protected by Title VII." The company was removed from the Human Rights Campaign's list of "allies" as a result.
International.
The chain operated in a number of international markets including Saudi Arabia, United Arab Emirates, Bahrain, Kazakhstan, Thailand, Mexico City and Puerto Rico.

</doc>
<doc id="29484" url="http://en.wikipedia.org/wiki?curid=29484" title="Seabee">
Seabee

A Seabee is a member of the United States Naval Construction Forces (NCF). The word "Seabee" comes from initials "CB" which in turn comes from the term Construction Battalions. The Seabees have a history of building bases, bulldozing and paving thousands of miles of roadway and airstrips, and accomplishing myriad other construction projects in a wide variety of military theaters dating back to World War II.
History.
World War II.
In December 1941, with U.S. involvement in war soon expected on both the Pacific and Atlantic Oceans, Rear Admiral Ben Moreell, Chief of the Navy's Bureau of Yards and Docks, recommended establishing Naval Construction Battalions at a newly constructed base at Davisville, Rhode Island (part of North Kingstown). With the attack on Pearl Harbor and the United States' entry into the war, he was given the go-ahead. The Davisville Advanced Base Depot became operational in June 1942. It eventually contained 500 Quonset huts for personnel. On August 11, 1942, the Naval Construction Training Center, known as Camp Endicott, was commissioned at Davisville. The Camp trained over 100,000 Seabees during the Second World War. Camp Thomas, a personnel-receiving station on the base, was established in October of that year.
In California in May 1942, a base for supporting the Naval Construction Force was established at Port Hueneme in Ventura County. This base became responsible for shipping massive amounts of equipment and material to the efforts in the Pacific.
The earliest Seabees were recruited from the civilian construction trades and were placed under the leadership of the Navy's Civil Engineer Corps. Because of the emphasis on experience and skill rather than physical standards, the average age of Seabees during WWII was 37.
More than 325,000 men served with the Seabees in World War II, fighting and building on six continents and more than 300 islands. In the Pacific, where most of the construction work was needed, the Seabees landed soon after the Marines and built major airstrips, bridges, roads, gasoline storage tanks, and Quonset huts for warehouses, hospitals, and housing. They often operated under fire and frequently were forced to take part in the fighting to defend themselves and their construction projects. In the Pacific Theater they built 111 major airstrips and 441 piers, tanks for the storage of 100 m gallons of fuel, housing for 1.5 million men and hospitals for 70,000 patients.
The Seabees were officially organized in the Naval Reserve on December 31, 1947.
With the general demobilization following the war, the Naval Construction Battalions (NCBs) were reduced to 3,300 men on active duty by 1950. Between 1949 and 1953, Naval Construction Battalions were organized into two types of units: Amphibious Construction Battalions (ACBs) and Mobile Construction Battalions (MCBs), which were later designated Naval Mobile Construction Battalions (NMCBs) in the early- to mid-1960s to eliminate confusion with Marine Corps Base (MCB) in Vietnam.
Korean War.
The Korean War saw a call-up of more than 10,000 men. The expansion of the Seabees came from the Naval Reserve Seabee program where individuals volunteered for active duty. The Seabees landed at Inchon with the assault troops. They fought enormous tides as well as enemy fire and provided causeways within hours of the initial landings. Their action here and at other landings emphasized the role of the Seabees, and there was no Seabee demobilization when the truce was declared.
During the Korean War, the Navy realized they needed a naval air station in this region. Cubi Point in the Philippines was selected, and civilian contractors were initially selected for the project. After seeing the forbidding Zambales Mountains and the maze of jungle, they claimed it could not be done.
The Navy then turned to the Seabees. The first Seabees to arrive were MCB-3 on October 2, 1951; followed by MCB-5 on November 5, 1951. Over the next five years, MCB-2, -7, -9, -11 and -13 were also deployed to Cubi Point.
Seabees cut a mountain in half to make way for a nearly two-mile-long runway. Cubi Point turned out to be one of the largest earth-moving projects in the world, equivalent to the construction of the Panama Canal. The $100 million facility was commissioned on July 25, 1956, and comprised an air station and an adjacent pier that was capable of docking the Navy's largest carriers.
Following Korea, the Seabees embarked on a new mission. From providing much needed assistance in the wake of a devastating earthquake in Greece in 1953 to providing construction work and training to underdeveloped countries, the Seabees became "The Navy's Goodwill Ambassadors". Seabees built or improved many roads, orphanages and public utilities in many remote parts of the world.
Antarctica.
In 1955, Seabees began deploying yearly to the continent of Antarctica. As participants in Operation Deep Freeze, their mission was to build and expand scientific bases located on the frozen continent. The first "wintering over" party included 200 Seabees who distinguished themselves by constructing a 6,000-foot ice runway on McMurdo Sound. Despite a blizzard that once destroyed the entire project, the airstrip was completed in time for the advance party of Deep Freeze II to become the first to arrive at the South Pole by plane.
Over the following years and under adverse conditions, Seabees added to their list of accomplishments such things as snow-compacted roads, underground storage, laboratories, and living areas. One of the most notable achievements took place in 1962, when the Navy's builders constructed Antarctica's first nuclear power plant, at McMurdo Station.
During the Cold War, the Seabees undertook a number of other missions, including constructing the Distant Early Warning Line in the Arctic. Again operating often under extreme conditions, the Seabees successfully completed every mission assigned to them.
Vietnam War.
Seabees were deployed to Vietnam throughout the conflict beginning in small numbers in June 1954 and extending to November 1972. By 1962, they began building camps for Special Forces. In June 1965, Construction Mechanic 3rd Class Marvin G. Shields, part of Seabee Team 1104, was actively engaged at the Battle of Dong Xoai and was posthumously awarded the Medal of Honor for his actions there. Shields remains the only Seabee ever to be awarded the Medal of Honor. These "Civic Action Teams" continued into the Vietnam War where Seabees, often fending off enemy forces alongside their Marine and Army counterparts, also built schools and infrastructure and provided health care service. Beginning in 1965, full Seabee battalions (MCBs) and Naval Construction Regiments (NCRs), along with other unit types, were deployed throughout Vietnam. Seabees from the Naval Reserve provided individual personnel early on to augment regular units and two battalions, MCB 12 and MCB 22.
In Vietnam, the Seabees supported the Marines and built a staggering number of aircraft-support facilities, roads, and bridges; they also paved roads that provided access to farms and markets, supplied fresh water to countless numbers of Vietnamese through hundreds of Seabee-dug wells, provided medical treatment to thousands of villagers, and built schools, hospitals, utilities systems, roads and other community facilities. Seabees also worked with and taught construction skills to the Vietnamese people.
After Vietnam, the Seabees built and repaired Navy bases in Puerto Rico, Japan, Guam, Greece, Sicily, and Spain. Their civic action projects focused on the Trust Territories of the Pacific.
In 1971, the Seabees began their largest peacetime construction on Diego Garcia, a small atoll in the Indian Ocean. This project took 11 years and cost $200 million. The complex accommodates the Navy's largest ships and the biggest military cargo jets. This base proved invaluable when Iraq invaded Kuwait in August 1990 and Operations Desert Shield and Desert Storm were launched.
From the Cold War to terrorism.
As the Cold War cooled off, new challenges were presented by the increased incidence of terrorism. Also there were ongoing support missions to Diego Garcia, Guam, Okinawa, Navy and Marine Bases in Japan, the Philippines, Puerto Rico, Guantanamo Bay, Guatemala, the Naval Support Facility for Polaris and Poseidon Submarines in Holy Loch Scotland, Rota Spain, Naples Italy and Suda Bay Crete.
Seabee construction efforts led to the expansion and improvement of Naval Air Facility, Sigonella Sicily, turning this into a major base for the Navy’s Sixth Fleet aviation activities.
There were combat roles as well. In 1983, a truck bomb demolished the barracks the Marines had secured in Beirut, Lebanon. After moving to the Beirut International Airport and setting up quarters there, Druse militia artillery began harassing the Marines. After consultations with the theater commander and Marine amphibious command and combat engineers, the forward deployed battalion, NMCB-1 in Rota Spain sent in a 70 man AirDet working party with heavy equipment. Construction of artillery resistant quarters went on from December 1983 until the Marines’ withdrawal in February 1984. Only one casualty occurred when an Equipment Operator using a bulldozer to clear fields of fire was wounded by an RPG attack. The Seabee, EO2 Kirt May was awarded the first Purple Heart awarded a Seabee since Vietnam. The Seabees were proud that the Marines had greatly improved protection from ongoing artillery harassment.
Robert Stethem was murdered by the Lebanese Shia militia Hezbollah when they hijacked TWA Flight 847 in 1985. Stethem was a Steelworker Second Class (SW2), a Seabee diver and member of Underwater Construction Team ONE. The USS "Stethem" (DDG-63) is named in his honor. On August 24, 2010, onboard USS "Stethem" in Yokosuka, Japan, Stethem was posthumously made an honorary Master Chief Constructionman (CUCM) by the Master Chief Petty Officer of the Navy.
Persian Gulf War.
During the Persian Gulf War, more than 5,000 Seabees (4,000 active and 1,000 reservists) served in the Middle East. In Saudi Arabia, Seabees built 10 camps for more than 42,000 personnel; 14 galleys capable of feeding 75,000 people; and 6 million ft² (600,000 m²) of aircraft parking apron and runways as well as 200+ Helo landing zones. They built and maintained two 500-bed Fleet Hospitals near the port city of Al-Jubayl.
Iraq War and the War in Afghanistan.
Seabees continue to provide critical construction skills in connection with the effort to rebuild the infrastructure of Afghanistan. All active and reserve Naval Mobile Construction Battalions (NMCBs) and Naval Construction Regiments (NCRs) have been deployed to both Iraq and Afghanistan. The Seabees have been deployed since the beginning of the invasion of Iraq in 2003. One of their most high profiles tasks in Iraq has been the removal of statues of Saddam Hussein in Baghdad. In Afghanistan, the Seabees' main task has been the construction of multiple Forward Operating Bases for U.S. and coalition forces.
Operation Enduring Freedom Southern Philippines.
Since 2002, Seabees have provided critical and tactical construction skills in an effort to win the hearts and minds of locals. Their efforts have begun to deter the rising influence of radical terrorists in the southern Philippines, most notably the Abu Sayyaf's jungle training area. Seabees work along with Army, Marines, and Air Force under Joint Special Operations Task Force-Philippines.
Disaster relief and recovery.
In 1969 when Hurricane Camile hit the gulf coast, the MCB-121 battalion stationed at Gulfport were called upon for cleanup, rescue, and community outreach for months to come. They fed displaced families and supported the community.
Seabees supported disaster recovery efforts for victims of the Northridge earthquake of 1994.
In summer 1992, Seabees were called on to provide recovery assistance for Homestead, Florida following Hurricane Andrew. Seabees were also vital to the humanitarian efforts in Somalia during Operation Restore Hope in 1992-1993. In 1994, they were again called on to provide assistance to the Haitian Relief effort at Guantanamo Bay Naval Base, Cuba. On Christmas Day 1995, Seabees arrived in Croatia to support the Army by building camps as part of Operation Joint Endeavor, the peacekeeping effort in Sarajevo, Bosnia-Herzegovina. NMCB 40 played a pivotal role serving with the U.S. Army 1st Infantry Division "The Big Red One", in assisting with the dismantling of FOB's during the IFOR/SFOR phase.
On September 23, 1998, Hurricane Georges plowed through the Caribbean Islands causing millions of dollars in damage and generating thousands of DRT (disaster recovery team) man hours for the Seabees. The Navy provided generators and water trucks that were taken to nearby cities and damage assessment teams were sent to the local islands.
Shortly after Hurricane Georges ravaged Puerto Rico and most of the Caribbean, the Seabees immediately turned their focus towards Hurricane Mitch, which was the most powerful hurricane of the 1998 season. Mitch left more than 17,000 people dead due to the high winds and heavy rains, which led to mud slides that buried thousands in Central America. The Seabees deployed to Honduras participating in operations with Joint Task Force Bravo, providing capabilities to conduct engineer reconnaissance, repair roads and bridges, clear debris, remove bridges, and build base camps. Naval Mobile Construction Battalion 7 was the first Navy element to arrive in Central America taking part in their second humanitarian mission on the deployment.
Seabees deployed in September 2004 in response to Hurricane Ivan’s destruction to the Naval Air Station Pensacola in Florida. The Seabees cleared hurricane debris, repaired roads, erected tents, and otherwise assisted fellow service members.
The Naval Construction Battalion Center in Gulfport, Mississippi, suffered damage during Hurricane Katrina in 2005. Seabees were tasked to rebuild the base and the Gulf Coast.
Seabees of Naval Mobile Construction Battalion Seven deployed to provide construction support and disaster relief to Haiti following the earthquake in 2010. Seabee divers from Underwater Construction Team One along with ACB-2 and the Army Engineer divers made repairs to the heavily damaged port facilities in Port-au-Prince. This resulted in the re-opening of the port to allow humanitarian supplies into the country.
Seabees from NMCB-133 and Underwater Construction Team Two deployed to Japan as part of the relief effort after the 2011 earthquake and tsunami.
Seabees of Naval Mobile Construction Battalion Eleven Air Detachment deployed for roughly two weeks to support federal, state, and local authorities in disaster recovery operations in the New Jersey and New York areas affected by Hurricane Sandy. The Air Detachment mounted out 90 personnel and 94 pieces of civil engineering support equipment including front-end loaders, backhoes, pumps, generators, storage containers, and other equipment which was convoyed to the disaster area.
Organization.
Unit nomenclature.
Battalion.
The battalion is the fundamental unit of the Naval Construction Force (NCF). Seabee battalions are constituted in such a way as to be self-sustaining in the field. The nomenclature for NCF battalions has evolved over the years. 
From the early 1960s through 1991, reserve battalions were referred to as Reserve Naval Mobile Construction Battalions (RNMCB). After 1991, all reserve battalions were renamed to NMCB, signifying the integration of the reserve units with the active units of the NCF.
Regiment.
During the rapid build-up of the Seabees during World War II, the number of battalions in a given area increased and larger construction programs were undertaken. This necessitated a higher command echelon to plan, coordinate, and assign the work of several battalions in one area. As a result, Naval Construction Regiments (NCR) were established in December 1942.
Brigade.
In April 1943, Naval Construction Brigades (NCB) were organized to coordinate the work of regiments. Brigades were the highest NCF command echelon until early in the 21st Century. At that time, the last two brigades were the SECOND Naval Construction Brigade (2nd NCB) and the THIRD Naval Construction Brigade (3rd NCB). The 2nd NCB commanded Atlantic Fleet Seabee units and the 3rd NCB commanded Pacific Fleet Seabee units. Both brigades were decommissioned in August 2002 and are no longer part of the NCF structure.
Division.
Shortly after the commencement of the Global War on Terror, it was realized that a single command interface for global Seabee operations would be required. On August 9, 2002, the FIRST Naval Construction Division (1 NCD) was stood-up and commissioned at NAB Little Creek in Virginia. Since January 2006, 1NCD has been a subordinate unit of Navy Expeditionary Combat Command (NECC). First Naval Construction Division (1NCD) was decommissioned May 31, 2013. The 1NCD staff will be integrated into NECC. Some 1NCD functions have been transferred to the newly created Naval Construction Groups (NCGs) in Gulfport, Mississippi, and Port Hueneme, California, which are now the East and West Coast continuity for the NCF.
Specialty Units.
Construction Battalion Maintenance Unit (CBMU).
When first organized during WWII, these units consisted of approximately one-fourth the personnel of an NCB and were intended to take over the maintenance of bases on which major construction had been completed. Today, CBMU's provide public works support at Naval Support Activities, Forward Operating Bases, and Fleet Hospital/Expeditionary Medical Facilities during wartime or contingency operations. They also provide disaster recovery support to Naval Regional Commanders in CONUS.
Underwater Construction Team (UCT).
UCT's deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction.
Naval Construction Groups (formerly Seabee Readiness Groups [SRG]).
In 2013, the SRG's were decommissioned, and re-formed into Naval Construction Groups ONE and TWO. They are regimental-level command groups tasked with administrative and tactical control of Seabee Battalions, as well as conducting pre-deployment training of NCF units in the NCG's respective homeport locations. Currently, Naval Construction Group TWO (NCG-2) is based at CBC Gulfport, and Naval Construction Group ONE (NCG-ONE) is based at CBC Port Hueneme.
Amphibious Construction Battalion (ACB).
ACB's (also abbreviated as PHIBCB) evolved out of pontoon assembly battalions formed as part of the Seabees during World War II. After the war, these battalions (originally MCBs 104 and 105) were renamed ACB's and assigned to Naval Beach Groups.
Today, while the ACBs are part of the NCF, they do not report to 1 NCD, instead reporting to surface TYCOMs. Additionally, the ACBs have a different personnel mix than an NMCB with half the enlisted personnel being traditional Seabee rates and the other half being fleet rates.
Obsolete units.
NCF unit types that are no longer in use include:
Construction Battalion Hospital Unit (CBHU)
Training.
The newcomers begin "A" School (preliminary training) fresh out of boot camp, or they come from the fleet after their service term is met, spending about 75% of the twelve weeks immersed in hands-on training. The remaining 25% is spent in classroom instruction. From "A" School, new Seabees most often report to an NMCB command for their first tour of duty. For training, the new Seabees attend a four-week course known as Expeditionary Combat Skills (ECS) at the Naval Construction Battalion Center in Gulfport, Mississippi, and Port Hueneme, California. ECS is also being taught to all personnel who report to a unit in the Naval Expeditionary Combat Command. ECS is a basic combat-skills course where the students spend time in a classroom environment learning map reading and land navigation, battlefield first aid, how to lay out defensive plans, how to conduct patrols, vehicle egress, and many other combat-related skills. Half of each course is spent at a rifle range where students learn basic rifle marksmanship and then qualify with the M16A2 and M16A3 service rifles. ECS students also learn fundamentals of the M9 service pistol and qualify. At the end of training, new Seabees are ready to perform with their new battalion. During their tenure with an NMCB, personnel may be assigned to a crew-served weapon, such as the MK 19 40 mm machine gun, the M2HB .50-caliber machine gun, or the M240 machine gun. Many reserve units still field variants of the M60 machine gun. Until 2012, Seabees wore the Woodland camouflage uniform or the legacy tri-color Desert Camouflage Uniform, the last members of the entire U.S. military to do so, but are now transitioning to the NWU Type III. Seabees use ALICE field gear as well as some units working with Marines use USMC issue ILBE gear.
About one-third of new Seabees are assigned to Public Works Departments (PWD) at naval installations both within the United States and overseas. While stationed at a Public Works Department, a Seabee has the opportunity to get specialized training and extensive experience in one or more facets of their rating.
Ratings.
There are seven source ratings for the Seabee community:
Badge.
The military qualification badge for the Seabees is known as the Seabee Combat Warfare Specialist insignia (SCW). It is issued to both officers and enlisted personnel and recognizes those who have been fully trained and qualified as a member of the various Naval Construction Force (NCF) units. Only members attached to a qualifying NCF unit are eligible for the SCWs pin. The qualifying units include: Naval Mobile Construction Battalions (NMCB), Amphibious Construction Battalions (ACB), Naval Construction Force Support Units (NCFSU), Underwater Construction Teams (UCT), and, since the end of 2008, Naval Construction Regiments (NCR).
The SCWs insignia has been in existence since it was officially approved for use in 1993.
Ranks.
The ranks of E-1 through E-3 in the Navy include Seaman (white stripes), Airman (green stripes), and Fireman (red stripes). E-1 through E-3 Seabees use the designation "Constructionman" and wear sky-blue stripes on their dress and service uniforms.
Logo.
Frank J. Iafrate, a civilian plan file clerk at Naval Air Station Quonset Point, Rhode Island, was the artist who designed the original Seabee logo ("Fighting 'Bee") in early 1942. The logo has remained in use, unchanged. In late 1942, after designing the logo, he enlisted in the Seabees.
During World War II, artists working for Walt Disney designed logos for about ten Naval Construction units, including the 60th Naval (Canal) Construction Battalion and the 133rd Naval Construction Battalion, in 1943.
Battalions.
During World War II, there were more than 140 battalions commissioned. In the years between then and the present, battalions have been activated and deactivated as required by shifting national defense priorities. At present, there are 5 active-duty naval mobile construction battalions (NMCBs) — known as Seabees ("Seabees") — in the United States Navy, split between the east and west coasts. The remaining battalions are Navy Reserve battalions:
Museums.
The U.S. Navy Seabee Museum is located at Naval Base Ventura County, Port Hueneme, California near the entrance but outside the main gate. Due to the location, visitors are able to visit the museum without having to enter the base itself. The museum re opened on July 22, 2011 in a new building built by Carlsbad-based RQ Construction. The design of the single-story, 38,833 square foot structure was inspired by the Seabee Quonset Hut. Inside are galleries for exhibition space, a grand hall, a theater for 45 people, collections storage, and research areas.
On February 7, 2011, the museum was certified as LEED Silver for utilizing a number of sustainable design and construction strategies. Features include the use of low-maintenance landscaping; a “cool” roofing system with high solar reflectance and thermal emittance; use of photocell controlled light fixtures and energy-efficient lighting fixtures; 30% use of regional materials and 80% construction debris was recycled and diverted from landfills; low Volatile Organic Compounds (VOCs); and, use of dual-flush toilets and low-flow aerator faucets.
The Seabee Heritage Center is located in Building 446 at the Naval Construction Battalion Center (Gulfport, Mississippi). The Heritage Center is the Atlantic Coast Annex of the Seabee Museum in Port Hueneme. Opened in 1995, the Museum Annex commemorates the history and achievements of the Atlantic Coast Naval Construction Force (Seabees) and the Navy's Civil Engineer Corps. Exhibits at the Gulfport Annex are provided by the Seabee Museum in Port Hueneme.
The Seabee Museum and Memorial Park in Davisville, Rhode Island was opened in the late 1990s by a group of former Seabees. The Fighting Seabee Statue is located here.

</doc>
