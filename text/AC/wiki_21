<doc id="28736" url="http://en.wikipedia.org/wiki?curid=28736" title="Speed of light">
Speed of light

The speed of light in vacuum, commonly denoted c, is a universal physical constant important in many areas of physics. Its value is exactly (≈ m/s), as the length of the metre is defined from this constant and the international standard for time. According to special relativity, "c" is the maximum speed at which all matter and information in the universe can travel. It is the speed at which all massless particles and changes of the associated fields (including electromagnetic radiation such as light and gravitational waves) travel in vacuum. Such particles and waves travel at "c" regardless of the motion of the source or the inertial frame of reference of the observer. In the theory of relativity, "c" interrelates space and time, and also appears in the famous equation of mass–energy equivalence "E" = "mc"2.
The speed at which light propagates through transparent materials, such as glass or air, is less than "c". The ratio between "c" and the speed "v" at which light travels in a material is called the refractive index "n" of the material ("n" = "c" / "v"). For example, for visible light the refractive index of glass is typically around 1.5, meaning that light in glass travels at "c" / 1.5 ≈ ; the refractive index of air for visible light is about 1.0003, so the speed of light in air is about or slower than "c".
For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. In communicating with distant space probes, it can take minutes to hours for a message to get from Earth to the spacecraft, or vice versa. The light seen from stars left them many years ago, allowing the study of the history of the universe by looking at distant objects. The finite speed of light also limits the theoretical maximum speed of computers, since information must be sent within the computer from chip to chip. The speed of light can be used with time of flight measurements to measure large distances to high precision.
Ole Rømer first demonstrated in 1676 that light travels at a finite speed (as opposed to instantaneously) by studying the apparent motion of Jupiter's moon Io. In 1865, James Clerk Maxwell proposed that light was an electromagnetic wave, and therefore travelled at the speed "c" appearing in his theory of electromagnetism. In 1905, Albert Einstein postulated that the speed of light with respect to any inertial frame is independent of the motion of the light source, and explored the consequences of that postulate by deriving the special theory of relativity and showing that the parameter "c" had relevance outside of the context of light and electromagnetism. After centuries of increasingly precise measurements, in 1975 the speed of light was known to be with a measurement uncertainty of 4 parts per billion. In 1983, the metre was redefined in the International System of Units (SI) as the distance travelled by light in vacuum in 1/ of a second. As a result, the numerical value of "c" in metres per second is now fixed exactly by the definition of the metre.
Numerical value, notation, and units.
The speed of light in vacuum is usually denoted by a lowercase "c", for "constant" or the Latin "celeritas" (meaning "swiftness"). Originally, the symbol "V" was used for the speed of light, introduced by James Clerk Maxwell in 1865. In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch had used "c" for a different constant later shown to equal √2 times the speed of light in vacuum. In 1894, Paul Drude redefined "c" with its modern meaning. Einstein used "V" in his original German-language papers on special relativity in 1905, but in 1907 he switched to "c", which by then had become the standard symbol.
Sometimes "c" is used for the speed of waves in "any" material medium, and "c"0 for the speed of light in vacuum. This subscripted notation, which is endorsed in official SI literature, has the same form as other related constants: namely, "μ"0 for the vacuum permeability or magnetic constant, "ε"0 for the vacuum permittivity or electric constant, and "Z"0 for the impedance of free space. This article uses "c" exclusively for the speed of light in vacuum.
Since 1983, the metre has been defined in the International System of Units (SI) as the distance light travels in vacuum in 1/ of a second. This definition fixes the speed of light in vacuum at exactly .
As a dimensional physical constant, the numerical value of "c" is different for different unit systems.
In branches of physics in which "c" appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where . Using these units, "c" does not appear explicitly because multiplication or division by 1 does not affect the result.
Fundamental role in physics.
The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer. This invariance of the speed of light was postulated by Einstein in 1905, after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous aether; it has since been consistently confirmed by many experiments. It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition. The special theory of relativity explores the consequences of this invariance of "c" with the assumption that the laws of physics are the same in all inertial frames of reference. One consequence is that "c" is the speed at which all massless particles and waves, including light, must travel in vacuum.
Special relativity has many counterintuitive and experimentally verified implications. These include the equivalence of mass and energy , length contraction (moving objects shorten), and time dilation (moving clocks run more slowly). The factor "γ" by which lengths contract and times dilate is known as the Lorentz factor and is given by , where "v" is the speed of the object. The difference of "γ" from 1 is negligible for speeds much slower than "c", such as most everyday speeds—in which case special relativity is closely approximated by Galilean relativity—but it increases at relativistic speeds and diverges to infinity as "v" approaches "c".
The results of special relativity can be summarized by treating space and time as a unified structure known as spacetime (with "c" relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz invariance, whose mathematical formulation contains the parameter "c". Lorentz invariance is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameter "c" is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts that "c" is also the speed of gravity and of gravitational waves. In non-inertial frames of reference (gravitationally curved space or accelerated reference frames), the "local" speed of light is constant and equal to "c", but the speed of light along a trajectory of finite length can differ from "c", depending on how distances and times are defined.
It is generally assumed that fundamental constants such as "c" have the same value throughout spacetime, meaning that they do not depend on location and do not vary with time. However, it has been suggested in various theories that the speed of light may have changed over time. No conclusive evidence for such changes has been found, but they remain the subject of ongoing research.
It also is generally assumed that the speed of light is isotropic, meaning that it has the same value regardless of the direction in which it is measured. Observations of the emissions from nuclear energy levels as a function of the orientation of the emitting nuclei in a magnetic field (see Hughes–Drever experiment), and of rotating optical resonators (see Resonator experiments) have put stringent limits on the possible two-way anisotropy.
Upper limit on speeds.
According to special relativity, the energy of an object with rest mass "m" and speed "v" is given by "γmc"2, where "γ" is the Lorentz factor defined above. When "v" is zero, "γ" is equal to one, giving rise to the famous formula for mass–energy equivalence. The "γ" factor approaches infinity as "v" approaches "c", and it would take an infinite amount of energy to accelerate an object with mass to the speed of light. The speed of light is the upper limit for the speeds of objects with positive rest mass. This is experimentally established in many tests of relativistic energy and momentum.
More generally, it is normally impossible for information or energy to travel faster than "c". One argument for this follows from the counter-intuitive implication of special relativity known as the relativity of simultaneity. If the spatial distance between two events A and B is greater than the time interval between them multiplied by "c" then there are frames of reference in which A precedes B, others in which B precedes A, and others in which they are simultaneous. As a result, if something were travelling faster than "c" relative to an inertial frame of reference, it would be travelling backwards in time relative to another frame, and causality would be violated. In such a frame of reference, an "effect" could be observed before its "cause". Such a violation of causality has never been recorded, and would lead to paradoxes such as the tachyonic antitelephone.
Faster-than-light observations and experiments.
There are situations in which it may seem that matter, energy, or information travels at speeds greater than "c", but they do not. For example, as is discussed in the propagation of light in a medium section below, many wave velocities can exceed "c". For example, the phase velocity of X-rays through most glasses can routinely exceed "c", but phase velocity does not determine the velocity at which waves convey information.
If a laser beam is swept quickly across a distant object, the spot of light can move faster than "c", although the initial movement of the spot is delayed because of the time it takes light to get to the distant object at the speed "c". However, the only physical entities that are moving are the laser and its emitted light, which travels at the speed "c" from the laser to the various positions of the spot. Similarly, a shadow projected onto a distant object can be made to move faster than "c", after a delay in time. In neither case does any matter, energy, or information travel faster than light.
The rate of change in the distance between two objects in a frame of reference with respect to which both are moving (their closing speed) may have a value in excess of "c". However, this does not represent the speed of any single object as measured in a single inertial frame.
Certain quantum effects appear to be transmitted instantaneously and therefore faster than "c", as in the EPR paradox. An example involves the quantum states of two particles that can be entangled. Until either of the particles is observed, they exist in a superposition of two quantum states. If the particles are separated and one particle's quantum state is observed, the other particle's quantum state is determined instantaneously (i.e., faster than light could travel from one particle to the other). However, it is impossible to control which quantum state the first particle will take on when it is observed, so information cannot be transmitted in this manner.
Another quantum effect that predicts the occurrence of faster-than-light speeds is called the Hartman effect; under certain conditions the time needed for a virtual particle to tunnel through a barrier is constant, regardless of the thickness of the barrier. This could result in a virtual particle crossing a large gap faster-than-light. However, no information can be sent using this effect.
So-called superluminal motion is seen in certain astronomical objects, such as the relativistic jets of radio galaxies and quasars. However, these jets are not moving at speeds in excess of the speed of light: the apparent superluminal motion is a projection effect caused by objects moving near the speed of light and approaching Earth at a small angle to the line of sight: since the light which was emitted when the jet was farther away took longer to reach the Earth, the time between two successive observations corresponds to a longer time between the instants at which the light rays were emitted.
In models of the expanding universe, the farther galaxies are from each other, the faster they drift apart. This receding is not due to motion "through" space, but rather to the expansion of space itself. For example, galaxies far away from Earth appear to be moving away from the Earth with a speed proportional to their distances. Beyond a boundary called the Hubble sphere, the rate at which their distance from Earth increases becomes greater than the speed of light.
Propagation of light.
In classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed "c" with which electromagnetic waves (such as light) propagate through the vacuum is related to the electric constant "ε"0 and the magnetic constant "μ"0 by the equation . In modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum.
Extensions of QED in which the photon has a mass have been considered. In such a theory, its speed would depend on its frequency, and the invariant speed "c" of special relativity would then be the upper limit of the speed of light in vacuum. No variation of the speed of light with frequency has been observed in rigorous testing, putting stringent limits on the mass of the photon. The limit obtained depends on the model used: if the massive photon is described by Proca theory, the experimental upper bound for its mass is about 10−57 grams; if photon mass is generated by a Higgs mechanism, the experimental upper limit is less sharp, "m" ≤ 10−14 eV/c2  (roughly 2 × 10−47 g).
Another reason for the speed of light to vary with its frequency would be the failure of special relativity to apply to arbitrarily small scales, as predicted by some proposed theories of quantum gravity. In 2009, the observation of the spectrum of gamma-ray burst GRB 090510 did not find any difference in the speeds of photons of different energies, confirming that Lorentz invariance is verified at least down to the scale of the Planck length ("l"P = √"ħ""G"/"c"3 ≈ ) divided by 1.2.
In a medium.
In a medium, light usually does not propagate at a speed equal to "c"; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity "v"p. An actual physical signal with a finite extent (a pulse of light) travels at a different speed. The largest part of the pulse travels at the group velocity "v"g, and its earliest part travels at the front velocity "v"f.
The phase velocity is important in determining how a light wave travels through a material or from one material to another. It is often represented in terms of a "refractive index". The refractive index of a material is defined as the ratio of "c" to the phase velocity "v"p in the material: larger indices of refraction indicate lower speeds. The refractive index of a material may depend on the light's frequency, intensity, polarization, or direction of propagation; in many cases, though, it can be treated as a material-dependent constant. The refractive index of air is approximately 1.0003. Denser media, such as water, glass, and diamond, have refractive indexes of around 1.3, 1.5 and 2.4, respectively, for visible light. In exotic materials like Bose–Einstein condensates near absolute zero, the effective speed of light may be only a few metres per second. However, this represents absorption and re-radiation delay between atoms, as do all slower-than-"c" speeds in material substances. As an extreme example of this, light "slowing" in matter, two independent teams of physicists claimed to bring light to a "complete standstill" by passing it through a Bose–Einstein Condensate of the element rubidium, one team at Harvard University and the Rowland Institute for Science in Cambridge, Mass., and the other at the Harvard–Smithsonian Center for Astrophysics, also in Cambridge. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrarily later time, as stimulated by a second laser pulse. During the time it had "stopped," it had ceased to be light. This type of behaviour is generally microscopically true of all transparent media which "slow" the speed of light.
In transparent materials, the refractive index generally is greater than 1, meaning that the phase velocity is less than "c". In other materials, it is possible for the refractive index to become smaller than 1 for some frequencies; in some exotic materials it is even possible for the index of refraction to become negative. The requirement that causality is not violated implies that the real and imaginary parts of the dielectric constant of any material, corresponding respectively to the index of refraction and to the attenuation coefficient, are linked by the Kramers–Kronig relations. In practical terms, this means that in a material with refractive index less than 1, the absorption of the wave is so quick that no signal can be sent faster than "c".
A pulse with different group and phase velocities (which occurs if the phase velocity is not the same for all the frequencies of the pulse) smears out over time, a process known as dispersion. Certain materials have an exceptionally low (or even zero) group velocity for light waves, a phenomenon called slow light, which has been confirmed in various experiments.
The opposite, group velocities exceeding "c", has also been shown in experiment. It should even be possible for the group velocity to become infinite or negative, with pulses travelling instantaneously or backwards in time.
None of these options, however, allow information to be transmitted faster than "c". It is impossible to transmit information with a light pulse any faster than the speed of the earliest part of the pulse (the front velocity). It can be shown that this is (under certain assumptions) always equal to "c". #redirect 
It is possible for a particle to travel through a medium faster than the phase velocity of light in that medium (but still slower than "c"). When a charged particle does that in a dielectric material, the electromagnetic equivalent of a shock wave, known as Cherenkov radiation, is emitted.
Practical effects of finiteness.
The speed of light is of relevance to communications: the one-way and round-trip delay time are greater than zero. This applies from small to astronomical scales. On the other hand, some techniques depend on the finite speed of light, for example in distance measurements.
Small scales.
In supercomputers, the speed of light imposes a limit on how quickly data can be sent between processors. If a processor operates at 1 gigahertz, a signal can only travel a maximum of about 30 cm in a single cycle. Processors must therefore be placed close to each other to minimize communication latencies; this can cause difficulty with cooling. If clock frequencies continue to increase, the speed of light will eventually become a limiting factor for the internal design of single chips.
Large distances on Earth.
For example, given the equatorial circumference of the Earth is about and "c" about , the theoretical shortest time for a piece of information to travel half the globe along the surface is about 67 milliseconds. When light is travelling around the globe in an optical fibre, the actual transit time is longer, in part because the speed of light is slower by about 35% in an optical fibre, depending on its refractive index "n". Furthermore, straight lines rarely occur in global communications situations, and delays are created when the signal passes through an electronic switch or signal regenerator.
Spaceflights and astronomy.
Similarly, communications between the Earth and spacecraft are not instantaneous. There is a brief delay from the source to the receiver, which becomes more noticeable as distances increase. This delay was significant for communications between ground control and Apollo 8 when it became the first manned spacecraft to orbit the Moon: for every question, the ground control station had to wait at least three seconds for the answer to arrive. The communications delay between Earth and Mars can vary between five and twenty minutes depending upon the relative positions of the two planets. As a consequence of this, if a robot on the surface of Mars were to encounter a problem, its human controllers would not be aware of it until at least five minutes later, and possibly up to twenty minutes later; it would then take a further five to twenty minutes for instructions to travel from Earth to Mars.
NASA must wait several hours for information from a probe orbiting Jupiter, and if it needs to correct a navigation error, the fix will not arrive at the spacecraft for an equal amount of time, creating a risk of the correction not arriving in time.
Receiving light and other signals from distant astronomical sources can even take much longer. For example, it has taken 13 billion (13×109) years for light to travel to Earth from the faraway galaxies viewed in the Hubble Ultra Deep Field images. Those photographs, taken today, capture images of the galaxies as they appeared 13 billion years ago, when the universe was less than a billion years old. The fact that more distant objects appear to be younger, due to the finite speed of light, allows astronomers to infer the evolution of stars, of galaxies, and of the universe itself.
Astronomical distances are sometimes expressed in light-years, especially in popular science publications and media. A light-year is the distance light travels in one year, around 9461 billion kilometres, 5879 billion miles, or 0.3066 parsecs. In round figures, a light year is nearly 10 trillion kilometres or nearly 6 trillion miles. Proxima Centauri, the closest star to Earth after the Sun, is around 4.2 light-years away.
Distance measurement.
Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about () in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging Experiment, radar astronomy and the Deep Space Network determine distances to the Moon, planets and spacecraft, respectively, by measuring round-trip transit times.
High-frequency trading.
The speed of light has become important in high-frequency trading, where traders seek to gain minute advantages by delivering their trades to exchanges fractions of a second ahead of other traders. For example traders have been switching to microwave communications between trading hubs, because of the advantage which microwaves travelling at near to the speed of light in air, have over fibre optic signals which travel 30–40% slower at the speed of light through glass.
Measurement.
There are different ways to determine the value of "c". One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and earth-based setups. However, it is also possible to determine "c" from other physical laws where it appears, for example, by determining the values of the electromagnetic constants "ε"0 and "μ"0 and using their relation to "c". Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling "c".
In 1983 the metre was defined as "the length of the path travelled by light in vacuum during a time interval of 1⁄ of a second", fixing the value of the speed of light at by definition, as described below. Consequently, accurate measurements of the speed of light yield an accurate realization of the metre rather than an accurate value of "c".
Astronomical measurements.
Outer space is a convenient setting for measuring the speed of light because of its large scale and nearly perfect vacuum. Typically, one measures the time needed for light to traverse some reference distance in the solar system, such as the radius of the Earth's orbit. Historically, such measurements could be made fairly accurately, compared to how accurately the length of the reference distance is known in Earth-based units. It is customary to express the results in astronomical units (AU) per day.
Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light. When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.
Another method is to use the aberration of light, discovered and explained by James Bradley in the 18th century. This effect results from the vector addition of the velocity of light arriving from a distant source (such as a star) and the velocity of its observer (see diagram on the right). A moving observer thus sees the light coming from a slightly different direction and consequently sees the source at a position shifted from its original position. Since the direction of the Earth's velocity changes continuously as the Earth orbits the Sun, this effect causes the apparent position of stars to move around. From the angular difference in the position of stars (maximally 20.5 arcseconds) it is possible to express the speed of light in terms of the Earth's velocity around the Sun, which with the known length of a year can be converted to the time needed to travel from the Sun to the Earth. In 1729, Bradley used this method to derive that light travelled 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.
Astronomical unit.
An astronomical unit (AU) is approximately the average distance between the Earth and Sun. It was redefined in 2012 as exactly . Previously the AU was not based on the International System of Units but in terms of the gravitational force exerted by the Sun in the framework of classical mechanics. The current definition uses the recommended value in metres for the previous definition of the astronomical unit, which was determined by measurement. This redefinition is analogous to that of the metre, and likewise has the effect of fixing the speed of light to an exact value in astronomical units per second (via the exact speed of light in metres per second).
Previously, the inverse of "c" expressed in seconds per astronomical unit was measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance could be obtained. For example, in 2009, the best estimate, as approved by the International Astronomical Union (IAU), was:
The relative uncertainty in these measurements is 0.02 parts per billion (), equivalent to the uncertainty in Earth-based measurements of length by interferometry. Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time in terms of the previous definition of the astronomical unit can also be interpreted as measuring the length of an AU (old definition) in metres.
Time of flight techniques.
A method of measuring the speed of light is to measure the time needed for light to travel to a mirror at a known distance and back. This is the working principle behind the Fizeau–Foucault apparatus developed by Hippolyte Fizeau and Léon Foucault.
The setup as used by Fizeau consists of a beam of light directed at a mirror 8 km away. On the way from the source to the mirror, the beam passes through a rotating cogwheel. At a certain rate of rotation, the beam passes through one gap on the way out and another on the way back, but at slightly higher or lower rates, the beam strikes a tooth and does not pass through the wheel. Knowing the distance between the wheel and the mirror, the number of teeth on the wheel, and the rate of rotation, the speed of light can be calculated.
The method of Foucault replaces the cogwheel by a rotating mirror. Because the mirror keeps rotating while the light travels to the distant mirror and back, the light is reflected from the rotating mirror at a different angle on its way out than it is on its way back. From this difference in angle, the known speed of rotation and the distance to the distant mirror the speed of light may be calculated.
Nowadays, using oscilloscopes with time resolutions of less than one nanosecond, the speed of light can be directly measured by timing the delay of a light pulse from a laser or an LED reflected from a mirror. This method is less precise (with errors of the order of 1%) than other modern techniques, but it is sometimes used as a laboratory experiment in college physics classes.#redirect 
Electromagnetic constants.
An option for deriving "c" that does not directly depend on a measurement of the propagation of electromagnetic waves is to use the relation between "c" and the vacuum permittivity "ε"0 and vacuum permeability "μ"0 established by Maxwell's theory: "c"2 = 1/("ε"0"μ"0). The vacuum permittivity may be determined by measuring the capacitance and dimensions of a capacitor, whereas the value of the vacuum permeability is fixed at exactly through the definition of the ampere. Rosa and Dorsey used this method in 1907 to find a value of .
Cavity resonance.
Another way to measure the speed of light is to independently measure the frequency "f" and wavelength "λ" of an electromagnetic wave in vacuum. The value of "c" can then be found by using the relation "c" = "fλ". One option is to measure the resonance frequency of a cavity resonator. If the dimensions of the resonance cavity are also known, these can be used determine the wavelength of the wave. In 1946, Louis Essen and A.C. Gordon-Smith established the frequency for a variety of normal modes of microwaves of a microwave cavity of precisely known dimensions. The dimensions were established to an accuracy of about ±0.8 μm using gauges calibrated by interferometry. As the wavelength of the modes was known from the geometry of the cavity and from electromagnetic theory, knowledge of the associated frequencies enabled a calculation of the speed of light.
The Essen–Gordon-Smith result, , was substantially more precise than those found by optical techniques. By 1950, repeated measurements by Essen established a result of .
A household demonstration of this technique is possible, using a microwave oven and food such as marshmallows or margarine: if the turntable is removed so that the food does not move, it will cook the fastest at the antinodes (the points at which the wave amplitude is the greatest), where it will begin to melt. The distance between two such spots is half the wavelength of the microwaves; by measuring this distance and multiplying the wavelength by the microwave frequency (usually displayed on the back of the oven, typically 2450 MHz), the value of "c" can be calculated, "often with less than 5% error".
Interferometry.
Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light. A coherent beam of light (e.g. from a laser), with a known frequency ("f"), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light ("λ") can be determined. The speed of light is then calculated using the equation "c" = "λf".
Before the advent of laser technology, coherent radio sources were used for interferometry measurements of the speed of light. However interferometric determination of wavelength becomes less precise with wavelength and the experiments were thus limited in precision by the long wavelength (~0.4 cm) of the radiowaves. The precision can be improved by using light with a shorter wavelength, but then it becomes difficult to directly measure the frequency of the light. One way around this problem is to start with a low frequency signal of which the frequency can be precisely measured, and from this signal progressively synthesize higher frequency signals whose frequency can then be linked to the original signal. A laser can then be locked to the frequency, and its wavelength can be determined using interferometry. This technique was due to a group at the National Bureau of Standards (NBS) (which later became NIST). They used it in 1972 to measure the speed of light in vacuum with a fractional uncertainty of .
History.
Until the early modern period, it was not known whether light travelled instantaneously or at a very fast finite speed. The first extant recorded examination of this subject was in ancient Greece. The ancient Greeks, Muslim scholars and classical European scientists long debated this until Rømer provided the first calculation of the speed of light. Einstein's Theory of Special Relativity concluded that the speed of light is constant regardless of one's frame of reference. Since then, scientists have provided increasingly accurate measurements.
Early history.
Empedocles (c. 490–430 BC) was the first to claim that light has a finite speed. He maintained that light was something in motion, and therefore must take some time to travel. Aristotle argued, to the contrary, that "light is due to the presence of something, but it is not a movement". Euclid and Ptolemy advanced Empedocles' emission theory of vision, where light is emitted from the eye, thus enabling sight. Based on that theory, Heron of Alexandria argued that the speed of light must be infinite because distant objects such as stars appear immediately upon opening the eyes.
Early Islamic philosophers initially agreed with the Aristotelian view that light had no speed of travel. In 1021, Alhazen (Ibn al-Haytham) published the "Book of Optics", in which he presented a series of arguments dismissing the emission theory of vision in favour of the now accepted intromission theory, in which light moves from an object into the eye. This led Alhazen to propose that light must have a finite speed, and that the speed of light is variable, decreasing in denser bodies. He argued that light is substantial matter, the propagation of which requires time, even if this is hidden from our senses. Also in the 11th century, Abū Rayhān al-Bīrūnī agreed that light has a finite speed, and observed that the speed of light is much faster than the speed of sound.
In the 13th century, Roger Bacon argued that the speed of light in air was not infinite, using philosophical arguments backed by the writing of Alhazen and Aristotle. In the 1270s, Witelo considered the possibility of light travelling at infinite speed in vacuum, but slowing down in denser bodies.
In the early 17th century, Johannes Kepler believed that the speed of light was infinite, since empty space presents no obstacle to it. René Descartes argued that if the speed of light were to be finite, the Sun, Earth, and Moon would be noticeably out of alignment during a lunar eclipse. Since such misalignment had not been observed, Descartes concluded the speed of light was infinite. Descartes speculated that if the speed of light were found to be finite, his whole system of philosophy might be demolished. In Descartes' derivation of Snell's law, he assumed that even though the speed of light was instantaneous, the more dense the medium, the faster was light's speed. Pierre de Fermat derived Snell's law using the opposing assumption, the more dense the medium the slower light traveled. Fermat also argued in support of a finite speed of light.
First measurement attempts.
In 1629, Isaac Beeckman proposed an experiment in which a person observes the flash of a cannon reflecting off a mirror about one mile (1.6 km) away. In 1638, Galileo Galilei proposed an experiment, with an apparent claim to having performed it some years earlier, to measure the speed of light by observing the delay between uncovering a lantern and its perception some distance away. He was unable to distinguish whether light travel was instantaneous or not, but concluded that if it were not, it must nevertheless be extraordinarily rapid. Galileo's experiment was carried out by the Accademia del Cimento of Florence, Italy, in 1667, with the lanterns separated by about one mile, but no delay was observed. The actual delay in this experiment would have been about 11 microseconds.
The first quantitative estimate of the speed of light was made in 1676 by Rømer (see Rømer's determination of the speed of light). From the observation that the periods of Jupiter's innermost moon Io appeared to be shorter when the Earth was approaching Jupiter than when receding from it, he concluded that light travels at a finite speed, and estimated that it takes light 22 minutes to cross the diameter of Earth's orbit. Christiaan Huygens combined this estimate with an estimate for the diameter of the Earth's orbit to obtain an estimate of speed of light of , 26% lower than the actual value.
In his 1704 book "Opticks", Isaac Newton reported Rømer's calculations of the finite speed of light and gave a value of "seven or eight minutes" for the time taken for light to travel from the Sun to the Earth (the modern value is 8 minutes 19 seconds). Newton queried whether Rømer's eclipse shadows were coloured; hearing that they were not, he concluded the different colours travelled at the same speed. In 1729, James Bradley discovered stellar aberration. From this effect he determined that light must travel 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.
Connections with electromagnetism.
In the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of . His method was improved upon by Léon Foucault who obtained a value of in 1862. In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/√"ε"0"μ"0, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed. In the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space at a speed equal to the above Weber/Kohrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an electromagnetic wave.
"Luminiferous aether".
It was thought at the time that empty space was filled with a background medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore it should be possible to measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert A. Michelson and Edward W. Morley in 1887. The detected motion was always less than the observational error. Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second.
Because of this experiment Hendrik Lorentz proposed that the motion of the apparatus through the aether may cause the apparatus to contract along its length in the direction of motion, and he further assumed, that the time variable for moving systems must also be changed accordingly ("local time"), which led to the formulation of the Lorentz transformation. Based on Lorentz's aether theory, Henri Poincaré (1900) showed that this local time (to first order in v/c) is indicated by clocks moving in the aether, which are synchronized under the assumption of constant light speed. In 1904, he speculated that the speed of light could be a limiting velocity in dynamics, provided that the assumptions of Lorentz's theory are all confirmed. In 1905, Poincaré brought Lorentz's aether theory into full observational agreement with the principle of relativity.
Special relativity.
In 1905 Einstein postulated from the outset that the speed of light in vacuum, measured by a non-accelerating observer, is independent of the motion of the source or observer. Using this and the principle of relativity as a basis he derived the special theory of relativity, in which the speed of light in vacuum "c" featured as a fundamental constant, also appearing in contexts unrelated to light. This made the concept of the stationary aether (to which Lorentz and Poincaré still adhered) useless and revolutionized the concepts of space and time.
Increased accuracy of "c" and redefinition of the metre and second.
In the second half of the 20th century much progress was made in increasing the accuracy of measurements of the speed of light, first by cavity resonance techniques and later by laser interferometer techniques. These were aided by new, more precise, definitions of the metre and second. In 1950, Louis Essen determined the speed as , using cavity resonance. This value was adopted by the 12th General Assembly of the Radio-Scientific Union in 1957. In 1960, the metre was redefined in terms of the wavelength of a particular spectral line of krypton-86, and, in 1967, the second was redefined in terms of the hyperfine transition frequency of the ground state of caesium-133.
In 1972, using the laser interferometer method and the new definitions, a group at NBS in Boulder, Colorado determined the speed of light in vacuum to be "c" = . This was 100 times less uncertain than the previously accepted value. The remaining uncertainty was mainly related to the definition of the metre. As similar experiments found comparable results for "c", the 15th Conférence Générale des Poids et Mesures (CGPM) in 1975 recommended using the value for the speed of light.
Defining the speed of light as an explicit constant.
In 1983 the 17th CGPM found that wavelengths from frequency measurements and a given value for the speed of light are more reproducible than the previous standard. They kept the 1967 definition of second, so the caesium hyperfine frequency would now determine both the second and the metre. To do this, they redefined the metre as: "The metre is the length of the path travelled by light in vacuum during a time interval of 1/ of a second." As a result of this definition, the value of the speed of light in vacuum is exactly and has become a defined constant in the SI system of units. Improved experimental techniques that prior to 1983 would have measured the speed of light, no longer affect the known value of the speed of light in SI units, but instead allow a more precise realization of the metre by more accurately measuring the wavelength of Krypton-86 and other light sources.
In 2011, the CGPM stated its intention to redefine all seven SI base units using what it calls "the explicit-constant formulation", where each "unit is defined indirectly by specifying explicitly an exact value for a well-recognized fundamental constant", as was done for the speed of light. It proposed a new, but completely equivalent, wording of the metre's definition: "The metre, symbol m, is the unit of length; its magnitude is set by fixing the numerical value of the speed of light in vacuum to be equal to exactly when it is expressed in the SI unit m s−1." This is one of the proposed changes to be incorporated in the next revision of the SI also termed the "New SI".
Further reading.
Historical references.
</dl>
Modern references.
</dl>

</doc>
<doc id="28737" url="http://en.wikipedia.org/wiki?curid=28737" title="Semaphore (disambiguation)">
Semaphore (disambiguation)

Semaphore usually refers to flag semaphore. It may also refer to;

</doc>
<doc id="28738" url="http://en.wikipedia.org/wiki?curid=28738" title="Synchronization">
Synchronization

Synchronization is the coordination of events to operate a system in unison. The familiar conductor of an orchestra serves to keep the orchestra "in time". Systems operating with all their parts in synchrony are said to be "synchronous" or "in sync".
Today, synchronization can occur on a global basis through the GPS-enabled timekeeping systems (and similar independent systems operated by the EU and Russia).
Transport.
Time-keeping and synchronization of clocks was a critical problem in long-distance ocean navigation; accurate time is required in conjunction with astronomical observations to determine how far East or West a vessel has traveled. The invention of an accurate marine chronometer revolutionized marine navigation. By the end of the 19th century, time signals in the form of a signal gun, flag, or dropping time ball, were provided at important ports so that mariners could check their chronometers for error.
Synchronization was important in the operation of 19th century railways, these being the first major means of transport fast enough for the differences in local time between adjacent towns to be noticeable. Each line handled the problem by synchronizing all its stations to headquarters as a standard railroad time. In some territories, sharing of single railroad tracks was controlled by the timetable. The need for strict timekeeping led the companies to settle on one standard, and civil authorities eventually abandoned local mean solar time in favor of that standard.
Communication.
In electrical engineering terms, for digital logic and data transfer, a synchronous circuit requires a clock signal. However, the use of the word "clock" in this sense is different from the typical sense of a clock as a device that keeps track of time-of-day; the clock signal simply signals the start and/or end of some time period, often very minute (measured in microseconds or nanoseconds), that has an arbitrary relationship to sidereal, solar, or lunar time, or to any other system of measurement of the passage of minutes, hours, and days.
In a different sense, electronic systems are sometimes synchronized to make events at points far apart appear simultaneous or near-simultaneous from a certain perspective. (Albert Einstein proved in 1905 in his first relativity paper that there actually are no such things as absolutely simultaneous events.) Timekeeping technologies such as the GPS satellites and Network Time Protocol (NTP) provide real-time access to a close approximation to the UTC timescale and are used for many terrestrial synchronization applications of this kind.
Synchronization is an important concept in the following fields:
Uses.
Some systems may be only approximately synchronized, or plesiochronous. Some applications require that relative offsets between events be determined. For others, only the order of the event is important.

</doc>
<doc id="28739" url="http://en.wikipedia.org/wiki?curid=28739" title="Sherwood Forest">
Sherwood Forest

Sherwood Forest is a royal forest in Nottinghamshire, England, famous by its historical association with the legend of Robin Hood.
A forest since the end of the Ice Age (as attested by pollen sampling cores), Sherwood Forest National Nature Reserve today encompasses 423.2 hectares, (1,045 acres) surrounding the village of Edwinstowe, the site of Thoresby Hall.
The forest that most people associate with Sherwood Forest is actually named Birklands and Bilhaugh. It is a remnant of an older, much larger, royal hunting forest, which derived its name from its status as the "shire (or sher) wood" of Nottinghamshire, which extended into several neighbouring counties (shires), bordered on the west along the River Erewash and the Forest of East Derbyshire. When the Domesday Book was compiled in 1086, the forest covered perhaps a quarter of Nottinghamshire in woodland and heath subject to the forest laws.
Management and conservation.
The Sherwood Forest Trust are a small charity that covers the ancient royal boundary and current national character area of Sherwood Forest. Their aims are based on conservation, heritage and communities, but also include tourism and the economy.
Nottinghamshire County Council and The Forestry Commission manages the ancient remnant of forest north of the village of Edwinstowe, and provides walks and trails and a host of other activities.
This central core of ancient Sherwood is a SSSI, NNR, and SAC. It is a very important site for ancient oaks, wood pasture, invertebrates and fungi, as well as being linked to the legends of Robin Hood.
Part of the forest was opened as a country park to the public in 1969 by Nottinghamshire County Council, which manages a small part of the forest under lease from the Thoresby Estate. In 2002, a portion of Sherwood Forest was designated a National Nature Reserve by English Nature. In 2007, Natural England officially incorporated the Budby South Forest, Nottinghamshire's largest area of dry lowland heath, into the Nature Reserve, nearly doubling its size from 220 to.
Some portions of the forest retain many very old oaks, especially in the portion known as the Dukeries, south of the town of Worksop, which was so called because it used to contain five ducal residences.
The River Idle, a tributary of the Trent, is formed in Sherwood Forest from the confluence of several minor streams.
Tourism.
Sherwood attracts between 360,000 and 1 million tourists annually, many from other countries. Visitor numbers have increased significantly since the launch of the BBC's "Robin Hood" television series in 2006.
Each August the nature reserve hosts an annual, week-long Robin Hood Festival. This event recreates a medieval atmosphere and features the major characters from the Robin Hood legend. The week's entertainment includes jousters and strolling players, dressed in medieval attire, in addition to a medieval encampment complete with jesters, musicians, rat-catchers, alchemists and fire eaters.
Throughout the year, visitors are attracted to the Sherwood Forest Art and Craft Centre, which is situated in the former Coach House and Stables of Edwinstowe Hall in the heart of the Forest. The centre contains art studios and a cafe, and hosts special events, including craft demonstrations and exhibitions.
Major Oak.
Sherwood Forest is home to the famous Major Oak, which, according to local folklore, was Robin Hood's principal hideout. The oak tree is between 800 and 1,000 years old and, since the Victorian era, its massive limbs have been partially supported by an elaborate system of scaffolding. In February 1998, a local company took cuttings from the Major Oak and began cultivating clones of the famous tree with the intention of sending saplings to be planted in major cities around the world.
The Major Oak was featured on the 2005 BBC TV programme "Seven Natural Wonders" as one of the natural wonders of the Midlands.
Thynghowe.
Thynghowe, an important Danelaw meeting place where people came to resolve disputes and settle issues, was lost to history until its rediscovery in 2005-06 by local history enthusiasts amidst the old oaks of an area known as the Birklands. Experts believe it may also yield clues as to the boundary of the ancient Anglo Saxon kingdoms of Mercia and Northumbria.
English Heritage have recently inspected the site, and have confirmed it was known as "Thynghowe" in 1334 and 1609.
Future attractions.
The current location of the Sherwood Forest Visitor Centre must be moved due to the classification of the area as a Special Area of Conservation.
SACs are strictly protected sites designated under the EC Habitats Directive. Article 3 of the Habitats Directive requires the establishment of a European network of important high-quality conservation sites that will make a significant contribution to conserving the 189 habitat types and 788 species identified in Annexes I and II of the Directive (as amended). The listed habitat types and species are those considered to be most in need of conservation at a European level (excluding birds). Of the Annex I habitat types, 78 are believed to occur in the UK. Of the Annex II species, 43 are native to, and normally resident in, the UK.
There should be a new visitor attraction located near by 2016.

</doc>
<doc id="28740" url="http://en.wikipedia.org/wiki?curid=28740" title="Sulawesi">
Sulawesi

Sulawesi (formerly known as Celebes or ) is an island in Indonesia. One of the four Greater Sunda Islands, and the world's eleventh-largest island, it is situated between Borneo and the Maluku Islands. In Indonesia, only Sumatra, Borneo, and Papua are larger in territory, and only Java and Sumatra have larger populations.
Sulawesi comprises four peninsulas: the northern Minahasa Peninsula; the East Peninsula; the South Peninsula; and the South-east Peninsula. Three gulfs separate these peninsulas: the Gulf of Tomini between northern Minahasa peninsula and East Peninsula; the Tolo Gulf between East and Southeast Peninsula; and the Bone Gulf between the South and Southeast Peninsula. The Strait of Makassar runs along the western side of the island and separates the island from Borneo.
Etymology.
The Portuguese were the first to refer to Sulawesi as 'Celebes'. The name 'Sulawesi' possibly comes from the words "sula" ('island') and "besi" ('iron') and may refer to the historical export of iron from the rich Lake Matano iron deposits.
Geology.
The island slopes up from the shores of the deep seas surrounding the island to a high, mostly non-volcanic, mountainous interior. Active volcanoes are found in the northern Minahassa Peninsula, stretching north to the Sangihe Islands. The northern peninsula contains several active volcanoes such as Mount Lokon, Mount Awu, Soputan, and Karangetang.
According to plate reconstructions, the island is believed to have been formed by the collision of terranes from the Asian Plate (forming the west and southwest), and from the Australian Plate (forming the southeast and Banggai), with island arcs previously in the Pacific (forming the north and east peninsulas). Because of its several tectonic origin, faults scar the land; as a result, the island is prone to earthquakes.
Sulawesi, in contrast to most of the other islands in the biogeographical region of Wallacea, is not truly oceanic, but a composite island at the centre of the Asia-Australia collision zone. Parts of the island were formerly attached to either the Asian or Australian continental margin and became separated from these areas by vicariant processes. For one. in the west, the opening of the Makassar Strait separated West Sulawesi from Sundaland in the Eocene c. 45 Mya. In the east, the traditional view of collisions of multiple micro-continental fragments sliced from New Guinea with an active volcanic margin in West Sulawesi at different times since the Early Miocene c. 20 Mya has recently been replaced by the hypothesis that extensional fragmentation has followed a single Miocene collision of West Sulawesi with the Sula Spur, the western end of an ancient folded belt of Variscan origin in the Late Paleozoic.
Prehistory.
Before October 2014, the settlement of South Sulawesi by modern humans had been dated to c. 30,000 BC on the basis of radiocarbon dates obtained from rock shelters in Maros. No earlier evidence of human occupation had at that point been found, but the island almost certainly formed part of the land bridge used for the settlement of Australia and New Guinea by at least 40,000 BCE. There is no evidence of "Homo erectus" having reached Sulawesi; crude stone tools first discovered in 1947 on the right bank of the Walennae river at Berru, which were thought to date to the Pleistocene on the basis of their association with vertebrate fossils, are now thought to date to perhaps 50,000 BC.
Following Peter Bellwood's model of a southward migration of Austronesian-speaking farmers (AN), radiocarbon dates from caves in Maros suggest a date in the mid-second millennium BC for the arrival of an AN group from east Borneo speaking a Proto-South Sulawesi language (PSS). Initial settlement was probably around the mouth of the Sa'dan river, on the northwest coast of the peninsula, although the south coast has also been suggested. Subsequent migrations across the mountainous landscape resulted in the geographical isolation of PSS speakers and the evolution of their languages into the eight families of the South Sulawesi language group. If each group can be said to have a homeland, that of the Bugis – today the most numerous group – was around lakes Témpé and Sidénréng in the Walennaé depression. Here for some 2,000 years lived the linguistic group that would become the modern Bugis; the archaic name of this group (which is preserved in other local languages) was Ugiq. Despite the fact that today they are closely linked with the Makasar, the closest linguistic neighbors of the Bugis are the Toraja.
Pre-1200 CE Bugis society was most likely organized into chiefdoms. Some anthropologists have speculated these chiefdoms would have warred and, in times of peace, exchanged women with each other. Further they have speculated that personal security would have been negligible, and head-hunting an established cultural practice. The political economy would have been a mixture of hunting and gathering and swidden or shifting agriculture. Speculative planting of wet rice may have taken place along the margins of the lakes and rivers.
In Central Sulawesi there are over 400 granite megaliths, which various archaeological studies have dated to be from 3000 BC to 1300 AD. They vary in size from a few centimetres to ca.4.5 m. The original purpose of the megaliths is unknown. About 30 of the megaliths represent human forms. Other megaliths are in form of large pots ("Kalamba") and stone plates ("Tutu'na").
In October 2014 it was announced that cave paintings in Maros had been dated as being about 40,000 years old. Dr Maxime Aubert, of Griffith University in Queensland, Australia, said that the minimum age for the outline of a hand was 39,900 years old, which made it "the oldest hand stencil in the world" and added, "Next to it is a pig that has a minimum age of 35,400 years old, and this is one of the oldest figurative depictions in the world, if not the oldest one."
History.
Starting in the 13th century, access to prestige trade goods and to sources of iron started to alter long-standing cultural patterns, and to permit ambitious individuals to build larger political units. It is not known why these two ingredients appeared together; one was perhaps the product of the other. By 1400, a number of nascent agricultural principalities had arisen in the western Cenrana valley, as well as on the south coast and on the west coast near modern Parepare.
The first Europeans to visit the island (which they believed to be an archipelago due to its contorted shape) were the Portuguese sailors Simão de Abreu, in 1523, and Gomes de Sequeira (among others) in 1525, sent from the Moluccas in search of gold, which the islands had the reputation of producing. A Portuguese base was installed in Makassar in the first decades of the 16th century, lasting until 1665, when it was taken by the Dutch. The Dutch had arrived in Sulawesi in 1605 and were quickly followed by the English, who established a factory in Makassar. From 1660, the Dutch were at war with Gowa, the major Makasar west coast power. In 1669, Admiral Speelman forced the ruler, Sultan Hasanuddin, to sign the Treaty of Bongaya, which handed control of trade to the Dutch East India Company. The Dutch were aided in their conquest by the Bugis warlord Arung Palakka, ruler of the Bugis kingdom of Bone. The Dutch built a fort at Ujung Pandang, while Arung Palakka became the regional overlord and Bone the dominant kingdom. Political and cultural development seems to have slowed as a result of the status quo. In 1905 the entire island became part of the Dutch state colony of the Netherlands East Indies until Japanese occupation in World War II. During the Indonesian National Revolution, the Dutch Captain 'Turk' Westerling led campaigns in which hundreds, maybe thousands were executed during the South Sulawesi Campaign. Following the transfer of sovereignty in December 1949, Sulawesi became part of the federal United States of Indonesia, which in 1950 became absorbed into the unitary Republic of Indonesia.
Central Sulawesi.
The Portuguese were rumoured to have a fort in Parigi in 1555 (Balinese of Parigi, Central Sulawesi (Davis 1976), however she gives no source). The Kaili were an important group based in the Palu valley and related to the Toraja. Scholars relate that their control swayed under Ternate and Makassar, but this might have been a decision by the Dutch to give their vassals a chance to govern a difficult group. Padbruge commented that in the 1700 Kaili numbers were significant and a highly militant society. In the 1850s a war erupted between the Kaili groups, including the Banawa, in which the Dutch decided to intervene. A complex conflict also involving the Sulu Island pirates and probably Wyndham (a British merchant who commented on being involved in arms dealing to the area in this period and causing a row).
In the late 19th century the Sarasins journeyed through the Palu valley as part of a major initiative to bring the Kaili under Dutch rule. Some very surprising and interesting photographs were taken of shamen called Tadulako. Further Christian religious missions entered the area to make one of the most detailed ethnographic studies in the early 20th century (Kruyt & Adriani). A Swede by the name of Walter Kaudern later studied much of the literature and produced a synthesis. Erskine Downs in the 1950s produced a summary of Kruyts and Andrianis work: "The religion of the Bare'e-Speaking Toradja of Central Celebes," which is invaluable for English-speaking researchers. One of the most recent publications is "When the bones are left," a study of the material culture of central Sulawesi (Eija-Maija Kotilainen – History – 1992), offering extensive analysis. Also worthy of study is the brilliant works of Monnig Atkinson on the Wana shamen who live in the Mori area.
Geography.
Sulawesi is the world's eleventh-largest island, covering an area of 174600 km². The island is surrounded by Borneo to the west, by the Philippines to the north, by Maluku to the east, and by Flores and Timor to the south. It has a distinctive shape, dominated by four large peninsulas: the Semenanjung Minahassa; the East Peninsula; the South Peninsula; and the South-east Peninsula. The central part of the island is ruggedly mountainous, such that the island's peninsulas have traditionally been remote from each other, with better connections by sea than by road. Three bays dominate the island: Gulf of Tomini, Tolo Gulf, and Bone Gulf, while the Strait of Makassar runs the western side of the island.
Minor islands.
The Selayar Islands make up a peninsula stretching southwards from Southwest Sulawesi into the Flores Sea are administratively part of Sulawesi. The Sangihe Islands and Talaud Islands stretch northward from the northeastern tip of Sulawesi, while Buton Island and its neighbours lie off its southeast peninsula, the Togian Islands are in the Gulf of Tomini, and Peleng Island and Banggai Islands form a cluster between Sulawesi and Maluku. All the above-mentioned islands, and many smaller ones, are administratively part of Sulawesi's six provinces.
Population.
The 2000 census population of the provinces of Sulawesi was 14,946,488, about 7.25% of Indonesia's total population. By the 2010 Census the total had reached 17,371,782, and the latest official estimate (for January 2014) is 18,455,058. The largest city is Makassar.
Religion.
Islam is the majority religion in Sulawesi. The conversion of the lowlands of the south western peninsula (South Sulawesi) to Islam occurred in the early 17th century. The kingdom of Luwu in the Gulf of Bone was the first to accept Islam in February 1605; the Makassar kingdom of Goa-Talloq, centered on the modern-day city of Makassar, followed suit in September. However, the Gorontalo and the Mongondow peoples of the northern peninsula largely converted to Islam only in the 19th century. Most Muslims are Sunnis.
Christians form a substantial minority on the island. According to the demographer Toby Alice Volkman, 17% of Sulawesi's population is Protestant and less than 2% is Roman Catholic. Christians are concentrated on the tip of the northern peninsula around the city of Manado, which is inhabited by the Minahasa, a predominantly Protestant people, and the northernmost Sangir and Talaud Islands. The famous Toraja people of Tana Toraja in Central Sulawesi have largely converted to Christianity since Indonesia's independence. There are also substantial numbers of Christians around Lake Poso in Central Sulawesi, among the Pamona speaking peoples of Central Sulawesi, and near Mamasa.
Though most people identify themselves as Muslims or Christians, they often subscribe to local beliefs and deities as well. It is not uncommon for Christians to make offerings to local gods, goddesses, and spirits.
Smaller communities of Buddhists and Hindus are also found on Sulawesi, usually among the Chinese, Balinese and Indian communities.
Administration.
The island is subdivided into six provinces: Gorontalo, West Sulawesi, South Sulawesi, Central Sulawesi, Southeast Sulawesi, and North Sulawesi. West Sulawesi is the newest province, created in 2004 from part of South Sulawesi. The largest cities on the island are Makassar, Manado, Palu, Kendari, Bitung, Gorontalo, Palopo and Bau-Bau.
Capital city of South Sulawesi, and Biggest city in Sulawesi, Makassar.
Flora and fauna.
Sulawesi is part of Wallacea, meaning that it has a mix of both Asian and Australasian species. There are 8 national parks on the island, of which 4 are mostly marine. The parks with the largest terrestrial area are Bogani Nani Wartabone with 2,871 km² and Lore Lindu National Park with 2,290 km². Bunaken National Park which protects a rich coral ecosystem has been proposed as an UNESCO World Heritage Site.
Mammals.
There are 127 known mammalian species in Sulawesi. A large percentage of these mammals, 62% (79 species) are endemic, meaning that they are found nowhere else in Indonesia or the world. The largest native mammals in Sulawesi are the two species of anoa or dwarf buffalo. Other mammalian species inhabiting Sulawesi are the babirusas, which are aberrant pigs, the Sulawesi palm civet, and primates including a number of tarsiers (the spectral, Dian's, Lariang and pygmy species) and several species of macaque, including the crested black macaque, the moor macaque and the booted macaque. Although virtually all Sulawesi's mammals are placental, and generally have close relatives in Asia, several species of cuscus, marsupials of Australasian origin, are also present.
Birds.
By contrast, Sulawesian bird species tend to be found on other nearby islands as well, such as Borneo; 31% of Sulawesi's birds are found nowhere else. One endemic bird is the largely ground-dwelling, chicken-sized maleo, a megapode which uses hot sand close to the island's volcanic vents to incubate its eggs. There are around 350 known bird species in Sulawesi. An international partnership of conservationists, donors, and local people have formed the Alliance for Tompotika Conservation, in an effort to raise awareness and protect the nesting grounds of these birds on the central-eastern arm of the island.
Freshwater fishes.
Sulawesi also has several endemic species of freshwater fish, such as those in the genus "Nomorhamphus", a species flock of viviparous halfbeaks containing 12 species that only are found on Sulawesi (others are from the Philippines). In addition to "Nomorhamphus", the majority of Sulawesi's 70+ freshwater fish species are ricefishes, gobies ("Glossogobius" and "Mugilogobius") and Telmatherinid sail-fin silversides. The last family is almost entirely restricted to Sulawesi, especially the Malili Lake system, consisting of Matano and Towuti, and the small Lontoa (Wawantoa), Mahalona and Masapi. Another unusual endemic is "Lagusia micracanthus" from rivers in South Sulawesi, which is the sole member of its genus and among the smallest grunters. The gudgeon "Bostrychus microphthalmus" from the Maros Karst is the only described species of cave-adapted fish from Sulawesi, but an apparently undescribed species from the same region and genus also exists.
Freshwater crustaceans and snails.
There are many species of "Caridina" freshwater shrimp and parathelphusid freshwater crabs ("Migmathelphusa", "Nautilothelphusa", "Parathelphusa", "Sundathelphusa" and "Syntripsa") that are endemic to Sulawesi. Several of these species have become very popular in the aquarium hobby, and since most are restricted to a single lake system, they are potentially vulnerable to habitat loss and overexploitation. There are also several endemic cave-adapted shrimp and crabs, especially in the Maros Karst. This includes "Cancrocaeca xenomorpha", which has been called the "most highly cave-adapted species of crab known in the world".
The genus "Tylomelania" of freshwater snails is also endemic to Sulawesi, with the majority of the species restricted to Lake Poso and the Malili Lake system.
Miscellaneous.
mimic octopus
Conservation.
The island was recently the subject of an Ecoregional Conservation Assessment, coordinated by The Nature Conservancy. Detailed reports about the vegetation of the island are available. The assessment produced a detailed and annotated list of 'conservation portfolio' sites. This information was widely distributed to local government agencies and nongovernmental organizations. Detailed conservation priorities have also been outlined in a recent publication.
The lowland forests on the island have mostly been removed. Because of the relative geological youth of the island and its dramatic and sharp topography, the lowland areas are naturally limited in their extent. The past decade has seen dramatic conversion of this rare and endangered habitat. The island also possesses one of the largest outcrops of serpentine soil in the world, which support an unusual and large community of specialized plant species. Overall, the flora and fauna of this unique center of global biodiversity is very poorly documented and understood and remains critically threatened.
Environment.
The largest environmental issue in Sulawesi is deforestation. In 2007, scientists found that 80 percent of Sulawesi's forest had been lost or degraded, especially centered in the lowlands and the mangroves. Forests have been felled for logging and large agricultural projects. Loss of forest has resulted in many of Sulawesi's endemic species becoming endangered. In addition, 99 percent of Sulawesi's wetlands have been lost or damaged.
Other environmental threats included bushmeat hunting and mining.
Parks.
The island of Sulawesi has six national parks and nineteen nature reserves. In addition, Sulawesi has three marine protected areas. Many of Sulawesi's parks are threatened by logging, mining, and deforestation for agriculture.
References.
This article incorporates CC-BY-4.0 text from the reference

</doc>
<doc id="28741" url="http://en.wikipedia.org/wiki?curid=28741" title="Southeast Asia">
Southeast Asia

Southeast Asia or Southeastern Asia is a subregion of Asia, consisting of the countries that are geographically south of China, east of India, west of New Guinea and north of Australia. The region lies near the intersection of geological plates, with heavy seismic and volcanic activity. Southeast Asia consists of two geographic regions:
The major religions are Islam, Buddhism and Christianity. However, a wide variety of religions are found throughout the region, including Hinduism and many animist-influenced practices.
Divisions.
Political.
Definitions of "Southeast Asia" vary, but most definitions include the area represented by the countries (sovereign states and dependent territories) listed below. All of the states excluding East Timor are members of the Association of Southeast Asian Nations (ASEAN). The area, together with part of South Asia, was widely known as the East Indies or simply the Indies until the 20th century. Christmas Island and are considered part of Southeast Asia though they are governed Sovereignty issues exist over some territories in the South China Sea. In some occasions, Hong Kong, Macau, and Taiwan, the three disputed regions or nations, are considered as part of the Southeast Asia. Papua New Guinea has stated that it might join ASEAN, and is currently an observer. 
Geographical.
Southeast Asia is geographically divided into two subregions, namely Mainland Southeast Asia (or Indochina) and Maritime Southeast Asia (or the similarly defined Malay Archipelago) (Indonesian: "Nusantara").
Mainland Southeast Asia includes:
Maritime Southeast Asia includes:
The Andaman and Nicobar Islands of India are geographically considered part of Southeast Asia. Eastern Bangladesh and the Seven Sister States of India are culturally part of Southeast Asia and sometimes considered both South Asian and Southeast Asian. The Seven Sister States of India are also geographically part of Southeast Asia. The rest of the island of New Guinea which is not part of Indonesia, namely, Papua New Guinea, is sometimes included so are Palau, Guam, and the Northern Mariana Islands, which were all part of the Spanish East Indies.
The eastern half of Indonesia and East Timor (east of the Wallace Line) are considered to be biogeographically part of Oceania.
History.
Prehistory.
"Homo sapiens" reached the region by around 45,000 years ago, having moved eastwards from the Indian subcontinent. "Homo floresiensis" also lived in the area up until 12,000 years ago, when they became extinct. Austronesian people, who form the majority of the modern population in Indonesia, Malaysia, Brunei, East Timor, and the Philippines, may have migrated to Southeast Asia from Taiwan. They arrived in Indonesia around 2000 BC,and as they spread through the archipelago, they often settled along coastal areas and confined indigenous peoples such as Negritos of the Philippines or Papuans of New Guinea to inland regions.
Studies presented by HUGO (Human Genome Organisation) through genetic studies of the various peoples of Asia, empirically points out that instead of the other way around, another migration from the south first entered Southeast Asia and then travelled slowly northwards.
Solheim and others have shown evidence for a "Nusantao" ("Nusantara") maritime trading network ranging from Vietnam to the rest of the archipelago as early as 5000 BC to 1 AD. The peoples of Southeast Asia, especially those of Austronesian descent, have been seafarers for thousands of years, some reaching the island of Madagascar. Their vessels, such as the vinta, were ocean-worthy. Magellan's voyage records how much more manoeuvrable their vessels were, as compared to the European ships.
Passage through the Indian Ocean aided the colonisation of Madagascar by the Austronesian people, as well as commerce between West Asia and Southeast Asia. Gold from Sumatra is thought to have reached as far west as Rome, while a slave from the Sulu Sea was believed to have been used in Magellan's voyage as a translator.
Originally most people were animist. This was later replaced by Hinduism. Theravada Buddhism soon followed in 525. In the 15th century, Islamic influences began to enter. This forced the last Hindu court in Indonesia to retreat to Bali.
In Mainland Southeast Asia, Burma, Cambodia and Thailand retained the Theravada form of Buddhism, brought to them from Sri Lanka. This type of Buddhism was fused with the Hindu-influenced Khmer culture.
Indianised kingdoms.
Very little is known about Southeast Asian religious beliefs and practices before the advent of Indian merchants and religious influences from the 2nd century BCE onwards. Prior to the 13th century CE, Hinduism and Buddhism were the main religions in Southeast Asia.
The Jawa Dwipa Hindu kingdom in Java and Sumatra existed around 200 BCE. The history of the Malay-speaking world began with the advent of Indian influence, which dates back to at least the 3rd century BCE. Indian traders came to the archipelago both for its abundant forest and maritime products and to trade with merchants from China, who also discovered the Malay world at an early date. Both Hinduism and Buddhism were well established in the Malay Peninsula by the beginning of the 1st century CE, and from there spread across the archipelago.
Cambodia was first influenced by Hinduism during the beginning of the Funan kingdom. Hinduism was one of the Khmer Empire's official religions. Cambodia is the home to one of the only two temples dedicated to Brahma in the world. Angkor Wat is also a famous Hindu temple of Cambodia.
The Champa civilisation was located in what is today central Vietnam, and was a highly Indianised Hindu Kingdom. The Vietnamese laucnhed a massive conquest against the Cham people during the 1471 Vietnamese invasion of Champa, ransacking and burning Champa, slaughtering thousands of Cham people, and forcibly assimilating them into Vietnamese culture.
The Majapahit Empire was an Indianised kingdom based in eastern Java from 1293 to around 1500. Its greatest ruler was Hayam Wuruk, whose reign from 1350 to 1389 marked the empire's peak when it dominated other kingdoms in the southern Malay Peninsula, Borneo, Sumatra, and Bali. Various sources such as the Nagarakertagama also mention that its influence spanned over parts of Sulawesi, Maluku, and some areas of western New Guinea and the Philippines, making it the largest empire to ever exist in Southeast Asian history.
The Cholas excelled in maritime activity in both military and the mercantile fields. Their raids of Kedah and the Srivijaya, and their continued commercial contacts with the Chinese Empire, enabled them to influence the local cultures. Many of the surviving examples of the Hindu cultural influence found today throughout Southeast Asia are the result of the Chola expeditions.
Spread of Islam.
In the 11th century, a turbulent period occurred in the history of Maritime Southeast Asia. The Indian Chola navy crossed the ocean and attacked the Srivijaya kingdom of Sangrama Vijayatungavarman in Kadaram (Kedah), the capital of the powerful maritime kingdom was sacked and the king was taken captive. Along with Kadaram, Pannai in present day Sumatra and Malaiyur and the Malayan peninsula were attacked too. Soon after that, the king of Kedah Phra Ong Mahawangsa became the first ruler to abandon the traditional Hindu faith, and converted to Islam with the Sultanate of Kedah established in year 1136. Samudera Pasai converted to Islam in the year 1267, the King of Malacca Parameswara married the princess of Pasai, and the son became the first sultan of Malacca. Soon, Malacca became the center of Islamic study and maritime trade, and other rulers followed suit. Indonesian religious leader and Islamic scholar Hamka (1908–1981) wrote in 1961: "The development of Islam in Indonesia and Malaya is intimately related to a Chinese Muslim, Admiral Zheng He."
There are several theories to the Islamisation process in Southeast Asia. Another theory is trade. The expansion of trade among West Asia, India and Southeast Asia helped the spread of the religion as Muslim traders from Southern Yemen (Hadramout) brought Islam to the region with their large volume of trade. Many settled in Indonesia, Singapore, and Malaysia. This is evident in the Arab-Indonesian, Arab-Singaporean, and Arab-Malay populations who were at one time very prominent in each of their countries. The second theory is the role of missionaries or Sufis. The Sufi missionaries played a significant role in spreading the faith by introducing Islamic ideas to the region. Finally, the ruling classes embraced Islam and that further aided the permeation of the religion throughout the region. The ruler of the region's most important port, Malacca Sultanate, embraced Islam in the 15th century, heralding a period of accelerated conversion of Islam throughout the region as Islam provided a positive force among the ruling and trading classes.
Trade and colonisation.
China.
Records from Magellan's voyage show that Brunei possessed more cannon than the European ships, so the Chinese must have been trading with them.
Malaysian legend has it that a Chinese Ming emperor sent a princess, Hang Li Po, to Malacca, with a retinue of 500, to marry Sultan Mansur Shah after the emperor was impressed by the wisdom of the sultan. Han Li Po's well (constructed 1459) is now a tourist attraction there, as is Bukit Cina, where her retinue settled.
The strategic value of the Strait of Malacca, which was controlled by Sultanate of Malacca in the 15th and early 16th century, did not go unnoticed by Portuguese writer Duarte Barbosa, who in 1500 wrote "He who is lord of Malacca has his hand on the throat of Venice".
From 111 BC to 938 AD northern Vietnam was under Chinese rule. Vietnam was successfully governed by a series of Chinese dynasties including the Han, Eastern Han, Eastern Wu, Cao Wei, Jin, Liu Song, Southern Qi, Liang, Sui, Tang, and Southern Han.
Europe.
Western influence started to enter in the 16th century, with the arrival of the Portuguese and Spanish in Maluku and the Philippines. Later the Dutch established the Dutch East Indies; the French Indochina; and the British Strait Settlements. Later, all Southeast Asian countries were colonised except for Thailand.
European explorers were reaching Southeast Asia from the west and from the east. Regular trade between the ships sailing east from the Indian Ocean and south from mainland Asia provided goods in return for natural products, such as honey and hornbill beaks from the islands of the archipelago.
Europeans brought Christianity allowing Christian missionaries to become widespread. Thailand also allowed Western scientists to enter its country to develop its own education system as well as start sending Royal members and Thai scholars to get higher education from Europe and Russia.
Japan.
During World War II, Imperial Japan invaded most of the former western colonies. The Shōwa occupation regime committed violent actions against civilians such as the Manila massacre and the implementation of a system of forced labour, such as the one involving 4 to 10 million "romusha" in Indonesia. A later UN report stated that four million people died in Indonesia as a result of famine and forced labour during the Japanese occupation. The Allied powers who defeated Japan in the South-East Asian theatre of World War II then contended with nationalists to whom the occupation authorities had granted independence.
Present.
Most countries in the region enjoy national autonomy. Democratic forms of government and the recognition of human rights are taking root. ASEAN provides a framework for the integration of commerce, and regional responses to international concerns.
Conflicting claims over the Spratly Islands are made by Brunei, China, Malaysia, Philippines, Taiwan, and Vietnam.
Geography.
Indonesia is the largest country in Southeast Asia and it also the largest archipelago in the world by size (according to the CIA World Factbook). Geologically, the Indonesian Archipelago is one of the most volcanically active regions in the world. Geological uplifts in the region have also produced some impressive mountains, culminating in Puncak Jaya in Papua, Indonesia at 5,030 m, on the island of New Guinea; it is the only place where ice glaciers can be found in Southeast Asia. The second tallest peak is Mount Kinabalu in Sabah, Malaysia on the island of Borneo with a height of 4,095 m. The highest mountain in Southeast Asia is Hkakabo Razi at 5,967 meters and can be found in northern Burma sharing the same range of its parent peak, Mount Everest. 
Mayon Volcano, despite being dangerously active, holds the record of the world's most perfect cone which is built from past and continuous eruption.
Boundaries.
Southeast Asia is bounded to the southeast by the Australian continent, a boundary which runs through Indonesia. But a cultural touch point lies between Papua New Guinea and the Indonesian region of the Papua and West Papua, which shares the island of New Guinea with Papua New Guinea.
Climate.
The climate in Southeast Asia is mainly tropical–hot and humid all year round with plentiful rainfall. Northern Vietnam and the Myanmar Himalayas are the only regions in Southeast Asia that feature a subtropical climate, which has a cold winter with snow. The majority of Southeast Asia has a wet and dry season caused by seasonal shift in winds or monsoon. The tropical rain belt causes additional rainfall during the monsoon season. The rain forest is the second largest on earth (with the Amazon being the largest). An exception to this type of climate and vegetation is the mountain areas in the northern region, where high altitudes lead to milder temperatures and drier landscape. Other parts fall out of this climate because they are desert like.
Environment.
The vast majority of Southeast Asia falls within the warm, humid tropics, and its climate generally can be characterised as monsoonal.
The animals of Southeast Asia are diverse; on the islands of Borneo and Sumatra, the orangutan, the Asian elephant, the Malayan tapir, the Sumatran rhinoceros and the Bornean clouded leopard can be also found. Six subspecies of the binturong or "bearcat" exist in the region, though the one endemic to the island of Palawan is now classed as vulnerable.
Tigers of three different subspecies are found on the island of Sumatra (the Sumatran tiger), in peninsular Malaysia (the Malayan tiger), and in Indochina (the Indochinese tiger); all of which are endangered species.
The Komodo dragon is the largest living species of lizard and inhabits the islands of Komodo, Rinca, Flores, and Gili Motang in Indonesia.
The Philippine eagle is the national bird of the Philippines. It is considered by scientists as the largest eagle in the world, and is endemic to the Philippines' forests.
The wild Asian water buffalo, and on various islands related dwarf species of "Bubalus" such as anoa were once widespread in Southeast Asia; nowadays the domestic Asian water buffalo is common across the region, but its remaining relatives are rare and endangered.
The mouse deer, a small tusked deer as large as a toy dog or cat, mostly can be found on Sumatra, Borneo (Indonesia) and in Palawan Islands (Philippines). The gaur, a gigantic wild ox larger than even wild water buffalo, is found mainly in Indochina. There is very little scientific information available regarding Southeast Asian amphibians.
Birds such as the peafowl and drongo live in this subregion as far east as Indonesia. The babirusa, a four-tusked pig, can be found in Indonesia as well. The hornbill was prized for its beak and used in trade with China. The horn of the rhinoceros, not part of its skull, was prized in China as well.
The Indonesian Archipelago is split by the Wallace Line. This line runs along what is now known to be a tectonic plate boundary, and separates Asian (Western) species from Australasian (Eastern) species. The islands between Java/Borneo and Papua form a mixed zone, where both types occur, known as Wallacea. As the pace of development accelerates and populations continue to expand in Southeast Asia, concern has increased regarding the impact of human activity on the region's environment. A significant portion of Southeast Asia, however, has not changed greatly and remains an unaltered home to wildlife. The nations of the region, with only few exceptions, have become aware of the need to maintain forest cover not only to prevent soil erosion but to preserve the diversity of flora and fauna. Indonesia, for example, has created an extensive system of national parks and preserves for this purpose. Even so, such species as the Javan rhinoceros face extinction, with only a handful of the animals remaining in western Java.
The shallow waters of the Southeast Asian coral reefs have the highest levels of biodiversity for the world's marine ecosystems, where coral, fish and molluscs abound. According to Conservation International, marine surveys suggest that the marine life diversity in the Raja Ampat (Indonesia) is the highest recorded on Earth. Diversity is considerably greater than any other area sampled in the Coral Triangle composed of Indonesia, Philippines, and Papua New Guinea. The Coral Triangle is the heart of the world's coral reef biodiversity, making Raja Ampat quite possibly the richest coral reef ecosystems in the world. The whale shark, the world's largest species of fish and 6 species of sea turtles can also be found in the South China Sea and the Pacific Ocean territories of the Philippines.
The trees and other plants of the region are tropical; in some countries where the mountains are tall enough, temperate-climate vegetation can be found. These rainforest areas are currently being logged-over, especially in Borneo.
While Southeast Asia is rich in flora and fauna, Southeast Asia is facing severe deforestation which causes habitat loss for various endangered species such as orangutan and the Sumatran tiger. Predictions have been made that more than 40% of the animal and plant species in Southeast Asia could be wiped out in the 21st century. At the same time, haze has been a regular occurrence. The two worst regional hazes were in 1997 and 2006 in which multiple countries were covered with thick haze, mostly caused by "slash and burn" activities in Sumatra and Borneo. In reaction, several countries in Southeast Asia signed the ASEAN Agreement on Transboundary Haze Pollution to combat haze pollution.
The 2013 Southeast Asian Haze saw API levels reach a hazardous level in some countries. Muar experienced the highest API level of 746 on 23 June 2013 at around 7 am.
Economy.
Even prior to the penetration of European interests, Southeast Asia was a critical part of the world trading system. A wide range of commodities originated in the region, but especially important were spices such as pepper, ginger, cloves, and nutmeg. The spice trade initially was developed by Indian and Arab merchants, but it also brought Europeans to the region. First Spaniards (Manila galleon) and Portuguese, then the Dutch, and finally the British and French became involved in this enterprise in various countries. The penetration of European commercial interests gradually evolved into annexation of territories, as traders lobbied for an extension of control to protect and expand their activities. As a result, the Dutch moved into Indonesia, the British into Malaya and parts of Borneo, the French into Indochina, and the Spanish and the US into the Philippines.
The overseas Chinese community has played a large role in the development of the economies in the region. These business communities are connected through the bamboo network, a network of overseas Chinese businesses operating in the markets of Southeast Asia that share common family and cultural ties. The origins of Chinese influence can be traced to the 16th century, when Chinese migrants from southern China settled in Indonesia, Thailand, and other Southeast Asian countries. Chinese populations in the region saw a rapid increase following the Communist Revolution in 1949, which forced many refugees to emigrate outside of China.
The region's economy greatly depends on agriculture; rice and rubber have long been prominent exports. Manufacturing and services are becoming more important. An emerging market, Indonesia is the largest economy in this region. Newly industrialised countries include Indonesia, Malaysia, Thailand, and the Philippines, while Singapore and Brunei are affluent developed economies. The rest of Southeast Asia is still heavily dependent on agriculture, but Vietnam is notably making steady progress in developing its industrial sectors. The region notably manufactures textiles, electronic high-tech goods such as microprocessors and heavy industrial products such as automobiles. Oil reserves in Southeast Asia are plentiful.
Seventeen telecommunications companies contracted to build the Asia-America Gateway submarine cable to connect Southeast Asia to the US This is to avoid disruption of the kind recently caused by the cutting of the undersea cable from Taiwan to the US in the 2006 Hengchun earthquake.
Tourism has been a key factor in economic development for many Southeast Asian countries, especially Cambodia. According to UNESCO, "tourism, if correctly conceived, can be a tremendous development tool and an effective means of preserving the cultural diversity of our planet." Since the early 1990s, "even the non-ASEAN nations such as Cambodia, Laos, Vietnam and Burma, where the income derived from tourism is low, are attempting to expand their own tourism industries." In 1995, Singapore was the regional leader in tourism receipts relative to GDP at over 8%. By 1998, those receipts had dropped to less than 6% of GDP while Thailand and Lao PDR increased receipts to over 7%. Since 2000, Cambodia has surpassed all other ASEAN countries and generated almost 15% of its GDP from tourism in 2006.
Indonesia is the only member of G-20 major economies and is the largest economy in the region. Indonesia's estimated gross domestic product (nominal) for 2008 was US$511.7 billion with estimated nominal per capita GDP was US$2,246, and per capita GDP PPP was US$3,979 (international dollars).
Stock markets in Southeast Asia have performed better than other bourses in the Asia-Pacific region in 2010, with the Philippines' PSE leading the way with 22 percent growth, followed by Thailand's SET with 21 percent and Indonesia's JKSE with 19 percent.
Demographics.
Southeast Asia has an area of approximately 4,000,000 km2 (1.6 million square miles). As of 2007, more than 593 million people lived in the region, more than a fifth of them (125 million) on the Indonesian island of Java, the most densely populated large island in the world. Indonesia is the most populous country with 230 million people and also the 4th most populous country in the world. The distribution of the religions and people is diverse in Southeast Asia and varies by country. Some 30 million overseas Chinese also live in Southeast Asia, most prominently in Christmas Island, Indonesia, Malaysia, the Philippines, Singapore, and Thailand, and also, as the Hoa, in Vietnam.
Ethnic groups.
In modern times, the Javanese are the largest ethnic group in Southeast Asia, with more than 100 million people, mostly concentrated in Java, Indonesia. In Burma, the Burmese account for more than two-thirds of the ethnic stock in this country, while ethnic Thais and Vietnamese account for about four-fifths of the respective populations of those countries. Indonesia is clearly dominated by the Javanese and Sundanese ethnic groups, while Malaysia is split between half Malays and one-quarter Chinese. Within the Philippines, the Tagalog, Cebuano, Ilocano, and Hiligaynon groups are significant.
Religion.
Islam is the most widely practised religion in Southeast Asia, numbering approximately 240 million adherents which translate to about 40% of the entire population, with majorities in Brunei, Indonesia, Malaysia and in Southern Philippines. Countries in Southeast Asia practice many different religions. Buddhism is predominant in Thailand, Cambodia, Laos, Burma, Vietnam and Singapore. Ancestor worship and Confucianism are also widely practised in Vietnam and Singapore. Christianity is predominant in the Philippines, eastern Indonesia, East Malaysia and East Timor. The Philippines has the largest Roman Catholic population in Asia. East Timor is also predominantly Roman Catholic due to a history of Portuguese rule.
The religious composition for each country is as follows: Some values are taken from the "CIA World Factbook":
Religions and peoples are diverse in Southeast Asia and not one country is homogeneous. In the world's most populous Muslim nation, Indonesia, Hinduism is dominant on islands such as Bali. Christianity also predominates in the rest of the part of the Philippines, New Guinea and Timor. Pockets of Hindu population can also be found around Southeast Asia in Singapore, Malaysia etc. Garuda (Sanskrit: Garuḍa), the phoenix who is the mount (vahanam) of Vishnu, is a national symbol in both Thailand and Indonesia; in the Philippines, gold images of Garuda have been found on Palawan; gold images of other Hindu gods and goddesses have also been found on Mindanao. Balinese Hinduism is somewhat different from Hinduism practised elsewhere, as Animism and local culture is incorporated into it. Christians can also be found throughout Southeast Asia; they are in the majority in East Timor and the Philippines, Asia's largest Christian nation. In addition, there are also older tribal religious practices in remote areas of Sarawak in East Malaysia,Highland Philippines and Papua in eastern Indonesia. In Burma, Sakka (Indra) is revered as a "nat". In Vietnam, Mahayana Buddhism is practised, which is influenced by native animism but with strong emphasis on Ancestor Worship.
Languages.
Each of the languages have been influenced by cultural pressures due to trade, immigration, and historical colonisation as well.
The language composition for each country is as follows: (official languages are in bold.)
Culture.
The culture in Southeast Asia is very diverse: on mainland Southeast Asia, the culture is a mix of Indian (Burma, Cambodia, Laos, western Malaysia and Thailand) and Chinese (Singapore and Vietnam). While in Indonesia, eastern Malaysia, and the Philippines, the culture is a mix of indigenous Austronesian, Indian, Islamic, Western, and Chinese cultures. Also Brunei shows a strong influence from Arabia. Singapore and Vietnam show more Chinese influence in that Singapore, although being geographically a Southeast Asian nation, is home to a large Chinese majority and Vietnam was in China's sphere of influence for much of its history. Indian influence in Singapore is only evident through the Tamil migrants, which influenced, to some extent, the cuisine of Singapore. Throughout Vietnam's history, it has had no direct influence from India - only through contact with the Thai, Khmer and Cham peoples.
Rice paddy agriculture has existed in Southeast Asia for thousands of years, ranging across the subregion. Some dramatic examples of these rice paddies populate the Banaue Rice Terraces in the mountains of Luzon in the Philippines. Maintenance of these paddies is very labour-intensive. The rice paddies are well-suited to the monsoon climate of the region.
Stilt houses can be found all over Southeast Asia, from Thailand and Laos, to Borneo, to Luzon in the Philippines, to Papua New Guinea. The region has diverse metalworking, especially in Indonesia. This include weaponry, such as the distinctive kris, and musical instruments, such as the gamelan.
Influences.
The region's chief cultural influences have been from some combination of Islam, India, and China. Diverse cultural influence is pronounced in the Philippines, derived particularly from the period of the Spanish and American rule, contact with Indian-influenced cultures, and the Chinese trading era. The Filipinos are of indigenous Austronesian blood with varying admixtures of Indian, Arab, Spanish, and Chinese.
As a rule, the peoples who ate with their fingers were more likely influenced by the culture of India, for example, than the culture of China, where the peoples ate with chopsticks; tea, as a beverage, can be found across the region. The fish sauces distinctive to the region tend to vary.
Arts.
The arts of Southeast Asia have affinity with the arts of other areas. Dance in much of Southeast Asia includes movement of the hands as well as the feet, to express the dance's emotion and meaning of the story that the ballerina is going to tell the audience. Most of Southeast Asia introduced dance into their court; in particular, Cambodian royal ballet represented them in the early 7th century before the Khmer Empire, which was highly influenced by Indian Hinduism. Apsara Dance, famous for strong hand and feet movement, is a great example of Hindu symbolic dance. 
Puppetry and shadow plays were also a favoured form of entertainment in past centuries, a famous one being Wayang from Indonesia. The arts and literature in some of Southeast Asia is quite influenced by Hinduism, which was brought to them centuries ago. Indonesia, despite conversion to Islam which opposes certain forms of art, has retained many forms of Hindu-influenced practices, culture, art and literature. An example is the Wayang Kulit (Shadow Puppet) and literature like the Ramayana. The wayang kulit show has been recognized by UNESCO on November 7 2003, as a Masterpiece of Oral and Intangible Heritage of Humanity.
It has been pointed out that Khmer and Indonesian classical arts were concerned with depicting the life of the gods, but to the Southeast Asian mind the life of the gods was the life of the peoples themselves—joyous, earthy, yet divine. The Tai, coming late into Southeast Asia, brought with them some Chinese artistic traditions, but they soon shed them in favour of the Khmer and Mon traditions, and the only indications of their earlier contact with Chinese arts were in the style of their temples, especially the tapering roof, and in their lacquerware.
Music.
Traditional music in Southeast Asia is as varied as its many ethnic and cultural divisions. Main styles of traditional music can be seen: Court music, folk music, music styles of smaller ethnic groups, and music influenced by genres outside the geographic region.
Of the court and folk genres, gong-chime ensembles and orchestras make up the majority (the exception being lowland areas of Vietnam). "Gamelan" and "Angklung" orchestras from Indonesia, "Piphat" /"Pinpeat" ensembles of Thailand and Cambodia and the "Kulintang" ensembles of the southern Philippines, Borneo, Sulawesi and Timor are the three main distinct styles of musical genres that have influenced other traditional musical styles in the region. String instruments also are popular in the region.
On November 18, 2010, UNESCO officially recognized angklung as a "Masterpiece of Oral and Intangible Heritage of Humanity", and encourage Indonesian people and government to safeguard, transmit, promote performances and to encourage the craftsmanship of angklung making.
Writing.
The history of Southeast Asia has led to a wealth of different authors, from both within and without writing about the region.
Originally, Indians were the ones who taught the native inhabitants about writing. This is shown through Brahmic forms of writing present in the region such as the Balinese script shown on split palm leaf called "lontar" (see image to the left — magnify the image to see the writing on the flat side, and the decoration on the reverse side).
The antiquity of this form of writing extends before the invention of paper around the year 100 in China. Note each palm leaf section was only several lines, written longitudinally across the leaf, and bound by twine to the other sections. The outer portion was decorated. The alphabets of Southeast Asia tended to be abugidas, until the arrival of the Europeans, who used words that also ended in consonants, not just vowels. Other forms of official documents, which did not use paper, included Javanese copperplate scrolls. This material would have been more durable than paper in the tropical climate of Southeast Asia.
In Malaysia, Brunei, and Singapore, the Malay language is now generally written in the Latin script. The same phenomenon is present in Indonesian, although different spelling standards are utilised (e.g. 'Teksi' in Malay and 'Taksi' in Indonesian for the word 'Taxi').
The use of Chinese characters, in the past and present, is only evident in Vietnam and more recently, Singapore and Malaysia. The adoption of Chinese characters in Vietnam dates back to around 111BC, when it was occupied by the Chinese. A Vietnamese script called Chu nom used modified Chinese characters to express the Vietnamese language. Both classical Chinese and Chu Nom were used up until the early 20th century.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="28742" url="http://en.wikipedia.org/wiki?curid=28742" title="Supercontinent">
Supercontinent

In geology, a supercontinent is the assembly of most or all of the Earth's continental blocks or cratons to form a single large landmass. However, the definition of a supercontinent can be ambiguous. Many tectonicists such as Hoffman (1999) use the term "supercontinent" to mean "a clustering of nearly all continents". This definition leaves room for interpretation when labeling a continental body and is easier to apply to Precambrian times. Using the first definition provided here, Gondwana (aka Gondwanaland) is not considered a supercontinent, because the landmasses of Baltica, Laurentia and Siberia also existed at the same time but physically separate from each other. The landmass of Pangaea is the collective name describing all of these continental masses when they were in a close proximity to one another. This would classify Pangaea as a supercontinent. According to the definition by Rogers and Santosh (2004), a supercontinent does not exist today. Supercontinents have assembled and dispersed multiple times in the geologic past (see table). The positions of continents have been accurately determined back to the early Jurassic. However, beyond 200 Ma, continental positions are much less certain.
General chronology.
There are two contrasting models for supercontinent evolution through geological time. The first model theorizes that at least two separate supercontinents existed comprising Vaalbara (from ~3600 to 2500 Ma) and Kenorland (from ~2700 to 2450 Ma). The Neoarchean supercontinent consisted of Superia and Sclavia. These parts of Neoarchean age broke off at ~2300 and 2090 Ma and portions of them later collided to form Nuna (Northern Europe North America) (~1750 Ma). Nuna continued to develop during the Mesoproterozoic, primarily by lateral accretion of juvenile arcs, and in ~1000 Ma Nuna collided with other land masses, forming Rodinia. Between ~800 and 700 Ma Rodinia broke apart. However, before completely breaking up, some fragments of Rodinia had already come together to form Gondwana (also known as Gondwanaland) by ~530 Ma. Pangaea formed by ~300 Ma through the collision of Gondwana, Laurentia, Baltica, and Siberia. 
The second model (Protopangea-Paleopangea) is based on both palaeomagnetic and geological evidence and proposes that the continental crust comprised a single supercontinent from ~2.7 Ga until break-up during the Ediacaran Period after ~0.6 Ga. The reconstruction is derived from the observation that palaeomagnetic poles converge to quasi-static positions for long intervals between ~2.7–2.2, 1.5–1.25, and 0.75–0.6 Ga with only small peripheral modifications to the reconstruction. During the intervening periods, the poles conform to a unified apparent polar wander path. Because this model shows that exceptional demands on the paleomagnetic data are satisfied by prolonged quasi-integrity, it must be regarded as superseding the first model proposing multiple diverse continents, although the first phase (Protopangea) essentially incorporates Vaalbara and Kenorland of the first model. The explanation for the prolonged duration of the Protopangea-Paleopangea supercontinent appears to be that Lid Tectonics (comparable to the tectonics operating on Mars and Venus) prevailed during Precambrian times. Plate Tectonics as seen on the contemporary Earth became dominant only during the latter part of geological times.
The Phanerozoic supercontinent Pangaea began to break up 180 Ma and is still doing so today. Because Pangaea is the most recent of Earth's supercontinents, it is the most well known and understood. Contributing to Pangaea's popularity in the classroom is the fact that its reconstruction is almost as simple as fitting the present continents bordering the Atlantic-type oceans like puzzle pieces.
Supercontinent cycles.
A supercontinent cycle is the break-up of one supercontinent and the development of another, which takes place on a global scale. Supercontinent cycles are not the same as the Wilson cycle, which is the opening and closing of an individual oceanic basin. The Wilson cycle rarely synchronizes with the timing of a supercontinent cycle. However, supercontinent cycles and Wilson cycles were both involved in the creation of Pangaea and Rodinia.
Secular trends such as carbonatites, granulites, eclogites, and greenstone belt deformation events are all possible indicators of Precambrian supercontinent cyclicity, although the Protopangea-Paleopangea solution implies that Phanerozoic style of supercontinent cycles did not operate during these times. Also there are instances where these secular trends have a weak, uneven or lack of imprint on the supercontinent cycle; secular methods for supercontinent reconstruction will produce results that have only one explanation and each explanation for a trend must fit in with the rest.
Supercontinents and volcanism.
The causes of supercontinent assembly and dispersal are thought to be driven by processes in the mantle. Approximately 660 km into the mantle, a discontinuity occurs, affecting the surface crust through processes like plumes and "superplumes". When a slab of crust that is subducted is denser than the surrounding mantle, it sinks to the discontinuity. Once the slabs build up, they will sink through to the lower mantle in what is known as a "slab avalanche". This displacement at the discontinuity will cause the lower mantle to compensate and rise elsewhere. The rising mantle can form a plume or superplume.
Besides having compositional effects on the upper mantle by replenishing the large-ion lithophile elements, volcanism affects the plate movement. The plates will be moved towards a geoidal low perhaps where the slab avalanche occurred and pushed away from the geoidal high that can be caused by the plumes or superplumes. This causes the continents to push together to form supercontinents and was evidently the process that operated to cause the early continental crust to aggregate into Protopangea. Dispersal of supercontinents is caused by the accumulation of heat underneath the crust due to the rising of very large convection cells or plumes, and a massive heat release resulted in the final break-up of Paleopangea. Accretion occurs over geoidal lows that can be caused by avalanche slabs or the downgoing limbs of convection cells. Evidence of the accretion and dispersion of supercontinents is seen in the geological rock record.
The influence of known volcanic eruptions does not compare to that of flood basalts. The timing of flood basalts has corresponded with large-scale continental break-up. However, due to a lack of data on the time required to produce flood basalts, the climatic impact is difficult to quantify. The timing of a single lava flow is also undetermined. These are important factors on how flood basalts influenced paleoclimate.
Supercontinents and plate tectonics.
Global paleogeography and plate interactions as far back as Pangaea are relatively well understood today. However, the evidence becomes more sparse further back in geologic history. Marine magnetic anomalies, passive margin match-ups, geologic interpretation of orogenic belts, paleomagnetism, paleobiogeography of fossils, and distribution of climatically sensitive strata are all methods to obtain evidence for continent locality and indicators of environment throughout time.
Phanerozoic (540 Ma to present) and Precambrian (4.6 Ga to 540 Ma) had primarily passive margins and detrital zircons (and orogenic granites), whereas the tenure of Pangaea contained few. Matching edges of continents are where passive margins form. The edges of these continents may rift. At this point, seafloor spreading becomes the driving force. Passive margins are therefore born during the break-up of supercontinents and die during supercontinent assembly. Pangaea's supercontinent cycle is a good example for the efficiency of using the presence, or lack of, these entities to record the development, tenure, and break-up of supercontinents. There is a sharp decrease in passive margins between 500 and 350 Ma during the timing of Pangaea's assembly. The tenure of Pangaea is marked by a low number of passive margins during 300 to 275 Ma, and its break-up is indicated accurately by an increase in passive margins.
Orogenic belts can form during the assembly of continents and supercontinents. The orogenic belts present on continental blocks are classified into three different categories and have implications of interpreting geologic bodies. Intercratonic orogenic belts are characteristic of ocean basin closure. Clear indicators of intercratonic activity contain ophiolites and other oceanic materials that are present in the suture zone. Intracratonic orogenic belts occur as thrust belts and do not contain any oceanic material. However, the absence of ophiolites is not strong evidence for intracratonic belts, because the oceanic material can be squeezed out and eroded away in an intercratonic environment. The third kind of orogenic belt is a confined orogenic belt which is the closure of small basins. The assembly of a supercontinent would have to show intercratonic orogenic belts. However, interpretation of orogenic belts can be difficult.
The collision of Gondwana and Laurasia occurred in the late Phanerozoic. By this collision, the Variscan mountain range was created, along the equator. This 6000-km-long mountain range is usually referred to in two parts: the Hercynian mountain range of the late Carboniferous makes up the eastern part, and the western part is called the Appalachians, uplifted in the early Permian. (The existence of a flat elevated plateau like the Tibetan Plateau is under much debate.) The locality of the Variscan range made it influential to both the northern and southern hemispheres. The elevation of the Appalachians would greatly influence global atmospheric circulation.
Supercontinental climate.
Continents, in particular large or supercontinents, will affect the climate of the planet drastically. In general the interaction of supercontinents and climate is similar to the interaction between present-day continents and climate, just on a different scale. Supercontinents have a larger effect on climate than do continents. The configuration and placement of the continents has a larger influence on climate. Continents modify global wind patterns, control ocean current paths and have a higher albedo than the oceans. Because continents are higher in the elevation, the temperature decreases with altitude. The wind is redirected by mountains. The albedo difference causes a shift in climate by onshore winds. "Continentality" occurs because the center of large continents are generally higher in elevations and are therefore cooler and dryer. This is seen today with Eurasia, and evidence is present in the rock record that this is true for the middle of Pangaea.
Glacial
The term glacio-epoch refers to a long episode of glaciation on Earth over millions of years. Glaciers have a major implications on the climate particularly through sea level change. Changes in the position and elevation of the continents, the paleolatitude and ocean circulation affect the glacio-epochs. There is an association between the rifting and breakup of continents and supercontinents and glacio-epochs. According to the first model for Precambrian supercontinents described above the breakup of Kenorland and Rodinia were associated with the Paleoproterozoic and Neoproterozoic glacio-epochs, respectively. In contrast, the second solution described above shows that these glaciations correlated with periods of low continental velocity and it is concluded that a fall in tectonic and corresponding volcanic activity was responsible for these intervals of global frigidity. During the accumulation of supercontinents with times of regional uplift, glacio-epochs seem to be rare with little supporting evidence. However, the lack of evidence does not allow for the conclusion that glacio-epochs are not associated with collisional assembly of supercontinents. This could just represent a preservation bias.
During the late Ordovician (~465 Ma), the particular configuration of Gondwana may have allowed for glaciation and high CO2 levels to occur at the same time. However, some geologists disagree and think that there was a temperature increase at this time. This increase may have been strongly influenced by the movement of Gondwana across the South Pole, which may have prevented lengthy snow accumulation. Although late Ordovician temperatures at the South Pole may have reached freezing, there were no ice sheets during the early Silurian (~440 Ma) through the late Mississippian (~330 Ma). Agreement can be met with the theory that continental snow can occur when the edge of a continent is near the pole. Therefore Gondwana, although located tangent to the South Pole, may have experienced glaciation along its coast.
Precipitation
Though precipitation rates during monsoonal circulations are difficult to predict, there is evidence for a large orographic barrier within the interior of Pangaea during the late Paleozoic (~250 Ma). The possibility of the SW-NE trending Appalachian-Hercynian Mountains makes the region's monsoonal circulations potentially relatable to present day monsoonal circulations surrounding the Tibetan Plateau, which is known to positively influence the magnitude of monsoonal periods within Eurasia. It is therefore somewhat expected that lower topography in other regions of the supercontinent during the Jurassic would negatively influence precipitation variations. The breakup of supercontinents may have affected local precipitation. When any supercontinent breaks up, there will be an increase in precipitation runoff over the surface of the continental land masses, increasing silicate weathering and the consumption of CO2.
Temperature
Even though during the Archaean solar radiation was reduced by 30 percent and the Cambrian-Precambrian boundary by six percent, the Earth has only experienced three ice ages throughout the Precambrian. It must be noted that erroneous conclusions are more likely to be made when models are limited to one climatic configuration (which is usually present day).
Cold winters in continental interiors are due to rate ratios of radiative cooling (greater) and heat transport from continental rims. To raise winter temperatures within continental interiors, the rate of heat transport must increase to become greater than the rate of radiative cooling. Through climate models, alterations in atmospheric CO2 content and ocean heat transport are not comparatively effective.
CO2 models suggest that values were low in the late Cenozoic and Carboniferous-Permian glaciations. Although early Paleozoic values are much larger (more than ten percent higher than that of today). This may be due to high seafloor spreading rates after the breakup of Precambrian supercontinents and the lack of land plants as a carbon sink.
During the late Permian, it is expected that seasonal Pangaean temperatures varied drastically. Subtropic summer temperatures were warmer than that of today by as much as 6–10 degrees and mid-latitudes in the winter were less than −30 degrees Celsius. These seasonal changes within the supercontinent were influenced by the large size of Pangaea. And, just like today, coastal regions experienced much less variation.
During the Jurassic, summer temperatures did not raise above zero degrees Celsius along the northern rim of Laurasia, which was the northernmost part of Pangaea (the southernmost portion of Pangaea was Gondwana). Ice-rafted dropstones sourced from Russia are indicators of this northern boundary. The Jurassic is thought to have been approximately 10 degrees Celsius warmer along 90 degrees East paleolongitude compared to the present temperature of today's central Eurasia.
Milankovitch cycles
Many studies of the Milankovitch fluctuations during supercontinent time periods have focused on the Mid-Cretaceous. Present amplitudes of Milankovitch cycles over present day Eurasia may be mirrored in both the southern and northern hemispheres of the supercontinent Pangaea. Climate modeling shows that summer fluctuations varied 14–16 degrees Celsius on Pangaea, which is similar or slightly higher than summer temperatures of Eurasia during the Pleistocene. The largest-amplitude Milankovitch cycles are expected to have been at mid- to high-latitudes during the Triassic and Jurassic.
Proxies.
Granites and detrital zircons have notably similar and episodic appearances in the rock record. Their fluctuations somewhat mirror Precambrian supercontinent cycles. The U–Pb zircon dates from orogenic granites are of the most reliable aging determinants. There are some issues with relying on granite sourced zircons, such as a lack of evenly globally sourced data and the loss of granite zircons by sedimentary coverage or plutonic consumption. Where granite zircons fall short, detrital zircons from sandstones appear and make up for the gaps. These detrital zircons are taken from the sands of major modern rivers and their drainage basins. Oceanic magnetic anomalies and paleomagnetic data are the primary resources used for reconstructing continent and supercontinent locations back to roughly 150 Ma.
Supercontinents and atmospheric gases.
Plate tectonics and the chemical composition of the atmosphere (specifically greenhouse gases) are the two most prevailing factors present within the geologic time scale. Continental drift influences both cold and warm climatic episodes. Atmospheric circulation and climate are strongly influenced by the location and formation of continents and megacontinents. Therefore, continental drift influences mean global temperature.
Oxygen levels of the Archaean Eon were negligible and today they are roughly 21 percent. It is thought that the Earth's oxygen content has risen in stages: six or seven steps that are timed very closely to the development of Earth's supercontinents.
The process of Earth's increase in atmospheric oxygen content is theorized to have started with continent-continent collision of huge land masses forming supercontinents, and therefore possibly supercontinent mountain ranges (supermountains). These supermountains would have eroded, and the mass amounts of nutrients, including iron and phosphorus, would have washed into oceans, just as we see happening today. The oceans would then be rich in nutrients essential to photosynthetic organisms, which would then be able to respire mass amounts of oxygen. (1: continents collide, 2: 'supermountains' form, 3: erosion of 'supermountains,' 4: large quantities of minerals and nutrients washed out to open ocean, 5: explosion of marine algae life (partly sourced from noted nutrients), and 6: mass amounts of oxygen produced during photosynthesis. There is an apparent direct relationship between orogeny and the atmospheric oxygen content). There is also evidence for increased sedimentation concurrent with the timing of these mass oxygenation events, meaning that the organic carbon and pyrite at these times were more likely to be buried beneath sediment and therefore unable to react with the free oxygen. This sustained the atmospheric oxygen increases.
During this time, 2.65 Ga there was an increase in Mo isotope fractionation. It was temporary, but supports the increase in atmospheric oxygen because molybdenum isotopes require free oxygen to fractionate. Between 2.45 and 2.32 Ga, the second period of oxygenation occurred, it has been called the 'great oxygenation event.' There are many pieces of evidence that support the existence of this event, including red bed appearance 2.3 Ga (meaning that Fe3+ was being produced and became an important component in soils). The third oxygenation stage approximately 1.8 Ga is indicated by the disappearance of iron formations. Neodymium isotopic studies suggest that iron formations are usually from continental sources, meaning that dissolved Fe and Fe2+ had to be transported during continental erosion. A rise in atmospheric oxygen prevents Fe transport, so the lack of iron formations may have been due to an increase in oxygen. The fourth oxygenation event, roughly 0.6 Ga, is based on modeled rates of S isotopes from marine carbonate-associated sulfates. An increase (near doubled concentration) of sulfur isotopes, which is suggested by these models, would require an increase in oxygen content of the deep oceans. Between 650 and 550 Ma there were three increases in ocean oxygen levels, this period is the fifth oxygenation stage. One of the reasons indicating this period to be an oxygenation event is the increase in redox-sensitive Mo in black shales. The sixth event occurred between 360 and 260 Ma and was identified by models suggesting shifts in the balance of 34S in sulfates and 13C in carbonates, which were strongly influenced by an increase in atmospheric oxygen.

</doc>
<doc id="28743" url="http://en.wikipedia.org/wiki?curid=28743" title="Slide rule">
Slide rule

The slide rule, also known colloquially in the United States as a slipstick, is a mechanical analog computer. The slide rule is used primarily for multiplication and division, and also for functions such as roots, logarithms and trigonometry, but is not normally used for addition or subtraction. Though similar in name and appearance to a standard ruler, the slide rule is not ordinarily used for measuring length or drawing straight lines.
Slide rules come in a diverse range of styles and generally appear in a linear or circular form with a standardized set of markings (scales) essential to performing mathematical computations. Slide rules manufactured for specialized fields such as aviation or finance typically feature additional scales that aid in calculations common to those fields.
The Reverend William Oughtred and others developed the slide rule in the 17th century based on the emerging work on logarithms by John Napier. Before the advent of the pocket calculator, it was the most commonly used calculation tool in science and engineering. The use of slide rules continued to grow through the 1950s and 1960s even as digital computing devices were being gradually introduced; but around 1974 the electronic scientific calculator made it largely obsolete and most suppliers left the business.
Basic concepts.
In its most basic form, the slide rule uses two logarithmic scales to allow rapid multiplication and division of numbers. These common operations can be time-consuming and error-prone when done on paper. More elaborate slide rules allow other calculations, such as square roots, exponentials, logarithms, and trigonometric functions.
Scales may be grouped in decades, which are numbers ranging from 1 to 10 (i.e. 10n to 10n+1). Thus single decade scales C and D range from 1 to 10 across the entire width of the slide rule while double decade scales A and B range from 1 to 100 over the width of the slide rule.
In general, mathematical calculations are performed by aligning a mark on the sliding central strip with a mark on one of the fixed strips, and then observing the relative positions of other marks on the strips. Numbers aligned with the marks give the approximate value of the product, quotient, or other calculated result.
The user determines the location of the decimal point in the result, based on mental estimation. Scientific notation is used to track the decimal point in more formal calculations. Addition and subtraction steps in a calculation are generally done mentally or on paper, not on the slide rule.
Most slide rules consist of three linear strips of the same length, aligned in parallel and interlocked so that the central strip can be moved lengthwise relative to the other two. The outer two strips are fixed so that their relative positions do not change.
Some slide rules ("duplex" models) have scales on both sides of the rule and slide strip, others on one side of the outer strips and both sides of the slide strip (which can usually be pulled out, flipped over and reinserted for convenience), still others on one side only ("simplex" rules). A sliding with a vertical alignment line is used to find corresponding points on scales that are not adjacent to each other or, in duplex models, are on the other side of the rule. The cursor can also record an intermediate result on any of the scales.
Operation.
Multiplication.
A logarithm transforms the operations of multiplication and division to addition and subtraction according to the rules formula_1 and formula_2.
Moving the top scale to the right by a distance of formula_3, by matching the beginning of the top scale with the label formula_4 on the bottom, aligns each number formula_5, at position formula_6 on the top scale, with the number at position formula_7 on the bottom scale. Because formula_8, this position on the bottom scale gives formula_9, the product of formula_4 and formula_5. For example, to calculate 3×2, the 1 on the top scale is moved to the 2 on the bottom scale. The answer, 6, is read off the bottom scale where 3 is on the top scale. In general, the 1 on the top is moved to a factor on the bottom, and the answer is read off the bottom where the other factor is on the top. This works because the distances from the "1" are proportional to the logarithms of the marked values:
Operations may go "off the scale;" for example, the diagram above shows that the slide rule has not positioned the 7 on the upper scale above any number on the lower scale, so it does not give any answer for 2×7. In such cases, the user may slide the upper scale to the left until its right index aligns with the 2, effectively dividing by 10 (by subtracting the full length of the C-scale) and then multiplying by 7, as in the illustration below:
Here the user of the slide rule must remember to adjust the decimal point appropriately to correct the final answer. We wanted to find 2×7, but instead we calculated (2/10)×7=0.2×7=1.4. So the true answer is not 1.4 but 14. Resetting the slide is not the only way to handle multiplications that would result in off-scale results, such as 2×7; some other methods are:
Method 1 is easy to understand, but entails a loss of precision. Method 3 has the advantage that it only involves two scales.
Division.
The illustration below demonstrates the computation of 5.5/2. The 2 on the top scale is placed over the 5.5 on the bottom scale. The 1 on the top scale lies above the quotient, 2.75. There is more than one method for doing division, but the method presented here has the advantage that the final result cannot be off-scale, because one has a choice of using the 1 at either end.
Other operations.
In addition to the logarithmic scales, some slide rules have other mathematical functions encoded on other auxiliary scales. The most popular were trigonometric, usually sine and tangent, common logarithm (log) (for taking the log of a value on a multiplier scale), natural logarithm (ln) and exponential ("ex") scales. Some rules include a Pythagorean scale, to figure sides of triangles, and a scale to figure circles. Others feature scales for calculating hyperbolic functions. On linear rules, the scales and their labeling are highly standardized, with variation usually occurring only in terms of which scales are included and in what order:
The Binary Slide Rule manufactured by Gilson in 1931 performed an addition and subtraction function limited to fractions.
Roots and powers.
There are single-decade (C and D), double-decade (A and B), and triple-decade (K) scales. To compute formula_12, for example, locate x on the D scale and read its square on the A scale. Inverting this process allows square roots to be found, and similarly for the powers 3, 1/3, 2/3, and 3/2. Care must be taken when the base, x, is found in more than one place on its scale. For instance, there are two nines on the A scale; to find the square root of nine, use the first one; the second one gives the square root of 90.
For formula_13 problems, use the LL scales. When several LL scales are present, use the one with "x" on it. First, align the leftmost 1 on the C scale with x on the LL scale. Then, find "y" on the C scale and go down to the LL scale with "x" on it. That scale will indicate the answer. If "y" is "off the scale," locate formula_14 and square it using the A and B scales as described above. Alternatively, use the rightmost 1 on the C scale, and read the answer off the next higher LL scale. For example, aligning the rightmost 1 on the C scale with 2 on the LL2 scale, 3 on the C scale lines up with 8 on the LL3 scale.
Trigonometry.
The S, T, and ST scales are used for trig functions and multiples of trig functions, for angles in degrees.
For angles from around 5.7 up to 90 degrees, sines are found by comparing the S scale with C (or D) scale; though on many closed-body rules the S scale relates to the A scale instead, and what follows must be adjusted appropriately. The S scale has a second set of angles (sometimes in a different color), which run in the opposite direction, and are used for cosines. Tangents are found by comparing the T scale with the C (or D) scale for angles less than 45 degrees. For angles greater than 45 degrees the CI scale is used. Common forms such as formula_15 can be read directly from "x" on the S scale to the result on the D scale, when the C-scale index is set at "k". For angles below 5.7 degrees, sines, tangents, and radians are approximately equal, and are found on the ST or SRT (sines, radians, and tangents) scale, or simply divided by 57.3 degrees/radian. Inverse trigonometric functions are found by reversing the process.
Many slide rules have S, T, and ST scales marked with degrees and minutes (e.g. some Keuffel and Esser models, late-model Teledyne-Post Mannheim-type rules). So-called "decitrig" models use decimal fractions of degrees instead.
Logarithms and exponentials.
Base-10 logarithms and exponentials are found using the L scale, which is linear. Some slide rules have a Ln scale, which is for base e. Logarithms to any other base can be calculated by reversing the procedure for calculating powers of a number. For example, log2 values can be determined by lining up either leftmost or rightmost 1 on the C scale with 2 on the LL2 scale, finding the number whose logarithm is to be calculated on the corresponding LL scale, and reading the log2 value on the C scale.
Addition and subtraction.
Slide rules are not typically used for addition and subtraction, but it is nevertheless possible to do so using two different techniques.
The first method to perform addition and subtraction on the C and D (or any comparable scales) requires converting the problem into one of division. For addition, the quotient of the two variables plus one times the divisor equals their sum:
For subtraction, the quotient of the two variables minus one times the divisor equals their difference:
This method is similar to the addition/subtraction technique used for high-speed electronic circuits with the logarithmic number system in specialized computer applications like the Gravity Pipe (GRAPE) supercomputer and hidden Markov models.
The second method utilizes a sliding linear L scale available on some models. Addition and subtraction are performed by sliding the cursor left (for subtraction) or right (for addition) then returning the slide to 0 to read the result.
Physical design.
Standard linear rules.
The width of the slide rule is quoted in terms of the nominal width of the scales. Scales on the most common "10-inch" models are actually 25 cm, as they were made to metric standards, though some rules offer slightly extended scales to simplify manipulation when a result overflowed. Pocket rules are typically 5 inches. Models a couple of metres wide were sold to be hung in classrooms for teaching purposes.
Typically the divisions mark a scale to a precision of two significant figures, and the user estimates the third figure. Some high-end slide rules have magnifier cursors that make the markings easier to see. Such cursors can effectively double the accuracy of readings, permitting a 10-inch slide rule to serve as well as a 20-inch.
Various other conveniences have been developed. Trigonometric scales are sometimes dual-labeled, in black and red, with complementary angles, the so-called "Darmstadt" style. Duplex slide rules often duplicate some of the scales on the back. Scales are often "split" to get higher accuracy.
Circular slide rules.
Circular slide rules come in two basic types, one with two cursors (left), and another with a free dish and one cursor (right). The dual cursor versions perform multiplication and division by holding a fast angle between the cursors as they are rotated around the dial. The onefold cursor version operates more like the standard slide rule through the appropriate alignment of the scales.
The basic advantage of a circular slide rule is that the widest dimension of the tool was reduced by a factor of about 3 (i.e. by π). For example, a 10 cm circular would have a maximum precision approximately equal to a 31.4 cm ordinary slide rule. Circular slide rules also eliminate "off-scale" calculations, because the scales were designed to "wrap around"; they never have to be reoriented when results are near 1.0—the rule is always on scale. However, for non-cyclical non-spiral scales such as S, T, and LL's, the scale width is narrowed to make room for end margins.
Circular slide rules are mechanically more rugged and smoother-moving, but their scale alignment precision is sensitive to the centering of a central pivot; a minute 0.1 mm off-centre of the pivot can result in a 0.2mm worst case alignment error. The pivot, however, does prevent scratching of the face and cursors. The highest accuracy scales are placed on the outer rings. Rather than "split" scales, high-end circular rules use spiral scales for more complex operations like log-of-log scales. One eight-inch premium circular rule had a 50-inch spiral log-log scale.
The main disadvantages of circular slide rules are the difficulty in locating figures along a dish, and limited number of scales. Another drawback of circular slide rules is that less-important scales are closer to the center, and have lower precisions. Most students learned slide rule use on the linear slide rules, and did not find reason to switch.
One slide rule remaining in daily use around the world is the E6B. This is a circular slide rule first created in the 1930s for aircraft pilots to help with dead reckoning. With the aid of scales printed on the frame it also helps with such miscellaneous tasks as converting time, distance, speed, and temperature values, compass errors, and calculating fuel use. The so-called "prayer wheel" is still available in flight shops, and remains widely used. While GPS has reduced the use of dead reckoning for aerial navigation, and handheld calculators have taken over many of its functions, the E6B remains widely used as a primary or backup device and the majority of flight schools demand that their students have some degree of proficiency in its use.
Proportion wheels are simple circular slide rules used in graphic design to broaden or slim images and photographs. Lining up the desired values on the emmer and inner wheels (which correspond to the original and desired sizes) will display the proportion as a percentage in a small window. They are not as common since the advent of computerized layout, but are still made and used.
In 1952, Swiss watch company Breitling introduced a pilot's wristwatch with an integrated circular slide rule specialized for flight calculations: the Breitling Navitimer. The Navitimer circular rule, referred to by Breitling as a "navigation computer", featured airspeed, rate/time of climb/descent, flight time, distance, and fuel consumption functions, as well as kilometer—nautical mile and gallon—liter fuel amount conversion functions.
Cylindrical slide rules.
There are two main types of cylindrical slide rules: those with helical scales such as the Fuller, the Otis King and the Bygrave slide rule, and those with bars, such as the Thacher and some Loga models. In either case, the advantage is a much longer scale, and hence potentially higher accuracy, than a straight or circular rule.
Materials.
Traditionally slide rules were made out of hard wood such as mahogany or boxwood with cursors of glass and metal. At least one high precision instrument was made of steel.
In 1895, a Japanese firm, Hemmi, started to make slide rules from bamboo, which had the advantages of being dimensionally stable, strong and naturally self-lubricating. These bamboo slide rules were introduced in Sweden in September, 1933, and probably only a little earlier in Germany. Scales were made of celluloid, plastic, or painted aluminium. Later cursors were acrylics or polycarbonates sliding on Teflon bearings.
All premium slide rules had numbers and scales engraved, and then filled with paint or other resin. Painted or imprinted slide rules were viewed as inferior because the markings could wear off. Nevertheless, Pickett, probably America's most successful slide rule company, made all printed scales. Premium slide rules included clever catches so the rule would not fall apart by accident, and bumpers to protect the scales and cursor from rubbing on tabletops. The recommended cleaning method for engraved markings is to scrub lightly with steel-wool. For painted slide rules use diluted commercial window-cleaning fluid and a soft cloth.
History.
The slide rule was invented around 1620–1630, shortly after John Napier's publication of the concept of the logarithm. Edmund Gunter of Oxford developed a calculating device with a single logarithmic scale; with additional measuring tools it could be used to multiply and divide. The first description of this scale was published in Paris in 1624 by Edmund Wingate (c.1593–1656), an English mathematician, in a book entitled "L'usage de la reigle de proportion en l'arithmetique & geometrie". The book contains a double scale, logarithmic on one side, tabular on the other. In 1630, William Oughtred of Cambridge invented a circular slide rule, and in 1632 combined two handheld Gunter rules to make a device that is recognizably the modern slide rule. Like his contemporary at Cambridge, Isaac Newton, Oughtred taught his ideas privately to his students. Also like Newton, he became involved in a vitriolic controversy over priority, with his one-time student Richard Delamain and the prior claims of Wingate. Oughtred's ideas were only made public in publications of his student William Forster in 1632 and 1653.
In 1677, Henry Coggeshall created a two-foot folding rule for timber measure, called the Coggeshall slide rule, expanding the slide rule's use beyond mathematical inquiry.
In 1722, Warner introduced the two- and three-decade scales, and in 1755 Everard included an inverted scale; a slide rule containing all of these scales is usually known as a "polyphase" rule.
In 1815, Peter Mark Roget invented the log log slide rule, which included a scale displaying the logarithm of the logarithm. This allowed the user to directly perform calculations involving roots and exponents. This was especially useful for fractional powers.
In 1821, Nathaniel Bowditch, described in the "American Practical Navigator" a "sliding rule" that contained scales trigonometric functions on the fixed part and a line of log-sines and log-tans on the slider used to solve navigation problems.
In 1845, Paul Cameron of Glasgow introduced a Nautical Slide-Rule capable of answering navigation questions, including right ascension and declination of the sun and principal stars.
Modern form.
A more modern form of slide rule was created in 1859 by French artillery lieutenant Amédée Mannheim, "who was fortunate in having his rule made by a firm of national reputation and in having it adopted by the French Artillery." It was around this time that engineering became a recognized profession, resulting in widespread slide rule use in Europe–but not in the United States. There Edwin Thacher's cylindrical rule took hold after 1881. The duplex rule was invented by William Cox in 1891, and was produced by Keuffel and Esser Co. of New York.
Astronomical work also required fine computations, and in 19th-century Germany a steel slide rule about 2 meters long was used at one observatory. It had a microscope attached, giving it accuracy to six decimal places.
Throughout the 1950s and 1960s the slide rule was the symbol of the engineer's profession in the same way the stethoscope is of the medical profession's. German rocket scientist Wernher von Braun brought two 1930s vintage "Nestler" slide rules with him when he moved to the U.S. after World War 2 to work on the American space effort. Throughout his life he never used any other pocket calculating device, even heading the NASA program that landed a man on the moon in 1969.
Aluminium Pickett-brand slide rules were carried on Project Apollo space missions. The model N600-ES owned by Buzz Aldrin that flew with him to the moon on Apollo 11 was sold at auction in 2007. The model N600-ES taken along on Apollo 13 in 1970 is owned by the National Air and Space Museum.
Some engineering students and engineers carried ten-inch slide rules in belt holsters, a common sight on campuses even into the mid-1970s. Until the advent of the pocket digital calculator students also might keep a ten- or twenty-inch rule for precision work at home or the office while carrying a five-inch pocket slide rule around with them.
In 2004, education researchers David B. Sher and Dean C. Nataro conceived a new type of slide rule based on "prosthaphaeresis", an algorithm for rapidly computing products that predates logarithms. However, there has been little practical interest in constructing one beyond the initial prototype.
Specialized calculators.
Slide rules have often been specialized to varying degrees for their field of use, such as excise, proof calculation, engineering, navigation, etc., but some slide rules are extremely specialized for very narrow applications. For example, the John Rabone & Sons 1892 catalog lists a "Measuring Tape and Cattle Gauge", a device to estimate the weight of a cow from its measurements.
There were many specialized slide rules for photographic applications; for example, the actinograph of Hurter and Driffield was a two-slide boxwood, brass, and cardboard device for estimating exposure from time of day, time of year, and latitude.
Specialized slide rules were invented for various forms of engineering, business and banking. These often had common calculations directly expressed as special scales, for example loan calculations, optimal purchase quantities, or particular engineering equations. For example, the Fisher Controls company distributed a customized slide rule adapted to solving the equations used for selecting the proper size of industrial flow control valves.
In World War II, bombardiers and navigators who required quick calculations often used specialized slide rules. One office of the U.S. Navy actually designed a generic slide rule "chassis" with an aluminium body and plastic cursor into which celluloid cards (printed on both sides) could be placed for special calculations. The process was invented to calculate range, fuel use and altitude for aircraft, and then adapted to many other purposes.
The E6-B is a circular slide rule used by pilots & navigators.
Decline.
The importance of the slide rule began to diminish as electronic computers, a new but rare resource in the 1950s, became more widely available to technical workers during the 1960s. (See History of computing hardware (1960s–present).)
Computers also changed the nature of calculation. With slide rules a great emphasis was put on working the algebra to get expressions into the most computable form. Users would simply approximate or drop small terms to simplify a calculation. FORTRAN allowed complicated formulas to be typed in from textbooks without the effort of reformulation. Numerical integration was often easier than trying to find closed-form solutions for difficult problems. The young engineer asking for computer time to solve a problem that could have been done by a few swipes on the slide rule became a humorous cliché.
The availability of mainframe computing did not however significantly affect the ubiquitous use of the slide rule until cheap hand held electronic calculators for scientific and engineering purposes became available in the mid-1970s, at which point it rapidly declined. The first included the Wang Laboratories LOCI-2, introduced in 1965, which used logarithms for multiplication and division and the Hewlett-Packard HP-9100, introduced in 1968. The HP-9100 had trigonometric functions (sin, cos, tan) in addition to exponentials and logarithms. It used the CORDIC (coordinate rotation digital computer) algorithm, which allows for calculation of trigonometric functions using only shift and add operations. This method facilitated the development of ever smaller scientific calculators.
As calculator price declined geometrically and functionality increased exponentially the slide rule's fate was sealed. The pocket-sized Hewlett-Packard HP-35 scientific calculator cost US$395 in 1972, too expensive for most students. By 1975 basic four-function electronic calculators could be purchased for less than $50, and by 1976 the TI-30 scientific calculator could be purchased for less than $25.
Compared to electronic digital calculators.
Most people find slide rules difficult to learn and use. Even during their heyday, they never caught on with the general public. Addition and subtraction are not well-supported operations on slide rules and doing a calculation on a slide rule tends to be slower than on a calculator. This led engineers to take mathematical shortcuts favoring operations that were easy on a slide rule, creating inaccuracies and mistakes. On the other hand, the spatial, manual operation of slide rules cultivates in the user an intuition for numerical relationships and scale that people who have used only digital calculators often lack. A slide rule will also display all the terms of a calculation along with the result, thus eliminating uncertainty about what calculation was actually performed.
A slide rule requires the user to separately compute the order of magnitude of the answer in order to position the decimal point in the results. For example, 1.5 × 30 (which equals 45) will show the same result as 1,500,000 × 0.03 (which equals 45,000). This separate calculation is less likely to lead to extreme calculation errors, but forces the user to keep track of magnitude in short-term memory (which is error-prone), keep notes (which is cumbersome) or reason about it in every step (which distracts from the other calculation requirements).
The typical precision of a slide rule is about three significant digits, compared to many digits on digital calculators. As order of magnitude gets the greatest prominence when using a slide rule, users are less likely to make errors of false precision.
When performing a sequence of multiplications or divisions by the same number, the answer can often be determined by merely glancing at the slide rule without any manipulation. This can be especially useful when calculating percentages (e.g. for test scores) or when comparing prices (e.g. in dollars per kilogram). Multiple speed-time-distance calculations can be performed hands-free at a glance with a slide rule. Other useful linear conversions such as pounds to kilograms can be easily marked on the rule and used directly in calculations.
Being entirely mechanical, a slide rule does not depend on electricity or batteries. However, mechanical imprecision in slide rules that were poorly constructed or warped by heat or use will lead to errors.
Many sailors keep slide rules as backups for navigation in case of electric failure or battery depletion on long route segments. Slide rules are still commonly used in aviation, particularly for smaller planes. They are only being replaced by integrated, special purpose and expensive flight computers, and not general-purpose calculators. The E6B circular slide rule used by pilots has been in continuous production and remains available in a variety of models. Some wrist watches designed for aviation use still feature slide rule scales to permit quick calculations. The Citizen Skyhawk AT is a notable example.
The slide rule today.
Even today some people prefer a slide rule over an electronic calculator as a practical computing device. Others keep their old slide rules out of a sense of nostalgia, or collect them as a hobby.
A popular collectible model is the Keuffel & Esser "Deci-Lon", a premium scientific and engineering slide rule available both in a ten-inch "regular" ("Deci-Lon 10") and a five-inch "pocket" ("Deci-Lon 5") variant. Another prized American model is the eight-inch Scientific Instruments circular rule. Of European rules, Faber-Castell's high-end models are the most popular among collectors.
Although there is a large supply of slide rules circulating on the market, specimens in good condition tend to be expensive. Many rules found for sale on are damaged or have missing parts, and the seller may not know enough to supply the relevant information. Replacement parts are scarce, expensive, and generally only available for separate purchase on individual collectors' web sites. The Keuffel and Esser rules from the period up to about 1950 are particularly problematic, because the end-pieces on the cursors, made of celluloid, tend to chemically break down over time.
There are still a handful of sources for brand new slide rules. The Concise Company of Tokyo, which began as a manufacturer of circular slide rules in July 1954, continues to make and sell them today. In September 2009, on-line retailer ThinkGeek introduced its own brand of straight slide rules, described as "faithful replica[s]" that are "individually hand tooled". These are no longer available in 2012. In addition, Faber-Castell has a number of slide rules still in inventory, available for international purchase through their web store. Proportion wheels are still used in graphic design.
Various simulator apps are available for Android based smart phones.

</doc>
<doc id="28745" url="http://en.wikipedia.org/wiki?curid=28745" title="Styx">
Styx

The Styx (; Ancient Greek: Στύξ ]) is a river in Greek mythology that formed the boundary between Earth and the Underworld (the domain often called Hades, which is also the name of its ruler). The rivers Styx, Phlegethon, Acheron, Lethe, and Cocytus all converge at the center of the underworld on a great marsh, which is also sometimes called the Styx. According to Herodotus the river Styx originates near Feneos.
Significance of the River Styx.
The gods were bound by the Styx and swore oaths on it. The reason for this is during the Titan war, Styx, the goddess of the river Styx, sided with Zeus. After the war, Zeus promised every oath be sworn upon her. Zeus swore to give Semele whatever she wanted and was then obliged to follow through when he realized to his horror that her request would lead to her death. Helios similarly promised his son Phaëton whatever he desired, also resulting in the boy's death.
According to some versions, Styx had miraculous powers and could make someone invulnerable. According to one tradition, Achilles was dipped in it in his childhood, acquiring invulnerability, with exception of his heel, by which his mother held him. This is the source of the expression "Achilles' heel," a metaphor for a vulnerable spot.
Styx was primarily a feature in the afterworld of Greek mythology, similar to the Christian area of Hell in texts such as "The Divine Comedy" and "Paradise Lost". The ferryman Charon is believed to have transported the souls of the newly dead across this river into the underworld, though in the original Greek and Roman sources, as well as in Dante, it was the river Acheron that Charon plied. Dante put Phlegyas as ferryman over the Styx and made it the fifth circle of Hell, where the wrathful and sullen are punished by being drowned in the muddy waters for eternity, with the wrathful fighting each other.
In ancient times some believed that placing a coin (Charon's obol) in the mouth of the deceased would help pay the toll for the ferry to help cross the Acheron River which would lead one to the entrance of the underworld. If someone could not pay the fee it was said that they would never be able to cross the river. This ritual was performed by the relatives.
The variant spelling Stix was sometimes used in translations of Classical Greek before the 20th century. By metonymy, the adjective "stygian" () came to refer to anything dark, dismal, and murky.
Goddess.
Styx was also the name of the daughter of Oceanus and Tethys, and goddess of the River Styx itself. She was wife to Pallas and bore him Zelus, Nike, Kratos and Bia (and sometimes Eos). Styx supported Zeus in the Titanomachy where she was the first to rush to his aid. For this reason her name was given the honor of being a binding oath for the gods.
Science.
As of 2 July 2013, Styx officially became the name of one of Pluto's moons. The other moons (Charon, Nix, Hydra, and Kerberos) also have names from Greco-Roman mythology related to the underworld.

</doc>
<doc id="28747" url="http://en.wikipedia.org/wiki?curid=28747" title="Sangha">
Sangha

 
Sangha (Pali: सङ्घ "saṅgha"; Sanskrit: संघ "saṃgha"; : "Sēngjiā"; Tibetan: དགེ་འདུན་ "dge 'dun") is a word in Pali and Sanskrit meaning "association", "assembly," "company" or "community" and most commonly refers in Buddhism to the monastic community of ordained Buddhist monks or nuns. This community is traditionally referred to as the "bhikkhu-sangha" or "bhikkhuni-sangha". As a separate category, those who have attained any of the four stages of enlightenment, whether or not they are members of the "bhikkhu-sangha" or "bhikkhuni-sangha", are referred to as the "ariya-sangha" or "noble Sangha".
The Sangha also includes laymen and laywomen who are personally dedicated to the discipline of "Dharma-Vinaya". This use of the word "Sangha" is only sometimes found in the Pali texts.
Definitions.
In a glossary of Buddhist terms, Richard Robinson et al. define Sangha as:
“Sangha. Community. This word has two levels of meaning: (1) on the ideal (arya) level, it denotes all of the Buddha’s followers, lay or ordained, who have at least attained the level of srotapanna; (2) on the conventional (samvtri) level, it denotes the orders of the Bhiksus and Bhiksunis.”
Some lay practitioners in the West these days use the word "Sangha" as a collective term for all Buddhists, but the Pali Canon uses the word "parisā" (Sanskrit, parisad) for the larger Buddhist community — the monks, nuns, lay men, and lay women who have taken the Three Refuges — reserving ‘Sangha’ for a more restricted use.”
“The two meanings overlap but are not necessarily identical. Some members of the ideal Sangha are not ordained; some monastics have yet to acquire the Dharma-eye”
“Unlike the present Sangha, the original Sangha viewed itself as following the mission laid down by the Master, viz, to go forth ‘…on tour for the blessing of the manyfolk, for the happiness of the manyfolk out of compassion for the world, for the welfare, the blessing, the happiness of deva and men"
Qualities of the Sangha.
The "Sangha" is the third of the Three Jewels in Buddhism. Due to the temptations and vicissitudes of life in the world, monastic life is considered to provide the safest and most suitable environment for advancing toward enlightenment and liberation.
In Buddhism, the Buddha, the Dharma and the Sangha each are described as having certain characteristics. These characteristics are chanted either on a daily basis and/or on Uposatha days, depending on the school of Buddhism. In Theravada tradition they are a part of daily chanting:
The Sangha: The Sangha of the Blessed One's disciples (Savakas) is:
That is, the four pairs of persons, the eight types of individuals - This Sangha of the Blessed One's disciples is:
Monastic tradition.
The Sangha of monks and the Sangha of nuns were originally established by Gautama Buddha in the 5th century BC in order to provide a means for those who wish to practice the Dhamma full-time, in a direct and highly disciplined way, free from the restrictions and responsibilities of the household life. The Sangha also fulfils the function of preserving the Buddha’s original teachings and of providing spiritual support for the Buddhist lay-community. The monastic "sangha" has historically assumed responsibility for maintaining the integrity of the doctrine as well as the translation and propagation of the teachings of the Buddha.
The key feature of Buddhist monasticism is the adherence to the vinaya which contains an elaborate set of rules of conduct including complete chastity and eating only before noon. Between midday and the next day, a strict life of scripture study, chanting, meditation, and occasional cleaning forms most of the Sangha's duties . Transgression of rules carries penalties ranging from confession to permanent expulsion from the Sangha. The founder of Japanese Tendai decided to reduce the number of rules down to about 60 (Enkai). In Kamakura Era, many sects (Zen, Pureland and Nichiren) that originated from Tendai sect abolished vinaya entirely.
Monks and nuns may own only the barest minimum of possessions due to their samaya as renunciates (ideally, three robes, an alms bowl, a cloth belt, a needle and thread, a razor for shaving the head, and a water filter). In practice, they often have a few additional personal possessions.
Traditionally, Buddhist monastics eschew ordinary clothes and wear robes. Originally the robes were sewn together from rags and stained with earth. The idea that robes were dyed with saffron seems unlikely to be true since it was and still is a very expensive commodity, and monks were poor. The color of modern robes varies from community to community (saffron is characteristic for southeast Asian Theravada and Mahayana groups, maroon in Tibet, gray in Korea, black in Japan etc.)
The word which is usually translated as monk is "bhikkhu" in Pali or "bhikshu" in Sanskrit. The feminine form is "bhikkhuni" or "bhikshuni". These words literally mean "beggar", learner, auspicious, adept, endowed with harmony and order; and it is traditional for bhikkhus to beg their food. In most places this has become an elaborate ritual, where lay people feed monastics in order to obtain merit which will ensure them a fortunate rebirth. Although monastics in India traditionally did not work for income, this changed when Buddhism moved to east Asia, so that in China and the surrounding countries monks often engage in agriculture.
The idea that all Buddhists, especially monks and nuns practice vegetarianism is a Western misperception. In some Sanskrit Mahayana Sutras meat eating is strongly discouraged. In Pali Canon the Buddha rejected a suggestion by Devadatta to impose vegetarianism on the Sangha. According to the Pali Texts, the Buddha ate meat [as long as the animal was not killed specifically for Him]. According to the Mahayana Sutras, the Buddha does not eat meat. The Buddha [in the Pali Texts] allowed Sangha members to eat whatever food is donated to them by laypeople, except that they may not eat meat if they know or suspect the animal was killed specifically for them. Consequently, the Theravadan tradition (Sri Lanka, Thailand, Laos, Cambodia and Burma) which follows the Pali scriptures does not practice vegetarianism though an individual may do so at his or her personal choice . On the other hand, the Mahayana and Vajrayana traditions accept both Theravada and Mahayana scriptures, and consequently the practice will vary depending on their interpretation of the sutras. In particular, East Asian monastics take on the bodhisattva vows from the Brahma Net Sutra which has a vow of vegetarianism as part of the Triple Platform Ordination where they receive the sramanera/sramanerika, bhikshu/bhikshuni and bodhisattva vows, whereas the Tibetan lineages transmit the bodhisattva vows from Asanga's Yogacarabhumi, which does not include a vow of vegetarianism. In some areas such as China, Korea and Vietnam one expects the Sangha to practice strict vegetarianism while in other areas such as Japan or Tibet one does not.
The lay community is responsible for the production of goods and services in society, and for the production and raising of children. According to Mahayana sutras, the Buddha always maintained that lay persons were capable of great wisdom in the Buddhadharma and of reaching enlightenment. In the west, there is a misconception that Theravada regards enlightenment to be an impossible goal outside the Sangha. This is incorrect. In Theravada suttas, it is clearly recorded that the Buddha's uncle—who was a lay follower—reached enlightenment by hearing the Buddha's discourse.
The distinction between Sangha and lay persons has always been important and forms the "Parisa", Buddhist community. Here, monastics teach and counsel the laity at request while laymen and laywomen offer donations for their future support. This inter-connectedness serves as a marriage and has sustained Buddhism to this day.
Women's role in the Sangha.
Although always maintaining that women were just as capable of attaining enlightenment as men, the canonical texts depict the Buddha as being reluctant to permit women to join the Sangha. After several entreaties from his aunt and foster-mother, Maha Pajapati Gotami, who wished to become ordained, and from his cousin and aide Ananda, who supported her cause, the Buddha relented and ordained Maha Pajapati and several others as nuns. It is interesting to note that this was one of the few issues about which the Buddha is recorded to have changed his mind. The Buddha later established the condition that each new ordination would be sanctioned by at least five bhikkhunis.
There have been several theories regarding the Buddha's reluctance to ordain women, including the possibility that it was due to fears that a community of women would not be safe in the society of his day. According to the scriptures the reason the Buddha himself gave was that the admission of women would weaken the Sangha and shorten its lifetime, and he laid down strict rules subordinating nuns to monks (The Eight Garudhammas).
Before the modern era, the Bhikkhuni Sangha spread to most Buddhist countries including Burma (also known as Myanmar), with the notable exceptions being Tibet and Thailand. However, in Sri Lanka, it died out in the 11th century during a civil war and was not revived. Consequently, as Theravada Buddhism spread to Thailand, the Theravada Sangha consisted only of monks.
In recent decades, there has been a serious attempt to revive the Theravada Bhikkhuni Sangha with the assistance of Mahayana bhikkhunis from the Chinese lineage. These were introduced from Sri Lanka in 433 C. E., following the Dharmaguptaka Vinaya, and subsequently spread to Korea, Taiwan, Vietnam, and Japan. This has resulted in a small but thriving community of nuns in Sri Lanka, who in turn ordained the first Theravada Buddhist nun in the history of Thailand, Ven. Dhammananda. However, the validity of these ordinations is strongly disputed by some of the conservative Theravada establishment.
Meanwhile, a similar process has produced the first fully ordained bhikkhunis in Tibetan Buddhism, where only the novice ordination for bhikkhunis existed. In the west, where feminism has been a strong influence, there have been many remarkable Buddhist nuns: three notable examples are Pema Chodron, Ayya Khema and Tenzin Palmo.
The first bhikkhuni ordination in Australia in the Theravadin tradition was held in Perth on October 22, 2009 at Bodhinyana Monastery. Venerable Ajahn Vayama together with Venerables Nirodha, Seri and Hasapanna were ordained as bhikkhunis by a dual sangha act of bhikkhus and bhikkhunis in full accordance with the pali vinaya.
Sangha as a general reference to Buddhist community.
Some scholars have noted that "sangha" is frequently (and according to them, mistakenly) used in the West to refer to any sort of Buddhist community. The terms "parisa" and "gana" are suggested as being more appropriate references to a community of Buddhists. Parisa means "following" and it refers to the four groups of the Buddha's followers: monks, nuns, laymen and laywomen. The Sanskrit term "gana" has meanings of "flock, troop, multitude, number, tribe, series, class", and is usable as well in more mundane senses.
The Buddhist-derived new religious movement, Soka Gakkai, which started as a lay organization associated with Nichiren Buddhism in Japan, disputes the traditional definition of sangha. They interpret the meaning of the Three Jewels of Buddhism, in particular the "treasure of the Sangha"—which they refer to in English as "Samgha"—includes not only the monastic community, but also lay persons that practice Buddhism correctly.

</doc>
<doc id="28748" url="http://en.wikipedia.org/wiki?curid=28748" title="Speed">
Speed

In everyday use and in kinematics, the speed of an object is the magnitude of its velocity (the rate of change of its position); it is thus a scalar quantity. The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval; the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero.
Like velocity, speed has the dimensions of a length divided by a time; the SI unit of speed is the metre per second, but the most usual unit of speed in everyday usage is the kilometre per hour or, in the US and the UK, miles per hour. For air and marine travel the knot is commonly used.
The fastest possible speed at which energy or information can travel, according to special relativity, is the speed of light in a vacuum "c" = metres per second (approximately or ). Matter cannot quite reach the speed of light, as this would require an infinite amount of energy. In relativity physics, the concept of rapidity replaces the classical idea of speed.
Definition.
The Italian physicist Galileo Galilei is credited with being the first to measure speed by considering the distance covered and the time it takes. Galileo defined speed as the distance covered per unit of time. In equation form, this is
where formula_2 is speed, formula_3 is distance, and formula_4 is time. A cyclist who covers 30 metres in a time of 2 seconds, for example, has a speed of 15 metres per second. Objects in motion often have variations in speed (a car might travel along a street at 50 km/h, slow to 0 km/h, and then reach 30 km/h).
In mathematical terms, the speed formula_2 is defined as the magnitude of the velocity formula_6, that is, the derivative of the position formula_7 with respect to time:
If formula_9 is the length of the path travelled until time formula_4, the speed equals the time derivative of formula_9:
In the special case where the velocity is constant (that is, constant speed in a straight line), this can be simplified to formula_13. The average speed over a finite time interval is the total distance travelled divided by the time duration.
Instantaneous speed.
By looking at a speedometer, one can read the speed of a car at any instant, or its "instantaneous speed". A car travelling at 50 km/h generally goes for less than one hour at a constant speed, but if it did go at that speed for a full hour, it would travel 50 km. If the vehicle continued at that speed for half an hour, it would cover half that distance (25 km). If it continued for only one minute, it would cover about 833 m.
Average speed.
Different from instantaneous speed, "average speed" is defined as the total distance covered over the time interval. For example, if a distance of 80 kilometres is driven in 1 hour, the average speed is 80 kilometres per hour. Likewise, if 320 kilometres are travelled in 4 hours, the average speed is also 80 kilometres per hour. When a distance in kilometres (km) is divided by a time in hours (h), the result is in kilometres per hour (km/h). Average speed does not describe the speed variations that may have taken place during shorter time intervals (as it is the entire distance covered divided by the total time of travel), and so average speed is often quite different from a value of instantaneous speed. If the average speed and the time of travel are known, the distance travelled can be calculated by rearranging the definition to
Using this equation for an average speed of 80 kilometres per hour on a 4-hour trip, the distance covered is found to be 320 kilometres. 
Expressed in graphical language, the slope of a tangent line at any point of a distance-time graph is the instantaneous speed at this point, while the slope of a chord line of the same graph is the average speed during the time interval covered by the chord.
Tangential speed.
Linear speed is the distance traveled per unit of time, while tangential speed (or tangential velocity) is the linear speed of something moving along a circular path. A point on the outside edge of a merry-go-round or turntable travels a greater distance in one complete rotation than a point nearer the center. Travelling a greater distance in the same time means a greater speed, and so linear speed is greater on the outer edge of a rotating object than it is closer to the axis. This speed along a circular path is known as "tangential speed" because the direction of motion is tangent to the circumference of the circle. For circular motion, the terms linear speed and tangential speed are used interchangeably, and both use units of m/s, km/h, and others.
Rotational speed (or "angular speed") involves the number of revolutions per unit of time. All parts of a rigid merry-go-round or turntable turn about the axis of rotation in the same amount of time. Thus, all parts share the same rate of rotation, or the same number of rotations or revolutions per unit of time. It is common to express rotational rates in revolutions per minute (RPM) or in terms of the number of "radians" turned in a unit of time. There are little more than 6 radians in a full rotation (2π radians exactly). When a direction is assigned to rotational speed, it is known as rotational velocity or angular velocity. Rotational velocity is a vector whose magnitude is the rotational speed.
Tangential speed and rotational speed are related: the greater the RPMs, the larger the speed in metres per second. Tangential speed is directly proportional to rotational speed at any fixed distance from the axis of rotation. However, tangential speed, unlike rotational speed, depends on radial distance (the distance from the axis). For a platform rotating with a fixed rotational speed, the tangential speed in the centre is zero. Towards the edge of the platform the tangential speed increases proportional to the distance from the axis. In equation form:
where "v" is tangential speed and ω (Greek letter omega) is rotational speed. One moves faster if the rate of rotation increases (a larger value for ω), and one also moves faster if movement farther from the axis occurs (a larger value for "r"). Move twice as far from the rotational axis at the centre and you move twice as fast. Move out three times as far and you have three times as much tangential speed. In any kind of rotating system, tangential speed depends on how far you are from the axis of rotation.
When proper units are used for tangential speed "v", rotational speed ω, and radial distance "r", the direct proportion of "v" to both "r" and ω becomes the exact equation
Thus, tangential speed will be directly proportional to "r" when all parts of a system simultaneously have the same ω, as for a wheel, disk, or rigid wand.
Units.
Units of speed include:
 (Values in bold face are exact.)
Examples of different speeds.
Vehicles often have a speedometer to measure the speed they are moving.

</doc>
<doc id="28749" url="http://en.wikipedia.org/wiki?curid=28749" title="Sutta">
Sutta

Sutta may refer to:
See also:

</doc>
<doc id="28751" url="http://en.wikipedia.org/wiki?curid=28751" title="Superluminal communication">
Superluminal communication

Superluminal communication is the hypothetical process by which one might send information at faster-than-light (FTL) speeds. The scientific consensus is that faster-than-light communication is not possible and to date superluminal communication has not been achieved in any experiment.
Some theories and experiments include:
According to the currently accepted theory, three of those four phenomena do not produce superluminal communication, even though they may give that appearance under some conditions. The third, tachyons, arguably do not exist as their existence is hypothetical; even if their existence were to be proven, attempts to quantize them appear to indicate that they may not be used for superluminal communication, because experiments to produce or absorb tachyons cannot be fully controlled.
If wormholes are possible, then ordinary subluminal methods of communication could be sent through them to achieve superluminal transmission speeds. Considering the immense energy that current theories suggest would be required to open a wormhole large enough to pass spacecraft through it may be that only atomic-scale wormholes would be practical to build, limiting their use solely to information transmission. Some theories of wormhole formation would prevent them from ever becoming "timeholes", allowing superluminal communication without the additional complication of allowing communication with the past.
The microscopic causality postulate of axiomatic quantum field theory implies the impossibility of superluminal communication using phenomena whose behavior can be described by orthodox quantum field theory. A special case of this is the no-communication theorem, which prevents communication using the quantum entanglement of a composite system shared between two spacelike-separated observers. Some authors have argued out that using the no-communication theorem to deduce the impossibility of superluminal communication is circular, since the no-communication theorem assumes to start with that the system is a composite system.
However, some argue that superluminal communication could be achieved "via" quantum entanglement using other methods that don't rely on cloning a quantum system. One suggested method would use an ensemble of entangled particles to transmit information, similar to a type of quantum eraser experiments where the observation of an interference pattern on half of an ensemble of entangled pairs is determined by the type of measurement performed on the other half. In these cases, though, the interference pattern only emerges with coincident measurements which requires a classical, subluminal communication channel between the two detectors. Physicist John G. Cramer at the University of Washington is attempting to perform one type of these experiment and demonstrate whether or not it can produce superluminal communication.

</doc>
<doc id="28752" url="http://en.wikipedia.org/wiki?curid=28752" title="Shah Jahan">
Shah Jahan

Shahabuddin Muhammad Shah Jahan (January 1594 – 22 January 1666) was the fifth Mughal Emperor of India. He is also known as Shah Jahan I. He ruled from 1628 until 1658. Born Prince Khurram, he was the son of Emperor Jahangir and his Hindu Rajput wife, Taj Bibi Bilqis Makani (13 May 1573 – 18 April 1619)
He was chosen as successor to the throne after the death of his father in 1627. He was considered one of the greatest Mughals. Like Akbar, he was eager to expand his vast empire. In 1658, he fell ill and was confined by his son and successor Aurangzeb in Agra Fort until his death in 1666.
Shah Jahan was a more Orthodox Muslim than his father and grandfather. His policies towards non-Muslims were less liberal than Jahangir and Akbar.
The period of his reign was considered the golden age of Mughal architecture. Shah Jahan erected many monuments, the most famous of which is the Taj Mahal at Agra, built in 1632–1654 as a tomb for his beloved wife Mumtaz Mahal.
Early life.
Born on 6 January 1592, Shah ab-ud-din Muhammad Khurram which was Shah Jahan's birth name, was the third son born to Emperor Jahangir, his mother was a Rajput princess from Marwar called Princess Manmati – her official name in Mughal chronicles being Bilquis Makani. The name "Khurram" was chosen for the young prince by his grandfather, Emperor Akbar, with whom the young prince shared a close relationship.
Just prior to Khurram’s birth, a soothsayer had reportedly predicted to childless Empress Ruqaiya Sultan Begum, Akbar's first wife, that the still unborn child was destined for imperial greatness. So, when Khurram was born in 1592 and was only six days old, Akbar ordered that the prince be taken away from his mother and handed him over to Ruqaiya so that he could grow up under her care and Akbar could fulfill his aging wife's wish, to raise a Mughal emperor. Ruqaiya assumed the primary responsibility for Khurram's upbringing and he grew up under her care. Her step-son, Jahangir, noted that Ruqaiya loved Khurram "A thousand times more than if he had been her own son."
Khurram remained with her, until he had turned 13. After the death of Akbar, the young prince was, finally, allowed to return to his father's household, and thus, be closer to his biological mother.
As a child, Khurram received a broad education befitting his status as a Mughal prince, which included martial training and exposure to a wide variety of cultural arts, such as poetry and music, most of which was inculcated, according to court chroniclers, under the watchful gaze of his grandfather and his step-grandmother, Empress Ruqaiya. In 1605, as the Akbar lay on his deathbed, Khurram, who at this point was 13, remained by his bedside and refused to move even after his mother tried to retrieve him. Given the politically uncertain times immediately preceding Akbar's death, Khurram was in a fair amount of physical danger of harm by political opponents of his father and his conduct at this time can be understood to be a precursor bravery that he would later be known for.
In 1605, his father succeeded to the throne, after crushing a rebellion by Prince Khausrau – Khurram remained distant from the court politics and intrigues in the immediate aftermath of that event, which was apparently a conscious decision on Jahangir's part. As the third son, Khurram did not challenge the two major power blocs of the time, his father's and his step-brother's; thus he enjoyed the benefits of Imperial protection and luxury, while being allowed to continue with his education and training. This relatively quiet and stable period of his life allowed Khurram to build his own support base in the Mughal court, which would be useful later on in his life.
Due to the long period of tensions between his father and step-brother, Khurram began to drift closer to his father and over time started to be considered the de facto heir apparent by court chroniclers. This status was given official sanction when Jahangir granted the jagir of Hissar-Feroza, which had traditionally been the fief of the heir apparent, to Khurram in 1607.
Marriages.
In 1608, Khurram was engaged to Mumtaz Mahal – when they were 15 and 14 years old, respectively. The young girl belonged to an illustrious Persian noble family which had been serving Mughal Emperors since the reign of Akbar, the family's patriarch was Itimad-ud-Daulah, who had been Jahangir's finance minister and his son; Asaf Khan – Arjumand Banu's father – played an important role in the Mughal court, eventually serving as Chief Minister. Her aunt was the Empress Nur Jahan and is thought to have played the matchmaker in arranging the marriage.
But for some reason, the prince was not married to Arjumand Banu Begum for five years, which was an unusually long engagement for the time. However, Shah Jahan married the daughter of great-grandson of Shah Ismail of Persia with whom he had a daughter, his first child.
Politically speaking, the betrothal allowed Khurram to be considered as having officially entered manhood, and he was granted several jagirs, including Hissar-Feroze and ennobled to a military rank of 8,000, which allowed him to take on official functions of state, an important step in establishing his own claim to the throne.
In 1612, aged 20, Khurram married Arjumand Banu Begum on an auspicious date chosen by court astrologers. The marriage was a happy one and Khurram remained devoted to her. She bore him fourteen children, out of whom seven survived into adulthood. In addition, Khurram had two children from his first two wives.
Though there was genuine love between the two, Arjumand Banu Begum was a politically astute woman and served as a crucial advisor and confidante to her husband, she even is said to have implored Khurram not to have children with his other wives, a call he listened. Later on, as empress, Mumtaz Mahal (Persian: the chosen one of the Palace‎) wielded immense power, such as being consulted by her husband in state matters and being responsible for the imperial seal, which allowed her to review official documents in their final draft.
Mumtaz Mahal died, aged 40, while giving birth to Gauhara Begum in Burhanpur, the cause of death being Postpartum hemorrhage, which caused considerable blood-loss after a painful labour of thirty hours. Contemporary historians note that Princess Jahanara, aged 17, was so distressed by her mother's pain that she started distributing gems to the poor, hoping for divine intervention and Shah Jahan, himself, was noted as being "paralysed by grief" and weeping fits.
Her body was temporarily buried in a walled pleasure garden known as Zainabad, originally constructed by Shah Jahan's uncle Prince Daniyal along the Tapti River. Her death had a profound impact on Shah Jahan's personality and inspired the construction of the Taj Mahal, where she was later reburied.
The intervening years had seen Khurram take three other wives, Kandahari Begum (m. 12 December 1609) and Izz un-Nisa Begum (m. 3 September 1617), the daughters of Muzaffar Husain Mirza Safawi and Shahnawaz Khan, son of Abdul Rahim Khan-I-Khana, respectively. But according to court chroniclers, his relationship with his other wives was more out of political consideration and they enjoyed only the status of being royal wives.
Military commander.
The first occasion for Khurram to test out his military prowess was during the Mughal campaign against the Rajput state of Mewar, which had been a hostile force to the Mughals since Akbar's reign. In 1614, commanding an army numbering around 200,000, Khurram began the offensive against the Rajput kingdom. After a year of the harsh war of attrition, Maharana Amar Singh II surrendered to the Mughal forces and became a vassal state of the Mughal Empire.
In 1617, Khurram was directed to deal with the Lodi in the Deccan, to secure the Empire's southern borders and to restore imperial control over the region. His successes in these conflicts led to Jahangir granting him the title of Shah Jahan (Persian: King of the World‎) and raised his military rank and allowed him a special throne in his Durbar, an unprecedented honour for a prince, thus further solidifying his status as crown prince.
Rebel prince.
Inheritance of power and wealth in the Mughal empire was not determined through primogeniture, but by princely sons competing to achieve military successes and consolidating their power at court. This often led to rebellions and wars of succession. As a result, a complex political climate surrounded the Mughal court in Khurram's formative years. In 1611 his father married Nur Jahan, the widowed daughter of an Afghan noble. She rapidly became an important member of Jahangir's court and, together with her brother Asaf Khan, wielded considerable influence. Arjumand was Asaf Khan's daughter and her marriage to Khurram consolidated Nur Jahan and Asaf Khan's positions at court.
Court intrigues, however, including Nur Jahan's decision to have her daughter from her first marriage wed Shah Jahan's youngest brother Shahzada Shahryar and her support for his claim to the throne led Khurram, supported by Mahabat Khan, into open revolt against his father in 1622.
The rebellion was quelled by Jahangir's forces in 1626 and Khurram was forced to submit unconditionally. Upon the death of Jahangir in 1627, Khurram succeeded to the Mughal throne as Abu ud-Muzaffar Shihab ud-Din Mohammad Sahib ud-Quiran ud-Thani Shah Jahan Padshah Ghazi (Urdu: شهاب الدین محمد خرم), or short Shah Jahan. His regnal name is divided into various parts. "Shihab ud-Din" mean "Star of the Faith", "Sahib al-Quiran ud-Thani" means "Second Lord of the Happy Conjunction of Jupiter and Venus". "Shah Jahan" means "King of the World", alluding to his pride in his Timurid roots and his ambitions. More epithets showed his secular and religious duties. He was also "Khalifat Panahi" ("Refuge of the Caliphate"), but "Zill-i Allahi", or the "Shadow of God on Earth".
His first act as ruler was to execute his chief rivals and imprison his step mother Nur Jahan. This allowed Shah Jahan to rule his empire without contention.
Emperor (1628–1658).
Administration of the Mughal Empire.
Evidence from the reign of Shah Jahan in the year 1648, states that the army consisted of 911,400 infantry, musketeers, and artillery men, and 185,000 Sowars commanded by princes and nobles and were maintained out of the revenues of the Mughal Empire which amounted to 120,071,876,840" dams".
During his reign the Marwari horse was introduced becoming Shah Jahan's favorite and various Mughal cannons were mass-produced in the Jaigarh Fort. Under his rule, the empire became a huge military machine and the nobles and their contingents multiplied almost fourfold, as did the demands for more revenue from the peasantry. But due to his measures in the financial and commercial fields, it was a period of general stability—the administration was centralized and court affairs systematized.
The Mughal Empire continued to expand moderately during his reign as his sons commanded large armies on different fronts. Above all it is obligatory to mention here that India became the richest center of the arts, crafts and architecture and some of the best of the architects, artisans, craftsmen, painters and writers of the world resided in his empire, it is believed that the Mughal Empire had the highest gross domestic produce in the world.
Rajput rebels.
Shah Jahan annexed the Rajput confederates of Baglana, Mewar and Bundelkhand. He then chose his 16-year old son Aurangzeb to serve in his place and subdue the rebellion by the Bundela Rajputs led by the renegade Jhujhar Singh.
Relations with the Deccan Sultanates.
Shah Jahan then chose Aurangzeb to become the Subahdar of the Deccan and ordered the annexation of Ahmednagar and the overthrow of the Nizam Shahi dynasty.
Sikh rebellion led by Guru Hargobind.
A rebellion of the Sikhs led by Guru Hargobind took place and in return Shah Jahan ordered the destruction of the Sikh temple in Lahore. Skirmishes were fought at Amritsar, Kartarpur and elsewhere.
Battle of Rohilla
Battle of Amritsar (1634)
Battle of Kartarpur
Relations with the Safavid dynasty.
Shah Jahan and his sons captured the city of Kandahar in 1638 from the Safavids, prompting the retaliation of the Persians led by their powerful ruler Abbas II of Persia, who recaptured it in 1649, the Mughal armies were unable to recapture it despite repeated sieges during the Mughal–Safavid War. Shah Jahan also expanded the Mughal Empire to the west beyond the Khyber Pass to Ghazna and Kandahar.
Relations with the Ottoman Empire.
While he was encamped in Baghdad, the Ottoman Sultan Murad IV is known to have met the Shah Jahan's ambassadors: Mir Zarif and Mir Baraka, who presented 1000 pieces of finely embroidered cloth and even armor. Murad IV presented them with the finest weapons, saddles and Kaftans and ordered his forces to accompany the Mughals to the port of Basra, where they set sail to Thatta and finally Surat.
Shah Jahan had exchanged ambassadors and documents with the Murad IV, it was through these exchanges led by the Mughal ambassador Sayyid Muhiuddin and his counterpart the Ottoman ambassador Arsalan Agha, that Mughal Emperor Shah Jahan received Mimar Yusuf, Isa Muhammad Effendi and Ismail Effendi, two Turkish architects and students of the famous Koca Mimar Sinan Agha. Both of them later comprised among the Mughal team that would design and build the Taj Mahal.
War with Portuguese.
Shah Jahan gave orders in 1631 to Qasim Khan, the Mughal viceroy of Bengal, to drive out the Portuguese from their trading post at Port Hoogly, the trading post was heavily armed with cannons, battleships, fortified walls, and other instruments of war. The Portuguese were accused of trafficking by high Mughal officials and due to commercial competition the Mughal-controlled port of Saptagram began to slump. The Mughal Emperor Shah Jahan was particularly outraged by the activities of Jesuits in that region particularly when they were accused of abducting peasants. On 25 September 1632 the Mughal Army raised imperial banners and gained control over the Bandel region and the renegade garrison was punished.
Patronage of the arts.
Shah Jahan also intended to construct his capitol at Agra as an urban center that would rival both Istanbul and Isfahan in all its wealth and cultural opulence.
Shah Jahan's reign saw some of India's most well-known architectural and artistic accomplishments. The land revenue of the Mughal Empire under Shah Jahan was higher than that of any other Mughal ruler. The magnificence of his court was commented upon by several European travelers and by ambassadors from other parts of the world, including Francois Bernier and Thomas Roe. His famous Peacock Throne, with its trail blazing in the shifting natural colors of rubies, sapphires, and emeralds, was valued by the jeweler Tavernier at 6½ million pounds sterling.
Under Shah Jahan's rule, Mughal artistic and architectural achievements reached their zenith. Shah Jahan was a prolific builder with a highly refined aesthetic sense. Among his surviving buildings are the Red Fort and Jama Masjid in Delhi, the Shalimar Gardens of Lahore, sections of the Lahore Fort(such as Sheesh Mahal, and Naulakha pavilion), and his Tomb of Jahangir.
Religious attitude.
Shah Jahan was a more orthodox Muslim than his father and grandfather. Upon his accession, he adopted new policies which steadfastly reversed Akbar's generally liberal treatment of non-Muslims. In 1633, his sixth regnal year, Shah Jahan began to impose Sharia provisions against construction or repair of churches and temples and subsequently ordered the demolitions of newly built Hindu temples. He celebrated Islamic festivals with great pomp and grandeur and with an enthusiasm unfamiliar to his predecessors. Long-dormant royal interest in the Holy Cities also had revived during his reign. 
However, during the reign of Shah Jahan, the “Annual Car Festival” of the Jagannath Temple of Puri was specially patronized.
Later life.
When Shah Jahan became ill in 1658, Dara Shikoh (Mumtaz Mahal's eldest son) assumed the role of regent in his father's stead, which swiftly incurred the animosity of his brothers. Upon learning of his assumption of the regency, his younger brothers, Shuja, Viceroy of Bengal, and Murad Baksh, Viceroy of Gujarat, declared their independence, and marched upon Agra in order to claim their riches. Aurangzeb, the third son, and ablest of the brothers, gathered a well trained army and became its chief commander. He faced Dara's army near Agra and defeated him during the Battle of Samugarh. Although Shah Jahan fully recovered from his illness, Aurangzeb declared him incompetent to rule and put him under house arrest in Agra Fort.
Jahanara Begum Sahib, Jahan's first daughter, voluntarily shared his 8-year confinement and nursed him in his dotage. In January 1666, Shah Jahan fell ill.Confined to bed, he became progressively weaker until, on 22 January, he commended the ladies of the imperial court, particularly his consort of later years Akbarabadi Mahal, to the care of Jahanara. After reciting the "Kal'ma" ("Laa ilaaha ill allah") and verses from the Quran, one of the greatest of the Mughal Emperors died, aged 72.
Shah Jahan's chaplain Sayyid Muhammad Qanauji and Kazi Qurban of Agra came to the fort, moved his body to a nearby hall, washed it, enshrouded it and put it in a coffin of sandalwood.
Princess Jahanara had planned a state funeral which was to include a procession with Shah Jahan's body carried by eminent nobles followed by the notable citizens of Agra and officials scattering coins for the poor and needy. Aurangzeb refused to accommodate such ostentation. The body was taken by river to the Taj Mahal and was interred there next to the body of his beloved wife Mumtaz Mahal.
Contributions to architecture.
Shah Jahan left behind a grand legacy of structures constructed during his reign. He was one of the greatest patrons of Mughal architecture. His most famous building was the Taj Mahal, which he built out of love for his wife the empress Mumtaz Mahal.
Its structure was drawn with great care and architects from all over the world were called for this purpose. The building took twenty years to complete and was constructed from white marble underlaid with brick. Upon his death, his son Aurangzeb had him interred in it next to Mumtaz Mahal. Among his other constructions are the Red Fort also called the "Delhi Fort" or "Lal Qila" in Urdu, large sections of Agra Fort, the Jama Masjid, the Wazir Khan Mosque, the Moti Masjid, the Shalimar Gardens, sections of the Lahore Fort, the Jahangir mausoleum—his father's tomb, the construction of which was overseen by his stepmother Nur Jahan and the Shahjahan Mosque. He also had the Peacock Throne, Takht e Taus, made to celebrate his rule. Shah Jahan also placed profound verses of the Quran on his masterpieces of architecture.
A famous seamless celestial globe was produced in 1659–1660, by the Sindhi astronomer Muhammad Salih Tahtawi of Thatta with Arabic and Persian inscriptions.
The Shah Jahan Mosque in Thatta, Sindh province of Pakistan (100 km / 60 miles from Karachi) was built in the reign Shah Jahan in 1647. The mosque is built with red bricks with blue coloured glaze tiles probably imported from another Sindh's town of Hala. The mosque has overall 93 domes and it is world's largest mosque having such number of domes. It has been built keeping acoustics in mind. A person speaking inside one end of the dome can be heard at the other end when the speech exceeds 100 decibels. It has been on the tentative UNESCO World Heritage list since 1993.
Contribution to the arts.
All the inscriptions on the Taj Mahal tombs of Shah Jahan and his wife are in Persian Calligraphy on the tombs and on the Agra Fort in Quranic calligraphy and a Persian poem in Nastaʿlīq. Shah Jahan's cenotaph is bigger than that of his wife, but reflects the same elements: a larger casket on a slightly taller base, again decorated with astonishing precision with lapidary and calligraphy that identifies him.
The pen box and writing tablet were traditional Persian funerary icons decorating the caskets of men and women respectively. The Ninety Nine Names of God are found as calligraphic inscriptions in Persian nast Nastaʿlīq inscription style of calligraphy on the sides of the actual tomb of Mumtaz Mahal, in the crypt including "O Noble, O Magnificent, O Majestic, O Unique, O Eternal, O Glorious... ". The tomb of Shah Jahan bears a calligraphic inscription that reads: "He traveled from this world to the banquet-hall of Eternity on the night of the twenty-sixth of the month of Rajab, in the year 1076 Hijri."
Shah Jahan was very interested in Persian inscription and a Persian poet who requested a famous Persian calligrapher to decorate his palace and castles.
Coins.
In 1629 Shah Jahan made a new currency. The coins were made from silver, gold,bronze and copper.
Full title.
His full title as emperor was:
Shahanshah Al-Sultan al-'Azam wal Khaqan al-Mukarram
Malik-ul-Sultanat
Ala Hazrat Abu'l-Muzaffar Shahab ud-din Muhammad Shah Jahan I
Sahib-i-Qiran-i-Sani
Padshah Ghazi Zillu'llah
Firdaus-Ashiyani
Shahanshah—E—Sultanant Ul Hindiya Wal Mughaliya

</doc>
<doc id="28754" url="http://en.wikipedia.org/wiki?curid=28754" title="Saul Bellow">
Saul Bellow

Saul Bellow (10 June 1915 – 5 April 2005) was a Canadian-born American writer. For his literary contributions, Bellow was awarded the Pulitzer Prize, the Nobel Prize for Literature, and the National Medal of Arts. He is the only writer to win the National Book Award for Fiction three times and he received the Foundation's lifetime Medal for Distinguished Contribution to American Letters in 1990.
In the words of the Swedish Nobel Committee, his writing exhibited "the mixture of rich picaresque novel and subtle analysis of our culture, of entertaining adventure, drastic and tragic episodes in quick succession interspersed with philosophic conversation, all developed by a commentator with a witty tongue and penetrating insight into the outer and inner complications that drive us to act, or prevent us from acting, and that can be called the dilemma of our age." His best-known works include "The Adventures of Augie March," "Henderson the Rain King", "Herzog", "Mr. Sammler's Planet", "Seize the Day", "Humboldt's Gift" and "Ravelstein". Widely regarded as one of the 20th century's greatest authors, Bellow has had a "huge literary influence."
Bellow said that of all his characters Eugene Henderson, of "Henderson the Rain King", was the one most like himself. Bellow grew up as an insolent slum kid, a "thick-necked" rowdy, and an immigrant from Quebec. As Christopher Hitchens describes it, Bellow's fiction and principal characters reflect his own yearning for transcendence, a battle "to overcome not just ghetto conditions but also ghetto psychoses." Bellow's protagonists, in one shape or another, all wrestle with what Corde (Albert Corde, the dean in "The Dean's December") called "the big-scale insanities of the 20th century." This transcendence of the "unutterably dismal" (a phrase from "Dangling Man") is achieved, if it can be achieved at all, through a "ferocious assimilation of learning" (Hitchens) and an emphasis on nobility.
Biography.
Early life.
Saul Bellow was born Solomon Bellows in Lachine, Quebec, two years after his parents, Lescha (née Gordin) and Abraham Bellows, emigrated from Saint Petersburg, Russia. Bellow's family was Lithuanian-Jewish, with father born in Vilnius. Bellow celebrated his birthday in June, although he may have been born in July (in the Jewish community, it was customary to record the Hebrew date of birth, which does not always coincide with the Gregorian calendar). Of his family's emigration, Bellow wrote: 
A period of illness from a respiratory infection at age eight both taught him self-reliance (he was a very fit man despite his sedentary occupation) and provided an opportunity to satisfy his hunger for reading: reportedly, he decided to be a writer when he first read Harriet Beecher Stowe's "Uncle Tom's Cabin."
When Bellow was nine, his family moved to the Humboldt Park neighborhood on the West Side of Chicago, the city that formed the backdrop of many of his novels. Bellow's father, Abraham, was an onion importer. He also worked in a bakery, as a coal delivery man, and as a bootlegger. Bellow's mother, Liza, died when he was 17. He was left with his father and brother Maurice. His mother was deeply religious, and wanted her youngest son, Saul, to become a rabbi or a concert violinist. But he rebelled against what he later called the "suffocating orthodoxy" of his religious upbringing, and he began writing at a young age. Bellow's lifelong love for the Bible began at four when he learned Hebrew. Bellow also grew up reading William Shakespeare and the great Russian novelists of the 19th century. In Chicago, he took part in anthroposophical studies. Bellow attended Tuley High School on Chicago's west side where he befriended fellow writer Isaac Rosenfeld. In his 1959 novel "Henderson the Rain King", Bellow modeled the character King Dahfu on Rosenfeld.
Education and early career.
Bellow attended the University of Chicago but later transferred to Northwestern University. He originally wanted to study literature, but he felt the English department was anti-Jewish. Instead, he graduated with honors in anthropology and sociology. It has been suggested Bellow's study of anthropology had an influence on his literary style, and anthropological references pepper his works. Bellow later did graduate work at the University of Wisconsin–Madison.
Paraphrasing Bellow's description of his close friend Allan Bloom (see "Ravelstein"), John Podhoretz has said that both Bellow and Bloom "inhaled books and ideas the way the rest of us breathe air."
In the 1930s, Bellow was part of the Chicago branch of the Works Progress Administration Writer's Project, which included such future Chicago literary luminaries as Richard Wright and Nelson Algren. Many of the writers were radical: if they were not members of the Communist Party USA, they were sympathetic to the cause. Bellow was a Trotskyist, but because of the greater numbers of Stalinist-leaning writers he had to suffer their taunts.
In 1941 Bellow became a naturalized US citizen. In 1943, Maxim Lieber was his literary agent.
During World War II, Bellow joined the merchant marine and during his service he completed his first novel, "Dangling Man" (1944) about a young Chicago man waiting to be drafted for the war.
From 1946 through 1948 Bellow taught at the University of Minnesota, living on Commonwealth Avenue, in St. Paul, Minnesota.
In 1948, Bellow was awarded a Guggenheim Fellowship that allowed him to move to Paris, where he began writing "The Adventures of Augie March" (1953). Critics have remarked on the resemblance between Bellow's picaresque novel and the great 17th Century Spanish classic "Don Quixote". The book starts with one of American literature's most famous opening paragraphs, and it follows its titular character through a series of careers and encounters, as he lives by his wits and his resolve. Written in a colloquial yet philosophical style, "The Adventures of Augie March" established Bellow's reputation as a major author.
In the spring term of 1961 he taught creative writing at the University of Puerto Rico at Río Piedras.
One of his students was William Kennedy, who was encouraged by Bellow to write fiction.
Return to Chicago and mid-career.
Bellow lived in New York City for a number of years, but he returned to Chicago in 1962 as a professor at the Committee on Social Thought at the University of Chicago. The committee's goal was to have professors work closely with talented graduate students on a multi-disciplinary approach to learning. Bellow taught on the committee for more than 30 years, alongside his close friend, the philosopher Allan Bloom.
There were also other reasons for Bellow's return to Chicago, where he moved into the Hyde Park neighborhood with his third wife, Susan Glassman. Bellow found Chicago vulgar but vital, and more representative of America than New York. He was able to stay in contact with old high school friends and a broad cross-section of society. In a 1982 profile, Bellow's neighborhood was described as a high-crime area in the city's center, and Bellow maintained he had to live in such a place as a writer and "stick to his guns."
Bellow hit the bestseller list in 1964 with his novel "Herzog". Bellow was surprised at the commercial success of this cerebral novel about a middle-aged and troubled college professor who writes letters to friends, scholars and the dead, but never sends them. Bellow returned to his exploration of mental instability, and its relationship to genius, in his 1975 novel "Humboldt's Gift". Bellow used his late friend and rival, the brilliant but self-destructive poet Delmore Schwartz, as his model for the novel's title character, Von Humboldt Fleisher. Bellow also used Rudolf Steiner's spiritual science, anthroposophy, as a theme in the book, having attended a study group in Chicago. He was elected a Fellow of the American Academy of Arts and Sciences in 1969.
Nobel Prize and later career.
Propelled by the success of "Humboldt's Gift", Bellow won the Nobel Prize in literature in 1976. In the 70-minute address he gave to an audience in Stockholm, Sweden, Bellow called on writers to be beacons for civilization and awaken it from intellectual torpor.
The following year, the National Endowment for the Humanities selected Bellow for the Jefferson Lecture, the U.S. federal government's highest honor for achievement in the humanities. Bellow's lecture was entitled "The Writer and His Country Look Each Other Over."
Bellow traveled widely throughout his life, mainly to Europe, which he sometimes visited twice a year. As a young man, Bellow went to Mexico City to meet Leon Trotsky, but the expatriate Russian revolutionary was assassinated the day before they were to meet. Bellow's social contacts were wide and varied. He tagged along with Robert F. Kennedy for a magazine profile he never wrote, he was close friends with the author Ralph Ellison. His many friends included the journalist Sydney J. Harris and the poet John Berryman. 
While sales of Bellow's first few novels were modest, that turned around with "Herzog". Bellow continued teaching well into his old age, enjoying its human interaction and exchange of ideas. He taught at Yale University, University of Minnesota, New York University, Princeton University, University of Puerto Rico, University of Chicago, Bard College and Boston University, where he co-taught a class with James Wood ('modestly absenting himself' when it was time to discuss "Seize the Day"). In order to take up his appointment at Boston, Bellow moved in 1993 from Chicago to Brookline, Massachusetts, where he died on 5 April 2005, at age 89. He is buried at the Jewish cemetery Shir HeHarim of Brattleboro, Vermont.
Bellow was married five times, with all but his last marriage ending in divorce. His son by his first marriage, Greg Bellow, became a psychotherapist; Greg Bellow published "Saul Bellow’s Heart: A Son’s Memoir" in 2013, nearly a decade after his father's death. Bellow's son by his second marriage, Adam, published a nonfiction book "In Praise of Nepotism" in 2003. Bellow's wives were Anita Goshkin, Alexandra (Sondra) Tsachacbasov, Susan Glassman, Alexandra Ionescu Tulcea and Janis Freedman. In 1999, when he was 84, Bellow had a daughter, Rosie, his fourth child, with Freedman.
While he read voluminously, Bellow also played the violin and followed sports. Work was a constant for him, but he at times toiled at a plodding pace on his novels, frustrating the publishing company.
His early works earned him the reputation as a major novelist of the 20th century, and by his death he was widely regarded as one of the greatest living novelists. He was the first writer to win three National Book Awards in all award categories. His friend and protege Philip Roth has said of him, "The backbone of 20th-century American literature has been provided by two novelists—William Faulkner and Saul Bellow. Together they are the Melville, Hawthorne, and Twain of the 20th century." James Wood, in a eulogy of Bellow in "The New Republic", wrote:
Themes and style.
The author's works speak to the disorienting nature of modern civilization, and the countervailing ability of humans to overcome their frailty and achieve greatness (or at least awareness). Bellow saw many flaws in modern civilization, and its ability to foster madness, materialism and misleading knowledge. Principal characters in Bellow's fiction have heroic potential, and many times they stand in contrast to the negative forces of society. Often these characters are Jewish and have a sense of alienation or otherness.
Jewish life and identity is a major theme in Bellow's work, although he bristled at being called a "Jewish writer." Bellow's work also shows a great appreciation of America, and a fascination with the uniqueness and vibrancy of the American experience.
Bellow's work abounds in references and quotes from the likes of Marcel Proust and Henry James, but he offsets these high-culture references with jokes. Bellow interspersed autobiographical elements into his fiction, and many of his principal characters were said to bear a resemblance to him.
Criticism, controversy and conservative cultural activism.
Martin Amis described Bellow as "The greatest American author ever, in my view".
For Linda Grant, "What Bellow had to tell us in his fiction was that it was worth it, being alive."
On the other hand, Bellow's detractors considered his work conventional and old-fashioned, as if the author was trying to revive the 19th-century European novel. In a private letter, Vladimir Nabokov once referred to Bellow as a "miserable mediocrity." Journalist and author Ron Rosenbaum described Bellow's "Ravelstein" (2000) as the only book that rose above Bellow's failings as an author. Rosenbaum wrote,
Sam Tanenhaus wrote in "New York Times Book Review" in 2007:
But, Tanenhaus went on to answer his question:
V. S. Pritchett praised Bellow, finding his shorter works to be his best. Pritchett called Bellow's novella "Seize the Day" a "small gray masterpiece."
As he grew older, Bellow moved decidedly away from leftist politics and became identified with cultural conservatism. His opponents included feminism, campus activism and postmodernism. Bellow also thrust himself into the often contentious realm of Jewish and African-American relations. Bellow has also been critical of multiculturalism and once said: "Who is the Tolstoy of the Zulus? The Proust of the Papuans? I'd be glad to read him."
Despite his identification with Chicago, he kept aloof from some of that city's more conventional writers. In a 2006 interview with "Stop Smiling" magazine, Studs Terkel said of Bellow: "I didn't know him too well. We disagreed on a number of things politically. In the protests in the beginning of Norman Mailer's "Armies of the Night", when Mailer, Robert Lowell and Paul Goodman were marching to protest the Vietnam War, Bellow was invited to a sort of counter-gathering. He said, 'Of course I'll attend'. But he made a big thing of it. Instead of just saying OK, he was proud of it. So I wrote him a letter and he didn't like it. He wrote me a letter back. He called me a Stalinist. But otherwise, we were friendly. He was a brilliant writer, of course. I love "Seize the Day"."

</doc>
<doc id="28756" url="http://en.wikipedia.org/wiki?curid=28756" title="Stereochemistry">
Stereochemistry

Stereochemistry, a subdiscipline of chemistry, involves the study of the relative spatial arrangement of atoms that form the structure of molecules and their manipulation. An important branch of stereochemistry is the study of chiral molecules.
Stereochemistry is also known as 3D chemistry because the prefix "stereo-" means "three-dimensionality".
The study of stereochemistry focuses on stereoisomers and spans the entire spectrum of organic, inorganic, biological, physical and especially supramolecular chemistry. Stereochemistry includes methods for determining and describing these relationships; the effect on the physical or biological properties these relationships impart upon the molecules in question, and the manner in which these relationships influence the reactivity of the molecules in question (dynamic stereochemistry).
History.
Louis Pasteur could rightly be described as the first stereochemist, having observed in 1849 that salts of tartaric acid collected from wine production vessels could rotate plane polarized light, but that salts from other sources did not. This property, the only physical property in which the two types of tartrate salts differed, is due to optical isomerism. In 1874, Jacobus Henricus van 't Hoff and Joseph Le Bel explained optical activity in terms of the tetrahedral arrangement of the atoms bound to carbon.
Significance.
Cahn-Ingold-Prelog priority rules are part of a system for describing a molecule's stereochemistry. They rank the atoms around a stereocenter in a standard way, allowing the relative position of these atoms in the molecule to be described unambiguously. A Fischer projection is a simplified way to depict the stereochemistry around a stereocenter.
Thalidomide example.
An often cited example of the importance of stereochemistry relates to the thalidomide disaster. Thalidomide is a pharmaceutical drug, first prepared in 1957 in Germany, prescribed for treating morning sickness in pregnant women. The drug was discovered to be teratogenic, causing serious genetic damage to early embryonic growth and development, leading to limb deformation in babies. Some of the several proposed mechanisms of teratogenecity involve a different biological function for the ("R")- and the ("S")-thalidomide enantiomers. In the human body however, thalidomide undergoes racemization: even if only one of the two enantiomers is administered as a drug, the other enantiomer is produced as a result of metabolism. Accordingly, it is incorrect to state that one of the stereoisomer is safe while the other is teratogenic. Thalidomide is currently used for the treatment of other diseases, notably cancer and leprosy. Strict regulations and controls have been enabled to avoid its use by pregnant women and prevent developmental deformations. This disaster was a driving force behind requiring strict testing of drugs before making them available to the public.
Definitions.
Many definitions that describe a specific conformer (IUPAC Gold Book) exist, developed by William Klyne and Vladimir Prelog, constituting their Klyne-Prelog System of nomenclature:
Torsional strain results from resistance to twisting about a bond.

</doc>
<doc id="28757" url="http://en.wikipedia.org/wiki?curid=28757" title="Simultaneity">
Simultaneity

Simultaneity is the property of two events happening at the same time in a frame of reference. According to Einstein's Theory of Relativity, simultaneity is not an absolute property between events; what is simultaneous in one frame of reference will not necessarily be simultaneous in another. (See Relativity of simultaneity.) For inertial frames moving at speeds small compared to the speed of light with respect to one another this effect is small and can for practical matters be ignored such that simultaneity can be treated as an absolute property.
The word derives from the Latin "simul", at the same time (see "sem"-1 in Indo-European Roots) plus the suffix "-taneous", abstracted from "spontaneous" (which in turn comes directly from Latin).
The noun "simult" means a supernatural coincidence, two or more divinely inspired events that occur at or near the same period of time that are related to each other in both noticeable and unnoticeable characteristics.

</doc>
<doc id="28758" url="http://en.wikipedia.org/wiki?curid=28758" title="Spacetime">
Spacetime

In physics, spacetime (also space–time, space time or space–time continuum) is any mathematical model that combines space and time into a single interwoven continuum. The spacetime of our universe is usually interpreted from a Euclidean space perspective, which regards space as consisting of three dimensions, and time as consisting of one dimension, the "fourth dimension". By combining space and time into a single manifold called Minkowski space, physicists have significantly simplified a large number of physical theories, as well as described in a more uniform way the workings of the universe at both the supergalactic and subatomic levels.
In non-relativistic classical mechanics, the use of Euclidean space instead of spacetime is appropriate, because time is treated as universal with a constant rate of passage that is independent of the state of motion of an observer. In relativistic contexts, time cannot be separated from the three dimensions of space, because the observed rate at which time passes for an object depends on the object's velocity relative to the observer and also on the strength of gravitational fields, which can slow the passage of time for an object as seen by an observer outside the field.
In cosmology, the concept of spacetime combines space and time to a single abstract universe. Mathematically it is a manifold consisting of "events" which are described by some type of coordinate system. Typically three spatial dimensions (length, width, height), and one temporal dimension (time) are required. Dimensions are independent components of a coordinate grid needed to locate a point in a certain defined "space". For example, on the globe the latitude and longitude are two independent coordinates which together uniquely determine a location. In spacetime, a coordinate grid that spans the 3+1 dimensions locates events (rather than just points in space), i.e., time is added as another dimension to the coordinate grid. This way the coordinates specify "where" and "when" events occur. However, the unified nature of spacetime and the freedom of coordinate choice it allows imply that to express the temporal coordinate in one coordinate system requires both temporal and spatial coordinates in another coordinate system. Unlike in normal spatial coordinates, there are still restrictions for how measurements can be made spatially and temporally (see Spacetime intervals). These restrictions correspond roughly to a particular mathematical model which differs from Euclidean space in its manifest symmetry.
Until the beginning of the 20th century, time was believed to be independent of motion, progressing at a fixed rate in all reference frames; however, later experiments revealed that time slows at higher speeds of the reference frame relative to another reference frame. Such slowing, called time dilation, is explained in special relativity theory. Many experiments have confirmed time dilation, such as the relativistic decay of muons from cosmic ray showers and the slowing of atomic clocks aboard a Space Shuttle relative to synchronized Earth-bound inertial clocks. The duration of time can therefore vary according to events and reference frames.
When dimensions are understood as mere components of the grid system, rather than physical attributes of space, it is easier to understand the alternate dimensional views as being simply the result of coordinate transformations.
The term "spacetime" has taken on a generalized meaning beyond treating spacetime events with the normal 3+1 dimensions. It is really the combination of space and time. Other proposed spacetime theories include additional dimensions—normally spatial but there exist some speculative theories that include additional temporal dimensions and even some that include dimensions that are neither temporal nor spatial (e.g., superspace). How many dimensions are needed to describe the universe is still an open question. Speculative theories such as string theory predict 10 or 26 dimensions (with M-theory predicting 11 dimensions: 10 spatial and 1 temporal), but the existence of more than four dimensions would only appear to make a difference at the subatomic level.
Spacetime in literature.
Incas regarded space and time as a single concept, referred to as pacha (Quechua: "pacha", Aymara: "pacha"). The peoples of the Andes maintain a similar understanding.
Arthur Schopenhauer wrote in §18 of "On the Fourfold Root of the Principle of Sufficient Reason" (1813): "the representation of coexistence is impossible in Time alone; it depends, for its completion, upon the representation of Space; because, in mere Time, all things follow one another, and in mere Space all things are side by side; it is accordingly only by the combination of Time and Space that the representation of coexistence arises".
The idea of a unified spacetime is stated by Edgar Allan Poe in his essay on cosmology titled "Eureka" (1848) that "Space and duration are one". In 1895, in his novel "The Time Machine", H. G. Wells wrote, "There is no difference between time and any of the three dimensions of space except that our consciousness moves along it", and that "any real body must have extension in four directions: it must have Length, Breadth, Thickness, and Duration".
Marcel Proust, in his novel "Swann's Way" (published 1913), describes the village church of his childhood's Combray as "a building which occupied, so to speak, four dimensions of space—the name of the fourth being Time".
Mathematical concept.
In Encyclopedie under the term "dimension" Jean le Rond d'Alembert speculated that duration (time) might be considered a fourth dimension if the idea was not too novel.
Another early venture was by Joseph Louis Lagrange in his "Theory of Analytic Functions" (1797, 1813). He said, "One may view mechanics as a geometry of four dimensions, and mechanical analysis as an extension of geometric analysis".
The ancient idea of the cosmos gradually was described mathematically with differential equations, differential geometry, and abstract algebra. These mathematical articulations blossomed in the nineteenth century as electrical technology stimulated men like Michael Faraday and James Clerk Maxwell to describe the reciprocal relations of electric and magnetic fields.
Daniel Siegel phrased Maxwell's role in relativity as follows:
[...] the idea of the propagation of forces at the velocity of light through the electromagnetic field as described by Maxwell's equations—rather than instantaneously at a distance—formed the necessary basis for relativity theory.
Maxwell used vortex models in his papers on On Physical Lines of Force, but ultimately gave up on any substance but the electromagnetic field. Pierre Duhem wrote:
[Maxwell] was not able to create the theory that he envisaged except by giving up the use of any model, and by extending by means of analogy the abstract system of electrodynamics to displacement currents.
In Siegel's estimation, "this very abstract view of the electromagnetic fields, involving no visualizable picture of what is going on out there in the field, is Maxwell's legacy."
Describing the behaviour of electric fields and magnetic fields led Maxwell to a unified view of an electromagnetic field. Being functions, these fields took values on a domain, a piece of spacetime. It is the intermingling of electric and magnetic manifestations, described by Maxwell's equations that give spacetime its structure. In particular, the rate of motion of an observer determines the electric and magnetic profiles of the electromagnetic field. The propagation of the field is determined by the electromagnetic wave equation which also requires spacetime for description.
Spacetime was described as an affine space with quadratic form in Minkowski space of 1908. In his 1914 textbook "The Theory of Relativity", Ludwik Silberstein used biquaternions to represent events in Minkowski space. He also exhibited the Lorentz transformations between observers of differing velocities as biquaternion mappings. Biquaternions were described in 1853 by W. R. Hamilton, so while the physical interpretation was new, the mathematics was well known in English literature, making relativity an instance of applied mathematics.
The first inkling of general relativity in spacetime was articulated by W. K. Clifford.
Description of the effect of gravitation on space and time was found to be most easily visualized as a "warp" or stretching in the geometrical fabric of space and time, in a smooth and continuous way that changed smoothly from point-to-point along the spacetime fabric. In 1947 James Jeans provided a concise summary of the development of spacetime theory in his book "The Growth of Physical Science".
Basic concepts.
Spacetimes are the arenas in which all physical events take place—an event is a point in spacetime specified by its time and place. For example, the motion of planets around the sun may be described in a particular type of spacetime, or the motion of light around a rotating star may be described in another type of spacetime. The basic elements of spacetime are events. In any given spacetime, an event is a unique position at a unique time. Because events are spacetime points, an example of an event in classical relativistic physics is formula_1, the location of an elementary (point-like) particle at a particular time. A spacetime itself can be viewed as the union of all events in the same way that a line is the union of all of its points, formally organized into a manifold, a space which can be described at small scales using coordinate systems.
A spacetime is independent of any observer. However, in describing physical phenomena (which occur at certain moments of time in a given region of space), each observer chooses a convenient metrical coordinate system. Events are specified by four real numbers in any such coordinate system. The trajectories of elementary (point-like) particles through space and time are thus a continuum of events called the world line of the particle. Extended or composite objects (consisting of many elementary particles) are thus a union of many world lines twisted together by virtue of their interactions through spacetime into a "world-braid".
However, in physics, it is common to treat an extended object as a "particle" or "field" with its own unique (e.g., center of mass) position at any given time, so that the world line of a particle or light beam is the path that this particle or beam takes in the spacetime and represents the history of the particle or beam. The world line of the orbit of the Earth (in such a description) is depicted in two spatial dimensions "x" and "y" (the plane of the Earth's orbit) and a time dimension orthogonal to "x" and "y". The orbit of the Earth is an ellipse in space alone, but its world line is a helix in spacetime.
The unification of space and time is exemplified by the common practice of selecting a metric (the measure that specifies the interval between two events in spacetime) such that all four dimensions are measured in terms of units of distance: representing an event as formula_2 (in the Lorentz metric) or formula_3 (in the original Minkowski metric) where formula_4 is the speed of light. The metrical descriptions of Minkowski Space and spacelike, lightlike, and timelike intervals given below follow this convention, as do the conventional formulations of the Lorentz transformation.
Spacetime intervals in flat space.
In a Euclidean space, the separation between two points is measured by the distance between the two points. The distance is purely spatial, and is always positive. In spacetime, the displacement four-vector Δ"R" is given by the space displacement vector Δ"r" and the time difference Δ"t" between the events. The "spacetime interval", also called "invariant interval", between the two events, "s"2, is defined as:
where "c" is the speed of light. The choice of signs for formula_5 above follows the space-like convention (−+++).
Spacetime intervals may be classified into three distinct types, based on whether the temporal separation (formula_6) or the spatial separation (formula_7) of the two events is greater: time-like, light-like or space-like.
Certain types of world lines are called geodesics of the spacetime – straight lines in the case of Minkowski space and their closest equivalent in the curved spacetime of general relativity. In the case of purely time-like paths, geodesics are (locally) the paths of greatest separation (spacetime interval) as measured along the path between two events, whereas in Euclidean space and Riemannian manifolds, geodesics are paths of shortest distance between two points. The concept of geodesics becomes central in general relativity, since geodesic motion may be thought of as "pure motion" (inertial motion) in spacetime, that is, free from any external influences.
Time-like interval.
For two events separated by a time-like interval, enough time passes between them that there could be a cause–effect relationship between the two events. For a particle traveling through space at less than the speed of light, any two events which occur to or by the particle must be separated by a time-like interval. Event pairs with time-like separation define a negative spacetime interval (formula_9) and may be said to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur in the same spatial location, but there is no reference frame in which the two events can occur at the same time.
The measure of a time-like spacetime interval is described by the proper time interval, formula_10:
</math>   (proper time interval).}} The proper time interval would be measured by an observer with a clock traveling between the two events in an inertial reference frame, when the observer's path intersects each event as that event occurs. (The proper time interval defines a real number, since the interior of the square root is positive.)
Light-like interval.
In a light-like interval, the spatial distance between two events is exactly balanced by the time between the two events. The events define a spacetime interval of zero (formula_12). Light-like intervals are also known as "null" intervals.
Events which occur to or are initiated by a photon along its path (i.e., while traveling at formula_4, the speed of light) all have light-like separation. Given one event, all those events which follow at light-like intervals define the propagation of a light cone, and all the events which preceded from a light-like interval define a second (graphically inverted, which is to say "pastward") light cone.
Space-like interval.
When a space-like interval separates two events, not enough time passes between their occurrences for there to exist a causal relationship crossing the spatial distance between the two events at the speed of light or slower. Generally, the events are considered not to occur in each other's future or past. There exists a reference frame such that the two events are observed to occur at the same time, but there is no reference frame in which the two events can occur in the same spatial location.
For these space-like event pairs with a positive spacetime interval (formula_15), the measurement of space-like separation is the proper distance, formula_16:
Like the proper time of time-like intervals, the proper distance of space-like spacetime intervals is a real number value.
Interval as area.
The interval has been presented as the area of an oriented rectangle formed by two events and isotropic lines through them. Time-like or space-like separations correspond to oppositely oriented rectangles, one type considered to have rectangles of negative area. The case of two events separated by light corresponds to the rectangle degenerating to the segment between the events and zero area. The transformations leaving interval-length invariant are the area-preserving squeeze mappings.
The parameters traditionally used rely on quadrature of the hyperbola, which is the natural logarithm. This transcendental function is essential in mathematical analysis as its inverse unites circular functions and hyperbolic functions: The exponential function, e"t",  "t" a real number, used in the hyperbola (e"t", e–"t" ), generates hyperbolic sectors and the hyperbolic angle parameter. The functions cosh and sinh, used with rapidity as hyperbolic angle, provide the common representation of squeeze in the form
formula_17 or as the split-complex unit 
formula_18
Mathematics of spacetimes.
For physical reasons, a spacetime continuum is mathematically defined as a four-dimensional, smooth, connected Lorentzian manifold formula_19. This means the smooth Lorentz metric formula_20 has signature formula_21. The metric determines the geometry of spacetime, as well as determining the geodesics of particles and light beams. About each point (event) on this manifold, coordinate charts are used to represent observers in reference frames. Usually, Cartesian coordinates formula_22 are used. Moreover, for simplicity's sake, units of measurement are usually chosen such that the speed of light formula_4 is equal to 1.
A reference frame (observer) can be identified with one of these coordinate charts; any such observer can describe any event formula_24. Another reference frame may be identified by a second coordinate chart about formula_24. Two observers (one in each reference frame) may describe the same event formula_24 but obtain different descriptions.
Usually, many overlapping coordinate charts are needed to cover a manifold. Given two coordinate charts, one containing formula_24 (representing an observer) and another containing formula_28 (representing another observer), the intersection of the charts represents the region of spacetime in which both observers can measure physical quantities and hence compare results. The relation between the two sets of measurements is given by a non-singular coordinate transformation on this intersection. The idea of coordinate charts as local observers who can perform measurements in their vicinity also makes good physical sense, as this is how one actually collects physical data—locally.
For example, two observers, one of whom is on Earth, but the other one who is on a fast rocket to Jupiter, may observe a comet crashing into Jupiter (this is the event formula_24). In general, they will disagree about the exact location and timing of this impact, i.e., they will have different 4-tuples formula_22 (as they are using different coordinate systems). Although their kinematic descriptions will differ, dynamical (physical) laws, such as momentum conservation and the first law of thermodynamics, will still hold. In fact, relativity theory requires more than this in the sense that it stipulates these (and all other physical) laws must take the same form in all coordinate systems. This introduces tensors into relativity, by which all physical quantities are represented.
Geodesics are said to be time-like, null, or space-like if the tangent vector to one point of the geodesic is of this nature. Paths of particles and light beams in spacetime are represented by time-like and null (light-like) geodesics, respectively.
Topology.
The assumptions contained in the definition of a spacetime are usually justified by the following considerations.
The connectedness assumption serves two main purposes. First, different observers making measurements (represented by coordinate charts) should be able to compare their observations on the non-empty intersection of the charts. If the connectedness assumption were dropped, this would not be possible. Second, for a manifold, the properties of connectedness and path-connectedness are equivalent, and one requires the existence of paths (in particular, geodesics) in the spacetime to represent the motion of particles and radiation.
Every spacetime is paracompact. This property, allied with the smoothness of the spacetime, gives rise to a smooth linear connection, an important structure in general relativity. Some important theorems on constructing spacetimes from compact and non-compact manifolds include the following:
Spacetime symmetries.
Often in relativity, spacetimes that have some form of symmetry are studied. As well as helping to classify spacetimes, these symmetries usually serve as a simplifying assumption in specialized work. Some of the most popular ones include:
Causal structure.
The causal structure of a spacetime describes causal relationships between pairs of points in the spacetime based on the existence of certain types of curves joining the points.
Spacetime in special relativity.
The geometry of spacetime in special relativity is described by the Minkowski metric on R4. This spacetime is called Minkowski space. The Minkowski metric is usually denoted by formula_31 and can be written as a four-by-four matrix:
where the Landau–Lifshitz space-like convention is being used. A basic assumption of relativity is that coordinate transformations must leave spacetime intervals invariant. Intervals are invariant under Lorentz transformations. This invariance property leads to the use of four-vectors (and other tensors) in describing physics.
Strictly speaking, one can also consider events in Newtonian physics as a single spacetime. This is Galilean–Newtonian relativity, and the coordinate systems are related by Galilean transformations. However, since these preserve spatial and temporal distances independently, such a spacetime can be decomposed into spatial coordinates plus temporal coordinates, which is not possible in the general case.
Spacetime in general relativity.
In general relativity, it is assumed that spacetime is curved by the presence of matter (energy), this curvature being represented by the Riemann tensor. In special relativity, the Riemann tensor is identically zero, and so this concept of "non-curvedness" is sometimes expressed by the statement "Minkowski spacetime is flat."
The earlier discussed notions of time-like, light-like and space-like intervals in special relativity can similarly be used to classify one-dimensional curves through curved spacetime. A time-like curve can be understood as one where the interval between any two infinitesimally close events on the curve is time-like, and likewise for light-like and space-like curves. Technically the three types of curves are usually defined in terms of whether the tangent vector at each point on the curve is time-like, light-like or space-like. The world line of a slower-than-light object will always be a time-like curve, the world line of a massless particle such as a photon will be a light-like curve, and a space-like curve could be the world line of a hypothetical tachyon. In the local neighborhood of any event, time-like curves that pass through the event will remain inside that event's past and future light cones, light-like curves that pass through the event will be on the surface of the light cones, and space-like curves that pass through the event will be outside the light cones. One can also define the notion of a three-dimensional "spacelike hypersurface", a continuous three-dimensional "slice" through the four-dimensional property with the property that every curve that is contained entirely within this hypersurface is a space-like curve.
Many spacetime continua have physical interpretations which most physicists would consider bizarre or unsettling. For example, a compact spacetime has closed timelike curves, which violate our usual ideas of causality (that is, future events could affect past ones). For this reason, mathematical physicists usually consider only restricted subsets of all the possible spacetimes. One way to do this is to study "realistic" solutions of the equations of general relativity. Another way is to add some additional "physically reasonable" but still fairly general geometric restrictions and try to prove interesting things about the resulting spacetimes. The latter approach has led to some important results, most notably the Penrose–Hawking singularity theorems.
Quantized spacetime.
In general relativity, spacetime is assumed to be smooth and continuous—and not just in the mathematical sense. In the theory of quantum mechanics, there is an inherent discreteness present in physics. In attempting to reconcile these two theories, it is sometimes postulated that spacetime should be quantized at the very smallest scales. Current theory is focused on the nature of spacetime at the Planck scale. Causal sets, loop quantum gravity, string theory, causal dynamical triangulation, and black hole thermodynamics all predict a quantized spacetime with agreement on the order of magnitude. Loop quantum gravity makes precise predictions about the geometry of spacetime at the Planck scale.

</doc>
<doc id="28760" url="http://en.wikipedia.org/wiki?curid=28760" title="SimCity (1989 video game)">
SimCity (1989 video game)

SimCity, later renamed SimCity Classic, is a city-building simulation video game, first released on February 2, 1989, and designed by Will Wright for the Macintosh computer. "SimCity" was Maxis's second product, which has since been ported into various personal computers and game consoles, and spawned several sequels including "SimCity 2000" in 1993, "SimCity 3000" in 1999, "SimCity 4" in 2003, "SimCity DS", "SimCity Societies" in 2007, and "SimCity" in 2013. Until the release of "The Sims" in 2000, the "SimCity" series was the best-selling line of computer games made by Maxis. "SimCity" spawned a series of "Sim" games.
On January 10, 2008 the "SimCity" source code was released under the free software GPL 3 license under the original working title- Micropolis.
History.
"SimCity" was originally developed by game designer Will Wright. The inspiration for "SimCity" came from a feature of the game "Raid on Bungeling Bay" that allowed Wright to create his own maps during development. Wright soon found he enjoyed creating maps more than playing the actual game, and "SimCity" was born. While developing "SimCity", Wright cultivated a real love of the intricacies and theories of urban planning and acknowledges the influence of System Dynamics which was developed by Jay Wright Forrester and whose book on the subject laid the foundations for the simulation. In addition, Wright also was inspired by reading "The Seventh Sally", a short story by Stanisław Lem, in which an engineer encounters a deposed tyrant, and creates a miniature city with artificial citizens for the tyrant to oppress. The game reflected Wright's approval of mass transit and disapproval of nuclear power; Maxis president Jeff Braun stated "We're pushing political agendas".
The first version of the game was developed for the Commodore 64 in 1985; it was not published for another four years. The original working title of SimCity was "Micropolis". The game represented an unusual paradigm in computer gaming, in that it could neither be won nor lost; as a result, game publishers did not believe it was possible to market and sell such a game successfully. Brøderbund declined to publish the title when Wright proposed it, and he pitched it to a range of major game publishers without success. Finally, Braun, founder of the tiny software company Maxis, agreed to publish "SimCity" as one of two initial games for the company.
Wright and Braun returned to Brøderbund to formally clear the rights to the game in 1988, when "SimCity" was near completion. After Brøderbund executives Gary Carlston and Don Daglow saw "SimCity", they signed Maxis to a distribution deal for both of its initial games. With that, four years after initial development, "SimCity" was released for the Amiga and Macintosh platforms, followed by the IBM PC and Commodore 64 later in 1989.
Objective.
The objective of "SimCity", as the name of the game suggests, is to build and design a city, without specific goals to achieve (except in the scenarios, see below). The player can mark land as being zoned as commercial, industrial, or residential, add buildings, change the tax rate, build a power grid, build transportation systems and take many other actions, in order to enhance the city. Once able to construct buildings in a particular area, the too-small-to-see residents, known as "Sims", may choose to construct and upgrade houses, apartment blocks, light or heavy industrial buildings, commercial buildings, hospitals, churches, and other structures. The Sims make these choices based on such factors as traffic levels, adequate electrical power, crime levels, and proximity to other types of buildings—for example, residential areas next to a power plant will seldom appreciate to the highest grade of housing.
Also, the player may face disasters including flooding, tornadoes, fires (often from air disasters or even shipwrecks), earthquakes and attacks by monsters. In addition, monsters and tornadoes can trigger train crashes by running into passing trains. There was also a reported case of a nuclear meltdown. Later disasters in the game's sequels included lightning strikes, volcanoes, meteors and attack by extraterrestrial craft. In the Super Nintendo version and later, one can also build rewards when they are given to them, such as a mayor's mansion or a casino.
Scenarios.
The original "SimCity" kicked off a tradition of goal-centered, timed scenarios that could be won or lost depending on the performance of the player/mayor. The scenarios were an addition suggested by Brøderbund in order to make "SimCity" more like a game. The original cities were all based on real world cities and attempted to re-create their general layout, a tradition carried on in "SimCity 2000" and in special scenario packs. While most scenarios either take place in a fictional timeline or have a city under siege by a fictional disaster, a handful of available scenarios are based on actual historical events.
The original scenarios are:
The PC version (IBM, Tandy compatible; on floppy disk), CD re-release, as well as the Amiga and Atari ST versions included two additional scenarios:
In addition, the later edition of "SimCity" on the Super NES included the basics of these two scenarios in two, more difficult scenarios that were made available after a player had completed the original scenarios:
While the scenarios were meant to be solved strategically, many players discovered that by dropping the tax rate to zero near the end of the allotted timespan, one could heavily influence public opinion and population growth. In scenarios such as San Francisco, where rebuilding and, by extension, maintaining population growth play a large part of the objective, this kind of manipulation can mean a relatively easy victory. Later titles in the series would take steps to prevent players from using the budget to influence the outcome of scenarios.
Ports and versions.
"SimCity" was originally released for home computers, including the Amiga, Atari ST and DOS-based IBM PC. After its success it was converted for several other computer platforms and video game consoles, specifically the Commodore 64, Macintosh, Acorn Archimedes, Amstrad CPC, Sinclair ZX Spectrum, BBC Micro, Acorn Electron, Super Nintendo Entertainment System (which was later released on Virtual Console), EPOC32, mobile phone, Internet, Windows, FM-Towns, OLPC XO-1 and NeWS HyperLook on Sun Unix. The game is also available as a multiplayer version for X11 TCL/Tk on various Unix, Linux, DESQview and OS/2 operating systems. In addition, a version was developed in 1991 for the Nintendo Entertainment System, and another Japanese version was initiated in 1992 for the MSX computers, but these two were never released. Certain versions have since been re-released with various add-ons, including extra scenarios. An additional extra add on for the Windows version of SimCity Classic was a level editor. This editor could be opened without use of the SimCity Classic disc. The level editor is a simple tool that allows the user to create grasslands, dirt land, and water portions.
The IBM version of SimCity is notable for the unusually large amount of graphics modes it supports; the game can run in CGA, Tandy, EGA low-resolution, EGA high resolution, EGA monochrome, and VGA monochrome. A later release dropped CGA, Tandy, and EGA low-resolution support and replaced them with color VGA modes.
In 2007 the developer Don Hopkins released a free and open source version of the original SimCity, renamed Micropolis (the original working title) for trademark reasons, for the One Laptop per Child XO-1.
In 2008, Maxis established an online browser-based version of SimCity. A second browser-based version was later released under the name "Micropolis". In 2013, another browser-based version—this time ported using Javascript and HTML5—was released as "micropolisJS".
SimCity for the Super Nintendo Entertainment System.
"SimCity" for the Super Nintendo Entertainment System features the same gameplay and scenario features; however, since it was developed and published by Nintendo, the company incorporated their own ideas. Instead of the Godzilla monster disaster, Bowser of the Super Mario series becomes the attacking monster, and once the city reaches a landmark 500,000 populace, the player receives a Mario statue that is placeable in the city. The Nintendo port also features special buildings the player may receive as rewards, similar to the rewards buildings in "SimCity 2000". The game also includes schools and hospitals, though they cannot be placed by the player. Instead, the game will sometimes turn an empty residential lot into one. There are also city classifications, such as becoming a metropolis at 100,000 people. Also unique to the SNES version is a character named "Dr. Wright" (whose physical appearance is based on Will Wright) who acts as an adviser to the player. The soundtrack to the Nintendo version was composed by Soyo Oka. This edition is featured as Nintendo's Player's Choice as a million seller.
In August 1996 a version of the game entitled "BS Sim City Machizukuri Taikai" was broadcast exclusively to Japanese players via the SNES's Satellaview subsystem. Later, an official Japan-only sequel titled "SimCity 64" was released for the Japan-only Nintendo 64 add-on, the Nintendo 64DD.
Micropolis.
In January 2008, the "SimCity" source code was released under the free software GPL 3 license. The release of the source code was related to the donation of SimCity software to the One Laptop Per Child program, as one of the principles of the OLPC laptop is the use of free and open source software. The open source version is called "Micropolis" (the initial name for "SimCity") since EA retains the trademark "Simcity". The version shipped on OLPC laptops will still be called "SimCity", but will have to be tested by EA quality assurance before each release to be able to use that name. The Micropolis source code has been translated to C++, integrated with Python and interfaced with both GTK+ and OpenLaszlo.
Micropolis (also called OLPC SimCity) is a release of the city-building sim game "SimCity", that was developed by Don Hopkins. It is based on the source code the X11 version of "SimCity" for the Unix operating system, which was donated to the One Laptop per Child (OLPC) project by Electronic Arts as free and open source software under the terms of the GNU General Public License in 2008.
There are two versions: The original version uses the Tcl/Tk user interface, and can be run on the OLPC, as a stand-alone game in any Linux or Mac OS X system with X11, or as a port for OpenBSD. The new version has a user interface implemented in Python code, which uses Cairo to draw graphics and Pango to draw text. The C core that is responsible for the simulation has been restructured and reworked into C++ code, which is cross-platform, and independent of the user interface and scripting language.
History.
The original version of "SimCity "was developed by Maxis on the Commodore 64, and ported to various platforms, including the Macintosh. Maxis licensed the Macintosh" SimCity" source code to DUX software, to port to Unix.
DUX Software contracted Don Hopkins to port "SimCity" to Unix, and he developed "SimCity HyperLook Edition", while working at The Turing Institute on HyperLook with Arthur van Hoff. The user interface was written in PostScript, which ran on the NeWS window system on Sun workstations, and it supported multiple zoomable views, pie menus, annotating and printing maps, and many user interface improvements.
After Sun canceled NeWS, DUX Software contracted Hopkins to rewrite the HyperLook user interface in TCL/Tk for X11, and he developed a multi-player networked user interface using the X11 protocol. The TCL/Tk version of SimCity has been ported to various Unix and non-Unix platforms, including SunOS, Solaris, IRIX, HP-UX, OSF/1, Quarterdeck Desqview/X, NCD X Terminals, Warp, and Linux. The contract to sell "SimCity" for Unix expired after ten years, so the TCL/Tk version was no longer commercially available.
"OLPC SimCity" is based on the TCL/Tk version of "SimCity", a trademark of Electronic Arts. Don Hopkins adapted it to the OLPC, thanks to the support of John Gilmore. "OLPC SimCity" will be shipped with the OLPC, and it has been run through EA's quality assurance process and reviewed for integrity. EA reserves the right to review and approve any version of the game distributed under the name SimCity.
"Micropolis" is the name of the current GPL open source code version of OLPC "SimCity".
Future.
Since "Micropolis" is licensed under the GPL, users can do anything they want with it that conforms with the GPL – the only restriction is that they cannot call it "SimCity" (along with a few other limitations to protect EA's trademarks). This allows other, differently named projects to be forked from the Micropolis source code. Improvements to the open source code base that merits EA's approval may be incorporated into the official "OLPC SimCity" source code, to be distributed with the OLPC under the trademarked name "OLPC SimCity", but only after it has been reviewed and approved by EA.
Comparison of different versions.
For other Sim games, see the list of "Sim" games.
Critical acclaim.
"SimCity" was very successful, selling one million copies by late 1992. It was critically acclaimed and received significant recognition within a year after its initial release. As of December 1990 (from a Maxis document by Sally Vandershaf, Maxis P.R. Coordinator) the game was reported to have won the following awards:
In addition, "SimCity" won the Origins Award for "Best Military or Strategy Computer Game" of 1989 in 1990, was named to "Computer Gaming World"‍ '​s Hall of Fame for games readers highly rated over time, and the multiplayer X11 version of the game was also nominated in 1992 as the Best Product of the Year in "Unix World". "SimCity" was named #4 "Ten Greatest PC Game Ever" by PC World in 2009. It was named one of the sixteen most influential games in history at Telespiele, a German technology and games trade show, in 2007. Sid Meier in 2008 named "SimCity" as one of the three most important innovations in videogame history, as it led to other games that encouraged players to create, not destroy. It was named #11 on IGN's 2009 "Top 25 PC Games of All Time" list.
The University of Southern California and University of Arizona used "SimCity" in urban planning and political science classes. In 1990 The "Providence Journal" invited five candidates for Mayor of Providence, Rhode Island to manage a "SimCity" town resembling the city; former mayor Buddy Cianci, who was the most successful, won election that year. Chuck Moss of "The Detroit News" found that Godzilla attacking the city in the 1972 Detroit scenario caused less destruction than the mayoralty of Coleman Young.
The "SimCity Terrain Editor" was reviewed in 1989 in "Dragon" #147 by Hartley, Patricia, and Kirk Lesser in "The Role of Computers" column. The reviewers gave the expansion 4 out of 5 stars.
The ZX Spectrum version was voted number 4 in the "Your Sinclair Readers' Top 100 Games of All Time".
On March 12, 2007, "The New York Times" reported that "SimCity" was named to a list of the ten most important video games of all time, the so-called game canon. The Library of Congress took up a video game preservation proposal and began with the games from this list, including "SimCity".
Legacy.
The subsequent success of "SimCity" speaks for itself: "Sim" games of many types were developed – with Will Wright and Maxis developing myriad titles including "SimEarth", "SimFarm", "SimTown", "Streets of SimCity", "SimCopter", "SimAnt", "SimLife", "", "SimTower", "SimPark", "SimSafari", and "The Sims", as well as the unreleased "SimsVille" and "SimMars." They also obtained licenses for some titles developed in Japan, such as "SimTower" and "Let's Take The A-Train" (just called "A-Train" outside of Japan). In 2000 "The Sims" was released, which spawned its own series. "Spore", released in 2008, was originally going to be titled "SimEverything" – a name that Will Wright thought might accurately describe what he was trying to achieve. "SimCity" yielded several sequels.
"SimCity" inspired a new genre of video games. "Software toys" that were open-ended with no set objective were developed trying to duplicate "SimCity"'s success. The most successful was most definitely Wright's own "The Sims", which went on to be the best selling computer game of all time. The ideas pioneered in "SimCity" have been incorporated into real-world applications as well. For example, VisitorVille simulates a city based on website statistics.
The series also spawned a collectible card game, produced by Mayfair Games.

</doc>
<doc id="28761" url="http://en.wikipedia.org/wiki?curid=28761" title="Sofonisba Anguissola">
Sofonisba Anguissola

Sofonisba Anguissola (also spelled Anguisciola) (c. 1532 – 16 November 1625) was an Italian Renaissance painter born in Cremona to a noble family, but a relatively poor one. She received a well-rounded education, that included the fine arts, and her apprenticeship with local painters set a precedent for women to be accepted as students of art. As a young woman, Anguissola traveled to Rome where she was introduced to Michelangelo, who immediately recognized her talent, and to Milan, where she painted the Duke of Alba. Elizabeth of Valois, the queen of Philip II of Spain, was a keen amateur painter, and in 1569 Anguissola was recruited to go to Madrid as her tutor, with the rank of lady-in-waiting. She later became an official court painter to the king, and adapted her style to the more formal requirements of official portraits for the Spanish court. After the queen's death, Philip helped arrange an aristocratic marriage for her. She moved to Palermo, and later Pisa and Genoa, where she continued to practice as a leading portrait painter, apparently with the support of her two husbands, living to the age of ninety-three.
Her most distinctive and attractive paintings are her portraits of herself and her family, painted before she moved to the Spanish court. In particular her depictions of children were fresh and closely observed. At the Spanish court she painted formal state portraits in the prevailing official style. In later life, she also painted religious themes, although many of her religious paintings have been lost. Anguissola became a wealthy patron of the arts after the weakening of her sight. In 1625, she died at age ninety-three in Palermo. Anguissola's example, as much as her oeuvre, had a lasting influence on subsequent generations of artists, and her great success opened the way for larger numbers of women to pursue serious careers as artists. Her paintings can be seen at galleries in Boston, MA (Isabella Stewart Gardner Museum), Bergamo, Brescia, Budapest, Madrid (Museo del Prado), Naples, Siena, and at the Uffizi Gallery in Florence.
The art historian Giorgio Vasari wrote about Anguissola that she "has shown greater application and better grace than any other woman of our age in her endeavors at drawing; she has thus succeeded not only in drawing, coloring and painting from nature, and copying excellently from others, but by herself has created rare and very beautiful paintings."
The Anguissola family.
Sofonisba Anguissola was born in Cremona, Lombardy, in 1532, the oldest of seven children, six of whom were girls. Her father, Amilcare Anguissola, was a member of the Genoese minor nobility, and her mother, Bianca Ponzone, was also of noble background. The Anguissola family believed they had a connection to the ancient Carthaginians and named their first daughter after the tragic Carthaginian figure Sophonisba.
Amilcare Anguissola encouraged all his daughters (Sofonisba, Elena, Lucia, Europa, Minerva and Anna Maria) to cultivate and perfect their talents. Four of the sisters (Elena, Lucia, Europa and Anna Maria) became painters, but Sofonisba was by far the most accomplished and renowned. Elena abandoned painting to become a nun. Both Anna Maria and Europa gave up art upon marrying, while Lucia Anguissola, the best painter of Sophonisba's sisters, died young. The remaining sister, Minerva, became a writer and Latin scholar. Asdrubale, Sophonisba's brother, studied music and Latin, but not painting.
Her aristocratic father made sure that Anguissola and her sisters received a well-rounded education that included the fine arts. Anguissola was fourteen when her father sent her and her sister Elena to study with Bernardino Campi, a respected portrait and religious painter of the Lombard school. When Campi moved to another city, Anguissola continued her studies with painter Bernardino Gatti (known as Il Sojaro). Anguissola's apprenticeship with local painters set a precedent for women to be accepted as students of art. Dates are uncertain, but Anguissola probably continued her studies under Gatti for about three years (1551–1553).
Anguissola's most important early work was "Bernardino Campi Painting Sofonisba Anguissola" (c. 1550). The double portrait depicts Anguissola's art teacher in the act of painting a portrait of her.
In 1554, at age twenty-two, Anguissola traveled to Rome, where she spent her time sketching various scenes and people. While in Rome, she was introduced to Michelangelo by another painter who was familiar with her work. Meeting Michelangelo was a great honor for Anguissola and she had the benefit of being informally trained by the great master. When he made a request for her to draw a weeping boy, Anguissola drew "Boy Bitten by a Crayfish" and sent it back to Michelangelo, who immediately recognized her talent. Michelangelo subsequently gave Anguissola sketches from his notebooks to draw in her own style and offered advice on the results. For at least two years, Anguissola continued this informal study, receiving substantial guidance from Michelangelo.
The great early art historian Giorgio Vasari later wrote this about Anguissola: "Anguissola has shown greater application and better grace than any other woman of our age in her endeavors at drawing; she has thus succeeded not only in drawing, coloring and painting from nature, and copying excellently from others, but by herself has created rare and very beautiful paintings."
Experiences as a female artist.
Although Anguissola enjoyed significantly more encouragement and support than the average woman of her day, her social class did not allow her to transcend the constraints of her sex. Without the possibility of studying anatomy or drawing from life (it was considered unacceptable for a lady to view nudes), she could not undertake the complex multi-figure compositions required for large-scale religious or history paintings.
Instead, she experimented with new styles of portraiture, setting subjects informally. Self-portraits and family members were her most frequent subjects, as seen in such paintings as "Self-Portrait" (1554, Kunsthistoriches Museum, Vienna), "The Chess Game" (1555, Muzeum Narodowe, Poznań), which depicted her sisters Lucia, Minerva and Europa, and "Portrait of Amilcare, Minerva and Asdrubale Anguissola" (c. 1557-1558, Nivaagaards Malerisambling, Niva, Denmark).When Sofonisba was 20 years old, she painted her most famous picture, of her "Three Sisters Playing Chess." It is a intimate representation of an everyday family scene, combining elaborate formal clothing with very informal facial expressions, which was unusual for Italian art at this time.
She became well-known outside of Italy and in 1559 King Phillip II of Spain asked her to be lady-in-waiting and art teacher to Queen Isabella of Valois, who was only 14 at the time. Queen Isabella and Sofonisba became good friends, and when the Regent died ten years later, Sofonisba left the court because she was so sad. She had painted the entire royal family and even the Pope commissioned Anguissola to do a portrait of the Queen.
At the Spanish Court.
In 1558, already established as a painter, Anguissola went to Milan, where she painted the Duke of Alba. He in turn recommended her to the Spanish king, Philip II. The following year, Anguissola was invited to join the Spanish Court, which was a turning point in her career.
Anguissola was approximately twenty-six when she left Italy to join the Spanish court. In the winter of 1559-1560, she arrived in Madrid to serve as a court painter and lady-in-waiting to the new queen, Elisabeth of Valois, Philip II’s third wife, who was herself an amateur portraitist. Anguissola soon gained Elisabeth's admiration and confidence and spent the following years painting many official portraits for the court, including Philip II’s sister, Juana, and his son, Don Carlos.
These types of paintings were far more demanding than the informal portraits upon which Anguissola had based her early reputation, as it took a tremendous amount of time and energy to render the many intricate designs of the fine fabrics and elaborate jewelry associated with royal subjects. Yet despite the challenge, Anguissola's paintings of Elisabeth of Valois – and later of Anne of Austria, Philip II’s fourth wife – were vibrant and full of life.
During her 14-year residence, she guided the artistic development of Queen Elisabeth of Spain, and influenced the artwork of her two daughters, Isabella Clara Eugenia and Caterina Michaela. Anguissola painted a portrait of the king's sister, Marguerite of Spain, for Pope Pius IV in 1561 and, after Queen Isabella's death in childbirth in 1568, painted the likeness of Anne of Austria, Philip's fourth wife. For the royal family, Anguissola produced detailed scenes of their lives that now hang in the Prado Museum. With the gifts and a dowry of 12,000 scudi she earned along with her salary as court painter and lady-in-waiting to the queen, she amassed an admirable return from her craft.
While in the service of Elizabeth of Valois, Anguissola worked closely with Alonso Sanchez Coello. So closely in fact, that the famous painting of the middle-aged King Philip II was long attributed to Coello or Juan Pantoja de la Cruz. Only recently has Anguissola been recognized as the painting's creator.
Personal life.
After the death of Elisabeth of Valois in 1568, Philip II took a special interest in Anguissola's future. He had wished to marry her to one of the nobles in the Spanish Court. In 1571, Anguissola entered an arranged marriage to a Sicilian nobleman chosen for her by the Spanish court. Philip II paid a dowry of 12,000 scudi for her marriage to , son of the Prince of Paterno, Viceroy of Sicily. Fabrizio was said to be supportive of her painting. She lived with him in Palermo from 1571 to 1579 and received a royal pension of 100 ducats that enabled her to continue working and tutoring would-be painters. Her private fortune also supported her family and brother Asdrubale following Amilcare Anguissola's financial decline and death. After eight years with the Spanish court, Anguissola and her husband left Spain with the king's permission sometime in 1578. The couple settled in Palermo, where Anguissola's husband died in 1579.
Two years later, while traveling to Genoa by sea, she fell in love with the ship's captain, sea merchant Orazio Lomellini. Against the wishes of her brother, they married in Pisa in January of 1580 and lived in Genoa until 1620. She had no children, but maintained cordial relationships with her nieces and her husband's son Giulio.
Late years.
Lomellino's fortune, plus a generous pension from Philip II, allowed Anguissola to paint freely and live comfortably. By now quite famous, Anguissola received many colleagues who came to visit and discuss the arts with her. Several of these were younger artists, eager to learn and mimic Anguissola's distinctive style.
In her later life, Anguissola painted not only portraits, but religious themes, as she had done in the days of her youth, although many of the latter have been lost. She was the leading portrait painter in Genoa until she moved to Palermo in her last years. In 1620, she painted her last self-portrait.
On 12 July 1624, Anguissola was visited by the young Flemish painter Anthony van Dyck, who recorded sketches from his visit to her in his sketchbook. Van Dyck, who believed her to be 96 years of age (she was actually 92) noted that although "her eyesight was weakened", Anguissola was still mentally alert. Excerpts of the advice she gave him about painting survive from this visit. Van Dyck drew her portrait while visiting her. This last portrait made of Anguissola survives in the Sackville Collection at Knole House. The next year, she returned to Sicily.
Contrary to later biographers' claims, she was never entirely blind, but perhaps suffered from cataracts. Anguissola became a wealthy patron of the arts after the weakening of her sight. In 1625, she died at age 93 in Palermo.
Anguissola's adoring second husband, who described her as small of frame, yet "great among mortals", buried her with honor in Palermo at the Church of San Giorgio dei Genovese. Seven years later, on the anniversary of what would have been her 100th birthday, her husband placed an inscription on her tomb that read in part: 
To Sofonisba, my wife, who is recorded among the illustrious women of the world, outstanding in portraying the images of man. Orazio Lomellino, in sorrow for the loss of his great love, in 1632, dedicated this little tribute to such a great woman.—Orazio Lomellino, Inscription on Sofonisba's tomb.
Style.
The influence of Campi, whose reputation was based on portraiture, is evident in Anguissola's early works, such as the "Self-Portrait" (Florence, Uffizi). Her work was allied to the worldly tradition of Cremona, influenced greatly by the art of Parma and Mantua, in which even religious works were imbued with extreme delicacy and charm. From Gatti, she seems to have absorbed elements reminiscent of Correggio, beginning a trend in Cremonese painting of the late 16th century. This new direction is reflected in "Lucia, Minerva and Europa Anguissola Playing Chess" (1555; Poznań, N. Mus.) in which portraiture merges into a quasi-genre scene, a characteristic derived from Brescian models.
The main body of Anguissola's earlier work consists of self-portraits (the many "autoritratti" reflect the fact that portraits of her were frequently requested due to her fame) and portraits of her family, which are considered by many to be her finest works.
Approximately fifty works have been attributed to Anguissola. Her paintings can be seen at galleries in Baltimore (Walters Art Museum), Bergamo, Boston (Museum of Fine Arts), Brescia (Pinacoteca Tosio Martinengo), Budapest, Florence (Uffizi Gallery), Graz (Joanneum Alte Galerie), Madrid (Museo del Prado), Milan (Pinacoteca di Brera), Naples (National Museum of Capodimonte), Poznań (National Museum, Poznań), Siena (Pinacoteca Nazionale), Southampton (City Art Gallery), and Vienna (Kunsthistorisches Museum).
Historical significance.
Sofonisba Anguissola's oeuvre had a lasting influence on subsequent generations of artists. Her portrait of Queen Elisabeth of Valois with a zibellino (the pelt of a marten set with a head and feet of jewelled gold) was widely copied by many of the finest artists of the time, such as Peter Paul Rubens.
Anguissola is significant to feminist art historians. Although there has never been a period in Western history in which women were completely absent in the visual arts, Anguissola's great success opened the way for larger numbers of women to pursue serious careers as artists. Some of her more well-known successors include Lavinia Fontana, Barbara Longhi, Fede Galizia and Artemisia Gentileschi.
A Cremonese school bears the name Liceo Statale Sofonisba Anguissola.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="28762" url="http://en.wikipedia.org/wiki?curid=28762" title="SameGame">
SameGame

SameGame (さめがめ) is a tile-matching puzzle video game originally released under the name "Chain Shot!'
' in 1985 by Kuniaki Moribe (Morisuke). It has since been ported to numerous computer platforms, handheld devices, and even TiVo. 
History.
"SameGame" was originally created as "Chain Shot!" in 1985 by Kuniaki Moribe. It was distributed for Fujitsu's FM-8 and FM-7 platforms in a Japanese monthly personal computer magazine called "Gekkan ASCII". In 1992, the game was ported as "SameGame" to Unix platforms by Eiji Fukumoto, and to the NEC PC-9801 series by Wataru Yoshioka. In 1993, it was ported to Windows 3.1 by Ikuo Hirohata. This version was translated into English by Hitoshi Ozawa, and is still available from his software archive.
In 1994, Takahiro Sumiya ported it to Macintosh. This version has some gameplay differences—three, instead of five, colors—and is probably the most widely distributed of the original series. It was the basis for the "Same Gnome" and "KSame" variations created for Linux.
Gameplay.
Game mechanics.
"SameGame" is played on a rectangular field, typically initially filled with four or five kinds of blocks placed at random. By selecting a group of adjoining blocks of the same color, a player may remove them from the screen. Blocks that are no longer supported will fall down, and a column without any blocks will be trimmed away by other columns always sliding to one side (often the left). The goal of the game is to remove as many blocks from the playing field as possible.
In most versions, there are no time constraints during the game. However, some implementations gradually push the rows upward or drop blocks from above. Sometimes the player can control the number and timing of blocks that drop from above in certain ways. For example, on some implementations for the iOS, this can be done by shaking the device. The game ends if a timer runs out or if no more blocks can be removed. Some versions, including some versions for Windows Mobile, include both portrait and landscape orientations.
Gallery.
Variations.
In one variation, the game starts with no blocks on the field. Blocks fall down to the playing field, and must be removed before they reach the top. If they reach the top and overflow, the game is over. In some variations, such as "Bubble Bang", circles or balls are used instead of blocks—which alters the gameplay, as the balls form different shapes than square blocks.
In three-dimensional variants, the playing field is a cube (containing smaller cubes) instead of a rectangle, and the player has the ability to rotate the cube. "Cubes" for iPhone OS uses this approach.
Some versions allow the player to rotate the playing field 90 degrees clockwise or counter-clockwise, which causes one of two things to happen:
In some variations, blocks can be removed when connected to blocks of the same color diagonally, not just horizontally and vertically. Some versions introduce new types of blocks. The different types of blocks interact in various ways with the play field; for example, one type might remove all the blocks in a row. An example of this is the "Revenge mode" in "PocketPop Revenge" ("PocketFun") for iPhone OS.
Scoring.
Most versions of the game give formula_1 points for removing formula_2 tiles at once, where formula_3 or formula_4, depending on the implementation. For instance, "Insane Game" for Texas Instruments calculators uses formula_5; Ikuo Hirohata's implementation uses the formula formula_6. The "Bubble Breaker" implementation for Windows Mobile uses the formula_7 formula. The 2001 version released by Jeff Reno uses the formula formula_8.
Some versions also offer a large bonus for removing all blocks from the screen, or leaving no more than a certain number of blocks. Others reduce the final score based on the number of blocks remaining at the end of the game. Some game versions award bonus points for clearing the field quickly, encouraging faster play. The faster the player finishes the level, the bigger the bonus. Still others offer combination, or "chain", bonuses for clearing the same color of blocks two or more times in succession.
Another scoring technique awards bonus points for each chain of a certain color that has a certain number of blocks (for example, two red blocks or 11 blue blocks). After receiving the bonus once, sometimes the bonus condition changes. "BPop" uses this scoring variation.
Some versions have a simple scoring system: each block removed is worth one point, and there is no bonus for removing more than two blocks at a time. This is seen in the "Same Pets" and "Same Hearths" variants.
Goal-based scoring.
Some versions award scores based on the attainment of goals. This is typically seen in multi-level versions of the game. There are four primary scoring systems for such games.
In one variation, each level has a target score. The player's score starts at zero, and the player must reach the target score. At the beginning of each level, the player's score is reset to zero; the target score increases with each level.
Other versions have a cumulative target score. In these versions, the player's score carries over from level to level. As a result, if the player substantially exceeds the target score on a given level, they may enter the subsequent level having already met that level's target score, as well. "BPop" has a cumulative target score.
Some versions maintain the same target score for each level; such variations can be played indefinitely. In such games, the player typically loses due to poor planning or a lapse in concentration. Examples of such games are "Same Pets" and "Same Hearths".
In games without a goal score, like "Bonkers for iPhone" and SameGameBros for iPhone, the goal is to clear the level completely. The game ends when the player fails to do so.
Visuals.
Blocks typically appear as colored squares, circles, or spheres. Some variations use gradient shading to give the illusion of dimensionality. Other tile themes, or "skins", include animals, hearts, stars, faces, Lego blocks, and jelly bears. Designs may follow a theme, such as Christmas or monochrome. Most games have only one skin, but others allow choosing from multiple skins.
There is a special visual aspect in some versions; instead of separate blocks, games like "iDrops" and "SameGameManiak" feature bordered areas for adjacent blocks of the same color. Some have elaborate tile graphics, featuring pictures or patterns inside the tile, like "KSame" and "Same GNOME".

</doc>
<doc id="28763" url="http://en.wikipedia.org/wiki?curid=28763" title="Sather">
Sather

Sather is an object-oriented programming language. It originated circa 1990 at the International Computer Science Institute (ICSI) at the University of California, Berkeley, developed by an international team led by Steve Omohundro. It supports garbage collection and generics by subtypes.
Originally, it was based on Eiffel, but it has diverged, and now includes several functional programming features. It is probably best to view it as an object-oriented language, with many ideas borrowed from Eiffel.
Even the name is inspired by Eiffel; the Sather Tower is a recognizable landmark at Berkeley, named after Jane Krom Sather, the widow of Peder Sather, who donated large sums to the foundation of the university.
Sather also takes inspiration from other programming languages and paradigms: iterators, design by contract, abstract classes, multiple inheritance, anonymous functions, operator overloading, contravariant type system.
The original Berkeley implementation (last stable version 1.1 was released in 1995, no longer maintained) has been adopted by the Free Software Foundation therefore becoming GNU Sather. Last stable GNU version (1.2.3) was released in July 2007 and the software is currently not maintained. There were several other variants: Sather-K from the University of Karlsruhe; Sather-W from the University of Waikato (implementation of Sather version 1.3); Peter Naulls' port of ICSI Sather 1.1 to RISC OS; and pSather, a parallel version of ICSI Sather addressing non-uniform memory access multiprocessor architectures but presenting a shared memory model to the programmer.
The former ICSI Sather compiler (now GNU Sather) is implemented as a compiler to C, i.e., the compiler does not output object or machine code, but takes Sather source code and generates C source code as an intermediate language. Optimizing is left to the C compiler.
The GNU Sather compiler, written in Sather itself, is dual licensed under the GNU GPL & LGPL.
Hello World.
A few remarks:
Example of iterators.
This program prints numbers from 1 to 10.
The codice_10 ... codice_11 construct is the preferred means of defining loops (although codice_12 and codice_13-codice_14 are also available). Within the construct, one or more iterators may be used. Iterator names always end with an exclamation mark (this convention is enforced by the compiler). codice_15 is a method of the integer class codice_16 accepting one codice_17 argument, meaning its value will not change as the iterator yields. codice_15 could be implemented in the codice_16 class like this:
Type information for variables is denoted by a postfix syntax codice_20. The type can often be inferred and thus the typing information is optional, like in codice_21. codice_22 is a convenience pseudo-class referring to the current class.

</doc>
<doc id="28764" url="http://en.wikipedia.org/wiki?curid=28764" title="Serotonin">
Serotonin

Serotonin or 5-hydroxytryptamine (5-HT) is a monoamine neurotransmitter. Biochemically derived from tryptophan, serotonin is primarily found in the gastrointestinal tract (GI tract), blood platelets, and the central nervous system (CNS) of animals, including humans. It is popularly thought to be a contributor to feelings of well-being and happiness.
Approximately 90% of the human body's total serotonin is located in the enterochromaffin cells in the GI tract, where it is used to regulate intestinal movements. The remainder is synthesized in serotonergic neurons of the CNS, where it has various functions. These include the regulation of mood, appetite, and sleep. Serotonin also has some cognitive functions, including memory and learning. Modulation of serotonin at synapses is thought to be a major action of several classes of pharmacological antidepressants.
Serotonin secreted from the enterochromaffin cells eventually finds its way out of tissues into the blood. There, it is actively taken up by blood platelets, which store it. When the platelets bind to a clot, they release serotonin, where it serves as a vasoconstrictor and helps to regulate hemostasis and blood clotting. Serotonin also is a growth factor for some types of cells, which may give it a role in wound healing. There are various serotonin receptors.
Serotonin is metabolized mainly to 5-HIAA, chiefly by the liver. Metabolism involves first oxidation by monoamine oxidase to the corresponding aldehyde. This is followed by oxidation by aldehyde dehydrogenase to 5-HIAA, the indole acetic acid derivative. The latter is then excreted by the kidneys. One type of tumor, called carcinoid, sometimes secretes large amounts of serotonin into the blood, which causes various forms of the carcinoid syndrome of flushing (serotonin itself does not cause flushing. Potential causes of flushing in carcinoid syndrome include bradykinins, prostaglandins, tachykinins, substance P, and/or histamine), diarrhea, and heart problems. Because of serotonin's growth-promoting effect on cardiac myocytes, a serotonin-secreting carcinoid tumour may cause a tricuspid valve disease syndrome, due to the proliferation of myocytes onto the valve.
In addition to animals, serotonin is found in fungi and plants. Serotonin's presence in insect venoms and plant spines serves to cause pain, which is a side-effect of serotonin injection. Serotonin is produced by pathogenic amoebae, and its effect on the gut causes diarrhea. Its widespread presence in many seeds and fruits may serve to stimulate the digestive tract into expelling the seeds.
Functions.
Serotonin is a neurotransmitter and is found in all bilateral animals, where it mediates gut movements and the animal's perceptions of resource availability . In less complex animals, such as some invertebrates, resources simply mean food availability. In more complex animals, such as arthropods and vertebrates, resources also can mean social dominance. In response to the perceived abundance or scarcity of resources, an animal's growth, reproduction or mood may be elevated or lowered. This may somewhat depend on how much serotonin the organism has at its disposal.
Gauge of food availability (appetite).
Serotonin functions as a neurotransmitter in the nervous systems of simple, as well as complex, animals. For example, in the roundworm "Caenorhabditis elegans", which feeds on bacteria, serotonin is released as a signal in response to positive events, e.g., finding a new source of food or in male animals finding a female with which to mate. When a well-fed worm feels bacteria on its cuticle, dopamine is released, which slows it down; if it is starved, serotonin also is released, which slows the animal down further. This mechanism increases the amount of time animals spend in the presence of food. The released serotonin activates the muscles used for feeding, while octopamine suppresses them. Serotonin diffuses to serotonin-sensitive neurons, which control the animal's perception of nutrient availability.
When humans smell food, dopamine is released to increase the appetite. But, unlike in worms, serotonin does not increase anticipatory behaviour in humans; instead, the serotonin released while consuming activates 5-HT2C receptors on dopamine-producing cells. This halts their dopamine release, and thereby serotonin decreases appetite. Drugs that block 5-HT2C receptors make the body unable to recognize when it is no longer hungry or otherwise in need of nutrients, and are associated with increased weight gain, especially in people with a low number of receptors. The expression of 5-HT2C receptors in the hippocampus follows a diurnal rhythm, just as the serotonin release in the ventromedial nucleus, which is characterised by a peak at morning when the motivation to eat is strongest.
Effects of food content.
Consuming purified tryptophan increases brain serotonin whereas eating foods containing tryptophan does not. This is because the transport system which brings tryptophan across the blood-brain barrier is also selective for the other amino acids contained in protein sources. High plasma levels of other large neutral amino acids compete for transport and prevent the elevated plasma tryptophan from increasing serotonin synthesis.
In the digestive tract (emetic).
The gut is surrounded by enterochromaffin cells, which release serotonin in response to food in the lumen. This makes the gut contract around the food. Platelets in the veins draining the gut collect excess serotonin.
If irritants are present in the food, the enterochromaffin cells release more serotonin to make the gut move faster, i.e., to cause diarrhea, so the gut is emptied of the noxious substance. If serotonin is released in the blood faster than the platelets can absorb it, the level of free serotonin in the blood is increased. This activates 5HT3 receptors in the chemoreceptor trigger zone that stimulate vomiting. The enterochromaffin cells not only react to bad food but are also very sensitive to irradiation and cancer chemotherapy. Drugs that block 5HT3 are very effective in controlling the nausea and vomiting produced by cancer treatment, and are considered the gold standard for this purpose.
Gauge of social situation.
How much food an animal gets not only depends on food availability but also depends on the animal's ability to compete with others. This is especially true for social animals, where the stronger individuals might steal food from the weaker (this is not to say anti-social animals do not concern themselves with the needs of others and do not steal food from others). Thus, serotonin is not only involved in the perception of food availability but also involved in social rank.
If a lobster is injected with serotonin, it behaves as if it were alpha, while octopamine causes subordinate behavior. A crayfish that is frightened may flip its tail to flee, and the effect of serotonin on this behavior depends largely on the animal's social status. Serotonin inhibits the fleeing reaction in subordinates, but enhances it in socially dominant or isolated individuals. The reason for this is social experience alters the proportion between serotonin receptors (5-HT receptors) that have opposing effects on the fight-or-flight response. The effect of 5-HT1 receptors predominates in subordinate animals, while 5-HT2 receptors predominates in dominants.
With Macaque monkeys, it has been found that alpha males have twice the level of serotonin released in the brain, as measured by the levels of 5-Hydoxyindolacetic acid (5-HIAA) in the cerebro-spinal fluid, than that found in subordinate males and females. Dominance and high levels of cerebro-serotonon levels seem to go hand in hand. When dominant males were removed from such groups, subordinate males begin competing for dominance. Once new dominance hierarchies were established serotonin levels of the new dominant individuals also rose to double that found in subdominant males and females. The reason why serotonin levels are only high in dominant males but not dominant females has not yet been established.
In humans, levels of 5-HT1A receptor activation in the brain show negative correlation with aggression, and a mutation in the gene that codes for the 5-HT2A receptor may double the risk of suicide for those with that genotype. Serotonin in the brain is not usually degraded after use, but is collected by serotonergic neurons by serotonin transporters on their cell surfaces. Studies have revealed nearly 10% of total variance in anxiety-related personality depends on variations in the description of where, when and how many serotonin transporters the neurons should deploy.
Growth and reproduction.
In the nematode "C. elegans", artificial depletion of serotonin or the increase of octopamine cues behavior typical of a low-food environment: "C. elegans" becomes more active, and mating and egg-laying are suppressed, while the opposite occurs if serotonin is increased or octopamine is decreased in this animal. Serotonin is necessary for normal nematode male mating behavior, and the inclination to leave food to search for a mate. The serotonergic signaling used to adapt the worm's behaviour to fast changes in the environment affects insulin-like signaling and the TGF beta signaling pathway, which control long-term adaption.
Aging and age-related phenotypes.
Serotonin is known to regulate aging, learning and memory. The first evidence comes from the study of longevity in "C. elegans". During early phase of aging, the level of serotonin increases, which alters locomotory behaviors and associative memory. The effect is restored by mutations and drugs (including mianserin and methiothepin) that inhibit serotonin receptors. The observation does not contradict with the notion that the serotonin level goes down in mammals and humans, which is typically seen in late but not early phase of aging.
Bone metabolism.
In mice and humans, alterations in serotonin levels and signalling have been shown to regulate bone mass. Mice that lack brain serotonin have osteopenia, while mice that lack gut serotonin have high bone density. In humans, increased blood serotonin levels have been shown to be significant negative predictor of low bone density. Serotonin can also be synthesized, albeit at very low levels, in the bone cells. It mediates its actions on bone cells using three different receptors. Through 5-HT1B receptors, it negatively regulates bone mass, while it does so positively through 5-HT2B receptors and 5-HT2C receptors. There is very delicate balance between physiological role of gut serotonin and its pathology. Increase in the extracellular content of serotonin results in a complex relay of signals in the osteoblasts culminating in FoxO1/ Creb and ATF4 dependent transcriptional events. These studies have opened a new area of research in bone metabolism that can be potentially harnessed to treat bone mass disorders.
Organ development.
Since serotonin signals resource availability it is not surprising that it affects organ development. Many human and animal studies have shown that nutrition in early life can influence, in adulthood, such things as body fatness, blood lipids, blood pressure, atherosclerosis, behavior, learning and longevity. Rodent experiments shows that early life exposure to SSRI:s makes persistent changes in the serotonergic transmission of the brain resulting in behavioral changes, which are reversed by treatment with antidepressants. By treating normal and knockout mice lacking the serotonin transporter with fluoxetine scientists showed that normal emotional reactions in adulthood, like a short latency to escape foot shocks and inclination to explore new environments were dependent on active serotonin transporters during the neonatal period.
In the fruit fly insulin both regulates both blood sugar as well as acting as a growth factor. Thus in the fruit fly, serotonergic neurons regulate the adult body size by affecting insulin secretion. Serotonin has also been identified as the trigger for swarm behavior in locusts. In humans, though insulin regulates blood sugar and IGF regulates growth, serotonin controls the release of both hormones, suppressing insulin release from the beta cells in the pancreas. Exposure to SSRIs during Pregnancy reduces fetal growth.
Human serotonin can also act as a growth factor directly. Liver damage increases cellular expression of 5-HT2A and 5-HT2B receptors, mediating liver compensatory regrowth (see ) Serotonin present in the blood then stimulates cellular growth to repair liver damage.
5HT2B receptors also activate osteocytes, which build up bone However, serotonin also inhibits osteoblasts, through 5-HT1B receptors.
Cardiovascular growth factor.
Serotonin, in addition, evokes endothelial nitric oxide synthase activation and stimulates, through a 5-HT1B receptor-mediated mechanism, the phosphorylation of p44/p42 mitogen-activated protein kinase activation in bovine aortic endothelial cell cultures. In blood, serotonin is collected from plasma by platelets, which store it. It is thus active wherever platelets bind in damaged tissue, as a vasoconstrictor to stop bleeding, and also as a fibrocyte mitotic (growth factor), to aid healing.
Some serotonergic agonist drugs also cause fibrosis anywhere in the body, particularly the syndrome of retroperitoneal fibrosis, as well as cardiac valve fibrosis.
In the past, three groups of serotonergic drugs have been epidemiologically linked with these syndromes. These are the serotonergic vasoconstrictive antimigraine drugs (ergotamine and methysergide), the serotonergic appetite suppressant drugs (fenfluramine, chlorphentermine, and aminorex), and certain anti-Parkinsonian dopaminergic agonists, which also stimulate serotonergic 5-HT2B receptors. These include pergolide and cabergoline, but not the more dopamine-specific lisuride.
As with fenfluramine, some of these drugs have been withdrawn from the market after groups taking them showed a statistical increase of one or more of the side effects described. An example is pergolide. The drug was declining in use since it was reported in 2003 to be associated with cardiac fibrosis.
Two independent studies published in the New England Journal of Medicine in January 2007, implicated pergolide, along with cabergoline, in causing valvular heart disease. As a result of this, the FDA removed pergolide from the U.S. market in March 2007. (Since cabergoline is not approved in the U.S. for Parkinson's Disease, but for hyperprolactinemia, the drug remains on the market. Treatment for hyperprolactinemia requires lower doses than that for Parkinson's Disease, diminishing the risk of valvular heart disease).
Deficiency.
Genetically altered "C. elegans" worms that lack serotonin have an increased reproductive lifespan, may become obese, and sometimes present with arrested development at a dormant larval state.
Serotonin in mammals is made by two different tryptophan hydroxylases: TPH1 produces serotonin in the pineal gland and the enterochromaffin cells, while TPH2 produces it in the Raphe nuclei and in the myenteric plexus.
Genetically altered mice lacking TPH1 develop progressive loss of heart strength early on. They have pale skin and breathing difficulties, are easily tired, and eventually die of heart failure.
Genetically altered mice that lack TPH2 are normal when they are born. However, after three days, they appear to be smaller and weaker, and have softer skin than their siblings. In a purebred strain, 50% of the mutants died during the first four weeks, but in a mixed strain, 90% survived. Normally, the mother weans the litter after three weeks, but the mutant animals needed five weeks. After that, they caught up in growth and had normal mortality rates. Subtle changes in the autonomic nervous system are present, but the most obvious difference from normal mice is the increased aggressiveness and impairment in maternal care of young.
Despite the blood-brain barrier, the loss of serotonin production in the brain is partially compensated by intestinal serotonin. The behavioural changes become greatly enhanced if one crosses TPH1- with TPH2-lacking mice and gets animals that lack TPH entirely.
In humans, defective signaling of serotonin in the brain may be the root cause of sudden infant death syndrome (SIDS). Scientists from the European Molecular Biology Laboratory in Monterotondo, Italy genetically modified lab mice to produce low levels of the neurotransmitter serotonin. The results showed the mice suffered drops in heart rate and other symptoms of SIDS, and many of the animals died at an early age.
Researchers now believe low levels of serotonin in the animals' brainstems, which controls heartbeat and breathing, may have caused sudden death.
If neurons that make serotonin — serotonergic neurons — are abnormal in human infants, there is a risk of sudden infant death syndrome (SIDS).
Recent research conducted at Rockefeller University shows, in both patients who suffer from depression as well as mice that model the disorder, levels of the p11 protein are decreased. This protein is related to serotonin transmission within the brain.
Depletion of serotonin is common between disorders such as obsessive-compulsive disorder, depression, and anxiety. However, Dr. Marazziti and his researchers at the University of Pisa in Italy found that depletion of serotonin also occurs in people who have recently fallen in love. This leads to the obsessive component associated with early stages of love.
Consumption of an average amount of alcohol (0.8g/kg of body weight) has been shown to decrease tryptophan by about 25%, leading to a similar decrease in serotonin. The sexual and impulsive behavior resulting from an intoxicated state is at least partially an effect of the decrease in serotonin because serotonin regulates these behaviors.
In the brain.
Gross anatomy.
The neurons of the Raphe nuclei are the principal source of 5-HT release in the brain.
There are 7 or 8 raphe nuclei (some scientists chose to group the "nuclei raphes lineares" into one nucleus), all of which located along the midline of the brainstem, and centered around the reticular formation.
Axons from the neurons of the raphe nuclei form a neurotransmitter system reaching almost every part of the central nervous system. Axons of neurons in the lower raphe nuclei terminate in the cerebellum and spinal cord, while the axons of the higher nuclei spread out in the entire brain.
Microanatomy.
Serotonin is released into the space between neurons, and diffuses over a relatively wide gap (>20 µm) to activate 5-HT receptors located on the dendrites, cell bodies and presynaptic terminals of adjacent neurons.
Receptors.
The 5-HT receptors, the receptors for serotonin, are located on the cell membrane of nerve cells and other cell types in animals, and mediate the effects of serotonin as the endogenous ligand and of a broad range of pharmaceutical and hallucinogenic drugs. With the exception of the 5-HT3 receptor, a ligand-gated ion channel, all other 5-HT receptors are G-protein-coupled receptors (also called seven-transmembrane, or heptahelical receptors) that activate an intracellular second messenger cascade.
Termination.
Serotonergic action is terminated primarily via uptake of 5-HT from the synapse. This is accomplished through the specific monoamine transporter for 5-HT, SERT, on the presynaptic neuron. Various agents can inhibit 5-HT reuptake, including cocaine, dextromethorphan (an antitussive), tricyclic antidepressants and selective serotonin reuptake inhibitors (SSRIs). A 2006 study conducted by the University of Washington suggested that a newly discovered monoamine transporter, known as PMAT, may account for "a significant percentage of 5-HT clearance".
Contrasting with the high-affinity SERT, the PMAT has been identified as a low-affinity transporter, with an apparent Km of 114 micromoles/l for serotonin; approximately 230 times higher than that of SERT. However, the PMAT, despite its relatively low serotonergic affinity, has a considerably higher transport 'capacity' than SERT, "..resulting in roughly comparable uptake efficiencies to SERT in heterologous expression systems." The study also suggests some SSRIs, such as fluoxetine and sertraline anti-depressants, inhibit PMAT but at IC50 values which surpass the therapeutic plasma concentrations by up to four orders of magnitude. Therefore, SSRI monotherapy is "ineffective" in PMAT inhibition. At present, no known pharmaceuticals are known to appreciably inhibit PMAT at normal therapeutic doses. The PMAT also suggestively transports dopamine and norepinephrine, albeit at Km values even higher than that of 5-HT (330–15,000 μmoles/L).
Serotonylation.
Serotonin can also signal through a nonreceptor mechanism called serotonylation, in which serotonin modifies proteins. This process underlies serotonin's effects upon platelet-forming cells (thrombocytes) in which it links to the modification of signaling enzymes called GTPases that then trigger the release of vesicle contents by exocytosis. A similar process underlies the pancreatic release of insulin.
The effects of serotonin upon vascular smooth muscle tone (this is the biological function from which serotonin originally got its name) depend upon the serotonylation of proteins involved in the contractile apparatus of muscle cells.
Biosynthesis.
In animals including humans, serotonin is synthesized from the amino acid -tryptophan by a short metabolic pathway consisting of two enzymes: tryptophan hydroxylase (TPH) and aromatic amino acid decarboxylase (DDC). The TPH-mediated reaction is the rate-limiting step in the pathway.
TPH has been shown to exist in two forms: TPH1, found in several tissues, and TPH2, which is a neuron-specific isoform.
Serotonin can be synthesized from tryptophan in the lab using "Aspergillus niger" and "Psilocybe coprophila" as catalysts. The first phase to 5-hydroxytryptophan would require letting tryptophan sit in ethanol and water for 7 days, then mixing in enough HCl (or other acid) to bring the pH to 3, and then adding NaOH to make a pH of 13 for 1 hour. "Asperigillus niger" would be the catalyst for this first phase. The second phase to synthesizing tryptophan itself from the 5-hydroxytryptophan intermediate would require adding ethanol and water, and letting sit for 30 days this time. The next two steps would be the same as the first phase: adding HCl to make the pH = 3, and then adding NaOH to make the pH very basic at 13 for 1 hour. This phase uses the "Psilocybe coprophila" as the catalyst for the reaction.
Serotonin taken orally does not pass into the serotonergic pathways of the central nervous system, because it does not cross the blood–brain barrier. However, tryptophan and its metabolite 5-hydroxytryptophan (5-HTP), from which serotonin is synthesized, does cross the blood–brain barrier. These agents are available as dietary supplements, and may be effective serotonergic agents.
One product of serotonin breakdown is 5-hydroxyindoleacetic acid (5-HIAA), which is excreted in the urine. Serotonin and 5-HIAA are sometimes produced in excess amounts by certain tumors or cancers, and levels of these substances may be measured in the urine to test for these tumors.
Drugs targeting the 5-HT system.
Several classes of drugs target the 5-HT system, including some antidepressants, antipsychotics, anxiolytics, antiemetics, and antimigraine drugs, as well as the psychedelic drugs and empathogens.
Psychedelic drugs.
The psychedelic drugs psilocin/psilocybin, DMT, mescaline, and LSD are agonists, primarily at 5HT2A/2C receptors. The empathogen-entactogen MDMA releases serotonin from synaptic vesicles of neurons.
Antidepressants.
Drugs that alter serotonin levels are used in treating depression, generalized anxiety disorder and social phobia. Monoamine oxidase inhibitors (MAOIs) prevent the breakdown of monoamine neurotransmitters (including serotonin), and therefore increase concentrations of the neurotransmitter in the brain. MAOI therapy is associated with many adverse drug reactions, and patients are at risk of hypertensive emergency triggered by foods with high tyramine content, and certain drugs. Some drugs inhibit the re-uptake of serotonin, making it stay in the synaptic cleft longer. The tricyclic antidepressants (TCAs) inhibit the reuptake of both serotonin and norepinephrine. The newer selective serotonin reuptake inhibitors (SSRIs) have fewer side-effects and fewer interactions with other drugs. The side-effects that have become apparent recently include a decrease in bone mass in elderly and increased risk for osteoporosis. However, it is not yet clear whether it is due to SSRI action on peripheral serotonin production and or action in the gut or in the brain. There is investigation into whether SSRIs benefits to mood are somehow related to dopamine receptor sensitivity indirectly influenced by antidepressant mechanisms. In one study, patients with depression taking an SSRI were given a low dose D2 receptor antagonist(sulpiride), and reported negative effect on mood.
Certain SSRI medications have been shown to lower serotonin levels below the baseline after chronic use, despite initial increases. This has been connected to the observation that the benefit of SSRIs may decrease in selected patients after a long-term treatment. A switch in medication will usually resolve this issue (up to 70% of the time).
The antidepressants mirtazapine and mianserin (5HT2 and 5HT3 receptors antagonists) have mood-elevating effects. This provides evidence for the theory that serotonin is most likely used to regulate the extent or intensity of moods, rather than level directly correlating with mood. In fact, the "5-HTTLPR" gene codes for the number of serotonin transporters in the brain, with more serotonin transporters causing decreased duration and magnitude of serotonergic signaling. The 5-HTTLPR polymorphism (l/l) causing more serotonin transporters to be formed is also found to be more resilient against depression and anxiety. Therefore, increasing levels of extracellular serotonin may be associated with increased affect, for good or for worse.
Although phobias and depression might be attenuated by serotonin-altering drugs, this does not mean the individual's situation has been improved, but only the individual's perception of the environment. Sometimes, a lower serotonin level might be beneficial, for example in the ultimatum game, where players with normal serotonin levels are more prone to accept unfair offers than participants whose serotonin levels have been artificially lowered.
Serotonin syndrome.
Extremely high levels of serotonin can cause a condition known as serotonin syndrome, with toxic and potentially fatal effects. In practice, such toxic levels are essentially impossible to reach through an overdose of a single antidepressant drug, but require a combination of serotonergic agents, such as an SSRI with an MAOI. The intensity of the symptoms of serotonin syndrome vary over a wide spectrum, and the milder forms are seen even at nontoxic levels.
Antiemetics.
Some 5-HT3 antagonists, such as ondansetron, granisetron, and tropisetron, are important antiemetic agents. They are particularly important in treating the nausea and vomiting that occur during anticancer chemotherapy using cytotoxic drugs. Another application is in the treatment of postoperative nausea and vomiting.
In unicellular organisms.
Serotonin is used by a variety of single-cell organisms for various purposes. SSRIs have been found to be toxic to algae. The gastrointestinal parasite "Entamoeba histolytica" secretes serotonin, causing a sustained secretory diarrhea in some patients. Patients infected with "E. histolytica" have been found to have highly elevated serum serotonin levels, which returned to normal following resolution of the infection. "E. histolytica" also responds to the presence of serotonin by becoming more virulent. This means serotonin secretion not only serves to increase the spread of enteamoebas by giving the host diarrhea but also serves to coordinate their behaviour according to their population density, a phenomenon known as quorum sensing. Outside a host, the density of entoamoebas is low, hence also the serotonin concentration. Low serotonin signals to the entoamoebas they are outside a host and they become less virulent to conserve energy. When they enter a new host, they multiply in the gut, and become more virulent as the serotonin concentration increases.
In plants.
In drying seeds, serotonin production is a way to get rid of the buildup of poisonous ammonia. The ammonia is collected and placed in the indole part of -tryptophan, which is then decarboxylated by tryptophan decarboxylase to give tryptamine, which is then hydroxylated by a cytochrome P450 monooxygenase, yielding serotonin.
However, since serotonin is a major gastrointestinal tract modulator, it may be produced by plants in fruits as a way of speeding the passage of seeds through the digestive tract, in the same way as many well-known seed and fruit associated laxatives. Serotonin is found in mushrooms, fruits and vegetables. The highest values of 25–400 mg/kg have been found in nuts of the walnut ("Juglans") and hickory ("Carya") genera. Serotonin concentrations of 3–30 mg/kg have been found in plantains, pineapples, banana, kiwifruit, plums, and tomatoes. Moderate levels from 0.1–3 mg/kg have been found in a wide range of tested vegetables.
Serotonin is one compound of the poison contained in stinging nettles ("Urtica dioica"), where it causes pain on injection in the same manner as its presence in insect venoms (see above). It is also naturally found in "Paramuricea clavata", or the Red Sea Fan.
Serotonin and tryptophan have been found in chocolate with varying cocoa contents. The highest serotonin content (2.93 µg/g) was found in chocolate with 85% cocoa, and the highest tryptophan content (13.27–13.34 µg/g) was found in 70–85% cocoa. The intermediate in the synthesis from tryptophan to serotonin, 5-hydroxytryptophan, was not found.
Methyl-tryptamines and hallucinogens.
Several plants contain serotonin together with a family of related tryptamines that are methylated at the amino (NH2) and (OH) groups, are "N"-oxides, or miss the OH group. These compounds do reach the brain, although some portion of them are metabolized by monoamine oxidase enzymes (mainly MAO-A) in the liver. Examples are plants from the "Anadenanthera" genus that are used in the hallucinogenic yopo snuff. These compounds are widely present in the leaves of many plants, and may serve as deterrents for animal ingestion. Serotonin occurs in several mushrooms of the genus "Panaeolus".
In insects.
Serotonin is evolutionary conserved and appears across the animal kingdom. It is seen in insect processes in roles similar to in the human central nervous system, such as memory, appetite, sleep, and behavior. Locust swarming is mediated by serotonin, by transforming social preference from aversion to a gregarious state that enables coherent groups. Learning in flies and honeybees is affected by the presence of serotonin. Insect 5-HT receptors have similar sequences to the vertebrate versions, but pharmacological differences have been seen. Invertebrate drug response has been far less characterized than mammalian pharmacology and the potential for species selective insecticides has been discussed.
Wasps and hornets have serotonin in their venom, as do scorpions.
History.
In 1935, Italian Vittorio Erspamer showed an extract from enterochromaffin cells made intestines contract. Some believed it contained adrenaline, but two years later, Erspamer was able to show it was a previously unknown amine, which he named "enteramine". In 1948, Maurice M. Rapport, Arda Green, and Irvine Page of the Cleveland Clinic discovered a vasoconstrictor substance in blood serum, and since it was a serum agent affecting vascular tone, they named it serotonin.
In 1952, enteramine was shown to be the same substance as serotonin, and as the broad range of physiological roles was elucidated, the abbreviation 5-HT of the proper chemical name 5-hydroxytryptamine became the preferred name in the pharmacological field. Synonyms of serotonin include: 5-hydroxytriptamine, thrombotin, enteramin, substance DS, and 3-(β-Aminoethyl)-5-hydroxyindole. In 1953, Betty Twarog and Page discovered serotonin in the central nervous system.

</doc>
<doc id="28765" url="http://en.wikipedia.org/wiki?curid=28765" title="Lawrence Alma-Tadema">
Lawrence Alma-Tadema

Sir Lawrence Alma-Tadema, OM, RA (; born Lourens Alma Tadema ]; 8 January 1836 – 25 June 1912) was a Frisian painter of special British denizenship.
Born in Dronrijp, the Netherlands, and trained at the Royal Academy of Antwerp, Belgium, he settled in England in 1870 and spent the rest of his life there. A classical-subject painter, he became famous for his depictions of the luxury and decadence of the Roman Empire, with languorous figures set in fabulous marbled interiors or against a backdrop of dazzling blue Mediterranean Sea and sky.
Though admired during his lifetime for his draftsmanship and depictions of Classical antiquity, his work fell into disrepute after his death, and only since the 1960s has it been re-evaluated for its importance within nineteenth-century English art.
Biography.
Early life.
Lourens Alma Tadema was born on 8 January 1836 in the village of Dronrijp in the province of Friesland in the north of the Netherlands. The surname "Tadema" is an old Frisian patronymic, meaning 'son of Tade', while the names "Lourens" and "Alma" came from his godfather. He was the sixth child of Pieter Jiltes Tadema (1797–1840), the village notary, and the third child of Hinke Dirks Brouwer ("c." 1800–1863). His father had three sons from a previous marriage. His parents' first child died young, and the second was Atje ("c." 1834–1876), Lourens' sister, for whom he had great affection.
The Tadema family moved in 1838 to the nearby city of Leeuwarden, where Pieter's position as a notary would be more lucrative. His father died when Lourens was four, leaving his mother with five children: Lourens, his sister, and three boys from his father’s first marriage. His mother had artistic leanings, and decided that drawing lessons should be incorporated into the children's education. He received his first art training with a local drawing master hired to teach his older half-brothers.
It was intended that the boy would become a lawyer; but in 1851 at the age of fifteen he suffered a physical and mental breakdown. Diagnosed as consumptive and given only a short time to live, he was allowed to spend his remaining days at his leisure, drawing and painting. Left to his own devices he regained his health and decided to pursue a career as an artist.
Move to Belgium.
In 1852 he entered the Royal Academy of Antwerp in Belgium where he studied early Dutch and Flemish art, under Egide Charles Gustave Wappers. During Alma-Tadema's four years as a registered student at the Academy, he won several respectable awards.
Before leaving school, towards the end of 1855, he became assistant to the painter and professor Louis (Lodewijk) Jan de Taeye, whose courses in history and historical costume he had greatly enjoyed at the Academy. Although de Taeye was not an outstanding painter, Alma-Tadema respected him and became his studio assistant, working with him for three years. De Taeye introduced him to books that influenced his desire to portray Merovingian subjects early in his career. He was encouraged to depict historical accuracy in his paintings, a trait for which the artist became known. 
Alma-Tadema left Taeye's studio in November 1858 returning to Leeuwarden before settling in Antwerp, where he began working with the painter Baron Jan August Hendrik Leys, whose studio was one of the most highly regarded in Belgium. Under his guidance Alma-Tadema painted his first major work: "The Education of the children of Clovis" (1861). This painting created a sensation among critics and artists when it was exhibited that year at the Artistic Congress in Antwerp. It is said to have laid the foundation of his fame and reputation. Alma-Tadema related that although Leys thought the completed painting better than he had expected, he was critical of the treatment of marble, which he compared to cheese. 
Alma-Tadema took this criticism very seriously, and it led him to improve his technique and to become the world's foremost painter of marble and variegated granite. Despite any reproaches from his master, "The Education of the Children of Clovis" was honorably received by critics and artists alike and was eventually purchased and subsequently given to King Leopold of Belgium.
Early works.
Merovingian themes were the painter's favourite subject up to the mid-1860s. It is perhaps in this series that we find the artist moved by the deepest feeling and the strongest spirit of romance. However Merovingian subjects did not have a wide international appeal, so he switched to themes of life in ancient Egypt that were more popular. On these scenes of Frankish and Egyptian life Alma-Tadema spent great energy and much research. In 1862 Alma-Tadema left Leys's studio and started his own career, establishing himself as a significant classical-subject European artist.
1863 was to alter the course of Alma-Tadema's personal and professional life: on 3 January his invalid mother died, and on 24 September he was married, in Antwerp City Hall, to Marie-Pauline Gressin Dumoulin, the daughter of Eugene Gressin Dumoulin, a French journalist living near Brussels. Nothing is known of their meeting and little of Pauline herself, as Alma-Tadema never spoke about her after her death in 1869. Her image appears in a number of oils, though he painted her portrait only three times, the most notable appearing in "My studio" (1867). The couple had three children. Their eldest and only son lived only a few months dying of smallpox. Their two daughters, Laurence (1864–1940) and Anna (1867–1943), both had artistic leanings: the former in literature, the latter in art. Neither would marry.
Alma-Tadema and his wife spent their honeymoon in Florence, Rome, Naples and Pompeii. This, his first visit to Italy, developed his interest in depicting the life of ancient Greece and Rome, especially the latter since he found new inspiration in the ruins of Pompeii, which fascinated him and would inspire much of his work in the coming decades.
During the summer of 1864, Tadema met Ernest Gambart, the most influential print publisher and art dealer of the period. Gambart was highly impressed with the work of Tadema, who was then painting "Egyptian chess players" (1865). The dealer, recognising at once the unusual gifts of the young painter, gave him an order for twenty-four pictures and arranged for three of Tadema's paintings to be shown in London. In 1865, Tadema relocated to Brussels where he was named a knight of the Order of Leopold.
On 28 May 1869, after years of ill health, Pauline died at Schaerbeek, in Belgium, at the age of thirty-two, of smallpox. Her death left Tadema disconsolate and depressed. He ceased painting for nearly four months. His sister Artje, who lived with the family, helped with the two daughters then aged five and two. Artje took over the role of housekeeper and remained with the family until 1873 when she married.
During the summer Tadema himself began to suffer from a medical problem which doctors in Brussels were frustratingly unable to diagnose. Gambart eventually advised him to go to England for another medical opinion. Soon after his arrival in London in December 1869, Alma-Tadema was invited to the home of the painter Ford Madox Brown. There he met Laura Theresa Epps, who was seventeen years old, and fell in love with her at first sight.
Move to England.
The outbreak of the Franco-Prussian War in July 1870 compelled Alma-Tadema to leave the continent and move to London. His infatuation with Laura Epps played a great part in his relocation to England and Gambart felt that the move would be advantageous to the artist's career. In stating his reasons for the move, Tadema simply said:
With his small daughters and sister Atje, Alma-Tadema arrived in London at the beginning of September 1870. The painter wasted no time in contacting Laura, and it was arranged that he would give her painting lessons. During one of these, he proposed marriage. As he was then thirty-four and Laura was now only eighteen, her father was initially opposed to the idea. Dr Epps finally agreed on the condition that they should wait until they knew each other better. They married in July 1871. Laura, under her married name, also won a high reputation as an artist, and appears in numerous of Alma-Tadema's canvases after their marriage ("The Women of Amphissa" (1887) being a notable example). This second marriage was enduring and happy, though childless, and Laura became stepmother to Anna and Laurence. Anna became a painter and Laurence became a novelist.
He would initially adopt the name "Laurence Alma Tadema" instead of "Lourens Alma Tadema" and later adopt the more English "Lawrence" for his forename, and incorporate "Alma" into his surname so that he appeared at the beginning of exhibition catalogues, under "A" rather than under "T". He did not actually hyphenate his last name, but it was done by others and this has since become the convention.
Victorian painter.
After his arrival in England, where he was to spend the rest of his life, Alma-Tadema's career was one of continued success. He became one of the most famous and highly paid artists of his time, acknowledged and rewarded. By 1871 he had met and befriended most of the major Pre-Raphaelite painters and it was in part due to their influence that the artist brightened his palette, varied his hues, and lightened his brushwork.
In 1872 Alma-Tadema organised his paintings into an identification system by including an opus number under his signature and assigning his earlier pictures numbers as well. "Portrait of my sister, Artje", painted in 1851, is numbered opus I, while two months before his death he completed "Preparations in the Coliseum", opus CCCCVIII. Such a system would make it difficult for fakes to be passed off as originals.
In 1873 Queen Victoria in Council by letters patent made Alma-Tadema and his wife what are now the last British Denizens (the legal process has theoretically not yet been abolished in the United Kingdom), with some limited special rights otherwise only accorded to and enjoyed by British subjects (what would now be called British citizens). The previous year he and his wife made a journey on the Continent that lasted five and a half months and took them through Brussels, Germany, and Italy. In Italy they were able to take in the ancient ruins again; this time he purchased several photographs, mostly of the ruins, which began his immense collection of folios with archival material sufficient for the documentation used in the completion of future paintings. In January 1876, he rented a studio in Rome. The family returned to London in April, visiting the Parisian Salon on their way back. In London he regularly met with fellow-artist Emil Fuchs.
Among the most important of his pictures during this period was "An Audience at Agrippa's" (1876). When an admirer of the painting offered to pay a substantial sum for a painting with a similar theme, Alma-Tadema simply turned the emperor around to show him leaving in "After the Audience".
On 19 June 1879, Alma-Tadema was made a full Academician, his most personally important award. Three years later a major retrospective of his entire oeuvre was organised at the Grosvenor Gallery in London, including 185 of his pictures.
In 1883 he returned to Rome and, most notably, Pompeii, where further excavations had taken place since his last visit. He spent a significant amount of time studying the site, going there daily. These excursions gave him an ample source of subject matter as he began to further his knowledge of daily Roman life. At times, however, he integrated so many objects into his paintings that some said they resembled museum catalogues.
One of his most famous paintings is "The Roses of Heliogabalus" (1888) – based on an episode from the life of the debauched Roman Emperor Elagabalus (Heliogabalus), the painting depicts the psychopathic Emperor suffocating his guest at an orgy under a cascade of rose petals. The blossoms depicted were sent weekly to the artist's London studio from the Riviera for four months during the winter of 1887–1888.
Among Alma-Tadema's works of this period are: "An Earthly Paradise" (1891), "Unconscious Rivals" (1893) "Spring" (1894), "The Coliseum" (1896) and "The Baths of Caracalla" (1899). Although Alma-Tadema's fame rests on his paintings set in Antiquity, he also painted portraits, landscapes and watercolours, and made some etchings himself (although many more were made of his paintings by others).
Personality.
For all the quiet charm and erudition of his paintings, Alma-Tadema himself preserved a youthful sense of mischief. He was childlike in his practical jokes and in his sudden bursts of bad temper, which could as suddenly subside into an engaging smile.
In his personal life, Alma-Tadema was an extrovert and had a remarkably warm personality. He had most of the characteristics of a child, coupled with the admirable traits of a consummate professional. A perfectionist, he remained in all respects a diligent, if somewhat obsessive and pedantic worker. He was an excellent businessman, and one of the wealthiest artists of the nineteenth century. Alma-Tadema was as firm in money matters as he was with the quality of his work.
As a man, Lawrence Alma-Tadema was a robust, fun loving and rather portly gentleman. There was not a hint of the delicate artist about him; he was a cheerful lover of wine, women and parties.
Later years.
Alma-Tadema's output decreased with time, partly on account of health, but also because of his obsession with decorating his new home, to which he moved in 1883. Nevertheless, he continued to exhibit throughout the 1880s and into the next decade, receiving a plentiful amount of accolades along the way, including the medal of Honour at the Paris Exposition Universelle of 1889, election to an honorary member of the Oxford University Dramatic Society in 1890, the Great Gold Medal at the International Exposition in Brussels of 1897. In 1899 he was Knighted in England, only the eighth artist from the Continent to receive the honour. Not only did he assist with the organisation of the British section at the 1900 Exposition Universelle in Paris, he also exhibited two works that earned him the Grand Prix Diploma. He also assisted with the St. Louis World's Fair of 1904 where he was well represented and received.
During this time, Alma-Tadema was very active with theatre design and production, designing many costumes. He also spread his artistic boundaries and began to design furniture, often modelled after Pompeian or Egyptian motifs, illustrations, textiles, and frame making. His diverse interests highlight his talents. Each of these exploits were used in his paintings, as he often incorporated some of his designed furniture into the composition, and must have used many of his own designs for the clothing of his female subjects. Through his last period of creativity Alma-Tadema continued to produce paintings, which repeat the successful formula of women in marble terraces overlooking the sea such as in "Silver Favourites" (1903). Between 1906 and his death six years later, Alma-Tadema painted less but still produced ambitious paintings like "The Finding of Moses" (1904).
On 15 August 1909 Alma-Tadema's wife, Laura, died at the age of fifty-seven. The grief-stricken widower outlived his second wife by less than three years. His last major composition was "Preparation in the Coliseum (1912)". In the summer of 1912, Alma Tadema was accompanied by his daughter Anna to Kaiserhof Spa, Wiesbaden, Germany where he was to undergo treatment for ulceration of the stomach. He died there on 28 June 1912 at the age of seventy-six. He was buried in a crypt in St Paul's Cathedral in London.
Style.
Alma-Tadema's works are remarkable for the way in which flowers, textures and hard reflecting substances, like metals, pottery, and especially marble, are painted – indeed, his realistic depiction of marble led him to be called the 'marbellous painter'. His work shows much of the fine execution and brilliant colour of the old Dutch masters. By the human interest with which he imbues all his scenes from ancient life he brings them within the scope of modern feeling, and charms us with gentle sentiment and playfulness.
From early in his career, Alma-Tadema was particularly concerned with architectural accuracy, often including objects that he would see at museums – such as the British Museum in London – in his works. He also read many books and took many images from them. He amassed an enormous number of photographs from ancient sites in Italy, which he used for the most precise accuracy in the details of his compositions.
Alma-Tadema was a perfectionist. He worked assiduously to make the most of his paintings, often repeatedly reworking parts of paintings before he found them satisfactory to his own high standards. One humorous story relates that one of his paintings was rejected and instead of keeping it, he gave the canvas to a maid who used it as her table cover. He was sensitive to every detail and architectural line of his paintings, as well as the settings he was depicting. For many of the objects in his paintings, he would depict what was in front of him, using fresh flowers imported from across the continent and even from Africa, rushing to finish the paintings before the flowers died. It was this commitment to veracity that earned him recognition but also caused many of his adversaries to take up arms against his almost encyclopaedic works.
Alma-Tadema's work has been linked with that of European Symbolist painters. As an artist of international reputation, he can be cited as an influence on European figures such as Gustav Klimt and Fernand Khnopff. Both painters incorporate classical motifs into their works and use Alma-Tadema's unconventional compositional devices such as abrupt cut-off at the edge of the canvas. They, like Alma-Tadema, also employ coded imagery to convey meaning to their paintings.
Reputation.
Alma-Tadema was among the most financially successful painters of the Victorian era, though never matching Edwin Henry Landseer. For over sixty years he gave his audience exactly what they wanted: distinctive, elaborate paintings of beautiful people in classical settings. His incredibly detailed reconstructions of ancient Rome, with languid men and women posed against white marble in dazzling sunlight provided his audience with a glimpse of a world of the kind they might one day construct for themselves at least in attitude if not in detail. As with other painters, the reproduction rights for prints were often worth more than the canvas, and a painting with its rights still attached may have been sold to Gambart for £10,000 in 1874; without rights it was sold again in 1903, when Alma-Tadema's prices were actually higher, for £2,625. Typical prices were between £2,000 and £3,000 in the 1880s, but at least three works sold for between £5,250 and £6,060 in the 1900s. Prices held well until the general collapse of Victorian prices in the early 1920s, when they fell to the hundreds, where they remained until the 1960s; by 1969 £4,600 had been reached again (the huge effect of inflation must of course be remembered for all these figures).
The last years of Alma-Tadema's life saw the rise of Post-Impressionism, Fauvism, Cubism and Futurism, of which he heartily disapproved. As his pupil John Collier wrote, 'it is impossible to reconcile the art of Alma-Tadema with that of Matisse, Gauguin and Picasso.'
His artistic legacy almost vanished. As attitudes of the public in general and the artists in particular became more sceptical of the possibilities of human achievement, his paintings were increasingly denounced. He was declared "the worst painter of the 19th century" by John Ruskin, and one critic even remarked that his paintings were "about worthy enough to adorn bourbon boxes." After this brief period of being actively derided, he was consigned to relative obscurity for many years. Only since the 1960s has Alma-Tadema's work been re-evaluated for its importance within the nineteenth century, and more specifically, within the evolution of English art.
He is now regarded as one of the principal classical-subject painters of the nineteenth century whose works demonstrate the care and exactitude of an era mesmerised by trying to visualise the past, some of which was being recovered through archaeological research.
Alma-Tadema's meticulous archaeological research, including research into Roman architecture (which was so thorough that every building featured in his canvases could have been built using Roman tools and methods) led to his paintings being used as source material by Hollywood directors in their vision of the ancient world for films such as D. W. Griffith's "Intolerance" (1916), "Ben Hur" (1926)," Cleopatra" (1934), and most notably of all, Cecil B. DeMille's epic remake of "The Ten Commandments" (1956). Indeed, Jesse Lasky Jr., the co-writer on "The Ten Commandments", described how the director would customarily spread out prints of Alma-Tadema paintings to indicate to his set designers the look he wanted to achieve. The designers of the Oscar-winning Roman epic "Gladiator" used the paintings of Alma-Tadema as a central source of inspiration. Alma-Tadema's paintings were also the inspiration for the design of the interior of Cair Paravel castle in the 2005 film "".
In 1962, New York art dealer Robert Isaacson mounted the first show of Alma-Tadema's work in fifty years; by the late 1960s, the revival of interest in Victorian painting gained impetus, and a number of well-attended exhibitions were held. Allen Funt, the creator and host of the American version of the television show "Candid Camera", was a collector of Alma-Tadema paintings at a time when the artist's reputation in the 20th century was at its nadir; in a relatively few years he bought 35 works, about ten percent of Alma-Tadema's output. After Funt was robbed by his accountant (who subsequently committed suicide), he was forced to sell his collection at Sotheby's in London in November 1973. From this sale, the interest in Alma-Tadema was re-awakened. 
In 1960, the Newman Gallery firstly tried to sell, then give away (without success) one of his most celebrated works, "The Finding of Moses" (1904). The initial purchaser had paid £5,250 for it on its completion, and subsequent sales were for £861 in 1935, £265 in 1942, and it was "bought in" at £252 in 1960 (having failed to meet its reserve), but when the same picture was auctioned at Christies in New York in May 1995, it sold for £1.75 million. On 4 November 2010 it was sold for $35,922,500 to an undisclosed bidder at Sotheby's New York, a new record for the artist and a Victorian painting. On 5 May 2011 his "The Meeting of Antony and Cleopatra: 41 BC" was sold at the same auction house for $29.2 million.
A blue plaque unveiled in 1975 commemorates Alma-Tadema at 44 Grove End Road, St John's Wood.

</doc>
<doc id="28766" url="http://en.wikipedia.org/wiki?curid=28766" title="Surrealism">
Surrealism

Surrealism is a cultural movement that began in the early 1920s, and is best known for its visual artworks and writings. The aim was to "resolve the previously contradictory conditions of dream and reality." Artists painted unnerving, illogical scenes with photographic precision, created strange creatures from everyday objects and developed painting techniques that allowed the unconscious to express itself.
Surrealist works feature the element of surprise, unexpected juxtapositions and non sequitur; however, many Surrealist artists and writers regard their work as an expression of the philosophical movement first and foremost, with the works being an artifact. Leader André Breton was explicit in his assertion that Surrealism was, above all, a revolutionary movement.
Surrealism developed out of the Dada activities during World War I and the most important center of the movement was Paris. From the 1920s onward, the movement spread around the globe, eventually affecting the visual arts, literature, film, and music of many countries and languages, as well as political thought and practice, philosophy, and social theory.
Founding of the movement.
The word 'surrealist' was coined by Guillaume Apollinaire and first appeared in the preface to his play "Les Mamelles de Tirésias", which was written in 1903 and first performed in 1917.
World War I scattered the writers and artists who had been based in Paris, and in the interim many became involved with Dada, believing that excessive rational thought and bourgeois values had brought the conflict of the war upon the world. The Dadaists protested with anti-art gatherings, performances, writings and art works. After the war, when they returned to Paris, the Dada activities continued.
During the war, André Breton, who had trained in medicine and psychiatry, served in a neurological hospital where he used Sigmund Freud's psychoanalytic methods with soldiers suffering from shell-shock. Meeting the young writer Jacques Vaché, Breton felt that Vaché was the spiritual son of writer and pataphysics founder Alfred Jarry. He admired the young writer's anti-social attitude and disdain for established artistic tradition. Later Breton wrote, "In literature, I was successively taken with Rimbaud, with Jarry, with Apollinaire, with Nouveau, with Lautréamont, but it is Jacques Vaché to whom I owe the most."
Back in Paris, Breton joined in Dada activities and started the literary journal "Littérature" along with Louis Aragon and Philippe Soupault. They began experimenting with automatic writing—spontaneously writing without censoring their thoughts—and published the writings, as well as accounts of dreams, in the magazine. Breton and Soupault delved deeper into automatism and wrote "The Magnetic Fields" (1920).
Continuing to write, they attracted more artists and writers; they came to believe that automatism was a better tactic for societal change than the Dada attack on prevailing values. The group grew to include Paul Éluard, Benjamin Péret, René Crevel, Robert Desnos, Jacques Baron, Max Morise, Pierre Naville, Roger Vitrac, Gala Éluard, Max Ernst, Salvador Dalí, Man Ray, Hans Arp, Georges Malkine, Michel Leiris, Georges Limbour, Antonin Artaud, Raymond Queneau, André Masson, Joan Miró, Marcel Duchamp, Jacques Prévert, and Yves Tanguy.
As they developed their philosophy, they believed that Surrealism would advocate the idea that ordinary and depictive expressions are vital and important, but that the sense of their arrangement must be open to the full range of imagination according to the Hegelian Dialectic. They also looked to the Marxist dialectic and the work of such theorists as Walter Benjamin and Herbert Marcuse.
Freud's work with free association, dream analysis, and the unconscious was of utmost importance to the Surrealists in developing methods to liberate imagination. They embraced idiosyncrasy, while rejecting the idea of an underlying madness. As Salvador Dalí later proclaimed, "There is only one difference between a madman and me. I am not mad."
Beside the use of dream analysis, they emphasized that "one could combine inside the same frame, elements not normally found together to produce illogical and startling effects." Breton included the idea of the startling juxtapositions in his 1924 manifesto, taking it in turn from a 1918 essay by poet Pierre Reverdy, which said: "a juxtaposition of two more or less distant realities. The more the relationship between the two juxtaposed realities is distant and true, the stronger the image will be -- the greater its emotional power and poetic reality.".
The group aimed to revolutionize human experience, in its personal, cultural, social, and political aspects. They wanted to free people from false rationality, and restrictive customs and structures. Breton proclaimed that the true aim of Surrealism was "long live the social revolution, and it alone!" To this goal, at various times Surrealists aligned with communism and anarchism.
In 1924 two Surrealist factions declared their philosophy in two separate Surrealist Manifestos. That same year the Bureau of Surrealist Research was established, and began publishing the journal "La Révolution surréaliste".
Surrealist Manifestos.
Leading up to 1924, two rival surrealist groups had formed. Each group claimed to be successors of a revolution launched by Guillaume Apollinaire. One group, led by Yvan Goll, consisted of Pierre Albert-Birot, Paul Dermée, Céline Arnauld, Francis Picabia, Tristan Tzara, Giuseppe Ungaretti, Pierre Reverdy, Marcel Arland, Joseph Delteil, Jean Painlevé and Robert Delaunay, among others.
The other group, led by Breton, included Louis Aragon, Robert Desnos, Paul Éluard, Jacques Baron, Jacques-André Boiffard, Jean Carrive, René Crevel and Georges Malkine, among others.
Yvan Goll published the "Manifeste du surréalisme", 1 October 1924, in his first and only issue of "Surréalisme" two weeks prior to the release of Breton's "Manifeste du surréalisme", published by Éditions du Sagittaire, 15 October 1924.
Goll and Breton clashed openly, at one point literally fighting, at the Comédie des Champs-Élysées, over the rights to the term 'Surrealism'. In the end, Breton won the battle through tactical and numerical superiority. Though the quarrel over the anteriority of 'Surrealism' concluded with the victory of Breton, the history of surrealism from that moment would remain marked by fractures, resignations, and resounding excommunications, with each surrealist having their own view of the issue and goals, and accepting more or less the definitions laid out by André Breton.
Breton's 1924 "Surrealist Manifesto" defines the purposes of Surrealism. He included citations of the influences on Surrealism, examples of Surrealist works, and discussion of Surrealist automatism. He provided the following definitions:
 Dictionary: Surrealism, n. Pure psychic automatism, by which one proposes to express, either verbally, in writing, or by any other manner, the real functioning of thought. Dictation of thought in the absence of all control exercised by reason, outside of all aesthetic and moral preoccupation.
Encyclopedia: Surrealism. Philosophy. Surrealism is based on the belief in the superior reality of certain forms of previously neglected associations, in the omnipotence of dream, in the disinterested play of thought. It tends to ruin once and for all other psychic mechanisms and to substitute itself for them in solving all the principal problems of life.
"La Révolution surréaliste".
Shortly after the release of Breton's "Surrealist Manifesto", the Surrealists published the inaugural issue of "La Révolution surréaliste". Publication continued into 1929. As the first directors, Naville and Péret modeled the format of the journal on the conservative scientific review "La Nature." To the Surrealists' delight, the journal was consistently scandalous and revolutionary. While the focus was on writing, the journal also included reproductions of art, among them works by Giorgio de Chirico, Ernst, Masson, and Man Ray.
Bureau of Surrealist Research.
The Bureau of Surrealist Research (Centrale Surréaliste) was the center for Surrealist writers and artists to meet, hold discussions, and conduct interviews. They investigated speech under trance.
Expansion.
The movement in the mid-1920s was characterized by meetings in cafes where the Surrealists played collaborative drawing games, discussed the theories of Surrealism, and developed a variety of techniques such as automatic drawing. Breton initially doubted that visual arts could even be useful in the Surrealist movement since they appeared to be less malleable and open to chance and automatism. This caution was overcome by the discovery of such techniques as frottage and decalcomania.
Soon more visual artists became involved, including Giorgio de Chirico, Max Ernst, Joan Miró, Francis Picabia, Yves Tanguy, Salvador Dalí, Luis Buñuel, Alberto Giacometti, Valentine Hugo, Méret Oppenheim, Toyen, Kansuke Yamamoto and later after the second war: Enrico Donati. Though Breton admired Pablo Picasso and Marcel Duchamp and courted them to join the movement, they remained peripheral. More writers also joined, including former Dadaist Tristan Tzara, René Char, and Georges Sadoul.
In 1925 an autonomous Surrealist group formed in Brussels. The group included the musician, poet, and artist E. L. T. Mesens, painter and writer René Magritte, Paul Nougé, Marcel Lecomte, and André Souris. In 1927 they were joined by the writer Louis Scutenaire. They corresponded regularly with the Paris group, and in 1927 both Goemans and Magritte moved to Paris and frequented Breton's circle. The artists, with their roots in Dada and Cubism, the abstraction of Wassily Kandinsky, Expressionism, and Post-Impressionism, also reached to older "bloodlines" such as Hieronymus Bosch, and the so-called primitive and naive arts.
André Masson's automatic drawings of 1923 are often used as the point of the acceptance of visual arts and the break from Dada, since they reflect the influence of the idea of the unconscious mind. Another example is Giacometti's 1925 "Torso", which marked his movement to simplified forms and inspiration from preclassical sculpture.
However, a striking example of the line used to divide Dada and Surrealism among art experts is the pairing of 1925's "Little Machine Constructed by Minimax Dadamax in Person (Von minimax dadamax selbst konstruiertes maschinchen)" with "The Kiss (Le Baiser)" from 1927 by Max Ernst. The first is generally held to have a distance, and erotic subtext, whereas the second presents an erotic act openly and directly. In the second the influence of Miró and the drawing style of Picasso is visible with the use of fluid curving and intersecting lines and colour, whereas the first takes a directness that would later be influential in movements such as Pop art.
Giorgio de Chirico, and his previous development of metaphysical art, was one of the important joining figures between the philosophical and visual aspects of Surrealism. Between 1911 and 1917, he adopted an unornamented depictional style whose surface would be adopted by others later. "The Red Tower (La tour rouge)" from 1913 shows the stark colour contrasts and illustrative style later adopted by Surrealist painters. His 1914 "The Nostalgia of the Poet (La Nostalgie du poète)" has the figure turned away from the viewer, and the juxtaposition of a bust with glasses and a fish as a relief defies conventional explanation. He was also a writer whose novel "Hebdomeros" presents a series of dreamscapes with an unusual use of punctuation, syntax, and grammar designed to create an atmosphere and frame around its images. His images, including set designs for the Ballets Russes, would create a decorative form of Surrealism, and he would be an influence on the two artists who would be even more closely associated with Surrealism in the public mind: Dalí and Magritte. He would, however, leave the Surrealist group in 1928.
In 1924, Miró and Masson applied Surrealism to painting. The first Surrealist exhibition, "La Peinture Surrealiste", was held at Galerie Pierre in Paris in 1925. It displayed works by Masson, Man Ray, Paul Klee, Miró, and others. The show confirmed that Surrealism had a component in the visual arts (though it had been initially debated whether this was possible), and techniques from Dada, such as photomontage, were used. The following year, on March 26, 1926 Galerie Surréaliste opened with an exhibition by Man Ray. Breton published "Surrealism and Painting" in 1928 which summarized the movement to that point, though he continued to update the work until the 1960s.
Writing continues.
The first Surrealist work, according to leader Breton, was "Les Champs Magnétiques" (May–June 1919). "Littérature" contained automatist works and accounts of dreams. The magazine and the portfolio both showed their disdain for literal meanings given to objects and focused rather on the undertones, the poetic undercurrents present. Not only did they give emphasis to the poetic undercurrents, but also to the connotations and the overtones which "exist in ambiguous relationships to the visual images."
Because Surrealist writers seldom, if ever, appear to organize their thoughts and the images they present, some people find much of their work difficult to parse. This notion however is a superficial comprehension, prompted no doubt by Breton's initial emphasis on automatic writing as the main route toward a higher reality. But—as in Breton's case—much of what is presented as purely automatic is actually edited and very "thought out". Breton himself later admitted that automatic writing's centrality had been overstated, and other elements were introduced, especially as the growing involvement of visual artists in the movement forced the issue, since automatic painting required a rather more strenuous set of approaches. Thus such elements as collage were introduced, arising partly from an ideal of startling juxtapositions as revealed in Pierre Reverdy's poetry. And—as in Magritte's case (where there is no obvious recourse to either automatic techniques or collage)—the very notion of convulsive joining became a tool for revelation in and of itself. Surrealism was meant to be always in flux—to be more modern than modern—and so it was natural there should be a rapid shuffling of the philosophy as new challenges arose.
Surrealists revived interest in Isidore Ducasse, known by his pseudonym Comte de Lautréamont, and for the line "beautiful as the chance meeting on a dissecting table of a sewing machine and an umbrella", and Arthur Rimbaud, two late 19th-century writers believed to be the precursors of Surrealism.
Examples of Surrealist literature are Artaud's "Le Pèse-Nerfs" (1926), Aragon's "Irene's Cunt" (1927), Péret's "Death to the Pigs" (1929), Crevel's "Mr. Knife Miss Fork" (1931), Sadegh Hedayat's "the Blind Owl" (1937), and Breton's "Sur la route de San Romano" (1948).
"La Révolution surréaliste" continued publication into 1929 with most pages densely packed with columns of text, but also included reproductions of art, among them works by de Chirico, Ernst, Masson, and Man Ray. Other works included books, poems, pamphlets, automatic texts and theoretical tracts.
Surrealist films.
Early films by Surrealists include:
Surrealist theatre.
The word "surrealist" was first used by Guillaume Apollinaire to describe his 1917 play Les Mamelles de Tirésias (The Breasts of Tiresias), which was later adapted into an opera by Francis Poulenc.
Antonin Artaud, an early Surrealist, rejected the majority of Western theatre as a perversion of its original intent, which he felt should be a mystical, metaphysical experience. He thought that rational discourse comprised "falsehood and illusion." Theorising a new theatrical form that would be immediate and direct, that would link the unconscious minds of performers and spectators in a sort of ritual event, Artaud created the Theatre of Cruelty, in which emotions, feelings, and the metaphysical were expressed not through language but physically, creating a mythological, archetypal, allegorical vision, closely related to the world of dreams.
The other major theatre practitioner to have experimented with surrealism in the theatre is the Spanish playwright and director Federico García Lorca, particularly in his plays "The Public" (1930), "When Five Years Pass" (1931), and "Play Without a Title" (1935). Other surrealist plays include Aragon's "Backs to the Wall" (1925) and Roger Vitrac's "The Mysteries of Love" (1927) and "Victor, or The Children Take Over" (1928). Gertrude Stein's opera "Doctor Faustus Lights the Lights" (1938) has also been described as "American Surrealism", though it is also related to a theatrical form of cubism.
Music by Surrealists.
In the 1920s several composers were influenced by Surrealism, or by individuals in the Surrealist movement. Among them were Bohuslav Martinů, André Souris, and Edgard Varèse, who stated that his work "Arcana" was drawn from a dream sequence. Souris in particular was associated with the movement: he had a long relationship with Magritte, and worked on Paul Nougé's publication "Adieu Marie".
Germaine Tailleferre of the French group Les Six wrote several works which could be considered to be inspired by Surrealism, including the 1948 Ballet "Paris-Magie" (scenario by Lise Deharme), the Operas "La Petite Sirène" (book by Philippe Soupault) and "Le Maître" (book by Eugène Ionesco). Tailleferre also wrote popular songs to texts by Claude Marci, the wife of Henri Jeanson, whose portrait had been painted by Magritte in the 1930s.
Even though Breton by 1946 responded rather negatively to the subject of music with his essay "Silence is Golden", later Surrealists, such as Paul Garon, have been interested in—and found parallels to—Surrealism in the improvisation of jazz and the blues. Jazz and blues musicians have occasionally reciprocated this interest. For example, the 1976 World Surrealist Exhibition included performances by David Honeyboy Edwards.
Surrealism and international politics.
Surrealism as a political force developed unevenly around the world: in some places more emphasis was on artistic practices, in other places on political practices, and in other places still, Surrealist praxis looked to supersede both the arts and politics. During the 1930s, the Surrealist idea spread from Europe to North America, South America (founding of the "Mandrágora" group in Chile in 1938), Central America, the Caribbean, and throughout Asia, as both an artistic idea and as an ideology of political change.
Politically, Surrealism was Trotskyist, communist, or anarchist. The split from Dada has been characterised as a split between anarchists and communists, with the Surrealists as communist. Breton and his comrades supported Leon Trotsky and his International Left Opposition for a while, though there was an openness to anarchism that manifested more fully after World War II. Some Surrealists, such as Benjamin Péret, Mary Low, and Juan Breá, aligned with forms of left communism. Others fought for complete liberty from political ideologies, like Wolfgang Paalen, who, after Trotzky´s assassination in Mexico, prepared a schism between art and politics through his counter-surrealist art-magazine DYN and so prepared the ground for the abstract expressionists. Dalí supported capitalism and the fascist dictatorship of Francisco Franco but cannot be said to represent a trend in Surrealism in this respect; in fact he was considered, by Breton and his associates, to have betrayed and left Surrealism. Benjamin Péret, Mary Low and Juan Breá joined the POUM during the Spanish Civil War.
Breton's followers, along with the Communist Party, were working for the "liberation of man." However, Breton's group refused to prioritize the proletarian struggle over radical creation such that their struggles with the Party made the late 1920s a turbulent time for both. Many individuals closely associated with Breton, notably Louis Aragon, left his group to work more closely with the Communists.
Surrealists have often sought to link their efforts with political ideals and activities. In the "Declaration of January 27, 1925", for example, members of the Paris-based Bureau of Surrealist Research (including André Breton, Louis Aragon, and, Antonin Artaud, as well as some two dozen others) declared their affinity for revolutionary politics. While this was initially a somewhat vague formulation, by the 1930s many Surrealists had strongly identified themselves with communism. The foremost document of this tendency within Surrealism is the "Manifesto for a Free Revolutionary Art", published under the names of Breton and Diego Rivera, but actually co-authored by Breton and Leon Trotsky.
However, in 1933 the Surrealists’ assertion that a 'proletarian literature' within a capitalist society was impossible led to their break with the Association des Ecrivains et Artistes Révolutionnaires, and the expulsion of Breton, Éluard and Crevel from the Communist Party.
In 1925, the Paris Surrealist group and the extreme left of the French Communist Party came together to support Abd-el-Krim, leader of the Rif uprising against French colonialism in Morocco. In an open letter to writer and French ambassador to Japan, Paul Claudel, the Paris group announced:
The anticolonial revolutionary and proletarian politics of "Murderous Humanitarianism" (1932) which was drafted mainly by René Crevel, signed by André Breton, Paul Éluard, Benjamin Péret, Yves Tanguy, and the Martiniquan Surrealists Pierre Yoyotte and J.M. Monnerot perhaps makes it the original document of what is later called 'black Surrealism', although it is the contact between Aimé Césaire and Breton in the 1940s in Martinique that really lead to the communication of what is known as 'black Surrealism'.
Anticolonial revolutionary writers in the Négritude movement of Martinique, a French colony at the time, took up Surrealism as a revolutionary method - a critique of European culture and a radical subjective. This linked with other Surrealists and was very important for the subsequent development of Surrealism as a revolutionary praxis. The journal "Tropiques", featuring the work of Césaire along with Suzanne Césaire, René Ménil, Lucie Thésée, Aristide Maugée and others, was first published in 1941.
It is interesting to note that when in 1938 André Breton traveled with his wife the painter Jacqueline Lamba to Mexico to meet Trotsky (staying as the guest of Diego Rivera's former wife Guadalupe Marin), he met Frida Kahlo and saw her paintings for the first time. Breton declared Kahlo to be an "innate" Surrealist painter.
Internal politics.
In 1929 the satellite group around the journal "Le Grand Jeu", including Roger Gilbert-Lecomte, Maurice Henry and the Czech painter Josef Sima, was ostracized. Also in February, Breton asked Surrealists to assess their "degree of moral competence", and theoretical refinements included in the second "manifeste du surréalisme" excluded anyone reluctant to commit to collective action, a list which included Leiris, Georges Limbour, Max Morise, Baron, Queneau, Prévert, Desnos, Masson and Boiffard. Excluded members launched a counterattack, sharply criticizing Breton in the pamphlet "Un Cadavre", which featured a picture of Breton wearing a crown of thorns. The pamphlet drew upon an earlier act of subversion by likening Breton to Anatole France, whose unquestioned value Breton had challenged in 1924.
In hindsight, the disunion of 1929-30 and the effects of "Un Cadavre" had very little negative impact upon Surrealism as Breton saw it, since core figures such as Aragon, Crevel, Dalí and Buñuel remained true the idea of group action, at least for the time being. The success (or at least the controversy) of Dalí and Buñuel's film L'Age d'Or in December 1930 had a regenerative effect, drawing a number of new recruits, and encouraging countless new artistic works the following year and throughout the 1930s.
Disgruntled surrealists moved to the periodical "Documents", edited by Georges Bataille, whose anti-idealist materialism formed a hybrid Surrealism intending to expose the base instincts of humans. To the dismay of many, "Documents" fizzled out in 1931, just as Surrealism seemed to be gathering more steam.
There were a number of reconciliations after this period of disunion, such as between Breton and Bataille, while Aragon left the group after committing himself to the French Communist Party in 1932. More members were ousted over the years for a variety of infractions, both political and personal, while others left of to pursue creativity of their own style.
By the end of World War II the surrealist group led by André Breton decided to explicitly embrace anarchism. In 1952 Breton wrote "It was in the black mirror of anarchism that surrealism first recognised itself." "Breton was consistent in his support for the francophone Anarchist Federation and he continued to offer his solidarity after the Platformists around Fontenis transformed the FA into the Fédération Communiste Libertaire. He was one of the few intellectuals who continued to offer his support to the FCL during the Algerian war when the FCL suffered severe repression and was forced underground. He sheltered Fontenis whilst he was in hiding. He refused to take sides on the splits in the French anarchist movement and both he and Peret expressed solidarity as well with the new Fédération anarchiste set up by the synthesist anarchists and worked in the Antifascist Committees of the 60s alongside the FA."
Golden age.
Throughout the 1930s, Surrealism continued to become more visible to the public at large. A Surrealist group developed in Britain and, according to Breton, their 1936 London International Surrealist Exhibition was a high-water mark of the period and became the model for international exhibitions.
Dalí and Magritte created the most widely recognized images of the movement. Dalí joined the group in 1929, and participated in the rapid establishment of the visual style between 1930 and 1935.
Surrealism as a visual movement had found a method: to expose psychological truth; stripping ordinary objects of their normal significance, to create a compelling image that was beyond ordinary formal organization, in order to evoke empathy from the viewer.
1931 was a year when several Surrealist painters produced works which marked turning points in their stylistic evolution: Magritte's "Voice of Space (La Voix des airs)" is an example of this process, where three large spheres representing bells hang above a landscape. Another Surrealist landscape from this same year is Yves Tanguy's "", with its molten forms and liquid shapes. Liquid shapes became the trademark of Dalí, particularly in his "The Persistence of Memory", which features the image of watches that sag as if they were melting.
The characteristics of this style—a combination of the depictive, the abstract, and the psychological—came to stand for the alienation which many people felt in the modern period, combined with the sense of reaching more deeply into the psyche, to be "made whole with one's individuality".
Between 1930 and 1933, the Surrealist Group in Paris issued the periodical "Le Surréalisme au service de la révolution" as the successor of "La Révolution surréaliste".
From 1936 through 1938 Wolfgang Paalen, Gordon Onslow Ford, and Roberto Matta joined the group. Paalen contributed Fumage and Onslow Ford Coulage as new pictorial automatic techniques.
Long after personal, political and professional tensions fragmented the Surrealist group, Magritte and Dalí continued to define a visual program in the arts. This program reached beyond painting, to encompass photography as well, as can be seen from a Man Ray self-portrait, whose use of assemblage influenced Robert Rauschenberg's collage boxes.
During the 1930s Peggy Guggenheim, an important American art collector, married Max Ernst and began promoting work by other Surrealists such as Yves Tanguy and the British artist John Tunnard.
Major exhibitions in the 1930s
World War II and the Post War period.
World War II created havoc not only for the general population of Europe but especially for the European artists and writers that opposed Fascism, and Nazism. Many important artists fled to North America, and relative safety in the United States. The art community in New York City in particular was already grappling with Surrealist ideas and several artists like Arshile Gorky, Jackson Pollock, and Robert Motherwell converged closely with the surrealist artists themselves, albeit with some suspicion and reservations. Ideas concerning the unconscious and dream imagery were quickly embraced. By the Second World War, the taste of the American avant-garde in New York City swung decisively towards Abstract Expressionism with the support of key taste makers, including Peggy Guggenheim, Leo Steinberg and Clement Greenberg. However, it should not be easily forgotten that Abstract Expressionism itself grew directly out of the meeting of American (particularly New York) artists with European Surrealists self-exiled during World War II. In particular, Gorky and Paalen influenced the development of this American art form, which, as Surrealism did, celebrated the instantaneous human act as the well-spring of creativity. The early work of many Abstract Expressionists reveals a tight bond between the more superficial aspects of both movements, and the emergence (at a later date) of aspects of Dadaistic humor in such artists as Rauschenberg sheds an even starker light upon the connection. Up until the emergence of Pop Art, Surrealism can be seen to have been the single most important influence on the sudden growth in American arts, and even in Pop, some of the humor manifested in Surrealism can be found, often turned to a cultural criticism.
The Second World War overshadowed, for a time, almost all intellectual and artistic production. In 1939 Wolfgang Paalen was the first to leave Paris for the New World as exile. After a long trip through the forests of British-Columbia, he settled in Mexico and founded his influential art-magazine Dyn. In 1940 Yves Tanguy married American Surrealist painter Kay Sage. In 1941, Breton went to the United States, where he co-founded the short-lived magazine "VVV" with Max Ernst, Marcel Duchamp, and the American artist David Hare. However, it was the American poet, Charles Henri Ford, and his magazine "View" which offered Breton a channel for promoting Surrealism in the United States. The "View" special issue on Duchamp was crucial for the public understanding of Surrealism in America. It stressed his connections to Surrealist methods, offered interpretations of his work by Breton, as well as Breton's view that Duchamp represented the bridge between early modern movements, such as Futurism and Cubism, to Surrealism. Wolfgang Paalen left the group in 1942 due to political/philosophical differences with Breton.
Though the war proved disruptive for Surrealism, the works continued. Many Surrealist artists continued to explore their vocabularies, including Magritte. Many members of the Surrealist movement continued to correspond and meet. While Dalí may have been excommunicated by Breton, he neither abandoned his themes from the 1930s, including references to the "persistence of time" in a later painting, nor did he become a depictive pompier. His classic period did not represent so sharp a break with the past as some descriptions of his work might portray, and some, such as Thirion, argued that there were works of his after this period that continued to have some relevance for the movement.
During the 1940s Surrealism's influence was also felt in England and America. Mark Rothko took an interest in biomorphic figures, and in England Henry Moore, Lucian Freud, Francis Bacon and Paul Nash used or experimented with Surrealist techniques. However, Conroy Maddox, one of the first British Surrealists whose work in this genre dated from 1935, remained within the movement, and organized an exhibition of current Surrealist work in 1978 in response to an earlier show which infuriated him because it did not properly represent Surrealism. Maddox's exhibition, titled "Surrealism Unlimited", was held in Paris and attracted international attention. He held his last one-man show in 2002, and died three years later.
Magritte's work became more realistic in its depiction of actual objects, while maintaining the element of juxtaposition, such as in 1951's "Personal Values (Les Valeurs Personnelles)" and 1954's "Empire of Light (L’Empire des lumières)". Magritte continued to produce works which have entered artistic vocabulary, such as "Castle in the Pyrenees (Le Château des Pyrénées)", which refers back to "Voix" from 1931, in its suspension over a landscape.
Other figures from the Surrealist movement were expelled. Several of these artists, like Roberto Matta (by his own description) "remained close to Surrealism."
After the crushing of the Hungarian Revolution of 1956, Endre Rozsda returned to Paris to continue creating his own word that had been transcended the surrealism. The preface to his first exhibition in the Furstenberg Gallery (1957) was written by Breton yet.
Many new artists explicitly took up the Surrealist banner for themselves. Dorothea Tanning and Louise Bourgeois continued to work, for example, with Tanning's "Rainy Day Canape" from 1970. Duchamp continued to produce sculpture in secret including an installation with the realistic depiction of a woman viewable only through a peephole.
Breton continued to write and espouse the importance of liberating of the human mind, as with the publication "The Tower of Light" in 1952. Breton's return to France after the War, began a new phase of Surrealist activity in Paris, and his critiques of rationalism and dualism found a new audience. Breton insisted that Surrealism was an ongoing revolt against the reduction of humanity to market relationships, religious gestures and misery and to espouse the importance of liberating the human mind.
Major exhibitions of the 1940s, '50s and '60s
Post-Breton Surrealism.
There is no clear consensus about the end, or if there was an end, to the Surrealist movement. Some art historians suggest that World War II effectively disbanded the movement. However, art historian Sarane Alexandrian (1970) states, "the death of André Breton in 1966 marked the end of Surrealism as an organized movement." There have also been attempts to tie the obituary of the movement to the 1989 death of Salvador Dalí.
In the 1960s, the artists and writers grouped around the Situationist International were closely associated with Surrealism. While Guy Debord was critical of and distanced himself from Surrealism, others, such as Asger Jorn, were explicitly using Surrealist techniques and methods. The events of May 1968 in France included a number of Surrealist ideas, and among the slogans the students spray-painted on the walls of the Sorbonne were familiar Surrealist ones. Joan Miró would commemorate this in a painting titled "May 1968." There were also groups who associated with both currents and were more attached to Surrealism, such as the Revolutionary Surrealist Group.
In Europe and all over the world since the 1960s, artists have combined Surrealism with what is believed to be a classical 16th century technique called mischtechnik, a kind of mix of egg tempera and oil paint rediscovered by Ernst Fuchs, a contemporary of Dalí, and now practiced and taught by many followers, including Robert Venosa and Chris Mars. The former curator of the San Francisco Museum of Modern Art, Michael Bell, has called this style "veristic Surrealism", which depicts with meticulous clarity and great detail a world analogous to the dream world. Other tempera artists, such as Robert Vickrey, regularly depict Surreal imagery.
During the 1980s, behind the Iron Curtain, Surrealism again entered into politics with an underground artistic opposition movement known as the Orange Alternative. The Orange Alternative was created in 1981 by Waldemar Fydrych (alias 'Major'), a graduate of history and art history at the University of Wrocław. They used Surrealist symbolism and terminology in their large scale happenings organized in the major Polish cities during the Jaruzelski regime, and painted Surrealist graffiti on spots covering up anti-regime slogans. Major himself was the author of a "Manifesto of Socialist Surrealism". In this manifesto, he stated that the socialist (communist) system had become so Surrealistic that it could be seen as an expression of art itself.
Surrealistic art also remains popular with museum patrons. The Guggenheim Museum in New York City held an exhibit, "Two Private Eyes", in 1999, and in 2001 Tate Modern held an exhibition of Surrealist art that attracted over 170,000 visitors. In 2002 the Met in New York City held a show, "Desire Unbound", and the Centre Georges Pompidou in Paris a show called "La Révolution surréaliste".
Impact of Surrealism.
While Surrealism is typically associated with the arts, it has been said to transcend them; Surrealism has had an impact in many other fields. In this sense, Surrealism does not specifically refer only to self-identified "Surrealists", or those sanctioned by Breton, rather, it refers to a range of creative acts of revolt and efforts to liberate imagination. In addition to Surrealist ideas that are grounded in the ideas of Hegel, Marx and Freud, Surrealism is seen by its advocates as being inherently dynamic and as dialectical in its thought.
Other sources used by Surrealism epigons.
Surrealists have also drawn on sources as seemingly diverse as Clark Ashton Smith, Montague Summers, Horace Walpole, Fantomas, The Residents, Bugs Bunny, comic strips, the obscure poet Samuel Greenberg and the hobo writer and humourist T-Bone Slim. One might say that Surrealist strands may be found in movements such as Free Jazz (Don Cherry, Sun Ra, Cecil Taylor etc.) and even in the daily lives of people in confrontation with limiting social conditions. Thought of as the effort of humanity to liberate imagination as an act of insurrection against society, Surrealism finds precedents in the alchemists, possibly Dante, Hieronymus Bosch, Marquis de Sade, Charles Fourier, Comte de Lautreamont and Arthur Rimbaud.
1960s riots.
Surrealists believe that non-Western cultures also provide a continued source of inspiration for Surrealist activity because some may strike up a better balance between instrumental reason and imagination in flight than Western culture. Surrealism has had an identifiable impact on radical and revolutionary politics, both directly — as in some Surrealists joining or allying themselves with radical political groups, movements and parties — and indirectly — through the way in which Surrealists emphasize the intimate link between freeing imagination and the mind, and liberation from repressive and archaic social structures. This was especially visible in the New Left of the 1960s and 1970s and the French revolt of May 1968, whose slogan "All power to the imagination" rose directly from French Surrealist thought and practice.
Postmodernism and popular culture.
Many significant literary movements in the later half of the 20th century were directly or indirectly influenced by Surrealism. This period is known as the Postmodern era; though there's no widely agreed upon central definition of Postmodernism, many themes and techniques commonly identified as Postmodern are nearly identical to Surrealism.
Many writers from and associated with the Beat Generation were influenced greatly by Surrealists. Philip Lamantia and Ted Joans are often categorized as both Beat and Surrealist writers. Many other Beat writers show significant evidence of Surrealist influence. A few examples include Bob Kaufman, Gregory Corso, Allen Ginsberg, and Lawrence Ferlinghetti. Artaud in particular was very influential to many of the Beats, but especially Ginsberg and Carl Solomon. Ginsberg cites Artaud's "Van Gogh -- The Man Suicided by Society" as a direct influence on "Howl", along with Apollinaire's "Zone", García Lorca's "Ode to Walt Whitman", and Schwitters' "Priimiititiii". The structure of Breton's "Free Union" had a significant influence on Ginsberg's "Kaddish". In Paris, Ginsberg and Corso met their heroes Tristan Tzara, Marcel Duchamp, Man Ray, and Benjamin Péret, and to show their admiration Ginsberg kissed Duchamp's feet and Corso cut off Duchamp's tie.
William S. Burroughs, a core member of the Beat Generation and a very influential postmodern novelist, developed what he called the "cut-up technique" with former surrealist Brion Gysin—in which chance is used to dictate the composition of a text from words cut out of other sources—referring to it as the "Surrealist Lark" and recognizing its debt to the techniques of Tristan Tzara.
Postmodern novelist Thomas Pynchon, who was also influenced by Beat fiction, experimented since the 1960s with the surrealist idea of startling juxtapositions; commenting on the "necessity of managing this procedure with some degree of care and skill", he added that "any old combination of details will not do. Spike Jones, Jr., whose father's orchestral recordings had a deep and indelible effect on me as a child, said once in an interview, 'One of the things that people don't realize about Dad's kind of music is, when you replace a C-sharp with a gunshot, it has to be a C-sharp gunshot or it sounds awful.'"
Many other postmodern fiction writers have been directly influenced by Surrealism. Paul Auster, for example, has translated Surrealist poetry and said the Surrealists were "a real discovery" for him. Salman Rushdie, when called a Magical Realist, said he saw his work instead "allied to surrealism". For the work of other postmodernists, such as Donald Barthelme and Robert Coover, a broad comparison to Surrealism is common.
Magic realism, a popular technique among novelists of the latter half of the 20th century especially among Latin American writers, has some obvious similarities to Surrealism with its juxtaposition of the normal and the dream-like, as in the work of Gabriel García Márquez. Carlos Fuentes was inspired by the revolutionary voice in Surrealist poetry and points to inspiration Breton and Artaud found in Fuentes' homeland, Mexico. Though Surrealism was a direct influence on Magic Realism in its early stages, many Magic Realist writers and critics, such as Amaryll Chanady and S. P. Ganguly, while acknowledging the similarities, cite the many differences obscured by the direct comparison of Magic Realism and Surrealism such as an interest in psychology and the artefacts of European culture they claim is not present in Magic Realism. A prominent example of a Magic Realist writer who points to Surrealism as an early influence is Alejo Carpentier who also later criticized Surrealism's delineation between real and unreal as not representing the true South American experience.
Surrealist groups.
Surrealist individuals and groups have attempted to carry on with Surrealism after the death of André Breton in 1966. The original Paris Surrealist Group was disbanded by member Jean Schuster in 1969.
Surrealism and the theatre.
Surrealist theatre and Artaud's "Theatre of Cruelty" were inspirational to many within the group of playwrights that the critic Martin Esslin called the "Theatre of the Absurd" (in his 1963 book of the same name). Though not an organized movement, Esslin grouped these playwrights together based on some similarities of theme and technique; Esslin argues that these similarities may be traced to an influence from the Surrealists. Eugène Ionesco in particular was fond of Surrealism, claiming at one point that Breton was one of the most important thinkers in history. Samuel Beckett was also fond of Surrealists, even translating much of the poetry into English. Other notable playwrights whom Esslin groups under the term, for example Arthur Adamov and Fernando Arrabal, were at some point members of the Surrealist group.
Criticism of Surrealism.
Feminist.
Feminists have in the past critiqued Surrealism, claiming that it is fundamentally a male movement and a male fellowship. Feminist critics believe that it adopts archaic attitudes toward women, such as worshiping them symbolically through stereotypes and sexist norms. Women are often made to represent higher values and transformed into objects of desire and of mystery.
A pioneer in the feminist critique of Surrealism was Xavière Gauthier, whose book, "Surréalisme et sexualité" (1971), inspired further scholarship on the marginalization of women in relation to "the avant-garde." This perspective was anticipated and critiqued as misunderstanding Surrealism's point in being a social critique and a reflection on the individual's presuppositions so that they may be critically questioned. Wolfgang Paalen eventually was the only Surrealist to defend feminism, although in a very archaic sense. However it was Leonora Carrington, who called Paalen "the only feminist of the whole group".
Art historian Whitney Chadwick has countered the critique of Surrealism: "Surrealism also battled the social institutions - church, state, and family - that regulate the place of women within patriarchy. In offering some women their first locus for artistic and social resistance, it became the first modernist movement in which a group of women could explore female subjectivity and give form (however tentatively) to a feminine imaginary."
Freudian.
Freud initiated the psychoanalytic critique of Surrealism with his remark that what interested him most about the Surrealists was not their unconscious but their conscious. His meaning was that the manifestations of and experiments with psychic automatism highlighted by Surrealists as the liberation of the unconscious were highly structured by ego activity, similar to the activities of the dream censorship in dreams, and that therefore it was in principle a mistake to regard Surrealist poems and other art works as direct manifestations of the unconscious, when they were indeed highly shaped and processed by the ego. In this view, the Surrealists may have been producing great works, but they were products of the conscious, not the unconscious mind, and they deceived themselves with regard to what they were doing with the unconscious. In psychoanalysis proper, the unconscious does not just express itself automatically but can only be uncovered through the analysis of resistance and transference in the psychoanalytic process.
Bibliography.
André Breton
Other sources

</doc>
<doc id="28767" url="http://en.wikipedia.org/wiki?curid=28767" title="Statics">
Statics

Statics is the branch of mechanics that is concerned with the analysis of loads (force and torque, or "moment") on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity. When in static equilibrium, the system is either at rest, or its center of mass moves at constant velocity.
By Newton's first law, this situation implies that the net force and net torque (also known as moment of force) on every part of the system is zero. From this constraint, such quantities as stress or pressure can be derived. The net forces equaling zero is known as the "first condition for equilibrium," and the net torque equaling zero is known as the "second condition for equilibrium." See statically determinate.
Vectors.
A scalar is a quantity, such as mass or temperature, which only has a magnitude. A vector has a magnitude and a direction and obeys the parallelogram law. There are several notations to identify a vector, including:
Vectors are added using the parallelogram law or the triangle law. Vectors contain components in orthogonal bases. Unit vectors i, j, and k are, by convention, along the x, y, and z axes, respectively.
Force.
Force is the action of one body on another. A force is either a push or a pull. A force tends to move a body in the direction of its action. The action of a force is characterized by its magnitude, by the direction of its action, and by its point of application. Thus force is a vector quantity, because its effect depends on the direction as well as on the magnitude of the action.
Forces are classified as either contact or body forces. A contact force is produced by direct physical contact; an example is the force exerted on a body by a supporting surface. A body force is generated by virtue of the position of a body within a force field such as a gravitational, electric, or magnetic field. An example of a body force is the weight of a body in the Earth's gravitational pull.
Moment of a force.
In addition to the tendency to move a body in the direction of its application, a force can also tend to rotate a body about an axis. The axis may be any line which neither intersects nor is parallel to the line of action of the force. This rotational tendency is known as the "moment" (M) of the force. Moment is also referred to as "torque".
Moment about a point.
The magnitude of the moment of a force at a point O, is equal to the perpendicular distance from O to the line of action of F, multiplied by the magnitude of the force: M = F * d, where
F = the force applied
d = the perpendicular distance from the axis to the line of action of the force. This perpendicular distance is called the moment arm.
The direction of the moment is given by the right hand rule, where counter clockwise (CCW) is out of the page, and clockwise (CW) is into the page. The moment direction may be accounted for by using a stated sign convention, such as a plus sign (+) for counterclockwise moments and a minus sign (−) for clockwise moments, or vice versa. Moments can be added together as vectors.
In vector format, the moment can be defined as the cross product between the radius vector, r (the vector from point O to the line of action), and the force vector, F:
Varignon's theorem.
Varignon's theorem states that the moment of a force about any point is equal to the sum of the moments of the components of the force about the same point.
Equilibrium equations.
The static equilibrium of a particle is an important concept in statics. A particle is in equilibrium only if the resultant of all forces acting on the particle is equal to zero. In a rectangular coordinate system the equilibrium equations can be represented by three scalar equations, where the sums of forces in all three directions are equal to zero. An engineering application of this concept is determining the tensions of up to three cables under load, for example the forces exerted on each cable of a hoist lifting an object or of guy wires restraining a hot air balloon to the ground.
Moment of inertia.
In classical mechanics, moment of inertia, also called mass moment, rotational inertia, polar moment of inertia of mass, or the angular mass, (SI units kg·m²) is a measure of an object's resistance to changes to its rotation. It is the inertia of a rotating body with respect to its rotation. The moment of inertia plays much the same role in rotational dynamics as mass does in linear dynamics, describing the relationship between angular momentum and angular velocity, torque and angular acceleration, and several other quantities. The symbols I and J are usually used to refer to the moment of inertia or polar moment of inertia.
While a simple scalar treatment of the moment of inertia suffices for many situations, a more advanced tensor treatment allows the analysis of such complicated systems as spinning tops and gyroscopic motion.
The concept was introduced by Leonhard Euler in his 1765 book "Theoria motus corporum solidorum seu rigidorum"; he discussed the moment of inertia and many related concepts, such as the principal axis of inertia.
Solids.
Statics is used in the analysis of structures, for instance in architectural and structural engineering. Strength of materials is a related field of mechanics that relies heavily on the application of static equilibrium. A key concept is the center of gravity of a body at rest: it represents an imaginary point at which all the mass of a body resides. The position of the point relative to the foundations on which a body lies determines its stability in response to external forces. If the center of gravity exists outside the foundations, then the body is unstable because there is a torque acting: any small disturbance will cause the body to fall or topple. If the center of gravity exists within the foundations, the body is stable since no net torque acts on the body. If the center of gravity coincides with the foundations, then the body is said to be metastable.
Fluids.
Hydrostatics, also known as fluid statics, is the study of fluids at rest (i.e. in static equilibrium). The characteristic of any fluid at rest is that the force exerted on any particle of the fluid is the same at all points at the same depth (or altitude) within the fluid. If the net force is greater than zero the fluid will move in the direction of the resulting force. This concept was first formulated in a slightly extended form by French mathematician and philosopher Blaise Pascal in 1647 and became known as Pascal's Law. It has many important applications in hydraulics. Archimedes, Abū Rayhān al-Bīrūnī, Al-Khazini and Galileo Galilei were also major figures in the development of hydrostatics.

</doc>
<doc id="28768" url="http://en.wikipedia.org/wiki?curid=28768" title="Southern Cross (disambiguation)">
Southern Cross (disambiguation)

The Southern Cross or Crux is a constellation visible in the Southern Hemisphere.
Southern Cross may also refer to:

</doc>
<doc id="28769" url="http://en.wikipedia.org/wiki?curid=28769" title="Ship transport">
Ship transport

Ship transport is watercraft carrying people (passengers) or goods (cargo).
Sea transport has been the largest carrier of freight throughout recorded history. Although the importance of sea travel for passengers has decreased due to aviation, it is effective for short trips and pleasure cruises. Transport by water is cheaper than transport by air , despite fluctuating exchange rates and CAF charges to account for such.
Ship transport can be over any distance by boat, ship, sailboat or barge, over oceans and lakes, through canals or along rivers. Shipping may be for commerce, recreation or the military purpose. Virtually any material can be moved by water; however, water transport becomes impractical when material delivery is highly time-critical.
Containerization revolutionized ship transport starting in the 1970s. "General cargo" includes goods packaged in boxes, cases, pallets, and barrels. When a cargo is carried in more than one mode, it is intermodal or co-modal.
Merchant shipping.
A nation's shipping fleet (merchant navy, merchant marine, merchant fleet) consists of the ships operated by civilian crews to transport passengers or cargo from one place to another. Professionals are merchant seaman, merchant sailor, and merchant mariner, or simply seaman, sailor, or mariner. The terms "seaman" or "sailor" may refer to a member of a country's navy.
According to the 2005 CIA World Factbook, the total number of merchant ships of at least 1,000 gross register tons in the world was 30,936. In 2010, it was 38,988, an increase of 26%. Statistics for individual countries are available at the list of merchant navy capacity by country.
Professional mariners.
A ship's complement can be divided into four categories: the deck department, the engineering department, the steward's department, and other.
Deck department.
 Officer positions in the deck department include but not limited to: Master and his Chief, Second, and Third officers. The official classifications for unlicensed members of the deck department are Able Seaman and Ordinary Seaman.
A common deck crew for a ship includes:
A deck cadet is person who is carrying out mandatory seatime to achieve their officer of the watch certificate. Their time onboard is spent learning the operations and tasks of everyday life on a merchant vessel.
Engineering department.
A ship's engineering department consists of the members of a ship's crew that operate and maintain the propulsion and other systems on board the vessel. Marine Engineering staff also deal with the "Hotel" facilities on board, notably the sewage, lighting, air conditioning and water systems. They deal with bulk fuel transfers, and require training in firefighting and first aid, as well as in dealing with the ship's boats and other nautical tasks- especially with cargo loading/discharging gear and safety systems, though the specific cargo discharge function remains the responsibility of deck officers and deck workers. On LPG and LNG tankers however, a cargo engineer works with the deck department during cargo operations, as well as being a watchkeeping engineer.
A common Engineering crew for a ship includes:
Many American ships also carry a Qualified Member of the Engine Department. Other possible positions include Motorman, Machinist, Electrician, Refrigeration Engineer, and Tankerman. Engine Cadets are trainee engineers who are completing sea time necessary before they can obtain a watchkeeping license.
Steward's department.
A typical Steward's department for a cargo ship would be composed of a Chief Steward, a Chief Cook, and a Steward's Assistant. All three positions are typically filled by unlicensed personnel.
The chief steward directs, instructs, and assigns personnel performing such functions as preparing and serving meals; cleaning and maintaining officers' quarters and steward department areas; and receiving, issuing, and inventorying stores.
On large passenger vessels, the Catering Department is headed by the Chief Purser and managed by assistant pursers. Although they enjoy the benefits of having officer rank, they generally progress through the ranks to become pursers. Under the pursers are the department heads - such as chief cook, head waiter, head barman etc. They are responsible for the administration of their own areas.
The chief steward also plans menus; compiles supply, overtime, and cost control records. May requisition or purchase stores and equipment. They may bake bread, rolls, cakes, pies, and pastries.
A chief steward's duties may overlap with those of the Steward's Assistant, the Chief Cook, and other Steward's Department crewmembers.
In the United States Merchant Marine, in order to be occupied as a chief steward a person has to have a Merchant Mariner's Document issued by the United States Coast Guard. Because of international conventions and agreements, all chief cooks who sail internationally are similarly documented by their respective countries.
Other Departments.
Staff officer positions on a ship, including Junior Assistant Purser, Senior Assistant Purser, Purser, Chief Purser, Medical Doctor, Professional Nurse, Marine Physician Assistant, and Hospital Corpsman, are considered administrative positions and are therefore regulated by Certificates of Registry issued by the United States Coast Guard. Pilots are also merchant marine officers and are licensed by the Coast Guard. Formerly, there was also a radio department, headed by a chief radio officer and supported by a number of radio officers. Since the introduction of GMDSS (Satellite communications) and the subsequent exemptions from carrying radio officers if the vessel is so equipped, this department has fallen away, although many ships do still carry specialist radio officers, particularly passenger vessels. Many radio officers became 'electro-technical officers', and transferred into the engineering department.
Life at sea.
Mariners spend much of their life beyond the reach of land. They sometime face dangerous conditions at sea. Yet men and women still go to sea. For some, the attraction is a life unencumbered with the restraints of life ashore. Seagoing adventure and a chance to see the world also appeal to many seafarers. Whatever the calling, those who live and work at sea invariably confront social isolation.
Findings by the Seafarer's International Research Center indicate a leading cause of mariners leaving the industry is "almost invariably because they want to be with their families." U.S. merchant ships typically do not allow family members to accompany seafarers on voyages. Industry experts increasingly recognize isolation, stress, and fatigue as occupational hazards. Advocacy groups such as International Labour Organization, a United Nations agency, and the Nautical Institute are seeking improved international standards for mariners. Satellite phones have improved communication and efficiency aboard sea-fairing ships. This technology has contributed to crew welfare, although both equipment and fees are expensive.
Ocean voyages are steeped in routine. Maritime tradition dictates that each day be divided into six four-hour periods. Three groups of watch keepers from the engine and deck departments work four hours on then have eight hours off watch keeping. However there are many overtime jobs to be done daily. This cycle repeats endlessly, 24 hours a day while the ship is at sea. Members of the steward department typically are day workers who put in at least eight-hour shifts. Operations at sea, including repairs, safeguarding against piracy, securing cargo, underway replenishment, and other duties provide opportunities for overtime work. Service aboard ships typically extends for months at a time, followed by protracted shore leave. However, some seamen secure jobs on ships they like and stay aboard for years.
The quick turnaround of many modern ships, spending only a few hours in port, limits a seafarer's free-time ashore. Moreover, some foreign seamen entering U.S. ports from a watch list of 25 countries face restrictions on shore leave due to security concerns. However, shore leave restrictions while in U.S. ports impact American seamen as well. For example, the International Organization of Masters, Mates & Pilots notes a trend of U.S. shipping terminal operators restricting seamen from traveling from the ship to the terminal gate. Furthermore, in cases where transit is allowed, special "security fees" are at times assessed.
Such restrictions on shore leave, coupled with reduced time in port, translate into longer periods at sea. Mariners report that extended periods at sea living and working with shipmates, who for the most part are strangers, takes getting used to. At the same time, there is an opportunity to meet people from other ethnic and cultural backgrounds. Recreational opportunities have improved aboard some U.S. ships, which may feature gyms and day rooms for watching movies, swapping sea stories, and other activities. And in some cases, especially tankers, it is possible for a mariner to be accompanied by members of his family. However, a mariner’s off-duty time is largely a solitary affair, pursuing hobbies, reading, writing letters, and sleeping.
On modern ocean-going vessels, typically registered with a flag of convenience, life has changed immensely in the last 20 years. Most large vessels include a gym and often a swimming pool for use by the crew. Since the "Exxon Valdez" incident, the focus of leisure time activity has shifted from having officer and crew bars, to simply having lounge-style areas where officers or crew can sit to watch movies. With many companies now providing TVs and DVD players in cabins, and enforcing strict smoking policies, it is not surprising that the bar is now a much quieter place on most ships. In some instances games consoles are provided for the officers and crew. The officers enjoy a much higher standard of living on board ocean-going vessels.
Crews are generally poorly paid, poorly qualified and have to complete contracts of approximately 9 months before returning home on leave. They often come from countries where the average industrial wage is still very low, such as the Philippines or India. Officers however, come from all over the world and it is not uncommon to mix the nationality of the officers on board ships. Officers are often the recipients of university degrees and have completed vast amounts of training in order to reach their rank. Officers benefit e.g. by having larger, more comfortable cabins and table service for their meals.
Contracts average at the 4 month mark for officers, with generous leave. Most ocean-going vessels now operate an unmanned engine room system allowing engineers to work days only. The engine room is computer controlled by night, although the duty engineer will make inspections during unmanned operation. Engineers work in a hot, humid, noisy atmosphere. Communication in the engine room is therefore by hand signals and lip-reading, and good teamwork often stands in place of any communication at all.
Ships and watercraft.
Ships and other watercraft are used for ship transport. Types can be distinguished by propulsion, size or cargo type. Recreational or educational craft still use wind power, while some smaller craft use internal combustion engines to drive one or more propellers, or in the case of jet boats, an inboard water jet. In shallow draft areas, such as the Everglades, some craft, such as the hovercraft, are propelled by large pusher-prop fans.
Most modern merchant ships can be placed in one of a few categories, such as:
Ships that fall outside these categories include Semi-submersible heavy-lift ships or OHGC.
Liners and Tramps.
A ship may also be categorised as to how it is operated.
A liner will have a regular run and operate to a schedule. The scheduled operation requires that such ships are better equipped to deal with causes of potential delay such as bad weather. They are generally higher powered than tramp ships with better seakeeping qualities, thus they are significantly more expensive to build. Liners are typically built for passenger and container operation though past common uses also included mail and general cargo.
A tramp has no fixed run but will go wherever a suitable cargo takes it. Thus a ship and crew may be chartered from the ship owner to fetch a cargo of grain from Canada to Latvia, the ship may then be required to carry a cargo of coal from Britain to Melanesia. Bulk carriers and cruise ships are examples of ships built to operate in this manner.
Typical in-transit times.
A cargo ship sailing from a European port to a US one will typically take 10–12 days depending on water currents and other factors. In order to make container ship transport more economical in the face of declining demand for intercontinental shipping, ship operators sometimes reduce cruising speed, thereby increasing transit time, to reduce fuel consumption, a strategy referred to as "slow steaming".
Ship transport infrastructure.
For a port to efficiently send and receive cargo, it requires infrastructure. Harbors, seaports and marinas host watercraft, and consist of components such as piers, wharfs, docks and roadsteads.

</doc>
<doc id="28771" url="http://en.wikipedia.org/wiki?curid=28771" title="St. John's, Antigua and Barbuda">
St. John's, Antigua and Barbuda

St. John's is the capital and largest city of Antigua and Barbuda, a country located in the West Indies in the Caribbean Sea. With a population of 81,799, St. John's is the commercial centre of the nation and the chief port of the island of Antigua.
History.
The settlement of St. John's has been the administrative centre of Antigua and Barbuda since the islands were first colonised in 1632, and it became the seat of government when the nation achieved independence in 1981. Saint John is also the capital of Antigua.
Economy.
St. John's is one of the most developed and cosmopolitan municipalities in the Lesser Antilles. The city is famous for its various shopping malls as well as boutiques throughout the city, selling designer jewellery and haute-couture clothing. There are also many independent, locally run establishments, selling a variety of fashions.
St. John's attracts tourists from the many exclusive resorts on the island and from the cruise ships which dock in its harbour at Heritage Quay and Redcliffe Quay several times a week.
The investment banking industry has a strong presence in the city. Many major world financial institutions have offices in St. John's.
There is a market on the southwestern edge of the city where fresh produce, meats, and fresh fish are sold daily.
The Antigua Rum Distillery is located at the Citadel and is the only rum distillery on the island. Annual production yields more than 180,000 gallons bottled.
Demographics.
The majority of the population of St. John's reflects that of the rest of Antigua: people of African and mixed European-African ancestry, with a European minority, including British and Portuguese. There is also a population of Levantine Christian Arabs.
Government.
The Eastern Caribbean Civil Aviation Authority has its headquarters on Factory Road in St. John's.
Culture.
Several museums, including the Museum of Antigua and Barbuda and the Museum of Marine Art, a small facility containing fossilised bedrock, volcanic stones, petrified wood, a collection of more than 10,000 shells, and artefacts from several English shipwrecks.
Just east of St. John's is the Sir Vivian Richards Stadium, a multi-use stadium in North Sound, that was created mostly for cricket matches, and has hosted the matches during the 2007 Cricket World Cup. The Antigua Recreation Ground, Antigua and Barbuda's national stadium, is located in St. John's.
Geography.
Nearby villages and settlements include St. Johnston.
Main sights.
The city's skyline is dominated by the white baroque towers of St. John's Cathedral.
The Botanical Garden is near the intersection of Factory Road and Independence Avenue. This small park's shaded benches and gazebo provide a quiet refuge from the bustle of activity of St. John's.
St. John's Antigua Light is a lighthouse located in the city's harbour ().
Fort James stands at the entrance to St. John's harbour. Other nearby forts include Fort George, Fort Charles, Fort Shirley, Fort Berkeley and Fort Barrington.
Transportation.
St. John's is served by the V. C. Bird International Airport.
Education.
St. John's is home to two medical schools called American University of Antigua and University of Health Sciences Antigua. Secondary schools include Christ the King High School, Princess Margaret School and the Antigua Girls High School.

</doc>
<doc id="28779" url="http://en.wikipedia.org/wiki?curid=28779" title="Sigtuna Municipality">
Sigtuna Municipality

Sigtuna Municipality ("Sigtuna kommun") is a municipality in Stockholm County in east central Sweden. Its seat is located in the town of Märsta, approximately 37 km north of the Swedish capital, Stockholm.
The municipality is a part of Metropolitan Stockholm.
The municipality consists of several former local government units and was formed in 1971. It got its name from the small, but very old, "City of Sigtuna", but the seat was placed in the larger modern town of Märsta.
The three towns of the municipality are Märsta (pop. 23,000), Sigtuna (pop. 8,000) and Rosersberg (pop. 1,400), of which Märsta is the municipal seat and Sigtuna with its old and important history is a popular tourist destination.
Industry.
In the municipality lies the largest workplace in Sweden, the Arlanda Airport, with 13,000 employees in 200 companies. As a result, Siguna is travelled through by 18,300,000 visitors yearly, and has the fourth most hotel stays, following to the commercial and regional centres Stockholm, Gothenburg and Malmö.
Scandinavian Airlines has its head office on the airport property. Swedavia, the Swedish airport management company, has its head office on the airport property. 
International relations.
Twin towns — Sister cities.
The municipality is twinned with:

</doc>
<doc id="28782" url="http://en.wikipedia.org/wiki?curid=28782" title="Self-similarity">
Self-similarity

In mathematics, a self-similar object is exactly or approximately similar to a part of itself (i.e. the whole has the same shape as one or more of the parts). Many objects in the real world, such as coastlines, are statistically self-similar: parts of them show the same statistical properties at many scales. Self-similarity is a typical property of fractals. Scale invariance is an exact form of self-similarity where at any magnification there is a smaller piece of the object that is similar to the whole. For instance, a side of the Koch snowflake is both symmetrical and scale-invariant; it can be continually magnified 3x without changing shape. The non-trivial similarity evident in fractals is distinguished by their fine structure, or detail on arbitrarily small scales. As a counterexample, whereas any portion of a straight line may resemble the whole, further detail is not revealed.
Definition.
A compact topological space "X" is self-similar if there exists a finite set "S" indexing a set of non-surjective homeomorphisms formula_1 for which
If formula_3, we call "X" self-similar if it is the only non-empty subset of "Y" such that the equation above holds for formula_4. We call
a "self-similar structure". The homeomorphisms may be iterated, resulting in an iterated function system. The composition of functions creates the algebraic structure of a monoid. When the set "S" has only two elements, the monoid is known as the dyadic monoid. The dyadic monoid can be visualized as an infinite binary tree; more generally, if the set "S" has "p" elements, then the monoid may be represented as a p-adic tree.
The automorphisms of the dyadic monoid is the modular group; the automorphisms can be pictured as hyperbolic rotations of the binary tree.
A more general notion than self-similarity is Self-affinity.
Examples.
The Mandelbrot set is also self-similar around Misiurewicz points.
Self-similarity has important consequences for the design of computer networks, as typical network traffic has self-similar properties. For example, in teletraffic engineering, packet switched data traffic patterns seem to be statistically self-similar. This property means that simple models using a Poisson distribution are inaccurate, and networks designed without taking self-similarity into account are likely to function in unexpected ways.
Similarly, stock market movements are described as displaying self-affinity, i.e. they appear self-similar when transformed via an appropriate affine transformation for the level of detail being shown. Andrew Lo describes stock market log return self-similarity in econometrics.
Finite subdivision rules are a powerful technique for building self-similar sets, including the Cantor set and the Sierpinski triangle.
In nature.
Self-similarity can be found in nature, as well. To the right is a mathematically generated, perfectly self-similar image of a fern, which bears a marked resemblance to natural ferns. Other plants, such as Romanesco broccoli, exhibit strong self-similarity.

</doc>
<doc id="28785" url="http://en.wikipedia.org/wiki?curid=28785" title="Small beer">
Small beer

Small beer (also, small ale) is a beer/ale that contains very little alcohol. Sometimes unfiltered and porridge-like, it was a favored drink in Medieval Europe and colonial North America to the expensive beer used for festivities. Small beer was also produced in households for consumption by children and servants at those occasions.
Main.
Before public sanitation, cholera and other water-transmitted diseases were a significant cause of death. Because the process of brewing any beer from malt involves boiling the water, drinking small beer instead of water was one way to escape infection.
It was not uncommon for workers (including sailors) who engaged in heavy physical labor to drink more than 10 Imperial pints (5.7 liters) of small beer during a workday to slake their thirst.
Small beer/small ale can also refer to a beer made of the "second runnings" from a very strong beer (e.g., scotch ale) mash. These beers can be as strong as a mild ale, depending on the strength of the original mash. (Drake's 24th Anniversary Imperial Small Beer was expected to reach above 9.5% abv.) This was done as an economy measure in household brewing in England up to the 18th century and is still done by some homebrewers. One commercial brewery, San Francisco's Anchor Brewing Company, also produces their "Anchor Small Beer" using the second runnings from their Old Foghorn Barleywine. The term is also used for commercially produced beers which are thought to taste too weak.
In art and history.
Literature.
Metaphorically, "small beer" means a trifle, or a thing of little importance.

</doc>
<doc id="28787" url="http://en.wikipedia.org/wiki?curid=28787" title="Sambia Peninsula">
Sambia Peninsula

Sambia (Russian: Земландский полуо́стров, "Zemlandsky poluostrov", literally the Zemlandsky Peninsula) or Samland is a peninsula in the Kaliningrad Oblast of Russia, on the southeastern shore of the Baltic Sea. The peninsula is bounded by the Curonian Lagoon (to the north-east, the Vistula Lagoon (on the southwest), the Pregel River (on the south), and the Deyma River (on the east). As Samland is surrounded on all sides by water, it is technically an island. Prior to 1945 it formed an important part of East Prussia.
Names.
Sambia is named after the Sambians, an extinct tribe of Old Prussians. "Samland" is the name for peninsula in the Germanic languages. Polish and Latin speakers call the area "Sambia", while the Lithuanian name is "Semba".
History.
Sambia was originally sparsely populated by the Sambians. The German Teutonic Knights conquered the region during the 13th century and the Bishopric of Samland became, along with Bishopric of Pomesania, Bishopric of Ermland, and Bishopric of Culm, one of the four dioceses of Prussia in 1243. Settlers from the Holy Roman Empire began colonizing the region, and the Sambian Prussians gradually became assimilated. The peninsula was the last area in which the Old Prussian language was spoken before becoming extinct at the beginning of the 18th century.
The peninsula became part of the Duchy of Prussia when Albert of Brandenburg-Ansbach, the 37th Grand Master, secularized the Monastic State of the Teutonic Knights in 1525. The Margraviate of Brandenburg inherited the duchy in 1618, and its Hohenzollern ruler proclaimed the Kingdom of Prussia in 1701. Sambia became part of the Province of East Prussia in 1773. (The Kingdom of Prussia completed the unification of Germany by setting up the German Empire in 1871.) After World War I Sambia formed part of the East Prussian province of Weimar Germany.
In 1945 after World War II, the Soviet Union annexed northern East Prussia, including Sambia (German: Samland), while southern East Prussia was given to Poland. Sambia became part of the Soviet Kaliningrad Oblast, named after the nearby city of Kaliningrad (historically German: "Königsberg"), and the new authorities expelled its German inhabitants.
The Soviet Union gradually repopulated the Kaliningrad Oblast, including Sambia, with Russians and Belarusians. Until the dissolution of the Soviet Union in 1991, much of the district was a closed military area.
Kursenieki.
While today the Kursenieki, also known as Kuršininkai are a nearly extinct Baltic ethnic group living along the Curonian Spit, in 1649 Kuršininkai settlement spanned from Memel (Klaipėda) to Danzig (Gdańsk), including the coastline of the Sambian Peninsula. The Kuršininkai were eventually assimilated by the Germans, except along the Curonian Spit where some still live. The Kuršininkai were considered Latvians until after World War I when Latvia gained independence from the Russian Empire, a consideration based on linguistic arguments. This was the rationale for Latvian claims over the Curonian Spit, Memel, and other territories of East Prussia which would be later dropped.
Geography and geology.
Baedeker describes Samland as "a fertile and partly-wooded district, with several lakes, lying to the north of Königsberg" (since 1946 Kaliningrad). The highest point, 360 feet, is found twelve miles north of Pereslavskoe ("Drugehnen") at the ski resort then called the Galtgarben. There also used to be a Samland railway station. s of 2010[ [update]] the Pereslavskoe railway station serves the "Blue Arrow" railway line from Kaliningrad to Svetlogorsk.
Sambia includes two famous seaside resorts, Zelenogradsk (former German name: Cranz) and Svetlogorsk (former German name: Rauschen).
Amber.
Amber has been found in the area for over two thousand years, especially on the coast near Kaliningrad. History and legends tell of the ancient trade routes known as the Amber Road leading from the Old Prussian settlements of Kaup (in Sambia) and Truso (near Elbląg - German: Elbing, near the mouth of the Vistula) southwards to the Black and Adriatic seas. In Imperial Germany, the right to collect amber was restricted to the Hohenzollern dynasty, and visitors to Samland's beaches were forbidden to pick up any fragments they found. Beginning in the 19th century, amber was mined on an industrial scale by the Germans before 1945 and by the Soviets / Russians thereafter at Yantarny (former German name: Palmnicken).

</doc>
<doc id="28791" url="http://en.wikipedia.org/wiki?curid=28791" title="Sovereignty">
Sovereignty

Sovereignty is understood in jurisprudence as the full right and power of a governing body to govern itself without any interference from outside sources or bodies. In political theory, sovereignty is a substantive term designating supreme authority over some polity. It is a basic principle underlying the dominant Westphalian model of state foundation.
Derived from Latin through French "souveraineté", its attainment and retention, in both Chinese and Western culture, has traditionally been associated with certain moral imperatives upon any claimant.
Different approaches.
The concept of sovereignty has been discussed throughout history, from the time before recorded history through to the present day. It has changed in its definition, concept, and application throughout, especially during the Age of Enlightenment. The current notion of state sovereignty contains four aspects consisting of territory, population, authority and recognition. According to Stephen D. Krasner, the term could also be understood in four different ways: 
Often, these four aspects all appear together, but this is not necessarily the case – they are not affected by one another, and there are historical examples of states that were non-sovereign in one aspect while at the same time being sovereign in another of these aspects. According to Immanuel Wallerstein, another fundamental feature of sovereignty is that it is a claim that must be recognised by others if it is to have any meaning: "Sovereignty is more than anything else a matter of legitimacy [...that] requires reciprocal recognition. Sovereignty is a hypothetical trade, in which two potentially conflicting sides, respecting de facto realities of power, exchange such recognitions as their least costly strategy."
History.
Classical.
The Roman jurist Ulpian observed that:
Ulpian was expressing the idea that the Emperor exercised a rather absolute form of sovereignty, although he did not use the term expressly.
Medieval.
Classical Ulpian's statements were known in medieval Europe, but sovereignty was an important concept in medieval times. Medieval monarchs were "not" sovereign, at least not strongly so, because they were constrained by, and shared power with, their feudal aristocracy. Furthermore, both were strongly constrained by custom.
Sovereignty existed during the Medieval Period as the "de jure" rights of nobility and royalty, and in the "de facto" capability of individuals to make their own choices in life.
Around c. 1380–1400, the issue of feminine sovereignty was addressed in Geoffrey Chaucer's Middle English collection of "Canterbury Tales", specifically in "The Wife of Bath's Tale."
A later English Arthurian romance, "The Wedding of Sir Gawain and Dame Ragnell" (c. 1450), uses much of the same elements of the Wife of Bath's tale, yet changes the setting to the court of King Arthur and the Knights of the Round Table. The story revolves around the knight Sir Gawain granting to Dame Ragnell, his new bride, what is purported to be wanted most by women: sovereignty.
We desire most from men,
From men both lund and poor,
To have sovereignty without lies.
For where we have sovereignty, all is ours,
Though a knight be ever so fierce,
And ever win mastery.
It is our desire to have master
Over such a sir.
Such is our purpose.—The Wedding of Sir Gawain and Dame Ragnell (c. 1450), 
Reformation.
Sovereignty reemerged as a concept in the late 16th century, a time when civil wars had created a craving for stronger central authority, when monarchs had begun to gather power onto their own hands at the expense of the nobility, and the modern nation state was emerging. Jean Bodin, partly in reaction to the chaos of the French wars of religion, presented theories of sovereignty calling for strong central authority in the form of absolute monarchy. In his 1576 treatise "" ("Six Books of the Republic") Bodin argued that it is inherent in the nature of the state that sovereignty must be:
Bodin rejected the notion of transference of sovereignty from people to sovereign; natural law and divine law confer upon the sovereign the right to rule. And the sovereign is not above divine law or natural law. He is above ("ie." not bound by) only positive law, that is, laws made by humans. The fact that the sovereign must obey divine and natural law imposes ethical constraints on him. Bodin also held that the "lois royales", the fundamental laws of the French monarchy which regulated matters such as succession, are natural laws and are binding on the French sovereign.
How divine and natural law could in practice be enforced on the sovereign is a problematic feature of Bodin's philosophy: any person capable of enforcing them on him would be above him.
Despite his commitment to absolutism, Bodin held some moderate opinions on how government should in practice be carried out. He held that although the sovereign is not obliged to, it is advisable for him, as a practical expedient, to convene a senate from whom he can obtain advice, to delegate some power to magistrates for the practical administration of the law, and to use the Estates as a means of communicating with the people.
With his doctrine that sovereignty is conferred by divine law, Bodin predefined the scope of the divine right of kings.
Age of Enlightenment.
During the Age of Enlightenment, the idea of sovereignty gained both legal and moral force as the main Western description of the meaning and power of a State. In particular, the "Social Contract" as a mechanism for establishing sovereignty was suggested and, by 1800, widely accepted, especially in the new United States and France, though also in Great Britain to a lesser extent.
Thomas Hobbes, in "Leviathan" (1651) borrowed Bodin's definition of sovereignty, which had just achieved legal status in the "Peace of Westphalia", and explained its origin. He created the first modern version of the social contract (or contractarian) theory, arguing that to overcome the "nasty, brutish and short" quality of life without the cooperation of other human beings, people must join in a "commonwealth" and submit to a "Soveraigne ["sic"] Power" that is able to compel them to act in the common good. This expediency argument attracted many of the early proponents of sovereignty. Hobbes strengthened the definition of sovereignty beyond either Westphalian or Bodin's, by saying that it must be:
Hobbes' hypothesis—that the ruler's sovereignty is contracted to him by the people in return for his maintaining their physical safety—led him to conclude that if and when the ruler fails, the people recover their ability to protect themselves by forming a new contract.
Hobbes's theories decisively shape the concept of sovereignty through the medium of social contract theories. Jean-Jacques Rousseau's (1712–1778) definition of popular sovereignty (with early antecedents in Francisco Suárez's theory of the origin of power), provides that the people are the legitimate sovereign. Rousseau considered sovereignty to be inalienable; he condemned the distinction between the origin and the exercise of sovereignty, a distinction upon which constitutional monarchy or representative democracy is founded. John Locke, and Montesquieu are also key figures in the unfolding of the concept of sovereignty; their views differ with Rousseau and with Hobbes on this issue of alienability.
The second book of Jean-Jacques Rousseau's "Du Contrat Social, ou Principes du droit politique" (1762) deals with sovereignty and its rights. Sovereignty, or the general will, is inalienable, for the will cannot be transmitted; it is indivisible, since it is essentially general; it is infallible and always right, determined and limited in its power by the common interest; it acts through laws. Law is the decision of the general will in regard to some object of common interest, but though the general will is always right and desires only good, its judgment is not always enlightened, and consequently does not always see wherein the common good lies; hence the necessity of the legislator. But the legislator has, of himself, no authority; he is only a guide who drafts and proposes laws, but the people alone (that is, the sovereign or general will) has authority to make and impose them.
Rousseau, in his 1763 treatise "Of the Social Contract"
argued, "the growth of the State giving the trustees of public authority more and means to abuse their power, the more the Government has to have force to contain the people, the more force the Sovereign should have in turn in order to contain the Government," with the understanding that the Sovereign is "a collective being of wonder" (Book II, Chapter I) resulting from "the general will" of the people, and that "what any man, whoever he may be, orders on his own, is not a law" (Book II, Chapter VI) – and furthermore predicated on the assumption that the people have an unbiased means by which to ascertain the general will. Thus the legal maxim, "there is no law without a sovereign."
Definition and types.
There exists perhaps no conception the meaning of which is more controversial than that of sovereignty. It is an indisputable fact that this conception, from the moment when it was introduced into political science until the present day, has never had a meaning which was universally agreed upon. 
Lassa Oppenheim (30-03-1858 – 07-10-1919) 
Absoluteness.
An important factor of sovereignty is its degree of absoluteness. A sovereign power has absolute sovereignty when it is not restricted by a constitution, by the laws of its predecessors, or by custom, and no areas of law or policy are reserved as being outside its control. International law; policies and actions of neighboring states; cooperation and respect of the populace; means of enforcement; and resources to enact policy are factors that might limit sovereignty. For example, parents are not guaranteed the right to decide some matters in the upbringing of their children independent of societal regulation, and municipalities do not have unlimited jurisdiction in local matters, thus neither parents nor municipalities have absolute sovereignty. Theorists have diverged over the desirability of increased absoluteness.
Exclusivity.
A key element of sovereignty in a legalistic sense is that of exclusivity of jurisdiction. Specifically, the degree to which decisions made by a sovereign entity might be contradicted by another authority. Along these lines, the German sociologist Max Weber proposed that sovereignty is a community's monopoly on the legitimate use of force; and thus any group claiming the same right must either be brought under the yoke of the sovereign, proven illegitimate, or otherwise contested and defeated for sovereignty to be genuine. International law, competing branches of government, and authorities reserved for subordinate entities (such as federated states or republics) represent legal infringements on exclusivity. Social institutions such as religious bodies, corporations, and competing political parties might represent de facto infringements on exclusivity.
De jure and de facto.
De jure, or legal, sovereignty concerns the expressed and institutionally recognised right to exercise control over a territory. De facto, or actual, sovereignty is concerned with whether control in fact exists. Cooperation and respect of the populace; control of resources in, or moved into, an area; means of enforcement and security; and ability to carry out various functions of state all represent measures of de facto sovereignty. When control is practiced predominately by military or police force it is considered "coercive sovereignty".
Sovereignty and independence.
State sovereignty is sometimes viewed synonymously with independence, however, sovereignty can be transferred as a legal right whereas independence cannot. A state can achieve de facto independence long after acquiring sovereignty, such as in the case of Cambodia, Laos and Vietnam. Additionally, independence can also be suspended when an entire region becomes subject to an occupation such as when Iraq had been overrun by the forces to take part in the Iraq War of 2003, Iraq had not been annexed by any country, so its sovereignty during this period was not contested by any state including those present on the territory. Alternatively, independence can be lost completely when sovereignty itself becomes the subject of dispute. The pre-World War II administrations of Latvia, Lithuania and Estonia maintained an exile existence (and considerable international recognition) whilst the entities were annexed by the Soviet Union and governed locally by their pro-Soviet functionaries. When in 1991 Latvia, Lithuania and Estonia re-enacted independence, it was done so on the basis of continuity directly from the pre-Soviet republics. Another complicated sovereignty scenario can arise when regime itself is the subject of dispute. In the case of Poland, the People's Republic of Poland which governed Poland from 1945 to 1989 is now seen to have been an illegal entity by the modern Polish administration. The post-1989 Polish state claims direct continuity from the Second Polish Republic which ended in 1939. For other reasons however, Poland maintains its communist-era outline as opposed to its pre-World War II shape which included areas now in Belarus, Czech Republic, Lithuania, Slovakia and Ukraine but did not include some of its western regions that were then in Germany.
At the opposite end of the scale, there is no dispute regarding the independence of Republic of Abkhazia, Republic of South Ossetia or the Republic of Kosovo (see List of states with limited recognition) since the factually governing bodies are neither part of a bigger state, nor are they subjected to supervision. The sovereignty (i.e. legal right to govern) however, is disputed in all three cases as the first two entities are claimed by Georgia and the third by Serbia.
Internal.
Internal sovereignty is the relationship between a sovereign power and its own subjects. A central concern is legitimacy: by what right does a government exercise authority? Claims of legitimacy might refer to the divine right of kings or to a social contract (i.e. popular sovereignty).
With Sovereignty meaning holding supreme, independent authority over a region or state, Internal Sovereignty refers to the internal affairs of the state and the location of supreme power within it. A state that has internal sovereignty is one with a government that has been elected by the people and has the popular legitimacy. Internal sovereignty examines the internal affairs of a state and how it operates. It is important to have strong internal sovereignty in relation to keeping order and peace. When you have weak internal sovereignty organization such as rebel groups will undermine the authority and disrupt the peace. The presence of a strong authority allows you to keep agreement and enforce sanctions for the violation of laws. The ability for leadership to prevent these violations is a key variable in determining internal sovereignty. The lack of internal sovereignty can cause war in one of two ways: first, undermining the value of agreement by allowing costly violations; and second, requiring such large subsidies for implementation that they render war cheaper than peace. Leadership needs to be able to promise members, especially those like armies, police forces, or paramilitaries will abide by agreements. The presence of strong internal sovereignty allows a state to deter opposition groups in exchange for bargaining. It has been said that a more decentralized authority would be more efficient in keeping peace because the deal must please not only the leadership but also the opposition group. While the operations and affairs within a state are relative to the level of sovereignty within that state, there is still an argument between who should hold the authority in a sovereign state.
This argument between who should hold the authority within a sovereign state is called the traditional doctrine of public sovereignty. This discussion is between an internal sovereign or an authority of public sovereignty. An internal sovereign is a political body that possesses ultimate, final and independent authority; one whose decisions are binding upon all citizens, groups and institutions in society. Early thinkers believe sovereignty should be vested in the hands of a single person, a monarch. They believed the overriding merit of vesting sovereignty in a single individual was that sovereignty would therefore be indivisible; it would be expressed in a single voice that could claim final authority. An example of an internal sovereign or monarch is Louis XIV of France during the seventeenth century; Louis XIV claimed that he was the state. Jean-Jacques Rousseau rejected monarchical rule in favor of the other type of authority within a sovereign state, public sovereignty. Public Sovereignty is the belief that ultimate authority is vested in the people themselves, expressed in the idea of the general will. This means that the power is elected and supported by its members, the authority has a central goal of the good of the people in mind. The idea of public sovereignty has often been the basis for modern democratic theory.
Modern internal sovereignty.
Within the modern governmental system, internal sovereignty is usually found in states that have public sovereignty and rarely found within a state controlled by an internal sovereign. A form of government that is a little different from both is the UK parliament system. From 1790 to 1859 it was argued that sovereignty in the UK was vested neither in the Crown nor in the people but in the "Monarch in Parliament". This is the origin of the doctrine of parliamentary sovereignty and is usually seen as the fundamental principle of the British constitution. With these principles of parliamentary sovereignty majority control can gain access to unlimited constitutional authority, creating what has been called "elective dictatorship" or "modern autocracy". Public sovereignty in modern governments is a lot more common with examples like the USA, Canada, Australia and India where government is divided into different levels.
External.
External sovereignty concerns the relationship between a sovereign power and other states. For example, the United Kingdom uses the following criterion when deciding under what conditions other states recognise a political entity as having sovereignty over some territory;
"Sovereignty." A government which exercises de facto administrative control over a country and is not subordinate to any other government in that country or a foreign sovereign state. 
("The Arantzazu Mendi", [1939] A.C. 256) 
External sovereignty is connected with questions of international law – such as: when, if ever, is intervention by one country onto another's territory permissible?
Following the Thirty Years' War, a European religious conflict that embroiled much of the continent, the Peace of Westphalia in 1648 established the notion of territorial sovereignty as a norm of noninterference in the affairs of other nations, so-called Westphalian sovereignty, even though the actual treaty itself reaffirmed the multiple levels of sovereignty of the Holy Roman Empire. This resulted as a natural extension of the older principle of "cuius regio, eius religio" (Whose realm, his religion), leaving the Roman Catholic Church with little ability to interfere with the internal affairs of many European states. It is a myth, however, that the Treaties of Westphalia created a new European order of equal sovereign states.
In international law, sovereignty means that a government possesses full control over affairs within a territorial or geographical area or limit. Determining whether a specific entity is sovereign is not an exact science, but often a matter of diplomatic dispute. There is usually an expectation that both de jure and de facto sovereignty rest in the same organisation at the place and time of concern. Foreign governments use varied criteria and political considerations when deciding whether or not to recognise the sovereignty of a state over a territory. Membership in the United Nations requires that "[t]he admission of any such state to membership in the United Nations will be effected by a decision of the General Assembly upon the recommendation of the Security Council."
Sovereignty may be recognized even when the sovereign body possesses no territory or its territory is under partial or total occupation by another power. The Holy See was in this position between the annexation in 1870 of the Papal States by Italy and the signing of the Lateran Treaties in 1929, a 59-year period during which it was recognised as sovereign by many (mostly Roman Catholic) states despite possessing no territory – a situation resolved when the Lateran Treaties granted the Holy See sovereignty over the Vatican City. Another case, "sui generis", is the Sovereign Military Order of Malta, the third sovereign entity inside Italian territory (after San Marino and the Vatican City State) and the second inside the Italian capital (since in 1869 the Palazzo di Malta and the Villa Malta receive extraterritorial rights, in this way becoming the only "sovereign" territorial possessions of the modern Order), which is the last existing heir to one of several once militarily significant, crusader states of sovereign military orders. In 1607 its Grand masters were also made Reichsfürst (princes of the Holy Roman Empire) by the Holy Roman Emperor, granting them seats in the Reichstag, at the time the closest permanent equivalent to a UN-type general assembly; confirmed 1620). These sovereign rights were never deposed, only the territories were lost. 100 modern states still maintain full diplomatic relations with the order (now de facto "the most prestigious service club"), and the UN awarded it observer status.
The governments-in-exile of many European states (for instance, Norway, Netherlands or Czechoslovakia) during the Second World War were regarded as sovereign despite their territories being under foreign occupation; their governance resumed as soon as the occupation had ended. The government of Kuwait was in a similar situation "vis-à-vis" the Iraqi occupation of its country during 1990–1991. The government of Republic of China was recognized as sovereign over China from 1911 to 1971 despite that its mainland China territory became occupied by Communist Chinese forces since 1949. In 1971 it lost UN recognition to Chinese Communist-led People's Republic of China and its sovereign and political status as a state became disputed and it lost its ability to use "China" as its name and therefore became commonly known as Taiwan.
The International Committee of the Red Cross is commonly mistaken to be sovereign. It has been granted various degrees of special privileges and legal immunities in many countries, that in cases like Switzerland are considerable, which are described. The Committee is a private organisation governed by Swiss law.
Shared and pooled.
Just as the office of head of state can be vested jointly in several persons within a state, the sovereign jurisdiction over a single political territory can be shared jointly by two or more consenting powers, notably in the form of a condominium.
Likewise the member states of international organizations may voluntarily bind themselves by treaty to a supranational organization, such as a continental union. In the case of the European Union members states this is called "pooled sovereignty".
Nation-states.
A community of people who claim the right of self-determination based on a common ethnicity, history and culture might seek to establish sovereignty over a region, thus creating a nation-state. Such nations are sometimes recognised as autonomous areas rather than as fully sovereign, independent states.
Federations.
In a federal system of government, "sovereignty" also refers to powers which a constituent state or republic possesses independently of the national government. In a confederation constituent entities retain the right to withdraw from the national body, but in a federation member states or republics do not hold that right.
Different interpretations of state sovereignty in the United States of America, as it related to the expansion of slavery and fugitive slave laws, led to the outbreak of the American Civil War. Depending on the particular issue, sometimes both northern and southern states justified their political positions by appealing to state sovereignty. Fearing that slavery would be threatened by results of the federal election, eleven slave states declared their independence from the federal Union and formed a new confederation. The United States government rejected the secessions as rebellion, declaring that secession from the Union by an individual state was unconstitutional, as the states were part of an indissolvable federation. 
Acquisition.
A number of modes of acquisition of sovereignty are presently or have historically been recognised by international law as lawful methods by which a state may acquire sovereignty over territory. The classification of these modes originally derived from Roman property law and from the 15th and 16th century with the development of international law. The modes are:
  full national jurisdiction and sovereignty
  restrictions on national jurisdiction and sovereignty
  international jurisdiction per common heritage of mankind
Justifications.
There exist vastly differing views on the moral basis of sovereignty. A fundamental polarity is between theories that assert that sovereignty is vested directly in the sovereign by divine or natural right and theories that assert it originates from the people. In the latter case there is a further division into those that assert that the people transfer their sovereignty to the sovereign (Hobbes), and those that assert that the people retain their sovereignty (Rousseau).
During the brief period of Absolute monarchies in Europe, the divine right of kings was an important competing justification for the exercise of sovereignty. The Mandate of Heaven had some similar implications in China.
A republic is a form of government in which the people, or some significant portion of them, retain sovereignty over the government and where offices of state are not granted through heritage. A common modern definition of a republic is a government having a head of state who is not a monarch.
Democracy is based on the concept of "popular sovereignty". In a direct democracy the public plays an active role in shaping and deciding policy. Representative democracy permits a transfer of the exercise of sovereignty from the people to a legislative body or an executive (or to some combination of legislature, executive and Judiciary). Many representative democracies provide limited direct democracy through referendum, initiative, and recall.
Parliamentary sovereignty refers to a representative democracy where the parliament is ultimately sovereign and not the executive power nor the judiciary.
Views.
According to Matteo Laruffa "sovereignty resides in every public action and policy as the exercise of executive powers by institutions open to the participation of citizens to the decision-making processes"
Relation to rule of law.
Another topic is whether the law is held to be sovereign, that is, whether it is above political or other interference. Sovereign law constitutes a true state of law, meaning the letter of the law (if constitutionally correct) is applicable and enforceable, even when against the political will of the nation, as long as not formally changed following the constitutional procedure. Strictly speaking, any deviation from this principle constitutes a revolution or a coup d'état, regardless of the intentions.

</doc>
<doc id="28800" url="http://en.wikipedia.org/wiki?curid=28800" title="Democratic Left Alliance">
Democratic Left Alliance

Democratic Left Alliance (Polish: "Sojusz Lewicy Demokratycznej", SLD) is a social-democratic political party in Poland. Formed in 1991 as an electoral alliance of centre-left parties, it was formally established as a single party on 15 April 1999. It is currently the third largest opposition party in Poland.
History.
Communist roots.
Many SLD politicians have their roots in the communist regime of the People's Republic of Poland. Most of the members who established the party in 1999 had previously been members of the Social Democracy of the Republic of Poland (SdRP) and the Polish Social Democratic Union, the two parties that were formed out of the remains of the Polish United Workers' Party (PZPR).
Ideology and support patterns.
The coalition can be classified as left-wing, however, during the 1990s, it managed to attract voters from the pro-market and even right-wing camp. The main support for SLD came from middle-rank state sector employees, retired people, former PZPR and OPZZ members and those who were unlikely to be frequent church-goers. The core of the coalition (Social Democracy of the Republic of Poland) rejected concepts such as lustration and de-communization, supported a parliamentarian regime with only the role of an arbiter for the president and criticized the right-wing camp for introduction of religious education into school. The excommunists criticized the economic reforms, pointing to the high social costs, without negating the reforms per se.
Coalition.
SdRP, SDU and some other socialist and social-democratic parties had formed the original Democratic Left Alliance as a centre-left coalition just prior to the nation's first free elections in 1991. In 1999 the coalition became a party, but lost some members.
At the time, the coalition's membership drew mostly from the former PZPR. An alliance between the SLD and the Polish People's Party ruled Poland in the years 1993–1997. However the coalition lost power to the right-wing Solidarity Electoral Action in the 1997 election as the right-wing opposition was united this time and because of the decline of support for SLD's coalition partner PSL, though the SLD itself actually gained votes.
Electoral victory.
SLD formed a coalition with Labour Union before the 2001 Polish election and won it overwhelmingly at last by capturing about 5.3 million votes, 42% of the whole and won 200 of 460 seats in the Sejm and 75 of 100 in the Senate. After the elections, the coalition was joined by the Polish People's Party (PSL) in forming a government and Leszek Miller became the Prime Minister. In March 2003, the PSL left the coalition.
Rywin-gate.
By 2004 the support for SLD in the polls had dropped from about 30% to just below 10%, and several high-ranking party members had been accused of taking part in high profile political scandals by the mainstream press (most notably the Rywin affair: Rywin-gate).
On 6 March 2004, Leszek Miller resigned as party leader and was replaced by Krzysztof Janik. On March 26 the Sejm speaker Marek Borowski, together with other high-ranking SLD officials, announced the creation of a new centre-left party, the Social Democratic Party of Poland. On the next day, Leszek Miller announced he would step down as Prime Minister on 2 May 2004, the day after Poland joined the European Union. Miller proceeded to do so.
Decline after Rywin-gate.
In the 2004 European Parliament election, it only received 9% of the votes, giving it 5 of 54 seats reserved for Poland in the European Parliament, as part of the Party of European Socialists. In the later 2009 European election the Democratic Left Alliance-Labor Union joint ticket received 12% of the vote and 7 MEPs were elected as part of the newly retitled Socialists & Democrats group.
Wojciech Olejniczak, the former Minister of Agriculture and Rural Development, was elected the president of SLD on 29 May 2004, succeeded Józef Oleksy, who resigned from the post of Polish Prime Minister due to alleged connections to the KGB.
2005–present.
However the SLD could not avoid from suffering a huge defeat in the 2005 parliamentary election, SLD only won 11.3% of the vote. This gave the party 55 seats, barely a quarter of what it had had prior to the election. It had also lost all of its Senators. In late 2006 a centre-left political alliance called Left and Democrats was created, comprising SLD and smaller centre-left parties, the Labour Union, the Social Democratic Party of Poland, and the liberal Democratic Party – demokraci.pl. The coalition won a disappointing 13% in the 2007 parliamentary election and was dissolved soon after in April 2008. On 31 May 2008, Olejniczak was replaced by Grzegorz Napieralski as SLD leader.
In the 2011 parliamentary election, SLD received 8.24% votes which gave it 27 seats in the Sejm. After the elections, one of the party members, Sławomir Kopyciński, decided to leave SLD and join Palikot's Movement. On December 10, 2011, Leszek Miller was chosen to return as the party leader.

</doc>
<doc id="28801" url="http://en.wikipedia.org/wiki?curid=28801" title="SLD">
SLD

SLD may refer to:

</doc>
<doc id="28803" url="http://en.wikipedia.org/wiki?curid=28803" title="Segmentation fault">
Segmentation fault

In computing, a segmentation fault (often shortened to segfault) or access violation is a fault raised by hardware with memory protection, notifying an operating system (OS) about a memory access violation; on x86 computers this is a form of general protection fault. The OS kernel will in response usually perform some corrective action, generally passing the fault on to the offending process by sending the process a signal. Processes can in some cases install a custom signal handler, allowing them to recover on their own, but otherwise the OS default signal handler is used, generally causing abnormal termination of the process (a program crash), and sometimes a core dump.
Segmentation faults have various causes, and are a common problem in programs written in the C programming language, where they arise primarily due to errors in use of pointers for virtual memory addressing, particularly illegal access. Another type of memory access error is a bus error, which also has various causes, but is today much rarer; these occur primarily due to incorrect "physical" memory addressing, or due to misaligned memory access – these are memory references that the hardware "cannot" address, rather than references that a process is not "allowed" to address.
Overview.
A segmentation fault occurs when a program attempts to access a memory location that it is not allowed to access, or attempts to access a memory location in a way that is not allowed (for example, attempting to write to a read-only location, or to overwrite part of the operating system).
The term "segmentation" has various uses in computing; in the context of "segmentation fault", a term used since the 1950s, it refers to the address space of a "program." With memory protection, only the program's address space is readable, and of this, only the stack and the read-write portion of the data segment of a program are writable, while read-only data and the code segment are not writable. Thus attempting to read outside of the program's address space, or writing to a read-only segment of the address space, results in a segmentation fault, hence the name.
This should not be confused with memory segmentation (segmentation of the computer's address space, not an individual program's address space), which is a historical term for the approach to memory management nowadays known as paging. The corresponding fault in this context is a "page fault", which has a different meaning. However, these are related in that an invalid page fault generally leads to a segmentation fault, and segmentation faults and page faults are both faults raised by the virtual memory management system. Segmentation faults can also occur independently of page faults: illegal access to a valid page is a segmentation fault, but not an invalid page fault, and segmentation faults can occur in the middle of a page (hence no page fault), for example in a buffer overflow that stays within a page but illegally overwrites memory.
At the hardware level, the fault is initially raised by the memory management unit (MMU) on illegal access (if the referenced memory exists), as part of its memory protection feature, or an invalid page fault (if the referenced memory does not exist). If the problem is not an invalid logical address but instead an invalid physical address, a bus error is raised instead, though these are not always distinguished.
At the operating system level, this fault is caught and a signal is passed on to the offending process, activating the process's handler for that signal. Different operating systems have different signal names to indicate that a segmentation fault has occurred. On Unix-like operating systems, a signal called SIGSEGV (abbreviated from "segmentation violation") is sent to the offending process. On Microsoft Windows, the offending process receives a STATUS_ACCESS_VIOLATION exception.
Causes.
The conditions under which segmentation violations occur and how they manifest themselves are specific to hardware and the operating system: different hardware raises different faults for given conditions, and different operating systems convert these to different signals that are passed on to processes. The proximate cause is a memory access violation, while the underlying cause is generally a software bug of some sort. Determining the root cause – debugging the bug – can be simple in some cases, where the program will consistently cause a segmentation fault (e.g., dereferencing a null pointer), while in other cases the bug can be difficult to reproduce and depend on memory allocation on each run (e.g., dereferencing a dangling pointer).
The following are some typical causes of a segmentation fault:
These in turn are often caused by programming errors that result in invalid memory access:
In C code, segmentation faults most often occur because of errors in pointer use, particularly in C dynamic memory allocation. Dereferencing a null pointer will always result in a segmentation fault, but wild pointers and dangling pointers point to memory that may or may not exist, and may or may not be readable or writable, and thus can result in transient bugs. For example:
Now, dereferencing any of these variables could cause a segmentation fault: dereferencing the null pointer generally will cause a segfault, while reading from the wild pointer may instead result in random data but no segfault, and reading from the dangling pointer may result in valid data for a while, and then random data as it is overwritten.
Handling.
The default action for a segmentation fault or bus error is abnormal termination of the process that triggered it. A core file may be generated to aid debugging, and other platform-dependent actions may also be performed. For example, Linux systems using the grsecurity patch may log SIGSEGV signals in order to monitor for possible intrusion attempts using buffer overflows.
Examples.
Writing to read-only memory.
Writing to read-only memory raises a segmentation fault. At the level of code errors, this occurs when the program writes to part of its own code segment or the read-only portion of the data segment, as these are loaded by the OS into read-only memory.
Here is an example of ANSI C code that will generally cause a segmentation fault on platforms with memory protection. It attempts to modify a string literal, which is undefined behavior according to the ANSI C standard. Most compilers will not catch this at compile time, and instead compile this to executable code that will crash:
When the program containing this code is compiled, the string "hello world" is placed in the rodata section of the program executable file: the read-only section of the data segment. When loaded, the operating system places it with other strings and constant data in a read-only segment of memory. When executed, a variable, "s", is set to point to the string's location, and an attempt is made to write an "H" character through the variable into the memory, causing a segmentation fault. Compiling such a program with a compiler that does not check for the assignment of read-only locations at compile time, and running it on a Unix-like operating system produces the following runtime error:
Backtrace of the core file from GDB:
This code can be corrected by using an array instead of a character pointer, as this allocates memory on stack and initializes it to the value of the string literal:
Even though string literals cannot be modified (rather, this has undefined behavior in the C standard), in C they are of codice_1 type, so there is no implicit conversion in the original code, while in C++ they are of codice_2 type, and thus there is an implicit conversion, so compilers will generally catch this particular error.
Null pointer dereference.
Because a very common program error is a null pointer dereference (a read or write through a null pointer, used in C to mean "pointer to no object" and as an error indicator), most operating systems map the null pointer's address such that accessing it causes a segmentation fault.
This sample code creates a null pointer, and then tries to access its value (read the value). Doing so causes a segmentation fault at runtime on many operating systems.
Dereferencing a null pointer and then assigning to it (writing a value to a non-existent target) also usually causes a segmentation fault:
The following code includes a null pointer dereference, but when compiled will often not result in a segmentation fault, as the value is unused and thus the dereference will often be optimized away by dead code elimination:
Stack overflow.
Another example is recursion without a base case:
which causes the stack to overflow which results in a segmentation fault. Infinite recursion may not necessarily result in a stack overflow depending on the language, optimizations performed by the compiler and the exact structure of a code. In this case, the behavior of unreachable code (the return statement) is undefined, so the compiler can eliminate it and use a tail call optimization that might result in no stack usage. Other optimizations could include translating the recursion into iteration, which given the structure of the example function would result in the program running forever, while probably not overflowing its stack.

</doc>
<doc id="28804" url="http://en.wikipedia.org/wiki?curid=28804" title="Source separation">
Source separation

Source separation problems in digital signal processing are those in which several signals have been mixed together into a combined signal and the objective is to recover the original component signals from the combined signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing. This was first analyzed by Colin Cherry.
Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent components analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.
The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.
Applications.
One of the practical applications being researched in this area is medical imaging of the brain with magnetoencephalography (MEG). This kind of imaging involves careful measurements of magnetic fields outside the head which yield an accurate 3D-picture of the interior of the head. However, external sources of electromagnetic fields, such as a wristwatch on the subject's arm, will significantly degrade the accuracy of the measurement. Applying source separation techniques on the measured signals can help remove undesired artifacts from the signal.
Another application is the separation of musical signals. For a stereo mix of relatively simple signals it is now possible to make a pretty accurate separation, although some artifacts remain.

</doc>
<doc id="28805" url="http://en.wikipedia.org/wiki?curid=28805" title="Stephen Cole Kleene">
Stephen Cole Kleene

Stephen Cole Kleene (January 5, 1909 – January 25, 1994) was an American mathematician. One of the students of Alonzo Church, Kleene, along with Alan Turing, Emil Post, and others, is best known as a founder of the branch of mathematical logic known as recursion theory, which subsequently helped to provide the foundations of theoretical computer science. Kleene's work grounds the study of which functions are computable. A number of mathematical concepts are named after him: Kleene hierarchy, Kleene algebra, the Kleene star (Kleene closure), Kleene's recursion theorem and the Kleene fixpoint theorem. He also invented regular expressions, and made significant contributions to the foundations of mathematical intuitionism.
Although his last name is commonly pronounced or , Kleene himself pronounced it . His son, Ken Kleene, wrote: "As far as I am aware this pronunciation is incorrect in all known languages. I believe that this novel pronunciation was invented by my father."
Biography.
Kleene was awarded the BA degree from Amherst College in 1930. He was awarded the Ph.D. in mathematics from Princeton University in 1934. His thesis, entitled "A Theory of Positive Integers in Formal Logic", was supervised by Alonzo Church. In the 1930s, he did important work on Church's lambda calculus. In 1935, he joined the mathematics department at the University of Wisconsin–Madison, where he spent nearly all of his career. After two years as an instructor, he was appointed assistant professor in 1937.
While a visiting scholar at the Institute for Advanced Study in Princeton, 1939–40, he laid the foundation for recursion theory, an area that would be his lifelong research interest. In 1941, he returned to Amherst College, where he spent one year as an associate professor of mathematics.
During World War II, Kleene was a lieutenant commander in the United States Navy. He was an instructor of navigation at the U.S. Naval Reserve's Midshipmen's School in New York, and then a project director at the Naval Research Laboratory in Washington, D.C.
In 1946, Kleene returned to Wisconsin, becoming a full professor in 1948 and the Cyrus C. MacDuffee professor of mathematics in 1964. He was chair of the Department of Mathematics and Computer Science, 1962–63, and Dean of the College of Letters and Science from 1969 to 1974. The latter appointment he took on despite the considerable student unrest of the day, stemming from the Vietnam War. He retired from the University of Wisconsin in 1979. The mathematics library at the University of Wisconsin was renamed in his honour. 
Kleene's teaching at Wisconsin resulted in three texts in mathematical logic, Kleene (1952, 1967) and Kleene and Vesley (1965), often cited and still in print. Kleene (1952) wrote alternative proofs to the Gödel's incompleteness theorems that enhanced their canonical status and made them easier to teach and understand. Kleene and Vesley (1965) is the classic American introduction to intuitionist logic and mathematics.
Kleene served as president of the Association for Symbolic Logic, 1956–58, and of the International Union of History and Philosophy of Science, 1961. In 1990, he was awarded the National Medal of Science.
Kleene and his wife Nancy Elliott had four children. He had a lifelong devotion to the family farm in Maine. An avid mountain climber, he had a strong interest in nature and the environment, and was active in many conservation causes.

</doc>
<doc id="28809" url="http://en.wikipedia.org/wiki?curid=28809" title="Shabbat">
Shabbat

Shabbat (Hebrew: שַׁבָּת‎, "rest" or "cessation") or Shabbos (Yiddish: שבת [traditional] or שאבעס [reformed spelling]) (English: Sabbath) is the Jewish day of rest and seventh day of the week, on which religious Jews remember the Biblical creation of the heavens and the earth in six days and the Exodus of the Hebrews, and look forward to a future Messianic Age. Shabbat observance entails refraining from work activities, often with great rigor, and engaging in restful activities to honor the day. The traditional Jewish position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution, though some suggest other origins. Variations upon Shabbat are widespread in Judaism and, with adaptations, throughout the Abrahamic and many other religions.
According to halakha, Shabbat is observed from a few minutes before sunset on Friday evening until the appearance of three stars in the sky on Saturday night. Shabbat is ushered in by lighting candles and reciting a blessing. Traditionally, three festive meals are eaten: in the evening, in the morning, and late in the afternoon. The evening dinner typically begins with kiddush and another blessing recited over two loaves of challah. Shabbat is closed the following evening with a "havdalah" blessing. Shabbat is a festive day when Jews exercise their freedom from the regular labors of everyday life. It offers an opportunity to contemplate the spiritual aspects of life and to spend time with family.
History.
Etymology.
The word "Shabbat" derives from the Hebrew verb "shavat" (Hebrew: שָׁבַת‎). Although frequently translated as "rest" (noun or verb), another accurate translation of these words is "ceasing [from work]", as resting is not necessarily denoted. The related modern Hebrew word "shevita" (labor strike), has the same implication of active rather than passive abstinence from work. The notion of active cessation from labor is also regarded as more consistent with an omnipotent God's activity on the seventh day of Creation according to Genesis.
Biblical sources.
Sabbath is given special status as a holy day at the very beginning of the Torah in . It is first commanded after the Exodus from Egypt, in (relating to the cessation of manna) and in (as the fourth of the Ten Commandments). Sabbath is commanded and commended many more times in the Torah and Tanakh; double the normal number of animal sacrifices are to be offered on the day. Sabbath is also described by the prophets Isaiah, Jeremiah, Ezekiel, Hosea, Amos, and Nehemiah.
Origins.
The longstanding traditional Jewish position is that unbroken seventh-day Shabbat originated among the Jewish people, as their first and most sacred institution. The Mosaic tradition quotes an origin in the Biblical mythos of special creation, though some suggest a later, naturalistic origin.
Seventh-day Shabbat did not originate with the Egyptians, to whom it was unknown; and other origin theories based on the day of Saturn, or on the planets generally, have also been abandoned.
The first non-Biblical reference to Sabbath is in an ostracon found in excavations at Mesad Hashavyahu, which is dated 630 BCE.
Other evidence of the non-Jewish origin of a Sabbath observance has been suggested in the designation of the seventh, fourteenth, nineteenth, twenty-first and twenty-eight days of a lunar month in an Assyrian religious calendar as a 'holy day', also called ‘evil days’ (meaning "unsuitable" for prohibited activities). The prohibitions on these days, spaced seven days apart, include abstaining from chariot riding, and the avoidance of eating meat by the King. On these days officials were prohibited from various activities and common men were forbidden to "make a wish", and at least the 28th was known as a "rest-day". The "Universal Jewish Encyclopedia" advanced a theory of Assyriologists like Friedrich Delitzsch (and of Marcello Craveri) that Shabbat originally arose from the lunar cycle in the Babylonian calendar containing four weeks ending in Sabbath, plus one or two additional unreckoned days per month. The difficulties of this theory include reconciling the differences between an unbroken week and a lunar week, and explaining the absence of texts naming the lunar week as Sabbath in any language.
Status as a holy day.
The Tanakh and siddur describe Shabbat as having three purposes:
Judaism accords Shabbat the status of a joyous holy day. In many ways, Jewish law gives Shabbat the status of being the most important holy day in the Jewish calendar:
Rituals.
Welcoming Sabbath.
Honoring Shabbat ("kavod Shabbat") on Preparation Day (Friday) includes bathing, having a haircut and cleaning and beautifying the home (with flowers, for example).
According to Jewish law, Shabbat starts a few minutes before sunset. Candles are lit at this time. It is customary in many communities to light the candles 18 minutes before sundown ("tosefet Shabbat", though sometimes 36 minutes), and most printed Jewish calendars adhere to this custom. The Kabbalat Shabbat service is a prayer service welcoming the arrival of Shabbat. Before Friday night dinner, it is customary to sing two songs, one "greeting" two Shabbat angels into the house and the other praising the woman of the house for all the work she has done over the past week. After blessings over the wine and challah, a festive meal is served. Singing is traditional at Sabbath meals.
According to rabbinic literature, God via the Torah commands Jews to "observe" (refrain from forbidden activity) and "remember" (with words, thoughts, and actions) Shabbat, and these two actions are symbolized by the customary two Shabbat candles. Candles are lit usually by the woman of the house (or else by a man who lives alone). Some families light more candles, sometimes in accordance with the number of children.
Other rituals.
Shabbat is a day of celebration as well as prayer. It is customary to eat three festive meals: Dinner on Shabbat eve (Friday night), lunch on Shabbat day (Saturday), and a third meal (a "Seudah Shlishit") in the late afternoon (Saturday).
Many Jews attend synagogue services on Shabbat even if they do not do so during the week. Services are held on Shabbat eve (Friday night), Shabbat morning (Saturday morning), and late Shabbat afternoon (Saturday afternoon).
With the exception of Yom Kippur, which is referred to in the Torah as "Shabbat of Shabbatoth", days of public fasting are postponed or advanced if they coincide with Shabbat. Mourners sitting "shivah" (week of mourning subsequent to the death of a spouse or first-degree relative) outwardly conduct themselves normally for the duration of the day and are forbidden to display public signs of mourning.
Although most Shabbat laws are restrictive, the fourth of the Ten Commandments in Exodus is taken by the Talmud and Maimonides to allude to the "positive" commandments of Shabbat. These include:
Bidding farewell.
"Havdalah" (Hebrew: הַבְדָּלָה, "separation") is a Jewish religious ceremony that marks the symbolic end of Shabbat, and ushers in the new week. At the conclusion of Shabbat at nightfall, after the appearance of three stars in the sky, the "havdalah" blessings are recited over a cup of wine, and with the use of fragrant spices and a candle, usually braided. Some communities delay "havdalah" later into the night in order to prolong Shabbat.
Prohibited activities.
Jewish law (halakha) prohibits doing any form of "melakhah" (מְלָאכָה, plural "melakhoth") on Shabbat, unless an urgent human or medical need is life-threatening. Though "melakhah" is commonly translated as "work" in English, a better definition is "deliberate activity" or "skill and craftmanship". There are 39 categories of prohibited activities ("melakhoth") listed in Mishnah Tractate Shabbat 7:2.
The term "shomer Shabbat" is used for a person (or organization) who adheres to Shabbat laws consistently. The "shomer Shabbat" is an archetype mentioned in Jewish songs (e.g., "Baruch El Elyon") and the intended audience for various treatises on Jewish law and practice for "Shabbat" (e.g., "Shemirat Shabbat ke-Hilkhata").
There are often disagreements between Orthodox Jews and non-Orthodox Jews as to the practical observance of the Sabbath. The (strict) observance of the Sabbath is often seen as a benchmark for orthodoxy and indeed has legal bearing on the way a Jew is seen by an orthodox religious court regarding their affiliation to Judaism. See Rabbi Joseph B. Soloveitchik's "Beis HaLevi" commentary on parasha Ki Tissa for further elaboration regarding the legal ramifications.
The 39 categories of "melakhah" are: 
The categories are exegetically derived (based on juxtaposition of corresponding Biblical passages) from the kinds of work that were necessary for the construction of the Tabernacle. They are not directly listed in the Torah; the Mishnah observes that "the laws of Shabbat ... are like mountains hanging by a hair, for they are little Scripture but many laws". Many rabbinic scholars have pointed out that these labors have in common activity that is "creative", or that exercises control or dominion over one's environment.
Orthodox and Conservative.
Different streams of Judaism view the prohibition on work in different ways. Observant Orthodox and Conservative Jews refrain from performing the 39 prohibited categories of activities. Each "melakhah" has derived prohibitions of various kinds. There are, therefore, many more forbidden activities on Shabbat; all are traced back to one of the 39 above principal "melakhoth".
Given the above, the 39 "melakhoth" are not so much activities as "categories of activity". For example, while "winnowing" usually refers exclusively to the separation of chaff from grain, and "selecting" refers exclusively to the separation of debris from grain, they refer in the Talmudic sense to any separation of intermixed materials which renders edible that which was inedible. Thus, filtering undrinkable water to make it drinkable falls under this category, as does picking small bones from fish (gefilte fish is one solution to this problem).
Electricity.
Orthodox and some Conservative authorities rule that turning electric devices on or off is prohibited as a "melakhah"; however, authorities are not in agreement about exactly which one(s). One view is that tiny sparks are created in a switch when the circuit is closed, and this would constitute lighting a fire (category 37). If the appliance is purposed for light or heat (such as an incandescent bulb or electric oven), then the lighting or heating elements may be considered as a type of fire that falls under both lighting a fire (category 37) and cooking (i.e., baking, category 11). Turning lights off would be extinguishing a fire (category 36).
Another view is that a device plugged into an electrical outlet of a wall becomes part of the building, but is nonfunctional while the switch is off; turning it on would then constitute building (category 35) and turning it off would be demolishing (category 34). Some schools of thought consider the use of electricity to be forbidden only by rabbinic injunction, rather than because it violates one of the original categories.
A common solution to the problem of electricity involves preset timers (Shabbat clocks) for electric appliances, to turn them on and off automatically, with no human intervention on Shabbat itself. Some Conservative authorities reject altogether the arguments for prohibiting the use of electricity. Some Orthodox also hire a "Shabbos goy", a Gentile to perform prohibited tasks (like operating light switches) on Shabbat.
Automobiles.
Orthodox and many Conservative authorities completely prohibit the use of automobiles on Shabbat as a violation of multiple categories, including lighting a fire, extinguishing a fire, and transferring between domains (category 39). However, the Conservative movement's Committee on Jewish Law and Standards permits driving to a synagogue on Shabbat, as an emergency measure, on the grounds that if Jews lost contact with synagogue life they would become lost to the Jewish people.
A halakhically authorized Shabbat module added to a power-operated mobility scooter may be used on the observance of Shabbat for those with walking limitations, often referred to as a Shabbat scooter. It is intended only for individuals whose limited mobility is dependent on a scooter or automobile consistently throughout the week.
Modifications.
Seemingly "forbidden" acts may be performed by modifying technology such that no law is actually violated. In Sabbath mode, a "Sabbath elevator" will stop automatically at every floor, allowing people to step on and off without anyone having to press any buttons, which would normally be needed to work. (Dynamic braking is also disabled if it is normally used, i.e., shunting energy collected from downward travel, and thus the gravitational potential energy of passengers, into a resistor network.) However, many rabbinical authorities consider the use of such elevators by those who are otherwise capable as a violation of Shabbat, with such workarounds being for the benefit of the frail and handicapped and not being in the spirit of the day.
Many observant Jews avoid the prohibition of carrying by use of an eruv. Others make their keys into a tie bar, part of a belt buckle, or a brooch, because a legitimate article of clothing or jewelry may be worn rather than carried. An elastic band with clips on both ends, and with keys placed between them as integral links, may be considered a belt.
Shabbat lamps have been developed to allow a light in a room to be turned on or off at will while the electricity remains on. A special mechanism blocks out the light when the off position is desired without violating Shabbat.
The Shabbos App is a proposed Android app claimed by its creators to enable Orthodox Jews, and all Jewish Sabbath-observers, to use a smartphone to text on the Jewish Sabbath. It has met with resistance from some authorities.
Permissions.
In the event that a human life is in danger ("pikuach nefesh"), a Jew is not only allowed, but required, to violate any halakhic law that stands in the way of saving that person (excluding murder, idolatry, and forbidden sexual acts). The concept of life being in danger is interpreted broadly: for example, it is mandated that one violate Shabbat to bring a woman in active labor to a hospital. Lesser rabbinic restrictions are often violated under much less urgent circumstances (a patient who is ill but not critically so).
We did everything to save lives, despite Shabbat. People asked, 'Why are you here? There are no Jews here', but we are here because the Torah orders us to save lives ... We are desecrating Shabbat with pride.—Mati Goldstein, commander of the Jewish ZAKA rescue-mission to the 2010 Haiti earthquake
Various other legal principles closely delineate which activities constitute desecration of Shabbat. Examples of these include the principle of "shinui" ("change" or "deviation"): A violation is not regarded as severe if the prohibited act was performed in a way that would be considered abnormal on a weekday. Examples include writing with one's nondominant hand, according to many rabbinic authorities. This legal principle operates "bedi'avad" ("ex post facto") and does not cause a forbidden activity to be permitted barring extenuating circumstances.
Reform and Reconstructionist.
Generally, adherents of Reform and Reconstructionist Judaism believe that the individual Jew determines whether to follow Shabbat prohibitions or not. For example, some Jews might find activities, such as writing or cooking for leisure, to be enjoyable enhancements to Shabbat and its holiness, and therefore may encourage such practices. Many Reform Jews believe that what constitutes "work" is different for each person, and that only what the person considers "work" is forbidden. Radical Hungarian-born Reform Rabbi Ignaz Einhorn even shifted his congregation's Shabbat worship to Sundays.
More rabbinically traditional Reform and Reconstructionist Jews believe that these halakhoth in general may be valid, but that it is up to each individual to decide how and when to apply them. A small fraction of Jews, in the Progressive Jewish community, accept these laws much the same way as Orthodox Jews.
Encouraged activities.
All Jewish denominations encourage the following activities on Shabbat:
Special Shabbatoth.
The Special Shabbatoth are the Shabbatoth that precede important Jewish holidays: e.g., "Shabbat haGadol" (Shabbat preceding Pesach), "Shabbat Zachor" (Shabbat preceding Purim), and "Shabbat Teshuva" (Shabbat preceding Yom Kippur).
Sabbath adaptation.
Most Christians do not observe Saturday Sabbath, but instead observe a weekly day of worship on Sunday, which is often called the "Lord's Day".
Several Christian denominations, such as the Seventh-day Adventist Church, the Church of God (7th Day), the Seventh Day Baptists, and many others, observe seventh-day Sabbath. This observance is celebrated from Friday sunset to Saturday sunset. Some of Messianic Judaism considers its Sabbath to be kept according to Jewish doctrinal tradition, while most of Rabbinic Judaism disagrees.
The principle of weekly Sabbath also exists in other beliefs. Examples include the Babylonian calendar, the Buddhist "uposatha", the Islamic "jumu'ah", and the Unification Church's Ahn Shi Il.

</doc>
<doc id="28810" url="http://en.wikipedia.org/wiki?curid=28810" title="Saki">
Saki

Hector Hugh Munro (18 December 1870 – 14 November 1916), better known by the pen name Saki, and also frequently as H. H. Munro, was a British writer whose witty, mischievous and sometimes macabre stories satirize Edwardian society and culture. He is considered a master of the short story, and often compared to O. Henry and Dorothy Parker. Influenced by Oscar Wilde, Lewis Carroll and Rudyard Kipling, he himself influenced A. A. Milne, Noël Coward and P. G. Wodehouse.
Besides his short stories (which were first published in newspapers, as was customary at the time, and then collected into several volumes), he wrote a full-length play, "The Watched Pot", in collaboration with Charles Maude; two one-act plays; a historical study, "The Rise of the Russian Empire", the only book published under his own name; a short novel, "The Unbearable Bassington"; the episodic "The Westminster Alice" (a parliamentary parody of "Alice in Wonderland"); and "When William Came", subtitled "A Story of London Under the Hohenzollerns", a fantasy about a future German invasion and occupation of Britain.
Life.
Born in Akyab, British Burma, which was then still part of the British Empire, Hector Hugh Munro was the son of Charles Augustus Munro, an Inspector General for the Indian Imperial Police, By his marriage to Mary Frances Mercer (1843–1872), the daughter of Rear Admiral Samuel Mercer. Her nephew, Cecil William Mercer, later became a famous novelist as Dornford Yates.
In 1872, on a home visit to England, Mary Munro was charged by a cow, and the shock caused her to miscarry. She never recovered and soon died. Charles Munro sent his children, including two-year-old Hector, home to England, where they were brought up by their grandmother and aunts in a strict and puritanical household.
The young Hector Munro was educated at Pencarwick School in Exmouth and then as a boarder at Bedford School. On a few occasions after he retired from Burma, Charles Munro travelled with Hector and his sister to fashionable European spas and resorts.
In 1893 Hector Munro followed his father into the Indian Imperial Police and was posted to Burma. Two years later, having contracted malaria, he resigned and returned to England.
At the start of the First World War Munro was 43 and officially over-age to enlist, but he refused a commission and joined the 2nd King Edward's Horse as an ordinary trooper. He later transferred to the 22nd Battalion of the Royal Fusiliers, in which he rose to the rank of lance sergeant. More than once he returned to the battlefield when officially still too sick or injured. In November 1916 he was sheltering in a shell crater near Beaumont-Hamel, France, during the Battle of the Ancre, when he was killed by a German sniper. According to several sources, his last words were "Put that bloody cigarette out!"
Munro has no known grave. He is commemorated on Pier and Face 8C 9A and 16A of the Thiepval Memorial.
In 2003 English Heritage marked Munro's flat at 97 Mortimer Street, in Fitzrovia with a blue plaque.
After his death his sister Ethel destroyed most of his papers and wrote her own account of their childhood.
Munro was homosexual, but in Britain at that time sexual activity between men was a crime. The Cleveland Street scandal (1889), followed by the downfall of Oscar Wilde (1895), meant "that side of [Munro's] life had to be secret".
Munro was a Tory and somewhat reactionary in his views.
Writing career.
The pen name "Saki" may be a reference to the cupbearer in the "Rubáiyát of Omar Khayyam", a poem mentioned disparagingly by the eponymous character in "Reginald on Christmas Presents" and alluded to in a few other stories. This reference is stated as fact by Emlyn Williams in his introduction to a Saki anthology published in 1978. However, "Saki" may also or instead be a reference to the South American monkey of that name, which at least two commentators, Tom Sharp and Will Self, have connected to the "small, long-tailed monkey from the Western Hemisphere" that is a central character in .
Munro started his writing career as a journalist for newspapers such as the "Westminster Gazette", the "Daily Express", the "Morning Post", and magazines such as the "Bystander" and "Outlook". His first book "The Rise of the Russian Empire", a historical study modelled upon Edward Gibbon's "The Decline and Fall of the Roman Empire", appeared in 1900, under his real name.
From 1902 to 1908 Munro worked as a foreign correspondent for the "Morning Post" in the Balkans, Warsaw, Russia (where he witnessed Bloody Sunday), and Paris. He then gave up foreign reporting and settled in London. Many of his stories from this period feature Reginald and Clovis, young men-about-town who take mischievous delight in the discomfort or downfall of their conventional, pretentious elders.
Shortly before the First World War, when "invasion literature" was selling well, Munro published a "what-if" novel, "When William Came", subtitled "A Story of London Under the Hohenzollerns", imagining the eponymous German emperor conquering and occupying Britain.
Example works.
Much of Saki's work contrasts the conventions and hypocrisies of Edwardian England with the ruthless but straightforward life-and-death struggles of nature. Nature generally wins in the end.
"The Interlopers".
"The Interlopers" is a story about two men, Georg Znaeym and Ulrich von Gradwitz, whose families have fought over a forest in the eastern Carpathian Mountains for generations. Ulrich's family legally owns the land, but Georg, believing that it rightfully belongs to him, hunts there anyway. One winter night Ulrich catches Georg hunting in his forest. Neither man can shoot the other without warning, as they would soil their family’s honour, so they hesitate to acknowledge one another. As an "act of God" a tree branch suddenly falls on each of them, trapping them both under a log. Gradually they realise the futility of their quarrel, become friends and end the feud. They call out for their men’s assistance and, after a brief period, Ulrich makes out nine or ten figures approaching over a hill. The story ends with Ulrich’s realisation that the "interlopers" on the hill are actually wolves.
"Gabriel-Ernest".
"Gabriel-Ernest" starts with a warning: "There is a wild beast in your woods …" As the story proceeds we learn from the narrator that Gabriel is indeed wild, feral, in fact a werewolf. The climax comes when Gabriel is revealed to have taken a small child home from Sunday school. A pursuit ensues, but Gabriel and the child disappear near a river. The only items found are Gabriel's clothes, and the two are never seen again.
"The Schartz-Metterklume Method".
At a railway station an arrogant and overbearing woman, Mrs Quabarl, mistakes the mischievous Lady Carlotta, who has been inadvertently left behind by a train, for the governess, Miss Hope, whom she has been expecting, Miss Hope having erred about the date of her arrival. Lady Carlotta decides not to correct the mistake, acknowledges herself as Miss Hope, a proponent of "the Schartz-Metterklume method" of making children understand history by acting it out themselves, and chooses the Rape of the Sabine Women (exemplified by a washerwoman's two girls) as the first lesson.
"The Toys of Peace".
Preferring not to give her young sons toy soldiers or guns, and having taken away their toy depicting the Siege of Adrianople, Eleanor instructs her brother Harvey to give them innovative "peace toys" as an Easter present. When the packages are opened young Bertie shouts "It's a fort!" and is disappointed when his uncle replies "It's a municipal dustbin." The boys are initially baffled as to how to obtain any enjoyment from models of a school of art and a public library, or from little figures of John Stuart Mill, Felicia Hemans and Sir John Herschel. Youthful inventiveness finds a way, however, as the boys combine their history lessons on Louis XIV with a lurid and violent play-story about the invasion of Britain and the storming of the Young Women's Christian Association. The end of the story has Harvey reporting failure to Eleanor, explaining "We have begun too late."
"The Storyteller".
An aunt is travelling by train with her two nieces and a nephew. The children are naughty and mischievous. A bachelor is sitting opposite. The aunt starts telling a moralistic story, but is unable to satisfy the children's curiosity. The bachelor intervenes and tells a story in which the "good" person ends up being unwittingly devoured by a wolf, to the children's delight. The bachelor is amused by the thought that in the future the children will embarrass their guardian by begging to be told "an improper story".
"The Open Window".
Framton Nuttel, a nervous man, has come to stay in the country for his health. His sister, who thinks he should socialise while he is there, has given him letters of introduction to families in the neighbourhood whom she got to know when she was staying there a few years previously. Framton goes to visit Mrs Sappleton and, while he is waiting for her to come down, is entertained by her fifteen-year-old niece. The niece tells him that the French window is kept open, even though it is October, because Mrs Sappleton believes that her husband and her brothers, who were killed in a shooting accident three years before, will come back one day. When Mrs Sappleton comes down she talks about her husband and her brothers, and how they are going to come back from shooting soon, and Framton, believing that she is deranged, tries to distract her by talking about his health. Then, to his horror, Mrs Sappleton points out that her husband and her brothers are coming, and he sees them walking towards the window with their dog. He thinks he is seeing ghosts and runs away. Mrs Sappleton can't understand why he has run away and, when her husband and her brothers come in, she tells them about the odd man who has just left. The niece explains that Framton Nuttel ran away because of the spaniel: he is afraid of dogs since he was hunted by a pack of pariah dogs in India. The last line summarizes the story, saying of the niece, "Romance at short notice was her speciality."
"The Unrest-Cure".
Saki's recurring hero Clovis Sangrail, a sly young man, overhears the complacent middle-aged Huddle complaining of his own addiction to routine and aversion to change. Huddle's friend makes the wry suggestion that he needs an "unrest-cure" (the opposite of a rest cure), to be performed, if possible, in the home. Clovis takes it upon himself to "help" the man and his sister by involving them in an invented outrage that will be a "blot on the twentieth century".
"Esmé".
A Baroness tells Clovis about a hyena that she and her friend Constance encountered alone in the countryside, and that cannot resist the urge to stop for a snack.
"Sredni Vashtar".
This is the story of a sickly child named Conradin whose cousin and guardian, Mrs De Ropp, "would never ... have confessed to herself that she disliked Conradin, though she might have been dimly aware that thwarting him 'for his good' was a duty which she did not find particularly irksome".
"Tobermory".
At a country-house party, one guest, Cornelius Appin, announces to the others that he has perfected a procedure for teaching animals human speech. He demonstrates this on his host's cat, Tobermory. Soon it is clear that animals are permitted to view many private things on the assumption that they will remain silent, such as the host Sir Wilfred's commentary on one guest's intelligence and the hope that she will buy his car, or the implied sexual activities of another guest, Major Barfield. The guests are angered, especially when Tobermory runs away to pursue a rival cat, but plans to poison him fail when Tobermory is instead killed by the rival cat. "An archangel ecstatically proclaiming the Millennium, and then finding that it clashed unpardonably with Henley and would have to be indefinitely postponed, could hardly have felt more crestfallen than Cornelius Appin at the reception of his wonderful achievement." Appin is killed shortly afterwards when attempting to teach an elephant in a zoo in Dresden to speak German.
"The Bull".
Tom Yorkfield, a farmer, receives a visit from his half-brother Laurence. Tom has no great liking for Laurence or respect for his profession as a painter of animals. Tom shows Laurence his prize bull and expects him to be impressed, but Laurence nonchalantly tells Tom that he has sold a painting of a different bull, which Tom has seen and does not like, for three hundred pounds. Tom is angry that a mere picture of a bull should be worth more than his real bull. This and Laurence's condescending attitude give him the urge to strike him. Laurence, running away across the field, is attacked by the bull, but is saved by Tom from serious injury. Tom, looking after Laurence as he recovers, feels no more rancour because he knows that, however valuable Laurence's painting might be, only a real bull like his can attack someone.
"The East Wing".
This is a "rediscovered" short story that was previously cited as a play. A house party is beset by a fire in the middle of the night in the east wing of the house. Begged by their hostess to save "my poor darling Eva – Eva of the golden hair," Lucien demurs, on the grounds that he has never even met her. It is only on discovering that Eva is not a flesh-and-blood daughter but Mrs Gramplain's painting of the daughter she wished that she had had, and which she has faithfully updated with the passing years, that Lucien declares a willingness to forfeit his life to rescue her, since "death in this case is more beautiful," a sentiment endorsed by the Major. As the two men disappear into the blaze, Mrs Gramplain recollects that she "sent Eva to Exeter to be cleaned". The two men have lost their lives for nothing.
Publications.
Posthumous publications:
Television.
A dramatisation of "The Schartz-Metterklume Method" was an episode in the series "Alfred Hitchcock Presents" in 1960.
"Saki: The Improper Stories of H. H. Munro" (a reference to the ending of "The Story Teller") was an eight-part series produced by Philip Mackie for Granada Television in 1962. Actors involved included Mark Burns as Clovis, Fenella Fielding as Mary Drakmanton, Heather Chasen as Agnes Huddle, Richard Vernon as the Major, Rosamund Greenwood as Veronique and Martita Hunt as Lady Bastable.
"Who Killed Mrs De Ropp?", a BBC TV production in 2007, starring Ben Daniels and Gemma Jones, showcased three of Saki's short stories, "The Storyteller", "The Lumber Room" and "Sredni Vashtar".

</doc>
<doc id="28811" url="http://en.wikipedia.org/wiki?curid=28811" title="Static program analysis">
Static program analysis

Static program analysis is the analysis of computer software that is performed without actually executing programs (analysis performed on executing programs is known as dynamic analysis). In most cases the analysis is performed on some version of the source code, and in the other cases, some form of the object code. The term is usually applied to the analysis performed by an automated tool, with human analysis being called program understanding, program comprehension, or code review.
Rationale.
The sophistication of the analysis performed by tools varies from those that only consider the behavior of individual statements and declarations, to those that include the complete source code of a program in their analysis. The uses of the information obtained from the analysis vary from highlighting possible coding errors (e.g., the lint tool) to formal methods that mathematically prove properties about a given program (e.g., its behavior matches that of its specification).
Software metrics and reverse engineering can be described as forms of static analysis. Deriving software metrics and static analysis are increasingly deployed together, especially in creation of embedded systems, by defining so-called "software quality objectives".
A growing commercial use of static analysis is in the verification of properties of software used in safety-critical computer systems and
locating potentially vulnerable code. For example the following industries have identified the use of static code analysis as a means of improving the quality of increasingly sophisticated and complex software:
A study in 2012 by VDC Research reports that 28.7% of the embedded software engineers surveyed currently use static analysis tools and 39.7% expect to use them within 2 years.
A study from 2010 found that 60% of the interviewed developers in European research projects made at least use of their basic IDE built-in static analyzers. However, only about 10% employed an additional other (and perhaps more advanced) analysis tool.
In the application security industry the name Static Application Security Testing (SAST) is also used.
Tool types.
The OMG (Object Management Group) published a study regarding the types of software analysis required for software quality measurement and assessment. This document on "How to Deliver Resilient, Secure, Efficient, and Easily Changed IT Systems in Line with CISQ Recommendations" describes three levels of software analysis.
Unit Level - Analysis that takes place within a specific program or subroutine, without connecting to the context of that program.
Technology Level - Analysis that takes into account interactions between unit programs to get a more holistic and semantic view of the overall program in order to find issues and avoid obvious false positives.
System Level - Analysis that takes into account the interactions between unit programs, but without being limited to one specific technology or programming language.
A further level of software analysis can be defined.
Mission/Business Level - Analysis that takes into account the business/mission layer terms, rules and processes that are implemented within the software system for its operation as part of enterprise or program/mission layer activities. These elements are implemented without being limited to one specific technology or programming language and in many cases are distributed across multiple languages but are statically extracted and analyzed for system understanding for mission assurance.
Formal methods.
Formal methods is the term applied to the analysis of software (and computer hardware) whose results are obtained purely through the use of rigorous mathematical methods. The mathematical techniques used include denotational semantics, axiomatic semantics, operational semantics, and abstract interpretation.
By a straightforward reduction to the halting problem, it is possible to prove that (for any Turing complete language), finding all possible run-time errors in an arbitrary program (or more generally any kind of violation of a specification on the final result of a program) is undecidable: there is no mechanical method that can always answer truthfully whether an arbitrary program may or may not exhibit runtime errors. This result dates from the works of Church, Gödel and Turing in the 1930s (see: Halting problem and Rice's theorem). As with many undecidable questions, one can still attempt to give useful approximate solutions.
Some of the implementation techniques of formal static analysis include:

</doc>
<doc id="28812" url="http://en.wikipedia.org/wiki?curid=28812" title="Samuel Mudd">
Samuel Mudd

Samuel Alexander Mudd I (December 20, 1833 – January 10, 1883) was an American physician who was imprisoned for conspiring with John Wilkes Booth in the assassination of U.S. President Abraham Lincoln.
While working as a doctor in Southern Maryland, Mudd also employed slaves on his tobacco-farm, and declared his belief in slavery as a God-given institution. The Civil War seriously damaged his business, especially when Maryland abolished slavery in 1864. At this time, he first met Booth, who was planning to kidnap Lincoln, and Mudd was seen in company with three of the conspirators. But his part in the plot, if any, remains unclear. 
After assassinating Lincoln on April 14, 1865, Booth rode with co-conspirator David Herold to Mudd’s home in the early hours of the 15th for surgery on his fractured leg, before crossing into Virginia. Some time that day, Mudd must have learned of the assassination, but did not report Booth’s visit to the authorities for another 24 hours. This appeared to link him to the crime, as did his various changes of story under interrogation, and on April 26, he was arrested. A military commission found him guilty of aiding and conspiring in a murder, and he was sentenced to life imprisonment, escaping the death penalty by a single vote.
Mudd was pardoned by President Andrew Johnson and released from prison in 1869. Despite repeated attempts by family members and others to have it expunged, his conviction has never been overturned.
Early years.
Born in Charles County, Maryland, Mudd was the fourth of ten children of Henry Lowe and Sarah Ann Reeves Mudd. He grew up on "Oak Hill", his father's tobacco plantation of several hundred acres which was located 30 mi southeast of downtown Washington, D.C., and which was worked by 89 slaves.
At the age of 15, after several years of home tutoring, Mudd went off to boarding school at St. Johns in Frederick, Maryland. Two years later, he enrolled at Georgetown College in Washington, D.C.. He then studied medicine at the University of Maryland, Baltimore, writing his thesis on dysentery.
Upon graduation in 1856, Mudd returned to Charles County to practice medicine, marrying his childhood sweetheart, Sarah Frances (Frankie) Dyer Mudd one year later.
As a wedding present, Mudd's father gave the couple 218 acre of his best farmland and a new house named St. Catherine. While the house was under construction, the young Mudds lived with Frankie's bachelor brother, Jeremiah Dyer, finally moving into their new home in 1859. They had nine children in all; four before Mudd's arrest and five after his release from prison. To supplement his income from his medical practice, Mudd became a small scale tobacco grower, using five slaves according to the 1860 U.S. Slave Census. Mudd believed that slavery was divinely ordained, writing a letter to the theologian Orestes Brownson to that effect.
With the advent of the American Civil War in 1861, the Southern Maryland slave system and the economy it supported rapidly began to collapse. In 1863, the Union Army established Camp Stanton just 10 mi from the Mudd farm to enlist black freedmen and run-away slaves. Six regiments totaling over 8,700 black soldiers, many from Southern Maryland, were trained there. In 1864, Maryland, which was exempt from Lincoln's 1863 Emancipation Proclamation, abolished slavery, making it difficult for growers like Mudd to operate their plantations. As a result, Mudd considered selling his farm and depending on his medical practice. As Mudd pondered his alternatives, he was introduced to someone who said he might be interested in buying his property, the 26-year-old actor John Wilkes Booth.
Booth connection.
Many historians agree that President Abraham Lincoln's future assassin, John Wilkes Booth, visited Bryantown, Maryland, in November and December 1864, claiming to look for real estate investments. Bryantown is about 25 mi from Washington, D.C., and about 5 mi from Mudd's farm. The real estate story was merely a cover; Booth's true purpose was to plan an escape route as part of a plan to kidnap Lincoln. Booth believed the federal government would ransom Lincoln by releasing a large number of Confederate prisoners of war.
Historians agree that Booth met Mudd at St. Mary's Catholic Church in Bryantown during one of those visits, probably the November visit. Booth visited Mudd at his farm the next day, and stayed there overnight. The following day, Booth purchased a horse from Mudd's neighbor and returned to Washington. Some historians believe that Booth used his visit to Bryantown to recruit Mudd to his kidnapping plot, although others believe that Mudd would have had no interest in such a scheme.
A short time later, on December 23, 1864, Mudd went to Washington where he met Booth again. Some historians believe the meeting had been arranged, but others disagree. The two men, as well as John Surratt, Jr. and Louis J. Weichmann, had a conversation and drinks together, first at Booth's hotel, and later at Mudd's.
According to a statement made by associated conspirator George Atzerodt, found long after his death and taken down while he was in federal custody on May 1, 1865, Mudd knew in advance about Booth's plans; Atzerodt was sure the doctor knew, he said, because Booth had "sent (as he told me) liquors & provisions [...] about two weeks before the murder to Dr. Mudd's".
After Booth shot President Lincoln on April 14, 1865, he broke his left fibula while fleeing Ford's Theater. Booth met up with David Herold and together they made for Virginia via Southern Maryland. They stopped at Mudd's house around four o'clock in the morning on April 15. After cutting out the shoe from it, Mudd splinted Booth's leg. He also arranged for a carpenter, John Best, to make a pair of crutches for Booth, and gave him a shoe to wear. Booth paid Mudd twenty-five dollars in greenbacks for his services. He and Herold spent between twelve and fifteen hours at Mudd's house. They slept in the front bedroom on the second floor. It is unclear whether Mudd had been informed that Booth had murdered President Lincoln at that point.
Mudd went to Bryantown during the day on April 15 to run errands; if he did not already know the news of the assassination from Booth, he certainly learned of it on this trip. He returned home that evening, and accounts differ as to whether Booth and Herold had already left, whether Mudd met them as they were leaving, or whether they left at Mudd's urging and with his assistance.
Whichever is true, Mudd did not immediately contact the authorities. When questioned, he stated that he had not wanted to leave his family alone in the house lest the assassins return and find him absent and his family unprotected. He waited until Mass the following day, Easter Sunday, when he asked his second cousin, Dr. George Mudd — a resident of Bryantown — to notify the 13th New York Cavalry in Bryantown under the command of Lieutenant David Dana. This delay in contacting the authorities drew suspicion and was a significant factor in tying Mudd to the conspiracy.
During his initial investigative interview on April 18, Mudd stated that he had never seen either of the parties before. In his sworn statement of April 22, he told about Booth's visit to Bryantown in November 1864, but then said "I have never seen Booth since that time to my knowledge until last Saturday morning." He deliberately hid the fact of his meeting with Booth in Washington in December 1864. In prison, Mudd belatedly admitted the Washington meeting, saying he ran into Booth by chance during a Christmas shopping trip. Mudd's failure to mention the meeting in his interview with detectives was a big mistake. When Louis J. Weichmann later told the authorities of this meeting, they realized Mudd had misled them, and immediately began to treat him as a suspect rather than a witness. 
During the conspiracy trial, Lieutenant Alexander Lovett testified that "On Friday, the 21st of April, I went to Mudd's again, for the purpose of arresting him. When he found we were going to search the house, he said something to his wife, and she went up stairs and brought down a boot. Mudd said he had cut it off the man's leg. I turned down the top of the boot, and saw the name 'J. Wilkes' written in it."
Trial.
After Booth's death (April 26, 1865), Mudd was arrested and charged with conspiracy to murder Abraham Lincoln. Congressman Frederick Stone was the senior defense counsel for Mudd.
On May 1, 1865, President Andrew Johnson ordered the formation of a nine-man military commission to try the conspirators. Mudd was represented by General Thomas Ewing, Jr.. The trial began on May 10, 1865. Mary Surratt, Lewis Powell, George Atzerodt, David Herold, Samuel Mudd, Michael O'Laughlen, Edmund Spangler and Samuel Arnold were all charged with conspiring to murder Lincoln. The prosecution called 366 witnesses.
The defense sought to prove that Mudd was a loyal citizen, citing his self-description as a "Union man" and asserting that he was "a deeply religious man, devoted to family, and a kind master to his slaves". The prosecution presented witnesses who testified that he had shot one of his slaves in the leg, and threatened to send others to Richmond, Virginia to assist in the construction of Confederate defenses. The prosecution also contended that he had been a member of a Confederate communications distribution agency and had sheltered Confederate soldiers on his plantation.
On June 29, 1865, Mudd was found guilty with the others. The testimony of Louis J. Weichmann was crucial in obtaining the convictions. According to the historian Edward Steers, the testimony presented by former slaves was also crucial, although it faded from public memory. Mudd escaped the death penalty by one vote and was sentenced to life imprisonment. Four of the defendants, Surratt, Powell, Atzerodt and Herold, were hanged at the Old Penitentiary at the Washington Arsenal on July 7, 1865.
Imprisonment.
Mudd, O'Laughlen, Arnold and Spangler were imprisoned at Fort Jefferson in the Dry Tortugas about 70 mi west of Key West, Florida. The fort housed Union Army deserters and held about six hundred prisoners when Mudd and the others arrived. Prisoners lived on the second tier of the fort, in unfinished open-air gun rooms called casemates. Mudd and his three companions lived in the casemate directly above the fort's main entrance, called the sally port.
In September, 1865, two months after Mudd arrived, control of Fort Jefferson was transferred from the 161st New York Volunteers to the 82nd United States Colored Infantry. On September 25, 1865, he attempted to escape from Fort Jefferson by stowing away on the transport "Thomas A. Scott". Some historians believe that as a recent slave owner and a person convicted of conspiring to kill the president whose presidency led to the freeing of the slaves, Mudd was fearful of his treatment by the incoming 82nd United States Colored Infantry and this is why he made his bid for freedom.
He was quickly discovered and placed in the fort's guardhouse. On October 18, he was transferred along with Samuel Arnold, Michael O'Laughlen, Edman Spangler, and George St. Leger Grenfell to a large empty ground-level gunroom the soldiers referred to as "the dungeon". Mudd and the others were let out of the dungeon six days a week to work around the fort. On Sundays and holidays they were confined inside. The men wore leg irons while working outside, but the irons were removed when inside the dungeon.
After three months in the dungeon, Mudd and the others were returned to the general prison population. However, because of his attempted escape, Mudd lost his privilege of working in the prison hospital and was assigned to work in the prison carpentry shop with Spangler.
There was an outbreak of yellow fever in the fall of 1867 at the fort. Michael O'Laughlen eventually died of it on September 23. The prison doctor died and Mudd agreed to take over the position. In this role he was able to help stem the spread of the disease. The soldiers in the fort wrote a petition to President Johnson in October 1867 stating of Mudd's assistance, "He inspired the hopeless with courage and by his constant presence in the midst of danger and infection... [Many] doubtless owe their lives to the care and treatment they received at his hands." Probably as a reward for his work in the yellow fever epidemic, Dr. Mudd was reassigned from the carpentry shop to a clerical job in the Provost Marshall's office, where he remained until his pardon.
Career after release.
Due in part to the influence of his defense attorney, Thomas Ewing Jr., who was influential in the President's administration, on February 8, 1869, Mudd was pardoned by President Andrew Johnson. He was released from prison on March 8, 1869 and returned home to Maryland on March 20, 1869. On March 1, 1869, three weeks after he pardoned Mudd, Johnson also pardoned Spangler and Arnold.
When Mudd returned home, well-wishing friends and strangers, as well as inquiring newspaper reporters, besieged him. Mudd was very reluctant to talk to the press because he felt they had misquoted him in the past. He gave one interview after his release to the New York Herald, but immediately regretted it, complaining that the article had several factual errors and that it misrepresented his work during the yellow fever epidemic. On the whole though, Mudd continued to enjoy the friendship of his friends and neighbors. He resumed his medical practice and slowly brought the family farm back to productivity.
In 1873, Spangler traveled to the Mudd farm, where Mudd and his wife welcomed him as the friend whom Mudd credited with saving his life while suffering with yellow fever at Fort Jefferson. Spangler lived with the Mudd family for about eighteen months, earning his keep by doing carpentry, gardening, and other farm chores, until his death on February 7, 1875.
Mudd always had an interest in politics. While in prison, he stayed abreast of political happenings through the newspapers he was sent. After his release, he became active again in community affairs. In 1874, he was elected chief officer of the local farmers association, the Bryantown Grange. In 1876, he was elected Vice President of the local Democratic Tilden-Hendricks presidential election committee. Tilden lost that year to Republican Rutherford B. Hayes in a hotly disputed election. The next year Mudd ran as a Democratic candidate for the Maryland House of Delegates, but was defeated by the popular Republican William Mitchell.
Mudd's ninth child, Mary Eleanor "Nettie" Mudd, was born in 1878. That same year, he and his wife temporarily took in a seven-year-old orphan named John Burke, one of 300 abandoned children sent to Maryland families from the New York City Foundling Asylum run by the Catholic Sisters of Charity. The Burke boy was permanently settled with farmer Ben Jenkins.
In 1880, the Port Tobacco Times reported that Mudd's barn containing almost eight thousand pounds of tobacco, two horses, a wagon, and farm implements was destroyed by fire.
Mudd was just 49 years old when he died of pneumonia on January 10, 1883. He is buried in the cemetery at St. Mary's Catholic Church in Bryantown, the same church where he once met with John Wilkes Booth.
Posthumous rehabilitation attempts.
Mudd's grandson, Dr. Richard Mudd, tried unsuccessfully to clear his grandfather's name from the stigma of aiding John Wilkes Booth. In 1951, he published "The Mudd Family of the United States", an encyclopedic two-volume history of the Mudd family in America, beginning with Thomas Mudd who arrived from England in 1665. A second edition was published in 1969. Following his death in 2002, Dr Mudd's papers, which detail his attempts to clear his grandfather's name, were donated to Georgetown University's Lauinger Library, where they are available to the public in the Special Collections Department.
Richard Mudd petitioned several successive presidents, receiving replies from presidents Jimmy Carter and Ronald Reagan. Carter, while sympathetic, responded that he had no authority under law to set aside the conviction; Reagan, that he had come to believe that Samuel Mudd was innocent of any wrongdoing. In 1992 Representatives Steny Hoyer and Thomas W. Ewing introduced House Bill 1885 to overturn the conviction, but it failed while in committee. Mudd then turned to the Army Board for Correction of Military Records, which recommended that the conviction be overturned on the basis that Mudd should have been tried by a civilian court. The recommendation was rejected by Acting Assistant Secretary of the Army William D. Clark. Several other legal venues were attempted, ending in 2003 when the U.S. Supreme Court refused the case, stating that the deadline for filing had been missed.
St. Catharine, also known as the Dr. Samuel A. Mudd House, was listed on the National Register of Historic Places in 1974.
Film, radio, and television.
Mudd's life was the subject of a 1936 John Ford-directed film "The Prisoner of Shark Island", based on a script by Nunnally Johnson. A episode of the radio series Encore Theatre, also titled The Prisoner of Shark Island, was broadcast in 1946. Another film, entitled "The Ordeal of Dr. Mudd," was made in 1980. It starred Dennis Weaver as Mudd, and espoused the point of view that Mudd was innocent of any conspiracy.
Roger Mudd, an Emmy Award-winning journalist and television host, is related to Samuel Mudd, though he is not a direct descendant, as has been mistakenly reported.
On the episode "Swiss Diplomacy" of the NBC Drama "The West Wing", first lady (and cardiac surgeon) Dr. Abby Bartlet commented on the duty of a physician to treat an injured patient despite potential legal repercussions. She responded to Mudd's treason conviction with "So that's the way it goes. You set the leg."
Samuel Mudd is sometimes given as the origin of the phrase "your name is mud", as in, for example, the 2007 film "". However, according to an online etymology dictionary, this phrase has its earliest known recorded instance in 1823, ten years before Mudd's birth, and is based on an obsolete sense of the word "mud" meaning "a stupid twaddling fellow".

</doc>
<doc id="28814" url="http://en.wikipedia.org/wiki?curid=28814" title="Secure Shell">
Secure Shell

Secure Shell, or SSH, is a cryptographic (encrypted) network protocol for initiating text-based shell sessions on remote machines in a secure way.
This allows a user to run commands on a machine's command prompt without them being physically present near the machine. It also allows a user to establish a secure channel over an insecure network in a client-server architecture, connecting an SSH client application with an SSH server. Common applications include remote command-line login and remote command execution, but any network service can be secured with SSH. The protocol specification distinguishes between two major versions, referred to as SSH-1 and SSH-2.
The most visible application of the protocol is for access to shell accounts on Unix-like operating systems, but it can also be used in a similar fashion on Windows.
SSH was designed as a replacement for Telnet and other insecure remote shell protocols such as the Berkeley rsh and rexec protocols, which send information, notably passwords, in plaintext, rendering them susceptible to interception and disclosure using packet analysis. The encryption used by SSH is intended to provide confidentiality and integrity of data over an unsecured network, such as the Internet, although files leaked by Edward Snowden indicate that the National Security Agency can sometimes decrypt SSH.
Definition.
SSH uses public-key cryptography to authenticate the remote computer and allow it to authenticate the user, if necessary. There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.
Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker's public key without validation will authorize an unauthorized attacker as a valid user.
Key management.
On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. This file is respected by ssh only if it is not writable by anything apart from the owner and root. When the public key is present on the remote end and the matching private key is present on the local end, typing in the password is no longer required (some software like Message Passing Interface (MPI) stack may need this password-less access to run properly). However, for additional security the private key itself can be locked with a passphrase.
The private key can also be looked for in standard places, and its full path can be specified as a command line setting (the option "-i" for ssh). The ssh-keygen utility produces the public and private keys, always in pairs.
SSH also supports password-based authentication that is encrypted by automatically generated keys. In this case the attacker could imitate the legitimate server side, ask for the password, and obtain it (man-in-the-middle attack). However, this is possible only if the two sides have never authenticated before, as SSH remembers the key that the server side previously used. The SSH client raises a warning before accepting the key of a new, previously unknown server. Password authentication can be disabled.
Usage.
SSH is typically used to log into a remote machine and execute commands, but it also supports tunneling, forwarding TCP ports and X11 connections; it can transfer files using the associated SSH file transfer (SFTP) or secure copy (SCP) protocols. SSH uses the client-server model.
The standard TCP port 22 has been assigned for contacting SSH servers.
An SSH client program is typically used for establishing connections to an SSH daemon accepting remote connections. Both are commonly present on most modern operating systems, including Mac OS X, most distributions of GNU/Linux, OpenBSD, FreeBSD, NetBSD, Solaris and OpenVMS. Notably, Windows is one of the few modern desktop/server OSs that does not include SSH by default. Proprietary, freeware and open source (e.g. PuTTY, and the version of openSSH which is part of Cygwin) versions of various levels of complexity and completeness exist. Native Linux file managers (e.g. Konqueror) can use the FISH protocol to provide a split-pane GUI with drag-and-drop. The open source Windows program WinSCP provides similar file management (synchronization, copy, remote delete) capability using PuTTY as a back-end. Both WinSCP and PuTTY are available packaged to run directly off of a USB drive, without requiring installation on the client machine. Setting up an SSH server in Windows typically involves installation (e.g. via installing Cygwin, or by installing a stripped down version of Cygwin with the SSH server).
SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine.
History and development.
Version 1.x.
In 1995, Tatu Ylönen, a researcher at Helsinki University of Technology, Finland, designed the first version of the protocol (now called SSH-1) prompted by a password-sniffing attack at his university network. The goal of SSH was to replace the earlier rlogin, TELNET and rsh protocols, which did not provide strong authentication nor guarantee confidentiality. Ylönen released his implementation as freeware in July 1995, and the tool quickly gained in popularity. Towards the end of 1995, the SSH user base had grown to 20,000 users in fifty countries.
In December 1995, Ylönen founded SSH Communications Security to market and develop SSH. The original version of the SSH software used various pieces of free software, such as GNU libgmp, but later versions released by SSH Communications Security evolved into increasingly proprietary software.
It is estimated that, as of 2000[ [update]], there were 2 million users of SSH.
Version 1.99.
In January 2006, well after version 2.1 was established, RFC 4253 specified that an SSH server which supports both 2.0 and prior versions of SSH should identify its protoversion as 1.99. This is not an actual version but a method to identify backward compatibility.
OpenSSH and OSSH.
In 1999, developers wanting a free software version to be available went back to the older 1.2.12 release of the original SSH program, which was the last released under an open source license. Björn Grönvall's OSSH was subsequently developed from this codebase. Shortly thereafter, OpenBSD developers forked Grönvall's code and did extensive work on it, creating OpenSSH, which shipped with the 2.6 release of OpenBSD. From this version, a "portability" branch was formed to port OpenSSH to other operating systems.
s of 2005[ [update]], OpenSSH was the single most popular SSH implementation, coming by default in a large number of operating systems. OSSH meanwhile has become obsolete. OpenSSH continues to be maintained and now supports both 1.x and 2.0 versions.
Version 2.x.
"Secsh" was the official Internet Engineering Task Force's (IETF) name for the IETF working group responsible for version 2 of the SSH protocol. In 2006, a revised version of the protocol, SSH-2, was adopted as a standard. This version is incompatible with SSH-1. SSH-2 features both security and feature improvements over SSH-1. Better security, for example, comes through Diffie–Hellman key exchange and strong integrity checking via message authentication codes. New features of SSH-2 include the ability to run any number of shell sessions over a single SSH connection. Due to SSH-2's superiority and popularity over SSH-1, some implementations such as Lsh and Dropbear support only the SSH-2 protocol.
Uses.
SSH is a protocol that can be used for many applications across many platforms including most Unix variants (Linux, the BSDs including Apple's OS X, & Solaris), as well as Microsoft Windows. Some of the applications below may require features that are only available or compatible with specific SSH clients or servers. For example, using the SSH protocol to implement a VPN is possible, but presently only with the OpenSSH server and client implementation.
File transfer protocols using SSH.
There are multiple mechanisms for transferring files using the Secure Shell protocols.
Architecture.
The SSH-2 protocol has an internal architecture (defined in RFC 4251) with well-separated layers, namely:
This open architecture provides considerable flexibility, allowing the use of SSH for a variety of purposes beyond a secure shell. The functionality of the transport layer alone is comparable to Transport Layer Security (TLS); the user-authentication layer is highly extensible with custom authentication methods; and the connection layer provides the ability to multiplex many secondary sessions into a single SSH connection, a feature comparable to BEEP and not available in TLS.
Enhancements.
These are intended for performance enhancements of SSH products:
Vulnerabilities.
Vulnerabilities in 1.x.
In 1998 a vulnerability was described in SSH 1.5 which allowed the unauthorized insertion of content into an encrypted SSH stream due to insufficient data integrity protection from CRC-32 used in this version of the protocol. A fix known as SSH Compensation Attack Detector was introduced into most implementations. Many of these updated implementations contained a new integer overflow vulnerability that allowed attackers to execute arbitrary code with the privileges of the SSH daemon, typically root.
In January 2001 a vulnerability was discovered that allows attackers to modify the last block of an IDEA-encrypted session. The same month, another vulnerability was discovered that allowed a malicious server to forward a client authentication to another server.
Since SSH-1 has inherent design flaws which make it vulnerable, it is now generally considered obsolete and should be avoided by explicitly disabling fallback to SSH-1. Most modern servers and clients support SSH-2.
Vulnerabilities in 2.x.
In November 2008, a theoretical vulnerability was discovered for all versions of SSH which allowed recovery of up to 32 bits of plaintext from a block of ciphertext that was encrypted using what was then the standard default encryption mode, CBC. The most straightforward solution is to use CTR mode instead of CBC mode, since this renders SSH resistant to the attack.
Unknown vulnerabilities.
On December 28, 2014 Der Spiegel published classified information leaked by whistleblower Edward Snowden which suggests that the National Security Agency may be able to decrypt some SSH traffic. The technical details associated with this attack were not released as a part of the publication.
Standards documentation.
The following RFC publications by the IETF "secsh" working group document SSH-2 as a proposed Internet standard.
It was later modified and expanded by the following publications.

</doc>
<doc id="28819" url="http://en.wikipedia.org/wiki?curid=28819" title="Generalissimo Francisco Franco is still dead">
Generalissimo Francisco Franco is still dead

"Generalissimo Francisco Franco is still dead" is a catchphrase that originated in 1975 during the first season of "NBC's Saturday Night" (now called "Saturday Night Live", or "SNL") and which mocked the weeks-long media reports of the Spanish dictator's impending death. It was one of the first catchphrases from the series to enter the general lexicon.
Origin.
The death of Spanish dictator Francisco Franco during the first season of "NBC's Saturday Night" served as the source of the phrase. Franco lingered near death for weeks before dying. On slow news days, United States network television newscasters sometimes noted that Franco was still alive, or not yet dead. The imminent death of Franco was a headline story on the NBC news for a number of weeks prior to his death on November 20, 1975.
After Franco's death, Chevy Chase, reader of the news on "NBC's Saturday Night"'s comedic news segment "Weekend Update", announced the dictator's death and read a quotation from Richard Nixon: "General Franco was a loyal friend and ally of the United States. He earned worldwide respect for Spain through firmness and fairness." As an ironic counterpoint to this, a picture was displayed behind Chase, showing Franco giving the fascist salute alongside Adolf Hitler.
In subsequent weeks Chase developed the joke into a parody of the earlier news coverage of Franco's illness, treating his death as the top story. "This breaking news just in", Chase would announce – "Generalissimo Francisco Franco is "still" dead!" Occasionally, Chase would change the wording slightly in order to keep the joke fresh, e.g. "Generalissimo Francisco Franco is still valiantly holding on in his fight to remain dead." The joke was sometimes combined with another running gag in which, rather than having a sign language interpreter visually presenting the news to aid the deaf, the show would provide assistance from Garrett Morris, "head of the New York School for the Hard of Hearing", whose "aid" involved cupping his hands around his mouth and shouting the news as Chase read it. The gag ran until early 1977.
The line was perceived as a slap at "NBC Nightly News" main anchor John Chancellor, who due to his background as a foreign correspondent, felt the network should weigh its news more heavily toward world events, and had kept Franco's deathwatch at the top of the headlines.
Legacy.
The phrase has remained in use in the decades since. James Taranto's "Best of the Web Today" column at OpinionJournal.com uses the phrase as a tag for newspaper headlines that indicate something is still happening when it should be obvious. On February 8, 2007, during Jack Cafferty's segment on CNN's "The Situation Room" on the day of the death of Anna Nicole Smith, he asked of CNN correspondent Wolf Blitzer "Is Anna Nicole Smith still dead, Wolf?" It was also used now and then on "NBC News Overnight" in the early 1980s, and Keith Olbermann occasionally used it on "Countdown".
"The Wall Street Journal" used the headline: "Generalísimo Francisco Franco Is Still Dead – And His Statues Are Next" (front page; March 2, 2009).
The practice of American television networks continually reporting that ailing world leaders are still alive remains widespread. Famous examples include Yasser Arafat in 2004, Pope John Paul II in 2005, Fidel Castro in late 2006 and early 2007, Hosni Mubarak in 2012 and Hugo Chávez in 2013.
Although "SNL"'s use is perhaps the most widely known, it is predated by the "'John Garfield Still Dead' syndrome," which originated as a result of extensive coverage in the wake of the actor John Garfield's death and funeral in 1952.
After a brief "in memoriam" during "SNL"'s 40th Anniversary Special on February 15, 2015, Bill Murray ended the segment with the famous phrase which "just came in from Spain".
References.
Notes
Citations

</doc>
<doc id="28820" url="http://en.wikipedia.org/wiki?curid=28820" title="Son House">
Son House

Eddie James "Son" House, Jr. (March 21, 1902 – October 19, 1988) was an American blues singer and guitarist, noted for his highly emotional style of singing and slide guitar playing.
After years of hostility to secular music, as a preacher, and for a few years also as a church pastor, he turned to blues performance at the age of 25. He quickly developed a unique style by applying the rhythmic drive, vocal power and emotional intensity of his preaching to the newly learned idiom. In a short career interrupted by a spell in Parchman Farm penitentiary, he developed to the point that Charley Patton, the foremost blues artist of the Mississippi Delta region, invited him to share engagements, and to accompany him to a 1930 recording session for Paramount Records.
Issued at the start of The Great Depression, the records did not sell and did not lead to national recognition. Locally, Son remained popular, and in the 1930s, together with Patton's associate, Willie Brown, he was the leading musician of Coahoma County. There he was a formative influence on Robert Johnson and Muddy Waters. In 1941 and 1942, House and the members of his band were recorded by Alan Lomax and John W. Work for Library of Congress and Fisk University. The following year, he left the Delta for Rochester, New York, and gave up music.
In 1964, a group of young record collectors discovered House, whom they knew of from his records issued by Paramount and by the Library of Congress. With their encouragement, he relearned his style and repertoire and enjoyed a career as an entertainer to young white audiences in the coffee houses, folk festivals and concert tours of the American folk music revival billed as a "folk blues" singer. He recorded several albums, and some informally taped concerts have also been issued as albums. Son House died in 1988.
In addition to his early influence on Robert Johnson and Muddy Waters, he became an inspiration to John Hammond, Alan Wilson (of Canned Heat), Bonnie Raitt, The White Stripes, Dallas Green and John Mooney.
Biography.
Early life.
The middle of three brothers, House was born in the hamlet of Lyon, north of Clarksdale, Mississippi and continued to live in the rural Mississippi Delta until his parents separated. His father, Eddie House, Sr., was a musician, playing the tuba in a band with his many brothers, and sometimes playing guitar. He was a church member, but also a drinker. This caused him to leave the church for a time, before giving up drink and becoming a deacon. Young Eddie House adopted the family concern with religion and churchgoing. He also absorbed the family love of music, but confined himself to singing, showing no interest in the family instrumental band, and feeling entirely hostile to the Blues on religious grounds.
Son's parents separated when he was about seven or eight. His mother took him to Tallulah, Louisiana, across the Mississippi River from Vicksburg, Mississippi. When Son was in his early teens, they moved to Algiers, New Orleans. Recalling these years, Son would later speak of his hatred of blues and his passion for churchgoing (he described himself as "churchy" and "churchified"). At fifteen, probably while living in Algiers, he began preaching sermons.
At the age of nineteen, while living in the Delta, he married an older woman from New Orleans named Carrie Martin. This was a significant step for House; he married in church and against family opposition. The couple moved to her hometown of Centreville, Louisiana to help run Carrie's father's farm. After a couple of years, feeling used and disillusioned, House recalls "I left her hanging on the gatepost, with her father tellin' me to come back so we could plow some more." In later years, House was still angry and said of Carrie "She wasn't nothin' but one of them New Orleans whores". At around the same time, probably 1922, Son's mother died.
House's resentment of farming extended to the many menial jobs he took in his young adult years. He moved around frequently, on one occasion taking off to East Saint Louis to work in a steel plant. The one job he enjoyed was on a Louisiana horse ranch, which later he celebrated by wearing a cowboy hat in his performances. He found some relief from constant manual labor when, following a conversion experience "getting religion" in his early twenties, he was accepted as a paid pastor, first in the Baptist Church, then in the Colored Methodist Episcopal Church. However, like his father before him, he fell into habits which conflicted with his calling — drinking like his father, and probably also womanizing. This led him after several years of conflict to "leave the church" — i.e. cease his full-time commitment — although he still felt the need to preach sermons from time to time.
Blues performer.
In 1927 at the age of 25, House underwent a change of musical perspective as rapid and dramatic as a religious conversion. In a hamlet south of Clarksdale, Son heard one of his drinking companions, either James McCoy or Willie Wilson (his recollections differed), playing bottleneck guitar, a style he had never heard before. He immediately changed his attitude to blues, bought a guitar from a musician called Frank Hoskins, and within weeks was playing with Hoskins, McCoy and Wilson. Two songs he learned from McCoy would later be among his best-known: "My Black Mama" and "Preachin' The Blues". Another source of inspiration was Reuben Lacy, a much better known performer who had recorded for Columbia Records in 1927 (no titles released) and for Paramount Records in 1928 (two titles released). In an astonishing short time, with only these four musicians as models, House developed to professional standard a blues style based on his religious singing and simple bottleneck guitar style.
After allegedly killing a man in self-defense, he spent time in prison in 1928 and 1929. The official story on the killing is that sometime around 1927 or 1928, he was playing in a juke joint when a man went on a shooting spree. Son was wounded in the leg, and shot the man dead. He received a 15-year sentence at the Mississippi State Penitentiary (Parchman Farm), of which he served two years. House credited his re-examination and release to an appeal by his family, but also spoke of the intervention by the influential white planter for whom they worked. The date of the killing and the duration of his sentence are unclear. House gave different accounts to different interviewers and searches by his biographer Daniel Beaumont found no details in the court records of Coahoma County or in the archive of the Mississippi Department of Corrections.
On his release in 1929 or early 1930, Son was strongly advised to leave Clarksdale and stay away. He walked to Jonestown and caught a train to the small town of Lula, Mississippi, sixteen miles north of Clarksdale, and eight miles from the blues hub of Helena, Arkansas. Coincidentally, the great star of Delta Blues, Charley Patton, was also in virtual exile in Lula, having been expelled from his base in the Dockery Plantation. With his partner Willie Brown, Patton dominated the local market for professional blues performance. Patton watched House busking when he arrived penniless at Lula station, but did not approach him. He then observed Son's showmanship attracting a crowd to the café and bootleg whiskey business of a woman called Sara Knight, and invited him to be a regular musical partner with him and Brown. Son formed a liaison with Knight, and both musicians profited from association with her bootlegging activities. The musical partnership is disputed by Patton's biographers Stephen Calt and Gayle Dean Wardlow. They consider that House's musicianship was too limited to play with Patton and Brown, who were also rumoured to be estranged at the time. They also cite one statement by House that he did not play for dances in Lula. Beaumont concludes that Son became a firm friend of Patton, traveling with him to gigs but playing separately.
Recording.
In 1930, Art Laibly of Paramount Records traveled to Lula to convince Patton to record several more sides in Grafton, Wisconsin. Along with Patton came House, Brown, and pianist Louise Johnson, who would all end up recording sides for the label. House recorded nine songs during that session, eight of which were released; but these were commercial failures, and House would not record again commercially for 35 years. House continued to play with Patton and Brown, even after Patton's death in 1934. During this time, House worked as a tractor driver for various plantations around the Lake Cormorant area.
Alan Lomax first recorded House for the Library of Congress in 1941. Willie Brown, mandolin player Fiddlin' Joe Martin, and harmonica player Leroy Williams played with House on these recordings. Lomax returned to the area in 1942, where he recorded House once more. He then faded from the public view, moving to Rochester, New York, in 1943, working as a railroad porter for the New York Central Railroad and as a chef.
Rediscovery.
In 1964, after a long search of the Mississippi Delta region by Nick Perls, Dick Waterman and Phil Spiro, he ended up being "rediscovered" in Rochester, NY. House had been retired from the music business for many years, and was unaware of the 1960s folk blues revival and international enthusiasm regarding his early recordings.
He subsequently toured extensively in the US and Europe and recorded for CBS Records. Like Mississippi John Hurt, he was welcomed into the music scene of the 1960s and played at the Newport Folk Festival in 1964, the New York Folk Festival in July 1965, and the October 1967 European tour of the American Folk Festival along with Skip James and Bukka White.
The young guitarist Alan Wilson (Canned Heat) was a fan of Son House. The producer John Hammond Sr asked Wilson, who was just 22 years old, to teach "Son House how to play like Son House," because Alan Wilson had such a good knowledge of the blues styles. The album "The Father of Delta Blues - The Complete 1965 Sessions" was the result. Son House played with Alan Wilson live. It can be heard on the album "John the Revelator: The 1970 London Sessions".
In the summer of 1970, House toured Europe once again, including an appearance at the Montreux Jazz Festival; a recording of his London concerts was released by Liberty Records. He also played at the two Days of Blues Festival in Toronto in 1974. On an appearance on the TV arts show "Camera Three", he was accompanied by blues guitarist Buddy Guy.
Ill health plagued House's later years and in 1974 he retired once again, and later moved to Detroit, Michigan, where he remained until his death from cancer of the larynx. He was buried at the Mt. Hazel Cemetery. Members of the Detroit Blues Society raised money through benefit concerts to put a monument on his grave. He had been married five times.
Discography.
78 RPM Recordings<br>
Recorded May 28, 1930, in Grafton, Wisconsin, for Paramount Records.
For Library of Congress/Fisk University
Recorded August 1941, Clack Store in Clack, Mississippi
Recorded 17 July 1942, Robbinsonville Mississippi
The music from both sessions and most of the recorded interviews have been reissued on LP and CD.
Singles
Other albums 

</doc>
<doc id="28822" url="http://en.wikipedia.org/wiki?curid=28822" title="Sex worker">
Sex worker

A sex worker is a person who works in the sex industry. The term is used in reference to all those in all areas of the sex industry including those who provide direct sexual services as well as the staff of such industries. Some sex workers are paid to engage in sexually explicit behavior which involve varying degrees of physical contact with clients (prostitutes, escorts, some but not all professional dominants); pornography models and actors engage in sexually explicit behavior which are filmed or photographed. Phone sex operators have sexually-oriented conversations with clients, and do auditive sexual roleplay. Other sex workers are paid to engage in live sexual performance, such as web cam sex and performers in live sex shows. Some sex workers perform erotic dances and other acts for an audience (striptease, Go-Go dancing, lap dancing, Neo-burlesque, and peep shows). Sexual surrogates often engage in sexual activity as part of therapy with their clients.
Thus, although the term sex worker is sometimes viewed as a synonym or euphemism for "prostitute", it is more general. Some people use the term to avoid invoking the stigma associated with the word "prostitute".
Etymology.
The term "sex worker" was coined in 1978 by sex worker activist Carol Leigh. Its use became popularized after publication of the anthology, "Sex Work: Writings By Women In The Sex Industry" in 1987, edited by Frédérique Delacoste and Priscilla Alexander. The term "sex worker" has since spread into much wider use, including in academic publications, by NGOs and labor unions, and by governmental and intergovernmental agencies, such as the World Health Organization. The term is listed in the Oxford English Dictionary and Merriam-Webster's Dictionary.
The term is strongly opposed, however, by many who are morally opposed to the sex industry, such as social conservatives, anti-prostitution feminists, and other prohibitionists. Such groups view prostitution variously as a crime or as victimization, and see the term "sex work" as legitimizing criminal activity or exploitation as a type of labor.
Sex work in practice.
Sex workers may be any gender and exchange sexual services or favors for money or other gifts. The motives of sex workers vary widely and can include debt, coercion, survival, or simply as a way to earn a living. These motives also align with varying climates surrounding sex work in different communities and cultures. In some cases, sex work is linked to tourism. Sex work can take the form of prostitution, stripping or lap dancing, performance in pornography, phone or internet sex, or any other exchange of sexual services for financial or material gain. The variety in the tasks encompassed by sex work lead to a large range in both severity and nature of risks that sex workers face in their occupations. Sex workers can act independently as individuals, work for a company or corporation, or work as part of a brothel. All of the above can be undertaken either by free choice or by coercion. Sex workers may also be hired to be companions on a trip or to perform sexual services within the context of a trip; either of these can be voluntary or forced labor. Transgender people are more likely than the general population to do sex work, particularly trans women and trans people of color. In a study of female Indian sex workers, illiteracy and lower social status were more prevalent than among the general female population.
Many studies struggle to gain demographic information about the prevalence of sex work, as many countries or cities have laws prohibiting prostitution or other sex work. In addition, sex trafficking, or forced sex work, is also difficult to quantify due to its underground and covert nature. In addition, finding a representative sample of sex workers in a given city can be nearly impossible because the size of the population itself is unknown. Maintaining privacy and confidentiality in research is also difficult because many sex workers may face prosecution and other consequences if their identities are revealed.
While demographic characteristics of sex workers vary by region and are hard to measure, some studies have attempted to estimate the composition of the sex work communities in various places. For example, one study of sex work in Tijuana, Mexico found that the majority of sex workers there are young, female and heterosexual. Many of these studies attempt to use smaller samples of sex workers and pimps in order to extrapolate about larger populations of sex workers. One report on the underground sex trade in the United States used known data on the illegal drug and weapon trades and interviews with sex workers and pimps in order to draw conclusions about the number of sex workers in eight American cities. However, studies like this one can come under scrutiny for a perceived emphasis on the activities and perspectives of pimps rather than those of sex workers themselves. Another criticism is that sex trafficking may not be adequately assessed in its relation to sex work in these studies.
Legal dimensions of sex work.
Depending on local law, sex workers' activities may be regulated, controlled, tolerated, or prohibited. In most countries, even those where sex work is legal, sex workers may be stigmatized and marginalized, which may prevent them from seeking legal redress for discrimination (e.g., racial discrimination by a strip club owner), non-payment by a client, assault or rape. Sex worker advocates have identified this as whorephobia.
The legality of different types of sex work varies within and between regions of the world. For example, while pornography is legal in the United States, prostitution is illegal in most parts of the US. However, in other regions of the world, both pornography and prostitution are illegal; in others, both are legal. In regions where sex work is illegal, advocates for sex workers' rights argue that the covert nature of illegal prostitution is a barrier to access to legal resources. However, some who oppose the legalization of prostitution argue that sex work is inherently exploitative and can never be legalized or practiced in a way that respects the rights of those who perform it.
Risk reduction.
Risk reduction in sex work is a highly debated topic. "Abolitionism" and "nonabolitionism" or "empowerment" are regarded as opposing ways in which risk reduction is approached. While abolitionism would call for an end to all sex work, empowerment would encourage the formation of networks among sex workers and enable them to prevent STIs and other health risks by communicating with each other. Both approaches aim to reduce rates of disease and other negative effects of sex work.
In addition, sex workers themselves have disputed the dichotomous nature of abolitionism and nonabolitionism, advocating instead a focus on sex workers' rights. In 1999, the Network of Sex Worker Projects claimed that "Historically, anti-trafficking measures have been more concerned with protecting 'innocent' women from becoming prostitutes than with ensuring the human rights of those in the sex industry. Penelope Saunders, a sex workers' rights advocate, claims that the sex workers' rights approach considers more of the historical context of sex work than either abolitionism or empowerment. In addition, Jo Doezema has written that the dichotomy of the voluntary and forced approaches to sex work has served to deny sex workers agency.
Health.
Sex workers are unlikely to disclose their work to healthcare providers. This can be due to embarrassment, fear of disapproval, or a disbelief that sex work can have effects on their health. The criminalization of sex work in many places can also lead to a reluctance to disclose for fear of being turned in for illegal activities. There are very few legal protections for sex workers due to criminalization; thus, in many cases, a sex worker reporting violence to a healthcare provider may not be able to take legal action against their aggressor.
Health risks of sex work relate primarily to sexually transmitted infections and to drug use. In one study, nearly 40% of sex workers who visited a health center reported illegal drug use. In general, transgender women sex workers have a higher risk of contracting HIV than male and female sex workers and transgender women who are not sex workers.
Condom use is one way to mitigate the risk of contracting an STI. However, negotiating condom use with one's clients and partners is often an obstacle to practicing safer sex. While there is not much data on rates of violence against sex workers, many sex workers do not use condoms due to the fear of resistance and violence from clients. Some countries also have laws prohibiting condom possession; this reduces the likelihood that sex workers will use condoms. Increased organization and networking among sex workers has been shown to increase condom use by increasing access to and education about STI prevention. Brothels with strong workplace health practices, including the availability of condoms, have also increased condom use among their workers.
Health Concerns of Exotic Dancers 
<br>
"Mental Health and Stigma" <br> In order to protect themselves from the stigma of sex work, many dancers resort to othering themselves. Othering involves constructing oneself as superior to one's peers, and the dancer persona provides an internal boundary that separates the “authentic” from the stripper self. This practice creates a lot of stress for the dancers, in turn leading many to resort to using drugs and alcohol to cope. Since it is so widespread, the use of drugs has become normalized in the exotic dance scene. <br>
Despite this normalization, passing as nonusers, or covering as users of less maligned drugs, is necessary. This is because strippers concurrently attribute a strong moral constitution to those that resist the drug atmosphere; it is a testament to personal strength and will power. It is also an occasion for dancers to ‘‘other’’ fellow strippers. Valorizing resistance to the drug space discursively positions ‘‘good’’ strippers against such a drug locale and indicates why dancers are motivated to closet hard drug use. <br>
Stigma causes strippers to hide their lifestyles from friends and family alienating themselves from a support system. Further, the stress of trying to hide their lifestyles from others due to fear of scrutiny affects the mental health of dancers. Stigma is a difficult area to address because it is more abstract, but it would be helpful to work toward normalizing sex work as a valid way of making a living. This normalization of sex work would relieve the stress many dancers experience increasing the likelihood that they will be open about their work. Being open will allow them access to a viable support system and reduce the othering and drug use so rampant in the sex industry. 
Forced sex work.
Forced sex work is when an individual enters into any sex trade due to coercion rather than by choice. Forced sex work increases the likelihood that a sex worker will contract HIV/AIDS or another sexually transmitted infection, particularly when an individual enters sex work before the age of 18. In addition, even when sex workers do consent to certain sex acts, they are often forced or coerced into others (often anal intercourse) by clients. Sex workers may also experience strong resistance to condom use by their clients, which may extend into a lack of consent by the worker to any sexual act performed in the encounter; this risk is magnified when sex workers are trafficked or forced into sex work.
Forced sex work often involves deception - workers are told that they can make a living and are then not allowed to leave. This deception can cause ill effects on the mental health of many sex workers. In addition, an assessment of studies estimates that between 40% and 70% of sex workers face violence within a year. Currently, there is little support for migrant workers in many countries, including those who have been trafficked to a location for sex.
Advocacy.
Sex worker's rights advocates argue that sex workers should have the same basic human and labour rights as other working people. For example, the Canadian Guild for Erotic Labour calls for the legalization of sex work, the elimination of state regulations that are more repressive than those imposed on other workers and businesses, the right to recognition and protection under labour and employment laws, the right to form and join professional associations or unions, and the right to legally cross borders to work. Advocacy for the interests of sex workers can come from a variety of sources, including non-governmental organizations, labor rights organizations, governments, or sex workers themselves.
Unionization of sex work.
The unionization of sex workers is a recent development. The first organization within the contemporary sex workers' rights movement was Call Off Your Old Tired Ethics (COYOTE), founded in 1973 in San Francisco, California. Many organizations in Western countries were established in the decade after the founding of COYOTE. Currently, a small number of sex worker unions exist worldwide. One of the largest is the International Union of Sex Workers, headquartered in the United Kingdom. The IUSW advocates for the rights of all sex workers, whether they chose freely or were coerced to enter the trade, and promotes policies that benefit the interests of sex workers both in the UK and abroad. Many regions are home to sex worker unions, including Latin America, Brazil, Canada, Europe, and Africa.
In unionizing, many sex workers face issues relating to communication and to the legality of sex work. Because sex work is illegal in many places where they wish to organize, it is difficult to communicate with other sex workers in order to organize. There is also concern with the legitimacy of sex work as a career and an activity that merits formal organizing, largely because of the sexism often present in sex work and the devaluation of sex work as not comparable to other paid labor and employment.
A factor affecting the unionization of sex work is that many sex workers belong to populations that historically have not had a strong representation in labor unions. While this unionization can be viewed as a way of empowering sex workers and granting them agency within their profession, it is also criticized as implicitly lending its approval to sexism and power imbalances already present in sex work. Unionization also implies a submission to or operation within the systems of capitalism, which is of concern to some feminists.
Unionizing Exotic Dancers <br>
"Independent contractor vs Employee"
<br>
Performers in general are problematic to categorize because they often exercise a high level of control over their work product, one characteristic of an independent contractor. Additionally, their work can be artistic in nature and often done on a freelance basis. Often, the work of performers does not possess the obvious attributes of employees such as regular working hours, places or duties. Consequently, employers misclassify them because they are unsure of their workers' status, or they purposely misclassify them to take advantage of independent contractors' low costs. Exotic dance clubs are one such employer that purposely misclassify their performers as independent contractors. <br>
There are additional hurdles in terms of self-esteem and commitment to unionize. On the most basic level, dancers themselves must have the desire to unionize for collective action. For those who wish not to conform to group activity or want to remain independent, a union may seem as controlling as club management since joining a union would obligate them to pay dues and abide by decisions made through majority vote, with or without their personal approval. <br>
In Lusty Lady case study, this strip club was the first all woman managed club to successfully unionize in 1996. Some of the working conditions they were able to address included “protest[ing] racist hiring practices, customers being allowed to videotape dancers without their consent via one-way mirrors, inconsistent disciplinary policies, lack of health benefits, and an overall dearth of job security.” Unionizing exotic dancers can certainly bring better work conditions and fair pay, but it is difficult to do at times because of their dubious employee categorization. Also, as is the case with many other unions, dancers are often reluctant to join them. This reluctance can be due many factors, ranging from the cost of joining a union to the dancers believing they do not need union support because they will not be exotic dancers for a long enough period of time to justify joining a union. 
Non-governmental organizations (NGOs).
NGOs often play a large role in outreach to sex workers, particularly in HIV and STI prevention efforts. However, NGO outreach to sex workers for HIV prevention is sometimes less coordinated and organized than similar HIV prevention programs targeted at different groups (such as men who have sex with men). This lack of organization may be due to the legal status of prostitution and other sex work in the country in question; in China, many sex work and drug abuse NGOs do not formally register with the government and thus run many of their programs on a small scale and discreetly.
While some NGOs have increased their programming to improve conditions within the context of sex work, these programs are criticized at times due to their failure to dismantle the oppressive structures of prostitution, particularly forced trafficking. Some scholars believe that advocating for rights within the institution of prostitution is not enough; rather, programs that seek to empower sex workers must empower them to leave sex work as well as improve their rights within the context of sex work.

</doc>
<doc id="28824" url="http://en.wikipedia.org/wiki?curid=28824" title="Stéphane Mallarmé">
Stéphane Mallarmé

Stéphane Mallarmé (; ]; 18 March 1842 – 9 September 1898), whose real name was Étienne Mallarmé, was a French poet and critic. He was a major French symbolist poet, and his work anticipated and inspired several revolutionary artistic schools of the early 20th century, such as Cubism, Futurism, Dadaism, and Surrealism.
Biography.
Stéphane Mallarmé was born in Paris. He worked as an English teacher and spent much of his life in relative poverty but was famed for his "salons", occasional gatherings of intellectuals at his house on the rue de Rome for discussions of poetry, art and philosophy. The group became known as "les Mardistes," because they met on Tuesdays (in French, "mardi"), and through it Mallarmé exerted considerable influence on the work of a generation of writers. For many years, those sessions, where Mallarmé held court as judge, jester, and king, were considered the heart of Paris intellectual life. Regular visitors included W.B. Yeats, Rainer Maria Rilke, Paul Valéry, Stefan George, Paul Verlaine, and many others.
On 10 August 1863, he married Maria Christina Gerhard. Their daughter, (Stéphanie Françoise) Geneviève Mallarmé, was born on 19 November 1864. Mallarmé died in Valvins (present-day Vulaines-sur-Seine) September 9, 1898.
Style.
Mallarmé's earlier work owes a great deal to the style of Charles Baudelaire who was recognised as the forerunner of literary Symbolism. Mallarmé's later "fin de siècle" style, on the other hand, anticipates many of the fusions between poetry and the other arts that were to blossom in the next century. Most of this later work explored the relationship between content and form, between the text and the arrangement of words and spaces on the page. This is particularly evident in his last major poem, "Un coup de dés jamais n'abolira le hasard" ('A roll of the dice will never abolish chance') of 1897.
Some consider Mallarmé one of the French poets most difficult to translate into English. The difficulty is due in part to the complex, multilayered nature of much of his work, but also to the important role that the sound of the words, rather than their meaning, plays in his poetry. When recited in French, his poems allow alternative meanings which are not evident on reading the work on the page. For example, Mallarmé's "Sonnet en '-yx"' opens with the phrase "ses purs ongles" ('her pure nails'), whose first syllables when spoken aloud sound very similar to the words "c'est pur son" ('it's pure sound'). Indeed, the 'pure sound' aspect of his poetry has been the subject of musical analysis and has inspired musical compositions. These phonetic ambiguities are very difficult to reproduce in a translation which must be faithful to the meaning of the words.
Influence.
General poetry.
Mallarmé's poetry has been the inspiration for several musical pieces, notably Claude Debussy's "Prélude à l'après-midi d'un faune" (1894), a free interpretation of Mallarmé's poem "L'après-midi d'un faune" (1876), which creates powerful impressions by the use of striking but isolated phrases. Maurice Ravel set Mallarmé's poetry to music in "Trois poèmes de Stéphane Mallarmé" (1913). Other composers to use his poetry in song include Darius Milhaud ("Chansons bas de Stéphane Mallarmé", 1917) and Pierre Boulez ("Pli selon pli", 1957–62).
Coldplay alludes to his final poem in their song, Vida la Vida. The second verse: "I used to roll the dice - Feel the fear in my enemy's eyes - Listen as the crowd would sing - 'Now the old king is dead! Long live the king!"' "The Viva la Vida" album art confirms the subject, featuring a reproduction of "Liberty Leading the People" by Eugène Delacroix, which commemorates the July Revolution of 1830.
Man Ray's last film, entitled "Les Mystères du Château de Dé (The Mystery of the Chateau of Dice)" (1929), was greatly influenced by Mallarmé's work, prominently featuring the line "A roll of the dice will never abolish chance".
Mallarmé is referred to extensively in the latter section of Joris-Karl Huysmans' À rebours, where Des Esseintes describes his fervour-infused enthusiasm for the poet: "These were Mallarmé's masterpieces and also ranked among the masterpieces of prose poetry, for they combined a style so magnificently that in itself it was as soothing as a melancholy incantation, an intoxicating melody, with irresistibly suggestive thoughts, the soul-throbs of a sensitive artist whose quivering nerves vibrate with an intensity that fills you with a painful ecstasy." [p. 198, Robert Baldick translation]
The critic and translator Barbara Johnson has emphasized Mallarmé's influence on twentieth-century French criticism and theory: "It was largely by learning the lesson of Mallarmé that critics like Roland Barthes came to speak of 'the death of the author' in the making of literature. Rather than seeing the text as the emanation of an individual author's intentions, structuralists and deconstructors followed the paths and patterns of the linguistic signifier, paying new attention to syntax, spacing, intertextuality, sound, semantics, etymology, and even individual letters. The theoretical styles of Jacques Derrida, Julia Kristeva, Maurice Blanchot, and especially Jacques Lacan also owe a great deal to Mallarmé's 'critical poem.'"
Un Coup de Dés.
It has been suggested that much of Mallarmé's work influenced the conception of hypertext, with his purposeful use of blank space and careful placement of words on the page, allowing multiple non-linear readings of the text. This becomes very apparent in his work "Un coup de dés".
On the publishing of "Un Coup de Dés" and its mishaps after the death of Mallarmé, consult the notes and commentary of Bertrand Marchal for his edition of the complete works of Mallarmé, Volume 1, Bibliothèque de la Pléiade, Gallimard 1998. To delve more deeply, consult "Igitur, Divagations, Un Coup de Dés," edited by Bertrand Marchal with a preface by Yves Bonnefoy, nfr Poésie/Gallimard.
Prior to 2004, "Un Coup de Dés" was never published in the typography and format conceived by Mallarmé. In 2004, 90 copies on vellum of a new edition were published by Michel Pierson et Ptyx. This edition reconstructs the typography originally designed by Mallarmé for the projected Vollard edition in 1897 and which was abandoned after the sudden death of the author in 1898. All the pages are printed in the format (38 cm by 28 cm) and in the typography chosen by the author. The reconstruction has been made from the proofs which are kept in the Bibliothèque Nationale de France, taking into account the written corrections and wishes of Mallarmé and correcting certain errors on the part of the printers Firmin-Didot.
A copy of this new edition can be consulted in the Bibliothèque François-Mitterrand. Copies have been acquired by the Bibliothèque littéraire Jacques-Doucet and University of California - Irvine, as well as by private collectors. A copy has been placed in the Museum Stéphane Mallarmé at Vulaines-sur-Seine, Valvins, where Mallarmé lived and died and where, according to Paul Valéry, he made his final corrections on the proofs prior to the projected printing of the poem.
The poet and visual artist Marcel Broodthaers created a purely graphical version of "Un coup de Dés", using Mallarmé's typographical layout but with the words replaced by black bars.
In 2015, published "A Roll of the Dice Will Never Abolish Chance", a definitive dual-language edition of the poem, translated by Robert Bononno and Jeff Clark (designer).
References and sources.
Hendrik Lücke: Mallarmé - Debussy. Eine vergleichende Studie zur Kunstanschauung am Beispiel von „L'Après-midi d'un Faune“. (= Studien zur Musikwissenschaft, Bd. 4). Dr. Kovac, Hamburg 2005, ISBN 3-8300-1685-9.

</doc>
<doc id="28825" url="http://en.wikipedia.org/wiki?curid=28825" title="Submarine">
Submarine

A submarine is a watercraft capable of independent operation underwater. It differs from a submersible, which has more limited underwater capability. The term most commonly refers to a large, crewed, autonomous vessel. It is also sometimes used historically or colloquially to refer to remotely operated vehicles and robots, as well as medium-sized or smaller vessels, such as the midget submarine and the wet sub. Used as an adjective in phrases such as "submarine cable", "submarine" means "under the sea". The noun "submarine" evolved as a shortened form of "submarine boat" (and is often further shortened to "sub"). For reasons of naval tradition, submarines are usually referred to as "boats" rather than as "ships", regardless of their size.
Although experimental submarines had been built before, submarine design took off during the 19th century, and they were adopted by several navies. Submarines were first widely used during World War I (1914–1918), and now figure in many navies large and small. Military usage includes attacking enemy surface ships (merchant and military), submarines, aircraft carrier protection, blockade running, ballistic missile submarines as part of a nuclear strike force, reconnaissance, conventional land attack (for example using a cruise missile), and covert insertion of special forces. Civilian uses for submarines include marine science, salvage, exploration and facility inspection/maintenance. Submarines can also be modified to perform more specialized functions such as search-and-rescue missions or undersea cable repair. Submarines are also used in tourism, and for undersea archaeology.
Most large submarines consist of a cylindrical body with hemispherical (and/or conical) ends and a vertical structure, usually located amidships, which houses communications and sensing devices as well as periscopes. In modern submarines, this structure is the "sail" in American usage, and "fin" in European usage. A "conning tower" was a feature of earlier designs: a separate pressure hull above the main body of the boat that allowed the use of shorter periscopes. There is a propeller (or pump jet) at the rear, and various hydrodynamic control fins. Smaller, deep diving and specialty submarines may deviate significantly from this traditional layout. Submarines change the amount of water and air in their ballast tanks to decrease buoyancy for submerging or increase it for surfacing.
Submarines have one of the widest ranges of types and capabilities of any vessel. They range from small autonomous examples and one or two-person vessels that operate for a few hours, to vessels that can remain submerged for six months—such as the Russian Typhoon class, the biggest submarines ever built. Submarines can work at greater depths than are survivable or practical for human divers. Modern deep-diving submarines derive from the bathyscaphe, which in turn evolved from the diving bell.
History.
Early submersibles.
According to a report attributed to Tahbir al-Tayseer in "Opusculum Taisnieri" published in 1562:
Two Greeks submerged and surfaced in the river Tagus near the City of Toledo several times in the presence of The Holy Roman Emperor Charles V, without getting wet and with the flame they carried in their hands still alight.
There are claims that Cosme García Sáez produced a viable submersible design predating those of Isaac Peral and Narciso Monturiol.
The first submersible of whose construction there exists reliable information was built in 1620 by Cornelius Drebbel, a Dutchman in the service of James I of England. It was created to the standards of the design outlined by English mathematician William Bourne. It was propelled by means of oars. The precise nature of the submarine type is a matter of some controversy; some claims suggest it was merely a bell towed by a boat.
By the mid-18th century, over a dozen patents for submarines/submersible boats had been granted in England. In 1747, Nathaniel Symons patented and built the first known working example of the use of a ballast tank for submersion. His design used leather bags that could fill with water to submerge the craft. A mechanism was used to twist the water out of the bags and cause the boat to resurface. In 1749, the Gentlemen's Magazine reported that a similar design had initially been proposed by Giovanni Borelli in 1680. By this point of development, further improvement in design necessarily stagnated for over a century, until new industrial technologies for propulsion and stability could be applied.
The first military submarine was the "Turtle" (1775), a hand-powered acorn-shaped device designed by the American David Bushnell to accommodate a single person. It was the first verified submarine capable of independent underwater operation and movement, and the first to use screws for propulsion. In 1800, France built a human-powered submarine designed by American Robert Fulton, the "Nautilus". The French eventually gave up on the experiment in 1804, as did the British when they later considered Fulton's submarine design.
In 1864, late in the American Civil War, the Confederate navy's "H. L. Hunley" became the first military submarine to sink an enemy vessel, the Union sloop-of-war "USS Housatonic". In the aftermath of its successful attack against the ship, the "Hunley" also sank, possibly because it was too close to its own exploding torpedo.
Mechanical power.
The first submarine not relying on human power for propulsion was the French "Plongeur" ("Diver"), launched in 1863, and using compressed air at 180 psi (1241 kPa).
The first air independent and combustion powered submarine was the "Ictineo II", designed by the Spanish intellectual, artist and engineer Narcís Monturiol. Launched in Barcelona in 1864, it was originally human-powered, but in 1867 Monturiol invented an air independent engine to power it underwater. The 14 m long craft was designed for a crew of two, performed dives of 30 m and remained underwater for two hours. Both the "Ictineo I" and "II" were double-hulled vessels that solved pressure and buoyancy control problems that had troubled and limited the functionality of earlier submarines.
The submarine became a potentially viable weapon with the development of the Whitehead torpedo, the first practical self-propelled or 'locomotive' torpedo. The spar torpedo that had been developed earlier by the Confederate navy was considered to be impracticable, as it was believed to have sunk both its intended target, and probably the "H. L. Hunley", the submarine that deployed it. The Whitehead torpedo was designed in 1866 by British engineer Robert Whitehead. His 'mine ship' was an 11-foot long, 14-inch diameter torpedo propelled by compressed air and carried an explosive warhead. The device had a speed of 7 knots (13 km/h) and it could hit a target 700 yards (640 m) away.
Discussions between the English clergyman and inventor George Garrett and the Swedish industrialist Thorsten Nordenfelt led to the first practical steam-powered submarines, armed with torpedoes and ready for military use. The first was the "Nordenfelt I", a 56 tonne, 19.5 metre (64 ft) vessel similar to Garret's ill-fated "Resurgam" (1879), with a range of 240 kilometres (150 mi, 130 nm), armed with a single torpedo, in 1885.
Like "Resurgam", "Nordenfelt I" operated on the surface by steam, then shut down its engine to dive. While submerged the submarine released pressure generated when the engine was running on the surface to provide propulsion for some distance underwater. Greece, fearful of the return of the Ottomans, purchased it. Nordenfelt then built "Nordenfelt II" ("Abdül Hamid") in 1886 and "Nordenfelt III" ("Abdül Mecid") in 1887, a pair of 30 metre (100 ft) submarines with twin torpedo tubes, for the Ottoman navy. "Abdülhamid" became the first submarine in history to fire a torpedo submerged. Nordenfelt's efforts culminated in 1887 with "Nordenfelt IV", which had twin motors and twin torpedoes. It was sold to the Russians, but proved unstable, ran aground, and was scrapped.
A reliable means of propulsion for the submerged vessel was only made possible in the 1880s with the advent of the necessary electric battery technology. The first electrically powered boats were built by James Franklin Waddington in England, Dupuy de Lôme and Gustave Zédé in France and Isaac Peral in Spain.
Waddington's "Porpoise" was similar in size to the "Resurgam" and its propulsion system consisted of 45 accumulator cells with a capacity of 660 ampere hours each. These were coupled in series to a motor driving a propeller at about 750 rpm, giving the ship a sustained speed of 8 mph for at least 8 hours. The boat was armed with two externally mounted torpedoes as well as a mine torpedo that could be detonated electronically. Although the boat performed well at trials, Waddington was unable to attract further contracts and went bankrupt.
The Spanish Peral Submarine was launched in 1888, and had 3 Schwarzkopf torpedoes 14 in (360 mm) and one torpedo tube in bow, new air systems, hull shape, propeller, and cruciform external controls anticipating much later designs. "Peral" was an all-electrical powered submarine. After two years of trials the project was scrapped by naval officialdom who cited, among other reasons, concerns over the range permitted by its batteries.
The "Gymnote" was launched by the French Navy in the same year. "Gymnote" was also an electrically powered and fully functional military submarine. It completed over 2,000 successful dives using a 204-cell battery. Although the "Gymnote" was scrapped for its limited range, its side hydroplanes became the standard for future submarine designs.
Modern submarines.
Submarines were not put into service for any widespread or routine use by navies until the early 1900s. The turn of the century marked a pivotal time in submarine development, and a number of important technologies appeared. A number of nations built and used submarines. Diesel electric propulsion became the dominant power system and equipment such as the periscope became standardized. Countries conducted many experiments on effective tactics and weapons for submarines, which led to their large impact in World War I.
The Irish inventor John Philip Holland built a model submarine in 1876 and a full scale one in 1878, followed by a number of unsuccessful ones. In 1896 he designed the Holland Type VI submarine. This vessel made use of internal combustion engine power on the surface and electric battery power for submerged operations. Launched on 17 May 1897 at Navy Lt. Lewis Nixon's Crescent Shipyard in Elizabeth, New Jersey, the Holland VI was purchased by the United States Navy on 11 April 1900, becoming the United States Navy's first commissioned submarine and renamed USS Holland.
Commissioned in June 1900, the French steam and electric "Narval" employed the now typical double-hull design, with a pressure hull inside the outer shell. These 200-ton ships had a range of over 100 miles (160 km) underwater. The French submarine "Aigrette" in 1904 further improved the concept by using a diesel rather than a gasoline engine for surface power. Large numbers of these submarines were built, with seventy-six completed before 1914.
The Royal Navy commissioned five Holland-class submarines from Vickers, Barrow-in-Furness, under licence from the Holland Torpedo Boat Company from 1901 to 1903. Construction of the boats took longer than anticipated, with the first only ready for a diving trial at sea on 6 April 1902. Although the design had been purchased entire from the US company, the actual design used was an untested improvement to the original Holland design using a new 180 hp petrol engine.
These types of submarines were first used during the Russo-Japanese War of 1904–05. Due to the blockade at Port Arthur, the Russians sent their submarines to Vladivostok, where by 1 January 1905 there were seven boats, enough to create the world's first "operational submarine fleet". The new submarine fleet sent out its first patrol on 14 February, usually lasting for about 24 hours. The first confrontation with Japanese warships occurred on 29 April 1905 when the Russian submarine "Som" was fired upon by Japanese torpedo boats, but then withdrew.
World War I.
Military submarines first made a significant impact in World War I. Forces such as the U-boats of Germany saw action in the First Battle of the Atlantic, and were responsible for the sinking of RMS "Lusitania", which was sunk as a result of unrestricted submarine warfare and is often cited among the reasons for the entry of the United States into the war.
At the very outbreak of war Germany had only 20 submarines immediately available for combat, although these included vessels of the diesel-engined U-19 class with the range (5,000 miles) and speed (eight knots) to operate effectively around the entire British coast. By contrast the Royal Navy had a total of 74 submarines, though of mixed effectiveness. In August 1914, a flotilla of ten U-boats sailed from their base in Heligoland to attack Royal Navy warships in the North Sea in the first submarine war patrol in history.
The U-boats' ability to function as practical war machines relied on new tactics, their numbers, and submarine technologies such as combination diesel-electric power system developed in the preceding years. More submersibles than true submarines, U-boats operated primarily on the surface using regular engines, submerging occasionally to attack under battery power. They were roughly triangular in cross-section, with a distinct keel to control rolling while surfaced, and a distinct bow. During World War I more than 5,000 Allied ships were sunk by U-boats.
World War II.
During World War II, Germany used submarines to devastating effect in the Second Battle of the Atlantic, where it attempted to cut Britain's supply routes by sinking more merchant ships than Britain could replace. (Shipping was vital to supply Britain's population with food, industry with raw material, and armed forces with fuel and armaments.) While U-boats destroyed a significant number of ships, the strategy ultimately failed. Although the U-boats had been updated in the interwar years, the major innovation was improved communications, encrypted using the famous Enigma cipher machine. This allowed for mass-attack naval tactics ("Rudeltaktik", commonly known as "wolfpack"), but was also ultimately the U-boats' downfall. By the end of the war, almost 3,000 Allied ships (175 warships, 2,825 merchantmen) had been sunk by U-boats.
The Imperial Japanese Navy operated the most varied fleet of submarines of any navy; including "Kaiten" crewed torpedoes, midget submarines (Type A Ko-hyoteki and "Kairyu" classes), medium-range submarines, purpose-built supply submarines and long-range fleet submarines. They also had submarines with the highest submerged speeds during World War II ("I-201"-class submarines) and submarines that could carry multiple aircraft ("I-400"-class submarines). They were also equipped with one of the most advanced torpedoes of the conflict, the oxygen-propelled Type 95. Nevertheless, despite their technical prowess, Japan had chosen to utilize its submarines for fleet warfare, and consequently were relatively unsuccessful, as warships were fast, maneuverable and well-defended compared to merchant ships.
The submarine force was the most effective anti-ship and anti-submarine weapon in the entire American arsenal. Submarines, though only about 2 percent of the U.S. Navy, destroyed over 30 percent of the Japanese Navy, including 8 aircraft carriers, 1 battleship and 11 cruisers. U.S. submarines also destroyed over 60 percent of the Japanese merchant fleet, crippling Japan's ability to supply its military forces and industrial war effort. Allied submarines in the Pacific War destroyed more Japanese shipping than all other weapons combined. This feat was considerably aided by the Imperial Japanese Navy's failure to provide adequate escort forces for the nation's merchant fleet.
During World War II, 314 submarines served in the U.S. Navy, of which nearly 260 were deployed to the Pacific. On December 7, 1941, 111 boats were in commission; 203 submarines from the "Gato", "Balao", and "Tench" classes were commissioned during the war. During the war, 52 US submarines were lost to all causes, with 48 directly due to hostilities. U.S. submarines sank 1,560 enemy vessels, a total tonnage of 5.3 million tons (55% of the total sunk).
The Royal Navy Submarine Service was used primarily in the classic British blockade role. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2 million tons of enemy shipping and 57 major warships, the latter including 35 submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when HMS "Venturer" engaged the "U864"; the "Venturer" crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost, the majority, 42, in the Mediterranean.
Post-war military models.
The first launch of a cruise missile (SSM-N-8 Regulus) from a submarine occurred in July 1953, from the deck of USS "Tunny", a World War II fleet boat modified to carry this missile with a nuclear warhead. "Tunny" and its sister boat, "Barbero", were the United States' first nuclear deterrent patrol submarines. In the 1950s, nuclear power partially replaced diesel-electric propulsion. Equipment was also developed to extract oxygen from sea water. These two innovations gave submarines the ability to remain submerged for weeks or months. Most of the naval submarines built since that time in the United States, the Soviet Union/Russia, Britain and France have been powered by nuclear reactors.
In 1959–1960, the first ballistic missile submarines were put into service by both the United States ("George Washington" class) and the Soviet Union (Golf class) as part of the Cold War nuclear deterrent strategy.
During the Cold War, the United States and the Soviet Union maintained large submarine fleets that engaged in cat-and-mouse games. The Soviet Union suffered the loss of at least four submarines during this period: "K-129" was lost in 1968 (which the CIA attempted to retrieve from the ocean floor with the Howard Hughes-designed ship "Glomar Explorer"), "K-8" in 1970, "K-219" in 1986, and "Komsomolets" in 1989 (which held a depth record among military submarines—1000 m). Many other Soviet subs, such as "K-19" (the first Soviet nuclear submarine, and the first Soviet sub to reach the North Pole) were badly damaged by fire or radiation leaks. The US lost two nuclear submarines during this time: USS "Thresher" due to equipment failure during a test dive while at its operational limit, and USS "Scorpion" due to unknown causes.
During the Indo-Pakistani War of 1971, the Pakistan Navy's "Hangor" sank the Indian frigate INS "Khukri". This was the first sinking by a submarine since World War II. In 1971, the "Ghazi", a "Tench"-class submarine on loan to Pakistan from the US, was sunk in the Indo-Pakistani War. It was the first submarine combat loss since World War II. In 1982 during the Falklands War, the Argentine cruiser "General Belgrano" was sunk by the British submarine HMS "Conqueror", the first sinking by a nuclear-powered submarine in war.
Usage.
Military.
Before and during World War II, the primary role of the submarine was anti-surface ship warfare. Submarines would attack either on the surface or submerged, using torpedoes or (on the surface) deck guns. They were particularly effective in sinking Allied transatlantic shipping in both World Wars, and in disrupting Japanese supply routes and naval operations in the Pacific in World War II.
Mine-laying submarines were developed in the early part of the 20th century. The facility was used in both World Wars. Submarines were also used for inserting and removing covert agents and military forces, for intelligence gathering, and to rescue aircrew during air attacks on islands, where the airmen would be told of safe places to crash-land so the submarines could rescue them. Submarines could carry cargo through hostile waters or act as supply vessels for other submarines.
Submarines could usually locate and attack other submarines only on the surface, although HMS "Venturer" managed to sink "U-864" with a four torpedo spread while both were submerged. The British developed a specialized anti-submarine submarine in WWI, the R class. After WWII, with the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to hunt each other effectively.
The development of submarine-launched ballistic missile and submarine-launched cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster bombs to nuclear weapons.
The primary defense of a submarine lies in its ability to remain concealed in the depths of the ocean. Early submarines could be detected by the sound they made. Water is an excellent conductor of sound (much better than air), and submarines can detect and track comparatively noisy surface ships from long distances. Modern submarines are built with an emphasis on stealth. Advanced propeller designs, extensive sound-reducing insulation, and special machinery help a submarine remain as quiet as ambient ocean noise, making them difficult to detect. It takes specialized technology to find and attack modern submarines.
Active sonar uses the reflection of sound emitted from the search equipment to detect submarines. It has been used since WWII by surface ships, submarines and aircraft (via dropped buoys and helicopter "dipping" arrays), but it gives away the position of the emitter and is susceptible to counter-measures.
A concealed military submarine is a real threat, and because of its stealth, can force an enemy navy to waste resources searching large areas of ocean and protecting ships against attack. This advantage was vividly demonstrated in the 1982 Falklands War when the British nuclear-powered submarine HMS "Conqueror" sank the Argentine cruiser "General Belgrano". After the sinking the Argentine Navy recognized that they had no effective defense against submarine attack, and the Argentine surface fleet withdrew to port for the remainder of the war, though an Argentine submarine remained at sea.
Civilian.
Although the majority of the world's submarines are military, there are some civilian submarines. They have a variety of uses, including tourism, exploration, oil and gas platform inspections, and pipeline surveys.
A fleet of 8 tourist submarines have been operated in Disneyland, California since it opened in 1954. They run around a closed course on rails, with small windows for the visitors to see the displays in the pool.
The first tourist submarine was launched in 1985, and by 1997 there were 45 of them operating around the world. Submarines with a crush depth in the range of 400 - are operated in several areas worldwide, typically with bottom depths around 100 to, with a carrying capacity of 50 to 100 passengers.
In a typical operation a surface vessel carries passengers to an offshore operating area, where passengers are exchanged with those of the submarine. The submarine then visits underwater points of interests, typically either natural or artificial reef structures. To surface safely without danger of collision the location of the submarine is marked with an air release and movement to the surface is coordinated by an observer in a support craft.
A recent development is the deployment of so-called narco submarines by South American drug smugglers to evade law enforcement detection. Although they occasionally deploy true submarines, most are self-propelled semi-submersibles, where a portion of the craft remains above water at all times. In September 2011, Colombian authorities seized a 16-meter-long submersible that could hold a crew of 5, costing about $2 million. The vessel belonged to FARC rebels and had the capacity to carry at least 7 tonnes of drugs.
Technology.
Submersion and trimming.
All surface ships, as well as surfaced submarines, are in a positively buoyant condition, weighing less than the volume of water they would displace if fully submerged. To submerge hydrostatically, a ship must have negative buoyancy, either by increasing its own weight or decreasing its displacement of water. To control their weight, submarines have ballast tanks, which can hold varying amounts of water and air.
For general submersion or surfacing, submarines use the forward and aft tanks, called Main Ballast Tanks, or MBTs, which are filled with water to submerge or with air to surface. Submerged, MBTs generally remain flooded, which simplifies their design, and on many submarines these tanks are a section of interhull space. For more precise and quick control of depth, submarines use smaller Depth Control Tanks, or DCTs – also called hard tanks, due to their ability to withstand higher pressure. The amount of water in depth control tanks can be controlled to change depth or to maintain a constant depth as outside conditions (chiefly water density) change. Depth control tanks may be located either near the submarine's center of gravity, or separated along the submarine body to prevent affecting trim.
When submerged, the water pressure on a submarine's hull can reach 4 MPa for steel submarines and up to 10 MPa for titanium submarines like "K-278 Komsomolets", while interior pressure remains relatively unchanged. This difference results in hull compression, which decreases displacement. Water density also marginally increases with depth, as the salinity and pressure are higher. This change in density incompletely compensates for hull compression, so buoyancy decreases as depth increases. A submerged submarine is in an unstable equilibrium, having a tendency to either fall or float to the surface. Keeping a constant depth requires continual operation of either the depth control tanks or control surfaces.
Submarines in a neutral buoyancy condition are not intrinsically trim-stable. To maintain desired trim, submarines use forward and aft trim tanks. Pumps can move water between these, changing weight distribution, creating a moment pointing the sub up or down. A similar system is sometimes used to maintain stability.
The hydrostatic effect of variable ballast tanks is not the only way to control the submarine underwater. Hydrodynamic maneuvering is done by several surfaces, which can be moved to create hydrodynamic forces when a submarine moves at sufficient speed. The stern planes, located near the propeller and normally horizontal, serve the same purpose as the trim tanks, controlling the trim, and are commonly used, while other control surfaces may not be present on many submarines. The fairwater planes on the sail and/or bow planes on the main body, both also horizontal, are closer to the centre of gravity, and are used to control depth with less effect on the trim.
When a submarine performs an emergency surfacing, all depth and trim methods are used simultaneously, together with propelling the boat upwards. Such surfacing is very quick, so the sub may even partially jump out of the water, potentially damaging submarine systems.
Hull.
Overview.
Modern submarines are cigar-shaped. This design, visible in early submarines (see below) is sometimes called a "teardrop hull". It reduces the hydrodynamic drag when submerged, but decreases the sea-keeping capabilities and increases drag while surfaced. Since the limitations of the propulsion systems of early submarines forced them to operate surfaced most of the time, their hull designs were a compromise. Because of the slow submerged speeds of those subs, usually well below 10 kt (18 km/h), the increased drag for underwater travel was acceptable. Late in World War II, when technology allowed faster and longer submerged operation and increased aircraft surveillance forced submarines to stay submerged, hull designs became teardrop shaped again to reduce drag and noise. On modern military submarines the outer hull is covered with a layer of sound-absorbing rubber, or anechoic plating, to reduce detection.
The occupied pressure hulls of deep diving submarines such as are spherical instead of cylindrical. This allows a more even distribution of stress at the great depth. A titanium frame is usually affixed to the pressure hull, providing attachment for ballast and trim systems, scientific instrumentation, battery packs, syntactic flotation foam, and lighting.
A raised tower on top of a submarine accommodates the periscope and electronics masts, which can include radio, radar, electronic warfare, and other systems including the snorkel mast. In many early classes of submarines (see history), the control room, or "conn", was located inside this tower, which was known as the "conning tower". Since then, the conn has been located within the hull of the submarine, and the tower is now called the "sail". The conn is distinct from the "bridge", a small open platform in the top of the sail, used for observation during surface operation.
"Bathtubs" are related to conning towers but are used on smaller submarines. The bathtub is a metal cylinder surrounding the hatch that prevents waves from breaking directly into the cabin. It is needed because surfaced submarines have limited freeboard, that is, they lie low in the water. Bathtubs help prevent swamping the vessel.
Single and double hulls.
Modern submarines and submersibles, as well as the oldest ones, usually have a single hull. Large submarines generally have an additional hull or hull sections outside. This external hull, which actually forms the shape of submarine, is called the outer hull ("casing" in the Royal Navy) or light hull, as it does not have to withstand a pressure difference. Inside the outer hull there is a strong hull, or pressure hull, which withstands sea pressure and has normal atmospheric pressure inside.
As early as World War I, it was realized that the optimal shape for withstanding pressure conflicted with the optimal shape for seakeeping and minimal drag, and construction difficulties further complicated the problem. This was solved either by a compromise shape, or by using two hulls; internal for holding pressure, and external for optimal shape. Until the end of World War II, most submarines had an additional partial cover on the top, bow and stern, built of thinner metal, which was flooded when submerged. Germany went further with the Type XXI, a general predecessor of modern submarines, in which the pressure hull was fully enclosed inside the light hull, but optimized for submerged navigation, unlike earlier designs that were optimized for surface operation.
After World War II, approaches split. The Soviet Union changed its designs, basing them on German developments. All post–World War II heavy Soviet and Russian submarines are built with a double hull structure. American and most other Western submarines switched to a primarily single-hull approach. They still have light hull sections in the bow and stern, which house main ballast tanks and provide a hydrodynamically optimized shape, but the main cylindrical hull section has only a single plating layer. The double hulls are being considered for future submarines in the United States to improve payload capacity, stealth and range.
Pressure hull.
The pressure hull is generally constructed of thick high-strength steel with a complex structure and high strength reserve, and is separated with watertight bulkheads into several compartments. There are also examples of more than two hulls in a submarine, like the Typhoon class, which has two main pressure hulls and three smaller ones for control room, torpedoes and steering gear, with the missile launch system between the main hulls.
The dive depth cannot be increased easily. Simply making the hull thicker increases the weight and requires reduction of onboard equipment weight, ultimately resulting in a "bathyscaphe". This is acceptable for civilian research submersibles, but not military submarines.
WWI submarines had hulls of carbon steel, with a 100 m maximum depth. During WWII, high-strength alloyed steel was introduced, allowing 200 m depths. High-strength alloy steel remains the primary material for submarines today, with 250 - depths, which cannot be exceeded on a military submarine without design compromises. To exceed that limit, a few submarines were built with titanium hulls. Titanium can be stronger than steel, lighter, and is not ferromagnetic, important for stealth. Titanium submarines were built by the Soviet Union, which developed specialized high-strength alloys. It has produced several types of titanium submarines. Titanium alloys allow a major increase in depth, but other systems must be redesigned to cope, so test depth was limited to 1000 m for the , the deepest-diving combat submarine. An Alfa-class submarine may have successfully operated at 1300 m, though continuous operation at such depths would produce excessive stress on many submarine systems. Titanium does not flex as readily as steel, and may become brittle during many dive cycles. Despite its benefits, the high cost of titanium construction led to the abandonment of titanium submarine construction as the Cold War ended. Deep diving civilian submarines have used thick acrylic pressure hulls.
The deepest deep-submergence vehicle (DSV) to date is "Trieste". On October 5, 1959 "Trieste" departed San Diego for Guam aboard the freighter "Santa Maria" to participate in "Project Nekton", a series of very deep dives in the Mariana Trench. On January 23, 1960, "Trieste" reached the ocean floor in the Challenger Deep (the deepest southern part of the Mariana Trench), carrying Jacques Piccard (son of Auguste) and Lieutenant Don Walsh, USN. This was the first time a vessel, manned or unmanned, had reached the deepest point in the Earth's oceans. The onboard systems indicated a depth of 11521 m, although this was later revised to 10916 m and more accurate measurements made in 1995 have found the Challenger Deep slightly shallower, at 10911 m.
Building a pressure hull is difficult, as it must withstand pressures at its required diving depth. When the hull is perfectly round in cross-section, the pressure is evenly distributed, and causes only hull compression. If the shape is not perfect, the hull is bent, with several points heavily strained. Inevitable minor deviations are resisted by stiffener rings, but even a one-inch (25 mm) deviation from roundness results in over 30 percent decrease of maximal hydrostatic load and consequently dive depth. The hull must therefore be constructed with high precision. All hull parts must be welded without defects, and all joints are checked multiple times with different methods, contributing to the high cost of modern submarines. (For example, each "Virginia"-class attack submarine costs US$2.6 billion, over US$200,000 per ton of displacement.)
Propulsion.
Originally, submarines were human propelled. The first mechanically driven submarine was the 1863 French "Plongeur", which used compressed air for propulsion. Anaerobic propulsion was first employed by the Spanish "Ictineo II" in 1864, which used a solution of zinc, manganese dioxide, and potassium chlorate to generate sufficient heat to power a steam engine, while also providing oxygen for the crew. A similar system was not employed again until 1940 when the German Navy tested a hydrogen peroxide-based system, the Walter turbine, on the experimental V-80 submarine and later on the naval "U-791" and type XVII submarines.
Until the advent of nuclear marine propulsion, most 20th-century submarines used batteries for running underwater and gasoline (petrol) or diesel engines on the surface, and for battery recharging. Early submarines used gasoline, but this quickly gave way to kerosene (paraffin), then diesel, because of reduced flammability. Diesel-electric became the standard means of propulsion. The diesel or gasoline engine and the electric motor, separated by clutches, were initially on the same shaft driving the propeller. This allowed the engine to drive the electric motor as a generator to recharge the batteries and also propel the submarine. The clutch between the motor and the engine would be disengaged when the submarine dived, so that the motor could drive the propeller. The motor could have multiple armatures on the shaft, which could be electrically coupled in series for slow speed and in parallel for high speed (these connections were called "group down" and "group up", respectively).
Electric transmission.
Diesel-electric.
Early submarines used a direct mechanical connection between the engine and propeller, switching between diesel engines for surface running, and battery-driven electric motors for submerged propulsion.
In 1928, the United States Navy's Bureau of Engineering proposed a diesel-electric transmission. Instead of driving the propeller directly while running on the surface, the submarine's diesel drove a generator that could either charge the submarine's batteries or drive the electric motor. This made electric motor speed independent of diesel engine speed, so the diesel could run at an optimum and non-critical speed. One or more diesel engines could be shut down for maintenance while the submarine continued to run on the remaining engine or battery power. The U.S. pioneered this concept in 1929, in the S-class submarines "S-3", "S-6", and "S-7". The first production submarines with this system were the "Porpoise"-class of the 1930s, and it was used on most subsequent US diesel submarines through the 1960s. No other navy adopted the system before 1945, apart from the Royal Navy's U-class submarines, though some submarines of the Imperial Japanese Navy used separate diesel generators for low speed running.
Other advantages of such an arrangement were that a submarine could travel slowly with the engines at full power to recharge the batteries quickly, reducing time on the surface or on snorkel. It was then possible to isolate the noisy diesel engines from the pressure hull, making the submarine quieter. Additionally, diesel-electric transmissions were more compact.
Air-independent propulsion.
During World War II, German Type XXI submarines (also known as "Elektroboote") were the first submarines designed to operate submerged for extended periods. Initially they were to carry hydrogen peroxide for long-term, fast air-independent propulsion, but were ultimately built with very large batteries instead. At the end of the War, the British and Soviets experimented with hydrogen peroxide/kerosene (paraffin) engines that could run surfaced and submerged. The results were not encouraging. Though the Soviet Union deployed a class of submarines with this engine type (codenamed Quebec by NATO), they were considered unsuccessful.
The United States also used hydrogen peroxide in an experimental midget submarine, X-1. It was originally powered by a hydrogen peroxide/diesel engine and battery system until an explosion of her hydrogen peroxide supply on 20 May 1957. X-1 was later converted to use diesel-electric drive.
Today several navies use air-independent propulsion. Notably Sweden uses Stirling technology on the "Gotland"-class and "Södermanland"-class submarines. The Stirling engine is heated by burning diesel fuel with liquid oxygen from cryogenic tanks. A newer development in air-independent propulsion is hydrogen fuel cells, first used on the German Type 212 submarine, with nine 34 kW or two 120 kW cells and soon to be used in the new Spanish S-80-class submarines.
Nuclear power.
Steam power was resurrected in the 1950s with a nuclear-powered steam turbine driving a generator. By eliminating the need for atmospheric oxygen, the length of time that a modern submarine could remain submerged was limited only by its food stores, as breathing air was recycled and fresh water distilled from seawater. More importantly, a nuclear submarine has unlimited range at top speed. This allows it to travel from its operating base to the combat zone in a much shorter time and makes it a far more difficult target for most anti-submarine weapons. Nuclear-powered submarines have a relatively small battery and diesel engine/generator powerplant for emergency use if the reactors must be shut down.
Nuclear power is now used in all large submarines, but due to the high cost and large size of nuclear reactors, smaller submarines still use diesel-electric propulsion. The ratio of larger to smaller submarines depends on strategic needs. The US Navy, French Navy, and the British Royal Navy operate only nuclear submarines, which is explained by the need for distant operations. Other major operators rely on a mix of nuclear submarines for strategic purposes and diesel-electric submarines for defense. Most fleets have no nuclear submarines, due to the limited availability of nuclear power and submarine technology.
Diesel-electric submarines have a stealth advantage over their nuclear counterparts. Nuclear submarines generate noise from coolant pumps and turbo-machinery needed to operate the reactor, even at low power levels. Some nuclear submarines such as the American "Ohio" class can operate with their reactor coolant pumps secured, making them quieter than electric subs. A conventional submarine operating on batteries is almost completely silent, the only noise coming from the shaft bearings, propeller, and flow noise around the hull, all of which stops when the sub hovers in mid-water to listen, leaving only the noise from crew activity. Commercial submarines usually rely only on batteries, since they never operate independently of a mother ship.
Several serious nuclear and radiation accidents have involved nuclear submarine mishaps. The reactor accident in 1961 resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. The accident in 1985 resulted in 10 fatalities and 49 other people suffered radiation injuries.
Alternative propulsion.
Oil-fired steam turbines powered the British K-class submarines, built during World War I and later, to give them the surface speed to keep up with the battle fleet. The K-class subs were not very successful, however.
Toward the end of the 20th century, some submarines—such as the British "Vanguard" class—began to be fitted with pump-jet propulsors instead of propellers. Though these are heavier, more expensive, and less efficient than a propeller, they are significantly quieter, providing an important tactical advantage.
Magnetohydrodynamic drive (MHD) was portrayed as the operating principle behind the titular submarine's nearly silent propulsion system in the film adaptation of "The Hunt for Red October". However, in the novel the "Red October" did not use MHD, but rather something more similar to the above-mentioned pump-jet.
Armament.
The success of the submarine is inextricably linked to the development of the torpedo, invented by Robert Whitehead in 1866. His invention is essentially the same now as it was 140 years ago. Only with self-propelled torpedoes could the submarine make the leap from novelty to a weapon of war. Until the perfection of the guided torpedo, multiple "straight-running" torpedoes were required to attack a target. With at most 20 to 25 torpedoes stored on board, the number of attacks was limited. To increase combat endurance most World War I submarines functioned as submersible gunboats, using their deck guns against unarmed targets, and diving to escape and engage enemy warships. The importance of guns encouraged the development of the unsuccessful Submarine Cruiser such as the French "Surcouf" and the Royal Navy's "X1" and M-class submarines. With the arrival of Anti-submarine warfare (ASW) aircraft, guns became more for defense than attack. A more practical method of increasing combat endurance was the external torpedo tube, loaded only in port.
The ability of submarines to approach enemy harbours covertly led to their use as minelayers. Minelaying submarines of World War I and World War II were specially built for that purpose. Modern submarine-laid mines, such as the British Mark 5 Stonefish and Mark 6 Sea Urchin, can deploy via a submarine's torpedo tubes.
After World War II, both the US and the USSR experimented with submarine-launched cruise missiles such as the SSM-N-8 Regulus and P-5 Pyatyorka. Such missiles required the submarine to surface to fire its missiles. They were the forerunners of modern submarine-launched cruise missiles, which can be fired from the torpedo tubes of submerged submarines, for example the US BGM-109 Tomahawk and Russian RPK-2 Viyuga and versions of surface to surface anti-ship missiles such as the Exocet and Harpoon, encapsulated for submarine launch. Ballistic missiles can also be fired from a submarine's torpedo tubes, for example missiles such as the anti-submarine SUBROC. With internal volume as limited as ever and the desire to carry heavier warloads, the idea of the external launch tube was revived, usually for encapsulated missiles, with such tubes being placed between the internal pressure and outer streamlined hulls.
The strategic mission of the SSM-N-8 and the P-5 were taken up by submarine-launched ballistic missile beginning with the US Navy's Polaris missile, and subsequently the Poseidon and Trident missiles.
Germany is working on the torpedo tube-launched short-range IDAS missile, which can be used against ASW helicopters, as well as surface ships and coastal targets.
Sensors.
A submarine can have a variety of sensors, depending on its missions. Modern military submarines rely almost entirely on a suite of passive and active sonars to find targets. Active sonar relies on an audible "ping" to generate echoes to reveal objects around the submarine. Active systems are rarely used, as doing so reveals the sub's presence. Passive sonar is a set of sensitive hydrophones set into the hull or trailed in a towed array, generally several hundred feet long. The towed array is the mainstay of NATO submarine detection systems, as it reduces the flow noise heard by operators. Hull mounted sonar is employed to back up the towed array, and in confined waters where obstacles could foul a towed array.
Submarines also carry radar equipment to detect surface ships and aircraft. Submarine captains are more likely to use radar detection gear than active radar to detect targets, as radar can be detected far beyond its own return range, revealing the submarine. Periscopes are rarely used, except for position fixes and to verify a contact's identity.
Civilian submarines, such as the or the Russian "Mir" submersibles, rely on small active sonar sets and viewing ports to navigate. The human eye cannot detect sunlight below about 300 ft underwater, so high intensity lights are used to illuminate the viewing area.
Navigation.
Early submarines had few navigation aids, but modern subs have a variety of navigation systems. Modern military submarines use an inertial guidance system for navigation while submerged, but drift error unavoidably builds over time. To counter this, the crew occasionally uses the Global Positioning System to obtain an accurate position. The periscope—a retractable tube with a prism system that provides a view of the surface—is only used occasionally in modern submarines, since the visibility range is short. The "Virginia"-class and "Astute"-class submarines use photonics masts rather than hull-penetrating optical periscopes. These masts must still be deployed above the surface, and use electronic sensors for visible light, infrared, laser range-finding, and electromagnetic surveillance. One benefit to hoisting the mast above the surface is that while the mast is above the water the entire sub is still below the water and is much harder to detect visibly or by radar.
Communication.
Military submarines use several systems to communicate with distant command centers or other ships. One is VLF (Very Low Frequency) radio, which can reach a submarine either on the surface or submerged to a fairly shallow depth, usually less than 250 ft. ELF (Extremely Low Frequency) can reach a submarine at much greater depths, but has a very low bandwidth and are generally used to call a submerged sub to a shallower depth where VLF signals can reach. A submarine also has the option of floating a long, buoyant wire antenna to a shallower depth, allowing VLF transmissions by a deeply submerged boat.
By extending a radio mast, a submarine can also use a "burst transmission" technique. A burst transmission takes only a fraction of a second, minimizing a submarine's risk of detection.
To communicate with other submarines, a system known as Gertrude is used. Gertrude is basically a sonar telephone. Voice communication from one submarine is transmitted by low power speakers into the water, where it is detected by passive sonars on the receiving submarine. The range of this system is probably very short, and using it radiates sound into the water, which can be heard by the enemy.
Civilian submarines can use similar, albeit less powerful systems to communicate with support ships or other submersibles in the area.
Life support systems.
With nuclear power or air-independent propulsion, submarines can remain submerged for months at a time. Conventional diesel submarines must periodically resurface or snorkel to recharge their batteries. Most modern military submarines generate breathing oxygen by electrolysis of water (using a device called an "Elektrolytic Oxygen Generator"). Atmosphere control equipment includes a CO2 scrubber, which uses an amine absorbent to remove the gas from air and diffuse it into waste pumped overboard. A machine that uses a catalyst to convert carbon monoxide into carbon dioxide (removed by the CO2 scrubber) and bonds hydrogen produced from the ship's storage battery with oxygen in the atmosphere to produce water, is also used. An atmosphere monitoring system samples the air from different areas of the ship for nitrogen, oxygen, hydrogen, R-12 and R-114 refrigerants, carbon dioxide, carbon monoxide, and other gases. Poisonous gases are removed, and oxygen is replenished by use of an oxygen bank located in a main ballast tank. Some heavier submarines have two oxygen bleed stations (forward and aft). The oxygen in the air is sometimes kept a few percent less than atmospheric concentration to reduce fire danger.
Fresh water is produced by either an evaporator or a reverse osmosis unit. The primary use for fresh water is to provide feed water for the reactor and steam propulsion plants. It is also available for showers, sinks, cooking and cleaning once propulsion plant needs have been met. Seawater is used to flush toilets, and the resulting "black water" is stored in a sanitary tank until it is blown overboard using pressurized air or pumped overboard by using a special sanitary pump. The method for blowing sanitaries overboard is difficult to operate, and the German Type VIIC boat "U-1206" was lost with casualties because of a mistake with the toilet. Water from showers and sinks is stored separately in "grey water" tanks, which are pumped overboard using the drain pump.
Trash on modern large submarines is usually disposed of using a tube called a Trash Disposal Unit (TDU), where it is compacted into a galvanized steel can. At the bottom of the TDU is a large ball valve. An ice plug is set on top of the ball valve to protect it, the cans atop the ice plug. The top breech door is shut, and the TDU is flooded and equalized with sea pressure, the ball valve is opened and the cans fall out assisted by scrap iron weights in the cans. The TDU is also flushed with seawater to ensure it is completely empty and the ball valve is clear before shutting the valve.
Crew.
A typical nuclear submarine has a crew of over 80. Non-nuclear boats typically have fewer than half as many. The conditions on a submarine can be difficult because crew members must work in isolation for long periods of time, without family contact. Submarines normally maintain radio silence to avoid detection. Operating a submarine is dangerous, even in peacetime, and many submarines have been lost in accidents.
Women.
Most navies prohibited women from serving on submarines, even after they had been permitted to serve on surface warships. The Royal Norwegian Navy became the first navy to allow female crew on its submarines in 1985. The Royal Danish Navy allowed female submariners in 1988. Others followed suit including the Swedish Navy (1989), the Royal Australian Navy (1998), the German Navy (2001) and the Canadian Navy (2002). In 1995, Solveig Krey of the Royal Norwegian Navy became the first female officer to assume command on a military submarine, HNoMS "Kobben".
On 8 December 2011, British Defence Secretary Philip Hammond announced that the UK's ban on women in submarines was to be lifted from 2013. Previously there were fears that women were more at risk from a build-up of carbon dioxide in the submarine. But a study showed no medical reason to exclude women, though pregnant women would still be excluded. Similar dangers to the pregnant woman and her fetus barred females from submarine service in Sweden 1983, when all other positions were made available for them in the Swedish Navy. Today, pregnant women are still not allowed to serve on submarines in Sweden. However, the policymakers thought that it was discriminatory with a general ban and demanded that females should be tried on their individual merits and have their suitability evaluated and compared to other candidates. Further, they noted that a female complying with such high demands is unlikely to become pregnant. In May 2014 it was announced that three women had become the RN's first female submariners.
Women have served on U.S. Navy surface ships since 1993, and as of 2011–2012[ [update]], began serving on submarines for the first time. Until presently, the Navy only allowed three exceptions to women being on board military submarines: female civilian technicians for a few days at most, women midshipmen on an overnight during summer training for Navy ROTC and Naval Academy, and family members for one-day dependent cruises. In 2009, senior officials, including then-Secretary of the Navy Ray Mabus, Joint Chief of Staff Admiral Michael Mullen, and Chief of Naval Operations Admiral Gary Roughead, began the process of finding a way to implement females onboard submarines. In 2011, the first classes of female submarine officers graduated from Naval Submarine School's Submarine Officer Basic Course (SOBC) at the Naval Submarine Base New London. Additionally, more senior ranking and experienced female supply officers from the surface warfare specialty attend SOBC as well, and proceed to fleet Ballistic Missile (SSBN) and Guided Missile (SSGN) submarines along with the new female submarine line officers beginning in late 2011/early 2012.
Both the U.S. and British navies operate nuclear-powered submarines that deploy for periods of six months or longer. Other navies that permit women to serve on submarines operate conventionally powered submarines, which deploy for much shorter periods—usually only for one or two months. Prior to the recent change by the U.S., no nation using nuclear submarines permitted women to serve on board.
In 2012, the U.S Navy announced that 2013 is the first year that women will serve on U.S. attack submarines. In 2013, U.S. Navy Secretary Ray Mabus said that the first women to join "Virginia"-class attack subs had been chosen. They were newly commissioned female officers scheduled to report to their subs in fiscal year 2015. On October 15, 2013, the US Navy announced that two submarines, the USS "Virginia" and the USS "Minnesota", would have female crew-members by January 2015. By late 2011 several women were assigned to USS "Wyoming".
In The Netherlands, it was announced in May 2015 that by 2025 in a new submarine class women will by allowed to serve on the submarines of the Dutch navy (Koninklijke Marine).
Abandoning the vessel.
In an emergency, submarines can transmit a signal to other ships. The crew can use Submarine Escape Immersion Equipment to abandon the submarine. The crew can prevent a lung injury from the pressure change known as pulmonary barotrauma by exhaling during the ascent. Following escape from a pressurized submarine, the crew is at risk of developing decompression sickness. An alternative escape means is via a Deep Submergence Rescue Vehicle that can dock onto the disabled submarine.
Bibliography.
1900/Russo-Japanese War 1904–1905
</dl>
External links.
Listen to this article ()
This audio file was created from a revision of the "Submarine" article dated 2006-01-11, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="28827" url="http://en.wikipedia.org/wiki?curid=28827" title="Second Epistle to the Thessalonians">
Second Epistle to the Thessalonians

The Second Epistle to the Thessalonians, often referred to as Second Thessalonians and written 2 Thessalonians, is a book from the New Testament of the Christian Bible. It is traditionally attributed to Paul, as it begins, "Paul, Silvanus, and Timothy, To the church of the Thessalonians" and ends, "I, Paul, write this greeting in my own hand, which is the distinguishing mark in all my letters." Modern scholarship is divided on whether Paul was the author or not; many scholars question its authenticity based on what they see as differences in style and theology between this and the First Epistle to the Thessalonians. Scholars who support its authenticity view it as having been written around 51-52 AD, shortly after the First Epistle. Those who do not view it as a later composition, from around 80 - 115 AD.
Composition.
The authenticity of this epistle is still in widespread dispute. As Ernest Best explains the problem, 
The structures of the two letters (to which Best refers) include opening greetings ("1 Thess." 1:1, "2 Thess." 1:1–2) and closing benedictions ("1 Thess." 5:28, "2 Thess." 3:16d-18) which frame two, balancing, sections (AA'). In "2 Thessalonians" these begin with similar successions of nine Greek words, at 1:3 and 2:13. The opening letter section (1:3–2:12) itself comprises two halves, 1:3–12 (where the introductory piece, A, is 1:3–5; the first development, B, is 1:6–10; and the paralleling and concluding development, B', is 1:11–12) and 2:1–12 (with pieces: A 2:1–4, B 2:5–7, B' 2:8–12). The second, balancing, letter section (2:13–3:16c) also comprises two halves: 2:13–3:5 (with pieces: A 2:13–14, B 2:15–17, B' 3:1–5) and 3:6-16c (with pieces: A 3:6–9, B 3:10–12, B' 3:13-16c). Of the twelve pieces in "2 Thessalonians" seven begin with 'brother' introductions. Of the eighteen pieces in "1 Thessalonians" fourteen begin with 'brother' introductions. In both letters the sections balance in size and focus, and in many details. In "2 Thessalonians", in 2:5 and 3:10, for example, there is a structural balance of the use of 'when I was with you...' and 'when we were with you...'.
Support for Authenticity.
Of the books in the New Testament suspected of pseudepigraphy, 2 Thessalonians has the most evidence to support its authenticity. While Paul's authorship of "Second Thessalonians" has been questioned more often than his authorship of 1 Thess., there is more evidence from early Christian writers for his authorship of Second Thessalonians than for First Thessalonians. The epistle was included in Marcion's canon and the Muratorian fragment; it was mentioned by name by Irenaeus, and quoted by Ignatius, Justin, and Polycarp.
G. Milligan observed that a church which possessed an authentic letter of Paul would be unlikely to accept a fake addressed to them. So also Colin Nicholl who has put forward a substantial argument for the authenticity of Second Thessalonians. He points out that 'the pseudonymous view is ... more vulnerable than most of its advocates conceded. ... The lack of consensus regarding a date and destination ... reflects a dilemma for this position: on the one hand, the date needs to be early enough for the letter to be have been accepted as Pauline ... [on] the other hand, the date and destination need to be such that the author could be confident that no contemporary of 1 Thessalonians ... could have exposed 2 Thessalonians as a ... forgery.'.
Another scholar who argues for the authenticity of this letter is Jerome Murphy-O'Connor. Admitting that there are stylistic problems between "Second Thessalonians" and "First Thessalonians", he argues that part of the problem is due to the composite nature of "First Thessalonians" (Murphy-O'Connor is only one of many scholars who argue that the current text of "Second Thessalonians" is the product of merging two or more authentic letters of Paul.) Once the text of this interpolated letter is removed and the two letters compared, Murphy-O'Connor asserts that this objection is "drastically weakened", and concludes, "The arguments against the authenticity of 2 Thessalonians are so weak that it is preferable to accept the traditional ascription of the letter to Paul."
Those who believe Paul was the author of "Second Thessalonians" also note how Paul drew attention to the authenticity of the letter by signing it himself: "I, Paul, write this greeting with my own hand, which is how I write in every letter.". Bruce Metzger writes, "Paul calls attention to his signature, which was added by his own hand as a token of genuineness to every letter of his (3:17)."
Other scholars who hold to authenticity include Beale, Green, Jones, Morris, and Witherington.
Opposed to Authenticity.
At least as early as 1798, when J.E.C. Schmidt published his opinion, Paul's authorship of this epistle was questioned. More recent challenges to this traditional belief came from scholars such as William Wrede in 1903 and Alfred Loisy in 1933, who challenged the traditional view of the authorship.
Many today believe that it was not written by Paul but by an associate or disciple after his death, representing what they believed was his message. See, for example, Ehrman, Gaventa, Smiles, Schnelle, Boring, and Kelly. Norman Perrin observes, "The best understanding of 2 Thessalonians ... is to see it as a deliberate imitation of 1 Thessalonians, updating the apostle's thought.". Perrin bases this claim on his hypothesis that prayer at the time usually treated God the Father as ultimate judge, rather than Jesus.
Background.
Thessalonica was the second city in Europe where Paul helped to create an organized Christian community. At some point after the first letter was sent, probably soon, some of the Thessalonicans grew concerned over whether those who had died would share in the parousia. This letter was written in response to this concern. The problem then arises, as Raymond Brown points out, whether this letter is an authentic writing of Paul written by one of his followers in his name.
If this letter is authentic, then it might have been written soon after Paul's first letter to this community—or possibly years later. Brown notes that Paul "most likely visited Thessalonica several times in his journeys to Macedonia". However, if the letter is not authentic, Brown notes that "in some ways interpretation becomes more complex." Brown believes that the majority of scholars who advocate pseudonymity would place it towards the end of the first century, the same time that Revelation was written. These scholars emphasize the appearance of "man of sin" in the second chapter of this letter, whether this personage is identified with the Antichrist of 1 John and Revelation, or with a historical person like Caligula.
Content.
The traditional view is that the second epistle to the Thessalonians was probably written from Corinth not many months after the first.
Biblical commentator and pastor John Macarthur writes, "The emphasis is on how to maintain a church with an effective testimony in proper response to sound eschatology and obedience to the truth."
Paul opens the letter praising this church for their faithfulness and perseverance in the face of persecution:
"We ought always to give thanks to God for you, brethren, as is only fitting, because your faith is greatly enlarged, and the love of each one of you toward one another grows ever greater; therefore, we ourselves speak proudly of you among the churches of God for your perseverance and faith in the midst of all your persecutions and afflictions which you endure" (2 Thess 1:3–5 [NASB]).
The letter contains a whole chapter regarding the second advent of Christ, among other themes and instructions.
From the inference of 2:1–2, the Thessalonians were faced with a false teaching, saying that Christ had already returned. This error is corrected in chapter 2 (), where Paul tells the Thessalonians that a great tribulation must occur before Christ's return. Seeing as how this series of events has not yet happened, his argument reads, Christ cannot have returned yet. He then expresses thanks that his readers were the elect of God, chosen for salvation and saved by His grace through faith, and thus not susceptible to the deception of the "Great Apostasy," (2 Thess 2:13–14) first mentioned here as is the "Katechon" (2 Thess 2:6–7).
The letter continues by encouraging the Thessalonian church to stand firm in their faith, and to "keep away from every brother who leads an unruly life and not according to the tradition which you received from us... do not associate with him, so that he will be put to shame. Yet do not regard him as an enemy, but admonish him as a brother" (2 Thess 3:6–7, 14–15).
Paul ends this letter by saying, "I, Paul, write this greeting with my own hand, and this is a distinguishing mark in every letter; this is the way I write. The grace of our Lord Jesus Christ be with you all" (2 Thess 3:17–18). Macarthur writes, "Paul added an identifying signature (cf. 1 Cor. 16:21; Col. 4:18) so his readers could be sure he was truly the author."
A passage from this book reading "For even when we were with you, this we commanded you, that if any would not work, neither should he eat", (), was later adapted by Vladimir Lenin as an adage of the Soviet Union, He who does not work, neither shall he eat.
Notes.
 incorporates text from a publication now in the public domain: 
External links.
Online translations of the Second Epistle to the Thessalonians:
Exegetical Papers on Second Thessalonians:

</doc>
<doc id="28828" url="http://en.wikipedia.org/wiki?curid=28828" title="Poetry slam">
Poetry slam

 A poetry slam is a competition at which poets read or recite original work. These performances are then judged on a numeric scale by previously selected members of the audience.
History.
American poet Marc Smith is credited with starting the poetry slam at the Get Me High Lounge in Chicago in November 1984. In July 1986, the slam moved to its permanent home, the Green Mill Jazz Club. In August 1988, the first poetry slam was held in New York City at the Nuyorican Poet's Cafe and hosted by Bob Holman. In 1990, the first National Poetry Slam took place in Fort Mason, San Francisco, involving a team from Chicago, a team from San Francisco, and an individual poet from New York. s of 2014[ [update]], the National Poetry Slam featured 72 certified teams, culminating in five days of competition. Da Poetry Lounge was started in Hollywood, CA in 1998.
Slams have spread all over the world, with slam competitions in Ireland, Nepal, Canada, Germany, France, Sweden, Austria, Israel, Ukraine, Russia, Switzerland, the Netherlands, Portugal, United Kingdom, Australia, New Zealand, Singapore, Hungary, the Czech Republic, Poland, Serbia, Bosnia, Denmark, Latvia, South Korea, Japan, India, Greece, Spain, Mexico, Madagascar, Azerbaijan, Morocco, Moldova and Brazil.
Format.
In a poetry slam, members of the audience are chosen by an M.C. or host to act as judges for the event. In the national slam, there are five judges, but smaller slams generally have three. After each poet performs, each judge awards a score to that poem. Scores generally range between zero and ten. The highest and lowest score are dropped, giving each performance a rating between zero and thirty points.
Before the competition begins, the host will often bring up a "sacrificial" poet, whom the judges will score in order to calibrate their judging.
A single round at a standard slam consists of performances by all eligible poets. Most slams last multiple rounds, and many involve the elimination of lower-scoring poets in successive rounds. An elimination rubric might run 8-4-2; eight poets in the first round, four in the second, and two in the last. Some slams do not eliminate poets at all. The Green Mill usually runs its slams with 6 poets in the first round.
The Portland Poetry Slam (Portland, OR) takes a different approach; it uses the 8-4-2 three-round rubric, but the poets go head-to-head in separate bouts within the round. Instead of five judges giving points, the audience decides who moves on to the next round by a loud, enthusiastic popular vote.
Props, costumes, and music are always forbidden in slams, distinguishing this category from its immediate predecessor, performance poetry. Additionally, most slams enforce a time limit of three minutes (and a grace period of ten seconds), after which a poet's score may be docked according to how long the poem exceeded the limit.
Competition types.
In an "Open Slam," the most common slam type, competition is open to all who wish to compete, given the number of slots available. In an "Invitational Slam," by contrast, only those invited to do so may compete.
Poetry Slam, Inc. holds several National and World Poetry Slams, including the Individual World Poetry Slam, The National Poetry Team Slam and The Women of the World Poetry Slam. The current (2013) IWPS champion is Ed Mabrey. Ed Mabrey is the only three-time IWPS champion in the history of the event. The current (2013) National Poetry Slam Team champions are Slam New Orleans (SNO), who have won the competition for the second year in a row. The current (2014) Women of the World Poetry Slam Champion is Dominique Christina.
A "Theme Slam" is one in which all performances must conform to a specified theme, genre, or formal constraint. Themes may include Nerd, Erotica, Queer, Improv, or other conceptual limitations. In theme slams, poets can sometimes be allowed to break "traditional" slam rules. For instance, they sometimes allow performance of work by another poet (e.g. the "Dead Poet Slam", in which all work must be by a deceased poet). They can also allow changes on the restrictions on costumes or props (e.g. the Swedish "Triathlon" slams that allow for a poet, musician, and dancer to all take the stage at the same time), changing the judging structure (e.g. having a specific guest judge), or changing the time limits (e.g. a "1-2-3" slam with three rounds of one minute, two minutes, and three minutes, respectively).
Although theme slams may seem restricting in nature, slam venues frequently use them to advocate participation by particular and perhaps underrepresented demographics (which vary from slam to slam), like younger poets and women.
Poetics.
Poetry slams can feature a broad range of voices, styles, cultural traditions, and approaches to writing and performance.
Some poets are closely associated with the vocal delivery style found in hip-hop music and draw heavily on the tradition of dub poetry, a rhythmic and politicized genre belonging to black and particularly West Indian culture. Others employ an unrhyming narrative formula. Some use traditional theatric devices including shifting voices and tones, while others may recite an entire poem in ironic monotone. Some poets use nothing but their words to deliver a poem, while others stretch the boundaries of the format, tap-dancing or beatboxing or using highly choreographed movements.
What is a dominant / successful style one year may be passe the next. Cristin O'Keefe Aptowicz, slam poet and author of "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam", was quoted in an interview on the Best American Poetry blog as saying:
One of the goals of a poetry slam is to challenge the authority of anyone who claims absolute authority over literary value. No poet is beyond critique, as everyone is dependent upon the goodwill of the audience. Since only the poets with the best cumulative scores advance to the final round of the night, the structure assures that the audience gets to choose from whom they will hear more poetry. Audience members furthermore become part of each poem's presence, thus breaking down the barriers between poet/performer, critic, and audience. Bob Holman, a poetry activist and former slammaster of the Nuyorican Poets Cafe, once called the movement "the democratization of verse." In 2005, Holman was also quoted as saying:
Slam critics.
In an interview in the "Paris Review," literary critic Harold Bloom said about slamming:
I can’t bear these accounts I read in the "Times" and elsewhere of these poetry slams, in which various young men and women in various late-spots are declaiming rant and nonsense at each other. The whole thing is judged by an applause meter which is actually not there, but might as well be. This isn’t even silly; it is the death of art. Kip Fulbeck, who teaches Spoken Word at the University of California, Santa Barbara, said, "I don’t like the idea of competition and art being put together. I think it often distills the quality of work down to a caricature of itself. Seeing poetry slams often reminds me of watching American Idol. You’ve got a series of judges, an audience that comes in looking for a certain shtick that they want to see and that’s what they’re going to cheer for."
Poet and lead singer of King Missile, John S. Hall has also long been a vocal opponent, taking issue with such factors as its inherently competitive nature and what he considers its lack of stylistic diversity. In his 2005 interview in "Words In Your Face: A Guided Tour Through Twenty Years of the New York City Poetry Slam," he recalls seeing his first slam, at the Nuyorican Poets Café:
The poet Tim Clare offers a "for and against" account of the phenomenon in "Slam: A Poetic Diaialogue".
Ironically, slam poetry movement founder Marc Smith has been critical of the commercially successful Def Poetry television and Broadway live stage shows produced by Russell Simmons, decrying it as "an exploitive entertainment [program that] diminished the value and aesthetic of performance poetry".
Academia and slam.
As of 2011, four poets who have competed at National Poetry Slam have won National Endowment of the Arts (NEA) Fellowships for Literature:
A number of poets belong to both academia and slam: as noted above Jeffrey McDaniel slammed on several poetry slam teams, and has since published several books and currently teaches at Sarah Lawrence College; Patricia Smith, a four-time national slam champion, went on to win several prestigious literary awards, including being nominated for the 2008 National Book Award, and being inducted into the International Literary Hall of Fame for Writers of African Descent in 2006; Bob Holman founded the Nuyorican Poetry Slam has taught for years at the New School, Bard, Columbia and NYU; Craig Arnold won the Yale Series of Younger Poets Competition and has competed at slams; Kip Fulbeck, a professor of Art at the University of California, Santa Barbara competed in slam in the early-1990s and initiated the first spoken word course to be taught as part of a college art program's core curriculum; and poet/academics such as Michael Salinger, Felice Belle, Javon Johnson was national slam poetry champion in 2003 and 2004, wrote his dissertation on slam poetry and recently published an article in text and performance quarterly about black masculinity and sexism in the slam communitiy, Susan Somers-Willett wrote the book The Cultural Politics of Slam poetry, exploring the relationships between slam, identity, and politics, Robbie Q. Telfer, Phil West, Ragan Fox writes about his ten years of experience as "a gay slam poet", Marie Fleischmann Timbreza, and Karyna McGlynn have devoted much attention to the merging of the poetry slam community and the academic community in their respective works.
Some renowned poets have competed in slams, with less successful results. Henry Taylor, winner of the 1985 Pulitzer Prize for Poetry, competed in the 1997 National Poetry Slam as an individual and placed 75th out of 150.
While slam poetry has often been ignored in traditional higher learning institutions, it slowly is finding its way into courses and programs of study. For example, at Berklee College of Music, in Boston, Slam Poetry is now available as a Minor course of study.
Youth poetry slam movement.
Slam poetry has found popularity as a form of self-expression among many teenagers. The World Poetry Bout Association sponsored the earliest slam poetry workshops for teenagers, through its "Poetry Education Project" in Taos, New Mexico, in the early 1990s. The first statewide competition for high school students was held at Taos High School in 1993, with the top teams and individual participants awarded plaques. Members of Taos' competitive teams earned athletic letters annually up until 2008. [cf. The Taos News, Taos, NM, articles, 1993 to present.]Youth Speaks
, a non-profit literary organization founded in 1996 by James Kass, patterned the slam competitions at the annual Brave New Voices festival after that seminal Taos event. Youth Speaks serves as one of the largest youth poetry organizations in America, offering opportunities for youth ages 13–19 to express their ideas on paper and stage.
, a non-profit literary organization founded in 1996 by James Kass, serves as one of the largest youth poetry organizations in America, offering opportunities for youth ages 13–19 to express their ideas on paper and stage.
Another group offering opportunities in education and performance to teens is out of New York City, formerly known as Youth Speaks New York. URBAN WORD NYC holds the largest youth slam in NYC annually, with over 500 young people. The non-profit organization provides free workshops for inner-city youth ran by Hip-Hop poet and mentor, Michael Cirelli.
 (YCA) provides workshops, mentoring, and competition opportunities to youth in the Chicago area. Every year YCA presents Louder Than A Bomb, the world's largest team-based youth slam and subject of a documentary by the same name.
The youth poetry slam movement was the focus of a documentary film series produced by HBO and released in 2009. It featured poets from Youth Speaks, Urban Word, Louder than a Bomb and other related youth poetry slam organizations.
In a 2005 interview, one of slam's best known poets Saul Williams praised the youth poetry slam movement, explaining:
In 2012 more than 12,000 young people took part in an England-wide youth slam "Shake the Dust", organised by Apples and Snakes as part of the London 2012 Festival.

</doc>
<doc id="28829" url="http://en.wikipedia.org/wiki?curid=28829" title="Sestina">
Sestina

A sestina (Old Occitan: "cledisat" ]; also known as "sestine", "sextine", "sextain") is a fixed verse form consisting of six stanzas of six lines each, normally followed by a three-line envoi. The words that end each line of the first stanza are used as line endings in each of the following stanzas, rotated in a set pattern.
The invention of the form is usually attributed to 12th-century troubadour Arnaut Daniel; after spreading to continental Europe, it first appeared in English in 1579, though sestinas were rarely written in Britain until the end of the 19th century. It remains a popular poetic form, and many continue to be written by contemporary poets.
History.
The oldest-known sestina is "Lo ferm voler qu'el cor m'intra", written around 1200 by Arnaut Daniel, a troubadour of Aquitanian origin; he refers to it as "cledisat", meaning, more or less, "interlock". Hence, Daniel is generally considered the form's inventor, though it has been suggested that he may only have innovated an already existing form. Nevertheless, two other original troubadouric sestinas are known, the best known being "Eras, pus vey mon benastruc" by Guilhem Peire Cazals de Caortz; there are also two contrafacta built on the same end-words, the best known being "Ben gran avoleza intra" by Bertran de Born. These early sestinas were written in Old Occitan; the form started spilling into Italian with Dante in the 13th century; by the 15th, it was used in Portuguese by Luís de Camões.
The involvement of Dante and Petrarch in establishing the sestina form, together with the contributions of others in the country, account for its classification as an Italian verse form—despite not originating there. The result was that the sestina was re-imported into France from Italy in the 16th century. Pontus de Tyard was the first poet to attempt the form in French, and the only one to do so prior to the 19th century; he introduced a partial rhyme scheme in his sestina.
English.
The first appearance of the sestina in English print is "Ye wastefull woodes", comprising lines 151–89 of the August Æglogue in Edmund Spenser's "Shepherd's Calendar", published in 1579. It is in unrhymed iambic pentameter, but the order of end-words in each stanza is non-standard – ending 123456, 612345, etc. – each stanza promoting the previous final end-word to the first line, but otherwise leaving the order intact; the envoi order is (1) 2 / (3) 4 / (5) 6. This scheme was set by the Spaniard Gutierre de Cetina.
Although they appeared in print later, Philip Sidney's three sestinas may have been written earlier, and are often credited as the first in English. The first published (toward the end of Book I of "The Countess of Pembroke's Arcadia", 1590) is the double sestina "Ye Goatherd Gods". In this variant the standard end-word pattern is repeated for twelve stanzas, ending with a three-line envoi, resulting in a poem of 75 lines. Two others were published in subsequent editions of the "Arcadia". The second, "Since wailing is a bud of causeful sorrow", is in the "standard" form. Like "Ye Goatherd Gods" it is written in unrhymed iambic pentameter and uses exclusively feminine endings, reflecting the Italian "endecasillabo". The third, "Farewell, O sun, Arcadia's clearest light", is the first rhyming sestina in English: it is in iambic pentameters and follows the standard end-word scheme, but rhymes ababcc in the first stanza (the rhyme scheme necessarily changes in each subsequent stanza, a consequence of which is that the 6th stanza is in rhyming couplets). Sidney uses the same envoi structure as Spenser. William Drummond of Hawthornden published two sestinas (which he called "sextains") in 1616, which copy the form of Sidney's rhyming sestina. After this, there is an absence of notable sestinas for over 250 years, with John Frederick Nims noting that, "... there is not a single sestina in the three volumes of the Oxford anthologies that cover the seventeenth, eighteenth and nineteenth centuries."
In the 1870s, there was a revival of interest in French forms, led by Andrew Lang, Austin Dobson, Edmund Gosse, W. E. Henley, John Payne, and others. The earliest sestina of this period is Algernon Charles Swinburne's "Sestina". It is in iambic pentameter rhyming ababab in the first stanza; each stanza begins by repeating the previous end-words 6 then 1, but the following 4 lines repeat the remaining end-words "ad lib"; the envoi is (1) 4 / (2) 3 / (5) 6. In the same volume ("Poems and Ballads, Second Series", 1878) Swinburne introduces a "double sestina" ("The Complaint of Lisa") that is unlike Sidney's: it comprises 12 stanzas of 12 iambic pentameter lines each, the first stanza rhyming abcabdcefedf. Similar to his "Sestina", each stanza first repeats end-words 12 then 1 of the previous stanza; the rest are "ad lib". The envoi is (12) 10 / (8) 9 / (7) 4 / (3) 6 / (2) 1 / (11) 5.
From the 1930s, a revival of the form took place across the English-speaking world, led by poets such as W. H. Auden, and the 1950s were described as the "age of the sestina" by James E. B. Breslin. "Sestina: Altaforte" by Ezra Pound and "Paysage moralisé" by W. H. Auden are distinguished modern examples of the sestina. The sestina remains a popular closed verse form, and many sestinas continue to be written by contemporary poets; notable examples include "The Guest Ellen at the Supper for Street People" by David Ferry and "IVF" by Kona Macphee.
Form.
Although the sestina has been subject to many revisions throughout its development there remain several features that define the form. The sestina is composed of six stanzas of six lines (sixains), followed by a stanza of three lines (a tercet). There is no rhyme within the stanzas; instead the sestina is structured through a recurrent pattern of the words that end each line, a technique known as "lexical repetition".
In the original form composed by Daniel, each line is of ten syllables, except the first of each stanza which are of seven. The established form, as developed by Petrarch and Dante, was in hendecasyllables. Since then, changes to the line length have been a relatively common variant, such that Stephen Burt has written: "sestinas, as the form exists today, [do not] require expertise with inherited meter ...".
The pattern that the line-ending words follow is often explained if the numbers 1 to 6 are allowed to stand for the end-words of the first stanza. Each successive stanza takes its pattern based upon a bottom-up pairing of the lines of the preceding stanza (i.e., last and first, then second-from-last and second, then third-from-last and third). Given that the pattern for the first stanza is 123456, this produces 615243 in the second stanza.
Another way of visualising the pattern of line-ending words for each stanza is by the procedure known as "retrogradatio cruciata", which may be rendered as "backward crossing". The second stanza can be seen to have been formed from three sets of pairs (6–1, 5–2, 4–3), or two triads (1–2–3, 4–5–6). The 1–2–3 triad appears in its original order, but the 4–5–6 triad is reversed and superimposed upon it.
The pattern of the line-ending words in a sestina is represented both numerically and alphabetically in the following table:
The sixth stanza is followed by a tercet that is known variably by the French term envoi, the Occitan term tornada, or, with reference to its size in relation to the preceding stanzas, a "half-stanza". It consists of three lines that include all six of the line-endings words of the preceding stanzas. This should take the pattern of 2–5, 4–3, 6–1 (numbers relative to the first stanza); the first end-word of each pair can occur anywhere in the line, while the second must end the line. However, the end-word order of the envoi is no longer strictly enforced.
<poem>
"Sestina"
"Time to plant tears" (6), says the almanac (5).
The grandmother (2) sings to the marvelous stove (4)
and the child (3) draws another inscrutable house (1).
</poem>
”
 The envoi to "Sestina"; the repeated words are emboldened and labelled.<br>Elizabeth Bishop (1965)
The sestina has been subject to some variations, with changes being made to both the size and number of stanzas, and also to individual line length. A "double sestina" is the name given to either: two sets of six six-line stanzas, with a three-line envoy (for a total of 75 lines), or twelve twelve-line stanzas, with a six-line envoy (for a total of 150 lines). Examples of either variation are rare; "Ye Goatherd Gods" by Philip Sidney is a notable example of the former variation, while "The Complaint of Lisa" by Algernon Charles Swinburne is a notable example of the latter variation. In the former variation, the original pattern of line-ending words, i.e. that of the first stanza, recurs in the seventh stanza, and thus the entire change of pattern occurs twice throughout. In the second variation, the pattern of line-ending words returns to the starting sequence in the eleventh stanza; thus it does not, unlike the "single" sestina, allow for every end-word to occupy each of the stanza ends; end-words 5 and 10 fail to couple between stanzas.
A "tritina" is a contraction of the sestina to three stanzas of three lines (tercets), with a one-line envoy. The order of the line-ending words follows the same pattern as that of the sestina, so that they appear: 123, 312, 231 with the envoy as 123.
<poem>
"Sestina"
In fair Provence, the land of lute and rose,
Arnaut, great master of the lore of love,
First wrought sestines to win his lady's heart,
For she was deaf when simpler staves he sang,
And for her sake he broke the bonds of rhyme,
And in this subtler measure hid his woe.
'Harsh be my lines,' cried Arnaut, 'harsh the woe
My lady, that enthorn'd and cruel rose,
Inflicts on him that made her live in rhyme!'
But through the metre spake the voice of Love,
And like a wild-wood nightingale he sang
Who thought in crabbed lays to ease his heart.
</poem>
”
 First two stanzas of the sestina "Sestina" <br>Edmund Gosse (1879)
Effect.
The structure of the sestina, which demands adherence to a strict and arbitrary order, produces several effects within a poem. Stephen Burt notes that, "The sestina has served, historically, as a complaint", its harsh demands acting as "signs for deprivation or duress". The structure can enhance the subject matter that it orders; in reference to Elizabeth Bishop's "A Miracle for Breakfast", David Caplan suggests that the form's "harshly arbitrary demands echo its subject's". Nevertheless, the form's structure has been criticised; Paul Fussell considers the sestina to be of "dubious structural expressiveness" when composed in English and, irrespective of how it is used, "would seem to be [a form] that gives more structural pleasure to the contriver than to the apprehender."
Margaret Spanos highlights "a number of corresponding levels of tension and resolution" resulting from the structural form, including: structural, semantic and aesthetic tensions. She believes that the aesthetic tension, which results from the ""conception" of its mathematical completeness and perfection", set against the ""experiences" of its labyrinthine complexities" can be resolved in the apprehension of the "harmony of the whole."
The strength of the sestina, according to Stephen Fry, is the "repetition and recycling of elusive patterns that cannot be quite held in the mind all at once". For Shanna Compton, these patterns are easily discernible by newcomers to the form; she says that: "Even someone unfamiliar with the form's rules can tell by the end of the second stanza ... what's going on ...".
External links.
</dl>

</doc>
<doc id="28830" url="http://en.wikipedia.org/wiki?curid=28830" title="Song">
Song

A song is an artistic form of expression based on sound, generally considered a single (and often standalone) work of music with distinct and fixed pitches, pattern, and form. It can be wordless or with words. Written words created specifically for music or for which music is specifically created, are called lyrics. If poetry, a pre-existing poem is set to composed music, that is an art song. Songs that are sung on repeated pitches without distinct contours and patterns that rise and fall are called chants. Chants may be slightly or highly ornamented. Songs may be sung by one singer or more than one, by a singer with background singers who accompany with minor parts, or by a group. Songs composed for personal use, for casual group activities, in simple style, are referred to as folk songs. Songs that are composed for professional entertainers are called popular songs, in that they do not require an education to necessarily appreciate, and that they have broad appeal to many people. These songs are composed with the intent to earn money by professional composers and lyricists. Art songs are composed by trained classical composers for concert performance. Songs may also appear in plays, musical plays, stage shows of any form, and within operas.
A song may be for a solo singer, a duet, trio, or larger ensemble involving more voices, although the term is generally not used for large vocal forms including opera and oratorio. Songs with more than one voice to a part are considered choral works. Songs can be broadly divided into many different forms, depending on the criteria used. One division is between "art songs", "pop songs", and "folk songs". Other common methods of classification are by purpose (sacred vs secular), by style (dance, ballad, Lied, etc.), or by time of origin (Renaissance, Contemporary, etc.).
Cultural types.
Art songs.
Art songs are songs created for performance by classical artists, usually with accompaniment, although they can be sung solo. Art songs require strong vocal technique, understanding of language, diction and poetry for interpretation. Though such singers may also perform popular or folk songs on their programs, these characteristics and the use of poetry are what distinguish art songs from popular songs. Art songs are a tradition from most European countries, and now other countries with classical music traditions. German-speaking communities use the term art song ("Kunstlied") to distinguish so-called "serious" compositions from folk song ("Volkslied"). The lyrics are often written by a poet or lyricist and the music separately by a composer. Art songs may be more formally complicated than popular or folk songs, though many early Lieder by the likes of Franz Schubert are in simple strophic form. The accompaniment of European art songs is considered as an important part of the composition. Some art songs are so revered that they take on characteristics of national identification.
Art songs emerge from the tradition of singing love songs of someone present or someone ideal or imaginary. They also come from religious songs. The troubadours and bards of Europe began the documented tradition of romantic songs, continued by the Elizabethan lutenists. Some of the earliest art songs are found in the music of Henry Purcell. The tradition of the romance, a love song with a flowing accompaniment, often in triple meter, entered opera in the 19th century, and spread from there throughout Europe. It spread into popular song and became one of the underpinnings of Popular Songs. While a romance generally has a simple accompaniment, art songs tend to have complicated, sophisticated accompaniments that underpin, embellish, illustrate or provide contrast to the voice. Sometimes it is the accompaniment that has the melody, while the voice sings a more dramatic part. 
Folk songs.
Folk songs are songs of often anonymous origin (or are public domain) that are transmitted orally. They are frequently a major aspect of national or cultural identity. Art songs often approach the status of folk songs when people forget who the author was. Folk songs are also frequently transmitted non-orally (that is, as sheet music), especially in the modern era. Folk songs exist in almost every culture. Popular songs may eventually become folk songs by the same process of detachment from its source. Folk songs are more-or-less in the public domain by definition, though there are many folk song entertainers who publish and record copyrighted original material. This tradition led also to the singer-songwriter style of performing, where an artist has written confessional poetry or personal statements and sings them set to music, most often a guitar.
Genres of popular songs are many, including torch songs, ballads, novelty songs, anthems, rock songs, blues, soul, and other commercial genres. Folk songs include ballads, lullabyes, plaints, love songs, mourning songs, dance songs, work songs, ritual songs, and many more.

</doc>
<doc id="28833" url="http://en.wikipedia.org/wiki?curid=28833" title="Sir Gawain and the Green Knight">
Sir Gawain and the Green Knight

Sir Gawain and the Green Knight (Middle English: "Sir Gawayn and þe Grene Knyȝt") is a late 14th-century Middle English chivalric romance. It is one of the best known Arthurian stories, and is of a type known as the "beheading game". The Green Knight is interpreted by some as a representation of the Green Man of folklore and by others as an allusion to Christ. Written in stanzas of alliterative verse, each of which ends in a rhyming bob and wheel, it draws on Welsh, Irish and English stories, as well as the French chivalric tradition. It is an important poem in the romance genre, which typically involves a hero who goes on a quest which tests his prowess, and it remains popular to this day in modern English renderings from J. R. R. Tolkien, Simon Armitage and others, as well as through film and stage adaptations.
It describes how Sir Gawain, a knight of King Arthur's Round Table, accepts a challenge from a mysterious "Green Knight" who challenges any knight to strike him with his axe if he will take a return blow in a year and a day. Gawain accepts and beheads him with his blow, at which the Green Knight stands up, picks up his head and reminds Gawain of the appointed time. In his struggles to keep his bargain Gawain demonstrates chivalry and loyalty until his honour is called into question by a test involving Lady Bertilak, the lady of the Green Knight's castle.
The poem survives in a single manuscript, the "Cotton Nero A.x.", which also includes three religious narrative poems: "Pearl", "Purity" and "Patience". All are thought to have been written by the same unknown author, possibly Cameron of Sutherland, dubbed the "Pearl Poet" or "Gawain Poet", since all four are written in a North West Midland dialect of Middle English.
Synopsis.
In Camelot on New Year's Day, King Arthur's court is exchanging gifts and waiting for the feasting to start when the king asks first to see or hear of an exciting adventure. At this a gigantic figure, entirely green in appearance and riding a green horse, rides unexpectedly into the hall. He wears no armour but bears an axe in one hand and a holly bough in the other. Refusing to fight anyone there on the grounds that they are all too weak to take him on, he insists he has come for a friendly "Christmas game": someone is to strike him once with his axe on condition that the Green Knight may return the blow in a year and a day. The splendid axe will belong to whoever takes him on. Arthur himself is prepared to accept the challenge when it appears no other knight will dare, but Sir Gawain, youngest of Arthur's knights and his nephew, begs for the honour instead. The giant bends and bares his neck before him and Gawain neatly beheads him in one stroke. However, the Green Knight neither falls nor falters, but instead reaches out, picks up his severed head and remounts, holding up his bleeding head to Queen Guinevere while its writhing lips remind Gawain that the two must meet again at the Green Chapel. He then rides away. Gawain and Arthur admire the axe, hang it up as a trophy and encourage Guinevere to treat the whole matter lightly.
As the date approaches, Sir Gawain sets off to find the Green Chapel and keep his side of the bargain. Many adventures and battles are alluded to (but not described) until Gawain comes across a splendid castle where he meets Bertilak de Hautdesert, the lord of the castle, and his beautiful wife, who are pleased to have such a renowned guest. Also present is an old and ugly lady, unnamed but treated with great honour by all. Gawain tells them of his New Year's appointment at the Green Chapel and that he only has a few days remaining. Bertilak laughs, explains that the Green Chapel is less than two miles away and proposes that Gawain rest at the castle till then. Relieved and grateful, Gawain agrees.
Before going hunting the next day Bertilak proposes a bargain: he will give Gawain whatever he catches on the condition that Gawain give him whatever he might gain during the day. Gawain accepts. After Bertilak leaves, Lady Bertilak visits Gawain's bedroom and behaves seductively, but despite her best efforts he yields nothing but a single kiss in his unwillingness to offend her. When Bertilak returns and gives Gawain the deer he has killed, his guest gives a kiss to Bertilak without divulging its source. The next day the lady comes again, Gawain again courteously foils her advances, and later that day there is a similar exchange of a hunted boar for two kisses. She comes once more on the third morning, this time offering Gawain a gold ring as a keepsake. He gently but steadfastly refuses but she pleads that he at least take her belt, a girdle of green and gold silk which, the lady assures him, is charmed and will keep him from all physical harm. Tempted, as he may otherwise die the next day, Gawain accepts it, and they exchange three kisses. That evening, Bertilak returns with a fox, which he exchanges with Gawain for the three kisses – but Gawain says nothing of the girdle.
The next day, Gawain leaves for the Green Chapel with the girdle wound twice around his waist. He finds the Green Knight sharpening an axe and, as promised, Gawain bends his bared neck to receive his blow. At the first swing Gawain flinches slightly and the Green Knight belittles him for it. Ashamed of himself, at the Green Knight's next swing Gawain does not flinch; but again the full force of the blow is withheld. The knight explains he was testing Gawain's nerve. Angrily Gawain tells him to deliver his blow and so the knight does, causing only a slight wound on Gawain's neck. The game is over. Gawain seizes his sword, helmet and shield, but the Green Knight, laughing, reveals himself to be the lord of the castle, Bertilak de Hautdesert, transformed by magic. He explains that the entire adventure was a trick of the 'elderly lady' Gawain saw at the castle, who is actually the sorceress Morgan le Fay, Arthur's sister, who intended to test Arthur's knights and frighten Guinevere to death. Gawain is ashamed to have behaved deceitfully but the Green Knight laughs at his scruples and the two part on cordial terms. Gawain returns to Camelot wearing the girdle as a token of his failure to keep his promise. The Knights of the Round Table absolve him of blame and decide that henceforth that they will wear a green sash in recognition of Gawain's adventure.
"Pearl Poet".
Though the real name of "The "Gawain" Poet" (or poets) is unknown, some inferences about him or her can be drawn from an informed reading of his or her works. The manuscript of "Gawain" is known in academic circles as "Cotton Nero A.x.", following a naming system used by one of its owners, Robert Cotton, a collector of Medieval English texts. Before the "Gawain" manuscript came into Cotton's possession, it was in the library of Henry Savile of Bank in Yorkshire. Little is known about its previous ownership, and until 1824, when the manuscript was introduced to the academic community in a second edition of Thomas Warton's "History" edited by Richard Price, it was almost entirely unknown. Even then, the "Gawain" poem was not published in its entirety until 1839. Now held in the British Library, it has been dated to the late 14th century, meaning the poet was a contemporary of Geoffrey Chaucer, author of "The Canterbury Tales", though it is unlikely that they ever met. The three other works found in the same manuscript as "Gawain" (commonly known as "Pearl", "Patience", and "Purity" or "Cleanliness") are often considered to be written by the same author. However, the manuscript containing these poems was transcribed by a copyist and not by the original poet. Although nothing explicitly suggests that all four poems are by the same poet, comparative analysis of dialect, verse form, and diction have pointed towards single authorship.
What is known today about the poet is largely general. As J. R. R. Tolkien and E. V. Gordon, after reviewing the text's allusions, style, and themes, concluded in 1925:
He was a man of serious and devout mind, though not without humour; he had an interest in theology, and some knowledge of it, though an amateur knowledge perhaps, rather than a professional; he had Latin and French and was well enough read in French books, both romantic and instructive; but his home was in the West Midlands of England; so much his language shows, and his metre, and his scenery.
The most commonly suggested candidate for authorship is John Massey of Cotton, Cheshire. He is known to have lived in the dialect region of the Pearl Poet and is thought to have written the poem "St. Erkenwald", which some scholars argue bears stylistic similarities to "Gawain". "St. Erkenwald", however, has been dated by some scholars to a time outside the Gawain poet's era. Thus, ascribing authorship to John Massey is still controversial and most critics consider the Gawain poet an unknown.
Verse form.
The 2,530 lines and 101 stanzas that make up "Sir Gawain and the Green Knight" are written in what linguists call the "Alliterative Revival" style typical of the 14th century. Instead of focusing on a metrical syllabic count and rhyme, the alliterative form of this period usually relied on the agreement of a pair of stressed syllables at the beginning of the line and another pair at the end. Each line always includes a pause, called a "caesura", at some point after the first two stresses, dividing it into two half-lines. Although he largely follows the form of his day, the Gawain poet was somewhat freer with convention than his or her predecessors. The poet broke the alliterative lines into variable-length groups and ended these nominal stanzas with a rhyming section of five lines known as the "bob and wheel", in which the "bob" is a very short line, sometimes of only two syllables, followed by the "wheel," longer lines with internal rhyme.
Similar stories.
The earliest known story to feature a beheading game is the 8th-century Middle Irish tale "Bricriu's Feast". This story parallels "Gawain" in that, like the Green Knight, Cú Chulainn's antagonist feints three blows with the axe before letting his target depart without injury. A beheading exchange also appears in the late 12th-century "Life of Caradoc", a Middle French narrative embedded in the anonymous First Continuation of Chrétien de Troyes' "Perceval, the Story of the Grail". A notable difference in this story is that Caradoc's challenger is his father in disguise, come to test his honour. Lancelot is given a beheading challenge in the early 13th-century "Perlesvaus", in which a knight begs him to chop off his head or else put his own in jeopardy. Lancelot reluctantly cuts it off, agreeing to come to the same place in a year to put his head in the same danger. When Lancelot arrives, the people of the town celebrate and announce that they have finally found a true knight, because many others had failed this test of chivalry.
The stories "The Girl with the Mule" (alternately titled "The Mule Without a Bridle") and "Hunbaut" feature Gawain in beheading game situations. In "Hunbaut," Gawain cuts off a man's head and, before he can replace it, removes the magic cloak keeping the man alive, thus killing him. Several stories tell of knights who struggle to stave off the advances of voluptuous women sent by their lords as a test; these stories include "Yder", the Lancelot-Grail, "Hunbaut", and "The Knight of the Sword". The last two involve Gawain specifically. Usually the temptress is the daughter or wife of a lord to whom the knight owes respect, and the knight is tested to see whether or not he will remain chaste in trying circumstances.
In the first branch of the medieval Welsh collection of tales known as the "Mabinogion", Pwyll exchanges places for a year with Arawn, the lord of Annwn (the Otherworld). Despite having his appearance changed to resemble Arawn exactly, Pwyll does not have sexual relations with Arawn's wife during this time, thus establishing a lasting friendship between the two men. This story may, then, provide a background Gawain's attempts to resist to the wife of the Green Knight; thus, the story of Sir Gawain and the Green Knight may be seen as a tale which combines elements of the Celtic beheading game and seduction test stories. Additionally, in both stories a year passes before the completion of the conclusion of the challenge or exchange is complete. Some scholars disagree with this interpretation, however, as Arawn seems to have accepted the notion that Pwyll may reciprocate with his wife, making it less of a "seduction test" per se, as seduction tests typically involve a Lord and Lady conspiring to seduce a knight, seemingly "against" the wishes of the Lord.
After the writing of "Sir Gawain and the Green Knight", several similar stories followed. "The Greene Knight" (15th–17th century) is a rhymed retelling of nearly the same tale. In it, the plot is simplified, motives are more fully explained, and some names are changed. Another story, "The Turke and Gowin" (15th century), begins with a Turk entering Arthur's court and asking, "Is there any will, as a brother, To give a buffett and take another?" At the end of this poem the Turk, rather than buffeting Gawain back, asks the knight to cut off his head, which Gawain does. The Turk then praises Gawain and showers him with gifts. "The Carle of Carlisle" (17th century) also resembles "Gawain" in a scene in which the Carle (Churl), a lord, takes Sir Gawain to a chamber where two swords are hanging and orders Gawain to cut off his head or suffer his own to be cut off. Gawain obliges and strikes, but the Carle rises, laughing and unharmed. Unlike the "Gawain" poem, no return blow is demanded or given. Other oral versions of the story suggest that Sir Gawain was the Green Knight, with more characters introduced such as Chief Eagle Eye.
Themes.
Temptation and testing.
At the heart of "Sir Gawain and the Green Knight" is the test of Gawain's adherence to the code of chivalry. The typical temptation fable of medieval literature presents a series of tribulations assembled as tests or "proofs" of moral virtue. The stories often describe several individuals' failures after which the main character is tested. Success in the proofs will often bring immunity or good fortune. Gawain's ability to pass the tests of his host are of utmost importance to his survival, though he does not know it. It is only by fortuity or "instinctive-courtesy" that Sir Gawain is able to pass his test.
In addition to the laws of chivalry, Gawain must respect another set of laws concerning courtly love. The knight's code of honour requires him to do whatever a damsel asks. Gawain must accept the girdle from the Lady, but he must also keep the promise he has made to his host that he will give whatever he gains that day. Gawain chooses to keep the girdle out of fear of death, thus breaking his promise to the host but honouring the lady. Upon learning that the Green Knight is actually his host (Bertilak), he realises that although he has completed his quest, he has failed to be virtuous. This test demonstrates the conflict between honour and knightly duties. In breaking his promise, Gawain believes he has lost his honour and failed in his duties.
Hunting and seduction.
Scholars have frequently noted the parallels between the three hunting scenes and the three seduction scenes in "Gawain". They are generally agreed that the fox chase has significant parallels to the third seduction scene, in which Gawain accepts the girdle from Bertilak's wife. Gawain, like the fox, fears for his life and is looking for a way to avoid death from the Green Knight's axe. Like his counterpart, he resorts to trickery in order to save his skin. The fox uses tactics so unlike the first two animals, and so unexpectedly, that Bertilak has the hardest time hunting it. Similarly, Gawain finds the Lady's advances in the third seduction scene more unpredictable and challenging to resist than her previous attempts. She changes her evasive language, typical of courtly love relationships, to a more assertive style. Her dress, relatively modest in earlier scenes, is suddenly voluptuous and revealing.
The deer- and boar-hunting scenes are less clearly connected, although scholars have attempted to link each animal to Gawain's reactions in the parallel seduction scene. Attempts to connect the deer hunt with the first seduction scene have unearthed a few parallels. Deer hunts of the time, like courtship, had to be done according to established rules. Women often favoured suitors who hunted well and skinned their animals, sometimes even watching while a deer was cleaned. The sequence describing the deer hunt is relatively unspecific and nonviolent, with an air of relaxation and exhilaration. The first seduction scene follows in a similar vein, with no overt physical advances and no apparent danger; the entire exchange is humorously portrayed.
The boar-hunting scene is, in contrast, laden with detail. Boars were (and are) much more difficult to hunt than deer; approaching one with only a sword was akin to challenging a knight to single combat. In the hunting sequence, the boar flees but is cornered before a ravine. He turns to face Bertilak with his back to the ravine, prepared to fight. Bertilak dismounts and in the ensuing fight kills the boar. He removes its head and displays it on a pike. In the seduction scene, Bertilak's wife, like the boar, is more forward, insisting that Gawain has a romantic reputation and that he must not disappoint her. Gawain, however, is successful in parrying her attacks, saying that surely she knows more than he about love. Both the boar hunt and the seduction scene can be seen as depictions of a moral victory: both Gawain and Bertilak face struggles alone and emerge triumphant.
Nature and chivalry.
Some argue that nature represents a chaotic, lawless order which is in direct confrontation with the civilisation of Camelot throughout "Sir Gawain and the Green Knight". The green horse and rider that first invade Arthur’s peaceful halls are iconic representations of nature's disturbance. Nature is presented throughout the poem as rough and indifferent, constantly threatening the order of men and courtly life. Nature invades and disrupts order in the major events of the narrative, both symbolically and through the inner nature of humanity. This element appears first with the disruption caused by the Green Knight, later when Gawain must fight off his natural lust for Bertilak’s wife, and again when Gawain breaks his vow to Bertilak by choosing to keep the green girdle, valuing survival over virtue. Represented by the sin-stained girdle, nature is an underlying force, forever within man and keeping him imperfect (in a chivalric sense). In this view, Gawain is part of a wider conflict between nature and chivalry, an examination of the ability of man's order to overcome the chaos of nature.
Several critics have made exactly the opposite interpretation, reading the poem as a comic critique of the Christianity of the time, particularly as embodied in the Christian chivalry of Arthur's court. In its zeal to extirpate all traces of paganism, Christianity had cut itself off from the sources of life in nature and the female. The green girdle represents all the pentangle lacks. The Arthurian enterprise is doomed unless it can acknowledge the unattainability of the ideals of the Round Table, and, for the sake of realism and wholeness, recognize and incorporate the pagan values represented by the Green Knight.
Games.
The word "gomen" (game) is found 18 times in "Gawain". Its similarity to the word "gome" (man), which appears 21 times, has led some scholars to see men and games as centrally linked. Games at this time were seen as tests of worthiness, as when the Green Knight challenges the court's right to its good name in a "Christmas game". The "game" of exchanging gifts was common in Germanic cultures. If a man received a gift, he was obliged to provide the giver with a better gift or risk losing his honour, almost like an exchange of blows in a fight (or in a "beheading game"). The poem revolves around two games: an exchange of beheading and an exchange of winnings. These appear at first to be unconnected. However, a victory in the first game will lead to a victory in the second. Elements of both games appear in other stories; however, the linkage of outcomes is unique to "Gawain".
Times and seasons.
Times, dates, seasons, and cycles within "Gawain" are often noted by scholars because of their symbolic nature. The story starts on New Year's Eve with a beheading and culminates on the next New Year's Day. Gawain leaves Camelot on All Saints Day and arrives at Bertilak's castle on Christmas Eve. Furthermore, the Green Knight tells Gawain to meet him at the Green Chapel in "a year and a day"—a period of time seen often in medieval literature. Some scholars interpret the yearly cycles, each beginning and ending in winter, as the poet's attempt to convey the inevitable fall of all things good and noble in the world. Such a theme is strengthened by the image of Troy, a powerful nation once thought to be invincible which, according to the "Aeneid", fell to the Greeks due to pride and ignorance. The Trojan connection shows itself in the presence of two virtually identical descriptions of Troy's destruction. The poem's first line reads: "Since the siege and the assault were ceased at Troy" and the final stanzaic line (before the bob and wheel) is "After the siege and the assault were ceased at Troy".
Symbolism.
Significance of the color green.
Given the varied and even contradictory interpretations of the color green, its precise meaning in the poem remains ambiguous. In English folklore and literature, green was traditionally used to symbolise nature and its associated attributes: fertility and rebirth. Stories of the medieval period also used it to allude to love and the base desires of man. Because of its connection with faeries and spirits in early English folklore, green also signified witchcraft, devilry and evil. It can also represent decay and toxicity. When combined with gold, as with the Green Knight and the girdle, green was often seen as representing youth's passing. In Celtic mythology, green was associated with misfortune and death, and therefore avoided in clothing. The green girdle, originally worn for protection, became a symbol of shame and cowardice; it is finally adopted as a symbol of honour by the knights of Camelot, signifying a transformation from good to evil and back again; this displays both the spoiling and regenerative connotations of the color green.
The Green Knight.
Scholars have puzzled over the Green Knight's symbolism since the discovery of the poem. He could be a version of the Green Man, a mythological being connected with nature in medieval art, a Christian symbol, or the Devil himself. British medieval scholar C. S. Lewis said the character was "as vivid and concrete as any image in literature" and J. R. R. Tolkien said he was the "most difficult character" to interpret in "Sir Gawain". His major role in Arthurian literature is that of a judge and tester of knights, thus he is at once terrifying, friendly, and mysterious. He appears in only two other poems: "The Greene Knight" and "King Arthur and King Cornwall". Scholars have attempted to connect him to other mythical characters, such as Jack in the green of English tradition and to Al-Khidr, but no definitive connection has yet been established.
However, there is a possibility, as Alice Buchanan has argued, that the color green is erroneously attributed to the Green Knight due to the poet's mistranslation or misunderstanding of the Irish word "glas", which could either mean grey or green. In the Death of Curoi (one of the Irish stories from Bricriu's Feast), Curoi stands in for Bercilak, and is often called "the man of the grey mantle". Though the words usually used for grey in the Death of Curoi are "lachtna" or "odar", roughly meaning milk-coloured and shadowy respectively, in later works featuring a green knight, the word "glas" is used and may have been the basis of misunderstanding.
Girdle.
The girdle's symbolic meaning, in "Sir Gawain and the Green Knight", has been construed in a variety of ways. Interpretations range from sexual in nature to spiritual. Those who argue for the sexual inference view the girdle as a "trophy". However, it is not entirely clear who the "winner" is: Sir Gawain or Lady Bertilak. The girdle is given to Gawain by Lady Bertilak in order to keep him safe when he confronts the Green Knight. When Lord Bertilak returns home from his hunting trip, Gawain does not reveal the girdle to his host but, instead, hides it. This introduces the spiritual interpretation, that Gawain’s acceptance of the girdle is a sign of his faltering faith in God, at least in the face of death. To some, the Green Knight is Christ, who overcomes death, while Gawain is the Every Christian, who in his struggles to follow Christ faithfully, chooses the easier path. In "Sir Gawain", the easier choice is the girdle, which promises what Gawain most desires. Faith in God, alternatively, requires one’s acceptance that what one most desires does not always coincide with what God has planned. It is arguably best to view the girdle not as an either–or situation, but as a complex, multi-faceted symbol that acts to test Gawain in more ways than one. While Gawain is able to resist Bertilak’s wife’s sexual advances, he is unable to resist the powers of the girdle. Gawain is operating under the laws of chivalry which, evidently, have rules that can contradict each other. In the story of "Sir Gawain", Gawain finds himself torn between doing what a damsel asks (accepting the girdle) and keeping his promise (returning anything given to him while his host is away). 
Pentangle.
The pentangle on Gawain's shield is seen by many critics as signifying Gawain's perfection and power over evil. The poem contains the only representation of such a symbol on Gawain's shield in the Gawain literature. What is more, the poet uses a total of 46 lines to describe the meaning of the pentangle; no other symbol in the poem receives as much attention or is described in such detail. The poem describes the pentangle as a symbol of faithfulness and an "endless knot". In line 625, it is described as "a sign by Solomon". Solomon, the third king of Israel, in the 10th century BC, was said to have the mark of the pentagram on his ring, which he received from the archangel Michael. The pentagram seal on this ring was said to give Solomon power over demons.
Along these lines, some academics link the Gawain pentangle to magical traditions. In Germany, the symbol was called a "Drudenfuß" and was placed on household objects to keep out evil. The symbol was also associated with magical charms that, if recited or written on a weapon, would call forth magical forces. However, concrete evidence tying the magical pentagram to Gawain's pentangle is scarce.
Gawain’s pentangle also symbolises the "phenomenon of physically endless objects signifying a temporally endless quality." Many poets use the symbol of the circle to show infinity or endlessness, but Gawain’s poet insisted on using something more complex. In medieval number theory, the number five is considered a "circular number", since it "reproduces itself in its last digit when raised to its powers". Furthermore, it replicates itself geometrically; that is, every pentangle has a smaller pentagon that allows a pentangle to be embedded in it and this "process may be repeated forever with decreasing pentangles". Thus, by reproducing the number five, which in medieval number symbolism signified incorruptibility, Gawain's pentangle represents his eternal incorruptibility.
The Lady's Ring.
Gawain's refusal of the Lady Bertilak's ring has major implications for the remainder of the story. While the modern student may tend to pay more attention to the girdle as the eminent object offered by the lady, readers in the time of Gawain would have noticed the significance of the offer of the ring as they believed that rings, and especially the embedded gems, had talismanic properties. This is especially true of the lady's ring as scholars believe it to be a ruby or carbuncle, indicated when the Gawain-Poet describes it as a "brygt sunne" (line 1819), a "fiery sun." Given the importance of magic rings in Arthurian romance, this remarkable ring would also have been believed to protect the wearer from harm.
Numbers.
The poet highlights number symbolism to add symmetry and meaning to the poem. For example, three kisses are exchanged between Gawain and Bertilak's wife; Gawain is tempted by her on three separate days; Bertilak goes hunting three times, and the Green Knight swings at Gawain three times with his axe. The number two also appears repeatedly, as in the two beheading scenes, two confession scenes, and two castles. The five points of the pentangle, the poet adds, represent Gawain's virtues, for he is "faithful five ways and five times each". The poet goes on to list the ways in which Gawain is virtuous: all five of his senses are without fault; his five fingers never fail him, and he always remembers the five wounds of Christ, as well as the five joys of the Virgin Mary. The fifth five is Gawain himself, who embodies the five moral virtues of the code of chivalry: "friendship, generosity, chastity, courtesy, and piety". All of these virtues reside, as the poet says, in the "Endless Knot" of the pentangle, which forever interlinks and is never broken. This intimate relationship between symbol and faith allows for rigorous allegorical interpretation, especially in the physical role that the shield plays in Gawain's quest. Thus, the poet makes Gawain the epitome of perfection in knighthood through number symbolism.
The number five is also found in the structure of the poem itself. "Sir Gawain" is 101 stanzas long, traditionally organised into four 'Fitts' of 21, 24, 34, and 22 stanzas. These divisions, however, have since been disputed; scholars have begun to believe that they are the work of the copyist and not of the poet. The original manuscript features a series of capital letters added after the fact by another scribe, and some scholars argue that these additions were an attempt to restore the original divisions. These letters divide the manuscript into nine parts. The first and last parts are 22 stanzas long. The second and second-to-last parts are only one stanza long, and the middle five parts are eleven stanzas long. The number eleven is associated with transgression in other medieval literature (being one more than ten, a number associated with the Ten Commandments). Thus, this set of five elevens (55 stanzas) creates the perfect mix of transgression and incorruption, suggesting that Gawain is faultless in his faults.
Wounds.
At the story's climax, Gawain is wounded superficially in the neck by the Green Knight's axe. During the medieval period, the body and the soul were believed to be so intimately connected that wounds were considered an outward sign of inward sin. The neck, specifically, was believed to correlate with the part of the soul related to will, connecting the reasoning part (the head) and the courageous part (the heart). Gawain's sin resulted from using his will to separate reasoning from courage. By accepting the girdle from the lady, he employs reason to do something less than courageous—evade death in a dishonest way. Gawain's wound is thus an outward sign of an internal wound. The Green Knight's series of tests shows Gawain the weakness that has been in him all along: the desire to use his will pridefully for personal gain, rather than submitting his will in humility to God. The Green Knight, by engaging with the greatest knight of Camelot, also reveals the moral weakness of pride in all of Camelot, and therefore all of humanity. However, the wounds of Christ, believed to offer healing to wounded souls and bodies, are mentioned throughout the poem in the hope that this sin of prideful "stiffneckedness" will be healed among fallen mortals.
Interpretations.
"Gawain" as medieval romance.
Many critics argue that "Sir Gawain and the Green Knight" should be viewed, above all, as a romance. Medieval romances typically recount the marvellous adventures of a chivalrous, heroic knight, often of super-human ability, who abides by chivalry's strict codes of honour and demeanour, embarks upon a quest and defeats monsters, thereby winning the favour of a lady. Thus, medieval romances focus not on love and sentiment (as the term "romance" implies today), but on adventure.
Gawain's function, as medieval scholar Alan Markman says, "is the function of the romance hero … to stand as the champion of the human race, and by submitting to strange and severe tests, to demonstrate human capabilities for good or bad action." Through Gawain's adventure, it becomes clear that he is merely human. The reader becomes attached to this human view in the midst of the poem's romanticism, relating to Gawain's humanity while respecting his knightly qualities. Gawain "shows us what moral conduct is. We shall probably not equal his behaviour, but we admire him for pointing out the way."
In viewing the poem as a chivalric romance, many scholars see it as intertwining chivalric and courtly love laws under the English Order of the Garter. The group's motto, 'honi soit qui mal y pense', or "Shamed be he who finds evil here," is written at the end of the poem. Some critics describe Gawain's peers wearing girdles of their own as evidence of the origin of the Order of the Garter. However, in the parallel poem "The Greene Knight", the lace is white, not green, and is considered the origin of the collar worn by the knights of the Bath, not the Order of the Garter. The motto on the poem was probably written by a copyist and not by the original author. Still, the connection made by the copyist to the Order is not extraordinary.
Christian interpretations.
The poem is in many ways deeply Christian, with frequent references to the fall of Adam and Eve and to Jesus Christ. Scholars have debated the depth of the Christian elements within the poem by looking at it in the context of the age in which it was written, coming up with varying views as to what represents a Christian element of the poem and what does not. For example, some critics compare "Sir Gawain" to the other three poems of the "Gawain" manuscript. Each has a heavily Christian theme, causing scholars to interpret "Gawain" similarly. Comparing it to the poem "Cleanness" (also known as "Purity"), for example, they see it as a story of the apocalyptic fall of a civilisation, in "Gawain's" case, Camelot. In this interpretation, Sir Gawain is like Noah, separated from his society and warned by the Green Knight (who is seen as God's representative) of the coming doom of Camelot. Gawain, judged worthy through his test, is spared the doom of the rest of Camelot. King Arthur and his knights, however, misunderstand Gawain's experience and wear garters themselves. In "Cleanness" the men who are saved are similarly helpless in warning their society of impending destruction.
One of the key points stressed in this interpretation is that salvation is an individual experience difficult to communicate to outsiders. In his depiction of Camelot, the poet reveals a concern for his society, whose inevitable fall will bring about the ultimate destruction intended by God. "Gawain" was written around the time of the Black Death and Peasants' Revolt, events which convinced many people that their world was coming to an apocalyptic end and this belief was reflected in literature and culture. However, other critics see weaknesses in this view, since the Green Knight is ultimately under the control of Morgan le Fay, usually viewed as a figure of evil in Camelot tales. This makes the knight's presence as a representative of God problematic.
While the character of the Green Knight is usually not viewed as a representation of Christ in "Sir Gawain and the Green Knight", critics do acknowledge a parallel. Lawrence Besserman, a specialist in medieval literature, explains that "the Green Knight is not a figurative representative of Christ. But the idea of Christ's divine/human nature provides a medieval conceptual framework that supports the poet's serious/comic account of the Green Knight's supernatural/human qualities and actions." This duality exemplifies the influence and importance of Christian teachings and views of Christ in the era of the Gawain Poet.
Furthermore, critics note the Christian reference to Christ's crown of thorns at the conclusion of "Sir Gawain and the Green Knight". After Gawain returns to Camelot and tells his story regarding the newly acquired green sash, the poem concludes with a brief prayer and a reference to "the thorn-crowned God". Besserman theorises that "with these final words the poet redirects our attention from the circular girdle-turned-sash (a double image of Gawain's "yntrawpe/renoun") to the circular Crown of Thorns (a double image of Christ's humiliation turned triumph)."
Throughout the poem, Gawain encounters numerous trials testing his devotion and faith in Christianity. When Gawain sets out on his journey to find the Green Chapel, he finds himself lost, and only after praying to the Virgin Mary does he find his way. As he continues his journey, Gawain once again faces anguish regarding his inevitable encounter with the Green Knight. Instead of praying to Mary, as before, Gawain places his faith in the girdle given to him by Bertilak's wife. From the Christian perspective, this leads to disastrous and embarrassing consequences for Gawain as he is forced to reevaluate his faith when the Green Knight points out his betrayal.
An analogy is also made between Gawain's trial and the Biblical test that Adam encounters in the Garden of Eden. Adam succumbs to Eve just as Gawain surrenders to Bertilak's wife by accepting the girdle. Although Gawain sins by putting his faith in the girdle and not confessing when he is caught, the Green Knight pardons him, thereby allowing him to become a better Christian by learning from his mistakes. Through the various games played and hardships endured, Gawain finds his place within the Christian world.
Feminist interpretations.
Feminist literary critics see the poem as portraying women's ultimate power over men. Morgan le Fay and Bertilak's wife, for example, are the most powerful characters in the poem—Morgan especially, as she begins the game by enchanting the Green Knight. The girdle and Gawain's scar can be seen as symbols of feminine power, each of them diminishing Gawain's masculinity. Gawain's misogynist passage, in which he blames all of his troubles on women and lists the many men who have fallen prey to women's wiles, further supports the feminist view of ultimate female power in the poem.
In contrast, others argue that the poem focuses mostly on the opinions, actions, and abilities of men. For example, on the surface, it appears that Bertilak's wife is a strong leading character. By adopting the masculine role, she appears to be an empowered individual, particularly in the bedroom scene. This is not entirely the case, however. While the Lady is being forward and outgoing, Gawain's feelings and emotions are the focus of the story, and Gawain stands to gain or lose the most. The Lady "makes the first move", so to speak, but Gawain ultimately decides what is to become of those actions. He, therefore, is in charge of the situation and even the relationship.
In the bedroom scene, both the negative and positive actions of the Lady are motivated by her desire. Her feelings cause her to step out of the typical female role and into that of the male, thus becoming more empowered. At the same time, those same actions make the Lady appear adulterous; some scholars compare her with Eve in the Bible. By forcing Gawain to take her girdle, i.e. the apple, the pact made with Bertilak—and therefore the Green Knight—is broken. In this sense, it is clear that at the hands of the Lady, Gawain is a "good man seduced".
Postcolonial interpretations.
From 1350 to 1400—the period in which the poem is thought to have been written—Wales experienced several raids at the hands of the English, who were attempting to colonise the area. The Gawain poet uses a North West Midlands dialect common on the Welsh–English border, potentially placing him in the midst of this conflict. Patricia Clare Ingham is credited with first viewing the poem through the lens of postcolonialism, and since then a great deal of dispute has emerged over the extent to which colonial differences play a role in the poem. Most critics agree that gender plays a role, but differ about whether gender supports the colonial ideals or replaces them as English and Welsh cultures interact in the poem.
A large amount of critical debate also surrounds the poem as it relates to the bi-cultural political landscape of the time. Some argue that Bertilak is an example of the hybrid Anglo-Welsh culture found on the Welsh–English border. They therefore view the poem as a reflection of a hybrid culture that plays strong cultures off one another to create a new set of cultural rules and traditions. Other scholars, however, argue that historically much Welsh blood was shed well into the 14th century, creating a situation far removed from the more friendly hybridisation suggested by Ingham. To support this argument further, it is suggested that the poem creates an "us versus them" scenario contrasting the knowledgeable civilised English with the uncivilised borderlands that are home to Bertilak and the other monsters that Gawain encounters.
In contrast to this perception of the colonial lands, others argue that the land of Hautdesert, Bertilak’s territory, has been misrepresented or ignored in modern criticism. They suggest that it is a land with its own moral agency, one that plays a central role in the story. Bonnie Lander, for example, argues that the denizens of Hautdesert are "intelligently immoral", choosing to follow certain codes and rejecting others, a position which creates a "distinction … of moral insight versus moral faith". Lander thinks that the border dwellers are more sophisticated because they do not unthinkingly embrace the chivalric codes but challenge them in a philosophical, and—in the case of Bertilak's appearance at Arthur’s court—literal sense. Lander’s argument about the superiority of the denizens of Hautdesert hinges on the lack of self-awareness present in Camelot, which leads to an unthinking populace that frowns on individualism. In this view, it is not Bertilak and his people, but Arthur and his court, who are the monsters.
Gawain's journey.
Several scholars have attempted to find a real-world correspondence for Gawain's journey to the Green Chapel. The Anglesey islands, for example, are mentioned in the poem. They exist today as a single island off the coast of Wales. In line 700, Gawain is said to pass the "Holy Head", believed by many scholars to be either Holywell or the Cistercian abbey of Poulton in Pulford. Holywell is associated with the beheading of Saint Winifred. As the story goes, Winifred was a virgin who was beheaded by a local leader after she refused his sexual advances. Her uncle, another saint, put her head back in place and healed the wound, leaving only a white scar. The parallels between this story and Gawain's make this area a likely candidate for the journey.
Gawain's trek leads him directly into the centre of the Pearl Poet's dialect region, where the candidates for the locations of the Castle at Hautdesert and the Green Chapel stand. Hautdesert is thought to be in the area of Swythamley in northwest Midland, as it lies in the writer's dialect area and matches the topographical features described in the poem. The area is also known to have housed all of the animals hunted by Bertilak (deer, boar, fox) in the 14th century. The Green Chapel is thought to be in either Lud's Church or Wetton Mill, as these areas closely match the descriptions given by the author. Ralph Elliott located the chapel ("two myle henne" v1078) from the old manor house at Swythamley Park at the bottom of a valley ("bothm of the brem valay" v2145) on a hillside ("loke a littel on the launde, on thi lyfte honde" v2147) in an enormous fissure ("an olde caue,/or a creuisse of an olde cragge" v2182-83).
Homoerotic Interpretations.
According to medieval scholar Richard Zeikowitz, the Green Knight represents a threat to homosocial friendship in his medieval world. Zeikowitz argues that the narrator of the poem seems entranced by the Knight's beauty, homoeroticising him in poetic form. The Green Knight's attractiveness challenges the homosocial rules of King Arthur's court and poses a threat to their way of life. Zeikowitz also states that Gawain seems to find Bertilak as attractive as the narrator finds the Green Knight. Bertilak, however, follows the homosocial code and develops a friendship with Gawain. Gawain's embracing and kissing Bertilak in several scenes thus represents not a homosexual but a homosocial expression. Men of the time often embraced and kissed and this was acceptable under the chivalric code. Nonetheless, the Green Knight blurs the lines between homosociality and homosexuality, representing the difficulty medieval writers sometimes had in separating the two.
Carolyn Dinshaw argues that the poem may have been a response to accusations that Richard II had a male lover—an attempt to reestablish the idea that heterosexuality was the Christian norm. Around the time the poem was written, the Catholic Church was beginning to express concerns about kissing between males. Many religious figures were trying to make the distinction between strong trust and friendship between males and homosexuality. Still, the Pearl Poet seems to have been simultaneously entranced and repulsed by homosexual desire. In his other poem "Cleanness", he points out several grievous sins, but spends lengthy passages describing them in minute detail. His obsession seems to carry into "Gawain" in his descriptions of the Green Knight.
Beyond this, Dinshaw proposes that Gawain can be read as a woman-like figure. He is the passive one in the advances of Lady Bertilak, as well as in his encounters with Lord Bertilak, where he acts the part of a woman in kissing the man. However, while the poem does have homosexual elements, these elements are brought up by the poet in order to establish heterosexuality as the normal lifestyle of Gawain's world. The poem does this by making the kisses between Lady Bertilak and Gawain sexual in nature, but rendering the kisses between Gawain and Lord Bertilak "unintelligible" to the medieval reader. In other words, the poet portrays kisses between a man and a woman as having the possibility of leading to sex, while in a heterosexual world kisses between a man and a man are portrayed as having no such possibility.
Modern adaptations.
Books.
Though the surviving manuscript dates from the fourteenth century, the first published version of the poem did not appear until as late as 1839, when Sir Frederic Madden of the British Museum recognized the poem as worth reading. Madden's scholarly, Middle English edition of the poem was followed in 1898 by the first Modern English translation – a prose version by literary scholar Jessie L. Weston. In 1925, J.R.R. Tolkien and E.V. Gordon published a scholarly edition of the Middle English text of "Sir Gawain and the Green Knight"; a revised edition of this text was prepared by Norman Davis and published in 1967. The book, featuring a text in Middle English with extensive scholarly notes, is frequently confused with the translation into Modern English that Tolkien prepared, along with translations of "Pearl" and "Sir Orfeo", late in his life. Many editions of the latter work, first published in 1975, shortly after his death, list Tolkien on the cover as author rather than translator.
For students, especially undergraduate students, the text is usually given in translation. Notable translators include Jessie Weston, whose 1898 prose translation and 1907 poetic translation took many liberties with the original; Theodore Banks, whose 1929 translation was praised for its adaptation of the language to modern usage; and Marie Borroff, whose imitative translation was first published in 1967 and "entered the academic canon" in 1968, in the second edition of the "Norton Anthology of English Literature". In 2010, her (slightly revised) translation was published as a Norton Critical Edition, with a foreword by Laura Howes. In 2007, Simon Armitage, who grew up near the Gawain poet's purported residence, published a translation which attracted attention in the US and the United Kingdom, and was published in the United States by Norton, which replaced Borroff's translation with Armitage's for the ninth edition of the "Norton Anthology of English Literature". Other modern translations include those by Brian Stone, James Winny, Helen Cooper, W. S. Merwin, Jacob Rosenberg, William Vantuono, Joseph Glaser, Bernard O'Donoghue, John Gardner, and Francis Ingledew.
In 2014, Zach Weiner published a children's book called "Augie and the Green Knight" that is a . Though the protagonist of the book is a young, modern-day girl, the events of the book largely follow the Arthurian legend. In 1997, Gerald Morris published a version of the story called The Squire, His Knight, and His Lady.
Film and television.
The poem has been adapted to film twice, on both occasions by writer-director Stephen Weeks: first as "Gawain and the Green Knight" in 1973 and again in 1984 as "", featuring Miles O'Keeffe as Gawain and Sean Connery as the Green Knight. Both films have been criticised for deviating from the plot. Gawain, for example, has an adventure in the 1973 version which is not a part of the poem between the time he leaves Camelot and the time he arrives at Bertilak's castle, in which he travels through New Earth to find his parents. Also, Bertilak and the Green Knight are never connected. French/Australian director Martin Beilby directed a short (30') film adaptation in 2014. There have been at least two television adaptations, "Gawain and the Green Knight" in 1991 and the animated "Sir Gawain and the Green Knight" in 2002. The BBC broadcast a documentary presented by Simon Armitage in which the journey depicted in the poem is traced, utilising what are believed to be the actual locations.
Theatre.
The Tyneside Theatre company presented a stage version of "Sir Gawain and the Green Knight" at the University Theatre, Newcastle at Christmas 1971. It was directed by Michael Bogdanov and adapted for the stage from the translation by Brian Stone. The music and lyrics were composed by Iwan Williams using medieval carols, such as the "Boar's Head Carol", as inspiration and folk instruments such as the Northumbrian pipes, whistles and bhodran to create a "rough" feel. Stone had referred Bogdanov to "Cuchulain and the Beheading Game", a sequence which is contained in The Grenoside Sword dance. Bogdanov found the pentangle theme to be contained in most sword dances, and so incorporated a long sword dance while Gawain lay tossing uneasily before getting up to go to the Green Chapel. The dancers made the knot of the pentangle around his drowsing head with their swords. The interlacing of the hunting and wooing scenes was achieved by frequent cutting of the action from hunt to bed-chamber and back again, while the locale of both remained on-stage.
In 1992 Simon Corble created an adaptation with medieval songs and music for The Midsommer Actors' Company. performed as walkabout productions in the summer 1992 at Thurstaston Common and Beeston Castle and in August 1995 at Brimham Rocks, North Yorkshire. Corble later wrote a substantially revised version which was produced indoors at the O'Reilly Theatre, Oxford in February 2014.
Opera.
"Sir Gawain and the Green Knight" was first adapted as an opera in 1978 by the composer Richard Blackford on commission from the village of Blewbury, Oxfordshire. The libretto was written for the adaptation by the children's novelist John Emlyn Edwards. The "Opera in Six Scenes" was subsequently recorded by Decca between March and June 1979 and released on the Argo label in November 1979.
"Sir Gawain and the Green Knight" was adapted into an opera called "Gawain" by Harrison Birtwistle, first performed in 1991. Birtwistle's opera was praised for maintaining the complexity of the poem while translating it into lyric, musical form. Another operatic adaptation is Lynne Plowman's "Gwyneth and the Green Knight", first performed in 2002. This opera uses "Sir Gawain" as the backdrop but refocuses the story on Gawain's female squire, Gwyneth, who is trying to become a knight. Plowman's version was praised for its approachability, as its target is the family audience and young children, but criticised for its use of modern language and occasional preachy nature.

</doc>
<doc id="28834" url="http://en.wikipedia.org/wiki?curid=28834" title="Sultan Bashiruddin Mahmood">
Sultan Bashiruddin Mahmood

Sultan Bashiruddin Mahmood(Urdu: سلطان بشیر الدین محمود‎; born 1940; "SI"), is a Pakistani nuclear engineer and a scholar on Islamic studies who was notoriously subjected for a criminal probe launched by the FIA on suspicions on unauthorized travel in Afghanistan prior to the deadliest terrorist attacks in the United States in 2001.
Having spent a distinguish career in PAEC, he founded the Ummah Tameer-e-Nau (UTN) in 1999– a right-wing organization that was banned and sanctioned by the United States in 2001. Mehmood was among those who were listed and sanctioned by the al-Qaeda sanction committee in December 2001. Having been cleared by the FIA, he has been living in "anonymity" in Islamabad, authoring books on relationship between Islam and science.
Life and education.
Mahmood was born in Amritsar, Punjab, British India to the Punjabi family. There are conflicting reports on concerning his date of birth; his personal admission noted the birth year as 1940, while the UN reports estimated as 1938. His father, Chaudhry Muhammad Sharif, was a local "Zamindar" (lit. feudal lord). His family emigrated from India to Pakistan in an events following the violent partition of India in 1947; the family settled in Lahore, Punjab.
After graduating with distinctions from a local high school standing at top of his class, Mehmood was awarded scholarship and enrolled at the famed Government College University to study electrical engineering. After spending a semester, he made a transfer to University of Engineering and Technology in Lahore, and graduated with bachelor's degree with honors in electrical engineering in 1960. His credentials led him to join the Pakistan Atomic Energy Commission (PAEC) where he gained scholarship to study in the United Kingdom.
In 1962, he went to attend the University of Manchester where he studied for double master's degree. First completing masters' program in control systems in 1965, then Mehmood received his another master's degree in nuclear engineering in 1969 from the Manchester University. While in Manchester, Mehmood was an expert on Manhattan Project and was reportedly in contacts with South African scientists in discussing the jet-nozzel method for uranium-enrichment. However, it remains unclear how much interaction was taken place during that time.
Pakistan Atomic Energy Commission.
Mehmood joined the PAEC in 1968, joining the Nuclear Physics Division at the Institute of Nuclear Science and Technology working under dr. Naeem Ahmad Khan. His collaboration took place with Samar Mubarakmand, Hafeez Qureshi and was a vital member of the group before it got discontinued in 1970. Mahmood was one of the foremost experts on civilian reactor technology and was a senior engineer at the KANUPP I— the first commercial nuclear power plant of the country. He gained notability and publicity in the physics community for inventing the scientific instrument, the "SBM probe" to detect leaks in steam pipes, a problem that was affecting nuclear plants all over the world and is still used worldwide.
After witnessing the war with India which saw the unconditional surrender of Pakistan in 1971, Mahmood attended the winter seminar at Multan and delivered a speech on atomic science. On 20 January 1972, President Zulfikar Ali Bhutto approved the crash program under Munir Ahmad Khan for a sake of "national survivor." Though, he continued his work at the KANUPP I engineering division.
In the aftermath of surprise nuclear test conducted by India, Munir Ahmad appointed Mehmood as the director of the enrichment division at the PAEC where majority of the calculations were conducted by dr. Khalil Qureshi– a physical chemist. Mehmood analyzed the diffusion, gas-centrifuge, jet-nozzle and laser methods for the uranium-enrichment; recommending the gas-centrifuge method as economical. After submitting the report, Mehmood was asked to depart to the Netherlands to interview dr. Abdul Qadeer Khan on behalf of President Bhutto in 1974. In 1975, his proposal was approved and the work on uranium project started with Mahmood being its director, a move that irked more qualified but more difficult to manage dr. Abdul Qadeer Khan who had coveted the job for himself. His relations with dr. Khan remains extremely tense and the pairs disagreed with each other and developed differences at great height. In private meetings with Munir Ahmad, Mehmood often complained and pictured him as "egomaniac". In 1976, Mahmood was removed from the enrichment division as dr. Abdul Qadeer Khan had him ejected and moving the enrichment division at the ERL under military control.
Eventually, Munir Ahmad removed him from other classified works and posted him back at the KANUPP-I with no reason given. In 1980s, Munir Ahmad secured him a job as project manager for the construction of the Khushab-I where he served as chief engineer and aided with the designing the coolant systems. In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.
After the reactor went critical in April 1998, Mahmood in an interview had said: ""This reactor (can produce enough plutonium for two to three nuclear weapons per year) Pakistan had "acquired the capability to produce... boosted thermonuclear weapons and hydrogen bombs"." In 1998, Mahmood was honored with Sitara-e-Imtiaz in a colourful ceremony by the Prime Minister, Nawaz Sharif.
In 1998, he was promoted as a director of the nuclear power division and held that position until 1999.
Radical politics and Ummah Tameer-e-Nau.
Endorsing publicly the decision of nuclear tests by Prime Minister Nawaz Sharif in 1998, Mahmood began appearing in news channels as an outspoken opponent of Prime Minister Sharif, as he vehemently opposed Pakistan becoming the signatory state of the NPT and CTBT. At country's popular news channels and newspapers, Mahmood gave numerous interviews, wrote articles, and lobbied against Prime Minister Sharif when learning that Prime Minister Sharif had been willing to be a signatory of anti-nuclear weapon treaties, prompting the government forcefully transferring Mahmood at the non-technical position in the PAEC.
Seeking premature retirement from PAEC in 1999, Mahmood moved towards publishing books and articles involving the relationship between Science and Islam. Mahmood founded the Ummah Tameer-e-Nau (UTN)– a rightwing organization– with his close associates. In 2000, he began attending the lectures and religious sessions with Dr. Israr Ahmed who would later influenced in his political views and philosophy. Through UTN, he steps in the more radical politics, and began visiting Afghanistan where he wanted to be focused on rebuilding educational institutions, hospitals, and relief work.
In August 2001, Mahmood and one of his colleagues at the UTN met with Osama bin Laden and Ayman al-Zawahiri in Kandahar, Afghanistan. Describing the meeting, the "New York Times" editorial quoted:""There is little doubt that Mahmood talked to the two al-Qaeda leaders about nuclear weapons, or that Al Qaeda desperately wanted the bomb".
2001 debriefing and detention.
Since 1999 and 2000 onwards, Pakistan's intelligence community had been tracking and monitoring Mehmood whose bushy beard advertised his deep attachment to Taliban. After the terrorist attacks in the United States, the FIA launched an active criminal investigations against him, leveling charges on unauthorized traveled to Afghanistan. Director CIA George Tenet later described intelligence reports of his meeting with Al Qaeda as "frustratingly vague"." When asked by Pakistani and American investigators about nature of UTN's work and discussions, Mahmood told that he had nothing to do with the al-Qaeda and was only working on humanitarian issues like food, health and education. Investigators from ISI and CIA were astonished and surprised when finding out that Mahmood knew nothing on nuclear weapons as contrary of being a nuclear engineer, and were unable to construct one by themselves.
During his debriefing, his son Dr. Asim Mahmood, who's a family medicine doctor told ISI officials that: "My father [Mahmood] did meet with Osama bin Laden and Osama Bin Laden seemed interested in that matter but my father showed no interest in the matter as he met him for food, water and healthcare matters on which his charity was working".
The FIA criminal probe continued for four months and yielded no concrete results. Pressure from the civil society and court inquiries against FIA's criminal probe led to his release in 2001. His family did confirmed his released but had been constantly under surveillance by the FIA; his name was placed in the "Exit Control List" in which he is not allowed to travel out of Pakistan and since his release, Mehmood has been out of the public eye and lives a very quiet life in Islamabad, Pakistan devoting most of his time to write books and doing research work on Islam and science.
Dr. Bashir Syed, former president of the Association of Pakistani Scientists and Engineers of North America (APSENA), said: "I know both of these persons and can tell you there is not an iota of truth that both these respected scientists and friends will do anything to harm the interest of their own country."
Mahmood-Hoodbhoy debates.
He has written over fifteen books, the most well-known being "The Mechanics of Doomsday and Life After Death", which is an analysis of the events leading to doomsday in light of scientific theories and Quranic knowledge. However, his scientific arguments and theories have been challenged by some prominent scientists in Pakistan. His religiosity and eccentricity began troubling the Pakistan's Physics Society; his peers often quoted him as "a rather strange man".
In 1988, Mehmood was invited through an invitation at the University of Islamabad to deliver a lecture on science. During his lecture at the university's "Physics Hall", he and several other academcians have debated on his book. While debating, a well known Dr. Pervez Hoodbhoy and Sultan Bashiruddin Mahmood had an acrimonious public debate in 1988 at the University of Islamabad's Physics Hall. Dr. Pervez Hoodbhoy had severely criticised Mr. Bashiruddin Mahmood's theories and the notion of Islamic science in general, calling it "ludicrous science." Bashiruddin Mahmood protested that Dr. Pervez Hoodbhoy misrepresented his views, quoting: "This is crossing all limits of decency," he wrote. "But should one expect any honesty or decency from anti-Islamic sources?"
Literature and Cosmology.
I
n his writings and speeches, Mahmood has advocated for nuclear sharing with other Islamic nations which he believed would give rise to Muslim dominance in the world. He has also written a Tafseer of the Quran in English.
Mahmood is reported to be fascinated "with the role sunspots played in triggering the French and Russian Revolutions, World War II and assorted anti-colonial uprisings." According to his book "Cosmology and Human Destiny", Mahmood argued that sunspots have influenced major human events, including the French Revolution, the Russian Revolution, and World War II. He concluded that governments across the world ""are already being subjected to great emotional aggression under the catalytic effect of the abnormally high sunspot activity under which they are most likely to adapt aggression as the natural solution for their problems". In this book which was first published in 1998, he predicts that the period from 2007 to 2014 would be of great turmoil and destruction in the world. Other books written by him include a biography of the Islamic prophet Muhammad titled "First and the Last", while his other books are focused more on the relation between Islam and science like "Miraculous Quran", "Life After Death and Doomsday", and "Kitab-e-Zindagi" (in Urdu).
One passage of the book reportedly states: "At the international level, terrorism will rule; and in this scenario use of mass destruction weapons cannot be ruled out. Millions, by 2020, may die through mass destruction weapons, hunger, disease, street violence, terrorist attacks, and suicide.""
Mahmood's lifelong friend, Parliamentarian Farhatullah Babar, who is currently serving as a spokesperson of President of Pakistan, while talking to media, said: "Mahmood predicted in Cosmology and Human Destiny that "the year 2002 was likely to be a year of maximum sunspot activity. It means upheaval, particularly on the South Asia, with the possibility of nuclear exchanges"."
Mahmood has published papers concerning djinni, which are described in the Qur'an as beings made of fire. He has proposed that djinni could be tapped to solve the energy crisis. "I think that if we develop our souls, we can develop communication with them," Mr. Bashiruddin Mahmood said about djinni in The Wall Street Journal in an interview in 1988. "Every new idea has its opponents," he added. "But there is no reason for this controversy over Islam and science because there is no conflict between Islam and science."
New York Times comments.
The New York Times has described Mahmood as "an autodidact intellectual with grand aspirations," and noted that "his fellow scientists at PAEC began to wonder if Mahmood was mentally sound." Mahmood made it clear that he believed Pakistan's bomb was "the property of the whole Ummah," referring to the worldwide Muslim community. "This guy was our ultimate nightmare," an American intelligence official told the Times in late 2001. The US Institute of Historical biographies mentions him in their ‘Who is Who’ list and presented him a gold medal in 1998. He has also been awarded Gold Medal by the Pakistan Academy of Sciences.
References.
</dl>

</doc>
<doc id="28837" url="http://en.wikipedia.org/wiki?curid=28837" title="Siege tower">
Siege tower

A siege tower (also breaching tower; or in the Middle Ages a belfry) is a specialized siege engine, constructed to protect assailants and ladders while approaching the defensive walls of a fortification. The tower was often rectangular with four wheels with its height roughly equal to that of the wall or sometimes higher to allow archers to stand on top of the tower and shoot into the fortification. Because the towers were wooden and thus flammable, they had to have some non-flammable covering of iron or fresh animal skins.
Used since the 11th century BC in the ancient Near East, the 4th century BC in Europe and also in antiquity in the Far East, siege towers were of unwieldy dimensions and, like trebuchets, were therefore mostly constructed on site of the siege. Taking considerable time to construct, siege towers were mainly built if the defense of the opposing fortification could not be overcome by ladder assault ("escalade"), by mining or by breaking walls or gates.
The siege tower sometimes housed spearmen, pikemen, swordsmen, archers or crossbowmen who shot arrows and quarrels at the defenders. Because of the size of the tower it would often be the first target of large stone catapults but it had its own projectiles with which to retaliate.
Siege towers were used to get troops over an enemy curtain wall. When a siege tower was near a wall, it would drop a gangplank between it and the wall. Troops could then rush onto the walls and into the castle or city.
Ancient use.
The oldest known siege towers were used by the armies of the Neo-Assyrian Empire in the 9th century BC, under Ashurnasirpal II (r. 884 BC-859 BC). Reliefs from his reign, and subsequent reigns, depict siege towers in use with a number of other siege works, including ramps and battering rams.
Centuries after they were employed in Assyria, the use of the siege tower spread throughout the Mediterranean. The biggest siege towers of antiquity, such as the "Helepolis" (meaning "The Taker of Cities") of the siege of Rhodes in 305 BC, could be as high as 135 feet and as wide as 67.5 feet. Such large engines would require a rack and pinion to be moved effectively. It was manned by 200 soldiers and was divided into nine stories; the different levels housed various types of catapults and ballistae. Subsequent siege towers down through the centuries often had similar engines.
But this huge tower was defeated by the defenders by flooding the ground in front of the wall, creating a moat that caused the tower to get bogged in the mud. The siege of Rhodes illustrates the important point that the larger siege towers needed level ground. Many castles and hill-top towns and forts were virtually invulnerable to siege tower attack simply due to topography. Smaller siege towers might be used on top of siege-mounds, made of earth, rubble and timber mounds in order to overtop a defensive wall. The remains of such a siege-ramp at Masada, for example, has survived almost 2,000 years and can still be seen today.
On the other hand, almost all the largest cities were on large rivers, or the coast, and so did have part of their circuit wall vulnerable to these towers. Furthermore, the tower for such a target might be prefabricated elsewhere and brought dismantled to the target city by water. In some rare circumstances, such towers were mounted on ships to assault the coastal wall of a city: at the siege of Cyzicus during the Third Mithridatic War, for example, towers were used in conjunction with more conventional siege weapons.
One of the oldest references to the mobile siege tower in Ancient China was ironically a written dialogue primarily discussing naval warfare. In the Chinese "Yuejueshu" (Lost Records of the State of Yue) written by the later Han Dynasty author Yuan Kang in the year 52 AD, Wu Zixu (526 BC-484 BC) purportedly discussed different ship types with King Helü of Wu (r. 514 BC-496 BC) while explaining military preparedness. Before labeling the types of warships used, Zixu said:
Nowadays in training naval forces we use the tactics of land forces for the best effect. Thus great wing ships correspond to the army's heavy chariots, little wing ships to light chariots, stomach strikers to battering rams, castle ships to mobile assault towers, and bridge ships to light cavalry.—
Medieval and later use.
With collapse of the Roman Empire in the West into independent states, and the Eastern Roman Empire on the defensive, the use of siege towers reached its height during the medieval period. Siege towers were used when the Avars laid siege unsuccessfully to Constantinople in 626, as the "Chronicon Paschale" recounts:
And in the section from the Polyandrion Gate as far as the Gate of St Romanus he prepared to station twelve lofty siege towers, which were advanced almost as far as the outworks, and he covered them with hides.—
At this siege the attackers also made use of "sows" - mobile armoured shelters which were used throughout the medieval period, and allowed workers to fill in moats with protection from the defenders (thus levelling the ground for the siege towers to be moved to the walls). However, the construction of a sloping talus at the base of a castle wall (as was common in Crusader fortification) could have reduced the effectiveness of this tactic to an extent.
Siege towers also became more elaborate during the medieval period; at the Siege of Kenilworth Castle in 1266, for example, 200 archers and 11 catapults operated from a single tower. Even then, the siege lasted almost a year, making it the longest siege in English history. They were not invulnerable either, as during the Fall of Constantinople in 1453, Ottoman siege towers were sprayed by the defenders with Greek fire.
Siege towers became vulnerable and obsolete with the development of large cannon. They had only ever existed to get assaulting troops over high walls and large cannon also made high walls obsolete as fortification took a new direction. However, later constructions known as battery-towers took on a similar role in the gunpowder age; like siege-towers, these were built out of wood on site for mounting siege artillery. One of these was built by the Russian military engineer Ivan Vyrodkov during the siege of Kazan in 1552 (as part of the Russo-Kazan Wars), and could hold ten large-calibre cannon and 50 lighter cannons. Likely, it was a development of the gulyay-gorod (that is a mobile fortification assembled on wagons or sleds from prefabricated wall-sized shields with holes for cannons). Later battery towers were often used by the Ukrainian Cossacks.
Modern parallels.
On 1 March 2007, police officers were lifted to the upper levels of the building using small boom cranes in a manner similar to siege towers to enter Ungdomshuset in Copenhagen, Denmark. The officers were placed in containers which were lifted to the windows, thus enabling the police to gain access to the illegally occupied structure.

</doc>
<doc id="28840" url="http://en.wikipedia.org/wiki?curid=28840" title="Sharia">
Sharia

Sharia, or sharia law, is the Islamic legal system derived from commands in the basic texts of Islam, the Quran and Hadith.
The term sharia comes from the Arabic language term "sharīʿah", Arabic: شريعة‎ (; also "shari'a", "šarīʿah", ]) which means a body of moral and religious law derived from religious prophecy, as opposed to human legislation. Therefore the Arabic term (unlike the English term derived from it) does not necessarily refer to the laws of Islam, and may for example refer to Jewish Talmudic law. 
Sharia deals with many topics, including crime, politics, and economics, as well as personal matters such as sexual intercourse, hygiene, diet, prayer, everyday etiquette and fasting.
Adherence to Islamic law has served as one of the distinguishing characteristics of the Muslim faith historically, and through the centuries Muslims have devoted much scholarly time and effort on its elaboration. Interpretations of sharia ("fiqh") vary between Islamic sects and respective schools of jurisprudence, yet in its strictest and most historically coherent definition, sharia is considered the infallible law of God.
Sharia is a significant source of legislation in various Muslim countries, namely Saudi Arabia, Sudan, Iran, Brunei, United Arab Emirates and Qatar. In those countries, harsh physical punishments such as flogging and stoning are said to be legally acceptable according to sharia. There are two primary sources of sharia: the precepts set forth in the Quranic verses (ayat), and the example set by the Islamic prophet Muhammad in the Sunnah. Where it has official status, sharia is interpreted by Islamic judges ("qadis") with varying responsibilities for the religious leaders ("imams"). For questions not directly addressed in the primary sources, the application of sharia is extended through consensus of the religious scholars ("ulama") thought to embody the consensus of the Muslim Community ("ijma"). Islamic jurisprudence will also sometimes incorporate analogies from the Quran and Sunnah through qiyas, though many scholars also prefer reasoning ("'aql") to analogy.
The introduction of sharia is a longstanding goal for Islamist movements globally, including in Western countries, but attempts to impose sharia have been accompanied by controversy, violence, and even warfare. Most countries do not recognize sharia; however, some countries in Asia, Africa and Europe recognize sharia and use it as the basis for divorce, inheritance and other personal affairs of their Islamic population. In Britain, the Muslim Arbitration Tribunal makes use of sharia family law to settle disputes, and this limited adoption of sharia is controversial.
The concept of crime, judicial process, justice and punishment embodied in sharia is different from that of secular law. The differences between sharia and secular laws have led to an ongoing controversy as to whether sharia is compatible with secular forms of government, human rights, freedom of thought, and women's rights.
Etymology and origins.
Scholars describe the word "sharia" as an archaic Arabic word denoting "pathway to be followed"(analogous to the Hebrew term Halakhah ["The Way to Go"]), or "path to the water hole". The latter definition comes from the fact that the path to water is the whole way of life in an arid desert environment.
The etymology of "sharia" as a "path" or "way" comes from the Quranic verse#Redirect : "Then we put thee on the (right) "Way" of religion so follow thou that (Way), and follow not the desires of those who know not." Malik Ghulam Farid in his "Dictionary of the Holy Quran", believes the "Way" in 45:18 (quoted above) derives from "shara'a" (as prf. 3rd. p.m. sing.), meaning "He ordained". Other forms also appear: "shara'u"#Redirect as (prf. 3rd. p.m. plu.), "they decreed (a law)"#Redirect ; and "shir'atun" (n.) meaning "spiritual law"#Redirect .
The Arabic word "sharīʿa" has origins in the concept of ‘religious law’; the word is commonly used by Arabic-speaking peoples of the Middle East and designates a prophetic religion in its totality. Thus, sharīʿat Mūsā means religious law of Moses (Judaism), sharīʿat al-Masīḥ means religious law of Christianity, sharīʿat al-Madjūs means religious law of Zoroastrianism. The Arabic expression شريعة الله (God’s Law) is a common translation for תורת אלוהים (‘God’s Law’ in Hebrew) and νόμος τοῦ θεοῦ (‘God’s Law’ in Greek in the New Testament [Rom. 7: 22]). In contemporary Islamic literature, sharia refers to divine law of Islam as revealed by prophet Muhammad, as well as in his function as model and exemplar of the law.
Sharia in the Islamic world is also known as "Qānūn-e Islāmī" (قانون اسلامی).
History.
In Islam, the origin of sharia is the Qu'ran, and traditions gathered from the life of the Islamic Prophet Muhammad (born ca. 570 CE in Mecca).
Sharia underwent fundamental development, beginning with the reigns of caliphs Abu Bakr (632–34) and Umar (634–44) for Sunni Muslims, and Imam Ali for Shia Muslims, during which time many questions were brought to the attention of Muhammad's closest comrades for consultation. During the reign of Muawiya b. Abu Sufyan ibn Harb, ca. 662 CE, Islam undertook an urban transformation, raising questions not originally covered by Islamic law. Since then, changes in Islamic society have played an ongoing role in developing sharia, which branches out into fiqh and Qanun respectively.
The formative period of "fiqh" stretches back to the time of the early Muslim communities. In this period, jurists were more concerned with pragmatic issues of authority and teaching than with theory. Progress in theory was started by 8th and 9th century Islamic scholars Abu Hanifa, Malik bin Anas, Al-Shafi'i, Ahmad ibn Hanbal and others. Al-Shafi‘i is credited with deriving the theory of valid norms for sharia ("uṣūl al-fiqh"), arguing for a traditionalist, literal interpretation of Quran, Hadiths and methodology for law as revealed therein, to formulate sharia.
A number of legal concepts and institutions were developed by Islamic jurists during the classical period of Islam, known as the Islamic Golden Age, dated from the 7th to 13th centuries. These shaped different versions of sharia in different schools of Islamic jurisprudence, called fiqhs.
The Umayyads initiated the office of appointing "qadis", or Islamic judges. The jurisdiction of the "qadi" extended only to Muslims, while non-Muslim populations retained their own legal institutions. The "qadis" were usually pious specialists in Islam. As these grew in number, they began to theorize and systemize Islamic jurisprudence. The Abbasid made the institution of "qadi" independent from the government, but this separation wasn't always respected.
Both the Umayyad caliph Umar II and the Abbasids had agreed that the caliph could not legislate contrary to the Quran or the sunnah. Imam Shafi'i declared: "a tradition from the Prophet must be accepted as soon as it become known...If there has been an action on the part of a caliph, and a tradition from the Prophet to the contrary becomes known later, that action must be discarded in favor of the tradition from the Prophet." Thus, under the Abbasids the main features of sharia were definitively established and sharia was recognized as the law of behavior for Muslims.
In modern times, the Muslim community have divided points of view: secularists believe that the law of the state should be based on secular principles, not on Islamic legal doctrines; traditionalists believe that the law of the state should be based on the traditional legal schools; reformers believe that new Islamic legal theories can produce modernized Islamic law and lead to acceptable opinions in areas such as women's rights. This division persists until the present day (Brown 1996, Hallaq 2001, Ramadan 2005, Aslan 2006, Safi 2003, Nenezich 2006).
There has been a growing religious revival in Islam, beginning in the eighteenth century and continuing today. This movement has expressed itself in various forms ranging from wars to efforts towards improving education.
Definitions and disagreements.
Sharia, in its strictest definition, is a divine law, as expressed in the Quran and Muhammad's example (often called the "sunnah"). As such, it is related to but different from fiqh, which is emphasized as the human interpretation of the law. Many scholars have pointed out that the sharia is not formally a code, nor a well-defined set of rules. The sharia is characterized as a discussion on the duties of Muslims based on both the opinion of the Muslim community and extensive literature. Hunt Janin and Andre Kahlmeyer thus conclude that the sharia is "long, diverse, and complicated."
From the 9th century onward, the power to interpret and refine law in traditional Islamic societies was in the hands of the scholars (ulema). This separation of powers served to limit the range of actions available to the ruler, who could not easily decree or reinterpret law independently and expect the continued support of the community. Through succeeding centuries and empires, the balance between the ulema and the rulers shifted and reformed, but the balance of power was never decisively changed. Over the course of many centuries, imperial, political and technological change, including the Industrial Revolution and the French Revolution, ushered in an era of European world hegemony that gradually included the domination of many of the lands which had previously been ruled by Islamic empires. At the end of the Second World War, the European powers found themselves too weakened to maintain their empires as before. The wide variety of forms of government, systems of law, attitudes toward modernity and interpretations of sharia are a result of the ensuing drives for independence and modernity in the Muslim world.
According to Jan Michiel Otto, Professor of Law and Governance in Developing Countries at Leiden University, "Anthropological research shows that people in local communities often do not distinguish clearly whether and to what extent their norms and practices are based on local tradition, tribal custom, or religion. Those who adhere to a confrontational view of sharia tend to ascribe many undesirable practices to sharia and religion overlooking custom and culture, even if high-ranking religious authorities have stated the opposite." Otto's analysis appears in a paper commissioned by the Netherlands Ministry of Foreign Affairs.
Sources of sharia law.
There are two sources of sharia (understood as the divine law): the Quran and Sunnah. The Quran is viewed as the unalterable word of God. It is considered in Islam an infallible part of sharia. The Quran covers a host of topics including God, personal laws for Muslim men and Muslim women, laws on community life, laws on expected interaction of Muslims with non-Muslims, apostates and ex-Muslims, laws on finance, morals, eschatology, and others. The Sunnah is the life and example of the Islamic prophet Muhammad. The Sunnah's importance as a source of sharia, is confirmed by several verses of the Quran (e.g. #Redirect ). The Sunnah is primarily contained in the hadith or reports of Muhammad's sayings, his actions, his tacit approval of actions and his demeanor. While there is only one Quran, there are many compilations of hadith, with the most authentic ones forming during the sahih period (850 to 915 CE). The six acclaimed Sunni collections were compiled by (in order of decreasing importance) Muhammad al-Bukhari, Muslim ibn al-Hajjaj, Abu Dawood, Tirmidhi, Al-Nasa'i, Ibn Majah. The collections by al-Bukhari and Muslim, regarded the most authentic, contain about 7,000 and 12,000 hadiths respectively (although the majority of entries are repetitions). The hadiths have been evaluated on authenticity, usually by determining the reliability of the narrators that transmitted them. For Shias, the Sunnah include life and sayings of The Twelve Imams.
Quran versus Hadith.
Muslims who reject the Hadith as a source of law, sometimes referred to as Quranists, suggest that only laws derived exclusively from the Quran are valid. They state that the hadiths in modern use are not explicitly mentioned in the Quran as a source of Islamic theology and practice, they were not recorded in written form until more than two centuries after the death of the prophet Muhammed. They also state that the authenticity of the hadiths remains a question.
The vast majority of Muslims, however, consider hadiths, which describe the words, conduct and example set by Muhammad during his life, as a source of law and religious authority second only to the Qur'an. Similarly, most Islamic scholars believe both Quran and sahih hadiths to be a valid source of sharia, with Quranic verse 33.21, among others, as justification for this belief.
Ye have indeed in the Messenger of Allah a beautiful pattern (of conduct) for any one whose hope is in Allah and the Final Day, and who engages much in the Praise of Allah.<br><br>
It is not fitting for a Believer, man or woman, when a matter has been decided by Allah and His Messenger to have any option about their decision: if any one disobeys Allah and His Messenger, he is indeed on a clearly wrong Path.—Qur'an, [Quran ]
For vast majority of Muslims, sharia has historically been, and continues to be derived from both Quran and Sahih Hadiths. The Sahih Hadiths contain "isnad", or a chain of guarantors reaching back to a companion of Muhammad who directly observed the words, conduct and example he set – thus providing the theological ground to consider the hadith to be a sound basis for sharia. For Sunni Muslims, the "musannaf" in Sahih Bukhari and Sahih Muslim is most trusted and relied upon as source for Sunni Sharia. Shia Muslims, however, do not consider the chain of transmitters of Sunni hadiths as reliable, given these transmitters belonged to Sunni side in Sunni-Shia civil wars that followed after Muhammad's death. Shia rely on their own chain of reliable guarantors, trusting compilations such as Kitab al-Kafi and Tahdhib al-Ahkam instead, and later hadiths (usually called "akhbār" by Shi'i). The Shia version of hadiths contain the words, conduct and example set by Muhammad and Imams, which they consider as sinless, infallible and an essential source of sharia for Shi'ite Muslims. However, in substance, the Shi'ite hadiths resemble the Sunni hadiths, with one difference – the Shia hadiths additionally include words and actions of its Imams ("al-hadith al-walawi"), the biological descendants of Muhammad, and these too are considered an important source for sharia by Shi'ites.
Islamic jurisprudence ("Fiqh").
"Fiqh" (school of Islamic jurisprudence) represents the process of deducing and applying sharia principles, as well as the collective body of specific laws deduced from sharia using the fiqh methodology. While Quran and Hadith sources are regarded as infallible, the fiqh standards may change in different contexts. Fiqh covers all aspects of law, including religious, civil, political, constitutional and procedural law. Fiqh deploys the following to create Islamic laws:
If the above two sources do not provide guidance for an issue, then different fiqhs deploy the following in a hierarchical way:
A Madhhab is a Muslim school of law that follows a "fiqh" (school of religious jurisprudence). In the first 150 years of Islam, there were many madhhab. Several of the "Sahābah", or contemporary "companions" of Muhammad, are credited with founding their own. In the Sunni sect of Islam, the Islamic jurisprudence schools of Medina (Al-Hijaz, now in Saudi Arabia) created the Maliki "madhhab", while those in Kufa (now in Iraq) created the Hanafi "madhhab". Abu al-Shafi'i, who started as a student of Maliki school of Islamic law, and later was influenced by Hanafi school of Islamic law, disagreed with some of the discretion these schools gave to jurists, and founded the more conservative Shafi'i "madhhab", which spread from jurisprudence schools in Baghdad (Iraq) and Cairo (Egypt). Ahmad ibn Hanbal, a student of al-Shafi'i, went further in his criticism of Maliki and Hanafi fiqhs, criticizing the abuse and corruption of sharia from jurist discretion and consensus of later generation Muslims, and he founded the more strict, traditionalist Hanbali school of Islamic law. Other schools such as the Jariri were established later, which eventually died out.
Sunni sect of Islam has four major surviving schools of sharia: Hanafi, Maliki, Shafi'i, Hanbali; one minor school is named Ẓāhirī. Shii sect of Islam has three: Ja'fari (major), Zaydi and Ismaili. There are other minority fiqhs as well, such as the Ibadi school of Khawarij sect, and those of Sufi and Ahmadi sects. All Sunni and Shia schools of sharia rely first on the Quran and the sayings/practices of Muhammad in the Sunnah. Their differences lie in the procedure each uses to create Islam-compliant laws when those two sources do not provide guidance on a topic. The Salafi movement creates sharia based on the Quran, Sunnah and the actions and sayings of the first three generations of Muslims.
Hanafi-based sharia spread with the patronage and military expansions led by Turkic Sultans and Ottoman Empire in West Asia, Southeast Europe, Central Asia and South Asia. It is currently the largest madhhab of Sunni Muslims. Maliki-based sharia is predominantly found in West Africa, North Africa and parts of Arabia. Shafii-based sharia spread with patronage and military expansions led by maritime Sultans, and is mostly found in coastal regions of East Africa, Arabia, South Asia, Southeast Asia and islands in the Indian ocean. The Hanbali-based sharia prevails in the smallest Sunni "madhhab", predominantly found in the Arabian peninsula. The Shia Jafari-based sharia is mostly found in Persian region and parts of West Asia and South Asia.
Along with interpretation, each fiqh classifies its interpretation of sharia into one of the following five categories: fard (obligatory), mustahabb (recommended), mubah (neutral), makruh (discouraged), and haraam (forbidden). A Muslim is expected to adhere to that tenet of sharia accordingly.
The recommended, neutral and discouraged categories are drawn largely from accounts of the life of Muhammad. To say a behaviour is sunnah is to say it is recommended as an example of the life and sayings of Muhammad. These categories form the basis for proper behaviour in matters such as courtesy and manners, interpersonal relations, generosity, personal habits and hygiene.
Areas of Islamic law.
The areas of Islamic law include:
Shari'ah law has been grouped in different ways, such as: Family relations, Crime and punishment, Inheritance and disposal of property, The economic system, External and other relations.
"Reliance of the Traveller", an English translation of a fourteenth-century CE reference on the Shafi'i school of fiqh written by Ahmad ibn Naqib al-Misri, organizes sharia law into the following topics: Purification, prayer, funeral prayer, taxes, fasting, pilgrimage, trade, inheritance, marriage, divorce and justice.
In some areas, there are substantial differences in the law between different schools of fiqh, countries, cultures and schools of thought.
Application.
Application by country.
Most Muslim-majority countries incorporate sharia at some level in their legal framework, with many calling it the highest law or the source of law of the land in their constitution. Most use sharia for personal law (marriage, divorce, domestic violence, child support, family law, inheritance and such matters). Elements of sharia are present, to varying extents, in the criminal justice system of many Muslim-majority countries. Saudi Arabia, Yemen, Brunei, Qatar, United Arab Emirates, Iraq, Iran, Pakistan, Afghanistan, Sudan and Mauritania apply the code predominantly or entirely.
Most Muslim-majority countries with sharia-prescribed hudud punishments in their legal code, do not prescribe it routinely and use other punishments instead. The harshest sharia penalties such as stoning, beheading and the death penalty are enforced with varying levels of consistency.
Since 1970s, most Muslim-majority countries have faced vociferous demands from their religious groups and political parties for immediate adoption of sharia as the sole, or at least primary legal framework. Some moderates and liberal scholars within these Muslim countries have argued for limited expansion of sharia.
With the growing muslim immigrant communities in Europe, there have been reports in some media of "no-go zones" being established where sharia law reigns supreme. However, there is no evidence of the existence of "no-go zones", and these allegations are sourced from anti-immigrant groups falsely equating low-income neighborhoods predominantly inhabited by immigrants as "no-go zones." 
Enforcement.
Sharia is enforced in Islamic nations in a number of ways, including "mutaween" and "hisbah".
The mutaween (Arabic: المطوعين، مطوعية‎ "muṭawwiʿīn, muṭawwiʿiyyah") are the government-authorized or government-recognized religious police (or clerical police) of Saudi Arabia. Elsewhere, enforcement of Islamic values in accordance with sharia is the responsibility of "Polisi Perda Syariah Islam" in Aceh province of Indonesia, Committee for the Propagation of Virtue and the Prevention of Vice (Gaza Strip) in parts of Palestine, and Basiji Force in Iran.
Hisbah (Arabic: حسبة‎ "ḥisb(ah)", or hisba) is a historic Islamic doctrine which means "accountability". Hisbah doctrine holds that it is a religious obligation of every Muslim that he or she report to the ruler (Sultan, government authorities) any wrong behavior of a neighbor or relative that violates sharia or insults Islam. The doctrine states that it is the divinely sanctioned duty of the ruler to intervene when such charges are made, and coercively "command right and forbid wrong" in order to keep everything in order according to sharia. Some Salafist suggest that enforcement of sharia under the Hisbah doctrine is the sacred duty of all Muslims, not just rulers. The doctrine of Hisbah in Islam has traditionally allowed any Muslim to accuse another Muslim, ex-Muslim or non-Muslim for beliefs or behavior that may harm Islamic society. This principle has been used in countries such as Egypt, Pakistan and others to bring blasphemy charges against apostates. For example, in Egypt, sharia was enforced on the Muslim scholar Nasr Abu Zayd, through the doctrine of Hasbah, when he committed apostasy. Similarly, in Nigeria, after twelve northern Muslim-majority states such as Kano adopted sharia-based penal code between 1999 and 2000, hisbah became the allowed method of sharia enforcement, where all Muslim citizens could police compliance of moral order based on sharia. In Aceh province of Indonesia, Islamic vigilante activists have invoked Hasbah doctrine to enforce sharia on fellow Muslims as well as demanding non-Muslims to respect sharia. Hisbah has been used in many Muslim majority countries, from Morocco to Egypt and in West Asia to enforce sharia restrictions on blasphemy and criticism of Islam over internet and social media.
Legal and court proceedings.
Sharia judicial proceedings have significant differences from other legal traditions, including those in both common law and civil law. Sharia courts traditionally do not rely on lawyers; plaintiffs and defendants represent themselves. Trials are conducted solely by the judge, and there is no jury system. There is no pre-trial discovery process, and no cross-examination of witnesses. Unlike common law, judges' verdicts do not set binding precedents under the principle of "stare decisis", and unlike civil law, sharia is left to the interpretation in each case and has no formally codified universal statutes.
The rules of evidence in sharia courts also maintain a distinctive custom of prioritizing oral testimony. Witnesses, in a sharia court system, must be faithful, that is Muslim. Male Muslim witnesses are deemed more reliable than female Muslim witnesses, and non-Muslim witnesses considered unreliable and receive no priority in a sharia court. In civil cases, a Muslim woman witness is considered half the worth and reliability than a Muslim man witness. In criminal cases, women witnesses are unacceptable in stricter, traditional interpretations of sharia, such as those found in Hanbali madhhab.
A confession, an oath, or the oral testimony of Muslim witnesses are the main evidence admissible, in sharia courts, for hudud crimes, that is the religious crimes of adultery, fornication, rape, accusing someone of illicit sex but failing to prove it, apostasy, drinking intoxicants and theft. Testimony must be from at least two free Muslim male witnesses, or one Muslim male and two Muslim females, who are not related parties and who are of sound mind and reliable character. Testimony to establish the crime of adultery, fornication or rape must be from four Muslim male witnesses, with some fiqhs allowing substitution of up to three male with six female witnesses; however, at least one must be a Muslim male. Forensic evidence ("i.e.", fingerprints, ballistics, blood samples, DNA etc.) and other circumstantial evidence is likewise rejected in hudud cases in favor of eyewitnesses, a practice which can cause severe difficulties for women plaintiffs in rape cases.
Muslim jurists have debated whether and when coerced confession and coerced witnesses are acceptable. The majority opinion of jurists in the Hanafi madhhab, for example, ruled that torture to get evidence is acceptable and such evidence is valid, but a 17th-century text by Hanafi jurist Muhammad Shaykhzade argued that coerced confession should be invalid; Shaykhzade acknowledged that beating to get confession has been authorized in fatwas by many Islamic jurists.
Quran recommends written contracts in the case of debt-related transactions, and oral contracts for commercial and other civil contracts. Marriage is solemnized as a written financial contract, in the presence of two Muslim male witnesses, and it includes a brideprice (Mahr) payable from a Muslim man to a Muslim woman. The brideprice is considered by a sharia court as a form of debt. Written contracts are paramount, in sharia courts, in the matters of dispute that are debt-related, which includes marriage contracts. Written contracts in debt-related cases, when notarized by a judge, is deemed more reliable.
In commercial and civil contracts, such as those relating to exchange of merchandise, agreement to supply or purchase goods or property, and others, oral contracts and the testimony of Muslim witnesses triumph over written contracts. Sharia system has held that written commercial contracts may be forged. Timur Kuran states that the treatment of written evidence in religious courts in Islamic regions created an incentive for opaque transactions, and the avoidance of written contracts in economic relations. This led to a continuation of a "largely oral contracting culture" in Muslim nations and communities.
In lieu of written evidence, oaths are accorded much greater weight; rather than being used simply to guarantee the truth of ensuing testimony, they are themselves used as evidence. Plaintiffs lacking other evidence to support their claims may demand that defendants take an oath swearing their innocence, refusal thereof can result in a verdict for the plaintiff. Taking an oath for Muslims can be a grave act; one study of courts in Morocco found that lying litigants would often "maintain their testimony 'right up to the moment of oath-taking and then to stop, refuse the oath, and surrender the case." Accordingly, defendants are not routinely required to swear before testifying, which would risk casually profaning the Quran should the defendant commit perjury; instead oaths are a solemn procedure performed as a final part of the evidence process.
Sharia courts treat women and men as unequal, with Muslim woman's life and blood-money compensation sentence (Diyya) as half as that of a Muslim man's life. Sharia also treats Muslims and non-Muslims as unequal in the sentencing process. Human Rights Watch and United States' Religious Freedom Report note that in sharia courts of Saudi Arabia, "The calculation of accidental death or injury compensation is discriminatory. In the event a court renders a judgment in favor of a plaintiff who is a Jewish or Christian male, the plaintiff is only entitled to receive 50 percent of the compensation a Muslim male would receive; all other non-Muslims [Buddhists, Hindus, Jains, Atheists] are only entitled to receive one-sixteenth of the amount a male Muslim would receive".
Saudi Arabia follows Hanbali sharia, whose historic jurisprudence texts considered a Christian or Jew life as half the worth of a Muslim. Jurists of other schools of law in Islam have ruled differently. For example, Shafi'i sharia considers a Christian or Jew life as a third the worth of a Muslim, and Maliki's sharia considers it worth half. The legal schools of Hanafi, Maliki and Shafi'i Sunni Islam as well as those of twelver Shia Islam have considered the life of polytheists and atheists as one-fifteenth the value of a Muslim during sentencing.
Support.
A 2013 survey based on the opinion of 38,000 individuals by the Pew Forum on Religion and Public Life found that support for making sharia the official law of the land is very high in many Muslim-majority Islamic countries. A majority of Muslims favor sharia as the law of land in Afghanistan (99%), Iraq (91%), Niger (86%), Malaysia (86%), Pakistan (84%), Morocco (83%), Bangladesh (82%), Egypt (74%), Indonesia (72%), Jordan (71%), Uganda (66%), Ethiopia (65%), Mali (63%), Ghana (58%), and Tunisia (56%). Among regional Muslim populations elsewhere, significant percentage favored sharia law: Nigeria (71%), Russia (42%), Kyrgyzstan (35%), Lebanon (29%), Kosovo (20%), Tanzania (37%). In Muslim-majority countries such as Egypt, Jordan, Afghanistan, Indonesia, Malaysia, Lebanon and Turkey, 40% to 74% of Muslims wanted sharia law to apply to non-Muslims as well. A 2008 YouGov poll in the United Kingdom found 40% of Muslims interviewed wanted sharia in British law.
Since the 1970s, the Islamist movements have become prominent; their goals are the establishment of Islamic states and sharia not just within their own borders; their means are political in nature. The Islamist power base is the millions of poor, particularly urban poor moving into the cities from the countryside. They are not international in nature (one exception being the Muslim Brotherhood). Their rhetoric opposes western culture and western power. Political groups wishing to return to more traditional Islamic values are the source of threat to Turkey's secular government. These movements can be considered neo-Sharism.
Extremism.
Fundamentalists, wishing to return to basic Islamic religious values and law, have in some instances imposed harsh sharia punishments for crimes, curtailed civil rights and violated human rights. Extremists have used the Quran and their own particular version of sharia to justify acts of war and terror against Muslim as well as non-Muslim individuals and governments, using alternate, conflicting interpretations of sharia and their notions of jihad.
The sharia basis of arguments of those advocating terrorism, however, remain controversial. Some scholars state that Islamic law prohibits the killing of civilian non-combatants; in contrast, others interpret Islamic law differently, concluding that all means are legitimate to reach their aims, including targeting Muslim non-combatants and the mass killing of non-Muslim civilians, in order to universalize Islam. Islam, in these interpretations, "does not make target differences between militaries and civilians but between Muslims and unbelievers. Therefore it is legitimated (sic) to spill civilians’ blood". Other scholars of Islam, interpret sharia differently, stating, according to Engeland-Nourai, "attacking innocent people is not courageous; it is stupid and will be punished on the Day of Judgment [...]. It’s not courageous to attack innocent children, women and civilians. It is courageous to protect freedom; it is courageous to defend one and not to attack".
Criticism.
Compatibility with democracy.
Ali Khan states that "constitutional orders founded on the principles of sharia are fully compatible with democracy, provided that religious minorities are protected and the incumbent Islamic leadership remains committed to the right to recall". Other scholars say sharia is not compatible with democracy, particularly where the country's constitution demands separation of religion and the democratic state.
Courts in non-Muslim majority nations have generally ruled against the implementation of sharia, both in jurisprudence and within a community context, based on sharia's religious background. In Muslim nations, sharia has wide support with some exceptions. For example, in 1998 the Constitutional Court of Turkey banned and dissolved Turkey's Refah Party on the grounds that "Democracy is the antithesis of Sharia", the latter of which Refah sought to introduce.
On appeal by Refah the European Court of Human Rights determined that "sharia is incompatible with the fundamental principles of democracy". Refah's sharia-based notion of a "plurality of legal systems, grounded on religion" was ruled to contravene the European Convention for the Protection of Human Rights and Fundamental Freedoms. It was determined that it would "do away with the State's role as the guarantor of individual rights and freedoms" and "infringe the principle of non-discrimination between individuals as regards their enjoyment of public freedoms, which is one of the fundamental principles of democracy".
Human rights.
Several major, predominantly Muslim countries have criticized the Universal Declaration of Human Rights (UDHR) for its perceived failure to take into account the cultural and religious context of non-Western countries. Iran declared in the UN assembly that UDHR was "a secular understanding of the Judeo-Christian tradition", which could not be implemented by Muslims without trespassing the Islamic law. Islamic scholars and Islamist political parties consider 'universal human rights' arguments as imposition of a non-Muslim culture on Muslim people, a disrespect of customary cultural practices and of Islam. In 1990, the Organisation of Islamic Cooperation, a group representing all Muslim majority nations, met in Cairo to respond to the UDHR, then adopted the Cairo Declaration on Human Rights in Islam.
Ann Elizabeth Mayer points to notable absences from the Cairo Declaration: provisions for democratic principles, protection for religious freedom, freedom of association and freedom of the press, as well as equality in rights and equal protection under the law. Article 24 of the Cairo declaration states that "all the rights and freedoms stipulated in this Declaration are subject to the Islamic "shari'a"".
In 2009, the journal "Free Inquiry" summarized the criticism of the Cairo Declaration in an editorial: "We are deeply concerned with the changes to the Universal Declaration of Human Rights by a coalition of Islamic states within the United Nations that wishes to prohibit any criticism of religion and would thus protect Islam's limited view of human rights. In view of the conditions inside the Islamic Republic of Iran, Egypt, Pakistan, Saudi Arabia, the Sudan, Syria, Bangdalesh, Iraq, and Afghanistan, we should expect that at the top of their human rights agenda would be to rectify the legal inequality of women, the suppression of political dissent, the curtailment of free expression, the persecution of ethnic minorities and religious dissenters — in short, protecting their citizens from egregious human rights violations. Instead, they are worrying about protecting Islam."
H. Patrick Glenn states that sharia is structured around the concept of mutual obligations of a collective, and it considers individual human rights as potentially disruptive and unnecessary to its revealed code of mutual obligations. In giving priority to this religious collective rather than individual liberty, the Islamic law justifies the formal inequality of individuals (women, non-Islamic people). Bassam Tibi states that sharia framework and human rights are incompatible. Abdel al-Hakeem Carney, in contrast, states that sharia is misunderstood from a failure to distinguish "sharia" from "siyasah" (politics).
Freedom of speech.
Blasphemy in Islam is any form of cursing, questioning or annoying God, Muhammad or anything considered sacred in Islam. The sharia of various Islamic schools of jurisprudence specify different punishment for blasphemy against Islam, by Muslims and non-Muslims, ranging from imprisonment, fines, flogging, amputation, hanging, or beheading. In some cases, sharia allows non-Muslims to escape death by converting and becoming a devout follower of Islam.
Blasphemy, as interpreted under sharia, is controversial. Muslim nations have petitioned the United Nations to limit "freedom of speech" because "unrestricted and disrespectful opinion against Islam creates hatred". Other nations, in contrast, consider blasphemy laws as violation of "freedom of speech", stating that freedom of expression is essential to empowering both Muslims and non-Muslims, and point to the abuse of blasphemy laws, where hundreds, often members of religious minorities, are being lynched, killed and incarcerated in Muslim nations, on flimsy accusations of insulting Islam.
Freedom of thought, conscience and religion.
According to the United Nations' Universal Declaration of Human Rights, every human has the right to freedom of thought, conscience and religion; this right includes freedom to change their religion or belief. Sharia has been criticized for not recognizing this human right. According to scholars of traditional Islamic law, the applicable rules for religious conversion under sharia are as follows:
According to sharia theory, conversion of disbelievers and non-Muslims to Islam is encouraged as a religious duty for all Muslims, and leaving Islam (apostasy), expressing contempt for Islam (blasphemy), and religious conversion of Muslims is prohibited. Not all Islamic scholars agree with this interpretation of sharia theory. In practice, as of 2011, 20 Islamic nations had laws declaring apostasy from Islam as illegal and a criminal offense. Such laws are incompatible with the UDHR's requirement of freedom of thought, conscience and religion. In another 2013 report based on international survey of religious attitudes, more than 50% of Muslim population in 6 out of 49 Islamic countries supported death penalty for any Muslim who leaves Islam (apostasy). However it is also shown that the majority of Muslims in the 43 nations surveyed did not agree with this interpretation of sharia.
Some scholars claim sharia allows religious freedom because a Shari'a verse teaches, "there is no compulsion in religion." Other scholars claim sharia recognizes only one proper religion, considers apostasy as sin punishable with death, and members of other religions as kafir (infidel); or hold that Shari'a demands that all apostates and kafir must be put to death, enslaved or be ransomed. Yet other scholars suggest that Shari'a has become a product of human interpretation and inevitably leads to disagreements about the “precise contents of the Shari'a." In the end, then, what is being applied is not sharia, but what a particular group of clerics and government decide is sharia. It is these differing interpretations of Shari'a that explain why many Islamic countries have laws that restrict and criminalize apostasy, proselytism and their citizens' freedom of conscience and religion.
LGBT rights.
Homosexual intercourse is illegal under sharia law, though the prescribed penalties differ from one school of jurisprudence to another. For example, only a few Muslim-majority countries impose the death penalty for acts perceived as sodomy and homosexual activities: Iran, Saudi Arabia, and Somalia. In other Muslim-majority countries such as Egypt, Iraq, and the Indonesian province of Aceh, same-sex sexual acts are illegal, and LGBT people regularly face violence and discrimination. In Turkey, Bahrain and Jordan, homosexual acts between consenting individuals are legal. There is a new movement of LGBT Muslims, particularly in Jordan, the UK with Imaan and Al-Fatiha in America. Books such as "Islam and Homosexuality" by Siraj Scott has also contributed to playing a proactive role in LGBT- and Islam-related ideas.
Women.
Domestic violence.
Many scholars claim Shari'a law encourages domestic violence against women, when a husband suspects "nushuz" (disobedience, disloyalty, rebellion, ill conduct) in his wife. Other scholars claim wife beating, for "nashizah", is not consistent with modern perspectives of the Quran.
One of the verses of the Quran relating to permissibility of domestic violence is Surah 4:34. In deference to Surah 4:34, many nations with Shari'a law have refused to consider or prosecute cases of domestic abuse. Shari'a has been criticized for ignoring women's rights in domestic abuse cases. Musawah/CEDAW, KAFA and other organizations have proposed ways to modify Shari'a-inspired laws to improve women's rights in Islamic nations, including women's rights in domestic abuse cases.
Personal status laws and child marriage.
Shari'a is the basis for personal status laws in most Islamic majority nations. These personal status laws determine rights of women in matters of marriage, divorce and child custody. A 2011 UNICEF report concludes that Shari'a law provisions are discriminatory against women from a human rights perspective. In legal proceedings under Shari'a law, a woman’s testimony is worth half of a man’s before a court.
Except for Iran, Lebanon and Bahrain which allow child marriages, the civil code in Islamic majority countries do not allow child marriage of girls. However, with Shari'a personal status laws, Shari'a courts in all these nations have the power to override the civil code. The religious courts permit girls less than 18 years old to marry. As of 2011, child marriages are common in a few Middle Eastern countries, accounting for 1 in 6 all marriages in Egypt and 1 in 3 marriages in Yemen. However, the average age at marriage in most Middle Eastern countries is steadily rising and is generally in the low to mid 20's for women. Rape is considered a crime in all countries, but Shari'a courts in Bahrain, Iraq, Jordan, Libya, Morocco, Syria and Tunisia in some cases allow a rapist to escape punishment by marrying his victim, while in other cases the victim who complains is often prosecuted with the crime of "Zina" (adultery).
Women's right to property and consent.
Sharia grants women the right to inherit property from other family members, and these rights are detailed in the Quran. A woman's inheritance is unequal and less than a man's, and dependent on many factors.#Redirect For instance, a daughter's inheritance is usually half that of her brother's.#Redirect 
Until the 20th century, Islamic law granted Muslim women certain legal rights, such as the right to own property received as Mahr (brideprice) at her marriage, that Western legal systems did not grant to women. However, Islamic law does not grant non-Muslim women the same legal rights as the few it did grant Muslim women. Sharia recognizes the basic inequality between master and women slave, between free women and slave women, between Believers and non-Believers, as well as their unequal rights. Sharia authorized the institution of slavery, using the words "abd" (slave) and the phrase "ma malakat aymanukum" ("that which your right hand owns") to refer to women slaves, seized as captives of war. Under Islamic law, Muslim men could have sexual relations with female captives and slaves without her consent.
Slave women under sharia did not have a right to own property, right to free movement or right to consent. Sharia, in Islam's history, provided religious foundation for enslaving non-Muslim women (and men), as well as encouraged slave's manumission. However, manumission required that the non-Muslim slave first convert to Islam. Non-Muslim slave women who bore children to their Muslim masters became legally free upon her master's death, and her children were presumed to be Muslims as their father, in Africa, and elsewhere.
Starting with the 20th century, Western legal systems evolved to expand women's rights, but women's rights under Islamic law have remained tied to Quran, hadiths and their faithful interpretation as sharia by Islamic jurists.
Parallels with Western legal systems.
Elements of Islamic law have influenced western legal systems. As example, the influence of Islamic influence on the development of an international law of the sea" can be discerned alongside that of the Roman influence.
Makdisi states Islamic law also influenced the legal scholastic system of the West. The study of legal text and degrees have parallels between Islamic studies of sharia and the Western system of legal studies. For example, the status of "faqih" (meaning "master of law"), "mufti" (meaning "professor of legal opinions") and "mudarris" (meaning "teacher"), which were later translated into Latin as "magister", "professor" and "doctor" respectively.
There are differences between Islamic and Western legal systems. For example, sharia classically recognizes only natural persons, and never developed the concept of a legal person, or corporation, i.e., a legal entity that limits the liabilities of its managers, shareholders, and employees; exists beyond the lifetimes of its founders; and that can own assets, sign contracts, and appear in court through representatives. Interest prohibitions also imposed secondary costs by discouraging record keeping, and delaying the introduction of modern accounting. Such factors, according to Timur Kuran, have played a significant role in retarding economic development in the Middle East.

</doc>
<doc id="28841" url="http://en.wikipedia.org/wiki?curid=28841" title="Sunnah">
Sunnah

Sunnah is the way of life prescribed as normative for Muslims on the basis of the teachings and practices of the Islamic prophet Muhammad and interpretations of the islamic holy book Quran. The word "sunnah" (سنة, ], plural سنن "sunan" ]) is derived from the root (سن ]), meaning smooth and easy flow or direct flow path. The word literally means a clear and well trodden path. In the discussion of the sources of religion, "sunnah" denotes the practices of Muhammad that he taught and practically instituted as a teacher of the sharī‘ah and the best exemplar. According to Muslim belief, this practice is to be adhered to in fulfilling the divine injunctions, carrying out religious rites, and moulding life in accord with the will of God. Instituting these practices was, as the Quran states, a part of Muhammad's responsibility as a Messenger of Allah. (Quran , ).
The "sunnah" of Muhammad includes his specific words, habits, practices, and silent approvals: it is significant because it addresses ways of life dealing with friends, family and government. Recording the "sunnah" was an Arabian tradition and, once people converted to Islam, they brought this custom to their religion. The "sunnah" is a source of Islamic law, second only to the Quran. The term "Sunni" denotes those who claim to practice these usages, as part of the "Ummah".
Etymology.
"Sunnah" (سنة ], plural سنن "sunan" ]) is an Arabic word that means "habit" or "usual practice". Sunnis are also referred to as Ahl as-Sunnah wa'l-Jamā'ah ("people of the tradition and the community (of Muhammad)") or "Ahl as-Sunnah" for short.
Sunnah and hadith.
In the context of biographical records of Muhammad, "sunnah" often stands synonymous with "hadith" since most of the personality traits of Muhammad are known from descriptions of him, his sayings and his actions after becoming a prophet at the age of forty. "Sunnah", which consists of what Muhammad believed, implied, or tacitly approved, was recorded by his companions in "hadith". Allegiance to the tribal "sunnah" had been partially replaced by submission to a new universal authority and the sense of brotherhood among Muslims.
Early Sunni scholars often considered "sunnah" equivalent to the biography of Muhammed ("sira") as the "hadith" which was then poorly validated while contemporary accounts of Muhammad's life were better known. As the "hadith" came to be better documented and the scholars who validated them gained prestige, the "sunnah" came often to be known mostly through the "hadith", especially as variant or fictional biographies of Muhammad spread.
How far "hadith" contributes to "sunnah" is disputed and highly dependent on context. Classical Islam often equates the "sunnah" with the "hadith". Scholars who studied the narrations according to their context ("matn") as well as their transmission ("isnad") in order to discriminate between them were influential in the development of early Muslim philosophy. In the context of sharia, Imam Malik and the Hanafi scholars are assumed to have differentiated between the two: for example Imam Malik is said to have rejected some traditions that reached him because, according to him, they were against the "established practice of the people of Medina".
Sunnah and Islam.
Passages in the Qur'an command that the Prophet be followed, such as 3:32, "Obey God and His messenger". For Muslims the imitation of Muhammad helps one to know and be loved by God: one lives in constant remembrance of God.
Shi'a Muslims do not use the six major "hadith" collections followed by the Sunni. Instead, their primary collections were written by three authors known as the 'Three Muhammads'. They are: "Kitab al-Kafi" by Muhammad ibn Ya'qub al-Kulayni al-Razi (329 AH), "Man la yahduruhu al-Faqih" by Muhammad ibn Babuya and "Al-Tahdhib" and "Al-Istibsar" both by Shaykh Muhammad Tusi. Unlike Akhbari Twelver Shi'a, Usuli Twelver Shi'a scholars do not believe that everything in the four major books is authentic. In Shia "hadith" one often finds sermons attributed to Ali in The Four Books or in the Nahj al-Balagha.
Sunni traditional Muslims believe that the "sunnah" is justified by verses such as "A similar (favour have ye already received) in that We have sent among you a Messenger of your own, rehearsing to you Our Signs, and sanctifying you, and instructing you in Scripture and Wisdom, and in new knowledge.(2:151) The verse "Verily in the messenger of God you have a good example for him who looks unto God and the Last Day and remembers God much" (33:21) further emphasizes that Muhammad's mission is to teach and exemplify the Qur'an, not just to relate its verses and leave. Muhammad was not to be worshipped or deified but his role was to deliver the Qur'an with comprehensive explanations of how to live according to the Qur'anic guidelines preserved in Sunnah. The teachings of "wisdom" have been declared to be a function of Muhammad along with the teachings of the scripture.
For a better understanding of "wisdom" ("hikmah"), one can refer to the Quran itself. For example, verse 4:113 states; "God reveals the Book (i.e. the books of revealed religion, especially the Qur'an) to you, and wisdom, and teaches you that which you did not know. The grace of God towards you has been very great". Verse 2:231 states; "And remember God's grace upon you, and that which He has revealed unto you of Scripture and wisdom whereby He exhorts you". Verse 33:34 states, "And bear in mind which is recited in your houses of the revelations of God and of wisdom".
Some have said that "wisdom" here is simply another name for "sunnah". Therefore, along with divine revelation the "sunnah" was directly taught by God. Modern Sunni scholars are beginning to examine both the "sira" and the "hadith" in order to justify modifications to jurisprudence ("fiqh"). The "sunna", in one form or another, would retain its central role in providing a moral example and ethical guidance.
Alternative views on Sunnah.
According to the view of some Sufi Muslims who incorporate both the outer and inner reality of Prophet Muhammad, the deeper and true Sunnah are the noble characteristics and inner state of Prophet Muhammad. To them the Prophet's attitude, his piety, the quality of his character constitute the truer and deeper aspect of what it means by Sunnah in Islam, rather than the external aspects alone. They argue that the external customs of the Prophet loses its meaning without the inner attitude and also many Hadeeths are simply custom of the Arabs, not something that is unique to Prophet. and "Khuluqin Azim" or 'Exalted Character' in the Quran, real Sunnah cannot be upheld.
According to some scholars, Sunnah predates both the Quran as well as Muhammad, and is actually the tradition of the Prophets of God, specifically the tradition of Abraham."(This was Our) Sunnah (rule or way) with the Messengers We sent before you (O Muhammad SAW), and you will not find any alteration in Our Sunnah (rule or way, etc.).
Al-Israa (17:77)
Perform As-Salat (Iqamat-as-Salat ie. Prayer) from mid-day till the darkness of the night (i.e. the Zuhr, 'Asr, Maghrib, and 'Isha' prayers), and recite the Quran in the early dawn (i.e. the morning prayer). Verily, the recitation of the Quran in the early dawn is ever witnessed (attended by the angels in charge of mankind of the day and the night).
Al-Israa (17:78) 
And in some parts of the night (also) offer the Salat (prayer) with it (i.e. recite the Quran in the prayer), as an additional prayer (Tahajjud optional prayer Nawafil) for you (O Muhammad SAW). It may be that your Lord will raise you to Maqaman Mahmuda (a station of praise and glory, i.e. the highest degree in Paradise!). Al-Israa (17:79)" How the prayer is practiced is retrieved from hadeeths(story, narrations, interpretations, traditions) of Muhammad by his companions. 
A broad form of Sunnah was already being practised by the Christians, Jews and the Arab descendants of Ishmael, when Muhammad reinstituted this practice as an integral part of Islam. Both Sunnah and Quran are equally authentic and the former includes worship rituals like salat, Zakah, Hajj, fasting in Ramadan as well as customs like circumcision.

</doc>
<doc id="28845" url="http://en.wikipedia.org/wiki?curid=28845" title="Safe sex">
Safe sex

Safe sex is sexual activity engaged in by people who have taken precautions to protect themselves against sexually transmitted infections (STIs) such as HIV/AIDS. It is also referred to as safer sex or protected sex, while unsafe or unprotected sex is sexual activity engaged in without precautions, especially forgoing condom use. 
Some sources prefer the term "safer sex" to more precisely reflect the fact that these practices reduce, but do not always completely eliminate, the risk of disease transmission. The term "sexually transmitted infections" ("STIs") has gradually become preferred over "sexually transmitted diseases" ("STDs") among medical sources, as it has a broader range of meaning; a person may be infected, and may potentially infect others, without showing signs of disease.
Safe sex practices became more prominent in the late 1980s as a result of the AIDS epidemic. Promoting safe sex is now one of the aims of sex education. Safe sex is regarded as a harm reduction strategy aimed at reducing risks. The risk reduction of safe sex is not absolute; for example, the reduced risk to the receptive partner of acquiring HIV from HIV-seropositive partners not wearing condoms compared to when they wear them is estimated to be about a four- to fivefold. Safe sex is effective in avoiding STIs only if both sexual partners agree on it and stick to it. During sexual penetration while using a condom, for example, the male could intentionally pull off the condom and continue penetrating without the female or male receptive partner's consent and notice. This is a high-risk behavior that betrays trust, as well as spreading disease.
Although some safe sex practices can be used as birth control (contraception), most forms of contraception do not protect against all or any STIs; likewise, some safe sex practices, like partner selection and low risk sex behavior, are not effective forms of contraception.
Terminology.
The term "safer sex" in Canada and the United States has gained greater use by health workers, reflecting that risk of transmission of sexually transmitted infections in various sexual activities is a continuum. The term "safe sex" is still in common use in the United Kingdom, Australia and New Zealand.
Although "safe sex" is used by individuals to refer to protection against both pregnancy and HIV/AIDS or other STI transmissions, the term was primarily derived in response to the HIV/AIDS epidemic. It is believed that the term of "safe sex" was used in the professional literature in 1984, in the content of a paper on the psychological effect that HIV/AIDS may have on homosexual men. The term was related with the need to develop educational programs for the group considered at risk, homosexual men. A year later, the same term appeared in an article in "The New York Times." This article emphasized that most specialists advised their AIDS patients to practice safe sex. The concept included limiting the number of sexual partners, using prophylactics, avoiding bodily fluid exchange, and resisting the use of drugs that reduced inhibitions for high-risk sexual behavior. Moreover, in 1985, the first safe sex guidelines were established by the 'Coalition for Sexual Responsibilities'. According to these guidelines, safe sex was practiced by using condoms also when engaging in anal or oral sex.
Although this term was primarily used in conjunction with the homosexual male population, in 1986 the concept was spread to the general population. Various programs were developed with the aim of promoting safe sex practices among college students. These programs were focused on promoting the use of the condom, a better knowledge about the partner's sexual history and limiting the number of sexual partners. The first book on this subject appeared in the same year. The book was entitled "Safe Sex in the Age of AIDS", it had 88 pages and it described both positive and negative approaches to the sexual life. Sexual behavior could be either safe (kissing, hugging, massage, body-to-body rubbing, mutual masturbation, exhibitionism and voyeurism, phone sex, sadomasochism without bruising or bleeding, and use of separate sex toys); possibly safe (use of condoms); and unsafe.
In 1997, specialists in this matter promoted the use of condoms as the most accessible safe sex method (besides abstinence) and they called for TV commercials featuring condoms. During the same year, the Catholic Church in the United States issued their own "safer sex" guidelines on which condoms were listed, though two years later the Vatican urged chastity and heterosexual marriage, attacking the American Catholic bishops' guidelines.
A study carried out in 2006 by Californian specialists showed that the most common definitions of safe sex are condom use (68% of the interviewed subjects), abstinence (31.1% of the interviewed subjects), monogamy (28.4% of the interviewed subjects) and safe partner (18.7% of the interviewed subjects).
"Safer sex" is thought to be a more aggressive term which may make it more obvious to individuals that any type of sexual activity carries a certain degree of risk.
The term "safe love" has also been used, notably by the French Sidaction in the promotion of men's underpants incorporating a condom pocket and including the red ribbon symbol in the design, which were sold to support the charity.
Safe sex precautions.
Solitary sex.
Known as "autoeroticism", solitary sexual activity is relatively safe. Masturbation, the simple act of stimulating one's own genitalia, is safe so long as contact is not made with other people's bodily fluids. Some activities, such as phone sex and cybersex, that allow for partners to engage in sexual activity without being in the same room, eliminate the risks involved with exchanging bodily fluids.
Non-penetrative sex.
A range of sex acts, sometimes called "outercourse", can be enjoyed with significantly reduced risks of infection or pregnancy. U.S. President Bill Clinton's surgeon general, Joycelyn Elders, tried to encourage the use of these practices among young people, but her position encountered opposition from a number of outlets, including the White House itself, and resulted in her being fired by President Clinton in December 1994.
Non-penetrative sex includes practices such as kissing, mutual masturbation, rubbing or stroking and, according to the Health Department of Western Australia, this sexual practice may prevent pregnancy and most STIs. However, non-penetrative sex may not protect against infections that can be transmitted skin-to-skin such as herpes and genital warts.
Barrier protection.
Various protective devices are used to avoid contact with blood, vaginal fluid, semen or other contaminant agents (like skin, hair and shared objects) during sexual activity. Sexual activity using these devices is called protected sex.
When latex barriers are used, oil-based lubrication can break down the structure of the latex and remove the protection it provides.
Condoms (male or female) are used to protect against STIs, and used with other forms of contraception to improve contraceptive effectiveness. For example, simultaneously using both the male condom and spermicide (applied separately, not pre-lubricated) is believed to reduce perfect-use pregnancy rates to those seen among implant users. However, if two condoms are used simultaneously (male condom on top of male condom, or male condom inside female condom), this increases the chance of condom failure.
Proper use of barriers, such as condoms, depends on the cleanliness of surfaces of the barrier, handling can pass contamination to and from surfaces of the barrier unless care is taken.
Studies of latex condom performance during use reported breakage and slippage rates varying from 1.46% to 18.60%. Condoms must be put on before any bodily fluid could be exchanged, and they must be used also during oral sex.
Female condoms are made of two flexible polyurethane rings and a loose-fitting polyurethane sheath. According to laboratory testing, female condoms are effective in preventing the leakage of body fluids and therefore the transmission of STIs and HIV. Several studies show that between 50% and 73% of women who have used this type of condoms during intercourse find them as or more comfortable than male condoms. On the other hand, acceptability of these condoms among the male population is somewhat less, at approximately 40%. Because the cost of female condoms is higher than male condoms, there have been studies carried out with the aim of detecting whether they can be reused. Research has shown that structural integrity of polyurethane female condoms is not damaged during up to five uses if it is disinfected with water and household bleach. However, regardless of this study, specialists still recommend that female condoms be used only once and then discarded.
Other precautions.
Acknowledging that it is usually impossible to have entirely risk-free sex with another person, proponents of safe sex recommend that some of the following methods be used to minimize the risks of STI transmission and unwanted pregnancy.
Limitations.
While the use of condoms can reduce transmission of HIV and other infectious agents, it does not do so completely. One study has suggested condoms might reduce HIV transmission by 85% to 95%; effectiveness beyond 95% was deemed unlikely because of slippage, breakage, and incorrect use. It also said, "In practice, inconsistent use may reduce the overall effectiveness of condoms to as low as 60–70%".p. 40.
During each act of anal intercourse, the risk of the receptive partner acquiring HIV from HIV seropositive partners not using condoms is about 1 in 120. Among people using condoms, the receptive partner's risk declines to 1 in 550, a four- to fivefold reduction. Where the partner's HIV status is unknown, "Estimated per-contact risk of protected receptive anal intercourse with HIV-positive and unknown serostatus partners, including episodes in which condoms failed, was two thirds the risk of unprotected receptive anal intercourse with the comparable set of partners."p. 310.
In March 2013, Bill Gates offered a US$100,000 grant through his foundation for a condom design that "significantly preserves or enhances pleasure" to encourage more males to adopt the use of condoms for safer sex. The grant information states: “The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?” The project has been named the "Next Generation Condom" and anyone who can provide a "testable hypothesis" is eligible to apply.
Ineffective methods.
Most methods of contraception, except for certain forms of "outercourse" and the barrier methods, are not effective at preventing the spread of STIs. This includes the birth control pills, vasectomy, tubal ligation, periodic abstinence and all non-barrier methods of pregnancy prevention.
The spermicide Nonoxynol-9 has been claimed to reduce the likelihood of STI transmission. However, a recent study by the World Health Organization has shown that Nonoxynol-9 is an irritant and can produce tiny tears in mucous membranes, which may increase the risk of transmission by offering pathogens more easy points of entry into the system. Condoms with Nonoxynol-9 lubricant do not have enough spermicide to increase contraceptive effectiveness and are not to be promoted.
The use of diaphragm or contraceptive sponge provides some women with better protection against certain sexually transmitted diseases, but they are not effective for all STIs.
The hormonal protecting methods are by no means effective against transmission of STIs, even though they are more than 95% effective against unwanted pregnancies. Most common hormonal methods are the oral contraceptive pill, depoprogesterone, the vaginal ring and the patch.
The copper intrauterine device and the hormonal intrauterine device provide an up to 99% protection against pregnancies but no protection against STIs. Women with copper intrauterine device present however a greater risk of being exposed to any type of STI, especially gonorrhea or chlamydia.
Coitus interruptus (or "pulling out"), in which the penis is removed from the vagina, anus, or mouth before ejaculation, is not safe sex and can result in STI transmission. This is because of the formation of pre-ejaculate, a fluid that oozes from the urethra before actual ejaculation, may contain pathogens such as HIV. Additionally, the microbes responsible for some diseases, including genital warts and syphilis, can be transmitted through skin-to-skin contact, even if the partners never engage in oral, vaginal, or anal sexual intercourse.
Abstinence.
Sexual abstinence is sometimes promoted as a way to avoid the risks associated with sexual contact, though STIs may also be transmitted through non-sexual means, or by involuntary sex. HIV may be transmitted through contaminated needles used in tattooing, body piercing, or injections. Medical or dental procedures using contaminated instruments can also spread HIV, while some health-care workers have acquired HIV through occupational exposure to accidental injuries with needles. Evidence does not support the use of abstinence only sex education. Abstinence-only education programs have been found to be ineffective in decreasing rates of HIV infection in the developed world and unplanned pregnancy.
Some groups, such as some Christian denominations, oppose sex outside marriage and therefore object to safe-sex education programs because they believe that providing such education promotes promiscuity. Virginity pledges and sexual abstinence education programs are often promoted in lieu of contraceptives and safe-sex education programs. This may entail exposing some teenagers to increased risk of sexually transmitted infections, because about 60 percent of teenagers who pledge virginity until marriage do engage in pre-marital sex and are then one-third less likely to use contraceptives than their peers who have received more conventional sex education.
Anal sex.
Unprotected anal penetration is a high risk activity, regardless of sexual orientation. Anal sex is a higher risk activity than vaginal intercourse because the thin tissues of the anus and rectum can be easily damaged. Slight injuries can allow the passage of bacteria and viruses, including HIV. This includes by the use of anal toys.
Condoms may be more likely to break during anal sex than during vaginal sex, increasing the risk.
Anal sex is practiced by many heterosexuals, as well as homosexual couples. The anal area has many erotic nerve endings in both men and women. Because of this, many couples (heterosexual or homosexual) can derive pleasure from some form of 'bottom stimulation'. Safety measures are required also when anal sex occurs between heterosexual partners. Apart from the STI transmission risks, other risks such as infection are high regarding anal intercourse. The main risks which individuals are exposed to when performing anal sex are the transmission of HIV, Hepatitis C and A and "Escherichia coli" and HPV.
Some researchers suggest that although gay men are more likely to engage in anal sex, heterosexual couples are more likely not to use condoms when doing so. Other researchers state that gay men are not necessarily more likely to engage in anal sex than heterosexual couples.
Precautions.
Anal sex should be avoided by couples in which one of the partners has been diagnosed with an STI until the treatment has proven to be effective.
In order to make anal sex safer, the couple must ensure that the anal area is clean and the bowel empty and the partner on whom anal penetration occurs should be able to relax. Regardless of whether anal penetration occurs by using a finger or the penis, the condom is the best barrier method to prevent transmission of STI.
Since the rectum can be easily damaged, the use of lubricants is highly recommended even when penetration occurs by using the finger. Especially for beginners, using a condom on the finger is both a protection measure against STI and a lubricant source. Most condoms are lubricated and they allow less painful and easier penetration. Oil-based lubricants damage latex and should not be used with condoms; water-based and silicone-based lubricants are available instead. Non-latex condoms are available for people who are allergic to latex (e.g., polyurethane condoms that are compatible with both oil-based and water-based lubricants). The "female condom" may also be used effectively by the anal receiving partner.
Anal stimulation with a sex toy requires similar safety measures to anal penetration with a penis, in this case using a condom on the sex toy in a similar way.
It is important that the man washes and cleans his penis after anal intercourse if he intends to penetrate the vagina. Bacteria from the rectum are easily transferred to the vagina, which may cause vaginal infections.
When anal-oral contact occurs, protection is required since this is a risky sexual behavior in which illnesses as Hepatitis A or STIs can be easily transmitted, as well as enteric infections. The dental dam or the plastic wrap are effective protection means whenever anilingus is performed.
Sex toys.
Putting a condom on a sex toy provides better sexual hygiene and can help to prevent transmission of infections if the sex toy is shared, provided the condom is replaced when used by a different partner. Some sex toys are made of porous materials, and pores retain viruses and bacteria, which makes it necessary to clean sex toys thoroughly, preferably with use of cleaners specifically for sex toys. Glass sex toys are non-porous and more easily sterilized between uses.
In cases in which one of the partners is treated for an STI, it is recommended that the couple will not use sex toys until the treatment has proved to be effective.
All sex toys have to be properly cleaned after use. The way in which a sex toy is cleaned varies on the type of material it is made of. Some sex toys can be boiled or cleaned in a dishwasher. Most of the sex toys come with advice on the best way to clean and store them and these instructions should be carefully followed. A sex toy should be cleaned not only when it is shared with other individuals but also when it is used on different parts of the body (such as mouth, vagina or anus).
A sex toy should regularly be checked for scratches or breaks that can be breeding ground for bacteria. It is best if the damaged sex toy is replaced by a new undamaged one. Even more hygiene protection should be considered by pregnant women when using sex toys. Sharing any type of sex toy that may draw blood, like whips or needles, is not recommended, and is not safe. 
The best way to prevent being infected or infecting someone with an STI is by using protection during sexual intercourse.

</doc>
<doc id="28846" url="http://en.wikipedia.org/wiki?curid=28846" title="STD">
STD

STD may refer to:

</doc>
<doc id="28848" url="http://en.wikipedia.org/wiki?curid=28848" title="Scabies">
Scabies

Scabies (from Latin: "scabere", "to scratch"), also known colloquially as the seven-year itch, is a contagious skin infection caused by the mite "Sarcoptes scabiei". The mite is a tiny, and usually not directly visible, parasite which burrows under the host's skin, which in most people causes an intense itching sensation caused by an allergic response. The infection in animals other than humans is caused by a different but related mite species, and is called sarcoptic mange.
Scabies is classified by the World Health Organization as a water-related disease. The disease may be transmitted from objects, but is most often transmitted by direct skin-to-skin contact, with a higher risk with prolonged contact. Initial infections require four to six weeks to become symptomatic. Reinfection, however, may manifest symptoms within as few as 24 hours. Because the symptoms are allergic, their delay in onset is often mirrored by a significant delay in relief after the parasites have been eradicated. Crusted scabies, formerly known as Norwegian scabies, is a more severe form of the infection often associated with immunosuppression.
Scabies is one of the three most common skin disorders in children, along with tinea and pyoderma. As of 2010 it affects approximately 100 million people (1.5% of the world population) and is equally common in both sexes.
Signs and symptoms.
The characteristic symptoms of a scabies infection include intense itching and superficial burrows. The burrow tracks are often linear, to the point that a neat "line" of four or more closely placed and equally developed mosquito-like "bites" is almost diagnostic of the disease.
Itching.
In the classic scenario, the itch is made worse by warmth, and is usually experienced as being worse at night, possibly because there are fewer distractions. As a symptom, it is less common in the elderly.
Rash.
The superficial burrows of scabies usually occur in the area of the finger webs, feet, ventral wrists, elbows, back, buttocks, and external genitals. Except in infants and the immunosuppressed, infection generally does not occur in the skin of the face or scalp. The burrows are created by excavation of the adult mite in the epidermis.
In most people, the trails of the burrowing mites are linear or "s"-shaped tracks in the skin often accompanied by rows of small, pimple-like mosquito or insect bites. These signs are often found in crevices of the body, such as on the webs of fingers and toes, around the genital area, in stomach folds of the skin, and under the breasts of women.
Symptoms typically appear two to six weeks after infestation for individuals never before exposed to scabies. For those having been previously exposed, the symptoms can appear within several days after infestation. However, it is not unknown for symptoms to appear after several months or years. Acropustulosis, or blisters and pustules on the palms and soles of the feet, are characteristic symptoms of scabies in infants.
Crusted scabies.
The elderly and people with an impaired immune system, such as HIV, cancer, or those on immunosuppressive medications, are susceptible to crusted scabies (formerly called Norwegian scabies). On those with weaker immune systems, the host becomes a more fertile breeding ground for the mites, which spread over the host's body, except the face. Sufferers of crusted scabies exhibit scaly rashes, slight itching, and thick crusts of skin that contain thousands of mites. Such areas make eradication of mites particularly difficult, as the crusts protect the mites from topical miticides/scabicides, necessitating prolonged treatment of these areas.
Cause.
In the 18th century, Italian biologist Diacinto Cestoni (1637–1718) described the mite now called "Sarcoptes scabiei", variety "hominis", as the cause of scabies. "Sarcoptes" is a genus of skin parasites and part of the larger family of mites collectively known as scab mites. These organisms have eight legs as adults, and are placed in the same phylogenetic class (Arachnida) as spiders and ticks.
"Sarcoptes scabiei" mites are under 0.5 mm in size but are sometimes visible as pinpoints of white. Pregnant females tunnel into the dead, outermost layer (stratum corneum) of a host's skin and deposit eggs in the shallow burrows. The eggs hatch into larvae in three to ten days. These young mites move about on the skin and molt into a "nymphal" stage, before maturing as adults, which live three to four weeks in the host's skin. Males roam on top of the skin, occasionally burrowing into the skin. In general, the total number of adult mites infesting a healthy hygienic person with non-crusted scabies is small; about 11 females in burrows, on average.
The movement of mites within and on the skin produces an intense itch, which has the characteristics of a delayed cell-mediated inflammatory response to allergens. IgE antibodies are present in the serum and the site of infection, which react to multiple protein allergens in the body of the mite. Some of these cross-react to allergens from house-dust mites. Immediate antibody-mediated allergic reactions (wheals) have been elicited in infected persons, but not in healthy persons; immediate hypersensitivity of this type is thought to explain the observed far more rapid allergic skin response to reinfection seen in persons having been previously infected (especially having been infected within the previous year or two). Because the host develops the symptoms as a reaction to the mites' presence over time, there is typically a delay of four to six weeks between the onset of infestation and the onset of itching. Similarly, symptoms often persist for one to several weeks after successful eradication of the mites. As noted, those re-exposed to scabies after successful treatment may exhibit symptoms of the new infestation in a much shorter period—as little as one to four days.
Scabies is contagious and can be contracted through prolonged (as opposed to momentary) physical contact with an infested person. This includes sexual intercourse, although a majority of cases are acquired through other forms of skin-to-skin contact. Less commonly, scabies infestation can happen through the sharing of clothes, towels, and bedding, but this is not a major mode of transmission; individual mites can only survive for two to three days, at most, away from human skin. As with lice, a latex condom is ineffective against scabies transmission during intercourse, because mites typically migrate from one individual to the next at sites other than the sex organs.
Pathophysiology.
The symptoms are caused by an allergic reaction of the host's body to mite proteins, though exactly which proteins remains a topic of study. The mite proteins are also present from the gut, in mite feces, which are deposited under the skin. The allergic reaction is both of the delayed (cell-mediated) and immediate (antibody-mediated) type, and involves IgE (antibodies, it is presumed, mediate the very rapid symptoms on reinfection). The allergy-type symptoms (itching) continue for some days, and even several weeks, after all mites are killed. New lesions may appear for a few days after mites are eradicated. Nodular lesions from scabies may continue to be symptomatic for weeks after the mites have been killed.
Diagnosis.
Scabies may be diagnosed clinically in geographical areas where it is common when diffuse itching presents along with either lesions in two typical spots or there is itchiness of another household member. The classical sign of scabies is the burrows made by the mites within the skin. To detect the burrow, the suspected area is rubbed with ink from a fountain pen or a topical tetracycline solution, which glows under a special light. The skin is then wiped with an alcohol pad. If the person is infected with scabies, the characteristic zigzag or "S" pattern of the burrow will appear across the skin; however, interpreting this test may be difficult, as the burrows are scarce and may be obscured by scratch marks. A definitive diagnosis is made by finding either the scabies mites or their eggs and fecal pellets. Searches for these signs involve either scraping a suspected area, mounting the sample in potassium hydroxide and examining it under a microscope, or using dermoscopy to examine the skin directly.
Differential diagnosis.
Symptoms of early scabies infestation mirror other skin diseases, including dermatitis, syphilis, various urticaria-related syndromes, allergic reactions, and other ectoparasites such as lice and fleas.
Prevention.
Mass treatment programs that use topical permethrin or oral ivermectin have been effective in reducing the prevalence of scabies in a number of populations. No vaccine is available for scabies. The simultaneous treatment of all close contacts is recommended, even if they show no symptoms of infection (asymptomatic), to reduce rates of recurrence. Since mites can survive for only two to three days without a host, other objects in the environment pose little risk of transmission except in the case of crusted scabies, thus cleaning is of little importance. Rooms used by those with crusted scabies require thorough cleaning.
Management.
A number of medications are effective in treating scabies. Treatment should involve the entire household, and any others who have had recent, prolonged contact with the infested individual. Options to control itchiness include antihistamines and prescription anti-inflammatory agents. Bedding, clothing and towels used during the previous three days should be washed in hot water and dried in a hot dryer.
Permethrin.
Permethrin is the most effective treatment for scabies, and remains the treatment of choice. It is applied from the neck down, usually before bedtime, and left on for about eight to 14 hours, then washed off in the morning. Care should be taken to coat the entire skin surface, not just symptomatic areas; any patch of skin left untreated can provide a "safe haven" for one or more mites to survive. One application is normally sufficient, as permethrin kills eggs and hatchlings as well as adult mites, though many physicians recommend a second application three to seven days later as a precaution. Crusted scabies may require multiple applications, or supplemental treatment with oral ivermectin (below). Permethrin may cause slight irritation of the skin that is usually tolerable.
Ivermectin.
Oral Ivermectin is effective in eradicating scabies, often in a single dose. It is the treatment of choice for crusted scabies, and is sometimes prescribed in combination with a topical agent. It has not been tested on infants, and is not recommended for children under six years of age.
Topical ivermectin preparations have been shown to be effective for scabies in adults, though only one such formulation is available in the United States at present, and it is not FDA approved as a scabies treatment. It has also been useful for sarcoptic mange (the veterinary analog of human scabies).
Others.
Other treatments include lindane, benzyl benzoate, crotamiton, malathion, and sulfur preparations. Lindane is effective, but concerns over potential neurotoxicity has limited its availability in many countries. It is banned in California, but may be used in other states as a second-line treatment. Sulfur ointments or benzyl benzoate are often used in the developing world due to their low cost; 10% sulfur solutions have been shown to be effective, and sulfur ointments are typically used for at least a week, though many people find the odor of sulfur products unpleasant. Crotamiton has been found to be less effective than permethrin in limited studies. Crotamiton or sulfur preparations are sometimes recommended instead of permethrin for children, due to concerns over dermal absorption of permethrin.
Communities.
Anne Frank was infected with scabies at Auschwitz concentration camp, and scabies is endemic in many developing countries, where it tends to be particularly problematic in rural and remote areas. In such settings community wide control strategies are required to reduce the rate of disease, as treatment of only individuals is ineffective due to the high rate of reinfection. Large-scale mass drug administration strategies may be required where coordinated interventions aim to treat whole communities in one concerted effort. Although such strategies have shown to be able to reduce the burden of scabies in these kinds of communities, debate remains about the best strategy to adopt, including the choice of drug.
The resources required to implement such large-scale interventions in a cost-effective and sustainable way are significant. Furthermore, since endemic scabies is largely restricted to poor and remote areas, it is a public health issue that has not attracted much attention from policy makers and international donors.
Epidemiology.
Scabies is one of the three most common skin disorders in children, along with tinea and pyoderma. As of 2010 it affects approximately 100 million people (1.5% of the population) and is equally common in both genders. The mites are distributed around the world and equally infect all ages, races, and socioeconomic classes in different climates. Scabies is more often seen in crowded areas with unhygienic living conditions. Globally as of 2009, an estimated 300 million cases of scabies occur each year, although various parties claim the figure is either over- or underestimated. About 1–10% of the global population is estimated to be infected with scabies, but in certain populations, the infection rate may be as high as 50–80%.
History.
Scabies has been observed in humans since ancient times. Archeological evidence from Egypt and the Middle East suggests scabies was present as early as 494 BC. The first recorded reference to scabies is believed to be from the Bible – it may be a type of "leprosy" mentioned in Leviticus "circa" 1200 BC or be mentioned among the curses of Deuteronomy 28. In the fourth century BC, Aristotle reported on "lice" that "escape from little pimples if they are pricked" — a description consistent with scabies.
The Greek encyclopedist and medical writer Aulus Cornelius Celsus (c. 25 BC – c. 50 AD) is credited with naming the disease "scabies" and describing its characteristic features. The parasitic etiology of scabies was documented by the Italian physician Giovanni Cosimo Bonomo (1663–1696) in his 1687 letter, "Observations concerning the fleshworms of the human body". Bonomo's description established scabies as one of the first human diseases with a well-understood cause.
Society and culture.
The International Alliance for the Control of Scabies (IACS) was started in 2012, and brings together over 70 researchers, clinicians and public health experts from more than 15 different countries. It has managed to bring the global health implications of scabies to the attention of the World Health Organization. Consequently, the WHO has included scabies on its official list of neglected tropical diseases and other neglected conditions.
Other animals.
Scabies may occur in a number of domestic and wild animals; the mites that cause these infestations are of different subspecies from the one typically causing the human form. These subspecies can infest animals that are not their usual hosts, but such infections do not last long. Scabies-infected animals suffer severe itching and secondary skin infections. They often lose weight and become frail.
The most frequently diagnosed form of scabies in domestic animals is sarcoptic mange, caused by the subspecies "Sarcoptes scabiei canid", most commonly in dogs and cats. Sarcoptic mange is transmissible to humans who come into prolonged contact with infested animals, and is distinguished from human scabies by its distribution on skin surfaces covered by clothing. Scabies-infected domestic fowl suffer what is known as "scaly leg". Domestic animals that have gone feral and have no veterinary care are frequently afflicted with scabies and a host of other ailments. Nondomestic animals have also been observed to suffer from scabies. Gorillas, for instance, are known to be susceptible to infection via contact with items used by humans.

</doc>
<doc id="28849" url="http://en.wikipedia.org/wiki?curid=28849" title="Shiva">
Shiva

Shiva (; Sanskrit: Śiva, meaning "The Auspicious One"), also known as "Mahadeva" ("Great God"), is one of the main deities of Hinduism. He is the supreme god within Shaivism, one of the three most influential denominations in contemporary Hinduism. He is one of the five primary forms of God in the Smarta tradition, and "the Destroyer" or "the Transformer" among the Trimurti, the Hindu Trinity of the primary aspects of the divine.
At the highest level, Shiva is regarded limitless, transcendent, unchanging and formless. Shiva also has many benevolent and fearsome forms. In benevolent aspects, he is depicted as an omniscient Yogi who lives an ascetic life on Mount Kailash, as well as a householder with wife Parvati and his two children, Ganesha and Kartikeya and in fierce aspects, he is often depicted slaying demons. Shiva is also regarded as the patron god of yoga and arts.
The main iconographical attributes of Shiva are the third eye on his forehead, the snake Vasuki around his neck, the crescent moon adorning, the holy river Ganga flowing from his matted hair, the trishula as his weapon and the damaru as his musical instrument. Shiva is usually worshiped in the aniconic form of Lingam.
Etymology and other names.
The Sanskrit word "Shiva" (Devanagari: शिव, "śiva") comes from Shri Rudram Chamakam of Taittiriya Samhita (TS 4.5, 4.7) of Krishna Yajurveda. The root word √śi means "auspicious". In simple English transliteration it is written either as "Shiva" or "Siva". The adjective "śiva", is used as an attributive epithet for several Vedic deities, including Rudra. Other popular names associated with Shiva are Mahadev, Mahesh, Maheshwar, Shankar, Shambhu, Rudra, Har, Trilochan, Devendra (meaning Chief of the gods) and Trilokinath (meaning "Lord of the three realms").
The Sanskrit word "śaiva" means "relating to the god Shiva", and this term is the Sanskrit name both for one of the principal sects of Hinduism and for a member of that sect. It is used as an adjective to characterize certain beliefs and practices, such as Shaivism.
Some authors associate the name with the Tamil word "śivappu" meaning "red", noting that Shiva is linked to the Sun ("śivan", "the Red one", in Tamil) and that Rudra is also called "Babhru" (brown, or red) in the Rigveda.
Adi Sankara, in his interpretation of the name "Shiva", the 27th and 600th name of Vishnu sahasranama, the thousand names of Vishnu interprets "Shiva" to have multiple meanings: "The Pure One", or "the One who is not affected by three Gunas of Prakrti (Sattva, Rajas, and Tamas)" or "the One who purifies everyone by the very utterance of His name." Swami Chinmayananda, in his translation of Vishnu sahasranama, further elaborates on that verse: "Shiva" means "the One who is eternally pure" or "the One who can never have any contamination of the imperfection of Rajas and Tamas".
Shiva's role as the primary deity of Shaivism is reflected in his epithets "Mahādeva" ("Great god"; "mahā" "Great" and "deva" "god"), "Maheśvara" ("Great Lord"; "mahā" "great" and "īśvara" "lord"), and "Parameśvara" ("Supreme Lord").
There are at least eight different versions of the "Shiva Sahasranama", devotional hymns (stotras) listing many names of Shiva. The version appearing in Book 13 (Anuśāsanaparvan) of the Mahabharata is considered the kernel of this tradition. Shiva also has Dasha-Sahasranamas (10,000 names) that are found in the Mahanyasa. The Shri Rudram Chamakam, also known as the "Śatarudriya", is a devotional hymn to Shiva hailing him by many names.
Historical development and literature.
The worship of Shiva is a pan-Hindu tradition, practiced widely across all of India, Nepal and Sri Lanka.
Assimilation of traditions.
The figure of Shiva as we know him today was built up over time, with the ideas of many regional sects being amalgamated into a single figure. How the persona of Shiva converged as a composite deity is not well documented. According to Vijay Nath: 
Visnu and Siva [...] began to absorb countless local cults and deities within their folds. The latter were either taken to represent the multiple facets of the same god or else were supposed to denote different forms and appellations by which the god came to be known and worshipped. [...] Siva became identified with countless local cults by the sheer suffixing of "Isa" or "Isvara" to the name of the local deity, e.g., Bhutesvara, Hatakesvara, Chandesvara."
Axel Michaels the Indologist suggests that Shaivism, like Vaishnavism, implies a unity which cannot be clearly found either in religious practice or in philosophical and esoteric doctrine. Furthermore, practice and doctrine must be kept separate.
An example of assimilation took place in Maharashtra, where a regional deity named Khandoba is a patron deity of farming and herding castes. The foremost center of worship of Khandoba in Maharashtra is in Jejuri. Khandoba has been assimilated as a form of Shiva himself, in which case he is worshipped in the form of a lingam. Khandoba's varied associations also include an identification with Surya and Karttikeya.
Indus Valley origins.
Many Indus valley seals show animals but one seal that has attracted attention shows a figure, either horned or wearing a horned headdress and possibly ithyphallic figure seated in a posture reminiscent of the Lotus position and surrounded by animals was named by early excavators of Mohenjo-daro "Pashupati" (lord of cattle), an epithet of the later Hindu gods Shiva and Rudra. Sir John Marshall and others have claimed that this figure is a prototype of Shiva and have described the figure as having three faces seated in a "yoga posture" with the knees out and feet joined.
While some academics like Gavin Flood and John Keay have expressed doubts. John Keay writes that "He may indeed be an early manifestation of Lord Shiva as Pashu- pati", but a couple of his specialties of this figure does not match with Rudra. Writing in 1997 Doris Srinivasan rejected Marshall's package of proto-Siva features, including that of three heads. She interprets what John Marshall interpreted as facial as not human but more bovine, possibly a divine buffalo-man. According to Iravatham Mahadevan symbols 47 and 48 of his Indus script glossary "The Indus Script: Texts, Concordance and Tables" (1977), representing seated human-like figures, could describe Hindu deity Murugan, popularly known as Shiva and Parvati's son.
Writing in 2002, Gregory L. Possehl concluded that while it would be appropriate to recognize the figure as a deity, its association with the water buffalo, and its posture as one of ritual discipline.
Indo-European origins.
Shiva's rise to a major position in the pantheon was facilitated by his identification with a host of Vedic deities, including Purusha, Rudra, Agni, Indra, Prajāpati, Vāyu, and others.
Rudra.
Shiva as we know him today shares many features with the Vedic god Rudra, and both Shiva and Rudra are viewed as the same personality in Hindu scriptures. The two names are used synonymously. Rudra, the god of the roaring storm, is usually portrayed in accordance with the element he represents as a fierce, destructive deity.
Hindu text Rig Veda, which is dated to between 1700 and 1100 BC based on linguistic and philological evidence. A god named Rudra is mentioned in the Rig Veda. The name Rudra is still used as a name for Shiva. In RV 2.33, he is described as the "Father of the Rudras", a group of storm gods. Furthermore, the Rudram, one of the most sacred hymns of Hinduism found both in the Rig and the Yajur Vedas and addressed to Rudra, invokes him as Shiva in several instances, but the term "Shiva" is used as an epithet for the gods Indra, Mitra and Agni many times. Since "Shiva" means "pure", the epithet is possibly used to describe a quality of these gods rather than to identify any of them with the God Shiva.
The identification of Shiva with the older god Rudhra is not universally accepted, as Axel Michaels explains:
Rudra is called "The Archer" (Sanskrit: "Śarva"), and the arrow is an essential attribute of Rudra. This name appears in the Shiva Sahasranama, and R. K. Sharma notes that it is used as a name of Shiva often in later languages.
The word is derived from the Sanskrit root "śarv-", which means "to injure" or "to kill", and Sharma uses that general sense in his interpretive translation of the name Śarva as "One who can kill the forces of darkness". The names Dhanvin ("Bowman") and Bāṇahasta ("Archer", literally "Armed with arrows in his hands") also refer to archery.
Agni.
Rudra and Agni have a close relationship. The identification between Agni and Rudra in the Vedic literature was an important factor in the process of Rudra's gradual development into the later character as Rudra-Shiva. The identification of Agni with Rudra is explicitly noted in the "Nirukta", an important early text on etymology, which says, "Agni is also called Rudra." The interconnections between the two deities are complex, and according to Stella Kramrisch:
The fire myth of Rudra-Śiva plays on the whole gamut of fire, valuing all its potentialities and phases, from conflagration to illumination.
In the "Śatarudrīya", some epithets of Rudra, such as Sasipañjara ("Of golden red hue as of flame") and Tivaṣīmati ("Flaming bright"), suggest a fusing of the two deities. Agni is said to be a bull, and Lord Shiva possesses a bull as his vehicle, Nandi. The horns of Agni, who is sometimes characterized as a bull, are mentioned. In medieval sculpture, both Agni and the form of Shiva known as Bhairava have flaming hair as a special feature.
Indra.
According to Wendy Doniger, the Puranic Shiva is a continuation of the Vedic Indra. Doniger gives several reasons for her hypothesis. Both are associated with mountains, rivers, male fertility, fierceness, fearlessness, warfare, transgression of established mores, the Aum sound, the Supreme Self. In the Rig Veda the term "śiva" is used to refer to Indra. (2.20.3, 6.45.17, and 8.93.3.) Indra, like Shiva, is likened to a bull. In the Rig Veda, Rudra is the father of the Maruts, but he is never associated with their warlike exploits as is Indra.
The Vedic beliefs and practices of the pre-classical era were closely related to the hypothesised Proto-Indo-European religion, and the Indo-Iranian religion. According to Anthony, the Old Indic religion probably emerged among Indo-European immigrants in the contact zone between the Zeravshan River (present-day Uzbekistan) and (present-day) Iran. It was "a syncretic mixture of old Central Asian and new Indo-European elements", which borrowed "distinctive religious beliefs and practices" from the Bactria–Margiana Culture. At least 383 non-Indo-European words were borrowed from this culture, including the god Indra and the ritual drink Soma. According to Anthony,
Many of the qualities of Indo-Iranian god of might/victory, Verethraghna, were transferred to the adopted god Indra, who became the central deity of the developing Old Indic culture. Indra was the subject of 250 hymns, a quarter of the "Rig Veda". He was associated more than any other deity with "Soma", a stimulant drug (perhaps derived from "Ephedra") probably borrowed from the BMAC religion. His rise to prominence was a peculiar trait of the Old Indic speakers.
Later Vedic literature.
Rudra's transformation from an ambiguously characterized deity to a supreme being began in the Shvetashvatara Upanishad (400-200 BC), which founded the tradition of Rudra-Shiva worship. Here they are identified as the creators of the cosmos and liberators of souls from the birth-rebirth cycle. The period of 200 BC to 100 AD also marks the beginning of the Shaiva tradition focused on the worship of Shiva, with references to Shaiva ascetics in Patanjali's Mahabhasya and in the Mahabharata.
Early historical paintings at the Bhimbetka rock shelters, depict Shiva dancing, Shiva's trident, and his mount Nandi but no other Vedic gods.
Sangam literature.
Sangam literature of Tamil(300BC-300CE) describes more times about lord Siva by various authors.for example in Silapathikaram,lord siva mentioned as Piraava yakkai periyon(பிறவா யாக்கைப் பெரியோன்),literally means Birth less Lord Siva.
Puranic literature.
The Shiva Puranas, particularly the Shiva Purana and the Linga Purana, discuss the various forms of Shiva and the cosmology associated with him.
Tantric literature.
The Tantras, composed between the 8th and 11th centuries, regard themselves as Sruti. Among these the Shaiva Agamas, are said to have been revealed by Shiva himself and are foundational texts for Shaiva Siddhanta.
Position within Hinduism.
Shaivism.
Shaivism (Sanskrit: शैव पंथ, "śaiva paṁtha") (Kannada: ) (Tamil: ) is the oldest of the four major sects of Hinduism, the others being Vaishnavism, Shaktism and Smartism. Followers of Shaivism, called "Shaivas", and also "Saivas" or "Saivites", revere Shiva as the Supreme Being. Shaivas believe that Shiva is All and in all, the creator, preserver, destroyer, revealer and concealer of all that is. The tantric Shaiva tradition consists of the Kapalikas, Kashmir Shaivism and Shaiva Siddhanta. The Shiva MahaPurana is one of the purāṇas, a genre of Hindu religious texts, dedicated to Shiva. Shaivism is widespread throughout India, Nepal, and Sri Lanka, mostly. Areas notable for the practice of Shaivism include parts of Southeast Asia, especially Malaysia, Singapore, and Indonesia.
Panchayatana puja.
Panchayatana puja is the system of worship ('puja') in the Smarta sampradaya of Hinduism. It is said to have been introduced by Adi Shankara, the 8th century AD Hindu philosopher. It consists of the worship of five deities: Shiva, Vishnu, Devi, Surya and Ganesha. Depending on the tradition followed by Smarta households, one of these deities is kept in the center and the other four surround it. Worship is offered to all the deities. The five are represented by small murtis, or by five kinds of stones, or by five marks drawn on the floor.
Trimurti.
The Trimurti is a concept in Hinduism in which the cosmic functions of creation, maintenance, and destruction are personified by the forms of Brahmā the creator, Vishnu the maintainer or preserver and Śhiva the destroyer or transformer. These three deities have been called "the Hindu triad" or the "Great Trinity", often addressed as "Brahma-Vishnu-Maheshwara."
Iconography and properties.
Lingam.
Apart from anthropomorphic images of Shiva, the worship of Shiva in the form of a "lingam", or "linga", is also important. These are depicted in various forms. One common form is the shape of a vertical rounded column. "Shiva" means auspiciousness, and "linga" means a sign or a symbol. Hence, the "Shivalinga" is regarded as a "symbol of the great God of the universe who is all-auspiciousness". "Shiva" also means "one in whom the whole creation sleeps after dissolution". "Linga" also means the same thing—a place where created objects get dissolved during the disintegration of the created universe. Since, according to Hinduism, it is the same god that creates, sustains and withdraws the universe, the Shivalinga represents symbolically God Himself. Some scholars, such as Monier Monier-Williams and Wendy Doniger, also view "linga" as a phallic symbol, although this interpretation is disputed by others, including Christopher Isherwood, Vivekananda, Swami Sivananda, and S.N. Balagangadhara.
Jyotirlinga.
The worship of the "Shiva-Linga" originated from the famous hymn in the "Atharva-Veda Samhitâ" sung in praise of the "Yupa-Stambha", the sacrificial post. In that hymn, a description is found of the beginningless and endless "Stambha" or "Skambha", and it is shown that the said "Skambha" is put in place of the eternal Brahman. Just as the Yajna (sacrificial) fire, its smoke, ashes, and flames, the "Soma" plant, and the ox that used to carry on its back the wood for the Vedic sacrifice gave place to the conceptions of the brightness of Shiva's body, his tawny matted hair, his blue throat, and the riding on the bull of the Shiva, the "Yupa-Skambha" gave place in time to the "Shiva-Linga". In the text "Linga Purana", the same hymn is expanded in the shape of stories, meant to establish the glory of the great Stambha and the superiority of Shiva as Mahadeva.
The sacred of all Shiva linga is worshipped as Jyotir linga. Jyoti means Radiance, apart from relating Shiva linga as a phallus symbol, there are also arguments that Shiva linga means 'mark' or a 'sign'. Jyotirlinga means "The Radiant sign of The Almighty". The Jyotirlingas are mentioned in Shiva Purana.
Shakti.
Shiva forms a Tantric couple with Shakti [Tamil : சக்தி ], the embodiment of energy, dynamism, and the motivating force behind all action and existence in the material universe. Shiva is her transcendent masculine aspect, providing the divine ground of all being. Shakti manifests in several female deities. Sati and Parvati are the main consorts of Shiva. She is also referred to as Uma, Durga (Parvata), Kali and Chandika. Kali is the manifestation of Shakti in her dreadful aspect. The name Kali comes from "kāla", which means black, time, death, lord of death, Shiva. Since Shiva is called Kāla, the eternal time, Kālī, his consort, also means "Time" or "Death" (as in "time has come"). Various Shakta Hindu cosmologies, as well as Shākta Tantric beliefs, worship her as the ultimate reality or "Brahman". She is also revered as Bhavatārini (literally "redeemer of the universe"). Kālī is represented as the consort of Lord Shiva, on whose body she is often seen standing or dancing. Shiva is the masculine force, the power of peace, while Shakti translates to power, and is considered as the feminine force. In the Vaishnava tradition, these realities are portrayed as Vishnu and Laxmi, or Radha and Krishna. These are differences in formulation rather than a fundamental difference in the principles. Both Shiva and Shakti have various forms. Shiva has forms like Yogi Raj (the common image of Himself meditating in the Himalayas), Rudra (a wrathful form) and Natarajar (Shiva's dance are the Lasya - the gentle form of dance, associated with the creation of the world, and the Tandava - the violent and dangerous dance, associated with the destruction of weary worldviews – weary perspectives and lifestyles).
The five mantras.
Five is a sacred number for Shiva. One of his most important mantras has five syllables (namaḥ śivāya).
Shiva's body is said to consist of five mantras, called the pañcabrahmans. As forms of God, each of these have their own names and distinct iconography:
These are represented as the five faces of Shiva and are associated in various texts with the five elements, the five senses, the five organs of perception, and the five organs of action. Doctrinal differences and, possibly, errors in transmission, have resulted in some differences between texts in details of how these five forms are linked with various attributes. The overall meaning of these associations is summarized by Stella Kramrisch:
Through these transcendent categories, Śiva, the ultimate reality, becomes the efficient and material cause of all that exists.
According to the "Pañcabrahma Upanishad":
One should know all things of the phenomenal world as of a fivefold character, for the reason that the eternal verity of Śiva is of the character of the fivefold Brahman. ("Pañcabrahma Upanishad" 31)
Forms and roles.
According to Gavin Flood, "Shiva is a god of ambiguity and paradox," whose attributes include opposing themes. The ambivalent nature of this deity is apparent in some of his names and the stories told about him.
Destroyer and Benefactor.
In the Yajurveda, two contrary sets of attributes for both malignant or terrific (Sanskrit: "rudra") and benign or auspicious (Sanskrit: "śiva") forms can be found, leading Chakravarti to conclude that "all the basic elements which created the complex Rudra-Śiva sect of later ages are to be found here". In the Mahabharata, Shiva is depicted as "the standard of invincibility, might, and terror", as well as a figure of honor, delight, and brilliance. The duality of Shiva's fearful and auspicious attributes appears in contrasted names.
The name "Rudra" (Sanskrit: "रुद्र") reflects his fearsome aspects. According to traditional etymologies, the Sanskrit name "Rudra" is derived from the root "rud-", which means "to cry, howl". Stella Kramrisch notes a different etymology connected with the adjectival form "raudra", which means "wild, of "rudra" nature", and translates the name "Rudra" as "the wild one" or "the fierce god". R. K. Sharma follows this alternate etymology and translates the name as "terrible". Hara (Sanskrit: "हर") is an important name that occurs three times in the Anushasanaparvan version of the Shiva sahasranama, where it is translated in different ways each time it occurs, following a commentorial tradition of not repeating an interpretation. Sharma translates the three as "one who captivates", "one who consolidates", and "one who destroys". Kramrisch translates it as "the ravisher". Another of Shiva's fearsome forms is as Kāla (Sanskrit: काल), "time", and as Mahākāla (Sanskrit: महाकाल), "great time", which ultimately destroys all things. Bhairava (Sanskrit: भैरव), "terrible" or "frightful", is a fierce form associated with annihilation.
In contrast, the name Śaṇkara (Sanskrit: "शङ्कर"), "beneficent" or "conferring happiness" reflects his benign form. This name was adopted by the great Vedanta philosopher Śaṇkara (c. 788 - 820 AD), who is also known as Shankaracharya. The name Śambhu (Sanskrit: शम्भु), "causing happiness", also reflects this benign aspect.
Ascetic and Householder.
He is depicted as both an ascetic yogi and as a householder, roles which have been traditionally mutually exclusive in Hindu society. When depicted as a yogi, he may be shown sitting and meditating. His epithet Mahāyogi ("the great Yogi: "Mahā" = "great", "Yogi" = "one who practices Yoga") refers to his association with yoga. While Vedic religion was conceived mainly in terms of sacrifice, it was during the Epic period that the concepts of tapas, yoga, and asceticism became more important, and the depiction of Shiva as an ascetic sitting in philosophical isolation reflects these later concepts. Shiva is also depicted as a corpse below Goddess Kali, it represents that Shiva is a corpse without Shakti. He remains inert. While Shiva is the static form, Mahakali or Shakti is the dynamic aspect without whom Shiva is powerless.
As a family man and householder, he has a wife, Parvati and two sons, Ganesha and Kartikeya. His epithet Umāpati ("The husband of Umā") refers to this idea, and Sharma notes that two other variants of this name that mean the same thing, Umākānta and Umādhava, also appear in the sahasranama. Umā in epic literature is known by many names, including the benign Pārvatī. She is identified with Devi, the Divine Mother; Shakti (divine energy) as well as goddesses like Tripura Sundari, Durga, Kamakshi and Meenakshi. The consorts of Shiva are the source of his creative energy. They represent the dynamic extension of Shiva onto this universe. His son Ganesha is worshipped throughout India and Nepal as the Remover of Obstacles, Lord of Beginnings and Lord of Obstacles. Kartikeya is worshipped in Southern India (especially in Tamil Nadu, Kerala and Karnataka) by the names Subrahmanya, Subrahmanyan, Shanmughan, Swaminathan and Murugan, and in Northern India by the names Skanda, Kumara, or Karttikeya.
Some regional deities are also identified as Shiva's children. As one story goes, Shiva is enticed by the beauty and charm of Mohini, Vishnu's female avatar, and procreates with her. As a result of this union, Shasta - identified with regional deities Ayyappa and Ayyanar - is born. Shiva is also mentioned in some scriptures to have had daughters like the serpent-goddess Manasa and Ashokasundari. The demons Andhaka and Jalandhara and the god Mangala are considered children of Shiva.
Nataraaja.
The depiction of Shiva as Nataraja (Sanskrit: "naṭarāja", "Lord of Dance") is popular. The names Nartaka ("dancer") and Nityanarta ("eternal dancer") appear in the Shiva Sahasranama. His association with dance and also with music is prominent in the Puranic period. In addition to the specific iconographic form known as Nataraja, various other types of dancing forms (Sanskrit: "nṛtyamūrti") are found in all parts of India, with many well-defined varieties in Tamil Nadu in particular. The two most common forms of the dance are the Tandava, which later came to denote the powerful and masculine dance as Kala-Mahakala associated with the destruction of the world. When it requires the world or universe to be destroyed, Lord Śiva does it by the tāṇḍavanṛtya. and Lasya, which is graceful and delicate and expresses emotions on a gentle level and is considered the feminine dance attributed to the goddess Parvati. "Lasya" is regarded as the female counterpart of "Tandava". The "Tandava"-"Lasya" dances are associated with the destruction-creation of the world.
Dakshinamurthy.
Dakshinamurthy, or Dakṣiṇāmūrti (Tamil:தட்சிணாமூர்த்தி, Telugu: దక్షిణామూర్తి, Sanskrit: दक्षिणामूर्ति), literally describes a form ("mūrti") of Shiva facing south ("dakṣiṇa"). This form represents Shiva in his aspect as a teacher of yoga, music, and wisdom and giving exposition on the shastras. This iconographic form for depicting Shiva in Indian art is mostly from Tamil Nadu. Elements of this motif can include Shiva seated upon a deer-throne and surrounded by sages who are receiving his instruction.
Ardhanarishvara.
An iconographic representation of Shiva called ("Ardhanārīśvara") shows him with one half of the body as male and the other half as female. According to Ellen Goldberg, the traditional Sanskrit name for this form ("Ardhanārīśvara") is best translated as "the lord who is half woman", not as "half-man, half-woman". According to legend, Lord Shiva is pleased by the difficult austerites performed by the goddess Parvati, grants her the left half of his body. This form of Shiva is quite similar to the Yin-Yang philosophy of Eastern Asia, though "Ardhanārīśvara" appears to be more ancient.
Tripurantaka.
Shiva is often depicted as an archer in the act of destroying the triple fortresses, "Tripura", of the Asuras. Shiva's name Tripurantaka (Sanskrit: त्रिपुरान्तक, "Tripurāntaka"), "ender of Tripura", refers to this important story. In this aspect, Shiva is depicted with four arms wielding a bow and arrow, but different from the Pinakapani murti. He holds an axe and a deer on the upper pair of his arms. In the lower pair of the arms, he holds a bow and an arrow respectively. After destroying Tripura, Tripurantaka Shiva smeared his forehead with three strokes of Ashes. This has become a prominent symbol of Shiva and is practiced even today by Shaivites.
Other forms, avatars, identifications.
Shiva, like some other Hindu deities, is said to have several incarnations, known as Avatars. Although Puranic scriptures contain occasional references to "ansh" avatars of Shiva, the idea is not universally accepted in Saivism. The Linga Purana speaks of twenty-eight forms of Shiva which are sometimes seen as avatars. According to the "Svetasvatara Upanishad", he has four avatars.
In the "Hanuman Chalisa", Hanuman is identified as the eleventh avatar of Shiva and this belief is universal. Hanuman is popularly known as “Rudraavtaar” “Rudra” being a name of “Shiva”. Rama– the Vishnu avatar is considered by some to be the eleventh avatar of Rudra (Shiva).
Other traditions regard the sage Durvasa, the sage Agastya, the philosopher Adi Shankara and Ashwatthama as avatars of Shiva. Other forms of Shiva include Virabhadra and Sharabha.
Festivals.
Maha Shivratri is a festival celebrated every year on the 13th night or the 14th day of the new moon in the Shukla Paksha of the month of Maagha or Phalguna in the Hindu calendar. This festival is of utmost importance to the devotees of Lord Shiva. Mahashivaratri marks the night when Lord Shiva performed the 'Tandava' and it is the day that Lord Shiva was married to Parvati. The holiday is often celebrated with special prayers and rituals offered up to Shiva, notably the Abhishek. This ritual, practiced throughout the night, is often performed every three hours with water, milk, yogurt, and honey. Bel (aegle marmelos) leaves are often offered up to the Hindu god, as it is considered necessary for a successful life. The offering of the leaves are considered so important that it is believed that someone who offers them without any intentions will be rewarded greatly.
Beyond Hinduism.
Buddhism.
Shiva is mentioned in Buddhist Tantra. Shiva as "Upaya" and Shakti as "Prajna". In cosmologies of buddhist tantra, Shiva is depicted as active, skillful, and more passive.
Sikhism.
The Japuji Sahib of the Guru Granth Sahib says, "The Guru is Shiva, the Guru is Vishnu and Brahma; the Guru is Paarvati and Lakhshmi." In the same chapter, it also says, "Shiva speaks, the Siddhas speak."
In Dasam Granth, Guru Gobind Singh have mentioned two avtars of Rudra: Dattatreya Avtar and Parasnath Avtar.
Others.
The worship of Lord Shiva became popular in Central Asia through the Hephthalite (White Hun) Dynasty, and Kushan Empire. Shaivism was also popular in Sogdiana and Eastern Turkestan as found from the wall painting from Penjikent on the river Zervashan. In this depiction, Shiva is portrayed with a sacred halo and a sacred thread ("Yajnopavita"). He is clad in tiger skin while his attendants are wearing Sodgian dress. In Eastern Turkestan in the Taklamakan Desert. There is a depiction of his four-legged seated cross-legged n a cushioned seat supported by two bulls. Another panel form Dandan-Uilip shows Shiva in His Trimurti form with His Shakti kneeling on her right thigh. It is also noted that Zoroastrian wind god Vayu-Vata took on the iconographic appearance of Shiva.
Kirant people, a Mongol tribe from Nepal, worship a form of Shiva as one of their major deity, identifying him as the lord of animals. It is also said that the physical form of Shiva as a yogi is derived from Kirants as it is mentioned in Mundhum that Shiva took human form as a child of Kirant. He is also said to give Kirants visions in form of a male deer.
In Indonesia, Shiva is also worshiped as Batara Guru. In the ancient times, all kingdoms were located on top of mountains. When he was young, before receiving his authority of power, his name was Sang Hyang Manikmaya. He is first of the children who hatched from the eggs laid by Manuk Patiaraja, wife of god Mulajadi na Bolon. This avatar is also worshiped in Malaysia. Shiva's other form in Indonesian Hinduism is "Maharaja Dewa" (Mahadeva).
Sources.
81-208-1381-2.
</dl>
</dl>
External links.
nepal is a very nice contry

</doc>
<doc id="28851" url="http://en.wikipedia.org/wiki?curid=28851" title="Sap beetle">
Sap beetle

The sap beetles are a family (Nitidulidae) of beetles.
They are small (2–6 mm) ovoid, usually dull-coloured beetles, with knobbed antennae. Some have red or yellow spots or bands. They feed mainly on decaying vegetable matter, over-ripe fruit, and sap. There are a few pest species. 
Common species include:
Classification.
The family includes these genera:

</doc>
<doc id="28852" url="http://en.wikipedia.org/wiki?curid=28852" title="Syphilis">
Syphilis

Syphilis is a sexually transmitted infection caused by the spirochete bacterium "Treponema pallidum" subspecies "pallidum". The primary route of transmission is through sexual contact; it may also be transmitted from mother to fetus during pregnancy or at birth, resulting in congenital syphilis. Other human diseases caused by related "Treponema pallidum" include yaws (subspecies "pertenue"), pinta (subspecies "carateum"), and bejel (subspecies "endemicum").
The signs and symptoms of syphilis vary depending in which of the four stages it presents (primary, secondary, latent, and tertiary). The primary stage classically presents with a single chancre (a firm, painless, non-itchy skin ulceration), secondary syphilis with a diffuse rash which frequently involves the palms of the hands and soles of the feet, latent syphilis with little to no symptoms, and tertiary syphilis with gummas, neurological, or cardiac symptoms. It has, however, been known as "the great imitator" due to its frequent atypical presentations. Diagnosis is usually made by using blood tests; however, the bacteria can also be detected using dark field microscopy. Syphilis can be effectively treated with antibiotics, specifically the preferred intramuscular benzathine penicillin G (or penicillin G potassium given intravenously for neurosyphilis), or else ceftriaxone, and in those who have a severe penicillin allergy, oral doxycycline or azithromycin.
Syphilis is thought to have infected 12 million additional people worldwide in 1999, with greater than 90% of cases in the developing world. After decreasing dramatically since the widespread availability of penicillin in the 1940s, rates of infection have increased since the turn of the millennium in many countries, often in combination with human immunodeficiency virus (HIV). This has been attributed partly to increased promiscuity, prostitution, decreasing use of condoms, and unsafe sexual practices among men who have sex with men.
Signs and symptoms.
Syphilis can present in one of four different stages: primary, secondary, latent, and tertiary, and may also occur congenitally. It was referred to as "the great imitator" by Sir William Osler due to its varied presentations.
Primary.
Primary syphilis is typically acquired by direct sexual contact with the infectious lesions of another person. Approximately 3 to 90 days after the initial exposure (average 21 days) a skin lesion, called a chancre, appears at the point of contact. This is classically (40% of the time) a single, firm, painless, non-itchy skin ulceration with a clean base and sharp borders between 0.3 and 3.0 cm in size. The lesion, however, may take on almost any form. In the classic form, it evolves from a macule to a papule and finally to an erosion or ulcer. Occasionally, multiple lesions may be present (~40%), with multiple lesions more common when coinfected with HIV. Lesions may be painful or tender (30%), and they may occur outside of the genitals (2–7%). The most common location in women is the cervix (44%), the penis in heterosexual men (99%), and anally and rectally relatively commonly in men who have sex with men (34%). Lymph node enlargement frequently (80%) occurs around the area of infection, occurring seven to 10 days after chancre formation. The lesion may persist for three to six weeks without treatment.
Secondary.
Secondary syphilis occurs approximately four to ten weeks after the primary infection. While secondary disease is known for the many different ways it can manifest, symptoms most commonly involve the skin, mucous membranes, and lymph nodes. There may be a symmetrical, reddish-pink, non-itchy rash on the trunk and extremities, including the palms and soles. The rash may become maculopapular or pustular. It may form flat, broad, whitish, wart-like lesions known as condyloma latum on mucous membranes. All of these lesions harbor bacteria and are infectious. Other symptoms may include fever, sore throat, malaise, weight loss, hair loss, and headache. Rare manifestations include liver inflammation, kidney disease, joint inflammation, periostitis, inflammation of the optic nerve, uveitis, and interstitial keratitis. The acute symptoms usually resolve after three to six weeks; however, about 25% of people may present with a recurrence of secondary symptoms. Many people who present with secondary syphilis (40–85% of women, 20–65% of men) do not report previously having had the classic chancre of primary syphilis.
Latent.
Latent syphilis is defined as having serologic proof of infection without symptoms of disease. It is further described as either early (less than 1 year after secondary syphilis) or late (more than 1 year after secondary syphilis) in the United States. The United Kingdom uses a cut-off of two years for early and late latent syphilis. Early latent syphilis may have a relapse of symptoms. Late latent syphilis is asymptomatic, and not as contagious as early latent syphilis.
Tertiary.
Tertiary syphilis may occur approximately 3 to 15 years after the initial infection, and may be divided into three different forms: gummatous syphilis (15%), late neurosyphilis (6.5%), and cardiovascular syphilis (10%). Without treatment, a third of infected people develop tertiary disease. People with tertiary syphilis are not infectious.
Gummatous syphilis or late benign syphilis usually occurs 1 to 46 years after the initial infection, with an average of 15 years. This stage is characterized by the formation of chronic gummas, which are soft, tumor-like balls of inflammation which may vary considerably in size. They typically affect the skin, bone, and liver, but can occur anywhere.
Neurosyphilis refers to an infection involving the central nervous system. It may occur early, being either asymptomatic or in the form of syphilitic meningitis, or late as meningovascular syphilis, general paresis, or tabes dorsalis, which is associated with poor balance and lightning pains in the lower extremities. Late neurosyphilis typically occurs 4 to 25 years after the initial infection. Meningovascular syphilis typically presents with apathy and seizure, and general paresis with dementia and tabes dorsalis. Also, there may be Argyll Robertson pupils, which are bilateral small pupils that constrict when the person focuses on near objects, but do not constrict when exposed to bright light.
Cardiovascular syphilis usually occurs 10–30 years after the initial infection. The most common complication is syphilitic aortitis, which may result in aneurysm formation.
Congenital.
Congenital syphilis is that which is transmitted during pregnancy or during birth. Two-thirds of syphilitic infants are born without symptoms. Common symptoms that develop over the first couple years of life include: enlargement of the liver and spleen (70%), rash (70%), fever (40%), neurosyphilis (20%), and lung inflammation (20%). If untreated, late congenital syphilis may occur in 40%, including: saddle nose deformation, Higoumenakis sign, saber shin, or Clutton's joints among others.
Cause.
Bacteriology.
"Treponema pallidum" subspecies" pallidum" is a spiral-shaped, Gram-negative, highly mobile bacterium. Three other human diseases are caused by related "Treponema pallidum", including yaws (subspecies "pertenue"), pinta (subspecies "carateum") and bejel (subspecies "endemicum"). Unlike subtype "pallidum", they do not cause neurological disease. Humans are the only known natural reservoir for subspecies "pallidum". It is unable to survive without a host for more than a few days. This is due to its small genome (1.14 MDa) failing to encode the metabolic pathways necessary to make most of its macronutrients. It has a slow doubling time of greater than 30 hours.
Transmission.
Syphilis is transmitted primarily by sexual contact or during pregnancy from a mother to her fetus; the spirochaete is able to pass through intact mucous membranes or compromised skin. It is thus transmissible by kissing near a lesion, as well as oral, vaginal, and anal sex. Approximately 30 to 60% of those exposed to primary or secondary syphilis will get the disease. Its infectivity is exemplified by the fact that an individual inoculated with only 57 organisms has a 50% chance of being infected. Most (60%) of new cases in the United States occur in men who have sex with men. It can be transmitted via blood products. However, it is tested for in many countries and thus the risk is low. The risk of transmission from sharing needles appears limited. 
It is not generally possible to contract syphilis through toilet seats, daily activities, hot tubs, or sharing eating utensils or clothing. This is mainly because the bacteria die very quickly outside of the body, making transmission via objects extremely difficult.
Diagnosis.
Syphilis is difficult to diagnose clinically early in its presentation. Confirmation is either via blood tests or direct visual inspection using microscopy. Blood tests are more commonly used, as they are easier to perform. Diagnostic tests are, however, unable to distinguish between the stages of the disease.
Blood tests.
Blood tests are divided into nontreponemal and treponemal tests. Nontreponemal tests are used initially, and include venereal disease research laboratory (VDRL) and rapid plasma reagin tests. However, as these tests are occasionally false positives, confirmation is required with a treponemal test, such as treponemal pallidum particle agglutination (TPHA) or fluorescent treponemal antibody absorption test (FTA-Abs). False positives on the nontreponemal tests can occur with some viral infections such as varicella and measles, as well as with lymphoma, tuberculosis, malaria, endocarditis, connective tissue disease, and pregnancy. Treponemal antibody tests usually become positive two to five weeks after the initial infection. Neurosyphilis is diagnosed by finding high numbers of leukocytes (predominately lymphocytes) and high protein levels in the cerebrospinal fluid in the setting of a known syphilis infection.
Direct testing.
Dark ground microscopy of serous fluid from a chancre may be used to make an immediate diagnosis. However, hospitals do not always have equipment or experienced staff members, whereas testing must be done within 10 minutes of acquiring the sample. Sensitivity has been reported to be nearly 80%, thus can only be used to confirm a diagnosis but not rule one out. Two other tests can be carried out on a sample from the chancre: direct fluorescent antibody testing and nucleic acid amplification tests. Direct fluorescent testing uses antibodies tagged with fluorescein, which attach to specific syphilis proteins, while nucleic acid amplification uses techniques, such as the polymerase chain reaction, to detect the presence of specific syphilis genes. These tests are not as time-sensitive, as they do not require living bacteria to make the diagnosis.
Prevention.
s of 2010[ [update]], there is no vaccine effective for prevention. Abstinence from intimate physical contact with an infected person is effective at reducing the transmission of syphilis, as is the proper use of a latex condom. Condom use, however, does not completely eliminate the risk. Thus, the Centers for Disease Control and Prevention recommends a long-term, mutually monogamous relationship with an uninfected partner and the avoidance of substances such as alcohol and other drugs that increase risky sexual behavior.
Congenital syphilis in the newborn can be prevented by screening mothers during early pregnancy and treating those who are infected. The United States Preventive Services Task Force (USPSTF) strongly recommends universal screening of all pregnant women, while the World Health Organization recommends all women be tested at their first antenatal visit and again in the third trimester. If they are positive, they recommend their partners also be treated. Congenital syphilis is, however, still common in the developing world, as many women do not receive antenatal care at all, and the antenatal care others do receive does not include screening, and it still occasionally occurs in the developed world, as those most likely to acquire syphilis (through drug use, etc.) are least likely to receive care during pregnancy. A number of measures to increase access to testing appear effective at reducing rates of congenital syphilis in low- to middle-income countries.
Syphilis is a notifiable disease in many countries, including Canada the European Union, and the United States. This means health care providers are required to notify public health authorities, which will then ideally provide partner notification to the person's partners. Physicians may also encourage patients to send their partners to seek care. The CDC recommends that sexually active men who have sex with men be tested at least yearly.
Treatment.
Early infections.
The first-choice treatment for uncomplicated syphilis remains a single dose of intramuscular benzathine penicillin G. Doxycycline and tetracycline are alternative choices for those allergic to penicillin; however, due to the risk of birth defects these are not recommended for pregnant women. Resistance to macrolides, rifampin, and clindamycin is often present. Ceftriaxone, a third-generation cephalosporin antibiotic, may be as effective as penicillin-based treatment. It is recommended that a treated person avoid sex until the sores are healed.
Late infections.
For neurosyphilis, due to the poor penetration of penicillin G into the central nervous system, those affected are recommended to be given large doses of intravenous penicillin for a minimum of 10 days. If a person is allergic, ceftriaxone may be used or penicillin desensitization attempted. Other late presentations may be treated with once-weekly intramuscular penicillin G for three weeks. If allergic, as in the case of early disease, doxycycline or tetracycline may be used, albeit for a longer duration. Treatment at this stage limits further progression, but has only slight effect on damage which has already occurred.
Jarisch-Herxheimer reaction.
One of the potential side effects of treatment is the Jarisch-Herxheimer reaction. It frequently starts within one hour and lasts for 24 hours, with symptoms of fever, muscles pains, headache, and a fast heart rate. It is caused by cytokines released by the immune system in response to lipoproteins released from rupturing syphilis bacteria.
Epidemiology.
Syphilis is believed to have infected 12 million additional people in 1999, with greater than 90% of cases in the developing world. It affects between 700,000 and 1.6 million pregnancies a year, resulting in spontaneous abortions, stillbirths, and congenital syphilis. During 2010 it caused about 113,000 deaths down from 202,000 in 1990. In sub-Saharan Africa, syphilis contributes to approximately 20% of perinatal deaths. Rates are proportionally higher among intravenous drug users, those who are infected with HIV, and men who have sex with men. In the United States, rates of syphilis as of 2007 were six times greater in men than women, while they were nearly equal in 1997. African Americans accounted for almost half of all cases in 2010. As of 2014, syphilis infections continue to increase in the United States.
Syphilis was very common in Europe during the 18th and 19th centuries. Flaubert found it universal among nineteenth-century Egyptian prostitutes. In the developed world during the early 20th century, infections declined rapidly with the widespread use of antibiotics, until the 1980s and 1990s. Since the year 2000, rates of syphilis have been increasing in the USA, Canada, the UK, Australia and Europe, primarily among men who have sex with men. Rates of syphilis among American women have, however, remained stable during this time, and rates among UK women have increased, but at a rate less than that of men. Increased rates among heterosexuals have occurred in China and Russia since the 1990s. This has been attributed to unsafe sexual practices, such as sexual promiscuity, prostitution, and decreasing use of barrier protection.
Untreated, it has a mortality of 8% to 58%, with a greater death rate in males. The symptoms of syphilis have become less severe over the 19th and 20th centuries, in part due to widespread availability of effective treatment and partly due to decreasing virulence of the spirochaete. With early treatment, few complications result. Syphilis increases the risk of HIV transmission by two to five times, and coinfection is common (30–60% in a number of urban centers).
History.
The exact origin of syphilis is disputed. Syphilis was indisputably present in the Americas before European contact. The dispute is over whether or not syphilis was also present elsewhere in the world at that time. One of the two primary hypotheses proposes that syphilis was carried from the Americas to Europe by the returning crewmen from Christopher Columbus's voyage to the Americas. The other hypothesis says that syphilis existed in Europe previously, but went unrecognized until shortly after Columbus' return. These are referred to as the "Columbian" and "pre-Columbian" hypotheses, respectively. The Columbian hypothesis is best supported by the available evidence. The first written records of an outbreak of syphilis in Europe occurred in 1494 or 1495 in Naples, Italy, during a French invasion (Italian War of 1494–98). As it was claimed to have been spread by French troops, it was initially known as the "French disease" by the people of Naples. In 1530, the pastoral name "syphilis" (the name of a character) was first used by the Italian physician and poet Girolamo Fracastoro as the title of his Latin poem in dactylic hexameter describing the ravages of the disease in Italy. It was also known historically as the "Great Pox".
The causative organism, "Treponema pallidum", was first identified by Fritz Schaudinn and Erich Hoffmann in 1905. The first effective treatment (Salvarsan) was developed in 1910 by Paul Ehrlich, which was followed by trials of penicillin and confirmation of its effectiveness in 1943. Before the discovery and use of antibiotics in the mid-twentieth century, mercury and isolation were commonly used, with treatments often worse than the disease.
Many famous historical figures (including Franz Schubert and Niccolò Paganini) are believed to have had the disease.
Society and culture.
Arts and literature.
The earliest known depiction of an individual with syphilis is Albrecht Dürer's "Syphilitic Man", a woodcut believed to represent a Landsknecht, a Northern European mercenary. The myth of the "femme fatale" or "poison women" of the 19th century is believed to be partly derived from the devastation of syphilis, with classic examples in literature including John Keats' "La Belle Dame sans Merci".
The artist Jan van der Straet painted a scene of a wealthy man receiving treatment for syphilis with the tropical wood guaiacum sometime around 1580. The title of the work is "Preparation and Use of Guayaco for Treating Syphilis". That the artist chose to include this image in a series of works celebrating the New World indicates how important a treatment, however ineffective, for syphilis was to the European elite at that time. The richly colored and detailed work depicts four servants preparing the concoction while a physician looks on, hiding something behind his back while the hapless patient drinks.
Tuskegee and Guatemala studies.
One of the most infamous United States cases of questionable medical ethics in the 20th century was the Tuskegee syphilis study. The study took place in Tuskegee, Alabama, and was supported by the U.S. Public Health Service (PHS) in partnership with the Tuskegee Institute. The study began in 1932, when syphilis was a widespread problem and there was no safe and effective treatment. The study was designed to measure the progression of untreated syphilis. By 1947, penicillin had been validated as an effective cure for syphilis and was becoming widely used to treat the disease. Study directors, however, continued the study and did not offer the participants treatment with penicillin. This is debated, and some have found that penicillin was given to many of the subjects. 
In the 1960s, Peter Buxtun sent a letter to the CDC, who controlled the study, expressing concern about the ethics of letting hundreds of black men die of a disease that could be cured. The CDC asserted that it needed to continue the study until all of the men had died. In 1972, Buxton went to the mainstream press, causing a public outcry. As a result, the program was terminated, a lawsuit brought those affected nine million dollars, and Congress created a commission empowered to write regulations to deter such abuses from occurring in the future. 
On May 16, 1997, as the thanks to the efforts of the Tuskegee Syphilis Study Legacy Committee formed in 1994, survivors of the study were invited to the White House to be present when President Bill Clinton apologized on behalf of the United States government for the study.
Syphilis experiments were also carried out in Guatemala from 1946 to 1948. They were United States-sponsored human experiments, conducted during the government of Juan José Arévalo with the cooperation of some Guatemalan health ministries and officials. Doctors infected soldiers, prisoners, and mental patients with syphilis and other sexually transmitted diseases, without the informed consent of the subjects, and then treated them with antibiotics. In October 2010, the U.S. formally apologized to Guatemala for conducting these experiments.
Research.
There is no vaccine available for people; however, several vaccines based on treponemal proteins reduce lesion development in an animal model, and research is ongoing.

</doc>
<doc id="28853" url="http://en.wikipedia.org/wiki?curid=28853" title="Smiling Buddha">
Smiling Buddha

Smiling Buddha (MEA designation: Pokhran-I) was the assigned code name of India's first nuclear weapon explosion on 18 May 1974. The device was detonated on the army base Rajasthan Pokhran Test Range by the Indian Army under the supervision of several key Indian army personnel. 
"Pokhran-I" was also the first confirmed nuclear test by a nation outside the five permanent members of the United Nations Security Council. Officially, the Indian Ministry of External Affairs (MEA) claimed this test was a "peaceful nuclear explosion", but it was actually an accelerated nuclear weapons program. The explosive yield is estimated to be around 8kt.
History.
Early origins, 1944–60s.
India started its own nuclear program in 1944 when Homi J. Bhabha founded the Tata Institute of Fundamental Research. Physicist Raja Ramanna played an essential role in nuclear weapons technology research. He expanded and supervised scientific research on nuclear weapons and was the first directing officer of the small team of scientists that supervised and carried out the test.<ref name = "http://nuclearweaponarchive.org"></ref>
After Indian independence from the United Kingdom, Indian Prime Minister Jawarharlal Nehru authorized the development of a nuclear program headed by Homi J. Bhabha. The "Atomic Energy Act" of 1948 focused on peaceful development. India was heavily involved in the development of the Nuclear Non-Proliferation Treaty, but ultimately opted not to sign it.
We must develop this atomic energy quite apart from war — indeed I think we must develop it for the purpose of using it for peaceful purposes. ... Of course, if we are compelled as a nation to use it for other purposes, possibly no pious sentiments of any of us will stop the nation from using it that way.—Jawaharalal Nehru, First Prime Minister of India, 
In 1954, Bhabha steered the nuclear program in the direction of weapons design and production. Two important infrastructure projects were commissioned. The first estabilshed Trombay Atomic Energy Establishment at Mumbai (Bombay). The other created a governmental secretariat, Department of Atomic Energy (DAE), of which Bhabha was the first secretary. From 1954 to 1959, the nuclear program grew swiftly. By 1958, the DAE had one-third of the defense budget for research purposes. In 1954, India reached a verbal understanding with the United States and Canada under the Atoms for Peace program; the United States and Canada ultimately agreed to provide and establish the CIRUS research reactor also at Trombay. The acquisition of CIRUS was a watershed event in nuclear proliferation with the understanding between India and the United States that the reactor would be used for peaceful purposes only. CIRUS was an ideal facility to develop a plutonium device, and therefore Nehru refused to accept nuclear fuel from Canada and started the program to develop an indigenous nuclear fuel cycle.
In July 1958, Nehru authorized "Project Phoenix" to build a reprocessing plant with a capacity of 20 tons of fuel a year – a size to match the production capacity of CIRUS. The plant used the PUREX process and was designed by the American firm Vitro International. Construction of the plutonium plant began at Trombay on 27 March 1961, and it was commissioned in mid-1964.
The nuclear program continued to mature, and by 1960, Nehru made the critical decision to move the program into production. About the same time, Nehru held discussions with the American firm Westinghouse Electric to construct India's first nuclear power plant in Tarapur, Maharashtra. Kenneth Nichols, a US Army engineer, recalls from a meeting with Nehru, "it was that time when Nehru turned to Bhabha and asked Bhabha for the timeline of the development of a nuclear weapon". Bhabha estimated he would need about a year to accomplish the task.
By 1962, the nuclear program was still developing, but at a slow rate. Nehru was distracted by the Sino-Indian War, during which India lost territory to China. Nehru turned to the Soviet Union for help, but the Soviet Union was preoccupied with the Cuban Missile Crisis. The Soviet Politburo turned down Nehru's request for arms and continued backing the Chinese. India concluded that the Soviet Union was an unreliable ally, and this conclusion strengthened India's determination to create a nuclear deterrent. Design work began in 1965 under Bhabha and proceeded under Raja Ramanna who took over the program after the former's death.
Weapons development, 1967–72.
Bhabha was now aggressively lobbying for nuclear weapons and made several speeches on Indian radio. In 1964, Bhabha told the Indian public via Indian radio that "such nuclear weapons are remarkably cheap" and supported his arguments by referring to the economical cost of American nuclear testing programme ("Plowshare"). Bhabha stated to the politicians that a 10 kt device would cost around $350,000, and $600,000 for a 2 Mt. From this, he estimated that "a stockpile" of around 50 atomic bombs would cost under $21 million and a stockpile of 50 two-megaton hydrogen bombs would cost around $31.5 million." Bhabha did not realize, however, that the U.S. "Plowshare" cost-figures were produced by a vast industrial complex costing tens of billions of dollars, which had already manufactured nuclear weapons numbering in the tens of thousands. The delivery systems for nuclear weapons typically cost several times as much as the weapons themselves.
The nuclear program was partially slowed down when Lal Bahadur Shastri became the prime minister. In 1965, Shastri faced another war with West Pakistan (now Pakistan). Shastri appointed physicist Vikram Sarabhai as the head of the nuclear programme but because of his Gandhian beliefs Sarabhai directed the programme toward peaceful purposes rather than military development.
In 1967, Indira Gandhi became the prime minister and work on the nuclear program resumed with renewed vigor. Homi Sethna, a chemical engineer, played a significant role in the development of weapon-grade plutonium while Ramanna designed and manufactured the whole nuclear device. The first nuclear bomb project did not employ more than 75 scientists because of its sensitivity. The nuclear weapons programme was now directed towards the production of plutonium rather than uranium.
In 1968–69, P. K. Iyengar visited the Soviet Union with three colleagues and toured the nuclear research facilities at Dubna, Russia. During his visit, Iyengar was impressed by the plutonium-fueled pulsed fast reactor. Upon his return to India, Iyengar set about developing plutonium reactors approved by the Indian political leadership in January 1969. The secret plutonium plant was known as "Purnima", and construction began in March 1969. The plant's leadership included Iyengar, Ramanna, Homi Sethna, and Sarabhai. Sarabhai's presence indicates that, with or without formal approval, the work on nuclear weapons at Trombay had been commenced.
Secrecy and test preparations, 1972–74.
India continued to harbour ambivalent feelings about nuclear weapons, and accorded low priority to their production until the Indo-Pakistani War of 1971. In December 1971, Richard Nixon sent a carrier battle group led by the into the Bay of Bengal in an attempt to intimidate India. The Soviet Union responded by sending a submarine armed with nuclear missiles from Vladivostok to trail the US task force. The Soviet response demonstrated the deterrent value and significance of nuclear weapons and ballistic missile submarines to Indira Gandhi. India gained the military and political initiative over Pakistan after acceding to the treaty that divided Pakistan into two different political entities. The 1971 war crushed the Pakistani military, which lost more than half its strength, and the Pakistan–China axis was proven to be a "paper tiger" after the defeat.
On 7 September 1972, near the peak of her post-war popularity, Indira Gandhi authorized the Bhabha Atomic Research Centre (BARC) to manufacture a nuclear device and prepare it for a test. Although the Indian Army was not fully involved in the nuclear testing, the army's highest command was kept fully informed of the test preparations. The preparations were carried out under the watchful eyes of the Indian political leadership, with civilian scientists assisting the Indian Army.
The device was formally called the "Peaceful Nuclear Explosive", but it was usually referred to as the "Smiling Buddha". The device was detonated on 18 May 1974, Buddha Jayanti (a festival day in India marking the birth of Gautama Buddha). Indira Gandhi maintained tight control of all aspects of the preparations of the "Smiling Buddha" test, which was conducted in extreme secrecy; besides Gandhi, only advisers Parmeshwar Haksar and Durga Dhar were kept informed. Scholar Raj Chengappa asserts the Indian Defense Minister Jagjivan Ram was not provided with any knowledge of this test and came to learn of it only after it was conducted. Swaran Singh, the Minister of External Affairs, was given 48 hours advance notice. The Indira Gandhi administration employed no more than 75 civilian scientists while General G. G. Bewoor, Indian army chief, and the commander of Indian Western Command were the only military commanders kept informed.
Development teams and sites.
The head of this entire nuclear bomb project was the director of the BARC, Dr. Raja Ramanna. In later years his role would be more deeply integrated. He remained head of the nuclear program most of his life. The designer and creator of the bomb was Dr. P. K. Iyengar, who was the second in command of this project. Iyengar's work was further assisted by the chief metallurgist, R. Chidambaram, and by Nagapattinam Sambasiva Venkatesan of the Terminal Ballistics Research Laboratory, who developed and manufactured the high explosive implosion system. The explosive materials and the detonation system were developed by Waman Dattatreya Patwardhan of the High Energy Materials Research Laboratory. The overall project was supervised by Homi Sethna, Chairman of the Atomic Energy Commission of India. Chidambaram, who would later coordinate work on the Pokhran-II tests, began work on the equation of state of plutonium in late 1967 or early 1968. To preserve secrecy, the project employed no more than 75 scientists and engineers from 1967–74. It is theorised that Abdul Kalam also arrived at the test site as the representative of the DRDO, although he had no role whatsoever in the development of the nuclear bomb or even in the nuclear programme.
The device was of the implosion-type design and had a close resemblance to the American nuclear bomb called the "Fat Man". The implosion system was assembled at the Terminal Ballistics Research Laboratory (TBRL) of the DRDO in Chandigarh. The detonation system was developed at the High Energy Materials Research Laboratory (HEMRL) of the DRDO in Pune, Maharashtra State. The 6 kg of plutonium came from the CIRUS reactor at BARC. The neutron initiator was of the polonium–beryllium type and code-named "Flower". The complete nuclear bomb was engineered and finally assembled by Indian engineers at Trombay before transportation to the test site.
Nuclear weapon design.
Cross-section.
The fully assembled device had a hexagonal cross section, 1.25 metres in diameter, and weighed 1400 kg. The device was mounted on a hexagonal metal tripod, and was transported to the shaft on rails which the army kept covered with sand. The device was detonated when Dastidar pushed the firing button at 8.05 a.m.; it was in a shaft 107 m under the army Pokhran test range in the Thar Desert (or Great Indian Desert), Rajasthan. Coordinates of the crater are .
Controversy regarding the yield.
The nuclear yield of this test still remains controversial, with unclear data provided by Indian sources, although Indian politicians have given the country's sensational press ranges from 20 kt to as low as 2 kt. The official yield was initially set at 12 kt; post-Operation Shakti claims have raised it to 13 kt. Independent seismic data from outside and analysis of the crater features indicate a lower figure. Analysts usually estimate the yield at 4 to 6 kt, using conventional seismic magnitude-to-yield conversion formulas. In recent years, both Homi Sethna and P. K. Iyengar have conceded the official yield to be an exaggeration. Iyengar has variously stated that the yield was actually 8–10 kt, that the device was designed to yield 10 kt, and that the yield was 8 kt "exactly as predicted". Although seismic scaling laws lead to an estimated yield range between 3.2 kt and 21 kt, an analysis of hard rock cratering effects suggests a narrow range of around 8 kt for the yield, which is within the uncertainties of the seismic yield estimate.
Aftermath.
Domestic reaction.
Indian Prime Minister Indira Gandhi had already gained much popularity and publicity after her successful military campaign against Pakistan in the 1971 war. The test caused an immediate revival of Indira Gandhi's popularity, which had flagged considerably from its high after the 1971 war. The overall popularity and image of the Congress Party was enhanced and the Congress Party was well received in the Indian Parliament. In 1975, Homi Sethna, a chemical engineer and the chairman of the Indian Atomic Energy Commission (AECI), Raja Ramanna of BARC, and Basanti Nagchaudhuri of DRDO, all were honored with the "Padma Vibhushan", India's second highest civilian award. Five other project members received the "Padma Shri", India's fourth highest civilian award. India consistently maintained that this was a peaceful nuclear bomb test and that it had no intentions of militarizing its nuclear programme. However, according to independent monitors, this test was actually part of an accelerated Indian nuclear programme.
In 1997 Raja Ramanna, speaking to the "Press Trust of India", maintained:
International reaction.
While India continued to state that the test was for peaceful purposes, it encountered opposition from many quarters. The Nuclear Suppliers Group (NSG) was formed in reaction to the Indian tests to check international nuclear proliferation. The NSG decided in 1992 to require full-scope IAEA safeguards for any new nuclear export deals, which effectively ruled out nuclear exports to India, but in 2008 it waived this restriction on nuclear trade with India as part of the Indo-US civilian nuclear agreement.
Pakistan.
Pakistan did not view the test as a "peaceful nuclear explosion", and cancelled talks scheduled for 10 June on normalization of relations. Pakistan's Prime Minister Zulfikar Ali Bhutto vowed in June 1974 that he would never succumb to "nuclear blackmail" or accept "Indian hegemony or domination over the subcontinent". The chairman of the Pakistan Atomic Energy Commission, Munir Ahmed Khan, said that the test would force Pakistan to test its own nuclear bomb. Pakistan's leading nuclear physicist, Pervez Hoodbhoy, stated in 2011 that he believed the test "pushed [Pakistan] further into the nuclear arena".
Canada and United States.
The plutonium used in the test was created in the CIRUS reactor supplied by Canada and using heavy water supplied by the United States. Both countries reacted negatively, especially in light of then ongoing negotiations on the Nuclear Non-Proliferation Treaty and the economic aid both countries had provided to India. Canada concluded that the test violated a 1971 understanding between the two states, and froze nuclear energy assistance for the two heavy water reactors then under construction. The United States concluded that the test did not violate any agreement and proceeded with a June 1974 shipment of enriched uranium for the Tarapur reactor.
France.
France sent a congratulatory telegram to India but later withdrew it.
Subsequent nuclear explosions.
Despite many proposals, India did not carry out further nuclear tests until 1998. After the 1998 general elections, Operation Shakti (also known as Pokhran-II) was carried out at the Pokhran test site, using devices designed and built over the preceding two decades.

</doc>
<doc id="28855" url="http://en.wikipedia.org/wiki?curid=28855" title="Shea Stadium">
Shea Stadium

William A. Shea Municipal Stadium (usually shortened to Shea Stadium or just Shea ) was a stadium in the New York City borough of Queens, in Flushing Meadows–Corona Park. It was the home baseball park of Major League Baseball's New York Mets from 1964 to 2008. Originally built as a multi-purpose stadium, Shea was also the home of the New York Jets football team from 1964 to 1983. It was named in honor of William A. Shea, the man who brought National League baseball back to New York. It was demolished in 2009 to furnish additional parking for the adjacent Citi Field, the current home of the Mets.
History.
Planning and construction.
The origins of Shea Stadium go back to the controversial west coast relocation of the Brooklyn Dodgers and the New York Giants which left New York without a National League presence. New York City official Robert Moses tried to interest Dodgers owner Walter O'Malley in this site as the location for a new stadium, but O'Malley refused, unable to agree on location, ownership and lease terms. O'Malley preferred to pay construction costs himself so he would own the stadium outright. He wanted total control over revenue from parking, concessions, and other events. The City, by contrast, wanted to build the stadium, rent it, and retain these ancillary revenue rights to pay off its construction bonds. Additionally, O'Malley wanted to build his new stadium in Brooklyn, while Moses insisted on Flushing Meadows. When Los Angeles offered O'Malley what the City of New York wouldn't—complete and absolute ownership of the facility—he left for southern California in a preemptive bid to install the Dodgers there before a new or existing major league franchise could beat him to it. At the same time, Horace Stoneham moved his New York Giants to the San Francisco Bay Area, ensuring that there would be two west coast NL teams and preserving the longstanding rivalry with O'Malley's Dodgers that continues to this day.
In 1960, the National League agreed to grant an expansion franchise to the owners of the New York franchise in the abortive Continental League, provided that a new stadium be built. Mayor Robert Wagner, Jr. had to personally wire all National League owners and assure them that the city would build a park.
On October 6, 1961, the Mets signed a 30-year stadium lease, with an option for a 10-year renewal. Rent for what was originally budgeted as a $9 million facility was set at $450,000 annually, with a reduction of $20,000 each year until it reaches $300,000 annually.
The Mets' inaugural season (1962) was played in the Polo Grounds, with original plans calling for the team to move to a new stadium in 1963. In October 1962, Mets official Tom Meany said, "Only a series of blizzards or some other unforeseen trouble might hamper construction." That unforeseen trouble surfaced in a number of ways: the severe winter of 1962–1963, along with the bankruptcies of two subcontractors and labor issues. The end result was that both the Mets and Jets played at the Polo Grounds for one more year.
It was originally to be called "Flushing Meadow Park Municipal Stadium" – the name of the public park on which it was built – but a movement was launched to name it in honor of William A. Shea, the man who brought National League baseball back to New York.
Opening.
After 29 months and $28.5 million, Shea Stadium opened on April 17, 1964, with the Pittsburgh Pirates beating the Mets 4–3 before a crowd of 50,312. The stadium opened five days before the 1964-65 New York World's Fair across Roosevelt Avenue from the stadium. Although not officially part of the Fairgrounds, the stadium sported steel panels on its exterior in the blue-and-orange colors of the Fair. The panels were removed in 1980.
Demolition.
In accordance with New York City law, Shea Stadium was dismantled, rather than imploded. The company with the rights to sell memorabilia was given two weeks after the final game to remove seats, signage and other potentially saleable/collectable items before demolition was to begin. The seats were the first ($869 per pair plus tax, a combination of '86 and '69), followed by other Shea memorabilia such as the foul poles, dugouts, stadium signage, and the giant letters that spell out "SHEA" at the front of the building.
After salvaging operations concluded, actual demolition of the ballpark began on October 14, 2008. On October 18, the scoreboard in right field was demolished, with the bleachers, batter's eye and bullpens soon to follow.
By November 10, the field, dugouts and the rest of the field level seats had been demolished.
By mid-December, all of the Loge level seats and a good portion of the Mezzanine level seating were gone as well, leaving only the outer shell remaining.
Demolition work on the upper deck began by January 1, 2009. The next day, all that remained of sections 26–48 of the upper deck was the steel framework. By January 8, the steel framework for sections 36–48 of the upper deck had been completely removed; all that remained of the "Live & In Person" advertising banner at the top above Gate A was the extreme right portion with the Shea Stadium Final Season logo. As of January 15, the far left field portion of Shea was completely demolished and the left field upper deck (sections 25–47) was stripped to its steel framework. The remaining letters at the top of the ballpark behind home plate were taken down on January 21. Approximately two-thirds of the stadium's outer superstructure was gone by January 24.
On January 31, Mets fans all over New York came to Shea for one final farewell to Shea Stadium. Fans took a tour of the site, told stories, and sang songs. The last remaining section of seats was demolished on February 18. Fans stood in awe as the remaining structure of Shea Stadium (one section of ramps) was torn down at 11:22 that morning.
Shea's home plate, pitcher's mound, and the bases are immortalized in Citi Field's parking lot, and feature engravings of the neon baseball players that once graced the exterior of the stadium.
Redevelopment.
On October 9, 2013, the New York City Council approved a plan to build a mall and entertainment center in the Citi Field parking lot where Shea Stadium once stood, as part of an effort by the city to redevelop the nearby neighborhood of Willets Point.
Stadium usage.
Sports.
Baseball.
Shea Stadium was the home of the New York Mets since 1964, and hosted the Major League Baseball All-Star Game that same year, with Johnny Callison of the Philadelphia Phillies hitting a home run in the ninth inning to win the only Mid-Summer Classic held in the Queens ballpark. A month earlier on Father's Day, Callison's teammate, future Hall of Fame member and United States Senator Jim Bunning, pitched a perfect game against the Mets.
The stadium was often criticized by baseball purists for many reasons, even though it was retrofitted to be a baseball-only stadium after the Jets left. The upper deck was one of the highest in the majors. The lower boxes were farther from the field than similar seats in other parks because they were still on the rails that swiveled the boxes into position for football. Outfield seating was always rather sparse, in part because the stadium was designed to be fully enclosed.
At one time, Shea's foul territory was one of the most expansive in the majors. This was very common for ballparks built during this era, in part due to the need to accommodate the larger football field. However, seats added over the years in the lower level greatly reduced the size of foul territory by the dawn of the 21st century. Also on the plus side, Shea always used a natural grass surface. This stood in contrast to multi-purpose stadiums such as Three Rivers Stadium. Veterans Stadium and Riverfront Stadium, which were built in the same era and style and used artificial turf instead of natural grass.
Shea Stadium hosted postseason baseball in 1969, 1973, 1986, 1988, 1999, 2000, and 2006; it hosted the World Series in 1969, 1973, 1986, and 2000. Shea Stadium had the distinction of being the home of the 1969 "Miracle Mets"—a team led by former Brooklyn Dodger Gil Hodges that defied 100–1 odds and won the World Series, this after recording seven straight seasons in last or next-to-last place. Shea became famous for the bedlam that took place after the Mets won the decisive Game 5 of the 1969 World Series, as fans stormed the field in celebration. A similar scene took place a few weeks earlier after the Mets defeated the Atlanta Braves in the first National League Championship Series to win the pennant.
Tommie Agee, Lenny Dykstra, Todd Pratt, Robin Ventura, and Benny Agbayani have all hit post-season, game-winning home runs at Shea.
Tommie Agee was the only player in the history of the ballpark to hit a home run into the upper deck in left field. The spot was marked with a sign featuring Agee's number, and the date of the event, April 10, 1969. Teammate Cleon Jones says the ball was still rising when it hit the seats, so it very likely could have been the longest home run ever hit at Shea Stadium.
In 1971, Dave Kingman---then with the San Francisco Giants; later to play for the Mets on two occasions – hit a home run that smashed off the windshield of the Giants' team bus, parked behind the left field bullpen.
For many years, the Mets' theme song, "Meet the Mets", was played at Shea before every home game. Jane Jarvis, a local jazz artist, played the popular songs on the Thomas organ at Mets games for many years at the stadium.
On October 3, 2004, the stadium was the venue of the last game in the history of the Montreal Expos when the Mets defeated the Expos 8–1. Their story ended where it had started 35 years earlier: at Shea Stadium. The following year, the Expos relocated to Washington, D.C., where they were renamed the Nationals.
As of June 10, 2005, the Mets had played more games at Shea Stadium than the Brooklyn Dodgers did at Ebbets Field.
The last game played at Shea Stadium was a loss to the Florida Marlins on September 28, 2008. However, the Mets were in the thick of the playoff chase until the last day. A win would have meant another game for Shea as the Mets were scheduled to play the Milwaukee Brewers in a one-game playoff for the NL Wild Card had they won. Following the game, there was a "Shea Goodbye" tribute in which many players from the Mets glory years entered the stadium and touched home plate one final time so that fans could pay their last respects to the players and the stadium the Mets called home for 45 years. The ceremony ended with Tom Seaver throwing a final pitch to Mike Piazza, then, as the Beatles "In My Life" played on the stadium speakers the two former Met stars walked out of the centerfield gate and closed it behind them, followed by a display of blue and orange fireworks.
Three National League Division Series were played at Shea Stadium. The Mets won all three, and never lost a Division Series game at Shea.
Seven National League Championship Series were played at Shea Stadium.
^ The decisive seventh game of this series was played at Shea Stadium, marking the only time that the Mets ever lost the deciding game of an NLCS at home.
Four World Series were played in Shea Stadium.
The Yankees World Series win in 2000 was the only time that visiting teams won a World Series at Shea Stadium. The Mets won both their World Series titles at Shea Stadium (in Game 5 in 1969, and Game 7 in 1986).
The New York Yankees played their home games in Shea Stadium during the 1974 and 1975 seasons while Yankee Stadium was being renovated. The move to Shea had been proposed earlier in the decade, but the Mets, as Shea's primary tenants, refused to sign off on the deal. However, when the city stepped in to pay for renovating Yankee Stadium, the Mets had little choice but to agree to share Shea with the Yankees.
Separately, on the afternoon of April 15, 1998, the Yankees also played one home game at Shea, against the Anaheim Angels after a beam collapsed at Yankee Stadium two days before, destroying several rows of seats. With the Mets playing a game at Shea that evening against the Chicago Cubs, the Yankees used the visitor's locker room and dugout and the Angels used the home dugout and old locker room of the New York Jets. Former Mets star Darryl Strawberry, then playing for the Yankees, hit a home run during the game. Stadium operators partially raised the Mets' home run apple signal before lowering it back down, much to the delight of the crowd present.
Shea Stadium also hosted the first extra-inning regular season baseball opener ever played in New York, on March 31, 1998, when the Mets opened their season against their rival Philadelphia Phillies, playing the longest scoreless opening day game in the National League and the longest one in the MLB since 1926. The Mets won the game 1–0 in the bottom of the 14th.
During the 1977 New York City blackout the stadium was plunged into darkness at approximately 9:30 p.m. during a game between the Mets and the Chicago Cubs. It occurred during the bottom of the sixth inning, with the Mets losing 2–1 and Lenny Randle at bat. Jane Jarvis, Shea's organist (affectionately known as Shea's "Queen of Melody") played "Jingle Bells" and "White Christmas". The game was eventually completed on September 16, with the Cubs winning 5–2.
Football.
The New York Jets of the American Football League and later, the National Football League played at Shea for twenty seasons, from 1964 to 1983 (excluding their first home game in 1977 played at Giants Stadium). The stadium hosted three Jets playoff games: the American Football League Championship in 1968 (beat the Oakland Raiders, 27–23), an AFL Divisional Playoff in 1969 (lost 13–6 to the Kansas City Chiefs) and the 1981 AFC Wild Card Playoff game (lost 31–27 to the Buffalo Bills).
For most of the Jets' tenure at Shea, they were burdened by onerous lease terms imposed at the insistence of the Mets. Until 1978, the Jets could not play their first home game until the Mets' season was finished. For instance, in 1969, the defending Super Bowl champion Jets didn't play a home game until October 20 due to the Mets advancing to (and winning) the World Series. Even after 1978, the Mets' status as Shea's primary tenants would require the Jets to go on long road trips (switching Shea from baseball to football configuration was a rather complex process, involving electrical, plumbing, field and other similar work). The stadium was also not well maintained in the 1970s. The Jets moved to Giants Stadium for the 1984 season, enticed by the additional 15,000+ seats offered there. Fans ripped Shea apart after the last game of the 1983 season, which also was the last NFL appearance for Pro Football Hall of Fame quarterback Terry Bradshaw, who threw two touchdown passes to lead the Pittsburgh Steelers to a 34–7 victory. Even the scoreboard operator had a field day, displaying the home team as the "N.J. Jets".
It was at Shea Stadium on December 16, 1973 that O.J. Simpson became the first running back to gain 2,000 yards in a single season (and, to date, the only player to do it in 14 games or fewer).
In the 1983 season, a Jets game against the Los Angeles Rams featured an 85-yard touchdown run by rookie Eric Dickerson, as well as a brawl between Rams offensive tackle Jackie Slater and Jets defensive end Mark Gastineau when Slater blindsided Gastineau after the Jet performed his infamous "Sack Dance" over fallen Rams quarterback Vince Ferragamo.
The NFL's New York Giants played their 1975 season at Shea while Giants Stadium was being built. The Giants were 5–9 that year (2–5 at Shea). Their coach was Bill Arnsparger and their quarterback was Craig Morton.
The football field at Shea extended from around home plate all the way to the outfield, with the baseline seating rotating out to fill left and right fields.
Soccer.
The first soccer game held at Shea Stadium occurred during tournament play from the International Soccer League on June 17, 1965. New York United of the American Soccer League called Shea home in 1980.
Other events.
On Sunday, August 15, 1965, the Beatles opened their 1965 North American tour there to a record audience of 55,600. "Beatlemania" was at one of its peaks at the Shea Concert. Film footage taken at the concert shows many teenagers and women crying, screaming, and even fainting. The crowd noise was such that security guards can be seen covering their ears as the Beatles enter the field. The sound of the crowd was so deafening that none of the Beatles (or anyone else) could hear what they were playing. Nevertheless, it was the first concert to be held at a major stadium and set records for attendance and revenue generation, demonstrating that outdoor concerts on a large scale could be successful and profitable, and led the Beatles to return to Shea for a successful encore on 23 August 1966. The attendance record stood until 1973 when it was broken by Led Zeppelin with 56,800 fans at Tampa Stadium.
The first major music event to play Shea Stadium after the Beatles successful run was the Summer Festival for Peace on August 6, 1970. It was a day-long fundraiser, which featured many of the era's biggest selling and seminal rock, folk, blues and jazz performers including: Janis Joplin, Paul Simon, Creedence Clearwater Revival, Steppenwolf, The James Gang, Miles Davis, Tom Paxton, John Sebastian and others.
The next music show at Shea Stadium was the historic 1971 concert by Grand Funk Railroad (Humble Pie opened) in 1971, where they broke the Beatles' then-record for fastest ticket sales. The same film makers for the documentary of the Rolling Stones concert at Altamont were commissioned to film this event, but the actual final film has, to date, never been released.
The stadium has hosted numerous concerts since, Jethro Tull with opening act Robin Trower in July 1976 (billed as Tull v. Boeing due to the stadium's proximity to LaGuardia Airport), The Who with opening act The Clash in October 1982, Simon & Garfunkel in August 1983. On August 18, 1983, The Police played in front of 70,000 fans at Shea, a concert that the band's singer and bassist Sting described as "like playing the top of Everest", and announced near the end of the concert: "We'd like to thank the Beatles for lending us their stadium." The Rolling Stones performed at Shea for a six-night run in October 1989, and Elton John & Eric Clapton played a concert in August 1992. Bruce Springsteen along with his famed backing band; the E Street Band performed at Shea in early October 2003.
The last concert event was a two-night engagement by Billy Joel on July 16, and July 18, 2008. The concert was dubbed the "The Last Play at Shea", and featured many special guest appearances, including former Beatle Paul McCartney who closed the second show with an emotional rendition of the Beatles classic "Let It Be". Other artists that joined Joel on stage for the shows were former Shea performer Roger Daltrey of The Who, Tony Bennett, Don Henley, John Mayer, John Mellencamp, Garth Brooks, and Steven Tyler of Aerosmith. The concert event is the subject of a documentary film of the same name, which is used along with Shea's history to tell the story of changes in American suburban life.
The 1978 International Convention of Jehovah's Witnesses was held at Shea Stadium from July 12 to July 16, 1978.
Shea Stadium was parodied as "Che Stadium" for The Rutles film "All You Need is Cash" for a sequence that spoofed the Beatles' concert at the stadium
During his tour of America in October 1979, Pope John Paul II was also among those hosted by Shea Stadium. On the morning of the Pontiff's visit, Shea Stadium was awash in torrential rain, causing ankle-deep mud puddles, and threatened to ruin the event. But as the Popemobile entered the stadium, the rain stopped.
On December 9, 1979, as part of the halftime show of an NFL game between the New York Jets and New England Patriots, a model airplane group put on a remote control airplane display. The grand finale was a remote control airplane, weighing 40 lbs, made to look like a red flying lawnmower. The pilot lost control of the airplane, and it crashed into the stands and hit John Bowen of Nashua, New Hampshire. Bowen died six days later.
Between 1972 and 1980, Shea also hosted 3 wrestling events held by the then World Wrestling Federation.
In 1987, Marvel Comics rented Shea Stadium to re-enact the wedding of their two characters Spider-Man/Peter Parker and Mary Jane Watson.
Recently on VH1's documentary series "7 Ages of Rock", Shea Stadium was named the most hallowed venue in all of rock music.
In "" the stadium was destroyed in a fight between Godzilla and Crackler.
Shea Stadium was used in the 1970s for filming the 1973 movie "Bang The Drum Slowly" starring Robert De Niro and Michael Moriarty and the 1978 film "The Wiz". In the latter film, the exterior pedestrian ramps were used for a motorcycle chase scene with Michael Jackson & Diana Ross.
In "Men in Black", a Mets game at Shea was featured in the film, with outfielder Bernard Gilkey dropping a flyball after being distracted by an alien spacecraft in the sky. Shea was also featured in "Men in Black 3" which is where K and J in 1969 intercept Griffin and the ArcNet before Boris the Animal can capture it.
In the aftermath of the September 11 attacks, the stadium became a staging area for rescuers, its parking lots filled with food, water, medical supplies, even makeshift shelters where relief workers could sleep. Ten days later Shea reopened for the first post-attack sporting event in New York where the Mets beat the Braves, behind a dramatic home run by Mets catcher Mike Piazza.
1975: Four teams, one stadium.
The Mets, Yankees, Jets and Giants all called Shea home in 1975, the only time in professional sports history that two baseball teams and two football teams shared the same facility in the same year. As Yankee Stadium was being renovated and Giants Stadium was nearing completion, there were scheduling clashes between the New York teams in baseball from April through September and both football teams from October through December. Even though Shibe Park housed the Phillies, A's, and Eagles collectively from 1940 to 1954 (excluding 1941), the 1975 sports calendar in Shea Stadium was unrivaled. The Jets and also the Giants could not play a "home" game at Shea Stadium until the baseball season had ended for the main tenant Mets and the temporary incumbent Yankees. In 1975 there was only a two-week overlap between the baseball and football seasons (the NFL season starting on Sunday, September 21 that year, and the MLB season ending on Sunday, September 28). This meant the Jets could open at home on Sunday, October 5, the third week of the season, and the Giants on Sunday, October 12, the season's fourth week. This also meant, however, that the Giants and Jets had to play a combined 14 home games in the final 12 weeks of the 14-week NFL season. To do this, the Giants played two Saturday afternoon home games, each of which was un-televised, and each of which was played the day before a Jets' Sunday home game. Either the Jets or the Giants had a Sunday home game every weekend from October 5 through December 21, inclusive.
The Mets attracted 1,730,566 to their games while the Yankees attracted 1,288,048 to their home games at Shea. Having both the Giants and Jets share Shea Stadium for one season foreshadowed what was to come in the future with the Meadowlands (a.k.a. Giants Stadium) after the Jets left Flushing Meadows for New Jersey following the 1983 NFL season. 
Features.
Design.
Shea was a circular stadium, with the grandstand forming a perfect circle around the field and ending a short distance beyond the foul lines. The remainder of the perimeter was mostly empty space beyond the outfield fences. This space was occupied by the bullpens, scoreboards, and a section of bleachers beyond the left field fence. The stadium boasted 54 restrooms, 21 escalators and seats for 57,343. It was big, airy, sparkling, with a massive 86' x 175' scoreboard. Also, rather than the standard light towers, Shea had lamps along its upper reaches, like a convoy of semis with their brights on, which gave the field a unique high-wattage glow. Praised for its convenience, even its "elegance," Shea was actually deemed a showplace. These special features helped make Shea more popular during its lifetime than other "cookie-cutter" venues, like RFK Stadium, Atlanta-Fulton County Stadium, Riverfront Stadium and Three Rivers Stadium.
The stadium was located close to LaGuardia Airport. For many years, interruptions for planes flying overhead were common at Shea, and the noise was so loud that radio and television broadcasts couldn't be heard. Later, flight plans were altered to alleviate the noise problem.
Shea was originally designed to convert from a baseball field into a rectangle field suitable for football using two motor-operated stands that allow the field level seats to rotate on underground railroad tracks. After the New York Jets football team moved to Giants Stadium in East Rutherford, New Jersey in 1984, the Mets took over operation of the stadium and retrofitted it for exclusive baseball use. As part of the refitting, Shea Stadium's exterior was painted blue and neon signs of baseball player silhouettes were added to the windscreens prior to the 1988 season. The original scoreboard was removed, and a new one installed in its place (fitting into the shell left behind by the old one), in 1988. Also at that time, the original (wooden) outfield wall was removed and replaced by a padded fence.
Banks of ramps that provided access from the ground to the upper levels were built around the outside circumference of the stadium. The ramps were not walled in and were visible from the outside. The ramps were originally partly covered with many rectangular panels in blue and orange (two of the team's colors). These panels can be seen in the 1970s movie "The Wiz"; it used the exterior pedestrian ramps for a motorcycle chase scene with Michael Jackson and Diana Ross. The 1960s-style decorations were removed in 1980. The banks of ramps resulted in the outer wall of the stadium jutting out where the banks existed. In some of the recessed bays between the banks, huge neon lights formed the figures of baseball players.
The design also allowed for Shea Stadium to be expandable to 90,000 seats (by completely enclosing the grandstand), or to be later enclosed by a dome if warranted. In March 1965, a plan was formally announced to add a glass dome and add 15,000 seats. The Mets strongly objected to the proposal. The idea was dropped after engineering studies concluded that the stadium's foundation would be unable to support the weight of the dome.
Initially, the distances to the right and left field fences were each 341 ft. There was a horizontal orange line that decided where a batted ball was a home run or still in play. In 1978, Manager Joe Torre helped move the fences in to 338 ft in the corners with a wall now in front of the original brick wall to help alleviate disputed calls.
Originally, all of the seats were wooden, with each level having a different color. The field boxes were yellow, the loge level seats were brown, the mezzanine seats were blue, and the upper deck seats were green. Each level above the field level was divided into box seats (below the portals) and reserved seats (above the portals). The box seats were of a darker shade than the reserved seats. The game ticket was the same color as the seat that it was for, and the signs in the lobby for that section were the same color as the seat and the ticket. Before the 1980 baseball season they were replaced with red (upper deck), green (mezzanine), blue (loge), and orange (field level) plastic seats.
Unlike the crosstown Yankee Stadium, Shea was built on an open field, so there was no need to have it conform to the surrounding streets.
Before Shea Stadium closed in 2008, it was the only stadium in the Major Leagues with orange foul poles. This tradition is carried on at Citi Field as the foul poles there are the same color.
After the Jets left Shea, the exterior of the stadium was painted blue and white, two of the Mets' team colors.
In 2003, large murals celebrating the Mets' two world championships were added, covering the two ends of the grandstand. The 1986 mural was removed after the 2006 season because of deterioration (the wall was re-painted solid blue, and a window was opened on the Mezzanine level where fans could view the progress of Citi Field), but the 1969 mural survived until the final game in 2008.
The scoreboard was topped by a representation of the New York Skyline, a prominent part of the team logo. Since the September 11 terrorist attacks, the Twin Towers of the World Trade Center were kept unlit, with a red-white-and-blue ribbon placed over them. The scoreboard was demolished in October 2008, but the skyline was preserved and is now located on the Shake Shack in Citi Field's "Taste Of The City" food court behind the giant scoreboard in center field.
For the 2007 and 2008 seasons, the construction of Citi Field was visible beyond the left and center field walls of Shea.
From 1973 to 1979, fans could estimate the distance of home run balls, since there were several signs beyond the outfield wall giving the distance in feet from home plate, in addition to the nine markers within the field.
Home Run Apple.
The Home Run Apple came out of a magic hat after every Mets home run at Shea Stadium. It was first installed in May 1980 as a symbol of the Mets' advertising slogan "The Magic Is Back!" (the hat originally said "Mets Magic" in script but was changed in the mid-1980s to a simple "Home Run" in block capital letters). A newer, bigger apple has been placed in center field at Citi Field; in 2009 Shea's original apple was installed inside Citi Field's Bullpen Gate and was visible from outside, on 126th Street. In 2010, the Shea apple was relocated outside the ballpark, in front of the Jackie Robinson Rotunda.
Homages.
Three players in the National League named their children after Shea Stadium.
Actor Kevin James, who is a devoted Mets fan named his youngest daughter, Shea Joelle after the ballpark.

</doc>
<doc id="28857" url="http://en.wikipedia.org/wiki?curid=28857" title="Signal transduction">
Signal transduction

Signal transduction occurs when an extracellular signaling molecule activates a specific receptor located on the cell surface or inside the cell. In turn, this receptor triggers a biochemical chain of events inside the cell, creating a response. Depending on the cell, the response alters the cell's metabolism, shape, gene expression, or ability to divide. The signal can be amplified at any step. Thus, one signaling molecule can cause many responses.
History.
In 1970, Martin Rodbell examined the effects of glucagon on a rat's liver cell membrane receptor. He noted that guanosine triphosphate disassociated glucagon from this receptor and stimulated the G-protein, which strongly influenced the cell's metabolism. Thus, he deduced that the G-protein is a transducer that accepts glucagon molecules and affects the cell. For this, he shared the 1994 Nobel Prize in Physiology or Medicine with Alfred G. Gilman.
The earliest MEDLINE entry for "signal transduction" dates from 1972. Some early articles used the terms "signal transmission" and "sensory transduction". In 2007, a total of 48,377 scientific papers—including 11,211 review papers—were published on the subject. The term first appeared in a paper's title in 1979. Widespread use of the term has been traced to a 1980 review article by Rodbell: Research papers focusing on signal transduction first appeared in large numbers in the late 1980s and early 1990s.
Signal transduction involves the binding of extracellular "signalling molecules" and ligands to cell-surface receptors that trigger events inside the cell. The combination of messenger with receptor causes a change in the conformation of the receptor, known as "receptor activation". This activation is always the initial step (the cause) leading to the cell's ultimate responses (effect) to the messenger. Despite the myriad of these ultimate responses, they are all directly due to changes in particular cell proteins. Intracellular signaling cascades can be started through cell-substratum interactions; examples are the integrin that binds ligands in the extracellular matrix and steroids. Most steroid hormones have receptors within the cytoplasm and act by stimulating the binding of their receptors to the promoter region of steroid-responsive genes. Examples of signaling molecules include the hormone melatonin, the neurotransmitter acetylcholine and the cytokine interferon γ.
The classifications of signalling molecules do not take into account the molecular nature of each class member; neurotransmitters range in size from small molecules such as dopamine to neuropeptides such as endorphins. Some molecules may fit into more than one class; for example, epinephrine is a neurotransmitter when secreted by the central nervous system and a hormone when secreted by the adrenal medulla.
Environmental stimuli.
With single-celled organisms, the variety of signal transduction processes influence its reaction to its environment. With multicellular organisms, numerous processes are required for coordinating individual cells to support the organism as a whole; the complexity of these processes tend to increase with the complexity of the organism. Sensing of environments at the cellular level relies on signal transduction; modeling signal transduction systems as self-organizing allows to explain how equilibria are maintained, many disease processes, such as diabetes and heart disease arise from defects or dysregulations in these pathways, highlighting the importance of this process in biology and medicine.
Various environmental stimuli exist that initiate signal transmission processes in multicellular organisms; examples include photons hitting cells in the retina of the eye, and odorants binding to odorant receptors in the nasal epithelium. Certain microbial molecules, such as viral nucleotides and protein antigens, can elicit an immune system response against invading pathogens mediated by signal transduction processes. This may occur independent of signal transduction stimulation by other molecules, as is the case for the toll-like receptor. It may occur with help from stimulatory molecules located at the cell surface of other cells, as with T-cell receptor signaling. Unicellular organisms may respond to environmental stimuli through the activation of signal transduction pathways. For example, slime molds secrete cyclic adenosine monophosphate upon starvation, stimulating individual cells in the immediate environment to aggregate, and yeast cells use mating factors to determine the mating types of other cells and to participate in sexual reproduction.
Receptors.
Receptors can be roughly divided into two major classes: intracellular receptors and extracellular receptors.
Extracellular.
Extracellular receptors are integral transmembrane proteins and make up most receptors. They span the plasma membrane of the cell, with one part of the receptor on the outside of the cell and the other on the inside. Signal transduction occurs as a result of a ligand binding to the outside; the molecule does not pass through the membrane. This binding stimulates a series of events inside the cell; different types of receptors stimulate different responses and receptors typically respond to only the binding of a specific ligand. Upon binding, the ligand induces a change in the conformation of the inside part of the receptor. These result in either the activation of an enzyme in the receptor or the exposure of a binding site for other intracellular signaling proteins within the cell, eventually propagating the signal through the cytoplasm.
In eukaryotic cells, most intracellular proteins activated by a ligand/receptor interaction possess an enzymatic activity; examples include tyrosine kinase and phosphatases. Some of them create second messengers such as cyclic AMP and IP3, the latter controlling the release of intracellular calcium stores into the cytoplasm. Other activated proteins interact with adaptor proteins that facilitate signalling protein interactions and coordination of signalling complexes necessary to respond to a particular stimulus. Enzymes and adaptor proteins are both responsive to various second messenger molecules.
Many adaptor proteins and enzymes activated as part of signal transduction possess specialized protein domains that bind to specific secondary messenger molecules. For example, calcium ions bind to the EF hand domains of calmodulin, allowing it to bind and activate calmodulin-dependent kinase. PIP3 and other phosphoinositides do the same thing to the Pleckstrin homology domains of proteins such as the kinase protein AKT.
G protein-coupled.
G protein-coupled receptors (GPCRs) are a family of integral transmembrane proteins that possess seven transmembrane domains and are linked to a heterotrimeric G protein. Many receptors are in this family, including adrenergic receptors and chemokine receptors.
Signal transduction by a GPCR begins with an inactive G protein coupled to the receptor; it exists as a heterotrimer consisting of Gα, Gβ, and Gγ. Once the GPCR recognizes a ligand, the conformation of the receptor changes to activate the G protein, causing Gα to bind a molecule of GTP and dissociate from the other two G-protein subunits. The dissociation exposes sites on the subunits that can interact with other molecules. The activated G protein subunits detach from the receptor and initiate signaling from many downstream effector proteins such as phospholipases and ion channels, the latter permitting the release of second messenger molecules. The total strength of signal amplification by a GPCR is determined by the lifetimes of the ligand-receptor complex and receptor-effector protein complex and the deactivation time of the activated receptor and effectors through intrinsic enzymatic activity.
A study was conducted where a point mutation was inserted into the gene encoding the chemokine receptor CXCR2; mutated cells underwent a malignant transformation due to the expression of CXCR2 in an active conformation despite the absence of chemokine-binding. This meant that chemokine receptors can contribute to cancer development.
Tyrosine and histidine kinase.
Receptor tyrosine kinases (RTKs) are transmembrane proteins with an intracellular kinase domain and an extracellular domain that binds ligands; examples include growth factor receptors such as the insulin receptor. To perform signal transduction, RTKs need to form dimers in the plasma membrane; the dimer is stabilized by ligands binding to the receptor. The interaction between the cytoplasmic domains stimulates the autophosphorylation of tyrosines within the domains of the RTKs, causing conformational changes. Subsequent to this, the receptors' kinase domains are activated, initiating phosphorylation signaling cascades of downstream cytoplasmic molecules that facilitate various cellular processes such as cell differentiation and metabolism.
As is the case with GPCRs, proteins that bind GTP play a major role in signal transduction from the activated RTK into the cell. In this case, the G proteins are members of the Ras, Rho, and Raf families, referred to collectively as small G proteins. They act as molecular switches usually tethered to membranes by isoprenyl groups linked to their carboxyl ends. Upon activation, they assign proteins to specific membrane subdomains where they participate in signaling. Activated RTKs in turn activate small G proteins that activate guanine nucleotide exchange factors such as SOS1. Once activated, these exchange factors can activate more small G proteins, thus amplifying the receptor's initial signal. The mutation of certain RTK genes, as with that of GPCRs, can result in the expression of receptors that exist in a constitutively activate state; such mutated genes may act as oncogenes.
Histidine-specific protein kinases are structurally distinct from other protein kinases and are found in prokaryotes, fungi, and plants as part of a two-component signal transduction mechanism: a phosphate group from ATP is first added to a histidine residue within the kinase, then transferred to an aspartate residue on a receiver domain on a different protein or the kinase itself, thus activating the aspartate residue.
Integrin.
Integrins are produced by a wide variety of cells; they play a role in cell attachment to other cells and the extracellular matrix and in the transduction of signals from extracellular matrix components such as fibronectin and collagen. Ligand binding to the extracellular domain of integrins changes the protein's conformation, clustering it at the cell membrane to initiate signal transduction. Integrins lack kinase activity; hence, integrin-mediated signal transduction is achieved through a variety of intracellular protein kinases and adaptor molecules, the main coordinator being integrin-linked kinase. As shown in the picture to the right, cooperative integrin-RTK signalling determines the timing of cellular survival, apoptosis, proliferation, and differentiation.
Important differences exist between integrin-signalling in circulating blood cells and non-circulating cells such as epithelial cells; integrins of circulating cells are normally inactive. For example, cell membrane integrins on circulating leukocytes are maintained in an inactive state to avoid epithelial cell attachment; they are activated only in response to stimuli such as those received at the site of an inflammatory response. In a similar manner, integrins at the cell membrane of circulating platelets are normally kept inactive to avoid thrombosis. Epithelial cells (which are non-circulating) normally have active integrins at their cell membrane, helping maintain their stable adhesion to underlying stromal cells that provide signals to maintain normal functioning.
Toll gate.
When activated, toll-like receptors (TLRs) take adapter molecules within the cytoplasm of cells in order to propagate a signal. Four adaptor molecules are known to be involved in signaling, which are Myd88, TIRAP, TRIF, and TRAM. These adapters activate other intracellular molecules such as IRAK1, IRAK4, TBK1, and IKKi that amplify the signal, eventually leading to the induction or suppression of genes that cause certain responses. Thousands of genes are activated by TLR signaling, implying that this method constitutes an important gateway for gene modulation.
Ligand-gated ion channel.
A ligand-gated ion channel, upon binding with a ligand, changes conformation to open a channel in the cell membrane through which ions relaying signals can pass. An example of this mechanism is found in the receiving cell of a neural synapse. The influx of ions that occurs in response to the opening of these channels induces action potentials, such as those that travel along nerves, by depolarizing the membrane of post-synaptic cells, resulting in the opening of voltage-gated ion channels.
An example of an ion allowed into the cell during a ligand-gated ion channel opening is Ca2+; it acts as a second messenger initiating signal transduction cascades and altering the physiology of the responding cell. This results in amplification of the synapse response between synaptic cells by remodelling the dendritic spines involved in the synapse.
Intracellular.
Intracellular receptors, such as nuclear receptors and cytoplasmic receptors, are soluble proteins localized within their respective areas. The typical ligands for nuclear receptors are lipophilic hormones like the steroid hormones testosterone and progesterone and derivatives of vitamins A and D. To initiate signal transduction, the ligand must pass through the plasma membrane by passive diffusion. On binding with the receptor, the ligands pass through the nuclear membrane into the nucleus, enabling gene transcription and protein production.
Activated nuclear receptors attach to the DNA at receptor-specific hormone-responsive element (HRE) sequences, located in the promoter region of the genes activated by the hormone-receptor complex. Due to their enabling gene transcription, they are alternatively called inductors of gene expression. All hormones that act by regulation of gene expression have two consequences in their mechanism of action; their effects are produced after a characteristically long period of time and their effects persist for another long period of time, even after their concentration has been reduced to zero, due to a relatively slow turnover of most enzymes and proteins that would either deactivate or terminate ligand binding onto the receptor.
Signal transduction via these receptors involves little proteins, but the details of gene regulation by this method are not well-understood. Nucleic receptors have DNA-binding domains containing zinc fingers and a ligand-binding domain; the zinc fingers stabilize DNA binding by holding its phosphate backbone. DNA sequences that match the receptor are usually hexameric repeats of any kind; the sequences are similar but their orientation and distance differentiate them. The ligand-binding domain is additionally responsible for dimerization of nucleic receptors prior to binding and providing structures for transactivation used for communication with the translational apparatus.
Steroid receptors are a subclass of nuclear receptors located primarily within the cytosol; in the absence of steroids, they cling together in an aporeceptor complex containing chaperone or heatshock proteins (HSPs). The HSPs are necessary to activate the receptor by assisting the protein to fold in a way such that the signal sequence enabling its passage into the nucleus is accessible. Steroid receptors, on the other hand, may be repressive on gene expression when their transactivation domain is hidden; activity can be enhanced by phosphorylation of serine residues at their N-terminal as a result of another signal transduction pathway, a process called crosstalk.
Retinoic acid receptors are another subset of nuclear receptors. They can be activated by an endocrine-synthesized ligand that entered the cell by diffusion, a ligand synthesised from a precursor like retinol brought to the cell through the bloodstream or a completely intracellularly synthesised ligand like prostaglandin. These receptors are located in the nucleus and are not accompanied by HSPs; they repress their gene by binding to their specific DNA sequence when no ligand binds to them, and vice versa.
Certain intracellular receptors of the immune system are cytoplasmic receptors; recently identified NOD-like receptors (NLRs) reside in the cytoplasm of some eukaryotic cells and interact with ligands using a leucine-rich repeat (LRR) motif similar to TLRs. Some of these molecules like NOD2 interact with RIP2 kinase that activates NF-κB signaling, whereas others like NALP3 interact with inflammatory caspases and initiate processing of particular cytokines like interleukin-1β.
Second messengers.
First messengers are the intercellular chemical messengers (hormones, neurotransmitters, and paracrine/autocrine agents) that reach the cell from the extracellular fluid and bind to their specific receptors. Second messengers are the substances that enter the cytoplasm and act within the cell to trigger a response. In essence, second messengers serve as chemical relays from the plasma membrane to the cytoplasm, thus carrying out intracellular signal transduction.
Calcium.
The release of calcium ions from the endoplasmic reticulum into the cytosol results in its binding to signaling proteins that are then activated; it is then sequestered in the smooth endoplasmic reticulum and the mitochondria. Two combined receptor/ion channel proteins control the transport of calcium: the InsP3-receptor that transports calcium upon interaction with inositol triphosphate on its cytosolic side; and the ryanodine receptor named after the alkaloid ryanodine, similar to the InsP3 receptor but having a feedback mechanism that releases more calcium upon binding with it. The nature of calcium in the cytosol means that it is active for only a very short time, meaning its free state concentration is very low and is mostly bound to organelle molecules like calreticulin when inactive.
Calcium is used in many processes including muscle contraction, neurotransmitter release from nerve endings, and cell migration. The three main pathways that lead to its activation are GPCR pathways, RTK pathways, and gated ion channels; it regulates proteins either directly or by binding to an enzyme.
Lipophilics.
Lipophilic second messenger molecules are derived from lipids residing in cellular membranes; enzymes stimulated by activated receptors activate the lipids by modifying them. Examples include diacylglycerol and ceramide, the former required for the activation of protein kinase C.
Nitric oxide.
Nitric oxide (NO) acts as a second messenger because it is a free radical that can diffuse through the plasma membrane and affect nearby cells. It is synthesised from arginine and oxygen by the NO synthase and works through activation of soluble guanylyl cyclase, which when activated produces another second messenger, cGMP. NO can also act through covalent modification of proteins or their metal co-factors; some have a redox mechanism and are reversible. It is toxic in high concentrations and causes damage during stroke, but is the cause of many other functions like relaxation of blood vessels, apoptosis, and penile erections.
Redox signaling.
In addition to nitric oxide, other electronically activated species are also signal-transducing agents in a process called redox signaling. Examples include superoxide, hydrogen peroxide, carbon monoxide, and hydrogen sulfide. Redox signaling also includes active modulation of electronic flows in semiconductive biological macromolecules.
Cellular responses.
Gene activations and metabolism alterations are examples of cellular responses to extracellular stimulation that require signal transduction. Gene activation leads to further cellular effects, since the products of responding genes include instigators of activation; transcription factors produced as a result of a signal transduction cascade can activate even more genes. Hence, an initial stimulus can trigger the expression of a large number of genes, leading to physiological events like the increased uptake of glucose from the blood stream and the migration of neutrophils to sites of infection. The set of genes and their activation order to certain stimuli is referred to as a genetic program.
Mammalian cells require stimulation for cell division and survival; in the absence of growth factor, apoptosis ensues. Such requirements for extracellular stimulation are necessary for controlling cell behavior in unicellular and multicellular organisms; signal transduction pathways are perceived to be so central to biological processes that a large number of diseases are attributed to their disregulation.
Three basic signals determine cellular growth:
The combination of these signals are integrated in an altered cytoplasmic machinery which leads to altered cell behaviour.
Major pathways.
Following are some major signaling pathways, demonstrating how ligands binding to their receptors can affect second messengers and eventually result in altered cellular responses.

</doc>
<doc id="28858" url="http://en.wikipedia.org/wiki?curid=28858" title="Stone–Weierstrass theorem">
Stone–Weierstrass theorem

In mathematical analysis, the Weierstrass approximation theorem states that every continuous function defined on a closed interval ["a", "b"] can be uniformly approximated as closely as desired by a polynomial function. Because polynomials are among the simplest functions, and because computers can directly evaluate polynomials, this theorem has both practical and theoretical relevance, especially in polynomial interpolation. The original version of this result was established by Karl Weierstrass in 1885 using the Weierstrass transform.
Marshall H. Stone considerably generalized the theorem and simplified the proof . His result is known as the Stone–Weierstrass theorem. The Stone–Weierstrass theorem generalizes the Weierstrass approximation theorem in two directions: instead of the real interval ["a", "b"], an arbitrary compact Hausdorff space X is considered, and instead of the algebra of polynomial functions, approximation with elements from more general subalgebras of C("X") is investigated. The Stone–Weierstrass theorem is a vital result in the study of the algebra of continuous functions on a compact Hausdorff space.
Further, there is a generalization of the Stone–Weierstrass theorem to noncompact Tychonoff spaces, namely, any continuous function on a Tychonoff space is approximated uniformly on compact sets by algebras of the type appearing in the Stone–Weierstrass theorem and described below.
A different generalization of Weierstrass' original theorem is Mergelyan's theorem, which generalizes it to functions defined on certain subsets of the complex plane.
Weierstrass approximation theorem.
The statement of the approximation theorem as originally discovered by Weierstrass is as follows:
A constructive proof of this theorem using Bernstein polynomials is outlined on that page.
Applications.
As a consequence of the Weierstrass approximation theorem, one can show that the space C["a", "b"] is separable: the polynomial functions are dense, and each polynomial function can be uniformly approximated by one with rational coefficients; there are only countably many polynomials with rational coefficients. Since C["a", "b"] is Hausdorff and separable it follows that C["a", "b"] has cardinality equal to 2ℵ0 — the same cardinality as the cardinality of the reals. (Remark: This cardinality result also follows from the fact that a continuous function on the reals is uniquely determined by its restriction to the rationals.)
Stone–Weierstrass theorem, real version.
The set C["a", "b"] of continuous real-valued functions on ["a", "b"], together with the supremum norm , is a Banach algebra, (i.e. an associative algebra and a Banach space such that || "fg"|| ≤ || "f" ||·||"g"|| for all  "f", "g"). The set of all polynomial functions forms a subalgebra of C["a", "b"] (i.e. a vector subspace of C["a", "b"] that is closed under multiplication of functions), and the content of the Weierstrass approximation theorem is that this subalgebra is dense in C["a", "b"].
Stone starts with an arbitrary compact Hausdorff space X and considers the algebra C("X", R) of real-valued continuous functions on X, with the topology of uniform convergence. He wants to find subalgebras of C("X", R) which are dense. It turns out that the crucial property that a subalgebra must satisfy is that it "separates points": a set A of functions defined on X is said to separate points if, for every two different points x and y in X there exists a function p in A with "p"("x") ≠ "p"("y"). Now we may state:
This implies Weierstrass’ original statement since the polynomials on ["a", "b"] form a subalgebra of C["a", "b"] which contains the constants and separates points.
Locally compact version.
A version of the Stone–Weierstrass theorem is also true when X is only locally compact. Let C0("X", R) be the space of real-valued continuous functions on X which vanish at infinity; that is, a continuous function "f" is in C0("X", R) if, for every "ε" > 0, there exists a compact set "K" ⊂ "X" such that "f"  < "ε" on "X" \ "K". Again, C0("X", R) is a Banach algebra with the supremum norm. A subalgebra A of C0("X", R) is said to vanish nowhere if not all of the elements of A simultaneously vanish at a point; that is, for every x in X, there is some "f" in A such that "f" ("x") ≠ 0. The theorem generalizes as follows:
This version clearly implies the previous version in the case when X is compact, since in that case C0("X", R) = C("X", R). There are also more general versions of the Stone–Weierstrass that weaken the assumption of local compactness.
Applications.
The Stone–Weierstrass theorem can be used to prove the following two statements which go beyond Weierstrass's result.
The theorem has many other applications to analysis, including:
Stone–Weierstrass theorem, complex version.
Slightly more general is the following theorem, where we consider the algebra C("X", C) of complex-valued continuous functions on the compact space X, again with the topology of uniform convergence. This is a C*-algebra with the *-operation given by pointwise complex conjugation.
The complex unital *-algebra generated by S consists of all those functions that can be obtained from the elements of S by throwing in the constant function 1 and adding them, multiplying them, conjugating them, or multiplying them with complex scalars, and repeating finitely many times. 
This theorem implies the real version, because if a sequence of complex-valued functions uniformly approximate a given function  "f" , then the real parts of those functions uniformly approximate the real part of  "f" . As in the real case, an analog of this theorem is true for locally compact Hausdorff spaces.
Stone–Weierstrass theorem, quaternion version.
Following : consider the algebra C("X", H) of quaternion-valued continuous functions on the compact space X, again with the topology of uniform convergence. If a quaternion "q" is written in the form "q"=a+"i"b+"j"c+"k"d then the scalar part a is the real number ("q"-"iqi"-"jqj"-"kqk")/4. Likewise being the scalar part of "-qi","-qj" and "-qk" : b,c and d are respectively the real numbers ("-qi"-"iq"+"jqk"-"kqj")/4,
("-qj"-"iqk"-"jq"+"kqi")/4 and ("-qk"+"iqj"-"jqk"-"kq")/4. Then we may state :
Lattice versions.
Let X be a compact Hausdorff space. Stone's original proof of the theorem used the idea of lattices in C("X", R). A subset L of C("X", R) is called a lattice if for any two elements  "f", "g" ∈ "L", the functions max{ "f", "g"}, min{ "f", "g"}also belong to L. The lattice version of the Stone–Weierstrass theorem states:
The above versions of Stone–Weierstrass can be proven from this version once one realizes that the lattice property can also be formulated using the absolute value | "f" | which in turn can be approximated by polynomials in  "f" . A variant of the theorem applies to linear subspaces of C("X", R) closed under max :
More precise information is available:
Bishop's theorem.
Another generalization of the Stone–Weierstrass theorem is due to Errett Bishop. Bishop's theorem is as follows :
 gives a short proof of Bishop's theorem using the Krein–Milman theorem in an essential way, as well as the Hahn–Banach theorem : the process of . See also .
References.
Historical works.
The historical publication of Weierstrass (in German language) is freely available from the digital online archive of the "":
Important historical works of Stone include:
Books.
'Optimization: Insights and Applications', Jan Brinkhuis and Vladimir Tikhomirov: 2005, Princeton University Press

</doc>
<doc id="28860" url="http://en.wikipedia.org/wiki?curid=28860" title="Simile">
Simile

A simile () is a figure of speech that directly compares two things through the explicit use of connecting words (such as "like, as, so, than," or various verbs such as "resemble"). Although similes and metaphors are sometimes considered as interchangeable, similes acknowledge the imperfections and limitations of the comparative relationship to a greater extent than metaphors. Metaphors are subtler and therefore rhetorically stronger in that metaphors equate two things rather than simply compare them. Similes also hedge/protect the author against outrageous, incomplete, or unfair comparison. Generally, metaphor is the stronger and more encompassing of the two forms of rhetorical analogies. Similies are mainly used in forms of poetry that are comparing an inanimate and a living object. There are also terms that have a similies and personification in them, which is often used for humorous purposes and for comparing (what a similie is).
Uses.
In comedy.
Similes are used extensively in British comedy, notably in the slapstick era of the 1960s and 70s. In comedy, the simile is often used in negative style, e.g. he was as daft as a brush. They are also used in comedic context where a sensitive subject is broached, and the comedian will test the audience with response to a subtle implicit simile before going deeper.
Using "like".
A simile can explicitly provide the basis of a comparison or leave this basis implicit. In the implicit case the simile leaves the audience to determine for themselves which features of the target are being predicated. It may be a type of sentence that uses "as" or "like" to connect the words being compared.
Using "as".
The use of "as" makes the simile more explicit
The song "Everything at Once" by Lenka is also notable for the use of 18 similes with "as" in every verse.
Without 'like' or 'as'.
Sometimes similes do not have any connecting words ('like' or 'as').

</doc>
<doc id="28861" url="http://en.wikipedia.org/wiki?curid=28861" title="Serengeti">
Serengeti

The Serengeti () ecosystem is a geographical region in Africa. It is located in north Tanzania and extends to south-western Kenya between latitudes 1 and 3 degrees south latitude and 34 and 36 degrees east longitude. It spans approximately 30000 km2. The Kenyan part of the Serengeti is known as Maasai Mara.
The Serengeti hosts the largest terrestrial mammal migration in the world, which helps secure it as one of the Seven Natural Wonders of Africa and one of the ten natural travel wonders of the world. The Serengeti is also renowned for its large lion population and is one of the best places to observe prides in their natural environment. The region contains the Serengeti National Park in Tanzania and several game reserves.
Approximately 70 larger mammal and 500 bird species are found there. This high diversity is a function of diverse habitats, including riverine forests, swamps, kopjes, grasslands, and woodlands. Blue wildebeests, gazelles, zebras, and buffalos are some of the commonly found large mammals in the region.
There has been controversy about a proposed road to be built through the Serengeti.
Serengeti is derived from the Maasai language, Maa; specifically, "Serengit" meaning "Endless Plains".
History.
Much of the Serengeti was known to outsiders as Maasailand. The Maasai are known as fierce warriors and live alongside most wild animals with an aversion to eating game and birds, subsisting exclusively on their cattle. Historically, their strength and reputation kept the newly arrived Europeans from exploiting the animals and resources of most of their land. A rinderpest epidemic and drought during the 1890s greatly reduced the numbers of both Maasai and animal populations. The Tanzanian government later in the 20th century re-settled the Maasai around the Ngorongoro Crater. Poaching and the absence of fires, which had been the result of human activity, set the stage for the development of dense woodlands and thickets over the next 30–50 years. Tsetse fly populations now prevented any significant human settlement in the area.
By the mid-1970s, wildebeest and the Cape buffalo populations had recovered and were increasingly cropping the grass, reducing the amount of fuel available for fires. The reduced intensity of fires has allowed Acacia to once again become established.
Great migration.
Each year around the same time, the circular great wildebeest migration begins in the Ngorongoro Conservation Area of the southern Serengeti in Tanzania. This migration is a natural phenomenon determined by the availability of grazing. This phase lasts from approximately January to March, when the calving season begins – a time when there is plenty of rain-ripened grass available for the 260,000 zebra that precede 1.7 million wildebeest and the following hundreds of thousands of other plains game, including around 470,000 gazelles.
During February, the wildebeest spend their time on the short grass plains of the southeastern part of the ecosystem, grazing and giving birth to approximately 500,000 calves within a 2 to 3-week period. Few calves are born ahead of time and of these, hardly any survive. The main reason is that very young calves are more noticeable to predators when mixed with older calves from the previous year. As the rains end in May, the animals start moving northwest into the areas around the Grumeti River, where they typically remain until late June. The crossings of the Grumeti and Mara rivers beginning in July are a popular safari attraction because crocodiles are lying in wait. The herds arrive in Kenya in late July / August, where they stay for the remainder of the dry season, except that the Thomson's and Grant's Gazelles move only east/west. In early November, with the start of the short rains the migration starts moving south again, to the short grass plains of the southeast, usually arriving in December in plenty of time for calving in February.
About 250,000 wildebeest die during the journey from Tanzania to the Maasai Mara National Reserve in southwestern Kenya, a total of 800 km. Death is usually from thirst, hunger, exhaustion, or predation.
Ecology.
The Serengeti has some of East Africa's finest game areas. The governments of Tanzania and Kenya maintain a number of protected areas, including national parks, conservation areas, and game reserves, that give legal protection to over 80 percent of the Serengeti.
The southeastern area lies in the rain shadow of the Ngorongoro Conservation Area's highlands and is composed of shortgrass treeless plains with abundant small dicots. Soils are high in nutrients, overlying a shallow calcareous hardpan. A gradient of soil depth northwestward across the plains results in changes in the herbaceous community and taller grass. About 70 km west, Acacia woodlands appear suddenly and stretch west to Lake Victoria and north to the Loita Plains, north of the Maasai Mara National Reserve. The sixteen Acacia species vary over this range, their distribution determined by edaphic conditions and soil depth. Near Lake Victoria, flood plains have developed from ancient lakebeds.
In the far northwest, Acacia woodlands are replaced by broadleaved Terminalia-Combretum woodlands, caused by a change in geology. This area has the highest rainfall in the system and forms a refuge for the migrating ungulates at the end of the dry season.
Altitudes in the Serengeti range from 920 to with mean temperatures varying from 15 to. Although the climate is usually warm and dry, rainfall occurs in two rainy seasons: March to May, and a shorter season in October and November. Rainfall amounts vary from a low of 508 mm in the lee of the Ngorongoro highlands to a high of 1200 mm on the shores of Lake Victoria. The highlands, which are considerably cooler than the plains and are covered by montane forest, mark the eastern border of the basin in which the Serengeti lies.
The Serengeti plain is punctuated by granite and gneiss outcroppings known as kopjes. These outcroppings are the result of volcanic activity. Kopjes provide a microhabitat for non-plains wildlife. One kopje likely to be seen by visitors to the Serengeti is the Simba Kopje (Lion Kopje). The Serengeti was used as inspiration for the animated Disney feature film "The Lion King" and subsequent theatrical production.
The area is also home to the Ngorongoro Conservation Area, which contains Ngorongoro Crater and the Olduvai Gorge, where some of the oldest hominid fossils have been found.

</doc>
<doc id="28863" url="http://en.wikipedia.org/wiki?curid=28863" title="Sea of Marmara">
Sea of Marmara

The Sea of Marmara (Turkish: "Marmara Denizi", ), also known as the Sea of Marmora or the Marmara Sea, and in the context of classical antiquity as the Propontis (Greek: Προποντίς), is the inland sea, entirely within the borders of Turkey, that connects the Black Sea to the Aegean Sea, thus separating Turkey's Asian and European parts. The Bosphorus strait connects it to the Black Sea and the Dardanelles strait to the Aegean. The former also separates Istanbul into its Asian and European sides. The Sea has an area of 11,350 km² (280 km x 80 km) with the greatest depth reaching 1,370 m.
Geography.
The surface salinity of the sea averages about 22 parts per thousand, which is slightly greater than that of the Black Sea but only about two-thirds that of most oceans. However, the water is much more saline at the sea-bottom, averaging salinities of around 38 parts per thousand — similar to that of the Mediterranean Sea. This high-density saline water, like that of the Black Sea itself, does not migrate to the surface. Water from the Susurluk, Biga (Granicus) and Gonen Rivers also reduces the salinity of the sea, though with less influence than on the Black Sea. With little land in Thrace draining southward, almost all of these rivers flow from Anatolia.
The sea contains the archipelago of the Princes' Islands and Marmara Island, Avşa and Paşalimanı.
The south coast of the sea is heavily indented, and includes the Gulf of Izmit (Turkish: "İzmit Körfezi"), the Gulf of Gemlik (Turkish: "Gemlik Körfezi") Gulf of Bandırma (Turkish: "Bandırma Körfezi") and the Gulf of Erdek (Turkish: "Erdek Körfezi"). During a storm on December 29, 1999, the Russian oil tanker "Volgoneft" broke in two in the Sea of Marmara, and more than 1,500 tonnes of oil were spilled into the water.
The North Anatolian fault, which has triggered many major earthquakes in recent years, such as the August and November 1999 earthquakes in Izmit and Düzce, respectively, runs under the sea.
Extent.
The International Hydrographic Organization defines the limits of the Sea of Marmara as follows:
Name.
The sea takes its name from the island of Marmara, which is rich in sources of marble, from the Greek "μάρμαρον" ("marmaron"), "marble".
The sea's ancient Greek name "Propontis" derives from "pro" (before) and "pont-" (sea), deriving from the fact that the Greeks sailed through it to reach the Black Sea. In Greek mythology, a storm on Propontis brought the Argonauts back to an island they had left, precipitating a battle where either Jason or Heracles killed King Cyzicus, who mistook them for his Pelasgian enemies.
Towns and cities.
Towns and cities on the Marmara Sea coast include:
Gallery.
 Media related to at Wikimedia Commons

</doc>
<doc id="28866" url="http://en.wikipedia.org/wiki?curid=28866" title="Saint John, New Brunswick">
Saint John, New Brunswick

Saint John is the largest city in New Brunswick and the second largest city in the maritime provinces. It is known as the Fundy City due to its location on the north shore of the Bay of Fundy at the mouth of the Saint John River, as well as being the only city on the bay. In 1785 Saint John became the first incorporated city in Canada.
Saint John had a population of 70,063 in 2011 over an area of 315.82 sqkm. The Saint John metropolitan area covers a land area of 3,362.95 sqkm across the Caledonia Highlands, with a population (as of 2011) of 127,761 making it the second largest CMA in New Brunswick behind Moncton and marking an increase of 4.4% since 2006.
Geography and climate.
Physical geography.
Situated in the south-central portion of the province, along the north shore of the Bay of Fundy at the mouth of the Saint John River, the city is split by the south-flowing river and the east side is bordered on the north by the Kennebecasis River where it meets the Saint John River at Grand Bay. The harbour is home to a terminal for cruise ships as well as being a fairly busy home for various container ships.
The Saint John River itself flows into the Bay of Fundy through a narrow gorge several hundred feet wide at the centre of the city. It contains a unique phenomenon called the Reversing Falls where the diurnal tides of the bay reverse the water flow of the river for several kilometres. A series of underwater ledges at the narrowest point of this gorge also create a series of rapids.
The topography surrounding Saint John is hilly; a result of the influence of two coastal mountain ranges which run along the Bay of Fundy – the "St. Croix Highlands" and the "Caledonia Highlands". The soil throughout the region is extremely rocky with frequent granite outcrops. The coastal plain hosts numerous freshwater lakes in the eastern, western and northern parts of the city.
In Saint John the height difference from low to high tide is approximately 8 metres (28 ft) due to the funnelling effect of the Bay of Fundy as it narrows. The Reversing Falls in Saint John, actually an area of strong rapids, provides one example of the power of these tides; at every high tide, ocean water is pushed through a narrow gorge in the middle of the city and forces the Saint John River to reverse its flow for several hours.
Neighbourhoods.
Saint John is a city of neighbourhoods, with residents closely identifying with their particular area.
South (End) Central Peninsula—Uptown.
The central peninsula on the east side of the harbour, and the area immediately opposite on the west side, hosts the site of the original city from the merger of Parrtown and Carleton. The western side of the central peninsula subsequently saw increased development and currently includes the central business district (CBD) and the Trinity Royal heritage district, which together are referred to as "Uptown" by residents throughout the city. The term "Uptown" comes from the time when the city was an active port city and people at the slips would go up the hill to the city. As well, most of this area in the central peninsula is situated on a hill, it is rarely called "Downtown." The south end of the central peninsula, south of Duke Street, is appropriately called the South End.
North End (Indiantown/Millidgeville/Mount Pleasant/Portland).
The area north of the Highway #1 from the South Central Peninsula is called the North End; both areas being predominantly urban residential older housing which is undergoing gentrification. Much of the North End is made up of the former city of Portland and comprises another former working class area which is slowly undergoing gentrification at the eastern end of Douglas Avenue; immediately north of Portland and upstream from the Reversing Falls is the former community of Indiantown.
Vessels navigating the Saint John River can only transit the Reversing Falls gorge at slack tide, thus Indiantown became a location during the 19th and 20th centuries where tugboats and paddle wheelers could dock to wait. Being located at the beginning of the navigable part of the Saint John River, Indiantown also became a major terminal for vessels departing to ply their trade upriver.
Further north of the central part of the city, and northeast of the North End and Portland, along the southern bank of the Kennebecasis River is the area of Millidgeville which is generally considered a neighbourhood separate from the North End. The boundary of Millidgeville is typically thought to begin at the "Y" intersection of Somerset Street and Millidge Ave or right after Tartan St. It is a middle to upper-class neighbourhood. Located here is University of New Brunswick, as well as New Brunswick's largest health care centre, the Saint John Regional Hospital, and Saint John's only completely French school and community centre, Centre Scolaire Communautaire Samuel-de-Champlain.
The eastern area of the North End plays host to the city's largest park, and one of Canada's largest urban parks. Rockwood Park encompasses 890 hectares of upland Acadian mixed forest, many hills and several caves, as well as several freshwater lakes, with an extensive trail network, a golf course, and the Cherry Brook Zoo. The park was designed by Calvert Vaux in the mid-to-late 19th century. Mount Pleasant borders the park, and is generally seen as distinct from the traditionally poorer North End.
East Side (Simonds/Loch Lomond).
To the east of the Courtney Bay / Forebay and south of New Brunswick Route 1 is the East Side, where the city has experienced its greatest suburban sprawl in recent decades with commercial retail centres and residential subdivisions. There has been significant and consistent commercial and retail development in the Westmorland Road-McAllister Drive-Consumer's Drive-Major's Brook Drive-Retail Drive corridor since the 1970s, including McAllister Place, the city's largest shopping mall, which opened in 1978, and with active year-to-year development since 1994. The city's current airport is located further east on the coastal plain among several lakes at the far eastern edge of the municipality. Far east side is Loch Lomond, including several urban neighbourhoods are found here, including Forest Hills, Champlain Heights, and Lakewood Heights. The malls were built by filling in Major's Brook (a tributary to Marsh Creek), making the area prone to flooding.
West Side (Carleton/Lancaster/Fairville).
The portion of the city west of the Saint John River is collectively referred to as West Side, although West Saint Johners typically divide this area into several neighbourhoods. As mentioned previously, the Lower West Side is the former working-class neighbourhood that was known as Carleton at the time of the city's formation in 1785. West and north of the Lower West Side is the former city of Lancaster (commonly referred to as Saint John West), which was amalgamated into Saint John in 1967. The dividing line is generally agreed upon to be Martello Tower and not Lancaster Avenue, with the streets east and south of Lancaster Avenue being considered to be the "West Side, and the streets north and west of Lancaster Avenue, having been renamed from Lancaster, NB to Saint John West, NB.
The southern part of Lancaster abutting Saint John Harbour and the Bay of Fundy is Bayshore and the location of Canadian Pacific Railway's Bayshore Yard. The north end of Lancaster, known as Fairville, is home to Moosehead Brewery and older neighbourhoods clustered along Manawagonish Road. North of Fairville are the communities of Milford and Randolph. Randolph, which is home to Dominion Park Beach, includes land on the city's largest island, and is joined by the Canal Bridge over Mosquito Cove on Greenhead Road. The area also contains the Irving Pulp and Paper mill, a highly visible manufacturing plant that sits directly next to the Reversing Falls and is owned and operated by J. D. Irving, Ltd.
West of Lancaster, the city hosts its second largest park, and one of the largest coastal urban parks in the country. The Irving Nature Park, along Saints' Rest Beach sits on an extensive peninsula called Taylor's Island extending into the western part of the harbour into the Bay of Fundy. The park is partially open to vehicles in summer and features ocean views and walking trails through mixed forests.
Suburbs.
Saint John's suburbs, just on the edge of the city limit, are Rothesay, Quispamsis, and Grand Bay-Westfield. Mainly residential, the suburbs have attracted many of Saint John's residents leading to, until the last census of 2011, the city's population to shrink.
Climate.
The climate of Saint John is humid continental (Köppen climate classification ""). The Bay of Fundy never fully freezes, thus moderating the winter temperatures compared with inland locations. Even so, with the prevailing wind blowing from the west (from land to sea), the average January temperature is about -8.2 C. Summers are usually warm to hot, and daytime temperatures often exceed 25 C. The highest temperature recorded in a given year is usually 30 °C (86 °F) or 31 °C (88 °F). The confluence of cold Bay of Fundy air and inland warmer temperatures often creates onshore winds that bring periods of fog and cooler temperatures during the summer months.
Precipitation in Saint John totals about 1390 mm annually and is well distributed throughout the year, although the late autumn and early winter is typically the wettest time of year. Snowfalls can often be heavy, but rain is as common as snow in winter, and it is not unusual for the ground to be snow-free even in mid-winter.
Buildings and structures.
National Historic Sites.
There are 13 National Historic Sites of Canada in Saint John.
Demography.
Population.
The population of the city declined from the 1970 to the early 21st century, but this trend has now reversed itself and has shown its first increase in many years in the 2011 census.
Metropolitan area.
In the year 2011 the population of the Greater Saint John area was 127,761, of whom 49% were male and 51% female. Children under five accounted for approximately 21% of the population. People 65 and over accounted for 27% of the population. In the years between 1996 and 2005, the population of Saint John declined 6.8%. When the census was taken in May 2006 the population of Saint John was 69,684 compared with 68,103 in 2001.
Ethnicity, religion and language.
Canada's 2006 Census found that amongst the Saint John population's reported ethnic origins, 42.1% of the population described their background as Canadian, followed by English (35.6%), Irish (33.6%), Scottish (27.3%), French (22.7%), German (6.0%), Dutch (3.2%), North American Indian (3.2%), Welsh (2.0%), and many others. (Numbers add to more than 100% due to multiple responses: e.g. "English & Scottish".)
With regard to religion, 89.2% identify as Christian (47.6% Protestant, 40.3% Roman Catholic, and 1.3% other Christian, mostly Orthodox and independent churches). 10.1% state no religious affiliation, and other religions including Islam, Judaism, Buddhism, and Hinduism together comprise less than 1%.
While New Brunswick is a bilingual province, the Greater Saint John area is overwhelmingly anglophone: of its 127,761 residents in 2011, only 5,520 were native French speakers, a much lower percentage than that for the province as a whole.
Municipal government (Common Council).
Responsibility.
Saint John is governed by a body of elected officials, referred to as "Common Council" whose responsibilities include
Composition.
The Common Council consists of:
One is elected by the council to serve as Deputy Mayor.
Current Council is
Shelly Rinehart Councillor at Large and Deputy Mayor
Shirley MacAlary, Councillor at Large
Ward 1
Bill Farren
Greg Norton
Ward 2
John MacKenzie
Susan Fullerton
Ward 3
Donna Reardon
Ward 4
Ray Strowbridge
David Meritthew
In the October 9, 2007 Plebiscite, it was decided that as of the May 2008 quadrennial municipal elections, the city will be divided into four wards of approximately equal population, with two councilors to be elected by the voters in that ward, and two councilors to be elected at large.
Economy.
Politically, socially, economically, as well as geographically, the sea has shaped Saint John. The Fundy City, as the city has been called as it is the only city located on the Bay of Fundy, has a long history of shipbuilding at the city's dry dock which is one of the largest in the world. Since 2003 shipbuilding has ended on the scale it once was forcing the city to adopt a new economic strategy. The University of New Brunswick, the New Brunswick Museum and the New Brunswick Community College are important institutions along with Radian6 and Horizon Health Network and many others are a part of Saint John's fast growing Research and Information Technology sectors. As the city moves away from its industrial past it now begins to capitalize on the other new growing economies in Saint John of tourism, having over 1.5 million visitors a year and 200,000 cruise ship visitors a year, creating a renaissance in the city's historic downtown (locally known as uptown) with many small businesses moving in and large scale waterfront developments underway such as the Fundy Quay being condo, hotel, office space along with the Saint John Law Courts and Three Sisters Harbour front condos.
The Arts & Culture sector play a large role in Saint John's economy. The Imperial Theatre is home to the highly acclaimed Saint John Theatre Company, and the Symphony New Brunswick and hosts a large collection of plays, concerts and other stage production year round. Harbour Station entertainment complex is home to the Saint John Sea Dogs of the QMHL and the Saint John Millrats of the NBL.
Art galleries in Saint John cover the uptown, more than any other atlantic Canadian city. Artists like Miller Brittain and Fred Ross have made Uptown Saint John their home and now the torch has been passed to artists like Gerard Collins, Cliff Turner and Peter Salmon and their respective galleries. Uptown art galleries also include the Trinity Galleries, Citadel Gallery, Handworks Gallery and the Saint John Arts Centre (SJAC). The SJAC located in the Carnegie Building, hosts art exhibits, workshops, local song writers circles and other shows too small to be featured at the grand Imperial Theatre.
Saint John still maintains industrial infrastructure in the city's east side such as Canada's largest oil refinery. Wealthy industrialist K.C. Irving and his family built an industrial conglomerate in the city during the 20th century with interests in oil, forestry, shipbuilding, media and transportation. Irving companies remain dominant employers in the region with North America's first deepwater oil terminal, a pulp mill, a paper mill and a tissue paper plant.
Other important economic activity in the city is generated by the Port of Saint John, the Moosehead Brewery (established in 1867, is Canada's only nationally distributed independent brewery [M. Nicholson]), James Ready Brewing Co., the New Brunswick Power Corporation which operates three electrical generating stations in the region including the Point Lepreau Nuclear Generating Station, Bell Aliant which operates out of the former New Brunswick Telephone headquarters, the Horizon Health Network, which operates 5 hospitals in the Saint John area, and numerous information technology companies. There are also a number of call centres which were established in the 1990s under provincial government incentives.
View from Fort Howe of the Saint John skyline prior to Peel Plaza
Maritime activities.
Until the early first decade of the 21st century, Canada's largest shipyard (Irving Shipbuilding) had been an important employer in the city. During the 1980s-early 1990s the shipyard was responsible for building 9 of the 12 Halifax class multi-purpose patrol frigates for the Canadian Navy. However, the shipyard failing to buckle to Union pressure shut down production. The 25 year Union contract with the shipyard is due to end at the end of the 2012 year. This would allow the shipyard to operate under a new contract.
Prior to the opening of the St. Lawrence Seaway in the late 1950s, the Port of Saint John functioned as the winter port for Montreal, Quebec when shipping was unable to traverse the sea ice in the Gulf of St. Lawrence and St. Lawrence River. The Canadian Pacific Railway opened a line to Saint John from Montreal in 1889 across the state of Maine and transferred the majority of its trans-Atlantic passenger and cargo shipping to the port during the winter months. The port fell into decline following the seaway opening and the start of year-round icebreaker services in the 1960s. In 1994 CPR left Saint John when it sold the line to shortline operator New Brunswick Southern Railway. The Canadian National Railway still services Saint John with a secondary mainline from Moncton.
Military.
Besides being the location of several historical forts, such as Fort Howe, Fort Dufferin, Fort Latour, and the Carleton Martello Tower, Saint John is the location of a number of reserve units of the Canadian Forces.
Retailing.
The following malls are located in the city:
East.
"See The East Saint John Mall District"
Energy projects.
Canaport LNG.
Canaport LNG, a partnership between Irving Oil (25%) and Repsol (75%), constructed a state-of-the-art LNG receiving and regasification terminal in Saint John, New Brunswick that began operations in 2009. It is the first LNG regasification plant in Canada, sending out natural gas to both Canadian and American markets. The terminal has a send-out capacity, or the ability to distribute via pipeline, 1 billion cubic feet (28 million cubic metres) of natural gas a day after it has been regasified from its liquid state.
Brunswick Pipeline.
Emera Inc. will invest approximately $350 million, for full ownership of a proposed pipeline which will deliver natural gas from the planned Canaport(TM) Liquefied Natural Gas (LNG) import terminal near Saint John, New Brunswick to markets in Canada and the US Northeast. Brunswick Pipeline will have a diameter of 30 in and will be capable of carrying approximately 850 Mcuft per day of re-gasified LNG. Capacity can be expanded with added compression.
The 145 km pipeline would extend through southwest New Brunswick to an interconnection with the Maritimes and Northeast Pipeline at the Canada/US border near St. Stephen, New Brunswick. The National Energy Board (NEB) has issued its Environmental Assessment Report (EA Report) on the proposed Brunswick Pipeline project. The main finding of the EA Report is that the project is not likely to result in significant adverse environmental effects, provided Brunswick Pipeline meets all of its environmental commitments, and all of the NEB’s recommendations are implemented. The pipeline's construction was completed on January 31, 2009.
Transportation.
Air service into Saint John is provided by the Saint John Airport/Aéroport de Saint-Jean, located near Loch Lomond approximately 25 kilometres by road northeast of the city centre. Flights are offered by Sunwing Airlines (seasonal) and Air Canada. WestJet recently decided to withdraw from the Saint John Airport. Quebec-based PASCAN Aviation announced its expansion into Saint John in late 2012, with direct flights from Saint John to Quebec City, Newfoundland, and other destinations beginning in September 2012.
The main highway in the city is the Saint John Throughway (Route 1). Route 1 extends west to St. Stephen, and northeast towards Moncton. A second major highway, Route 7, connects Saint John with Fredericton. There are two main road crossings over the Saint John River: the Harbour Bridge and the Reversing Falls Bridge, approximately 1 nmi upstream.
The Reversing Falls Railway Bridge carries rail traffic for the New Brunswick Southern Railway on the route from Saint John to Maine. Passenger rail service in Saint John was discontinued in 1994, although the Canadian National Railway and New Brunswick Southern Railway continue to provide freight service.
Bay Ferries operates a ferry service across the Bay of Fundy to Digby, Nova Scotia. The Summerville to Millidgeville Ferry, a free propeller (as opposed to cable) ferry service operated by the New Brunswick Department of Transportation, connects the Millidgeville neighbourhood with Summerville, New Brunswick, across the Kennebecasis River on the Kingston Peninsula.
Acadian Lines used to operate regular inter-city bus services between New Brunswick, Prince Edward Island and Nova Scotia as well as Rivière-du-Loup, and Quebec (connecting with Orléans Express). In 2011, Acadian Lines cancelled bus service on the route between Saint John and Bangor, Maine due to low ticket sales. In November 2012, due to inability to agree a contract, Acadian Lines ceased operations.
Bus service is provided by Saint John Transit (locally) and Maritime Bus (regionally).
Culture.
Saint John shares much of the same cultural roots found in cities like Boston and New York. The presence of Irish heritage is very apparent along with strong maritime traditions. Saint John is a true maritime city with ties to the fisheries and shipbuilding, and is known for the Marco Polo as its flagship vessel. The city has been a traditional hub for creativity, boasting many notable artists, actors and musicians, including Walter Pidgeon, Donald Sutherland, Louis B. Mayer, Fred Ross and Miller Brittain.
Saint John has a long history of brewers, such as Simeon Jones, The Olands, and James Ready. The city is now home to Moosehead Breweries, James Ready Brewing Co., and Big Tide Brewing Co.
Dance, music, and theatre ensembles in the city include:
Saint John has several small private art galleries, as well as concert series hosted by local churches and schools. Cultural festivals and venues include:
The following museums are also located in Saint John:
National Historic Sites of Canada located in Saint John include the following:
Sports.
The following teams are based in Saint John:
The following sporting events have been held here:
Saint John is also home to Exhibition Park Raceway, a Harness Racing facility that has been hosting this form of Horse Racing for over the past 120 years. Prior to 1950 it was known as Moosepath Park.
Education.
In 1964, the University of New Brunswick created UNB Saint John. Initially located in buildings throughout the downtown CBD, in 1968 UNBSJ opened a new campus in the city's Tucker Park neighbourhood. This campus has undergone expansion over the years and is the fastest growing component of the UNB system with many new buildings constructed between the 1970s-first decade of the 21st century. A trend in recent years has been a growth in the number of international students. The city also hosts a New Brunswick Community College campus in the East End of the city. There has also been a satellite campus of Dalhousie Medical School added within the UNBSJ campus in 2010, instructing 30 medical students each year.
In the fall of 2007, a report commissioned by the provincial government recommended that UNBSJ and the NBCC be reformed and consolidated into a new polytechnic post-secondary institute. The proposal immediately came under heavy criticism and led to the organizing of several protests in the uptown area. The diminishment of UNB as a nationally accredited university, the reduction in accessibility to receive degrees, and there are only a couple of the reasons why the community was enraged by the recommendation with support slightly below 90% to keep UNBSJ as it was, and expand the university under its current structure. Seeing that too much political capital would be lost, and that several Saint John are MPs were likely not to support the initiative if the policies recommended by the report were legislated, the government abandoned the commission's report and created an intra-provincial post-secondary commission.
Saint John is served by two school boards; District 8 for Anglophone schools and District 1 (based out of Dieppe, New Brunswick) for the city's only Francophone school, Centre-Scolaire-Communautaire Samuel-de-Champlain. Saint John is also home to Canada's oldest publicly funded school, Saint John High School. The other high schools in the city are Harbour View High School, St. Malachy's High School, and Simonds High School.

</doc>
<doc id="28867" url="http://en.wikipedia.org/wiki?curid=28867" title="Sigyn">
Sigyn

In Norse mythology, Sigyn (Old Norse "victorious girl-friend") is a goddess and is the wife of Loki. Sigyn is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", little information is provided about Sigyn other than her role in assisting Loki during his captivity. In the "Prose Edda", her role in helping her husband through his time spent in bondage is stated again, she appears in various kennings, and her status as a goddess is mentioned twice. Sigyn may appear on the Gosforth Cross and has been the subject of an amount of theory and cultural references.
Attestations.
Sigyn is attested in the following works:
"Poetic Edda".
In stanza 35 of the "Poetic Edda" poem "Völuspá", a völva tells Odin that, amongst many other things, she sees Sigyn sitting very unhappily with her bound husband, Loki, under a "grove of hot springs". Sigyn is mentioned a second (and final) time in the ending prose section of the poem "Lokasenna". In the prose, Loki has been bound by the gods with the guts of his son Nari, his son Váli is described as having been turned into a wolf, and the goddess Skaði fastens a venomous snake over Loki's face, from which venom drips. Sigyn, again described as Loki's wife, holds a basin under the dripping venom. The basin grows full, and she pulls it away, during which time venom drops on Loki, causing him to writhe so violently that earthquakes occur that shake the entire earth.
"Prose Edda".
Sigyn appears in the books "Gylfaginning" and "Skáldskaparmál" in the "Prose Edda". In "Gylfaginning", Sigyn is introduced in chapter 31. There, she is introduced as the wife of Loki, and that they have a son by the name of "Nari or Narfi". Sigyn is mentioned again in "Gylfaginning" in chapter 50, where events are described differently than in "Lokasenna". Here, the gods have captured Loki and his two sons, who are stated as Váli, described as a son of Loki, and "Nari or Narfi", the latter earlier described as also a son of Sigyn. Váli is changed into a wolf by the gods, and rips apart his brother "Nari or Narfi". The guts of "Nari or Narfi" are then used to tie Loki to three stones, after which the guts turn to iron, and Skaði places a snake above Loki. Sigyn places herself beside him, where she holds out a bowl to catch the dripping venom. However, when the bowl becomes full she leaves to pour out the venom. As a result, Loki is again described as shaking so violently that the planet shakes, and this process repeats until he breaks free, setting Ragnarök into motion.
Sigyn is introduced as a goddess, an ásynja, in the "Prose Edda" book "Skáldskaparmál", where the gods are holding a grand feast for the visiting Ægir, and in kennings for Loki: "husband of Sigyn", "cargo [Loki] of incantation-fetter's [Sigyn's] arms", and in a passage quoted from the 9th-century "Haustlöng", "the burden of Sigyn's arms". The final mention of Sigyn in "Skáldskaparmál" is in the list of ásynjur in the appended Nafnaþulur section, chapter 75.
Archaeological record.
The mid-11th century Gosforth Cross located in Cumbria, England, has been interpreted as featuring various figures from Norse mythology. The bottom portion of the west side of the cross features a depiction of a long-haired female, kneeling figure holding an object above another prostrate, bound figure. Above and to their left is a knotted serpent. This has been interpreted as Sigyn soothing the bound Loki.
Theories.
While the name "Sigyn" is found as a female personal name in Old Norse sources (Old Norse "sigr" meaning 'victory' and "vina" meaning 'girl-friend'), and though in surviving sources she is largely restricted to a single role, she appears in the 9th century skaldic poem "Haustlöng" from pagan times, written by the skald Þjóðólfr of Hvinir. Due to this early connection with Loki, Sigyn has been theorized as being a goddess dating back to an older form of Germanic paganism.
Modern influence.
The scene of Sigyn assisting Loki has been depicted on a number of paintings, including "Loke och Sigyn" (1850) by Nils Blommér, "Loke och Sigyn" (1863) by Mårten Eskil Winge, "Loki och Sigyn (1879) by Oscar Wergeland, and the illustration "Loki und Sigyn; Hel mit dem Hunde Garm" (1883) by K. Ehrenberg. Various objects and places have been named after Sigyn in modern times, including the Norwegian stiff-straw winter wheat varieties "Sigyn I" and "Sigyn II", a Marvel Comics character (1978) of the same name the Swedish vessel MS Sigyn, which transports spent nuclear fuel in an allusion to Sigyn holding a bowl beneath the venom to spare Loki, and the arctic Sigyn Glacier.
References.
</dl>

</doc>
<doc id="28868" url="http://en.wikipedia.org/wiki?curid=28868" title="Saudi–Iraqi neutral zone">
Saudi–Iraqi neutral zone

The Saudi–Iraqi neutral zone was an area of 7,044 km² on the border between Saudi Arabia and Iraq within which the border between the two countries had not been settled. The neutral zone came into existence following the Uqair Protocol of 1922 which defined the border between Iraq and the Sultanate of Nejd (Saudi Arabia's predecessor state). The neutral zone officially ended in 1981, when Iraq and Saudi Arabia agreed on the partition of the zone.
History.
The Treaty of Muhammarah (Khorramshahr) of 5 May 1922, forestalled the imminent conflict between the United Kingdom, which held the mandate for Iraq, and the Kingdom of Nejd, which later became Saudi Arabia (when combined with the Kingdom of Hejaz). The treaty specifically avoided defining boundaries. Following further negotiations, the Protocol of Uqair (Uqayr), 2 December 1922, defined most of the borders between them and created the neutral zone.
No military or permanent buildings were to be built in or near the neutral zone and the nomads of both countries were to have unimpeded access to its pastures and wells.
Administrative division of the zone was achieved in 1975, and a border treaty concluded in 1981. For unknown reasons the treaty was not filed with the United Nations and nobody outside Iraq and Saudi Arabia was notified of the change or shown maps with details of the new boundary. As the Gulf War approached in early 1991, Iraq cancelled all international agreements with Saudi Arabia since 1968. Saudi Arabia responded by registering all previous boundary agreements negotiated with Iraq at the United Nations in June 1991. Thus ended the legal existence of the Saudi–Iraqi neutral zone.
Most official maps no longer show the diamond-shaped neutral zone, but rather draw the border line approximately through the centre of the territory. For example, the United States Office of the Geographer regarded the area as only having an approximate boundary, rather than a precise one.
The Saudi–Iraqi neutral zone formerly had the ISO 3166-1 codes NT and NTZ. These codes were discontinued in 1993. The FIPS 10-4 code for the Saudi–Iraqi neutral zone was IY; that code was deleted in 1992.

</doc>
<doc id="28869" url="http://en.wikipedia.org/wiki?curid=28869" title="Solidarity (Polish trade union)">
Solidarity (Polish trade union)

Solidarity (Polish: "Solidarność", pronounced ]; full name: Independent Self-governing Trade Union "Solidarity" – "Niezależny Samorządny Związek Zawodowy "Solidarność"" ]) is a Polish trade union that was founded on 17 September 1980 at the Gdańsk Shipyard under the leadership of Lech Wałęsa. It was the first trade union in a Warsaw Pact country that was not controlled by the Communist Party. Its membership reached 9.5 million members before its September 1981 Congress (when it reached 10 million), which constituted one third of the total working-age population of Poland.
In the 1980s, Solidarity was a broad anti-bureaucratic social movement, using the methods of civil resistance to advance the causes of workers' rights and social change. The government attempted to destroy the union by imposing martial law in Poland, which lasted from December 1981 to July 1983 and was followed by several years of political repression, but in the end it was forced to negotiate with Solidarity. In the union's clandestine years, the United States provided significant financial support, estimated to be as much as 50 million US dollars.
The round table talks between the government and the Solidarity-led opposition led to semi-free elections in 1989. By the end of August, a Solidarity-led coalition government was formed. In December 1990, Wałęsa was elected President of Poland. Since then Solidarity has become a more traditional, liberal trade union.
30 years after its foundation, the federation's membership dropped to 680,000 in 2010 and 400,000 in 2011.
History.
In the 1970s Poland's government raised food prices while wages stagnated. This (and other stresses) led to the June 1976 protests and subsequent government crackdown on dissent. Groups like the KOR and ROPCIO began to form underground networks to monitor and oppose the government's abusive behavior. Labor unions formed an important part of this network.
In 1979, the Polish economy shrank for the first time since World War II by 2 percent. The foreign debt reached around $18 billion by 1980.
For participation in the illegal trade union, Anna Walentynowicz was fired from work at the Gdańsk Shipyard on 7 August 1980, 5 months before she was due to retire. This management decision enraged the workers of the Shipyard, who staged a strike action on 14 August defending Anna Walentynowicz and demanding her return. Anna Waletynowicz and Alina Pienkowska transformed a strike over bread and butter issues into a solidarity strike in sympathy with other striking establishments.
Solidarity emerged on 31 August 1980 in Gdańsk at the Lenin Shipyards when the communist government of Poland signed the agreement allowing for its existence. On 17 September 1980, over 20 Inter-factory Founding Committees of free trade unions merged at the congress into one national organization NSZZ Solidarity. It officially registered on 10 November 1980.
Lech Wałęsa and others formed a broad anti-Soviet social movement ranging from people associated with the Catholic Church to members of the anti-Soviet left. Solidarity advocated non-violence in its members' activities. In September 1981 Solidarity's first national congress elected Lech Wałęsa as a president and adopted a republican program, the "Self-governing Republic". The government attempted to destroy the union with the martial law of 1981 and several years of repression, but in the end it had to start negotiating with the union.
In Poland, the Roundtable Talks between the government and Solidarity-led opposition led to semi-free elections in 1989. By the end of August a Solidarity-led coalition government was formed and in December Tadeusz Mazowiecki was elected Prime Minister. Since 1989 Solidarity has become a more traditional trade union, and had relatively little impact on the political scene of Poland in the early 1990s. A political arm founded in 1996 as Solidarity Electoral Action (AWS) won the parliamentary election in 1997, but lost the following 2001 election. Currently, as a political party "Solidarity" has little influence on modern Polish politics.
Catholic social teaching.
In "Sollicitudo rei socialis", a major document of Catholic Social Teaching, Pope John Paul II identifies the concept of solidarity with the poor and marginalized as a constitutive element of the Gospel and human participation in the common good. The Roman Catholic Church, under the leadership of Pope John Paul II, was a very powerful supporter of the union and was greatly responsible for its success. Lech Wałęsa, who himself publicly displayed Catholic piety, confirmed the Pope's influence, saying: "The Holy Father, through his meetings, demonstrated how numerous we were. He told us not to be afraid".
In addition, the priest Jerzy Popiełuszko, who regularly gave sermons to the striking workers, was eventually killed by the Communist regime for his association with Solidarity. Polish workers themselves were closely associated with the Church, which can be seen in the photographs taken during strikes in the 1980s. On the walls of several factories, portraits of the Virgin Mary or John Paul II were visible.
Influence abroad.
The survival of Solidarity was an unprecedented event not only in Poland, a satellite state of the USSR ruled (in practice) by a one-party Communist regime, but the whole of the Eastern bloc. It meant a break in the hard-line stance of the communist Polish United Workers' Party, which had bloodily ended a 1970 protest with machine gun fire (killing dozens and injuring over 1,000), and the broader Soviet communist regime in the Eastern Bloc, which had quelled both the 1956 Hungarian Uprising and the 1968 Prague Spring with Soviet-led invasions.
Solidarity's influence led to the intensification and spread of anti-communist ideals and movements throughout the countries of the Eastern Bloc, weakening their communist governments. As a result of the Round Table Agreement between the Polish government and the Solidarity-led opposition, elections were held in Poland on 4 June 1989, in which the opposition were allowed to field candidates against the Communist Party - the first free elections in any Soviet bloc country. A new upper chamber (the Senate) was created in the Polish parliament and all of its 100 seats were contestable in the election, as well as one third of the seats in the more important lower chamber (the Sejm). Solidarity won 99 of the 100 Senate seats and all 161 contestable seats in the Sejm - a victory that also triggered a chain reaction across the Soviet Union’s satellite states, leading to almost entirely peaceful anti-communist revolutions in Central and Eastern Europe known as the Revolutions of 1989 ("Jesień Ludów" or "Wiosna Obywatelów"), which ended in the overthrow of each Moscow-imposed regime, and ultimately to the dissolution of the Soviet Union in the early 1990s.
Solidarity also supported the struggles of trade unions in capitalist countries. During the UK miner's strike of 1984-85, the President of Upper Silesia Solidarity David Jastrzębski voiced his support of the striking miners - "Neither the British government’s mounted police charges nor its truncheon blows, any more than the Polish junta’s tanks or rifle fire, can break our common will to struggle for a better future for the working class." This was despite the fact that Arthur Scargill, president of the British National Union of Mineworkers had been highly critical of Solidarity and publically supported the Polish government's attempts to crush the union after the imposition of martial law in 1981.
In late 2008, several democratic opposition groups in the Russian Federation formed a Solidarity movement.
Solidarity also inspired and influenced the protests of the Arab Spring as well as the Occupy movement. Former leader Lech Wałęsa visited protestors in Tunisia in 2011.
Secular philosophical underpinnings.
Although Leszek Kołakowski's works were officially banned in Poland, underground copies of them influenced the opinions of the Polish intellectual opposition. His 1971 essay "Theses on Hope and Hopelessness", which suggested that self-organized social groups could gradually expand the spheres of civil society in a totalitarian state, helped inspire the dissident movements of the 1970s that led to the creation of Solidarity and provided a philosophical underpinning for the movement.
Organization.
The union was officially founded on 17 September 1980, the union's supreme powers were vested in a legislative body, the "Convention of Delegates" ("Zjazd Delegatów"). The executive branch was the National Coordinating Commission ("Krajowa Komisja Porozumiewawcza"), later renamed the National Commission ("Komisja Krajowa"). The Union had a regional structure, comprising 38 regions ("region") and two districts ("okręg"). At its highest, the Union had over 10 million members, which became the largest union membership in the world. During the communist era the 38 regional delegates were arrested and jailed when martial law came into effect on 13 December 1981 under General Wojciech Jaruzelski. After a one-year prison term the high-ranking members of the union were offered one way trips to any country accepting them (including Canada, the United States, and nations in the Middle East).
Solidarity was organized as an industrial union, or more specifically according to the One Big Union principle, along the lines of the Industrial Workers of the World and the Spanish Confederación Nacional del Trabajo (workers in every trade were organized by region, rather than by craft).
In 2010, Solidarity had more than 400,000 members. National Commission of Independent Self-Governing Trade Union is located in Gdańsk and is composed of Delegates from Regional General Congresses.
Regional structure.
Solidarity is divided into 37 regions, and the territorial structure to a large degree reflects the shape of Polish voivodeships, established in 1975 and annulled in 1998 (see: Administrative division of People's Republic of Poland). The regions are:
Network of key factories.
The network of Solidarity branches of the key factories of Poland was created on 14 April 1981 in Gdańsk. It was made of representatives of seventeen factories; each stood for the most important factory of every voivodeship of the pre-1975 Poland (see: Administrative division of People's Republic of Poland). However, there were two exceptions. There was no representative of the Koszalin Voivodeship, and the Katowice Voivodeship was represented by two factories:

</doc>
