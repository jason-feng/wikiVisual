<doc id="28876" url="http://en.wikipedia.org/wiki?curid=28876" title="Surtsey">
Surtsey

Surtsey ("Surtr's island" in Icelandic, pronounced ˈsʏr̥tsei) is a volcanic island located in the Vestmannaeyjar archipelago off the southern coast of Iceland. At , Surtsey is the southernmost point of Iceland. It was formed in a volcanic eruption which began 130 metres (426 ft) below sea level, and reached the surface on 14 November 1963. The eruption lasted until 5 June 1967, when the island reached its maximum size of 2.7 km2. Since then, wave erosion has caused the island to steadily diminish in size: as of 2002, its surface area was 1.4 km2. The most recent survey (2007) shows the island's maximum elevation at 155 m above sea level.
The new island was named after "Surtr", a fire "jötunn" or giant from Norse mythology. It was intensively studied by volcanologists during its eruption, and afterwards by botanists and other biologists as life forms gradually colonised the originally barren island. The undersea vents that produced Surtsey are part of the "Vestmannaeyjar" submarine volcanic system, part of the fissure of the sea floor called the Mid-Atlantic Ridge. Vestmannaeyjar also produced the famous eruption of "Eldfell" on the island of Heimaey in 1973. The eruption that created Surtsey also created a few other small islands along this volcanic chain, such as "Jólnir" and unnamed other peaks. Most of these eroded away fairly quickly.
Geology.
Formation.
The eruption was unexpected, and almost certainly began some days before it became apparent at the surface. The sea floor at the eruption site is 130 metres (426 ft) below sea level, and at this depth volcanic emissions and explosions would be suppressed, quenched and dissipated by the water pressure and density. Gradually, as repeated flows built up a mound of material that approached sea level, the explosions could no longer be contained, and activity broke the surface.
The first noticeable indications of volcanic activity were recorded at the seismic station in Kirkjubæjarklaustur, Iceland from 6 to 8 November, which detected weak tremors emanating from an epicentre approximately west-south-west at a distance of 140 km, the location of Surtsey. Another station in Reykjavík recorded even weaker tremors for ten hours on 12 November at an undetermined location, when seismic activity ceased until 21 November. That same day, people in the coastal town of Vík 80 km away noticed a smell of hydrogen sulphide. On 13 November, a fishing vessel in search of herring, equipped with sensitive thermometers, noted sea temperatures 3.2 km SW of the eruption center were 2.4°C (4.3°F) higher than surrounding waters.
Eruption at the surface.
At 07:15 UTC on 14 November 1963, the cook of "Ísleifur II", a trawler sailing these same waters spotted a rising column of dark smoke southwest of the boat. The captain thought it might have been a boat on fire, and ordered the vessel to investigate. Instead, they encountered explosive eruptions giving off black columns of ash, indicating that a volcanic eruption had begun to penetrate the surface of the sea. By 11:00 the same day, the eruption column had reached several kilometres in height. At first the eruptions took place at three separate vents along a northeast by southwest trending fissure, but by the afternoon the separate eruption columns had merged into one along the erupting fissure. Over the next week, explosions were continuous, and after just a few days the new island, formed mainly of scoria, measured over 500 metres (1640 ft) in length and had reached a height of 45 metres (147 ft).
As the eruptions continued, they became concentrated at one vent along the fissure and began to build the island into a more circular shape. By 24 November, the island measured about 900 metres by 650 metres (2950 by 2130 ft). The violent explosions caused by the meeting of lava and sea water meant that the island consisted of a loose pile of volcanic rock (scoria), which was eroded rapidly by North Atlantic storms during the winter. However, eruptions more than kept pace with wave erosion, and by February 1964, the island had a maximum diameter of over 1300 metres (4265 ft).
The explosive phreatomagmatic eruptions caused by the easy access of water to the erupting vents threw rocks up to a kilometre (0.6 mi) away from the island, and sent ash clouds as high as 10 km (6 mi) up into the atmosphere. The loose pile of unconsolidated tephra would quickly have been washed away had the supply of fresh magma dwindled, and large clouds of dust were often seen blowing away from the island during this stage of the eruption.
The new island was named after the fire jötunn Surtur from Norse mythology ("Surts" is the genitive case of "Surtur", plus -ey, "island"). Three French journalists representing the magazine "Paris Match" notably landed there on 6 December 1963, staying for about 15 minutes before violent explosions encouraged them to leave. The journalists jokingly claimed French sovereignty over the island, but Iceland quickly asserted that the new island belonged to it.
Permanent island.
By early 1964, though, the continuing eruptions had built the island to such a size that sea water could no longer easily reach the vents, and the volcanic activity became much less explosive. Instead, lava fountains and flows became the main form of activity. These resulted in a hard cap of extremely erosion-resistant rock being laid down on top of much of the loose volcanic pile, which prevented the island from being washed away rapidly. Effusive eruptions continued until 1965, by which time the island had a surface area of 2.5 km2.
On 28 December 1963 submarine activity 2.5 km (1.5 mi) to the north-east of Surtsey caused the formation of a ridge 100 m (328 ft) high on the sea floor. This seamount was named Surtla, but never reached sea level. Eruptions at Surtla ended on 6 January 1964, and it has since been eroded from its minimum depth of 23 m (75 ft) to 47 m (154 ft) below sea level.
Subsequent volcanic activity.
In 1965 the activity on the main island diminished, but at the end of May that year an eruption began at a vent 0.6 km (0.37 mi) off the northern shore. By 28 May an island had appeared, and was named Syrtlingur (Little Surtsey). The new island was washed away during early June, but reappeared on 14 June. Eruptions at Syrtlingur were much smaller in scale than those that had built Surtsey, with the average rate of emission of volcanic materials being about a tenth of the rate at the main vent. Activity was short-lived, continuing until the beginning of October 1965, by which time the islet had an area of 0.15 km2. Once the eruptions had ceased, wave erosion rapidly wore the island away, and it disappeared beneath the waves on 24 October.
During December 1965, more submarine activity occurred 0.9 km (0.56 mi) southwest of Surtsey, and another island was formed. It was named Jólnir, and over the following eight months it appeared and disappeared several times, as wave erosion and volcanic activity alternated in dominance. Activity at Jólnir was much weaker than the activity at the main vent, and even weaker than that seen at Syrtlingur, but the island eventually grew to a maximum size of 70 m (230 ft) in height, covering an area of 0.3 km2, during July and early August 1966. Like Syrtlingur, though, after activity ceased on 8 August 1966, it was rapidly eroded, and dropped below sea level during October 1966.
Effusive eruptions on the main island returned on 19 August 1966, with fresh lava flows giving it further resistance to erosion. The eruption rate diminished steadily, though, and on 5 June 1967, the eruption ended. The volcano has been dormant ever since. The total volume of lava emitted during the three-and-a-half-year eruption was about one cubic kilometre (0.24 cu mi), and the island's highest point was 174 metres (570 ft) above sea level at that time.
Since the end of the eruption, erosion has seen the island diminish in size. A large area on the southeast side has been eroded away completely, while a sand spit called "Norðurtangi" (north point) has grown on the north side of the island. It is estimated that about 0.024 km3 of material has been lost due to erosion—this represents about a quarter of the original above-sea-level volume of the island. Its maximum elevation has diminished to 155 m.
Recent development.
Following the end of the eruption, scientists established a grid of benchmarks against which they measured the change in the shape of the island. In the 20 years following the end of the eruption, measurements revealed that the island was steadily slumping vertically and had lost about one metre in height. The rate of slumping was initially about 20 cm (8 in) per year but slowed to 1–2 cm (0.4–0.8 in) a year by the 1990s. It had several causes: settling of the loose tephra forming the bulk of the volcano, compaction of sea floor sediments underlying the island, and downward warping of the lithosphere due to the weight of the volcano.
The typical pattern of volcanism in the Vestmannaeyjar archipelago is for each eruption site to see just a single eruption, and so the island is unlikely to be enlarged in the future by further eruptions. The heavy seas around the island have been eroding it ever since the island appeared, and since the end of the eruption almost half its original area has been lost. The island currently loses about 1.0 ha of its surface area each year.
Future.
This island is unlikely to disappear entirely in the near future. The eroded area consisted mostly of loose tephra, easily washed away. Most of the remaining area is capped by hard lava flows, which are much more resistant to erosion. In addition, complex chemical reactions within the loose tephra within the island have gradually formed highly erosion resistant tuff material, in a process known as palagonitization. On Surtsey this process has happened quite rapidly, due to high temperatures not far below the surface.
Estimates of how long Surtsey will survive are based on the rate of erosion seen up to the present day. Assuming that the current rate does not change, the island will be mostly at or below sea level by 2100. However, the rate of erosion is likely to slow as the tougher core of the island is exposed: an assessment assuming that the rate of erosion will slow exponentially suggests that the island will survive for many centuries. An idea of what it will look like in the future is given by the other small islands in the Vestmannaeyjar archipelago, which formed in the same way as Surtsey several thousand years ago, and have eroded away substantially since they were formed.
Biology.
Settlement of life.
A classic site for the study of biocolonisation from founder populations that arrive from outside ("allochthonous"), Surtsey was declared a nature reserve in 1965 while the eruption was still in active progress. Today only a few scientists are permitted to land on Surtsey; the only way anyone else can see it closely is from a small plane. This allows the natural ecological succession for the island to proceed without outside interference. In 2008, UNESCO declared the island a World Heritage Site, in recognition of its great scientific value.
Plant life.
In the spring of 1965 first vascular plant was found growing on the northern shore of Surtsey, mosses became visible in 1967 and lichens were first found on the Surtsey lava in 1970. Plant colonisation on Surtsey has been closely studied, the vascular plants in particular as they have been of far greater significance than mosses, lichens and fungi in the development of vegetation.
Mosses and lichens now cover much of the island. During the island's first 20 years, 20 species of plants were observed at one time or another, but only 10 became established in the nutrient-poor sandy soil.
As birds began nesting on the island, soil conditions improved, and more vascular plant species were able to survive. In 1998, the first bush was found on the island—a tea-leaved willow ("Salix phylicifolia"), which can grow to heights of up to 4 metres (13 ft). By 2008, 69 species of plant had been found on Surtsey, of which about 30 had become established. This compares to the approximately 490 species found on mainland Iceland. More species continue to arrive, at a typical rate of roughly 2–5 new species per year.
Birds.
The expansion of bird life on the island has both relied on and helped to advance the spread of plant life. Birds use plants for nesting material, but also assist in the spreading of seeds, and fertilize the soil with their guano. Birds began nesting on Surtsey three years after the eruptions ended, with fulmar and guillemot the first species to set up home. Twelve species are now regularly found on the island.
A gull colony has been present since 1984, although gulls were seen briefly on the shores of the new island only weeks after it first appeared. The gull colony has been particularly important in developing the plant life on Surtsey, and the gulls have had much more of an impact on plant colonisation than other breeding species due to their abundance. An expedition in 2004 found the first evidence of nesting Atlantic puffins, which are extremely common in the rest of the archipelago.
As well as providing a home for some species of birds, Surtsey has also been used as a stopping-off point for migrating birds, particularly those en-route between Europe and Iceland. Species that have been seen briefly on the island include whooper swans, various species of geese, and common ravens. Although Surtsey lies to the west of the main migration routes to Iceland, it has become a more common stopping point as its vegetation has improved. In 2008, the 14th bird species was detected with the discovery of a common raven's nest.
According to a 30 May 2009 report, a golden plover was nesting on the island with four eggs.
Marine life.
Soon after the island's formation, seals were seen around the island. They soon began basking there, particularly on the northern spit, which grew as the waves eroded the island. Seals were found to be breeding on the island in 1983, and a group of up to 70 made the island their breeding spot. Grey seals are more common on the island than harbour seals, but both are now well established. The presence of seals attracts orcas, which are frequently seen in the waters around the Vestmannaeyjar archipelago and now frequent the waters around Surtsey.
On the submarine portion of the island, many marine species are found. Starfish are abundant, as are sea urchins and limpets. The rocks are covered in algae, and seaweed covers much of the submarine slopes of the volcano, with its densest cover between 10 and 20 metres (33 to 66 ft) below sea level.
Other life.
Insects arrived on Surtsey soon after its formation, and were first detected in 1964. The original arrivals were flying insects, carried to the island by winds and their own power. Some were believed to have been blown across from as far away as mainland Europe. Later insect life arrived on floating driftwood, and both live animals and carcasses washed up on the island. When a large, grass-covered tussock was washed ashore in 1974, scientists took half of it for analysis and discovered 663 land invertebrates, mostly mites and springtails, the great majority of which had survived the crossing.
The establishment of insect life provided some food for birds, and birds in turn helped many species to become established on the island. The bodies of dead birds provide sustenance for carnivorous insects, while the fertilisation of the soil and resulting promotion of plant life provides a viable habitat for herbivorous insects.
Some higher forms of land life are now colonising the soil of Surtsey. The first earthworm was found in a soil sample in 1993, probably carried over from Heimaey by a bird. Slugs were found in 1998, and appeared to be similar to varieties found in the southern Icelandic mainland. Spiders and beetles have also become established.
Human impact.
The only other significant human impact is a small prefabricated hut which is used by researchers while staying on the island. The hut includes a few bunk beds and a solar power source to drive an emergency radio and other key electronics. All visitors check themselves and belongings to ensure no seeds are accidentally introduced by humans to this ecosystem. It is believed that some young boys tried to introduce potatoes, which were promptly dug up once discovered. An improperly handled human defecation resulted in a tomato plant taking root which was also destroyed.
In 2009 a weather station for weather observations and a webcam were installed on Surtsey.

</doc>
<doc id="28878" url="http://en.wikipedia.org/wiki?curid=28878" title="Software Engineering Body of Knowledge">
Software Engineering Body of Knowledge

The Software Engineering Body of Knowledge (SWEBOK) is an international standard ISO/IEC TR 19759:2005 specifying a guide to the generally accepted Software Engineering Body of Knowledge.
The Guide to the Software Engineering Body of Knowledge (SWEBOK Guide) has been created through cooperation among several professional bodies and members of industry and is published by the IEEE Computer Society (IEEE). The standard can be accessed freely from the IEEE Computer Society. In late 2013, SWEBOK V3 was approved for publication and released.
SWEBOK Version 3.
The published version of SWEBOK V3 has the following 15 knowledge areas (KAs) within the field of software engineering:
It also recognized, but did not define, these related disciplines:
2004 Edition of the SWEBOK.
The 2004 edition of the SWEBOK guide defined ten knowledge areas (KAs) within the field of software engineering:
The SWEBOK also defines disciplines related to software engineering:
Similar Efforts.
A similar effort to define a body of knowledge for software engineering is the "Computing Curriculum Software Engineering (CCSE)," officially named Software Engineering 2004 (SE2004). The curriculum largely overlaps with the 2004 SWEBOK V2 because the SWEBOK has been used as one of its sources; however, it is more directed towards academia. Whereas the SWEBOK Guide defines the software engineering knowledge that practitioners should have after four years of practice, SE2004 defines the knowledge that an undergraduate software engineering student should possess upon graduation (including knowledge of mathematics, general engineering principles, and other related areas). SWEBOK V3 aims to address these intersections.

</doc>
<doc id="28884" url="http://en.wikipedia.org/wiki?curid=28884" title="SOE">
SOE

SOE may refer to:

</doc>
<doc id="28887" url="http://en.wikipedia.org/wiki?curid=28887" title="Scottish National Party">
Scottish National Party

The Scottish National Party (SNP; Scottish Gaelic: "Pàrtaidh Nàiseanta na h-Alba", Scots: "Scots Naitional Pairtie") is a Scottish nationalist and social-democratic political party in Scotland. The SNP supports and campaigns for Scottish independence. It is the third-largest political party by membership in the United Kingdom, as well as by overall representation in the House of Commons, behind the Labour Party and the Conservative Party, and is the largest party by far in Scotland itself, where it dominates both the national government and the country's parliamentary delegation to Westminster. Its leader, Nicola Sturgeon, is the current First Minister of Scotland.
Founded in 1934 with the merger of the National Party of Scotland and the Scottish Party, the party has had continuous parliamentary representation since Winnie Ewing won the 1967 Hamilton by-election. With the advent of the Scottish Parliament in 1999, the SNP became the second largest party, serving two terms as the opposition. The SNP came to power in the 2007 Scottish general election, forming a minority government, before going on to win the 2011 election, after which it formed Scotland's first majority government.
As of 2015, the SNP is the largest political party in Scotland in terms of membership. With over 115,000 members, including 64 MSPs, 56 MPs and over 400 local councillors, more than 2% of the Scottish gross population is a member of the party. The SNP also currently has 2 MEPs in the European Parliament, who sit in The Greens/European Free Alliance (Greens/EFA) group. The SNP is a member of the European Free Alliance (EFA).
History.
The SNP was formed in 1934 through the merger of the National Party of Scotland and the Scottish Party, with Robert Bontine Cunninghame Graham as its first president. Professor Douglas Young, who was the leader of the Scottish National Party from 1942 to 1945 campaigned for the Scottish people to refuse conscription and his activities were popularly vilified as undermining the British war effort against the Axis powers. Young was imprisoned for refusing to be conscripted.
The SNP first won a parliamentary seat at the Motherwell by-election in 1945, but Robert McIntyre MP lost the seat at the general election three months later. They next won a seat in 1967, when Winnie Ewing was the surprise winner of a by-election in the previously safe Labour seat of Hamilton. This brought the SNP to national prominence, leading to the establishment of the Kilbrandon Commission.
The high point in a UK general election was when the SNP polled almost a third of all votes in Scotland at the October 1974 general election and returned 11 MPs to Westminster, the most MPs it had until the 2015 general election. However, the party experienced a large drop in its support at the 1979 General election, followed by a further drop at the 1983 election.
In the 2007 Scottish Parliamentary general election the SNP emerged as the largest party with 47 seats, narrowly ousting the Scottish Labour Party with 46 seats and Alex Salmond became Scottish First Minister. The Scottish Green Party supported Salmond's election as First Minister, and his subsequent appointments of ministers, in return for early tabling of the climate change bill and the SNP nominating a Green MSP to chair a parliamentary committee.
In May 2011, the SNP won an overall majority in the Scottish Parliament with 69 seats. Overall majorities are unusual in the Additional Member system that is used for elections to the Scottish Parliament, which was specially designed by the Labour UK government in 1999 to prevent any party gaining overall control of the parliament.
Although the SNP delivered on its promise to hold a referendum on Scottish independence in 2014, the "No" vote prevailed in a close-fought campaign, prompting the resignation of First Minister Alex Salmond. Forty-five percent of Scottish voters cast their ballots for independence in the high-turnout race, although the "Yes" side received less support than pollsters predicted.
The SNP rebounded from the loss in the independence referendum at the UK general election in May 2015, led by Salmond's successor as first minister, Nicola Sturgeon. The party went from holding six seats in the House of Commons to 56, mostly at the expense of the Labour Party. All but three of the fifty nine constituencies in the country elected an SNP candidate. The BBC News described the historic result as a "Scots landslide".
Constitution and structure.
The primary level of organisation in the SNP are the local Branches. All of the Branches within each Scottish Parliament constituency form a Constituency Association, which coordinates the work of the Branches within the constituency, coordinates the activities of the party in the constituency, and acts as a point of liaison between an MSP or MP and the party. Constituency Associations are composed of delegates from all of the Branches within the constituency.
The annual National Conference is the supreme governing body of the SNP, and is responsible for determining party policy and electing the National Executive Committee. The National Conference is composed of:
The National Council serves as the SNP’s governing body between National Conferences, and its decisions are binding, unless rescinded or modified by the National Conference. There are also regular meetings of the National Assembly, which provides a forum for detailed discussion of party policy by party members.
The party has an active youth wing, the Young Scots for Independence, as well as a student wing, the Federation of Student Nationalists. There is also an SNP Trade Union Group. There is an independently-owned monthly newspaper, "The Scots Independent", which is highly supportive of the party.
The SNP's leadership is vested in its National Executive Committee (NEC), which is made up of the party's elected office bearers and six elected members (voted for at conference). The SNP parliamentarians (Scottish, Westminster and European) and councillors have representation on the NEC, as do the Trade Union Group, the youth wing and the student wing.
Membership.
Since 18 September 2014 (the day of the Scottish independence referendum) party membership has more than quadrupled (from 25,642), surpassing the Liberal Democrats to become the third largest political party in the United Kingdom in terms of membership. On 22 November 2014 an announcement was made by Nicola Sturgeon that the party's membership as of that afternoon stood at 92,187. As of March 2015, the Party had well exceeded the 100,000 membership mark.
According to accounts filed with the Electoral Commission for the year ending 2012, the party had a total income of £2,300,459 and a total expenditure of about £2,656,059.
European affiliation.
The SNP retains close links with Plaid Cymru, its counterpart in Wales. MPs from both parties co-operate closely with each other and work as a single parliamentary group within the House of Commons. The SNP and Plaid Cymru were involved in joint campaigning during the 2005 General Election campaign. Both the SNP and Plaid Cymru, along with Mebyon Kernow from Cornwall, are members of the European Free Alliance (EFA), a European political party comprising regionalist political parties. The EFA co-operates with the larger European Green Party to form The Greens–European Free Alliance (Greens/EFA) group in the European Parliament.
Prior to its affiliation with The Greens–European Free Alliance, the SNP had previously been allied with the European Progressive Democrats (1979-1984), Rainbow Group (1989–1994) and European Radical Alliance (1994–1999).
Party ideology.
The SNP's policy base is mostly in the mainstream European social democratic tradition. Among its policies are commitments to same-sex marriage, reducing the voting age to 16, unilateral nuclear disarmament, progressive personal taxation, the eradication of poverty, the building of affordable social housing, government subsidized higher education, opposition to the building of new nuclear power plants, investment in renewable energy, the abolition of Air Passenger Duty, and a pay increase for nurses.
The Scottish National Party did not have a clear ideological position until the 1970s, when it sought to explicitly present itself as a social democratic party in terms of party policy and publicity. During the period from its foundation until the 1960s, the SNP was essentially a moderate centrist party. Debate within the party focused more on the SNP being distinct as an all-Scotland national movement, with it being neither of the left or the right, but constituting a new politics that sought to put Scotland first.
The SNP was formed through the merger of the centre-left National Party of Scotland (NPS) and the centre-right Scottish Party. The SNP’s founders were united over self-determination in principle, though not its exact nature, or the best strategic means to achieve self-government. From the mid-1940s onwards, SNP policy was radical and redistributionist in relation to land and in favour of ‘the diffusion of economic power’, including the decentralization of industries such as coal to include the involvement of local authorities and regional planning bodies to control industrial structure and development. Party policies supported the economic and social policy status quo of the post-war welfare state.
By the 1960s, the SNP was starting to become defined ideologically, with a social democratic tradition emerging as the party grew in urban, industrial Scotland, and its membership experienced an influx of social democrats from the Labour Party, the trade unions and the Campaign for Nuclear Disarmament. The emergence of Billy Wolfe as a leading figure in the SNP also contributed to this movement to the left. By this period, the Labour Party were also the dominant party in Scotland, in terms of electoral support and representation. Targeting Labour through emphasising left-of-centre policies and values was therefore electorally logical for the SNP, as well as tying in with the ideological preferences of many new party members. In 1961 the SNP conference expressed the party’s opposition to the siting of the US Polaris submarine base at the Holy Loch. This policy was followed in 1963 by a motion opposed to nuclear weapons: a policy that has remained in place ever since. The 1964 policy document, "SNP & You", contained a clear centre-left policy platform, including commitments to full employment, government intervention in fuel, power and transport, a state bank to guide economic development, encouragement of cooperatives and credit unions, extensive building of council houses by central and local government, pensions adjusted to cost of living, a minimum wage and an improved national health service.
The ’60s also saw the beginnings of the SNP’s efforts to establish an industrial organisation and mobilise amongst trade unionists in Scotland, with the establishment of the SNP Trade Union Group, and identifying the SNP with industrial campaigns, such as the Upper-Clyde Shipbuilders Work-in and the attempt of the workers at the Scottish "Daily Express" to run as a cooperative. For the party manifestos for the two 1974 general elections, the SNP finally self-identified as a social democratic party, and proposed a range of social democratic policies. There was also an unsuccessful proposal at the 1975 party conference to rename the party as the "Scottish National Party (Social Democrats)".
There were further ideological and internal struggles after 1979 with the 79 Group attempting to move the SNP further to the left, away from being what could be described a "social-democratic" party, to an expressly "socialist" party. Members of the 79 Group - including future party leader and First Minister Alex Salmond - were expelled from the party. This produced a response in the shape of the Campaign for Nationalism in Scotland from those who wanted the SNP to remain a "broad church", apart from arguments of left vs. right. The 1980s saw the SNP further define itself as a party of the political left, such as campaigning against the poll tax.
Ideological tensions inside the SNP are further complicated by arguments between the so-called SNP gradualists and SNP fundamentalists. In essence, gradualists seek to advance Scotland to independence through further devolution, in a "step-by-step" strategy. They tend to be in the moderate left grouping, though much of the 79 Group was gradualist in approach. However, this 79 Group gradualism was as much a reaction against the fundamentalists of the day, many of whom believed the SNP should not take a clear left or right position. In its economic and welfare state policies, the party has in recent years adopted a markedly feminist profile, influenced by thinkers such as Ailsa McKay.
The SNP is also a Pro-European party, in which it would like to see an Independent Scotland as a member of the European Union. The SNP also opposes a referendum on Britain's membership of the European Union.
Elected representatives (current).
Councillors.
The SNP had 425 councillors in Local Government elected from the Scottish local elections, 2012.

</doc>
<doc id="28889" url="http://en.wikipedia.org/wiki?curid=28889" title="Scotch-Irish">
Scotch-Irish

Scotch-Irish or Scots-Irish may refer to;

</doc>
<doc id="28891" url="http://en.wikipedia.org/wiki?curid=28891" title="Snowy Mountains">
Snowy Mountains

The Snowy Mountains, known informally as "The Snowies", is an IBRA subregion and the highest mountain range on the island country/continent of Australia. It contains the Australian mainland's highest mountain, Mount Kosciuszko, which reaches to a height of 2228 m above sea level. The range also contains the five highest peaks on the Australian mainland (including Mount Kosciuszko), all of which are above 2100 m. They are located in southern New South Wales and are part of the larger Australian Alps and Great Dividing Range. Unusual for Australia, the mountain range experiences large natural snowfalls every winter. Snow normally falls the most during June, July and early August. Therefore, most of the snow usually melts by late spring. The Tasmanian highlands makes up the other (major) alpine region present in Australia.
The range is host to the mountain plum-pine, a low-lying type of conifer that is suspected of being the world's oldest living plant. It is considered to be one of the centres of the Australian ski industry during the winter months, with all four snow resorts in New South Wales being located in the region.
The Alpine Way and Snowy Mountains Highway are the major roads through the Snowy Mountains region.
History.
The mountain range is thought to have had Aboriginal occupation for 20,000 years. Large scale intertribal gatherings were held in the High Country during summer for collective feasting on the Bogong moth. This practice continued until around 1865.
The area was first explored by Europeans in 1835, and in 1840, Edmund Strzelecki ascended Mount Kosciuszko and named it after a Polish patriot. High country stockmen followed who used the Snowy Mountains for grazing during the summer months. Banjo Paterson's famous poem The Man From Snowy River recalls this era. The cattle graziers have left a legacy of mountain huts scattered across the area. Today these huts are maintained by the National Parks and Wildlife Service or volunteer organisations like the.
In the 19th century gold was mined on the high plains near Kiandra. At its height this community had a population of about 4,000 people, and ran 14 hotels. Since the last resident left in 1974, Kiandra has become a ghost town of ruins and abandoned diggings.
The Kosciuszko National Park came into existence as the National Chase Snowy Mountains on 5 December 1906. In 1944 this became the Kosciuszko State Park, and then the Kosciuszko National Park in 1967.
Recreational skiing began at Kiandra in the 1860s and experienced a boom in the 20th century following the commencement of the construction of the Snowy Mountains Hydro-Electric Scheme between 1949 and 1976 which brought many European workers to the district and opened up access to the ranges.
Skiing.
The discovery of gold at Kiandra (elevation 1400 m), in 1859, briefly enticed a population of thousands above the snowline and saw the introduction of recreational skiing to the Snowy Mountains around 1861. The Kiandra Goldrush was short-lived, but the township remained a service centre for recreational and survival skiing for over a century. Australia's first T-Bar was installed at Kiandra in 1957, but the ski facilities were finally shifted up the hill to Selwyn Snowfields in 1978. Steeper slopes and more reliable snows lie further to the south and in the 20th Century, the focus of recreational skiing in New South Wales shifted southward, to the Mount Kosciuszko region.
The first Kosciuszko Chalet was built at Charlotte Pass in 1930, giving relatively comfortable access to Australia's highest terrain. In 1964, Australia briefly boasted the "World's Longest Chairlift", designed to carry skiers from the Thredbo Valley to Charlotte Pass, but technical difficulties soon closed the facility. At 1760 m, Charlotte Pass has the highest village base elevation of any Australia ski resort and can only be accessed via over-snow transport in winter. The growing number of ski enthusiasts heading to Charlotte Pass led to the establishment of a cafe at Smiggin Holes around 1939, where horse-drawn sleighs would deliver skiers to be begin the arduous ski to the Kosciusko Chalet. It was the construction of the vast Snowy Mountains Hydro-Electric Scheme from 1949 that really opened up the Snowy Mountains for large scale development of a ski industry and led to the establishment of Thredbo and Perisher as leading Australian resorts. The Construction of Guthega Dam brought skiers to the isolated Guthega district and a rope tow was installed there in 1957.
Skifields up by Kosciusko's side were also established during this period, though their existence is now little realised. The Australian Alpine Club was founded in 1950 by Charles Anton. Huts were constructed in the "Backcountry" close to Mount Kosciusko, including Kunama Hut, which opened for the 1953 season. A rope tow was installed on Mount Northcote at the site and opened in 1954. The site proved excellent for speed skiing, but the hut was destroyed in an avalanche, which also killed one person, in 1956.
Anton also recognised the potential of the Thredbo Valley for construction of a major resort and village, with good vertical terrain. Construction began in 1957. Today, Thredbo has 14 ski-lifts and possesses Australia's longest ski resort run, the 5.9 km from Karel's T-Bar to Friday Flat; Australia's greatest vertical drop of 672m; and the highest lifted point in Australia at 2037 m.
The last establishment of a major skifield in NSW came with the development of Mount Blue Cow in the 1980s. In 1987 the Swiss designed Skitube Alpine Railway opened to deliver skiers from Bullocks Flat, on the Alpine Way, to Perisher Valley and to Blue Cow, which also opened in 1987. The operators of Blue Cow purchased Guthega in 1991, and the new combined resort later merged with Perisher-Smiggins to become the largest ski resort in the Southern Hemisphere. In 2009 Perisher had 48 lifts covering 1,245 hectares and four village base areas: Perisher Valley, Blue Cow, Smiggin Holes and Guthega.
Snowy Mountains Scheme.
The Snowy Mountains also feed the Murrumbidgee and Murray rivers from the Tooma River, Whites River and Yarrangobilly River. The range is perhaps best known for the Snowy Mountains Scheme, a project to dam the Snowy River, providing both water for irrigation and hydroelectricity.
The project began in 1949 employing 100,000 men, two-thirds of whom came from thirty other countries during the post-World War II years. Socially this project symbolises a period during which Australia became an ethnic "melting pot" of the twentieth century but which also changed Australia's character and increased its appreciation for a wide range of cultural diversity.
The Scheme built several temporary towns for its construction workers, several of which have become permanent: Cabramurra (the highest town in Australia); and Khancoban. Additionally, the economy of Cooma has been sustained by the Scheme. Townships at Adaminaby, Jindabyne and Talbingo were inundated by the construction of Lakes Eucumbene, Jindabyne and Talbingo. Improved vehicular access to the High Country enabled ski-resort villages to be constructed at Thredbo and Guthega in the 1950s by ex-Snowy Scheme workers who realised the potential for expansion of the Australian ski industry.
By 1974, 145 km of underground tunnels and 80 km of aqueducts connected the sixteen dams, seven power stations (two underground), and one pumping station. The American Society of Civil Engineers has rated the Snowy Scheme as "a world-class civil engineering project".
The principal lakes created by the scheme include: Lake Eucumbene, Blowering Dam, Talbingo Dam, Lake Jindabyne and Tantangara Dam.
Geography.
Climate.
The higher regions of the park experience an alpine climate which is unusual on mainland Australia. However, only the peaks of the main range are subject to consistent heavy winter snow. The climate station at Charlotte Pass recorded Australia's lowest temperature of -23.0 °C on 28 June 1994.
Glacial lakes.
Part of the mountains known as Main Range contains mainland Australia's five glacial lakes. The largest of these lakes is Blue Lake, one of the headwaters of the Snowy River. The other four glacial lakes are Lake Albina, Lake Cootapatamba, Club Lake and Headley Tarn.
During the last ice age, which peaked about 20,000 years ago in the Pleistocene epoch, the highest peaks of the main range near Mount Kosciuszko experienced a climate which favoured the formation of glaciers, evidence of which can still be seen today. Cirques moraines, tarn lakes, roche moutonnées and other glacial features can all be seen in the area. Lake Cootapatamba, which was formed by an ice spilling from Mount Kosciuszko's southern flank, is the highest lake on the Australian mainland. Lake Albina, Club Lake, Blue Lake, and Hedley Tarn also have glacial origins.
There is some disagreement as to exactly how widespread Pleistocene glaciation was on the main range, and little or no evidence from earlier glacial periods exists. The 'David Moraine', a one kilometre long ridge running across Spencers Creek valley seems to indicate a larger glacier existed in this area at some time, however the glacial origin of this feature is disputed.
There is evidence of periglacial activity in the area. Solifluction appears to have created terraces on the north west flank of Mount Northcote. Frost heave is also a significant agent of soil erosion in the Kosciuszko Area.
Ecology.
The Snowy Mountains cover a variety of climatic regions which support several distinct ecosystems. The alpine area above the tree line is one of the most fragile and covers the smallest area. This area is a patchwork of alpine heaths, herbfields, feldmarks, bogs and fens. The windswept feldmark ecotope is endemic to the alpine region, and covers a mere 300000 m2. It is most vulnerable to the wandering footsteps of unmindful tourists.
Fauna.
Many rare or threatened plant and animal species occur within the Snowy Mountains. The Kosciuszko National Park is home to one of Australia's most threatened species (the Corroboree frog). The endangered Mountain Pygmy Possum and the more common Dusky Antechinus are located in the high country of the park.
By 2008, wild horse numbers in the National Park had reached 1,700 with that figure growing by 300 each year, resulting in park authorities coordinating their culling and relocation.
Flora.
The high country is dominated by alpine woodlands, characterised by the Snow Gum. Montane and wet sclerophyll forests also occur across the ranges, supporting large stands of Alpine Ash and Mountain Gum. In the southern Byadbo wilderness area, dry sclerophyll and wattle forests predominate. Amongst the many different native trees in the park, the large Chinese Elm has become naturalised.
The bushfires in 2003 severely damaged tree cover in the region. Fires are a natural feature of the park's ecosystem, but it will take some time for the region to return to its pre 2003 condition.

</doc>
<doc id="28892" url="http://en.wikipedia.org/wiki?curid=28892" title="Skara Brae">
Skara Brae

Skara Brae is a stone-built Neolithic settlement, located on the Bay of Skaill on the west coast of Mainland, the largest island in the Orkney archipelago of Scotland. It consists of eight clustered houses, and was occupied from roughly 3180 BCE–2500 BCE. Europe's most complete Neolithic village, Skara Brae gained UNESCO World Heritage Site status as one of four sites making up "The Heart of Neolithic Orkney."a Older than Stonehenge and the Great Pyramids, it has been called the "Scottish Pompeii" because of its excellent preservation.
Discovery and early exploration.
In the winter of 1850, a severe storm hit Scotland causing widespread damage and over 200 deaths. In the Bay of Skaill, the storm stripped the earth from a large irregular knoll, known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after uncovering four houses the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artifacts. In 1924 another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated. The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.
Neolithic lifestyle.
Skara Brae's people were makers and users of grooved ware, a distinctive style of pottery that appeared in northern Scotland not long before the establishment of the village. The houses used earth sheltering, being sunk into the ground. In fact, they were sunk into mounds of pre-existing prehistoric domestic waste known as middens. The midden provided the houses with a small degree of stability and also acted as insulation against Orkney's harsh winter climate. On average, the houses measure 40 m2 in size with a large square room containing a stone hearth used for heating and cooking. Given the number of homes, it seems likely that no more than fifty people lived in Skara Brae at any given time.
It is by no means clear what material the inhabitants burned in their hearths. Gordon Childe was sure that the fuel was peat, but a detailed analysis of vegetation patterns and trends suggests that climatic conditions conducive to the development of thick beds of peat did not develop in this part of Orkney until after Skara Brae was abandoned. Other possible fuels include driftwood and animal dung. There's evidence that dried seaweed may have been used significantly. At a number of sites in Orkney investigators have found a glassy, slag-like material called "Kelp" or "Cramp" that may be residual burnt seaweed.
The dwellings contain a number of stone-built pieces of furniture, including cupboards, dressers, seats, and storage boxes. Each dwelling was entered through a low doorway that had a stone slab door that could be closed "by a bar that slid in bar-holes cut in the stone door jambs". A sophisticated drainage system was incorporated into the village's design. It included a primitive form of toilet in each dwelling. 
Seven of the houses have similar furniture, with the beds and dresser in the same places in each house. The dresser stands against the wall opposite the door, and was the first thing seen by anyone entering the dwelling. Each of these houses had the larger bed on the right side of the doorway and the smaller on the left. Lloyd Laing noted that this pattern accorded with Hebridean custom up to the early 20th century suggesting that the husband's bed was the larger and the wife's was the smaller. The discovery of beads and paint-pots in some of the smaller beds may support this interpretation. (Central Asian yurt dwellings have an identical internal spatial gender assignment, central fire and storage chest opposite the entrance.) Additional support may come from the recognition that stone boxes lie to the left of most doorways, forcing the person entering the house to turn to the right-hand, 'male', side of the dwelling. At the front of each bed lie the stumps of stone pillars that may have supported a canopy of fur; another link with recent Hebridean style.
One house, called House 8, has no storage boxes or dresser. It has been divided into something resembling small cubicles. When this house was excavated, fragments of stone, bone and antler were found. It is possible that this building was used as a house to make simple tools such as bone needles or flint axes. The presence of heat-damaged volcanic rocks and what appears to be a flue, support this interpretation. House 8 is distinctive in other ways as well. It is a stand-alone structure not surrounded by midden, instead it is above ground and has walls over 2 m thick. It has a "porch" protecting the entrance.
The site provided the earliest known record of the human flea "Pulex irritans" in Europe.
The Grooved Ware People who built Skara Brae were primarily pastoralists who raised cattle and sheep. Childe originally believed that the inhabitants did not practice agriculture, but excavations in 1972 unearthed seed grains from a midden suggesting that barley was cultivated. Fish bones and shells are common in the middens indicating that dwellers ate seafood. Limpet shells are common and may have been fish-bait that was kept in stone boxes in the homes. The boxes were formed from thin slabs with joints carefully sealed with clay to render them waterproof.
This pastoral lifestyle is in sharp contrast to some of the more exotic interpretations of the culture of the Skara Brae people. Euan MacKie suggested that Skara Brae might be the home of a privileged theocratic class of wise men who engaged in astronomical and magical ceremonies at nearby Ring of Brodgar and the Standing Stones of Stenness. Graham and Anna Ritchie cast doubt on this interpretation noting that there is no archaeological evidence for this claim, although a Neolithic "low road" that goes from Skara Brae passes near both these sites and ends at the magnificent chambered tomb of Maeshowe, . Low roads connect Neolithic ceremonial sites throughout Britain. 
Dating and abandonment.
Originally, Childe believed that the settlement dated from around 500 BCE. This interpretation was coming under increasing challenge by the time new excavations in 1972–73 settled the question. Radiocarbon results obtained from samples collected during these excavations indicate that occupation of Skara Brae began about 3180 BCE with occupation continuing for about six hundred years. Around 2500 BCE, after the climate changed, becoming much colder and wetter, the settlement may have been abandoned by its inhabitants. There are many theories as to why the people of Skara Brae left; particularly popular interpretations involve a major storm. Evan Hadingham combined evidence from found objects with the storm scenario to imagine a dramatic end to the settlement:
As was the case at Pompeii, the inhabitants seem to have been taken by surprise and fled in haste, for many of their prized possessions, such as necklaces made from animal teeth and bone, or pins of walrus ivory, were left behind. The remains of choice meat joints were discovered in some of the beds, presumably forming part of the villagers' last supper. One woman was in such haste that her necklace broke as she squeezed through the narrow doorway of her home, scattering a stream of beads along the passageway outside as she fled the encroaching sand.
Anna Ritchie strongly disagrees with catastrophic interpretations of the village's abandonment:
A popular myth would have the village abandoned during a massive storm that threatened to bury it in sand instantly, but the truth is that its burial was gradual and that it had already been abandoned — for what reason, no one can tell.
The site was farther from the sea than it is today, and it is possible that Skara Brae was built adjacent to a freshwater lagoon protected by dunes. Although the visible buildings give an impression of an organic whole, it is certain that an unknown quantity of additional structures had already been lost to sea erosion before the site's rediscovery and subsequent protection by a seawall. Uncovered remains are known to exist immediately adjacent to the ancient monument in areas presently covered by fields, and others, of uncertain date, can be seen eroding out of the cliff edge a little to the south of the enclosed area.
Artifacts.
A number of enigmatic Carved Stone Balls have been found at the site and some are on display in the museum. Similar objects have been found throughout northern Scotland. The spiral ornamentation on some of these "balls" has been stylistically linked to objects found in the Boyne Valley in Ireland. Similar symbols have been found carved into stone lintels and bed posts. These symbols, sometimes referred to as "runic writings", have been subjected to controversial translations. For example, Castleden suggested that "colons" found punctuating vertical and diagonal symbols may represent separations between words.
Lumps of red ochre found here and at other Neolithic sites have been interpreted as evidence that body painting may have been practiced. Nodules of haematite with highly polished surfaces have been found as well; the shiny surfaces suggest that the nodules were used to finish leather.
Other artifacts excavated on site made of animal, fish, bird, and whalebone, whale and walrus ivory, and killer whale teeth included awls, needles, knives, beads, adzes, shovels, small bowls and, most remarkably, ivory pins up to 10 in long. These pins are very similar to examples found in passage graves in the Boyne Valley, another piece of evidence suggesting a linkage between the two cultures. So-called Skaill knives were commonly used tools in Skara Brae; these consist of large flakes knocked off sandstone cobbles. Skaill knives have been found throughout Orkney and Shetland.
The 1972 excavations reached layers that had remained waterlogged and had preserved items that otherwise would have been destroyed. These include a twisted skein of heather, one of a very few known examples of Neolithic rope. and a wooden handle.
Related sites in Orkney.
A comparable, though smaller, site exists at Rinyo on Rousay. Unusually, no Maeshowe-type tombs have been found on Rousay and although there are a large number of Orkney–Cromarty chambered cairns, these were built by Unstan ware people.
Knap of Howar on the Orkney island of Papa Westray, is a well preserved Neolithic farmstead. Dating from 3500 BCE to 3100 BCE, it is similar in design to Skara Brae, but from an earlier period, and it is thought to be the oldest preserved standing building in northern Europe.
There is also a site currently under excavation at Links of Noltland on Westray that appears to have similarities to Skara Brae.
World Heritage status.
"The Heart of Neolithic Orkney" was inscribed as a World Heritage site in December 1999. In addition to Skara Brae the site includes Maeshowe, the Ring of Brodgar, the Standing Stones of Stenness and other nearby sites. It is managed by Historic Scotland, whose 'Statement of Significance' for the site begins:
The monuments at the heart of Neolithic Orkney and Skara Brae proclaim the triumphs of the human spirit in early ages and isolated places. They were approximately contemporary with the mastabas of the archaic period of Egypt (first and second dynasties), the brick temples of Sumeria, and the first cities of the Harappa culture in India, and a century or two earlier than the Golden Age of China. Unusually fine for their early date, and with a remarkably rich survival of evidence, these sites stand as a visible symbol of the achievements of early peoples away from the traditional centres of civilisation.
Notes.
^a It is one of four UNESCO World Heritage Sites in Scotland, the others being the Old Town and New Town of Edinburgh; New Lanark in South Lanarkshire; and St Kilda in the Western Isles 

</doc>
<doc id="28893" url="http://en.wikipedia.org/wiki?curid=28893" title="Sinners in the Hands of an Angry God">
Sinners in the Hands of an Angry God

"Sinners in the Hands of an Angry God" is a sermon written by British Colonial Christian theologian Jonathan Edwards, preached to his own congregation in Northampton, Massachusetts to unknown effect, and again on July 8, 1741 in Enfield, Connecticut. Like Edwards' other works, it combines vivid imagery of Hell with observations of the world and citations of the scripture. It is Edwards' most famous written work, is a fitting representation of his preaching style, and is widely studied by Christians and historians, providing a glimpse into the theology of the Great Awakening of c. 1730–1755.
This is a typical sermon of the Great Awakening, emphasizing the belief that Hell is a real place. Edwards hoped that the imagery and message of his sermon would awaken his audience to the horrific reality that awaited them should they continue without Christ. The underlying point is that God has given humanity a chance to rectify their sins. Edwards says that it is the will of God that keeps wicked men from the depths of Hell. This act of restraint has given humanity a chance to mend their ways and return to Christ.
Doctrine.
"There is nothing that keeps wicked men at any one moment out of hell, but the mere pleasure of God."
Most of the sermon's text consists of ten "considerations":
Purpose.
One church in Enfield, Connecticut had been largely unaffected during the Great Awakening of New England. Edwards was invited by the pastor of the church to preach to them. Edwards's aim was to teach his listeners about the horrors of hell, the dangers of sin and the terrors of being lost. Edwards described the shaky position of those who do not follow Christ's urgent call to receive forgiveness.
Application.
In the final section of "Sinners in the Hands of an Angry God," Edwards shows his theological argument throughout scripture and biblical history. Invoking stories and examples throughout the whole Bible. Edwards ends the sermon with one final appeal, "Therefore let everyone that is out of Christ, now awake and fly from the wrath to come." According to Edwards, only by returning to Christ can one escape the stark fate he outlines.
Effect and legacy.
Jonathan Edwards was interrupted many times before finishing the sermon by people moaning and crying out, "What shall I do to be saved?" Although the sermon has received criticism, Edwards' words have endured and are still read to this day. Edwards' sermon continues to be the leading example of a Great Awakening sermon and is still used in religious and academic studies.

</doc>
<doc id="28894" url="http://en.wikipedia.org/wiki?curid=28894" title="Scottish Highlands">
Scottish Highlands

The Scottish Highlands, known locally simply as the Highlands (Scottish Gaelic: "A' Ghàidhealtachd", "the place of the Gaels"; Scots: "the Hielands") are a historic region of Scotland. The region became culturally distinguishable from the Lowlands from the later Middle Ages into the modern period, when Lowland Scots replaced Scottish Gaelic throughout most of the Lowlands. The term is also used for the area north and west of the Highland Boundary Fault, although the exact boundaries are not clearly defined, particularly to the east. The Great Glen divides the Grampian Mountains to the southeast from the Northwest Highlands. The Scottish Gaelic name of "A' Ghàidhealtachd" literally means "the place of the Gaels" and traditionally, from a Gaelic-speaking point of view, includes both the Western Isles and the Highlands.
The area is very sparsely populated, with many mountain ranges dominating the region, and includes the highest mountain in the British Isles, Ben Nevis. Before the 19th century the Highlands was home to a much larger population, but due to a combination of factors including the outlawing of the traditional Highland way of life following the Jacobite Rising of 1745, the infamous Highland Clearances, and mass migration to urban areas during the Industrial Revolution, the area is now one of the most sparsely populated in Europe. At 9.1 per km2 in 2012, the population density in the Highlands and Islands is less than 1/7th of Scotland's as a whole, comparable with that of Bolivia, Chad and Russia.
The Highland Council is the administrative body for much of the Highlands, with its administrative centre at Inverness. However, the Highlands also includes parts of the council areas of Aberdeenshire, Angus, Argyll and Bute, Moray, Perth and Kinross, and Stirling. Although the Isle of Arran administratively belongs to North Ayrshire, its northern part is generally regarded as part of the Highlands.
The latest Census figures released by the National Register of Scotland, show that the Highland’s population has risen by 23,000 between 2001 and 2011 to 232,000.
History.
Culture.
Between the 15th century and the 20th century, the area differed from most of the Lowlands in terms of language. In Scottish Gaelic, the region is known as the "Gàidhealtachd", because it was traditionally the Gaelic-speaking part of Scotland, although the language is now largely confined to the Outer Hebrides. The terms are sometimes used interchangeably but have different meanings in their respective languages. Scottish English (in its Highland form) is the predominant language of the area today, though Highland English has been influenced by Gaelic speech to a significant extent. Historically, the "Highland line" distinguished the two Scottish cultures. While the Highland line broadly followed the geography of the Grampians in the south, it continued in the north, cutting off the north-eastern areas, that is Caithness, Orkney and Shetland, from the more Gaelic Highlands and Hebrides.
In the aftermath of the Jacobite risings, the British government enacted a series of laws to try to speed up the destruction of the clan system, including bans on the bearing of arms and the wearing of tartan, and limitations on the activities of the Episcopalian Church. Most of this legislation was repealed by the end of the 18th century as the Jacobite threat subsided. There was soon a rehabilitation of Highland culture. Tartan was adopted for Highland regiments in the British Army, which poor Highlanders joined in large numbers in the era of the Revolutionary and Napoleonic wars (1790–1815). Tartan had largely been abandoned by the ordinary people of the region, but in the 1820s, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe. The international craze for tartan, and for idealising a romanticised Highlands, was set off by the Ossian cycle, and further popularised by the works of Walter Scott. His "staging" of the visit of King George IV to Scotland in 1822 and the king's wearing of tartan resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish woollen industry. Individual clan tartans were largely designated in this period and they became a major symbol of Scottish identity. This "Highlandism", by which all of Scotland was identified with the culture of the Highlands, was cemented by Queen Victoria's interest in the country, her adoption of Balmoral as a major royal retreat, and her interest in "tartenry".
Economy.
The Highlands before 1800 were very poor and traditional, and were not much affected by the uplift of the Scottish Enlightenment or the Industrial Revolution that was sweeping the Lowlands of Scotland. The period of the Napoleonic wars brought prosperity, optimism, and economic growth to the Highlands. The economy grew thanks to wages paid in industries such as kelping (in which kelp was burned for the useful chemicals obtained from the ashes), fisheries, and weaving, as well as large-scale infrastructure spending such as the Caledonian Canal project. On the East Coast, farmlands were improved, and high prices for cattle brought money to the area. Service in the Army was also attractive to young men from the Highlands, who sent pay home and retired there with their army pensions. This prosperity ended after 1815, and long-term negative factors began to undermine the economic position of the poor tenant farmers, who typically rented a few acres, and were known as crofters. Landowners were increasingly market-oriented in the century after 1750, and this tended to dissolve the traditional social and economic structure of the North-West Highlands and the Hebrides, causing great disruption for the crofters. The Highland Clearances and the end of the township system followed changes in land ownership and tenancy and the replacement of cattle by sheep. The Great Irish Famine of the 1840s was caused by a plant disease that reached the Highlands in 1846, causing great distress. In a complex form of chain migration, many Highlanders emigrated. Clan leaders would designate which young people should emigrate, where to, and in which order. The first arrivals would prepare the way for their kinsmen who continued to arrive in the chain migration.
The unequal concentration of land ownership remained an emotional and controversial subject, of enormous importance to the Highland economy, and eventually became a cornerstone of liberal radicalism. The poor crofters were politically powerless, and many of them turned to religion. They embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800. Most joined the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. The religious change energised the crofters and separated them from the landlords; it helped prepare them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League.
Violence erupted, starting on the Isle of Skye, when Highland landlords cleared their lands for sheep and deer parks. It was quietened when the government stepped in, passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. This contrasted with the Irish Land War under way at the same time, where the Irish were intensely politicised through roots in Irish nationalism, while political dimensions were limited. In 1885 three Independent Crofter candidates were elected to Parliament, which listened to their pleas. The results included explicit security for the Scottish smallholders; the legal right to bequeath tenancies to descendants; and the creation of a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained their votes.
Religion.
The Scottish Reformation achieved partial success in the Highlands. Roman Catholicism remained strong in some areas, owing to remote locations and the efforts of Franciscan missionaries from Ireland, who regularly came to celebrate Mass. Although the presence of Roman Catholicism has faded, there remain significant Catholic strongholds within the Highlands and Islands such as Moidart and Morar on the mainland and South Uist and Barra in the southern Outer Hebrides. 
The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw somewhat greater success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society after the Battle of Culloden in 1746. In the 19th century, the evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly, appealing much more strongly than did the established church.
For the most part, however, the Highlands are considered predominantly Protestant, loyal to the Church of Scotland. In contrast to the Catholic southern islands, the northern Outer Hebrides islands (Lewis, Harris and North Uist) have an exceptionally high proportion of their population belonging to the Protestant Free Church of Scotland or the Free Presbyterian Church of Scotland. The Outer Hebrides have been described as the last bastion of Calvinism in Britain and the Sabbath remains widely observed. Inverness and the surrounding area has a majority Protestant population, with most locals belonging to either The Kirk or the Free Church of Scotland. The church maintains a noticeable presence within the area, with church attendance notably higher than in other Scottish cities. Religion continues to play an important role in Highland culture, with Sabbath observance still widely practised, particularly in the Hebrides.
Historical geography.
In traditional Scottish geography, the Highlands refers to that part of Scotland north-west of the Highland Boundary Fault, which crosses mainland Scotland in a near-straight line from Helensburgh to Stonehaven. However the flat coastal lands that occupy parts of the counties of Nairnshire, Morayshire, Banffshire and Aberdeenshire are often excluded as they do not share the distinctive geographical and cultural features of the rest of the Highlands. The north-east of Caithness, as well as Orkney and Shetland, are also often excluded from the Highlands, although the Hebrides are usually included. The Highland area, as so defined, differed from the Lowlands in language and tradition, having preserved Gaelic speech and customs centuries after the anglicisation of the latter; this led to a growing perception of a divide, with the cultural distinction between Highlander and Lowlander first noted towards the end of the 14th century. In Aberdeenshire, the boundary between the Highlands and the Lowlands is not well defined. There is a stone beside the A93 road near the village of Dinnet on Royal Deeside which states 'You are now in the Highlands', although there are areas of Highland character to the east of this point.
A much wider definition of the Highlands is that used by the Scotch Whisky industry. Highland Single Malts are produced at distilleries north of an imaginary line between Dundee and Greenock, thus including all of Aberdeenshire and Angus.
Inverness is traditionally regarded as the capital of the Highlands, although less so in the Highland parts of Aberdeenshire, Angus, Perthshire and Stirlingshire which look more to Aberdeen, Perth, Dundee and Stirling as their commercial centres. Under some of the wider definitions in use, Aberdeen could be considered the largest city in the Highlands, although it does not share the recent Gaelic cultural history typical of the Highlands proper.
Highland Council area.
The Highland Council area, created as one of the local government regions of Scotland, has been a unitary council area since 1996. The council area excludes a large area of the southern and eastern Highlands, and the Western Isles, but includes Caithness. "Highlands" is sometimes used, however, as a name for the council area, as in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", is also used to refer to the area covered by the fire and rescue service. This area consists of the Highland council area and the island council areas of Orkney, Shetland and the Western Isles.
Highland Council signs in the Pass of Drumochter, between Glen Garry and Dalwhinnie, say "Welcome to the Highlands".
Highlands and Islands.
Much of the Highlands area overlaps the Highlands and Islands area. An electoral region called "Highlands and Islands" is used in elections to the Scottish Parliament: this area includes Orkney and Shetland, as well as the Highland Council local government area, the Western Isles and most of the Argyll and Bute and Moray local government areas. "Highlands and Islands" has, however, different meanings in different contexts. It means Highland (the local government area), Orkney, Shetland, and the Western Isles in "Highlands and Islands Fire and Rescue Service". "Northern", as in "Northern Constabulary", refers to the same area as that covered by the fire and rescue service.
Historical crossings.
There have been trackways from the Lowlands to the Highlands since prehistoric times. Many traverse the Mounth, a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven. The most well-known and historically important trackways are the Causey Mounth, Elsick Mounth, Cryne Corse Mounth and Cairnamounth.
Courier Delivery.
Although most of the Highlands is geographically on the British mainland, it is somewhat less accessible than the rest of Britain; thus most UK couriers categorise it separately, alongside Northern Ireland, the Isle of Man, and other offshore islands. They thus charge additional fees for delivery to the Highlands, or exclude the area entirely. Whilst the physical remoteness from the largest population centres inevitably leads to higher transit cost, there is confusion and consternation over the scale of the fees charged and the effectiveness of their communication, and the use of the word Mainland in their justification. The Royal Mail is the only delivery network bound by a Universal Service Obligation to charge a uniform tariff across the UK. This, however, applies only to mail items and not larger packages which are dealt with by its Parcelforce division.
Geology.
The Highlands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland is largely composed of ancient rocks from the Cambrian and Precambrian periods which were uplifted during the later Caledonian Orogeny. Smaller formations of Lewisian gneiss in the northwest are up to 3 billion years old. The overlying rocks of the Torridonian sandstone form mountains in the Torridon Hills such as Liathach and Beinn Eighe in Wester Ross.
These foundations are interspersed with many igneous intrusions of a more recent age, the remnants of which have formed mountain massifs such as the Cairngorms and the Cuillin of Skye. A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast and partially down the Highland Boundary Fault. The Jurassic beds found in isolated locations on Skye and Applecross reflect the complex underlying geology. They are the original source of much North Sea oil. The Great Glen is a transform fault which divides the Grampian Mountains to the southeast from the Northwest Highlands.
The entire region was covered by ice sheets during the Pleistocene ice ages, save perhaps for a few nunataks. The complex geomorphology includes incised valleys and lochs carved by the action of mountain streams and ice, and a topography of irregularly distributed mountains whose summits have similar heights above sea-level, but whose bases depend upon the amount of denudation to which the plateau has been subjected in various places.

</doc>
<doc id="28896" url="http://en.wikipedia.org/wiki?curid=28896" title="Scotch whisky">
Scotch whisky

Scotch whisky, often simply called Scotch, is malt whisky or grain whisky made in Scotland. Scotch whisky must be made in a manner specified by law.
All Scotch whisky was originally made from malted barley. Commercial distilleries began introducing whisky made from wheat and rye in the late 18th century. Scotch whisky is divided into five distinct categories: single malt Scotch whisky, single grain Scotch whisky, blended malt Scotch whisky (formerly called "vatted malt" or "pure malt"), blended grain Scotch whisky, and blended Scotch whisky.
All Scotch whisky must be aged in oak barrels for at least three years and one day. Any age statement on a bottle of Scotch whisky, expressed in numerical form, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed-age whisky.
The first written mention of Scotch whisky is in the Exchequer Rolls of Scotland, 1495. A friar named John Cor was the distiller at Lindores Abbey in the Kingdom of Fife.
Many Scotch whisky drinkers will refer to a unit for drinking as a dram.
Regulations and labelling.
Legal definition.
As of 23 November 2009, the Scotch Whisky Regulations 2009 (SWR) define and regulate the production, labelling, packaging as well as the advertising of Scotch whisky in the United Kingdom. They replace previous regulations that focused solely on production. International trade agreements have the effect of making some provisions of the SWR apply in various other countries as well as in the UK. The SWR define "Scotch whisky" as whisky that is:
Labelling.
A Scotch whisky label comprises several elements that indicate aspects of production, age, bottling, and ownership. Some of these elements are regulated by the SWR, and some reflect tradition and marketing. The spelling of the term "whisky" is often debated by journalists and consumers. Scottish, Australian and Canadian whiskies use "whisky", Irish whiskies use "whiskey", while American and other styles vary in their spelling of the term.
The label always features a declaration of the malt or grain whiskies used. A single malt Scotch whisky is one that is entirely produced from malt in one distillery. One may also encounter the term "single cask", signifying the bottling comes entirely from one cask. The term "blended malt" signifies that single malt whisky from different distilleries are blended in the bottle. The Cardhu distillery also began using the term "pure malt" for the same purpose, causing a controversy in the process over clarity in labelling – the Glenfiddich distillery was using the term to describe some single malt bottlings. As a result, the Scotch Whisky Association declared that a mixture of single malt whiskies must be labelled a "blended malt". The use of the former terms "vatted malt" and "pure malt" is prohibited. The term "blended malt" is still debated, as some bottlers maintain that consumers confuse the term with "blended Scotch whisky", which contains some proportion of grain whisky.
The brand name featured on the label is usually the same as the distillery name (for example, the Talisker Distillery labels its whiskies with the Talisker name). Indeed, the SWR prohibit bottlers from using a distillery name when the whisky was not made there. A bottler name may also be listed, sometimes independent of the distillery. In addition to requiring that Scotch whisky be distilled in Scotland, the SWR require that it also be bottled and labeled in Scotland. Labels may also indicate the region of the distillery (for example, Islay or Speyside).
Alcoholic strength is expressed on the label with "Alcohol By Volume" ("ABV") or sometimes simply "Vol". Typically, bottled whisky is between 40% and 46% ABV. Whisky is considerably stronger when first emerging from the cask—normally 60–63% ABV. Water is then added to create the desired bottling strength. If the whisky is not diluted before bottling, it can be labelled as cask strength.
A whisky's age may be listed on the bottle providing a guarantee of the youngest whisky used. An age statement on the bottle, in the form of a number, must reflect the age of the youngest whisky used to produce that product. A whisky with an age statement is known as guaranteed age whisky. Scotch whisky without an age statement may, by law, be as young as three years old. In the early 21st century, such "No age statement" whiskies became more common, as distilleries responded to the depletion of aged stocks caused by improved sales. 
 A label may carry a distillation date or a bottling date. Whisky does not mature once bottled, so if no age statement is provided, one may calculate the age of the whisky if both the distillation date and bottling date are given.
Labels may also carry various declarations of filtration techniques or final maturation processes. A Scotch whisky labelled as "natural" or "non-chill-filtered" has not been through a filtration process during bottling that removes compounds that some consumers see as desirable. Some whiskies are placed in different casks—often sherry or port casks—during a portion of maturation. Since this takes place at the end of the maturation process, these whiskies may be labelled as "wood finished", "sherry/port finished", and so on.
Types.
There are two basic types of Scotch whisky, from which all blends are made:
Excluded from the definition of "single grain Scotch whisky" is any spirit that qualifies as a single malt Scotch whisky or as a blended Scotch whisky. The latter exclusion is to ensure that a blended Scotch whisky produced from single malt(s) and single grain(s) distilled at the same distillery does not also qualify as single grain Scotch whisky.
Three types of blends are defined for Scotch whisky:
The five Scotch whisky definitions are structured in such a way that the categories are mutually exclusive. The 2009 regulations changed the formal definition of blended Scotch whisky to achieve this result, but in a way that reflected traditional and current practice: before the 2009 SWR, any combination of Scotch whiskies qualified as a blended Scotch whisky, including for example a blend of single malt Scotch whiskies.
As was the case under the Scotch Whisky Act 1988, regulation 5 of the SWR 2009 stipulates that the only whisky that may be manufactured in Scotland is Scotch whisky. The definition of "manufacture" is "keeping for the purpose of maturation; and keeping, or using, for the purpose of blending, except for domestic blending for domestic consumption". This provision prevents the existence of two "grades" of whisky originating from Scotland, one “Scotch whisky” and the other, a "whisky – product of Scotland" that complies with the generic EU standard for whisky. According to the Scotch Whisky Association, allowing non-Scotch whisky production in Scotland would make it difficult to protect Scotch whisky as a distinctive product.
Single grain.
The majority of grain whisky produced in Scotland goes to make blended Scotch whisky. The average blended whisky is 60%–85% grain whisky.
Some higher-quality grain whisky from a single distillery is bottled as single grain whisky.
Blended malt.
Blended malt whisky—formerly called "vatted malt" or "pure malt" (terms that are now prohibited in the SWR 2009)—is one of the least common types of Scotch: a blend of single malts from more than one distillery (possibly with differing ages). Blended malts contain only malt whiskies—no grain whiskies—and are usually distinguished from other types of whisky by the absence of the word 'single' before 'malt' on the bottle, and the absence of a distillery name. The age of the vat is that of the youngest of the original ingredients. For example, a blended malt marked "8 years old" may include older whiskies, with the youngest constituent being eight years old. Johnnie Walker Green Label and Monkey Shoulder are examples of blended malt whisky. Starting from November 2011 no Scotch whisky could be labelled as a vatted malt or pure malt, the SWR requiring them to be labelled blended malt instead.
Blended.
Blended Scotch whisky constitutes about 90% of the whisky produced in Scotland. Blended Scotch whiskies contain both malt whisky and grain whisky. Producers combine the various malts and grain whiskies to produce a consistent brand style. Notable blended Scotch whisky brands include Bells, Dewar's, Johnnie Walker, Whyte and Mackay, Cutty Sark, J&B, The Famous Grouse, Ballantine's and Chivas Regal.
Independent bottlers.
Most malt distilleries sell a significant amount of whisky by the cask for blending, and sometimes to private buyers as well. Whisky from such casks is sometimes bottled as a single malt by "independent bottling" firms such as Duncan Taylor, Master of Malt, Gordon & MacPhail, Cadenhead's, The Scotch Malt Whisky Society, Murray McDavid, Berry Brothers & Rudd, Douglas Laing, and others. These are usually labelled with the distillery's name, but not using the distillery's trademarked logos or typefaces. An "official bottling" (or "proprietary bottling"), by comparison, is from the distillery (or its owner). Many independent bottlings are from single casks, and they may sometimes be very different from an official bottling.
For a variety of reasons, some independent brands do not identify which facility distilled the whisky in the bottle. They may instead identify only the general geographical area of the source, or they simply market the product using their own brand name without identifying their source. This may, in some cases, be simply to give the independent bottling company the flexibility to purchase from multiple distillers without changing their labels.
History.
To Friar John Cor, by order of the King, to make aqua vitae, VIII bolls of malt.—Exchequer Rolls of Scotland, 1 June 1495.
According to the Scotch Whisky Association, Scotch whisky evolved from a Scottish drink called "uisge beatha", which means "water of life". The earliest record of distillation in Scotland occurred as long ago as 1494, as documented in the "Exchequer Rolls", which were records of royal income and expenditure. The quote above records eight bolls of malt given to Friar John Cor wherewith to make aqua vitae over the previous year. This would be enough for 1,500 bottles, which suggests that distillation was well-established by the late 15th century.
Whisky production was first taxed in 1644, causing a rise in illicit whisky distilling in the country. Around 1780, there were about eight legal distilleries and 400 illegal ones. In 1823, Parliament eased restrictions on licensed distilleries with the "Excise Act", while at the same time making it harder for the illegal stills to operate, thereby ushering in the modern era of Scotch production. Two events helped to increase whisky's popularity: first, the introduction in 1831 of the column still; the whisky produced with this process was generally less expensive to produce and also less intense and smoother, because a column still can perform the equivalent of multiple distillation steps in a continuous distillation process. Second, the "phylloxera" bug destroyed wine and cognac production in France in 1880.
Regions.
Scotland was traditionally divided into four regions: The Highlands, Lowland, Islay, and Campbeltown.
Speyside, encompassing the Spey river valley in north-east Scotland, once considered part of the Highlands, has almost half of the total number (approx. 105 as of 2013) of distilleries in Scotland within its geographic boundaries; consequently it is officially recognized as a distinct region.
The Islands is not recognised as a region by the Scotch Whisky Association (SWA) and is considered part of the Highlands region.
Although only five regions are specifically described, any Scottish locale may be used to describe a whisky if it is distilled entirely within that place; for example a single malt whisky distilled on Orkney could be described as "Orkney Single Malt Scotch Whisky" instead of as a Highland whisky.
References.
Notes
Cited sources
Other sources

</doc>
<doc id="28897" url="http://en.wikipedia.org/wiki?curid=28897" title="Special drawing rights">
Special drawing rights

Special drawing rights (XDR aka SDR) are supplementary foreign exchange reserve assets defined and maintained by the International Monetary Fund (IMF). Their value is based on a basket of key international currencies reviewed by IMF every five years. Based on the latest review conducted on December 30, 2010, the XDR basket consists of the following four currencies: U.S. dollars ($) 41.9 percent (compared with 44 percent at the 2005 review), euro (€) 37.4 percent (compared with 34 percent at the 2005 review), pounds sterling (£) 11.3 percent (compared with 11 percent at the 2005 review), and the Japanese yen (¥) 9.4 percent (compared with 11 percent at the 2005 review). The weights assigned to each currency in the XDR basket are adjusted to take into account their current prominence in terms of international trade and national foreign exchange reserves.
The XDR is not a currency "per se". They instead represent a claim to currency held by IMF member countries for which they may be exchanged. As they can only be exchanged for U.S. dollars ($), euro (€), pounds sterling (£), or Japanese yen (¥), XDRs may actually represent a potential claim on IMF member countries' nongold foreign exchange reserves, which are usually held in those currencies. While they may appear to have a far more important part to play or, perhaps, an important future role, being the unit of account for the IMF has long been the main function of the XDR.
Created in 1969 to supplement a shortfall of preferred foreign exchange reserve assets, namely gold and the U.S. dollar, the value of the XDR is defined by a weighted currency basket of four major currencies: the U.S. dollar, the euro, the British pound, and the Japanese yen. Special Drawing Rights are denoted with the ISO 4217 currency code XDR.
XDRs are allocated to countries by the IMF. Private parties do not hold or use them. The amount of XDRs in existence was around XDR 21.4 billion in August 2009. During the global financial crisis of 2009, an additional XDR 182.6 billion were allocated to "provide liquidity to the global economic system and supplement member countries’ official reserves". By October 2014, the amount of XDRs in existence was XDR 204 billion.
Name.
While the ISO 4217 currency code for Special Drawing Rights is XDR, they are often referred to by their acronym SDR. Both refer to the name "Special Drawing Rights".
Intentionally innocuous and free of connotations due to disagreements over the nature of this new reserve asset during its creation, the name derives from a debate about its primary function—money or credit. While the name would offend neither side, it can be argued that prior to 1981 the XDR was a debt security and so a form of credit. Member countries receiving XDR allocations were required by the reconstitution provision of the XDR articles to hold a prescribed number of XDRs. If a state used any of its allotment, it was expected to rebuild its XDR holdings. As the reconstitution provisions were abrogated in 1981, the XDR now functions less like credit than previously. Countries are still expected to maintain their XDR holdings at a certain level, but penalties for holding fewer than the allocated amount are now less onerous.
The name may actually derive from an early proposal for IMF "reserve drawing rights". The word "reserve" was later replaced with "special" because the idea that the IMF was creating a foreign exchange reserve asset was contentious.
History.
Special drawing rights were created by the IMF in 1969 and were intended to be an asset held in foreign exchange reserves under the Bretton Woods system of fixed exchange rates. 1 XDR was initially defined as 1 USD, equal to 0.888671 g of gold. After the collapse of that system in the early 1970s the XDR has taken on a less important role. Acting as the unit of account for the IMF has been its primary purpose since 1972.
The IMF itself calls the current role of the XDR "insignificant". Developed countries, who hold the greatest number of XDRs, are unlikely to use them for any purpose. The only actual users of XDRs may be those developing countries that see them as "a rather cheap line of credit".
One reason XDRs may not see much use as foreign exchange reserve assets is that they must be exchanged into a currency before use. This is due in part to the fact private parties do not hold XDRs: they are only used and held by IMF member countries, the IMF itself, and a select few organizations licensed to do so by the IMF. Basic functions of foreign exchange reserves, such as market intervention and liquidity provision, as well as some less prosaic ones, such as maintaining export competitiveness via favorable exchange rates, cannot be accomplished directly using XDRs. This fact has led the IMF to label the XDR as an "imperfect reserve asset".
Another reason they may see little use is that the number of XDRs in existence is relatively few. As of January 2011, XDRs represented less than 4% of global foreign exchange reserve assets. To function well a foreign exchange reserve asset must have sufficient liquidity, but XDRs, due to their small number, may be perceived to be an illiquid asset. The IMF says, "expanding the volume of official XDRs is a prerequisite for them to play a more meaningful role as a substitute reserve asset."
Alternative to U.S. dollar.
The XDR comes to prominence when the U.S. dollar is weak or otherwise unsuitable to be a foreign exchange reserve asset. This usually manifests itself as an allocation of XDRs to IMF member countries. Distrust of the U.S. dollar is not the only stated reason allocations have been made, however.
One of its first roles was to alleviate an expected shortfall of U.S. dollars c. 1970. At this time, the United States had a conservative monetary policy and did not want to increase the total amount of U.S. dollars in existence. If the United States had continued down this path, the dollar would have become a less attractive foreign exchange reserve asset: it would not have had the necessary liquidity to serve this function. Soon after XDR allocations began, the United States reversed its former policy and provided sufficient liquidity. In the process a potential role for the XDR was removed. During this first round of allocations, 9.3 billion XDRs were distributed to IMF member countries.
The XDR resurfaced in 1978 when many countries were wary of taking on more foreign exchange reserve assets denominated in U.S. dollars. This suspicion of the dollar precipitated an allocation of 12 billion XDRs over a period of four years.
Concomitant with the financial crisis of 2007–2010, the third round of XDR allocations occurred in the years 2009 and 2011. The IMF recognized the financial crisis as the cause for distributing the large majority of these third-round allotments, but some allocations were couched as distributing XDRs to countries that had never received any and others as a re-balancing of IMF quotas, which determine how many XDRs a country is allotted, to better represent the economic strength of emerging markets. In total, 203.4 billion XDRs were allocated in this round.
During this time China, a country with large holdings of U.S. dollar foreign exchange reserves, voiced its displeasure at the current international monetary system promoting measures that would allow the XDR to "fully satisfy the member countries' demand for a reserve currency". These comments, made by a chairman of the People's Bank of China, Zhou Xiaochuan, drew media attention, and the IMF showed some support for China's stance. It produced a paper exploring ways the substance and function of the XDR could be increased. China has also suggested the creation of a substitution account to allow exchange of U.S. dollars into XDRs. When substitution was proposed before, in 1978, the United States appeared reluctant to allow such a mechanism to become operational. It is likely just as reluctant today.
Use by developing countries.
In 2001, the UN suggested allocating XDRs to developing countries for use by them as cost-free alternatives to building foreign exchange reserves though borrowing or running current account surpluses. In 2009, an XDR allocation was made to countries that had joined the IMF after the 1979–1981 round of allocations was complete (and so had never been allocated any). First proposed in 1997, many of the beneficiaries of this 2009 allocation were developing countries.
Value definition.
The value of the XDR is determined by the value of several currencies important to the world’s trading and financial systems. Initially its value was fixed, so that 1 XDR = 1 U.S. dollar, but this was abandoned in favor of a currency basket after the 1973 collapse of the Bretton Woods system of fixed exchange rates. From July 1974 to December 1980, the XDR consisted of a basket of 16 currencies. From January 1981 until the birth of the euro, the basket was updated to include U.S. dollar, the Deutsche mark, the French franc, the British pound and the Japanese yen as the constituents. Composed of the U.S. dollar, the euro, the British pound and the Japanese yen, the current basket of currencies used to value the XDR is "weighted" meaning that the more important currencies have a larger impact on its value. As of December 2010, the value of one XDR is equal to the sum of 0.423 Euro, 12.1 yen, 0.111 pounds, and 0.66 U.S. dollars.
This basket is re-evaluated every five years, and the currencies included as well as the weights given to them can then change. A currency's importance is currently measured by the degree to which it is used as a foreign exchange reserve asset and the amount of exports sold in that currency.
Due to fluctuating exchange rates, the relative value of each currency varies continuously and so does the value of the XDR. The IMF fixes the value of one XDR in terms of U.S. dollars every day. The latest U.S. dollar valuation of the XDR is published on the IMF website.
Interest rate.
Special drawing rights carry a weekly determined interest rate, but no party pays interest if an IMF member country maintains the amount of XDRs allocated to it. Based on "a weighted average of representative interest rates on short-term debt in the money markets of the XDR basket currencies", interest is paid by an IMF member country if it holds less XDRs than it was allocated, and interest is paid to a member country if it holds more XDRs than the amount it was allocated.
Allocations.
Special drawing rights are allocated to member countries by the IMF. A country's IMF quota, the maximum amount of financial resources that it is obligated to contribute to the fund, determines its allotment of XDRs. Any new allocations must be voted on in the XDR Department of the IMF and pass with an 85% majority. All IMF member countries are represented in the XDR Department, but this is not a one country, one vote system; voting power is determined by a member country's IMF quota. For example, the United States has 16.7% of the vote as of March 2, 2011.
Allocations are not made on a regular basis and have only occurred on several occasions. The first round took place due to a situation that was soon reversed, the possibility of an insufficient amount of U.S. dollars because of U.S. reluctance to run the deficit necessary to supply future demand. Extraordinary circumstances have, likewise, led to the other XDR allocation events.
Exchange.
In order to use its XDRs, a country must find a willing party to buy them. The IMF acts as an intermediary in this voluntary exchange; it also has the authority under the designation mechanism to ask member countries with strong foreign exchange reserves to purchase XDRs from those with weak reserves. The maximum obligation any country has under this mechanism is currency equal to twice the amount of its XDR allocation. As of 2011, XDRs may only be exchanged for euro, Japanese yen, UK pounds, or U.S. dollars. The IMF says exchanging XDRs can take "several days".
It is not, however, the IMF that pays out foreign currency in exchange for XDRs: the claim to currency that XDRs represent is not a claim on the IMF.
Other uses.
Unit of account.
Some international organizations use the XDR as a unit of account. The IMF says using the XDR in this way "help[s] cope with exchange rate volatility". As of 2001, organizations that use the XDR as a unit of account, besides the IMF itself, include: African Development Bank, Arab Monetary Fund, Asian Development Bank, Bank for International Settlements, Common Fund for Commodities, East African Development Bank, Economic Community of West African States, International Center for Settlement of Investment Disputes, International Fund for Agricultural Development, and Islamic Development Bank. It is not only international organizations that use the XDR in this way. JETRO uses XDRs to price foreign aid. In addition, charges, liabilities, and fees prescribed by some international treaties are denominated in XDRs. In 2003, the Bank for International Settlements ceased to use the gold franc as their currency, in favour of XDR.
Use in international law.
In some international treaties and agreements, XDRs are used to value penalties, charges or prices. For example, the Convention on Limitation of Liability for Maritime Claims caps personal liability for damages to ships at XDR 330,000. The Montreal Convention and other treaties also use XDRs in this way.
Currency peg.
According to the IMF, "the SDR may not be any country’s optimal basket", but a few countries do peg their currencies to the XDR. One possible benefit to nations with XDR pegs is that they may be perceived to be more transparent. As of 2000, the number of countries that did so was four. This is a substantial decrease from 1983, when 14 countries had XDR pegs. As of 2007 and 2010, Syria pegs its pound to the XDR.
References.
Works cited.
</dl>

</doc>
<doc id="28898" url="http://en.wikipedia.org/wiki?curid=28898" title="Special Operations Executive">
Special Operations Executive

The Special Operations Executive (SOE) was a British World War II organisation. Following Cabinet approval, it was officially formed by Minister of Economic Warfare Hugh Dalton on 22 July 1940, to conduct espionage, sabotage and reconnaissance in occupied Europe (and later, in South East Asia also) against the Axis powers, and to aid local resistance movements.
It was initially also involved in the formation of the Auxiliary Units, a top secret "stay-behind" resistance organisation which would have been activated in the event of a German invasion of Britain.
Few people were aware of SOE's existence. To those who were part of it or liaised with it, it was sometimes referred to as "the Baker Street Irregulars", after the location of its London headquarters. It was also known as "Churchill's Secret Army" or the "Ministry of Ungentlemanly Warfare". Its various branches, and sometimes the organisation as a whole, were concealed for security purposes behind names such as the "Joint Technical Board" or the "Inter-Service Research Bureau", or fictitious branches of the Air Ministry, Admiralty or War Office.
SOE operated in all countries or former countries occupied by or attacked by the Axis forces, except where demarcation lines were agreed with Britain's principal allies (the Soviet Union and the United States). It also made use of neutral territory on occasion, or made plans and preparations in case neutral countries were attacked by the Axis. The organisation directly employed or controlled just over 13,000 people, about 3,200 of whom were women.
After the war, the organisation was officially dissolved on 15 January 1946. A memorial to SOE's agents was unveiled on the Albert Embankment by Lambeth Palace in London in October 2009.
History.
Origins.
The organisation was formed from the merger of three existing secret departments, which had been formed shortly before the outbreak of the Second World War. Immediately after Germany annexed Austria (the "Anschluss") in March 1938, the Foreign Office created a propaganda organisation known as Department EH (after Electra House, its headquarters), run by Canadian newspaper magnate Sir Campbell Stuart. Later that month, the Secret Intelligence Service (SIS, also known as MI6) formed a section known as Section D, under Major Lawrence Grand RE, to investigate the use of sabotage, propaganda and other irregular means to weaken an enemy. In the autumn of the same year, the War Office expanded an existing research department known as GS (R) and appointed Major J. C. Holland RE as its head to conduct research into guerrilla warfare. GS (R) was renamed MI(R) in early 1939.
These three departments worked with few resources until the outbreak of war. There was much overlap between their activities and Section D and EH duplicated much of each other's work. On the other hand, the heads of Section D and MI(R) knew each other and shared information. They agreed a rough division of their activities; MI(R) researched irregular operations which could be undertaken by regular uniformed troops, while Section D dealt with truly undercover work.
During the early months of the war, Section D was based first at St Ermin's Hotel in Westminster and then the Metropole Hotel near Trafalgar Square. The Section attempted unsuccessfully to sabotage deliveries of vital strategic materials to Germany from neutral countries by mining the Iron Gate on the River Danube. MI(R) meanwhile produced pamphlets and technical handbooks for guerrilla leaders. MI(R) was also involved in the formation of the Independent Companies, autonomous units formed from within second-line Territorial Army divisions which were intended for sabotage and guerrilla operations behind enemy lines in the Norwegian Campaign and which were later absorbed by the British Commandos; and the Auxiliary Units, stay-behind resistance groups which would act in the event of an Axis invasion of Britain, as seemed possible in the early years of the war.
Formation.
On 13 June 1940, at the instigation of newly appointed Prime Minister Winston Churchill, Lord Hankey (who held the Cabinet post of Chancellor of the Duchy of Lancaster) persuaded Section D and MI R that their operations should be coordinated. On 1 July, a Cabinet level meeting arranged the formation of a single sabotage organisation. On 16 July, Hugh Dalton, the Minister of Economic Warfare, was appointed to take political responsibility for the new organisation, which was formally created on 22 July. Dalton used the Irish Republican Army (IRA) during the Irish war of Independence as a model for the organisation. Churchill ordered SOE to "set Europe ablaze". Majors Grand and Holland both returned to service in the regular army and Campbell Stuart left the organisation.
One department of MI R, MI R(C), which was involved in the development of weapons for irregular warfare, was not integrated into SOE but became an independent body codenamed MD1. It was nicknamed "Churchill's Toyshop" from the Prime Minister's close interest in it and his enthusiastic support.
Leadership.
The Director of SOE was usually referred to by the initials "CD". The first Director to be appointed was Sir Frank Nelson, a former head of a trading firm in India, a back bench Conservative Member of Parliament and Consul in Basel, Switzerland.
Dalton was replaced as Minister of Economic Warfare by Lord Selborne in February 1942. Selborne in turn retired Nelson, who had suffered ill health as a result of his hard work, and appointed Sir Charles Hambro, head of the English banking firm Hambro's to replace him. Hambro had been a close friend of Churchill before the war and had won the Military Cross in the First World War. Selborne also transferred Gladwyn Jebb, the senior civil servant who had run the Ministry's day-to-day dealings with SOE, back to the Foreign Office.
Selborne and Hambro cooperated closely until August 1943, when they fell out over the question of whether SOE should remain a separate body or coordinate its operations with those of the British Army in several theatres of war. Hambro felt that any loss of autonomy would cause a number of problems for SOE in the future. At the same time, Hambro was found to have failed to pass on vital information to Selborne. He was dismissed as Director, and became head of a raw materials purchasing commission in Washington, D.C., which was involved in the exchange of nuclear information.
As part of the subsequent closer ties between the Imperial General Staff and SOE (although SOE had no representation on the Chiefs of Staff Committee), Hambro's replacement as Director from September 1943 was the former Deputy Director, Major General Colin Gubbins. Gubbins had wide experience of commando and clandestine operations and had played a major part in MI R's early operations. He also put in practice many of the lessons he learned from the IRA during the Irish War of Independence.
Organisation.
The organization of SOE continually evolved and changed during the war. Initially, it consisted of three broad departments: SO1, which dealt with propaganda; SO2 (Operations); and SO3 (Research). SO3 was quickly overloaded with paperwork and was merged into SO2. In August 1941, following quarrels between the Ministry of Economic Warfare and the Ministry of Information over their relative responsibilities, SO1 was removed from SOE and became an independent organisation, the Political Warfare Executive.
Thereafter there was a single, broad "Operations" department which controlled the Sections operating into enemy and sometimes neutral territory, and the selection and training of agents. Sections were assigned to a single country. Some enemy-occupied countries had two or more sections assigned to deal with politically disparate resistance movements. (France had no less than six).
Four departments and some smaller groups were controlled by the Director of Scientific Research, Professor Dudley Maurice Newitt, and were concerned with the development or acquisition and production of special equipment. A few other sections were involved with finance, security, economic research and administration, although SOE had no central registry or filing system. When Gubbins was appointed Director, he formalised some of the administrative practices which had grown in an "ad hoc" fashion and appointed an Establishment Officer to oversee the manpower and other requirements of the various departments.
The Director of SOE had either a Deputy from the Army, or (once Gubbins became Director) an army officer as Chief of Staff. The main controlling body of SOE was its Council, consisting of around fifteen heads of departments or sections. About half of the Council were from the armed forces (although some were specialists who were only commissioned after the outbreak of war), the rest were various civil servants, lawyers, or business or industrial experts. Most of the members of the Council, and the senior officers and functionaries of SOE generally, were recruited by word of mouth among public school alumni and Oxbridge graduates, although this did not notably affect SOE's political complexion.
Several subsidiary SOE headquarters and stations were set up to manage operations which were too distant for London to control directly. SOE's operations in the Middle East and Balkans were controlled from a headquarters in Cairo, which was notorious for poor security, infighting and conflicts with other agencies. It finally became known in April 1944 as Special Operations (Mediterranean), or SO(M). Shortly after the Allied landings in North Africa, a station codenamed "Massingham" was established near Algiers in late 1942, which operated into Southern France. Following the Allied invasion of Italy, personnel from "Massingham" established forward stations in Brindisi and near Naples. A subsidiary headquarters initially known as "Force 133" was later set up in Bari in Southern Italy, under the Cairo headquarters, to control operations in the Balkans and Northern Italy.
An SOE station, which was first called the "India Mission", and was subsequently known as "GS I(k)" was set up in India late in 1940. It subsequently moved to Ceylon so as to be closer to the headquarters of the Allied South East Asia Command and became known as Force 136. A "Singapore Mission" was set up at the same time as the India Mission but was unable to overcome official opposition to its attempts to form resistance movements in Malaya before the Japanese overran Singapore. Force 136 took over its surviving staff and operations.
There was also a branch office in New York, formally titled British Security Coordination, and headed by the Canadian businessman Sir William Stephenson. This branch office, located at Room 3603, 630 Fifth Avenue, Rockefeller Center, coordinated the work of SOE, SIS and MI5 with the American F.B.I. and Office of Strategic Services.
Relationships.
SOE cooperated fairly well with Combined Operations Headquarters during the middle years of the war, usually on technical matters as SOE's equipment was readily adopted by commandos and other raiders. This support was lost when Vice Admiral Louis Mountbatten left Combined Operations, though by this time SOE had its own transport and had no need to rely on Combined Operations for resources. On the other hand, the Admiralty objected to SOE developing its own underwater vessels, and the duplication of effort this involved. The Royal Air Force, and in particular RAF Bomber Command under "Bomber" Harris also objected to aircraft being allocated to SOE.
SOE's relationships with the Foreign Office were difficult on several occasions, as various governments in exile protested at operations taking place without their knowledge or approval, which resulted in Axis reprisals against civilian populations. SOE nevertheless generally adhered to the rule, "No bangs without Foreign Office approval." There was also tension between SOE and SIS, which the Foreign Office controlled. Where SIS preferred placid conditions in which it could gather intelligence and work through influential persons or authorities, SOE was intended to create unrest and turbulence, and often backed anti-establishment organisations, such as the Communists, in several countries. At one stage, SIS actively hindered SOE's attempts to infiltrate agents into enemy-occupied France.
Towards the end of the war, as Allied forces began to liberate territories occupied by the Axis and in which SOE had established resistance forces, SOE also liaised with and to some extent came under the control of the Allied theatre commands. Relationships with Supreme Headquarters Allied Expeditionary Force in north-west Europe (whose commander was General Dwight D. Eisenhower) and South East Asia Command (whose commander was Admiral Louis Mountbatten, already well known to SOE) were generally excellent. However, there were continued difficulties with the Commanders in Chief in the Mediterranean, partly because of the complaints over impropriety at SOE's Cairo headquarters during 1941 and partly because both the supreme command in the Mediterranean and SOE's establishments were split in 1942 and 1943, leading to divisions of responsibility and authority.
Dissolution.
Towards the end of the war (in late 1944), Lord Selborne advocated keeping SOE, or a similar body, in being and that it would report to the Ministry of Defence. The Foreign Secretary, Anthony Eden, insisted that his ministry, already responsible for the SIS, should control SOE or its successors. The debate continued for several months until on 22 May 1945, Selborne wrote:
In view of the Russian menace, the situation in Italy, Central Europe and the Balkans and the smouldering volcanoes in the Middle East, I think it would be madness to allow SOE to be stifled at this juncture. In handing it over to the Foreign Office, I cannot help feeling that to ask Sir Orme Sergent [shortly to become Permanent Under-Secretary of State for Foreign Affairs] to supervise SOE is like inviting an abbess to supervise a brothel! But SOE is no base instrument, it is a highly specialized weapon which will be required by HMG whenever we are threatened and whenever it is necessary to contact the common people of foreign lands.
Churchill took no immediate decision, and after he lost the general election on 5 July 1945, the matter was dealt with by the Labour Prime Minister, Clement Attlee. Selborne told Attlee that SOE still possessed a worldwide network of clandestine radio networks and sympathisers. Attlee replied that he had no wish to own a British Comintern, and closed Selborne's network down at 48 hours' notice.
SOE was dissolved officially on 15 January 1946. Some of its senior staff moved easily into financial services in the City of London, although some of them had not lost their undercover mentality and did little for the City's name. Most of SOE's other personnel reverted to their peacetime occupations or regular service in the armed forces, but 280 of them were taken into the "Special Operations Branch" of MI6. Some of these had served as agents in the field, but MI6 was most interested in SOE's training and research staff. Sir Stewart Menzies, the head of MI6 (who was generally known simply as "C") soon decided that a separate branch was unsound, and merged it into the general body of MI6.
Gubbins was not given further employment by the Army, but he later founded the Special Forces Club for former members of SOE and similar organisations.
Locations.
SOE maintained a large number of training, research and development or administrative centres. It was a joke that "SOE" stood for "Stately 'omes of England", after the large number of country houses and estates it requisitioned and used.
After working from temporary offices in Central London, the headquarters of SOE was moved on 31 October 1940 into 64 Baker Street (hence the nickname "the Baker Street Irregulars"). Ultimately, SOE occupied much of the western side of Baker Street.
The establishments connected with experimentation and production of equipment were mainly concentrated in Hertfordshire and were designated by roman numbers. The main weapons and devices research establishments were "The Firs", the home of MD1 near Aylesbury in Buckinghamshire, and Station IX at The Frythe, a former hotel outside Welwyn Garden City where, under the cover name of ISRB (Inter Services Research Bureau), SOE developed radios, weapons, explosive devices and booby traps. Station XII at Aston House near Stevenage in Hertfordshire originally conducted research and development but later became a production, storage and distribution centre for devices already developed.
Station XV, at the Thatched Barn near Borehamwood, was devoted to camouflage, which usually meant equipping agents with authentic local clothing and personal effects. Various sub-stations in London, and Station XIV near Roydon in Essex which specialised in forgery of identity papers, ration books and so on, were also involved in this task. Station XV and other camouflage sections also devised methods of hiding weapons, explosives or radios in innocuous-seeming items.
The training establishments and properties used by country sections were widely distributed and were designated by arabic numbers. The initial training centres of the SOE were at country houses such as Wanborough Manor, Guildford. Agents destined to serve in the field underwent commando training at Arisaig in Scotland, where they were taught armed and unarmed combat skills by William E. Fairbairn and Eric A. Sykes, former Inspectors in the Shanghai Municipal Police. They then attended courses in security and "tradecraft" at Group B schools around Beaulieu in Hampshire. Finally, they received specialist training in skills such as demolition techniques or Morse code telegraphy at various country houses in England and parachute training (if necessary) by STS 51 and 51a situated near Altrincham, Cheshire with the assistance of No.1 Parachute Training School RAF, at RAF Ringway (which later became Manchester Airport).
A commando training centre similar to Arisaig and run by Fairbairn was later set up at Oshawa, for Canadian members of SOE and members of the newly created American organisation, the Office of Strategic Services.
Agents.
A variety of people from all classes and pre-war occupations served SOE in the field. The backgrounds of agents in F Section, for example, ranged from Indian royalty (Noor Inayat Khan) to working class, with some even reputedly from the criminal underworld.
In most cases, the primary quality required of an agent was a deep knowledge of the country in which he or she was to operate, and especially its language, if the agent was to pass as a native of the country. Dual nationality was often a prized attribute. This was particularly so of France. In other cases, especially in the Balkans, a lesser degree of fluency was required as the resistance groups concerned were already in open rebellion and a clandestine existence was unnecessary. A flair for diplomacy combined with a taste for rough soldiering was more necessary. Some regular army officers proved adept as envoys, although others (such as the former diplomat Fitzroy Maclean or the classicist Christopher Woodhouse) were commissioned only during wartime.
Exiled or escaped members of the armed forces of some occupied countries were obvious sources of agents. This was particularly true of Norway and the Netherlands. In other cases (such as Frenchmen owing loyalty to Charles de Gaulle and especially the Poles), the agents' first loyalty was to their leaders or governments in exile, and they treated SOE only as a means to an end. This could occasionally lead to mistrust and strained relations in Britain.
The organisation was prepared to ignore almost any contemporary social convention in its fight against the Axis. It employed known homosexuals, people with criminal records (some of whom taught skills such as lock-picking) or bad conduct records in the armed forces, Communists and anti-British nationalists. Although some of these might have been considered a security risk, there is practically no known case of an SOE agent wholeheartedly going over to the enemy. However, there were cases such as that of Henri Déricourt, in which the conduct of agents was questionable but it was impossible to establish whether they were acting under secret orders from SOE or MI6.
SOE was also far ahead of contemporary attitudes in its use of women in armed combat. Although women were first considered only as couriers in the field or as wireless operators or administrative staff in Britain, those sent into the field were trained to use weapons and in unarmed combat. Most were commissioned into either the First Aid Nursing Yeomanry (FANY) or the Women's Auxiliary Air Force. Some (such as Pearl Witherington) became the organisers of resistance networks. Others such as Odette Hallowes or Violette Szabo were decorated for bravery, posthumously in Szabo's case. Of SOE's 55 female agents, thirteen were killed in action or died in Nazi concentration camps.
Communications.
Radio.
Most of the resistance networks which SOE formed or liaised with were controlled by radio directly from Britain or one of SOE's subsidiary headquarters. All resistance circuits contained at least one wireless operator, and all drops or landings were arranged by radio, except for some early exploratory missions sent "blind" into enemy-occupied territory.
At first, SOE's radio traffic went through the SIS-controlled radio station at Bletchley Park. From 1 June 1942 SOE used its own transmitting and receiving stations at Grendon Underwood and Poundon, both in Buckinghamshire (and near Bletchley Park; the location and topography were suitable for all three sites). Teleprinters linked the radio stations with SOE's HQ in Baker Street. Operators in the Balkans worked to radio stations in Cairo.
SOE was highly dependent upon the security of radio transmissions. There were three factors involved in this: the physical qualities and capabilities of the radio sets, the security of the transmission procedures and the provision of proper ciphers.
SOE's first radios were supplied by SIS. They were large, clumsy and required large amounts of power. SOE acquired a few, much more suitable, sets from the Poles in exile, but eventually designed and manufactured their own, such as the Paraset. The A Mk III, with its batteries and accessories, weighed only 9 lb, and could fit into a small attache case, although the 3 Mk II, otherwise known as the B2, which weighed 32 lb, was required to work over ranges greater than about 500 mi.
Operating procedures were insecure at first. Operators were forced to transmit verbose messages on fixed frequencies and at fixed times and intervals. This allowed German direction finding teams time to triangulate their positions. After several operators were captured or killed, procedures were made more flexible and secure. The SOE wireless operators were also known as "The Pianists".
As with their first radio sets, SOE's first ciphers were inherited from SIS. Leo Marks, SOE's chief cryptographer, was responsible for the development of better codes to replace the insecure poem codes. Eventually, SOE settled on single use ciphers, printed on silk. Unlike paper, which would be given away by rustling, silk would not be detected by a casual search if it was concealed in the lining of clothing.
British Broadcasting Corporation.
The BBC also played its part in communications with agents or groups in the field. During the war, it broadcast to almost all Axis-occupied countries, and was avidly listened to, even at risk of arrest. The BBC included various "personal messages" in its broadcasts, which could include lines of poetry or apparently nonsensical items. They could be used to announce the safe arrival of an agent or message in London for example, or could be instructions to carry out operations on a given date.
Other methods.
In the field, agents could sometimes make use of the postal services, though these were slow, not always reliable and letters were almost certain to be opened and read by the Axis security services. In training, agents were taught to use a variety of easily available substances to make invisible ink, though most of these could be detected by a cursory examination, or to hide coded messages in apparently innocent letters. The telephone services were even more certain to be intercepted and listened to by the enemy, and could be used only with great care.
The most secure method of communication in the field was by courier. In the earlier part of the war, most women sent as agents in the field were employed as couriers, on the assumption that they would be less likely to be suspected of illicit activities.
Equipment.
Weapons.
Although SOE used some silenced assassination weapons such as the De Lisle carbine and the Welrod (specifically developed for SOE at Station IX), it took the view that weapons issued to resisters should not require extensive training in their use, or need careful maintenance. The crude and cheap Sten was a favourite. For issue to large forces such as the Yugoslav Partisans, SOE used captured German or Italian weapons. These were available in large quantities after the Tunisian and Sicilian campaigns and the surrender of Italy, and the partisans could acquire ammunition for these weapons (and the Sten) from enemy sources.
SOE also adhered to the principle that resistance fighters would be handicapped rather than helped by heavy equipment such as mortars or anti-tank guns. These were awkward to transport, almost impossible to conceal and required skilled and highly trained operators. Later in the war however, when resistance groups staged open rebellions against enemy occupation, some heavy weapons were dispatched, for example to the Maquis du Vercors.
Most SOE agents received training on captured enemy weapons before being sent into enemy-occupied territory. Ordinary SOE agents were also armed with handguns acquired abroad, such as, from 1941, a variety of US pistols, and a large quantity of the Spanish Llama .38 ACP in 1944. Such was SOE's demand for weapons, a consignment of 8,000 Ballester-Molina .45 calibre weapons was purchased from Argentina, apparently with the mediation of USA.
SOE agents were issued with the Fairbairn–Sykes fighting knife also issued to Commandos. For specialised operations or use in extreme circumstances, SOE issued small fighting knives which could be concealed in the heel of a hard leather shoe or behind a coat lapel. Given the likely fate of agents captured by the Gestapo, SOE also disguised suicide pills as coat buttons.
Sabotage.
SOE developed a wide range of explosive devices for sabotage, such as limpet mines, shaped charges and time fuses. These were also used by commando units. SOE pioneered the use of plastic explosive. (The term "plastique" comes from plastic explosive packaged by SOE and originally destined for France but taken to the United States instead.) Plastic explosive could be shaped and cut to perform almost any demolition task. It was also inert and required a powerful detonator to cause it to explode, and was therefore safe to transport and store. It was used in everything from car bombs, to exploding rats designed to destroy coal-fired boilers.
Other, more subtle sabotage methods included lubricants laced with grinding materials, incendiaries disguised as innocuous objects, explosive material concealed in coal piles to destroy locomotives, and land mines disguised as cow or elephant dung. On the other hand, some sabotage methods were extremely simple but effective, such as using sledgehammers to crack cast-iron mountings for machinery.
Submarines.
Station IX developed several miniature submersible craft. The Welman submarine and "Sleeping Beauty" were offensive weapons, intended to place explosive charges on or adjacent to enemy vessels at anchor. The Welman was used once or twice in action, but without success. The Welfreighter was intended to deliver stores to beaches or inlets, but it too was unsuccessful.
A sea trials unit was set up in West Wales at Goodwick, by Fishguard (station IXa) where these craft were tested. In late 1944 craft were dispatched to Australia to the Allied Intelligence Bureau (SRD), for tropical testing.
Other.
SOE also revived some medieval devices, such as the caltrop, which could be used to burst the tyres of vehicles or injure foot soldiers and crossbows powered by multiple rubber bands to shoot incendiary bolts. There were two types, known as "Big Joe" and "Li‍ '​l Joe" respectively. They had tubular alloy skeleton stocks and were designed to be collapsible for ease of concealment.
An important section of SOE was the Operation Research and Trials Section, which was formally established in August 1943. The section had the responsibility both for issuing formal requirements and specifications to the relevant development and production sections, and for testing prototypes of the devices produced under conditions which closely matched those to be expected in the field. Over the twelve-month period from 1 November 1943 to 1 November 1944 for example, the section tested 78 devices. Some of these were weapons such as the Sleeve gun, or fuses or adhesion devices to be used in sabotage, others again were utility objects such as waterproof containers for stores to be dropped by parachute or night glasses (lightweight binoculars with plastic lenses). Of the devices tested, 47% were accepted for use with little or no modification, 31% were accepted only after considerable modification and the remaining 22% were rejected.
Before SOE's research and development procedures were formalised in 1943, a variety of more or less useful devices were developed. Some of the more imaginative devices invented by SOE included exploding pens with enough explosive power to blast a hole in the bearer's body, or guns concealed in tobacco pipes, though there is no record of any of these being used in action.
Transport.
The continent of Europe was largely closed to normal travel. Although it was possible in some cases to cross frontiers from neutral countries such as Spain or Sweden, this was slow and there were issues over violating these countries' neutrality. SOE had to rely largely on its own air or sea transport for movement of people, arms and equipment.
Air.
SOE was engaged in disputes with the RAF from its early days. In January 1941, an intended ambush (Operation Savanna) against the aircrew of a German "pathfinder" air group near Vannes in Brittany was thwarted when Air Vice Marshal Charles Portal, the Chief of the Air Staff, objected on moral grounds to parachuting what he regarded as assassins. Although Portal's objections were later overcome (and "Savanna" was mounted, unsuccessfully), Air Marshal Harris ("Bomber Harris"), the Commander-in-Chief of Bomber Command, resented the diversion of bombers to SOE purposes (or indeed any purposes other than the offensive against German cities). He too was overruled and by April 1942, SOE had the services of 138 and 161 squadrons at RAF Tempsford.
The aircraft used by SOE included the Westland Lysander, which could carry up to three passengers and two panniers loaded with stores, and had an effective range of 700 mi. It could use rough landing strips only 400 yd in length, or even less. Lysanders were used to transport 101 agents to and 128 agents from Nazi-occupied Europe. The Lockheed Hudson had a range 200 mi greater and could carry more passengers (ten or more), but required landing strips twice as long as those needed for the Lysander.
To deliver agents and stores by parachute, SOE could use several aircraft originally designed as bombers: the Armstrong Whitworth Whitley until November 1942, the Handley Page Halifax and the Short Stirling. The Stirling could carry a particularly large load, but only the Halifax had the range to reach dropping zones in eastern Poland (and even then, only from bases in Southern Italy). Later in the war, SOE also used the American-supplied Douglas Dakota, which was often landed at airfields in territory held by partisans in the Balkans.
Stores were usually parachuted in cylindrical containers. The "C" type was 69 in long and when fully loaded could weigh up to 224 lb. The "H" type was the same size overall but could be broken down into five smaller sections. This made it easier to carry and conceal but made it impossible to carry long loads such as rifles. Some inert stores such as boots and blankets were "free-dropped" i.e. simply thrown out of the aircraft bundled together without a parachute, often to the hazard of any receiving committee on the ground.
Station IX developed a miniature folding motorbike (the "Welbike") for use by parachutists, though this was noisy and conspicuous, and was of little use on rough ground.
Locating and homing equipment.
Some devices used by SOE were designed specifically to guide aircraft to landing strips and dropping zones. Such sites could be marked by an agent on the ground with bonfires or bicycle lamps, but this required good visibility, as the pilot of an aircraft had not only to spot the ground signals, but also to navigate by visible landmarks to correct dead reckoning. Many landings or drops were thwarted by bad weather. To overcome these problems, SOE and Allied airborne forces used the Rebecca/Eureka transponding radar, which enabled an aircraft to home in on a point on the ground even in thick weather. It was however difficult to carry or conceal. SOE also developed the S-Phone, which allowed a pilot or radio operator aboard an aircraft to communicate by voice with the "reception committee". Sound quality was good enough for voices to be recognisable, so that a mission could be aborted if there was any doubt of an agent's identity.
Sea.
SOE also experienced difficulties with the Royal Navy, who were usually unwilling to allow SOE to use its submarines or motor torpedo boats to deliver agents or equipment. Submarines were regarded as too valuable to risk within range of enemy coastal defences, and MTBs were in any case often too noisy and conspicuous for clandestine landings. However, SOE often used clandestine craft such as local fishing boats or caiques and eventually ran quite large fleets of these, from the Helford estuary, Algiers, the Shetland Islands (a service termed the Shetland Bus), Ceylon etc.
Operations.
France.
SOE's operations were usually mounted in order to feel out resistance groups willing to work with the Allies in preparation for invasion. In France, personnel were directed by two London-based country sections. F Section was under British control, while RF Section was linked to General de Gaulle's Free French government in exile. Most native French agents served in RF. There were also two smaller sections: EU/P Section, which dealt with the Polish community in France, and the DF Section which was responsible for establishing escape routes. During the latter part of 1942 another section known as AMF was established in Algiers, to operate into Southern France.
On 5 May 1941, Georges Bégué (1911–1993) became the first SOE agent dropped into German occupied France. He then set up radio communications and met the next agents parachuted into France. Between Bégué's first drop in May 1941 and August 1944, more than four hundred F Section agents were sent into occupied France. They served in a variety of functions including arms and sabotage instructors, couriers, circuit organisers, liaison officers and radio operators. RF sent about the same number; AMF sent 600 (although not all of these belonged to SOE). EU/P and DF sent a few dozen agents each.
SOE included a number of women (who were often commissioned into women's branches of the armed forces such as the First Aid Nursing Yeomanry). F Section alone sent 39 female agents into the field, of whom 13 did not return. The Valençay SOE Memorial was unveiled at Valençay in the Indre "département" of France on 6 May 1991, marking the fiftieth anniversary of the despatch of F Section's first agent to France. The memorial's roll of honour lists the names of the 91 men and 13 women members of the SOE who gave their lives for France's freedom.
To support the Allied invasion of France on D Day in June 1944, three-man parties were dropped into various parts of France as part of Operation Jedburgh, to coordinate widespread overt (as opposed to clandestine) acts of resistance. A total of 100 men were eventually dropped, together with 6,000 tons of military stores (4,000 tons had been dropped during the years before D-Day). At the same time, all the various sections operating in France (except EU/P) were nominally placed under a London-based HQ titled État-major des Forces Françaises de l'Intérieur (EMFFI).
Poland.
SOE did not need to instigate Polish resistance, because unlike the Vichy French the Poles overwhelmingly refused to collaborate with the Nazis. Early in the war the Poles established the Polish Home Army, led by a clandestine resistance government known as the Polish Secret State. Nevertheless, there were many Polish members of SOE and much cooperation between the SOE and the Polish resistance.
SOE assisted the Polish government in exile with training facilities and logistical support for its 605 special forces operatives known as the Cichociemni, or "The Dark and Silent". Members of the unit, which was based in Audley End House, Essex, were rigorously trained before being parachuted into occupied Poland. Because of the distance involved in air travel to Poland, customised aircraft with extra fuel capacity were used in Polish operations such as Operation Wildhorn III. Sue Ryder chose the title Baroness Ryder of Warsaw in honour of these operations.
Secret Intelligence Service member Krystyna Skarbek ("nom de guerre" Christine Granville) was a founder member of SOE and helped establish a cell of Polish spies in Central Europe. She ran several operations in Poland, Egypt, Hungary (with Andrzej Kowerski) and France, often using the staunchly anti-Nazi Polish expatriate community as a secure international network. Non-official cover agents Elzbieta Zawacka and Jan Nowak-Jezioranski perfected the Gibraltar courier route out of occupied Europe. Maciej Kalenkiewicz was parachuted into occupied Poland, only to be killed by the Soviets. A Polish agent was integral to SOE's Operation Foxley, the plan to assassinate Hitler.
Thanks to cooperation between SOE and the Polish Home Army, the Poles were able to deliver the first Allied intelligence on the Holocaust to London in June 1942. Witold Pilecki of the Polish Home Army designed a joint operation with SOE to liberate Auschwitz, but the British rejected it as infeasible. Joint Anglo-Polish operations provided London with vital intelligence on the V-2 rocket, German troops movements on the Eastern Front, and the Soviet repressions of Polish citizens.
RAF 'Special Duties Flights' were sent to Poland to assist the Warsaw Uprising against the Nazis. The rebellion was defeated with a loss of 200,000 casualties (mostly German executions of Polish civilians) after the nearby Red Army refused military assistance to the Polish Home Army. RAF Special Duties Flights were refused landing rights at Soviet-held airfields near Warsaw, even when requiring emergency landings after battle damage. These flights were also attacked by Soviet fighters, despite the U.S.S.R.'s officially Allied status.
Germany.
Due to the dangers and lack of friendly population few operations were conducted in Germany itself. The German and Austrian section of SOE was run by Lieutenant Colonel Ronald Thornley for most of the war, and was mainly involved with black propaganda and administrative sabotage in collaboration with the German section of the Political Warfare Executive. After D-Day, the section was re-organised and enlarged with Major General Gerald Templer heading the Directorate, with Thornley as his deputy.
Several major operations were planned, including Operation "Foxley", a plan to assassinate Hitler, and Operation "Periwig", an ingenious plan to simulate the existence of a large-scale anti-Nazi resistance movement within Germany. "Foxley" was never carried out but "Periwig" went ahead despite restrictions placed on it by SIS and SHAEF. Several German prisoners of war were trained as agents, briefed to make contact with the anti-Nazi resistance and to conduct sabotage. They were then parachuted into Germany in the hope that they would either hand themselves in to the "Gestapo" or be captured by them, and reveal their supposed mission. Fake coded wireless transmissions were broadcast to Germany and various pieces of agent paraphernalia such as code books and wireless receivers were allowed to fall into the hands of the German authorities.
The Netherlands.
Section N of SOE ran operations in the Netherlands. They committed some of SOE's worst blunders in security, which allowed the Germans to capture many agents and much sabotage material, in what the Germans called the 'Englandspiel'. SOE apparently ignored the absence of security checks in radio transmissions, and other warnings from their chief cryptographer, Leo Marks, that the Germans were running the supposed resistance networks. A total of 50 agents were caught and brought to Camp Haaren in the South of the Netherlands.
Five captured men managed to escape from the camp. Two of them, Pieter Dourlein and Ben Ubbink, escaped on 29 August 1943 and found their way to Switzerland. There, the Netherlands Embassy sent messages over their controlled sets to England that SOE Netherlands was compromised. SOE set up new networks, which continued to operate until the Netherlands were liberated at the end of the war.
Belgium.
Section T established some effective networks in Belgium, in part orchestrated by fashion designer Hardy Amies, who rose to the rank of Lieutenant Colonel. Amies adapted names of fashion accessories for use as code words, while managing some of the most murderous and ruthless agents in the field.
In the aftermath of the Battle of Normandy, British armoured forces liberated the country in less than a week, giving the resistance little time to stage an uprising. They did assist British forces to bypass German rearguards, and this allowed the Allies to capture the vital docks at Antwerp intact (although a protracted and bloody Battle of the Scheldt was later fought to clear the Scheldt estuary before the Allies could use the port).
After Brussels was liberated, Amies outraged his superiors by setting up a "Vogue" photo-shoot in Belgium. In 1946, he was Knighted in Belgium for his service with SOE, being a Named Officier de l'Ordre de la Couronne.
Italy.
As both an enemy country, and supposedly a monolithic fascist state with no organised opposition which SOE could use, SOE made little effort in Italy before mid-1943, when Mussolini's government collapsed and Allied forces already occupied Sicily. In April 1941, in a mission codenamed "Yak", Peter Fleming attempted to recruit agents from among the many thousands of Italian prisoners of war captured in the Western Desert Campaign. He met with no response. Attempts to search among Italian immigrants in the United States, Britain and Canada for agents to be sent to Italy had similarly poor results.
During the first three years of war, the most important "episode" of the collaboration between SOE and Italian anti-fascism was a project of an anti-fascist uprising in Sardinia, which the SOE supported at some stage but did not receive approval from the Foreign Office.
In the aftermath of the Italian collapse, SOE (in Italy renamed N. 1 Special Force) helped build a large resistance organisation in the cities of Northern Italy, and in the Alps. Italian partisans harassed German forces in Italy throughout the autumn and winter of 1944, and in the Spring 1945 offensive in Italy they captured Genoa and other cities unaided by Allied forces. SOE helped the Italian Resistance send British missions to the partisan formations and supply war material to the bands of patriots, a supply made without political prejudices, and which also helped the Communist formations (Brigate Garibaldi).
Late in 1943, SOE established a base at Bari in Southern Italy, from which they operated their networks and agents in the Balkans. This organisation had the codename "Force 133". This later became "Force 266", reserving 133 for operations run from Cairo rather than the heel of Italy. Flights from Brindisi were run to the Balkans and Poland, particularly once control had been wrested from SOE's Cairo headquarters and was exercised directly by Gubbins. SOE established a new packing station for the parachute containers close to Brindisi Air base, along the lines of those created at Saffron Walden. This was ME 54, a factory employing hundreds, the American (OSS) side of which was known as "Paradise Camp".
Yugoslavia.
In the aftermath of the German invasion in 1941, the Kingdom of Yugoslavia fragmented. In Croatia, there was a substantial pro-Axis movement, the Ustaše. In Croatia as well as the remainder of Yugoslavia, two resistance movements formed; the royalist Chetniks under Draža Mihailović, and the Communist Partisans under Josip Broz Tito.
Mihailović was the first to attempt to contact the Allies, and SOE despatched a party on 20 September 1941 under Major "Marko" Hudson. Hudson also encountered Tito's forces. Through the royalist government in exile, SOE at first supported the Chetniks. Eventually, however, due to reports that the Chetniks were less effective and even collaborating with German and Italian forces on occasion, British support was redirected to the Partisans, even before the Tehran Conference in 1943.
Although relations were often touchy throughout the war, it can be argued that SOE's unstinting support was a factor in Yugoslavia's maintaining a neutral stance during the Cold War. However, accounts vary dramatically between all historical works on the "Chetnik controversy".
Hungary.
SOE was unable to establish links or contacts in Hungary before the regime of Miklós Horthy aligned itself with the Axis Powers. Distance and lack of such contacts prevented any effort being made by SOE until the Hungarians themselves dispatched a diplomat (László Veress) in a clandestine attempt to contact the Western Allies. SOE facilitated his return, with some radio sets. Before the Allied governments could agree terms, Hungary was placed under German military occupation and Veress was forced to flee the country.
Two missions subsequently dropped "blind" i.e. without prior arrangement for a reception party, failed. So too did an attempt by Basil Davidson to incite a partisan movement in Hungary, after he made his way there from northeastern Yugoslavia.
Greece.
Greece was overrun by the Axis after a desperate defence lasting several months. In the aftermath, SIS and another intelligence organisation, SIME, discouraged attempts at sabotage or resistance as this might imperil relations with Turkey, although SOE maintained contacts with resistance groups in Crete. When an agent, "Odysseus", a former tobacco-smuggler, attempted to contact potential resistance groups in Greece, he reported that no group was prepared to cooperate with the monarchist government in exile in Cairo.
In late 1942, at the army's instigation, SOE mounted its first operation, codenamed Operation "Harling", into Greece in an attempt to disrupt the railway which was being used to move materials to the German Panzer Army Africa. A party under Colonel (later Brigadier) Eddie Myers, assisted by Christopher Woodhouse, was parachuted into Greece and discovered two guerrilla groups operating in the mountains: the pro-Communist ELAS and the republican EDES. On 25 November 1942, Myers's party blew up one of the spans of the railway viaduct at Gorgopotamos, supported by 150 Greek partisans from these two organisations who engaged Italians guarding the viaduct. This cut the railway linking Thessaloniki with Athens and Piraeus.
Relations between the resistance groups and the British soured. When the British needed once again to disrupt the railway across Greece as part of the deception operations preceding Operation "Husky", the Allied invasion of Sicily, the resistance groups refused to take part, rightly fearing German reprisals against civilians. Instead, a six-man commando party from the British and New Zealand armies carried out the destruction of the Asopos viaduct on 21 June 1943.
EDES received most aid from SOE, but ELAS secured many weapons when Italy collapsed and Italian military forces in Greece dissolved. ELAS and EDES fought a vicious civil war in 1943 until SOE brokered an uneasy armistice (the Plaka agreement).
A lesser known, but important function of the SOE in Greece was to inform the Cairo headquarters of the movement of the German military aircraft that were serviced and repaired at the two former Greek military aircraft facilities in and around Athens.
Eventually, the British Army occupied Athens and Piraeus in the aftermath of the German withdrawal, and fought a street-by-street battle to drive ELAS from these cities and impose an interim government under Archbishop Damaskinos. SOE's last act was to evacuate several hundred disarmed EDES fighters to Corfu, preventing their massacre by ELAS.
Crete.
In Crete there were several resistance groups and Allied stay-behind parties after the Germans occupied the island in the Battle of Crete. SOE's operations on Crete involved figures such as Patrick Leigh Fermor, John Lewis, Harry Rudolph Fox Burr, Tom Dunbabin, Sandy Rendel, John Houseman, Xan Fielding and Bill Stanley Moss. Some of the most famous moments included the abduction of General Heinrich Kreipe led by Leigh Fermor and Moss - subsequently portrayed in the film "Ill Met by Moonlight", and the sabotage of Damasta led by Moss.
Albania.
Albania had been under Italian influence since 1923, and was occupied by the Italian Army in 1939. In 1943, a small liaison party entered Albania from northwestern Greece. SOE agents who entered Albania then or later included Julian Amery, Anthony Quayle, David Smiley and Neil "Billy" McLean. They discovered another internecine war between the Communist partisans under Enver Hoxha, and the republican Balli Kombëtar. As the latter had collaborated with the Italian occupiers, Hoxha gained Allied support.
SOE's envoy to Albania, Brigadier Edmund "Trotsky" Davies, was captured by the Germans early in 1944. Some SOE officers warned that Hoxha's aim was primacy after the war, rather than fighting Germans. They were ignored, but Albania was never a major factor in the effort against the Germans.
Czechoslovakia.
SOE sent many missions into the Czech areas of the so-called Protectorate of Bohemia and Moravia, and later into Slovakia. The most famous mission was Operation Anthropoid, the assassination of SS-Obergruppenführer Reinhard Heydrich in Prague. From 1942 to 1943 the Czechoslovaks had their own Special Training School (STS) at Chicheley Hall in Buckinghamshire. In 1944, SOE sent men to support the Slovak National Uprising.
Norway.
In March 1941 a group performing commando raids in Norway, Norwegian Independent Company 1 (NOR.I.C.1) was organised under leadership of Captain Martin Linge. Their initial raid in 1941 was Operation Archery, the best known raid was probably the Norwegian heavy water sabotage. Communication lines with London were gradually improved so that by 1945, 64 radio operators were spread throughout Norway.
Denmark.
Most of the actions conducted by the Danish resistance were railway sabotage to hinder German troop and material movements from and to Norway. However, there were examples of sabotage on a much larger scale especially by BOPA. In all over 1,000 operations were conducted from 1942 and onwards.
In October 1943 the Danish resistance also saved nearly all of the Danish Jews from certain death in German concentration camps. This was a massive overnight operation and is to this day recognised among Jews as one of the most significant displays of public defiance against the Germans.
The Danish resistance assisted SOE in its activities in neutral Sweden. For example, SOE was able to obtain several shiploads of vital ball-bearings which had been interned in Swedish ports. The Danes also pioneered several secure communications methods; for example, a burst transmitter/receiver which transcribed Morse code onto a paper tape faster than a human operator could handle.
Romania.
In 1943 an SOE delegation was parachuted into Romania to instigate resistance against the Nazi occupation at "any cost" (Operation Autonomous). The delegation, including Colonel Gardyne de Chastelain, Captain Silviu Meţianu and Ivor Porter, was captured by the Romanian Gendarmerie and held until the night of King Michael's Coup on 23 August 1944.
Other operations in Europe.
Through cooperation with the Special Operations Executive and the British intelligence service, a group of Jewish volunteers from Palestine were sent on missions to several countries in Nazi-occupied Europe from 1943 to 1945.
Abyssinia.
Abyssinia was the scene of some of SOE's earliest and most successful efforts. SOE organised a force of Ethiopian irregulars under Orde Charles Wingate in support of the exiled Emperor Haile Selassie. This force (named Gideon Force by Wingate) caused heavy casualties to the Italian occupation forces, and contributed to the successful British campaign there. Wingate was to use his experience to create the Chindits in Burma.
Southeast Asia.
As early as 1940, SOE was preparing plans for operations in Southeast Asia. As in Europe, after initial Allied military disasters, SOE built up indigenous resistance organisations and guerrilla armies in enemy (Japanese) occupied territory. SOE also launched "Operation Remorse" (1944–45), which was ultimately aimed at protecting the economic and political status of Hong Kong. Force 136 engaged in covert trading of goods and currencies in China. Its agents proved remarkably successful, raising £77m through their activities, which were used to provide assistance for Allied prisoners of war and, more controversially, to buy influence locally in order to facilitate a smooth return to pre-war conditions.
Later analysis and commentaries.
The mode of warfare encouraged and promoted by SOE is considered by several modern commentators to have established the modern model that many alleged terrorist organisations emulate.
Two opposed views were quoted by Tony Geraghty in "The Irish War: The Hidden Conflict Between the IRA and British Intelligence". M. R. D. Foot, who wrote several official histories of SOE wrote:
The Irish [thanks to the example set by Collins and followed by the SOE] can thus claim that their resistance provide the originating impulse for resistance to tyrannies worse than any they had to endure themselves. And the Irish resistance as Collins led it, showed the rest of the world an economical way to fight wars the only sane way they can be fought in the age of the Nuclear bomb.
However the British military historian John Keegan wrote:
We must recognise that our response to the scourge of terrorism is compromised by what we did through SOE. The justification ... That we had no other means of striking back at the enemy ... is exactly the argument used by the Red Brigades, the Baader-Meinhoff gang, the PFLP, the IRA and every other half-articulate terrorist organisation on Earth. Futile to argue that we were a democracy and Hitler a tyrant. Means besmirch ends. SOE besmirched Britain.

</doc>
<doc id="28899" url="http://en.wikipedia.org/wiki?curid=28899" title="System request">
System request

System request (often abbreviated SysRq or Sys Req) is a key on keyboards for PCs that has no standard use. This key can be traced back to the operator interrupt key used on IBM 3270-type console keyboards of the IBM System/370 mainframe computer, which was used to cause the operating system such as VM/370 or MVS to allow the console to give input to the operating system.
History.
Introduced by IBM with the PC/AT, it was intended to be available as a special key to directly invoke low-level operating system functions with no possibility of conflicting with any existing software. A special BIOS routine — software interrupt 0x15, subfunction 0x85 — was added to signal the OS when SysRq was pushed or released. Unlike most keys, when it is pressed nothing is stored in the keyboard buffer.
The specific low level function that the SysRq key was meant for was to switch between operating systems. When the original IBM-PC was created in 1980, there were three leading competing operating systems: PC DOS, CP/M-86, and UCSD p-System, while Xenix was added in 1983-1984. The SysRq key was added so that multiple operating systems could be run on the same computer, making use of the capabilities of the 286 chip in the PC/AT.
A special key was needed because most software of the day operated at a low level, often bypassing the OS entirely, and typically made use of many hotkey combinations. The use of Terminate and Stay Resident (TSR) programs further complicated matters. To implement a task switching or multitasking environment, it was thought that a special, separate key was needed. This is similar to the way “Control-Alt-Delete” is used under Windows NT.
On 84-key keyboards (except the 84-key IBM Model M space saver keyboard), SysRq was a key of its own. On the later 101-key keyboard, it shares a physical key with the Print Screen key function. One must hold down the Alt key while pressing this “dual-function” key to invoke SysRq.
The default BIOS keyboard routines simply ignore SysRq and return without taking action. So did the MS-DOS input routines. The keyboard routines in libraries supplied with many high-level languages followed suit. Although it is still included on most PC keyboards manufactured, and though it is used by some debugging software, the key is of no use for the vast majority of users.
Other uses.
In Linux, the kernel can be configured to provide functions for system debugging and crash recovery. This use is known as the “Magic SysRq key”.
Microsoft has also used SysRq for various OS- and application-level debuggers. In the CodeView debugger, it was sometimes used to break into the debugging during program execution. For the Windows NT remote kernel debugger, it can be used to force the system into the debugger.
On the Hyundai/Hynix Super-16 computer, pressing Ctrl+SysRq will hard boot the system (it will reboot when Ctrl+Alt+Del is unresponsive, and it will invoke startup memory tests that are bypassed on soft-boot).
In embedded systems, SysRq key is usually used to assert low-level on RESET# signal.

</doc>
<doc id="28900" url="http://en.wikipedia.org/wiki?curid=28900" title="Split infinitive">
Split infinitive

In the English language, a split infinitive or cleft infinitive is a grammatical construction in which a word or phrase divides the "to" and the bare infinitive of the "to" form of the infinitive verb. Usually, it is the interrupting adverb or adverbial phrase which comes between "to" and the verb.
A well-known example occurs in the opening sequence of the "Star Trek" television series: "to boldly go where no man has gone before"; the adverb "boldly" splits the infinitive "to go". More rarely, more than one word splits the infinitive in a compound split infinitive, as in: "The population is expected to more than double in the next ten years".
As the split infinitive became more common in the 19th century, some grammatical authorities sought to introduce a prescriptive rule against it. The construction is still the subject of disagreement among English speakers as to whether it is grammatically correct or good style: "No other grammatical issue has so divided English speakers since the split infinitive was declared to be a solecism in the 19c [19th century]: raise the subject of English usage in any conversation today and it is sure to be mentioned." However, most modern English usage guides have dropped the objection to the split infinitive.
History of the construction.
Middle English.
In Old English, infinitives were single words ending in "-n" or "-an" (compare modern Dutch and German "-n", "-en"). Gerunds were formed using "to" followed by a verbal noun in the dative case, which ended in "-anne" or "-enne" (e.g. "tō cumenne" = "coming, to come"). In Middle English, the bare infinitive and the gerund coalesced into the same form ending in "-(e)n" (e.g. "comen" "come"; "to comen" "to come"). The "to" infinitive was not split in Old or Early Middle English.
The first known example of a split infinitive in English, in which a pronoun rather than an adverb splits the infinitive, is in Layamon's "Brut" (early 13th century):
This may be a poetic inversion for the sake of meter, and therefore says little about whether Layamon would have felt the construction to be syntactically natural. However, no such reservation applies to the following prose example from John Wycliffe (14th century), who was fond of splitting infinitives:
Modern English.
After its rise in Middle English, the construction became rare in the 15th and 16th centuries. William Shakespeare used only one, which appears to be a syntactical inversion for the sake of rhyme:
Edmund Spenser, John Dryden, Alexander Pope, and the King James Version of the Bible used none, and they are very rare in the writing of Samuel Johnson. John Donne used them several times, though, and Samuel Pepys also used at least one. No reason for the near disappearance of the split infinitive is known; in particular, no prohibition is recorded.
Split infinitives reappeared in the 18th century and became more common in the 19th.
Daniel Defoe, Benjamin Franklin, William Wordsworth, Abraham Lincoln, George Eliot, Henry James, and Willa Cather are among the writers who used them. Examples in the poems of Robert Burns attest its presence also in 18th century Scots:
However, it was especially in colloquial use that the construction experienced a veritable boom. Today, according to the "American Heritage Book of English Usage", "people split infinitives all the time without giving it a thought." In corpora of contemporary spoken English, some adverbs such as "always" and "completely" appear more often in the split position than the unsplit.
Theories of origins.
Although it is difficult to say why the construction developed in Middle English, or why it revived so powerfully in Modern English, a number of theories have been postulated.
Analogy.
Traditional grammarians have suggested that the construction appeared because people frequently place adverbs before finite verbs. George Curme writes: "If the adverb should immediately precede the finite verb, we feel that it should immediately precede also the infinitive…" Thus if one says:
one may, by analogy, wish to say:
This is supported by the fact that split infinitives are often used as echoes, as in the following exchange, in which the riposte parodies the slightly odd collocation in the original sentence:
Here is an example of an adverb being transferred into split infinitive position from a parallel position in a different construction.
Transformational grammar.
Transformational grammarians have attributed the construction to a re-analysis of the role of "to".
Types.
In the modern language, splitting usually involves a single adverb coming between the verb and its marker. Very frequently, this is an emphatic adverb, for example:
Sometimes it is a negation, as in the self-referential joke:
However, in modern colloquial English almost any adverb may be found in this syntactic position, especially when the adverb and the verb form a close syntactic unit (really-pull, not-split).
Compound split infinitives, i.e., infinitives split by more than one word, usually involve a pair of adverbs or a multi-word adverbial:
Examples of non-adverbial elements participating in the split-infinitive construction seem rarer in Modern English than in Middle English. The pronoun "all" commonly appears in this position:
and may even be combined with an adverb:
This is an extension of the subject pronoun ("you all"). However an object pronoun, as in the Layamon example above, would be unusual in modern English, perhaps because this might cause a listener to misunderstand the "to" as a preposition:
Other parts of speech would be very unusual in this position. However, in verse, poetic inversion for the sake of meter or of bringing a rhyme word to the end of a line often results in abnormal syntax, as with Shakespeare's split infinitive ("to pitied be", cited above), in fact an inverted passive construction in which the infinitive is split by a past participle. Presumably, this would not have occurred in a prose text by the same author.
History of the term.
It was not until the very end of the 19th century that terminology emerged to describe the construction. According to the main etymological dictionaries, the earliest use of the term "split infinitive" on record dates from 1897, with "infinitive-splitting" and "infinitive-splitter" following in 1926 and 1927 respectively. The now rare "cleft infinitive" is slightly older, attested from 1893. The term "compound split infinitive" is not found in these dictionaries and appears to be very recent.
This terminology implies analysing the full infinitive as a two-word infinitive, which not all grammarians accept. As one who used "infinitive" to mean the single-word verb, Otto Jespersen challenged the epithet: "'To' is no more an essential part of an infinitive than the definite article is an essential part of a nominative, and no one would think of calling 'the good man' a split nominative." However, no alternative terminology has been proposed.
History of the controversy.
Although it is sometimes reported that a prohibition on split infinitives goes back to Renaissance times, and frequently the 18th century scholar Robert Lowth is cited as the originator of the prescriptive rule, such a rule is not to be found in Lowth's writing and is not known to appear in any other text prior to the mid-19th century.
Possibly the earliest comment against split infinitives was by an anonymous American in 1834:
I am not conscious, that any rule has been heretofore given in relation to this point [...] The practice, however, of not separating the particle from its verb, is so general and uniform among good authors, and the exceptions are so rare, that the rule which I am about to propose will, I believe, prove to be as accurate as most rules, and may be found beneficial to inexperienced writers. It is this :—"The particle," TO, "which comes before the verb in the infinitive mode, must not be separated from it by the intervention of an adverb or any other word or phrase; but the adverb should immediately precede the particle, or immediately follow the verb."
In 1840, Richard Taylor also condemned split infinitives as a "disagreeable affectation", and in 1859, Solomon Barrett, Jr., called them "a common fault". However, the issue seems not to have attracted wider public attention until Henry Alford addressed it in his "Plea for the Queen's English" in 1864:
A correspondent states as his own usage, and defends, the insertion of an adverb between the sign of the infinitive mood and the verb. He gives as an instance, "to scientifically illustrate". But surely this is a practice entirely unknown to English speakers and writers. It seems to me, that we ever regard the "to" of the infinitive as inseparable from its verb. And, when we have already a choice between two forms of expression, "scientifically to illustrate" and "to illustrate scientifically," there seems no good reason for flying in the face of common usage.
Others quickly followed, among them Bache, 1869 ("The "to" of the infinitive mood is inseparable from the verb"); William B. Hodgson, 1889; and Raub, 1897 ("The sign "to" must not be separated from the remaining part of the infinitive by an intervening word").
Even as these authorities were condemning the split infinitive, others were endorsing it: Brown, 1851 (saying some grammarians had criticized it and it was less elegant than other adverb placements but sometimes clearer); Hall, 1882; Onions, 1904; Jespersen, 1905; and Fowler and Fowler, 1906. Despite the defence by some grammarians, by the beginning of the 20th century the prohibition was firmly established in the press and popular belief. In the 1907 edition of "The King's English", the Fowler brothers wrote:
In large parts of the school system, the construction was opposed with ruthless vigour. A correspondent to the BBC on a programme about English grammar in 1983 remarked:
As a result, the debate took on a degree of passion which the bare facts of the matter never warranted. There was frequent skirmishing between the splitters and anti-splitters until the 1960s. George Bernard Shaw wrote letters to newspapers supporting writers who used the split infinitive, and Raymond Chandler complained to the editor of "The Atlantic Monthly" about a proofreader who changed Chandler's split infinitives:
Post-1960 authorities show a strong tendency to accept the split infinitive. Follett, in "Modern American Usage" (1966) writes: "The split infinitive has its place in good composition. It should be used when it is expressive and well led up to." Fowler (Gower's revised second edition, 1983) offers the following example of the consequences of refusal to split infinitives: "The greatest difficulty about assessing the economic achievements of the Soviet Union is that its spokesmen try "absurdly to exaggerate" them; in consequence the visitor may tend "badly to underrate" them" (italics added). This question results: "Has dread of the split infinitive led the writer to attach the adverbs ['absurdly' and 'badly'] to the wrong verbs, and would he not have done better "to boldly split" both infinitives, since he cannot put the adverbs after them without spoiling his rhythm" (italics added)? Bernstein (1985) argues that, although infinitives should not always be split, they should be split where doing so improves the sentence: "The natural position for a modifier is before the word it modifies. Thus the natural position for an adverb modifying an infinitive should be just . . . "after" the to" (italics added). Bernstein continues: "Curme's contention that the split infinitive is often an improvement . . . cannot be disputed." Heffernan and Lincoln, in their modern English composition textbook, agree with the above authors. Some sentences, they write, "are weakened by . . . cumbersome splitting," but in other sentences "an infinitive may be split by a one-word modifier that would be awkward in any other position."
Principal objections to the split infinitive.
Objections to the split infinitive fall into three categories, of which only the first is accorded any credence by linguists.
The descriptivist objection.
Like most linguistic prescription, disapproval of the split infinitive was originally based on the descriptive observation that it was not in fact a feature of the prestige form of English which those proscribing it wished to champion. This is made explicit in the anonymous 1834 text, the first known statement of the position, and in Alford's objection in 1864, the first truly influential objection to the construction, both cited above. The descriptivist objection involves a person whose idiolect does not have the construction advising against its use on the grounds that it is not the norm: for example, many English speakers avoid split infinitives not because they follow a prescriptive rule, but simply because it was not part of the language that they learned as children. However, as the construction grows in popularity, the strength of the descriptivist objection is progressively reduced.
Many of those who avoid split infinitives differentiate according to type and register. Infinitives split by multi-word phrases ("compound split infinitives") and those split by pronouns are demonstrably less usual than the straightforward example of an infinitive split by an adverb. Likewise, split infinitives are far more common in speech and informal writing than in academic writing. Thus, while an outright rejection of the split infinitive is no longer sustainable on descriptive grounds (as it was in 1834), the advice to avoid it in formal settings, and to avoid some types in particular, remains a tenable position. The prescriptive rule of thumb draws on the descriptive observation that certain split infinitives are not usual in certain situations. For examples of modern linguists using such arguments, see the current views section below.
The argument from the full infinitive.
A second argument is summed up by Alford's statement "It seems to me that we ever regard the "to" of the infinitive as inseparable from its verb."
The "to" in the infinitive construction, which is found throughout the Germanic languages, is originally a preposition before the dative of a verbal noun, but in the modern languages it is widely regarded as a particle which serves as a marker of the infinitive. In German, this marker ("zu") sometimes precedes the infinitive, but is not regarded as part of it. In English, on the other hand, it is traditional to speak of the "bare infinitive" without "to" and the "full infinitive" with it, and to conceive of "to" as part of the full infinitive. (In the sentence "I made my daughter clean her room," "clean" is a bare infinitive; in "She decided to clean her room," "to clean" is a full infinitive.) Possibly this is because the absence of an "inflected" infinitive form made it useful to include the particle in the citation form of the verb, and in some nominal constructions in which other Germanic languages would omit it (e.g. "to know her is to love her"). The concept of a two-word infinitive can reinforce an intuitive sense that the two words belong together. For instance, the rhetorician John Duncan Quackenbos said, ""To have" is as much one thing, and as inseparable by modifiers, as the original form "habban", or the Latin "habere"." The usage writer John Opdycke based a similar argument on the closest French, German, and Latin translations.
The two-part infinitive is disputed, however, and some linguists would say that the infinitive in English is also a single-word verb form, which may or may not be preceded by the particle "to". Some modern generative analysts classify "to" as a "peculiar" auxiliary verb; other analysts, as the infinitival subordinator. Moreover, even when the concept of the full infinitive is accepted, it does not necessarily follow that any two words that belong together grammatically need be adjacent to each other. They usually are, but counter-examples are easily found, such as an adverb splitting a two-word finite verb ("will not do", "has not done").
The argument from classical languages.
A frequently discussed argument states that the split-infinitive prohibition is based on Latin. An infinitive in Latin is never used with a marker equivalent to English "to", and thus there is no parallel there for the construction. The claim that those who dislike split infinitives are applying rules of Latin grammar to English is asserted in many references that accept the split infinitive. One example is in the "American Heritage Book of English Usage": "The only rationale for condemning the construction is based on a false analogy with Latin." In more detail, the usage author Marilyn Moriarty states:
The rule forbidding a split infinitive comes from the time when Latin was the universal language of the world. All scholarly, respectable writing was done in Latin. Scientists and scholars even took Latin names to show that they were learned. In Latin, infinitives appear as a single word. The rule which prohibits splitting an infinite ["sic"] shows deference to Latin and to the time when the rules which governed Latin grammar were applied to other languages.
The assertion is also made in the "Oxford Guide to Plain English", "Compact Oxford English Dictionary", and Steven Pinker’s "Language Instinct," among other sources.
The argument implies an adherence to the humanist idea of the greater purity of the classics, which particularly in Renaissance times led people to regard aspects of English that differed from Latin as inferior. However by the 19th century such views were no longer widespread; Moriarty is in error about the age of the prohibition. It has also been stated that an argument from Latin would be fallacious because "there is no precedent in these languages for condemning the split infinitive because in Greek and Latin (and all the other romance languages) the infinitive is a single word that is impossible to sever."
Furthermore, this argument something of a straw man argument as very few "proponents" of the rule argue from Latin in any case. Certainly, it is clear that dislike of the split infinitive does not originate from Latin. As shown above, none of the prescriptivists who started the split-infinitive controversy in the 19th century mentioned Latin in connection with it. Occasionally teachers and bloggers can be found who do oppose the split infinitive with such an argument, but it is not found in any statements of the position from the 19th or early 20th century, when the prohibition developed. Of the writers cited here (and the many others consulted) who ascribe the split-infinitive prohibition to Latinism, none cite an authority who condemned the construction on that basis. According to Richard Bailey, the prohibition does not come from a comparison with Latin, and the belief that it does is “part of the folklore of linguistics.”
Current views.
"When I split an infinitive, God damn it, I split it so it will stay split."
 Raymond Chandler, "1947".
Present style and usage manuals deem simple split infinitives unobjectionable in many situations. For example, Curme's "Grammar of the English Language" (1931) says that not only is the split infinitive correct, but it "should be furthered rather than censured, for it makes for clearer expression." "The Columbia Guide to Standard American English" notes that the split infinitive "eliminates all possibility of ambiguity," in contrast to the "potential for confusion" in an unsplit construction. "Merriam-Webster's Dictionary of English Usage" says, "the objection to the split infinitive has never had a rational basis." According to Mignon Fogarty, "today almost everyone agrees that it is OK to split infinitives."
Nevertheless, many teachers of English still admonish students against using split infinitives in writing. Because the prohibition has become so widely known, the "Columbia Guide" recommends that writers "follow the conservative path [of avoiding split infinitives when they are not necessary], especially when you're uncertain of your readers' expectations and sensitivities in this matter." Likewise, the Oxford Dictionaries do not regard the split infinitive as ungrammatical, but on balance consider it likely to produce a weak style and advise against its use for formal correspondence. R. W. Burchfield's revision of Fowler's "Modern English Usage" goes farther (quoting Burchfield's own 1981 book "The Spoken Word"): "Avoid splitting infinitives whenever possible, but do not suffer undue remorse if a split infinitive is unavoidable for the completion of a sentence already begun." Still more strongly, the style guide of "The Economist" says, "Happy the man who has never been told that it is wrong to split an infinitive: the ban is pointless. Unfortunately, to see it broken is so annoying to so many people that you should observe it."
As well as register, tolerance of split infinitives varies according to type. While most authorities accept split infinitives in general, it is not hard to construct an example which any native speaker would reject. Interestingly, Wycliff's Middle English compound split would, if transferred to modern English, be regarded by most people as un-English:
Attempts to define the boundaries of normality are controversial. In 1996, the usage panel of "The American Heritage Book" was evenly divided for and against such sentences as,
but more than three-quarters of the panel rejected
Here the problem appears to be the breaking up of the verbal phrase "to be seeking a plan to relieve": a segment of the head verbal phrase is so far removed from the remainder that the listener or reader must expend greater effort to understand the sentence. By contrast, 87 percent of the panel deemed acceptable the multi-word adverbial in
not surprisingly perhaps, because here there is no other place to put the words "more than" without substantially recasting the sentence.
Although the usage of 'not' in splitting infinitives is an issue that has not attracted much attention from the English users, contemporary English grammar puts the phrase into the same category as above
 This appears to be because the traditional idiom, placing the negation before the marker ("I soon learned not to provoke her") or with verbs of desire, negating the finite verb ("I don't want to see you anymore") remains easy and natural, and is still overwhelmingly the more common construction in British English. 
On the other hand, in the case of American English, search on the corpus of contemporary American English displays a different aspect. 
Its usage is more commonly found in the American context, where we can find at least 2200 cases of the usage of 'not' in splitting infinitives.
Some argue that the two forms have different meanings, while others see a grammatical difference, but most speakers do not make use of such a distinction.
Avoiding split infinitives.
Writers who avoid splitting infinitives either place the splitting element elsewhere in the sentence (as noted in the 1834 proscription) or reformulate the sentence, perhaps rephrasing it without an infinitive and thus avoiding the issue. Considering that many English speakers throughout history have not known the construction, or have known it only passively, there can be no situation in which it is a necessary part of natural speech. However, a sentence with a split infinitive such as "to more than double" must be completely rewritten; it is ungrammatical to put the words "more than" anywhere else in the sentence. While split infinitives can be avoided, a writer must be careful not to produce an awkward or ambiguous sentence. Fowler (1926) stressed that, if a sentence is to be rewritten to remove a split infinitive, this must be done without compromising the language:
It is of no avail merely to fling oneself desperately out of temptation; one must so do it that no traces of the struggle remain; that is, sentences must be thoroughly remodeled instead of having a word lifted from its original place & dumped elsewhere:...
In some cases, moving the adverbial creates an ungrammatical sentence or changes the meaning. R.L. Trask uses this example:
The sentence can be rewritten to maintain its meaning, however, by using a noun or a different grammatical aspect of the verb, or by eschewing the informal "get rid":
Fowler notes that the option of rewriting is always available but questions whether it is always worth the trouble.
External links.
Listen to this article ()
This audio file was created from a revision of the "Split infinitive" article dated 2005-03-13, and does not reflect subsequent edits to the article. ()
More spoken articles

</doc>
<doc id="28901" url="http://en.wikipedia.org/wiki?curid=28901" title="Symmetric group">
Symmetric group

In abstract algebra, the symmetric group S"n" on a finite set of "n" symbols is the group whose elements are all the permutations of the "n" symbols, and whose group operation is the composition of such permutations, which are treated as bijective functions from the set of symbols to itself. Since there are "n"! ("n" factorial) possible permutations of a set of "n" symbols, it follows that the order (the number of elements) of the symmetric group S"n" is "n"!.
Although symmetric groups can be defined on infinite sets as well, this article discusses only the finite symmetric groups: their applications, their elements, their conjugacy classes, a finite presentation, their subgroups, their automorphism groups, and their representation theory. For the remainder of this article, "symmetric group" will mean a symmetric group on a finite set.
The symmetric group is important to diverse areas of mathematics such as Galois theory, invariant theory, the representation theory of Lie groups, and combinatorics. Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on "G".
Definition and first properties.
The symmetric group on a finite set "X" is the group whose elements are all bijective functions from "X" to "X" and whose group operation is that of function composition. For finite sets, "permutations" and "bijective functions" refer to the same operation, namely rearrangement. The symmetric group of degree "n" is the symmetric group on the set "X" = { 1, 2, ..., "n" }.
The symmetric group on a set "X" is denoted in various ways including S"X", 𝔖"X", Σ"X", "X"! and Sym("X"). If "X" is the set { 1, 2, ..., "n" }, then the symmetric group on "X" is also denoted S"n", 𝔖"n", Σ"n", and Sym("n").
Symmetric groups on infinite sets behave quite differently from symmetric groups on finite sets, and are discussed in , , and . This article concentrates on the finite symmetric groups.
The symmetric group on a set of "n" elements has order "n"! It is abelian if and only if "n" ≤ 2. For "n" = 0 and "n" = 1 (the empty set and the singleton set) the symmetric group is trivial (note that this agrees with 0! = 1! = 1), and in these cases the alternating group equals the symmetric group, rather than being an index two subgroup. The group S"n" is solvable if and only if "n" ≤ 4. This is an essential part of the proof of the Abel–Ruffini theorem that shows that for every "n" > 4 there are polynomials of degree "n" which are not solvable by radicals, i.e., the solutions cannot be expressed by performing a finite number of operations of addition, subtraction, multiplication, division and root extraction on the polynomial's coefficients.
Applications.
The symmetric group on a set of size "n" is the Galois group of the general polynomial of degree "n" and plays an important role in Galois theory. In invariant theory, the symmetric group acts on the variables of a multi-variate function, and the functions left invariant are the so-called symmetric functions. In the representation theory of Lie groups, the representation theory of the symmetric group plays a fundamental role through the ideas of Schur functors. In the theory of Coxeter groups, the symmetric group is the Coxeter group of type A"n" and occurs as the Weyl group of the general linear group. In combinatorics, the symmetric groups, their elements (permutations), and their representations provide a rich source of problems involving Young tableaux, plactic monoids, and the Bruhat order. Subgroups of symmetric groups are called permutation groups and are widely studied because of their importance in understanding group actions, homogeneous spaces, and automorphism groups of graphs, such as the Higman–Sims group and the Higman–Sims graph.
Elements.
The elements of the symmetric group on a set "X" are the permutations of "X".
Multiplication.
The group operation in a symmetric group is function composition, denoted by the symbol ∘ or simply by juxtaposition of the permutations. The composition "f" ∘ "g" of permutations "f" and "g", pronounced ""f" of "g"", maps any element "x" of "X" to "f"("g"("x")). Concretely, let (see permutation for an explanation of notation):
Applying "f" after "g" maps 1 first to 2 and then 2 to itself; 2 to 5 and then to 4; 3 to 4 and then to 5, and so on. So composing "f" and "g" gives
A cycle of length "L" = "k" · "m", taken to the "k"-th power, will decompose into "k" cycles of length "m": For example ("k" = 2, "m" = 3),
Verification of group axioms.
To check that the symmetric group on a set "X" is indeed a group, it is necessary to verify the group axioms of closure, associativity, identity, and inverses. 1) The operation of function composition is closed in the set of permutations of the given set "X", 2) function composition is always associative, 3) The trivial bijection that assigns each element of "X" to itself serves as an identity for the group, and 4) Every bijection has an inverse function that undoes its action, and thus each element of a symmetric group does have an inverse which is a permutation too.
Transpositions.
A transposition is a permutation which exchanges two elements and keeps all others fixed; for example (1 3) is a transposition. Every permutation can be written as a product of transpositions; for instance, the permutation "g" from above can be written as "g" = (1 2)(2 5)(3 4). Since "g" can be written as a product of an odd number of transpositions, it is then called an odd permutation, whereas "f" is an even permutation.
The representation of a permutation as a product of transpositions is not unique; however, the number of transpositions needed to represent a given permutation is either always even or always odd. There are several short proofs of the invariance of this parity of a permutation.
The product of two even permutations is even, the product of two odd permutations is even, and all other products are odd. Thus we can define the sign of a permutation:
With this definition,
is a group homomorphism ({+1, –1} is a group under multiplication, where +1 is e, the neutral element). The kernel of this homomorphism, i.e. the set of all even permutations, is called the alternating group A"n". It is a normal subgroup of S"n", and for "n" ≥ 2 it has "n"!/2 elements. The group S"n" is the semidirect product of A"n" and any subgroup generated by a single transposition.
Furthermore, every permutation can be written as a product of "adjacent transpositions", that is, transpositions of the form ("a" "a"+1). For instance, the permutation "g" from above can also be written as "g" = (4 5)(3 4)(4 5)(1 2)(2 3)(3 4)(4 5). The sorting algorithm Bubble sort is an application of this fact. The representation of a permutation as a product of adjacent transpositions is also not unique.
Cycles.
A cycle of "length" "k" is a permutation "f" for which there exists an element "x" in {1...,"n"} such that "x", "f"("x"), "f"2("x"), ..., "f""k"("x") = "x" are the only elements moved by "f"; it is required that "k" ≥ 2 since with "k" = 1 the element "x" itself would not be moved either. The permutation "h" defined by
is a cycle of length three, since "h"(1) = 4, "h"(4) = 3 and "h"(3) = 1, leaving 2 and 5 untouched. We denote such a cycle by (1 4 3), but it could equally well be written (4 3 1) or (3 1 4) by starting at a different point. The order of a cycle is equal to its length. Cycles of length two are transpositions. Two cycles are "disjoint" if they move disjoint subsets of elements. Disjoint cycles commute, e.g. in "S"6 we have (4 1 3)(2 5 6) = (2 5 6)(4 1 3). Every element of S"n" can be written as a product of disjoint cycles; this representation is unique up to the order of the factors, and the freedom present in representing each individual cycle by choosing its starting point.
Special elements.
Certain elements of the symmetric group of {1, 2, ..., "n"} are of particular interest (these can be generalized to the symmetric group of any finite totally ordered set, but not to that of an unordered set).
The order reversing permutation is the one given by:
This is the unique maximal element with respect to the Bruhat order and the
longest element in the symmetric group with respect to generating set consisting of the adjacent transpositions ("i" "i"+1), 1 ≤ "i" ≤ "n" − 1.
This is an involution, and consists of formula_9 (non-adjacent) transpositions
so it thus has sign:
which is 4-periodic in "n".
In S2"n", the "perfect shuffle" is the permutation that splits the set into 2 piles and interleaves them. Its sign is also formula_13
Note that the reverse on "n" elements and perfect shuffle on 2"n" elements have the same sign; these are important to the classification of Clifford algebras, which are 8-periodic.
Conjugacy classes.
The conjugacy classes of S"n" correspond to the cycle structures of permutations; that is, two elements of S"n" are conjugate in S"n" if and only if they consist of the same number of disjoint cycles of the same lengths. For instance, in S5, (1 2 3)(4 5) and (1 4 3)(2 5) are conjugate; (1 2 3)(4 5) and (1 2)(4 5) are not. A conjugating element of S"n" can be constructed in "two line notation" by placing the "cycle notations" of the two conjugate permutations on top of one another. Continuing the previous example:
which can be written as the product of cycles, namely: (2 4).
This permutation then relates (1 2 3)(4 5) and (1 4 3)(2 5) via conjugation, i.e.
It is clear that such a permutation is not unique.
Low degree groups.
The low-degree symmetric groups have simpler and exceptional structure, and often must be treated separately.
Maps between symmetric groups.
Other than the trivial map Sym("n") → 1 ≅ Sym(0) ≅ Sym(1) and the sign map Sym("n") → Sym(2), the notable maps between symmetric groups, in order of relative dimension, are:
Properties.
Symmetric groups are Coxeter groups and reflection groups. They can be realized as a group of reflections with respect to hyperplanes "x""i" = "x""j", 1 ≤ "i" < "j" ≤ "n". Braid groups B"n" admit symmetric groups S"n" as quotient groups.
Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on the elements of "G", as a group acts on itself faithfully by (left or right) multiplication.
Relation with alternating group.
For "n" ≥ 5, the alternating group A"n" is simple, and the induced quotient is the sign map: A"n" → S"n" → S2 which is split by taking a transposition of two elements. Thus S"n" is the semidirect product A"n" ⋊ S2, and has no other proper normal subgroups, as they would intersect An in either the identity (and thus themselves be the identity or a 2-element group, which is not normal), or in A"n" (and thus themselves be A"n" or S"n").
S"n" acts on its subgroup A"n" by conjugation, and for "n" ≠ 6, S"n" is the full automorphism group of A"n": Aut(A"n") ≅ S"n". Conjugation by even elements are inner automorphisms of A"n" while the outer automorphism of A"n" of order 2 corresponds to conjugation by an odd element. For "n" = 6, there is an exceptional outer automorphism of A"n" so S"n" is not the full automorphism group of A"n".
Conversely, for "n" ≠ 6, S"n" has no outer automorphisms, and for "n" ≠ 2 it has no center, so for "n" ≠ 2, 6 it is a complete group, as discussed in automorphism group, below.
For "n" ≥ 5, S"n" is an almost simple group, as it lies between the simple group A"n" and its group of automorphisms.
Generators and relations.
The symmetric group on "n"-letters, S"n", may be described as follows. It has generators: formula_16 and relations:
One thinks of formula_20 as swapping the "i"th and ("i" + 1)th position.
Other popular generating sets include the set of transpositions that swap 1 and "i" for 2 ≤ "i" ≤ "n" and a set containing any "n"-cycle and a 2-cycle of adjacent elements in the n-cycle.
Subgroup structure.
A subgroup of a symmetric group is called a permutation group.
Normal subgroups.
The normal subgroups of the finite symmetric groups are well understood. If "n" ≤ 2, S"n" has at most 2 elements, and so has no nontrivial proper subgroups. The alternating group of degree "n" is always a normal subgroup, a proper one for "n" ≥ 2 and nontrivial for "n" ≥ 3; for "n" ≥ 3 it is in fact the only non-identity proper normal subgroup of S"n", except when "n" = 4 where there is one additional such normal subgroup, which is isomorphic to the Klein four group.
The symmetric group on an infinite set does not have an associated alternating group: not all elements can be written as a (finite) product of transpositions. However it does contain a normal subgroup "S" of permutations that fix all but finitely many elements, and such permutations can be classified as either even or odd. The even elements of "S" form the alternating subgroup "A" of "S", and since "A" is even a characteristic subgroup of "S", it is also a normal subgroup of the full symmetric group of the infinite set. The groups "A" and "S" are the only non-identity proper normal subgroups of the symmetric group on a countably infinite set. For more details see or .
Maximal subgroups.
The maximal subgroups of the finite symmetric groups fall into three classes: the intransitive, the imprimitive, and the primitive. The intransitive maximal subgroups are exactly those of the form Sym("k") × Sym("n" − "k") for 1 ≤ "k" < "n"/2. The imprimitive maximal subgroups are exactly those of the form Sym("k") wr Sym("n"/"k") where 2 ≤ "k" ≤ "n"/2 is a proper divisor of "n" and "wr" denotes the wreath product acting imprimitively. The primitive maximal subgroups are more difficult to identify, but with the assistance of the O'Nan–Scott theorem and the classification of finite simple groups, gave a fairly satisfactory description of the maximal subgroups of this type according to .
Sylow subgroups.
The Sylow subgroups of the symmetric groups are important examples of "p"-groups. They are more easily described in special cases first:
The Sylow "p"-subgroups of the symmetric group of degree "p" are just the cyclic subgroups generated by "p"-cycles. There are ("p" − 1)!/("p" − 1) = ("p" − 2)! such subgroups simply by counting generators. The normalizer therefore has order "p"·("p"−1) and is known as a Frobenius group "F""p"("p" − 1) (especially for "p" = 5), and is the affine general linear group, AGL(1, "p").
The Sylow "p"-subgroups of the symmetric group of degree "p"2 are the wreath product of two cyclic groups of order "p". For instance, when "p" = 3, a Sylow 3-subgroup of Sym(9) is generated by "a" = (1 4 7)(2 5 8)(3 6 9) and the elements "x" = (1 2 3), "y" = (4 5 6), "z" = (7 8 9), and every element of the Sylow 3-subgroup has the form "a""i""x""j""y""k""z""l" for 0 ≤ "i","j","k","l" ≤ 2.
The Sylow "p"-subgroups of the symmetric group of degree "p""n" are sometimes denoted W"p"("n"), and using this notation one has that W"p"("n" + 1) is the wreath product of W"p"("n") and W"p"(1).
In general, the Sylow "p"-subgroups of the symmetric group of degree "n" are a direct product of "a""i" copies of W"p"("i"), where 0 ≤ "ai" ≤ "p" − 1 and "n" = "a"0 + "p"·"a"1 + ... + "p"k·"a""k".
For instance, W2(1) = C2 and W2(2) = D8, the dihedral group of order 8, and so a Sylow 2-subgroup of the symmetric group of degree 7 is generated by { (1,3)(2,4), (1,2), (3,4), (5,6) } and is isomorphic to D8 × C2.
These calculations are attributed to and described in more detail in . Note however that attributes the result to an 1844 work of Cauchy, and mentions that it is even covered in textbook form in .
Transitive subgroups.
A transitive subgroup of S"n" is a subgroup whose action on {1, 2, ..., "n"} is transitive. For example, the Galois group of a (finite) Galois extension is a transitive subgroup of S"n", for some "n".
Automorphism group.
For "n" ≠ 2, 6, S"n" is a complete group: its center and outer automorphism group are both trivial.
For "n" = 2, the automorphism group is trivial, but S2 is not trivial: it is isomorphic to C2, which is abelian, and hence the center is the whole group.
For "n" = 6, it has an outer automorphism of order 2: Out(S6) = C2, and the automorphism group is a semidirect product
In fact, for any set "X" of cardinality other than 6, every automorphism of the symmetric group on "X" is inner, a result first due to according to .
Homology.
The group homology of S"n" is quite regular and stabilizes: the first homology (concretely, the abelianization) is:
The first homology group is the abelianization, and corresponds to the sign map S"n" → S2 which is the abelianization for "n" ≥ 2; for "n" < 2 the symmetric group is trivial. This homology is easily computed as follows: S"n" is generated by involutions (2-cycles, which have order 2), so the only non-trivial maps S"n" → C"p" are to S2 and all involutions are conjugate, hence map to the same element in the abelianization (since conjugation is trivial in abelian groups). Thus the only possible maps S"n" → S2 ≅ {±1} send an involution to 1 (the trivial map) or to −1 (the sign map). One must also show that the sign map is well-defined, but assuming that, this gives the first homology of S"n".
The second homology (concretely, the Schur multiplier) is:
This was computed in , and corresponds to the double cover of the symmetric group, 2 · S"n".
Note that the exceptional low-dimensional homology of the alternating group (formula_24 corresponding to non-trivial abelianization, and formula_25 due to the exceptional 3-fold cover) does not change the homology of the symmetric group; the alternating group phenomena do yield symmetric group phenomena – the map formula_26 extends to formula_27 and the triple covers of A6 and A7 extend to triple covers of S6 and S7 – but these are not "homological" – the map formula_28 does not change the abelianization of S4, and the triple covers do not correspond to homology either.
The homology "stabilizes" in the sense of stable homotopy theory: there is an inclusion map S"n" → S"n" + 1, and for fixed "k", the induced map on homology "H""k"(S"n") → "H""k"(S"n" + 1) is an isomorphism for sufficiently high "n". This is analogous to the homology of families Lie groups stabilizing.
The homology of the infinite symmetric group is computed in , with the cohomology algebra forming a Hopf algebra.
Representation theory.
The representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.
The symmetric group S"n" has order "n"!. Its conjugacy classes are labeled by partitions of "n". Therefore according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of "n". Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representation by the same set that parametrizes conjugacy classes, namely by partitions of "n" or equivalently Young diagrams of size "n".
Each such irreducible representation can be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram.
Over other fields the situation can become much more complicated. If the field "K" has characteristic equal to zero or greater than "n" then by Maschke's theorem the group algebra "K"S"n" is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).
However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called "Specht modules", and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general. 
The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.
References.
</dl>

</doc>
<doc id="28902" url="http://en.wikipedia.org/wiki?curid=28902" title="SMS (disambiguation)">
SMS (disambiguation)

SMS is Short Message Service, a form of text messaging communication on phones and mobile phones.
SMS may also refer to:

</doc>
<doc id="28904" url="http://en.wikipedia.org/wiki?curid=28904" title="Short Message Peer-to-Peer">
Short Message Peer-to-Peer

The Short Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESME), Routing Entities (RE) and Message Centres.
SMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, Voice Mail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, the SMPP is the most commonly used protocol for short message exchange outside SS7 networks.
History.
SMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (now split off and known as Acision). The protocol was originally created by a developer, Ian J Chambers, to test functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Acision due to the disbanding of the SMS forum.
To date SMPP development is suspended and SMS forum is disbanded. From SMS forum website:
July 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007
A press release, attached to the news, also warns that site will be suspended soon. In spite of this the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).
The site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Acision for the SMPP specification. The files may also be available from other websites.
SMPP Operation.
Contrary to its name, the SMPP uses the client-server model of operation. The Message Center usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.
The protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.
Before exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.
Message exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a "window"; for the best performance both communicating sides must be configured with the same window size.
SMPP Versions.
The SMPP standard has evolved during the time. The most commonly used versions of SMPP are:
The applicable version is passed in the interface_version parameter of a bind command.
PDU Format.
The SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:
PDU Header.
Each PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:
All numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).
Example.
This is an example of the binary encoding of a 60-octet "submit_sm" PDU.
The data is shown in Hex octet values as a single dump and followed by a header
and body break-down of that PDU.
This is best compared with the definition of the submit_sm PDU from the 
SMPP specification in order to understand how the encoding matches
the field by field definition.
The value break-downs are shown with decimal in parentheses and Hex values 
after that. Where you see one or several hex octets appended, this is because
the given field size uses 1 or more octets encoding.
Again, reading the definition of the submit_sm PDU from the spec will 
make all this clearer.
PDU Header.
 'command_length', (60) ... 00 00 00 3C
 'command_id', (4) ... 00 00 00 04
 'command_status', (0) ... 00 00 00 00
 'sequence_number', (5) ... 00 00 00 05
PDU Body.
 'service_type', () ... 00
 'source_addr_ton', (2) ... 02
 'source_addr_npi', (8) ... 08
 'source_addr', (555) ... 35 35 35 00
 'dest_addr_ton', (1) ... 01
 'dest_addr_npi', (1) ... 01
 'dest_addr', (555555555) ... 35 35 35 35 35 35 35 35 35 00
 'esm_class', (0) ... 00
 'protocol_id', (0) ... 00
 'priority_flag', (0) ... 00
 'schedule_delivery_time', (0) ... 00
 'validity_period', (0) ... 00
 'registered_delivery', (0) ... 00
 'replace_if_present_flag', (0) ... 00
 'data_coding', (3) ... 03
 'sm_default_msg_id', (0) ... 00
 'sm_length', (15) ... 0F
 'short_message', (Hello wikipedia) ... 48 65 6C 6C 6F 20 77 69 6B 69 70 65 64 69 61""
Note that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:
The meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers—it may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.
Implementations.
SMPP has been implemented for Java in the jSMPP project. This is used in Apache Camel and various other popular free software projects for SMS messaging. Alternative Java implementation is . The python-smpp project provides SMPP for Python users.
SMPP Quirks.
Despite its wide acceptance, the SMPP has a number of problematic features:
No data_coding for GSM 7 bit default alphabet.
Although data_coding values in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7 bit default alphabet.
Not standardized meaning of data_coding=0.
According to SMPP 3.4 and 5.0 the data_coding=0 means ″SMSC Default Alphabet″. Which encoding it really is, depends on the type of the SMSC and its configuration.
Unclear support for Shift-JIS encoding.
One of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.
Incompatibility of submit_sm_resp between SMPP versions.
When a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ″empty″ message_id.
For the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.
Message Id in SMPP 3.3 SMSC Delivery Receipts.
The only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:
Extensibility, Compatibility and Interoperability.
Since introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ″Be conservative in what you send, be liberal in what you accept″. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:
Information applicable to one version of SMPP can often be found in another version of SMPP; e.g. the only way how to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose.

</doc>
<doc id="28906" url="http://en.wikipedia.org/wiki?curid=28906" title="Strike from the record">
Strike from the record

To strike from the record is for a judge to forbid a decision maker (such as a juror) to consider a particular piece of testimony or other evidence when deciding the case even though he or she has already learned what that evidence or testimony concerned. The commonly heard request is "move to strike", with the intent to erase previous testimony or court proceeding from record.

</doc>
<doc id="28908" url="http://en.wikipedia.org/wiki?curid=28908" title="Suburb">
Suburb

A suburb is a residential area or a mixed use area, either existing as part of a city or urban area or as a separate residential community within commuting distance of a city. In most English-speaking regions, suburban areas are defined in contrast to central or inner city areas, but in Australian English, "suburb" has become largely synonymous with what is called a "neighborhood" in other countries. In some areas, such as Australia, China, New Zealand, the United Kingdom, and a few U.S. states, new suburbs are routinely annexed by adjacent cities. In others, such as France, Arabia, most of the United States, and Canada, many suburbs remain separate municipalities or are governed as part of a larger local government area such as a county.
Suburbs first emerged on a large scale in the 19th and 20th centuries as a result of improved rail and road transport, which led to an increase in commuting. In general, they have lower population densities than inner city neighborhoods within an metropolitan area, and most residents commute to central cities or other business districts; however, there are many exceptions, including industrial suburbs, planned communities, and satellite cities. Suburbs tend to proliferate around cities that have an abundance of adjacent flat land.
Etymology and usage.
The word is derived from the Old French "subburbe", which is in turn derived from the Latin "suburbium", formed from "sub" (meaning "under" or "below") and "urbs" ("city"). In Ancient Rome, wealthy and important people tended to live on the hills of Rome, while poorer citizens lived at lower elevations – hence "below the city". The first recorded usage of the term in English, according to the "Oxford English Dictionary", was made by John Wycliffe in 1380, where the form "subarbis" was used.
North America.
In the United States and Canada, "suburb" can refer either to an outlying residential area of a city or town or to a separate municipality or unincorporated area outside a town or city.
British Isles.
In the United Kingdom, "suburb" merely refers to a residential area outside the city centre, regardless of administrative boundaries. Suburbs in this sense can be separated by open countryside from the city centre. In large cities such as London, suburbs include formerly separate towns and villages that have been gradually absorbed during a city's growth and expansion, such as Ealing or Bromley.
Australia and New Zealand.
In Australia and New Zealand, suburbs have become formalised as geographic subdivisions of a city and are used by postal services in addressing. In rural areas in both countries, their equivalents are called localities (see suburbs and localities). The terms "inner suburb" and "outer suburb" are used to differentiate between the higher-density suburbs in proximity to the city center, and the lower-density suburbs on the outskirts of the urban area. Inner suburbs, such as Te Aro in Wellington, Mt Eden in Auckland, Prahran in Melbourne and Ultimo in Sydney, are usually characterised by higher density apartment housing and greater integration between commercial and residential areas.
History.
Late history.
The earliest appearance of suburbs coincided with the spread of the first urban settlements. Large walled towns tended to be the focus around which smaller villages grew up in a symbiotic relationship with the market town. The word 'suburbani' was first used by the Roman statesman, Cicero, in reference to the large villas and estates built by the wealthy patricians of Rome on the city's outskirts.
As populations grew during the Early Modern Period in Europe, urban towns swelled with a steady influx of people from the countryside. In some places, nearby settlements were swallowed up as the main city expanded. The peripheral areas on the outskirts of the city were generally inhabited by the very poorest.
Origins of the modern suburb.
Due to the rapid migration of the rural poor to the industrialising cities of England in the late 18th century, a trend in the opposite direction began to develop; - newly rich members of the middle classes began to purchase estates and villas on the outskirts of London. This trend accelerated through the 19th century, especially in cities like London and Manchester that were experiencing tremendous growth, and the first suburban districts sprung up around the city centre to accommodate those who wanted to escape the squalid conditions of the industrial town. Toward the end of the century, with the development of public transit systems such as the underground railways, trams and buses, it became possible for the majority of the city's population to reside outside the city and to commute into the centre for work.
By the mid-19th century, the first major suburban areas were springing up around London as the city (then the largest in the world) became more overcrowded and unsanitary. A major catalyst in suburban growth came from the opening of the Metropolitan Railway in the 1860s. The line joined the capital's financial heart in the City to what were to become the suburbs of Middlesex. Harrow was reached in 1880, and the line eventually extended as far as Verney Junction in Buckinghamshire, more than 50 mi from Baker Street and the centre of London.
Unlike other railway companies, which were required to dispose of surplus land, the Met was allowed to retain such land that it believed was necessary for future railway use. Initially, the surplus land was managed by the Land Committee, and, from the 1880s, the land was developed and sold to domestic buyers in places like Willesden Park Estate, Cecil Park, near Pinner and at Wembley Park.
In 1912, it was suggested that a specially formed company should take over from the Surplus Lands Committee and develop suburban estates near the railway. However, World War I delayed these plans and it was only in 1919, with expectation of a postwar housing boom, that the Metropolitan Railway Country Estates Limited (MRCE) was formed. The MRCE went on to develop estates at Kingsbury Garden Village near Neasden, Wembley Park, Cecil Park and Grange Estate at Pinner and the Cedars Estate at Rickmansworth and create places such as Harrow Garden Village.
The term "Metro-land" was coined by the Met's marketing department in 1915 when the "Guide to the Extension Line" became the "Metro-land" guide, priced at 1d. This promoted the land served by the Met for the walker, visitor and later the house-hunter. Published annually until 1932, the last full year of independence for the Met, the guide extolled the benefits of "The good air of the Chilterns", using language such as "Each lover of Metroland may well have his own favourite wood beech and coppice — all tremulous green loveliness in Spring and russet and gold in October". The dream promoted was of a modern home in beautiful countryside with a fast railway service to central London. By 1915, people from across London had flocked to live the new suburban dream in large newly built areas across North West London.
Interwar suburban expansion in England.
Suburbanisation in the interwar period was heavily influenced by the garden city movement of Ebenezer Howard and the creation of the first garden suburbs at the turn of the 20th century. The first garden suburb was developed through the efforts of social reformer Henrietta Barnett and her husband; inspired by Ebenezer Howard and the model housing development movement (then exemplified by Letchworth garden city), as well as the desire to protect part of Hampstead Heath from development, they established trusts in 1904 which bought 243 acres of land along the newly opened Northern line extension to Golders Green and created the Hampstead Garden Suburb. The suburb attracted the talents of architects including Raymond Unwin and Sir Edwin Lutyens, and it ultimately grew to encompass over 800 acres.
During the First World War the Tudor Walters Committee was commissioned to make recommendations for the post war reconstruction and housebuilding. In part, this was a response to the shocking lack of fitness amongst many recruits during World War One, attributed to poor living conditions; a belief summed up in a housing poster of the period "you cannot expect to get an A1 population out of C3 homes" - referring to military fitness classifications of the period.
The Committee's report of 1917 was taken up by the government, which passed the Housing, Town Planning, &c. Act 1919, also known as the Addison Act after Dr. Christopher Addison, the then Minister for Housing. The Act allowed for the building of large new housing estates in the suburbs after the First World War, and marked the start of a long 20th century tradition of state-owned housing, which would later evolve into council estates.
The Report also legislated on the required, minimum standards necessary for further suburban construction; this included regulation on the maximum housing density and their arrangement and it even made recommendations on the ideal number of bedrooms and other rooms per house. Although the semi-detached house was first designed by the Shaws (a father and son architectural partnership) in the 19th century, it was during the suburban housing boom of the interwar period that that the design first proliferated as a suburban icon, being preferred by middle class home owners to the smaller terraced houses. The design of many of these houses, highly characteristic of the era, was heavily influenced by the Art Deco movement, taking influence from Tudor Revival, chalet style, and even ship design.
Within just a decade suburbs dramatically increased in size. Harrow Weald went from just 1,500 to over 10,000 while Pinner jumped from 3,000 to over 20,000. During the 1930s, over 4 million new suburban houses were built, the 'suburban revolution' had made England the most heavily suburbanized country in the world, by a considerable margin.
Post-war suburban expansion in North America.
The suburban population in North America exploded during the post-World War II economic expansion. Returning veterans wishing to start a settled life moved in masses to the suburbs. Levittown developed as a major prototype of mass-produced housing. At the same time, African Americans were rapidly moving north for better jobs and educational opportunities than were available to them in the segregated South. Their arrival in Northern cities en masse, in addition to being followed by race riots in several large cities such as Detroit, Chicago, Washington, D.C., and Philadelphia, further stimulated white suburban migration. The growth of the suburbs was facilitated by the development of zoning laws, redlining and numerous innovations in transport. After World War II availability of FHA loans stimulated a housing boom in American suburbs. In the older cities of the northeast U.S., streetcar suburbs originally developed along train or trolley lines that could shuttle workers into and out of city centers where the jobs were located. This practice gave rise to the term "bedroom community", meaning that most daytime business activity took place in the city, with the working population leaving the city at night for the purpose of going home to sleep.
The first large–scale suburban area in the United States to develop was Long Island, New York, a suburb derived mostly from the upper-middle-class development of entire communities in the late nineteenth century, and the rapid population growth that occurred as a result. As car ownership rose and wider roads were built, the commuting trend accelerated in North America. This trend towards living away from towns and cities has been termed the urban exodus.
Economic growth in the United States encouraged the suburbanization of American cities that required massive investments for the new infrastructure and homes. Consumer patterns were also shifting at this time, as purchasing power was becoming stronger and more accessible to a wider range of families. Suburban houses also brought about needs for products that were not needed in urban neighborhoods, such as lawnmowers and automobiles. During this time commercial shopping malls were being developed near suburbs to satisfy consumers' needs and their car–dependent lifestyle.
Zoning laws also contributed to the location of residential areas outside of the city center by creating wide areas or "zones" where only residential buildings were permitted. These suburban residences are built on larger lots of land than in the central city. For example, the lot size for a residence in Chicago is usually 125 ft deep, while the width can vary from 14 ft wide for a row house to 45 ft wide for a large stand–alone house. In the suburbs, where stand–alone houses are the rule, lots may be 85 ft wide by 115 ft deep, as in the Chicago suburb of Naperville. Manufacturing and commercial buildings were segregated in other areas of the city.
Alongside suburbanization, many companies began locating their offices and other facilities in the outer areas of the cities, which resulted in the increased density of older suburbs and the growth of lower density suburbs even further from city centers. An alternative strategy is the deliberate design of "new towns" and the protection of green belts around cities. Some social reformers attempted to combine the best of both concepts in the garden city movement.
In the U.S., 1950 was the first year that more people lived in suburbs than elsewhere. In the U.S, the development of the skyscraper and the sharp inflation of downtown real estate prices also led to downtowns being more fully dedicated to businesses, thus pushing residents outside the city center.
Suburbs worldwide.
United States.
In the 20th century, many suburban areas began to see independence from the central city as an asset. In some cases, white suburbanites saw self-government as a means to keep out people who could not afford the added suburban property maintenance costs not needed in city living. Federal subsidies for suburban development accelerated this process as did the practice of redlining by banks and other lending institutions. In some cities such as Miami and San Francisco, the main city is much smaller than the surrounding suburban areas, leaving the city proper with a small portion of the metro area's population and land area. Cleveland, Ohio is typical of many American central cities; its municipal borders have changed little since 1922, even though the Cleveland urbanized area has grown many times over. Several layers of suburban municipalities now surround cities like Boston, Cleveland, Chicago, Detroit, Los Angeles, Dallas, Denver, Fort Worth, Houston, New York City, San Francisco, Sacramento, Atlanta, Miami, Pittsburgh, Philadelphia, Minneapolis, and Washington, D.C..
Suburbs in America have a prevalence of usually detached single-family homes.
They are characterized by:
By 2010 suburbs increasingly gained people in racial minority groups, as many members of minority groups became better educated, more affluent, and sought more favorable living conditions compared to inner city areas; many white Americans also moved back to city centers. Many major city downtowns (such as Downtown Miami, Downtown Detroit, Center City Philadelphia or Downtown Los Angeles) are experiencing a renewal, with large population growth, residential apartment construction, and increased social, cultural, and infrastructural investments. Better public transit, proximity to work and cultural attractions, and frustration with suburban life and gridlock have attracted young Americans to the city centers.
Canadian suburbs.
Compared to the American counterpart, Canadian suburbs are more dense (mostly in major cities), and land use patterns are often more mixed-use. There are often high- or mid-rise developments interspersed with low-rise housing tracts and in many suburban areas, there are numerous slab-style residential highrises that were constructed in the 1970s and onward. In Canada, densities are generally slightly higher than in Australia, but below typical European values. Often, Canadian suburbs are less automobile-centred and public transit use is encouraged but can be notably unused. Throughout Canada, especially in Toronto and Vancouver, there are comprehensive plans in place to curb sprawl, such as Ontario's Places to Grow act.
Canada is an urbanized nation where over 80% of the population live in urban areas (loosely defined), and roughly two-thirds live in one of Canada's 33 census metropolitan areas (CMAs) with a population of over 100,000. However, of this metropolitan population, in 2001 nearly half lived in low-density neighborhoods, with only one in five living in a typical "urban" neighborhood. The percentage living in low-density neighborhoods varied from a high of nearly two-thirds of Calgary CMA residents (67%), to a low of about one-third of Montreal CMA residents (34%).
Population and income growth in Canadian suburbs had tended to outpace growth in core urban or rural areas, but in many areas this trend has now reversed. The suburban population increased 87% between 1981 and 2001, well ahead of urban growth. The majority of recent population growth in Canada's three largest metropolitan areas (Greater Toronto, Greater Montreal, and Greater Vancouver) has occurred in non-core municipalities, although this trend has already reversed itself in Toronto, where a building boom has begun to take place. This trend is also beginning to take effect in Vancouver, and to a lesser extent, Montreal. In certain cities, particularly Edmonton and Calgary, suburban growth takes place within the city boundaries as opposed to in bedroom communities. This is due to annexation and large geographic footprint within the city borders.
Other countries.
In many parts of the developed world, suburbs can be economically distressed areas, inhabited by higher proportions of recent immigrants, with higher delinquency rates and social problems. Sometimes the notion of suburb may even refer to people in real misery, who are kept at the limit of the city borders for economic, social, and sometimes ethnic reasons. An example in the developed world would be the "banlieues" of France, or the concrete suburbs of Sweden, even if the suburbs of these countries also include middle-class and upper-class neighborhoods that often consist of single-family houses. Thus some of the suburbs of most of the developed world are comparable to several inner cities of the U.S. and Canada.
The growth in the use of trains, and later automobiles and highways, increased the ease with which workers could have a job in the city while commuting in from the suburbs. In the United Kingdom, as mentioned above, railways stimulated the first mass exodus to the suburbs. The Metropolitan Railway, for example, was active in building and promoting its own housing estates in the north-west of London, consisting mostly of detached houses on large plots, which it then marketed as "Metro-land". The Australian and New Zealand usage came about as outer areas were quickly surrounded in fast-growing cities, but retained the appellation "suburb"; the term was eventually applied to the original core as well.
In the UK, the government is seeking to impose minimum densities on newly approved housing schemes in parts of South East England. The goal is to "build sustainable communities" rather than housing estates. However, commercial concerns tend to delay the opening of services until a large number of residents have occupied the new neighbourhood.
In Mexico, suburbs are generally similar to their United States counterparts. Houses are made in many different architectural styles which may be of European, American and International architecture and which vary in size. Suburbs can be found in Guadalajara, Mexico City, Monterrey, and most major cities. Lomas de Chapultepec is an example of an affluent suburb, although it is located inside the city and by no means is today a suburb in the strict sense of the word. In the rest of Latin America, the situation is similar to that of Mexico, with many suburbs being built, most notably in Argentina, Brazil, and Chile, which have experienced a boom in the construction of suburbs since the late 1970s and early 80s. As the growth of middle-class and upper-class suburbs increased, low-class squatter areas have increased, most notably "lost cities" in Mexico, campamentos in Chile, barriadas in Peru, villa miserias in Argentina, asentamientos in Guatemala and favelas of Brazil.
In Africa, since the beginning of the 1990s, the development of middle-class suburbs boomed. Due to the industrialization of many African countries, particularly in cities such as Cairo, Johannesburg and Lagos, the middle class has grown. In an illustrative case of South Africa, RDP housing has been built. In much of Soweto, many houses are American in appearance, but are smaller, and often consist of a kitchen and living room, two or three bedrooms, and a bathroom. However, there are more affluent neighborhoods, more comparable to American suburbs, particularly east of the FNB Stadium. In Cape Town there is a distinct European style which is due to the European influence during the mid-1600s when the Dutch conquered the area. Houses like these are called Cape Dutch Houses and can be found in the affluent suburbs of Constantia and Bishopscourt.
In the illustrative case of Rome, Italy, in the 1920s and 1930s, suburbs were intentionally created "ex novo" in order to give lower classes a destination, in consideration of the actual and foreseen massive arrival of poor people from other areas of the country. Many critics have seen in this development pattern (which was circularly distributed in every direction) also a quick solution to a problem of public order (keeping the unwelcome poorest classes together with the criminals, in this way better controlled, comfortably remote from the elegant "official" town). On the other hand, the expected huge expansion of the town soon effectively covered the distance from the central town, and now those suburbs are completely engulfed by the main territory of the town. Other newer suburbs (called exurbs) were created at a further distance from them.
In Russia, the term suburb refers to high-rise residential apartments which usually consist of two bedrooms, one bathroom, a kitchen and a living room. These suburbs, however are usually not in poor neighborhoods, unlike the banlieuees.
In China, the term suburb is new, although suburbs are already being constructed rapidly. Chinese suburbs mostly consist of rows upon rows of apartment blocks and condos that end abruptly into the countryside. Also new town developments are extremely common. Single family suburban homes tend to be similar to their Western equivalents; although primarily outside Beijing and Shanghai, also mimic Spanish and Italian architecture. In Hong Kong, however, suburbs are mostly government-planned new towns containing numerous public housing estates. New Towns such as Tin Shui Wai may gain notoriety as a slum. However, other new towns also contain private housing estates and low density developments for the upper classes.
In Japan, the construction of suburbs has boomed since the end of World War II and many cities are experiencing the urban sprawl effect.
In Malaysia, suburbs are common, especially in areas surrounding the Klang Valley, which is the largest conurbation in the country. These suburbs also serve as major housing areas and commuter towns. Terraced houses, semi-detached houses and shophouses are common concepts in suburbs. In certain areas such as Klang, Subang Jaya and Petaling Jaya, suburbs form the core of these places. The latter one has been turned into a satellite city of Kuala Lumpur. Suburbs are also evident in other smaller conurbations including Ipoh, Johor Bahru, Kota Kinabalu, Kuching, Alor Setar and Penang.
Traffic flows.
Suburbs typically have longer travel times to work than traditional neighborhoods. Only the traffic "within" the short streets themselves is less. This is due to three factors: almost-mandatory automobile ownership due to poor suburban bus systems, longer travel distances and the hierarchy system, which is less efficient at distributing traffic than the traditional grid of streets.
In the suburban system, most trips from one component to another component requires that cars enter a collector road, no matter how short or long the distance is. This is compounded by the hierarchy of streets, where entire neighborhoods and subdivisions are dependent on one or two collector roads. Because all traffic is forced onto these roads, they are often heavy with traffic all day. If a traffic accident occurs on a collector road, or if road construction inhibits the flow, then the entire road system may be rendered useless until the blockage is cleared. The traditional "grown" grid, in turn, allows for a larger number of choices and alternate routes.
Suburban systems of the sprawl type are also quite inefficient for cyclists or pedestrians, as the direct route is usually not available for them either. This encourages car trips even for distances as low as several hundreds of yards or meters (which may have become up to several miles or kilometers due to the road network). Improved sprawl systems, though retaining the car detours, possess cycle paths and footpaths connecting across the arms of the sprawl system, allowing a more direct route while still keeping the cars out of the residential and side streets.
According to "Governing, Cities and Localities" section More commonly, central cities seek ways to tax nonresidents working downtown – known as commuter taxes – as property tax bases dwindle. Taken together, these two groups of taxpayers represent a largely untapped source of potential revenue that cities may begin to target more aggressively, particularly if they're struggling. According to struggling cities, this will help bring in a substantial revenue for the city which is a great way to tax the people who make the most use of the highways and repairs.
Today more companies settle down in suburbs because of low property costs.
Academic study of suburbs.
The history of suburbia is part of the study of urban history, which focuses on the origins, growth, diverse typologies, culture, and politics of suburbs, as well as on the gendered and family-oriented nature of suburban space. Many people have assumed that early-20th-century suburbs were enclaves for middle-class whites, a concept that carries tremendous cultural influence yet is actually stereotypical. Many suburbs are based on a heterogeneous society of working-class and minority residents, many of whom want to own their own house. Mary Corbin Sies argues that it is necessary to examine how "suburb" is defined as well as the distinction made between cities and suburbs, geography, economic circumstances, and the interaction of numerous factors that move research beyond acceptance of stereotyping and its influence on scholarly assumptions.
In popular culture.
Suburbs and suburban living have been the subject for a wide variety of films, books, television shows and songs.
French songs like "La Zone" by Fréhel (1933), "Aux quatre coins de la banlieue" by Damia (1936), "Ma banlieue" by Reda Caire (1937), or "Banlieue" by Robert Lamoureux (1953), evoke the suburbs of Paris explicitly since the 1930s. Those singers give a sunny festive, almost bucolic, image of the suburbs, yet still few urbanized. During the fifties and the sixties, French singer-songwriter Léo Ferré evokes in his songs popular and proletarian suburbs of Paris, to oppose them to the city, considered by comparison as a bourgeois and conservative place.
French cinema was although soon interested in urban changes in the suburbs, with such movies as "Mon oncle" by Jacques Tati (1958), "L'Amour existe" by Maurice Pialat (1961) or "Two or Three Things I Know About Her" by Jean-Luc Godard (1967).
In his one-act opera "Trouble in Tahiti" (1952), Leonard Bernstein skewers American suburbia, which produces misery instead of happiness.
The American photojournalist Bill Owens documented the culture of suburbia in the 1970s, most notably in his book "Suburbia". The 1962 song "Little Boxes" by Malvina Reynolds lampoons the development of suburbia and its perceived bourgeois and conformist values, while the 1982 song "Subdivisions" by the Canadian band Rush also discusses suburbia, as does Rocking the Suburbs by Ben Folds. The 2010 album "The Suburbs" by the Canadian-based alternative band Arcade Fire dealt with aspects of growing up in suburbia, suggesting aimlessness, apathy and endless rushing are ingrained into the suburban culture and mentality. was written by Robert S. Cohen and David Javerbaum. Over the Hedge is a syndicated comic strip written and drawn by Michael Fry and T. Lewis. It tells the story of a raccoon, turtle, a squirrel, and their friends who come to terms with their woodlands being taken over by suburbia, trying to survive the increasing flow of humanity and technology while becoming enticed by it at the same time. A film adaptation of Over the Hedge was produced in 2006.
British television series such as "The Good Life", "Butterflies" and "The Fall and Rise of Reginald Perrin" have depicted suburbia as well-manicured but relentlessly boring, and its residents as either overly conforming or prone to going stir crazy. Contrastingly, U.S. shows – such as "Desperate Housewives" or "Weeds" – portray the suburbs as concealing darker secrets behind a façade of perfectly manicured lawns, friendly people, and beautifully up-kept houses. Films such as "The 'Burbs", "Disturbia" and "Hot Fuzz", have brought this theme to the cinema. This trope was also used in the episode of "The X-Files" "Arcadia" and on one level of the video game "Psychonauts".

</doc>
<doc id="28910" url="http://en.wikipedia.org/wiki?curid=28910" title="Shōnen manga">
Shōnen manga

Shōnen, shonen, or shounen manga (少年漫画, shōnen manga) is manga aimed at a young male audience. It appeals to a large age group, though its primary audience is under the age of fourteen. The kanji characters (少年) mean "boy" or "youth"; the characters (漫画) mean "cartoon" or "comic". The complete phrase literally means "young person's comic" or simply "boys' comic". Shōnen manga has been said to be the most popular form of manga. The female equivalent to shōnen manga is shōjo manga.
Summary.
Shōnen (少年) manga (漫画) is typically characterized by high-action, often humorous plots featuring male protagonists. The camaraderie between boys or men on sports teams, fighting squads and the like is often emphasized. Attractive female characters with exaggerated features are also common, such as Bulma from "Dragon Ball" or Nami from "One Piece" (see fan service). Main characters may also feature an ongoing desire to better themselves.
Such manga often portray challenges to the protagonist's abilities, skills, and maturity, stressing self-perfection, austere self-discipline, sacrifice in the cause of duty, and honorable service to society, community, family, and friends.
None of these listed characteristics are a requirement, as seen in shōnen manga like "Yotsuba&!", which features a female lead and almost no fan service or action; what most defines whether or not a series is shōnen are things like the magazine (see "Weekly Shōnen Jump") it is serialized in or the time slot it airs on T.V. After the case of Tsutomu Miyazaki, depictions of violence and sexual matters became more highly regulated in manga in general, but especially in shōnen manga. The art style of shōnen is generally less "flowery" than that of shōjo manga, although this varies greatly from artist to artist, and some artists draw both shōnen and shōjo manga.
Different shōnen manga stories may feature different themes, such as martial arts, robots, science fiction, sports, terror, and mythological creatures.
Shōnen manga today.
Akira Toriyama's "Dragon Ball" (1984–1995) is credited with setting the trend of popular shōnen manga from the 1980s onward, with manga critic Jason Thompson in 2011 calling it "by far the most influential shōnen manga of the last 30 years." The influence and popularity of the series had reached many currently successful shōnen authors such as Eiichiro Oda, Masashi Kishimoto, Tite Kubo, Togashi Yoshihiro, Akira Amano, Mitsutoshi Shimabukuro and Hiro Mashima, who continue to pay homage to his impact and influence in their own works.
History.
Before World War II.
Manga has been said to have existed since the eighteenth century, but originally did not target a specific gender or age group. By 1905, however, a boom in publishing manga magazines occurred, and began targeting genders as evidenced by their names, such as "Shōnen Sekai", "Shōjo Sekai", and "Shōnen Pakku" (a kodomo manga magazine). "Shōnen Sekai" was one of the first shōnen manga magazines, and was published from 1895 to 1914.
Post-Occupation.
The post-World War II occupation of Japan had a profound impact on its culture during the 1950s and beyond (see culture of Post-occupation Japan), including on manga. Modern manga developed during this period, including the modern format of shōnen manga we experience today, of which boys and young men were among the earliest readers. During this time, Shōnen manga focused on topics thought to interest the archetypical boy: sci-tech subjects like robots and space travel, and heroic action-adventure. Osamu Tezuka, creator of "Astro Boy" (鉄腕アトム, Tetsuwan Atomu, "Mighty Atom," lit. "Iron Arm Atom") is said to have played an influential role in manga during this period. Between 1950 and 1969, an increasingly large readership for manga emerged in Japan with the solidification of its two main marketing genres, shōnen manga aimed at boys and shōjo manga aimed at girls.
The magazine "Weekly Shōnen Jump" began production in 1968, and continues to be produced today as the best-selling manga magazine in Japan. Many of the most popular shōnen manga titles have been serialized in "Jump", including "Dragon Ball", "Naruto", "Bleach", "One Piece", "Slam Dunk", "Captain Tsubasa", and others (see List of series run in "Weekly Shōnen Jump").
With the relaxation of censorship in Japan in the 1990s, a wide variety of explicit sexual themes appeared in manga intended for male readers, and correspondingly occur in English translations; however, in 2010 the Tokyo Metropolitan Government passed a bill to restrict harmful content.
Women's roles in shōnen manga.
In early shōnen manga, men and boys played all the major roles, with women and girls having only auxiliary places as sisters, mothers, and occasionally girlfriends. Of the nine cyborgs in Shotaro Ishinomori's 1964 "Cyborg 009", only one is female, and she soon vanishes from the action. Some recent shōnen manga virtually omit women, e.g. the martial arts story "Baki the Grappler" by Itagaki Keisuke, and the supernatural fantasy "Sand Land" by Akira Toriyama. By the 1980s, however, girls and women began to play increasingly important roles in shōnen manga. For example, in Toriyama's 1980 "Dr. Slump", the main character is the mischievous and powerful girl robot Arale Norimaki.
The role of girls and women in manga for male readers has evolved considerably since Arale. One class is the pretty girl (bishōjo). Sometimes the woman is unattainable, and she is always an object of the hero's emotional and/or sexual interest, like Belldandy from "Oh My Goddess!" by Kōsuke Fujishima and Shao-lin from "Guardian Angel Getten" by Minene Sakurano. In other stories, the hero is surrounded by such girls and women, as in ' by Ken Akamatsu and "Hanaukyo Maid Team" by Morishige. The male protagonist does not always succeed in forming a relationship with the woman, for example when Bright Honda and Aimi Komori fail to bond in "Shadow Lady" by Masakazu Katsura. In other cases, a successful couple's sexual activities are depicted or implied, like in "Outlanders" by Johji Manabe. In still other cases, the initially naive and immature hero grows up to become a man by learning how to deal and live with women emotionally and sexually; examples of heroes who follow this path include Yota in "Video Girl Ai" by Masakazu Katsura and Train Man in ' by Hidenori Hara. However, since the 90s, women have acquired a heightened role in various manga, albeit lesser in number. They are often portrayed as central characters or characters with important roles in manga. Some examples include "InuYasha", "Ranma ½", and "Fairy Tail".
Examples of shōnen manga that feature female protagonists include "Soul Eater" and "It's Not My Fault That I'm Not Popular!"
List of shōnen manga.
Examples of shōnen manga include, but are not limited to: "Shaman King", "Baka and Test", "Fist of the North Star", "Death Note", "Dragon Ball", "InuYasha", "JoJo's Bizarre Adventure", "Beyblade", "One Piece", "Naruto", "Astro Boy", "Pandora Hearts", "Rurouni Kenshin", "Kinnikuman", "Saint Seiya", "Dr. Slump", "Gin Tama", "Fighting Spirit", "Detective Conan", "YuYu Hakusho", "Code Geass", "Hunter × Hunter", "Reborn!", "Bleach", "Soul Eater", "Air Gear", "Slam Dunk", "Zatch Bell!", "Fairy Tail", ', "Yu-Gi-Oh!", "Fullmetal Alchemist", "Ranma ½", "Attack on Titan", "Blue Exorcist", ', "Sket Dance", "Kuroko's Basketball", "Deadman Wonderland", "Toriko", D.Gray-man", "Pokemon Adventures", and "Kuroshitsuji".

</doc>
<doc id="28911" url="http://en.wikipedia.org/wiki?curid=28911" title="Shōjo manga">
Shōjo manga

Shōjo, shojo, or shoujo manga (少女漫画, shōjo manga) is manga aimed at a teenage female readership. The name romanizes the Japanese 少女 (shōjo), literally "young girl". Shōjo manga covers many subjects in a variety of narrative and graphic styles, from historical drama to science fiction — often with a strong focus on human and romantic relationships and emotions. Strictly speaking, shōjo manga does not comprise a style or a genre "per se", but rather indicates a target demographic.
Examples include "Boys Over Flowers", "Candy Candy", "Cardcaptor Sakura", "Fruits Basket", "Fushigi Yuugi", "Ouran High School Host Club", "Pretty Cure", "Princess Ai", "Princess Tutu", "Revolutionary Girl Utena", "Lovely Complex", "Sailor Moon", "Skip Beat", "Shugo Chara!", "Tokyo Mew Mew", "Rose of Versailles", "Kaichou wa Maid-sama", "Vampire Knight", "Special A", "Nana", "Itazura na Kiss", "We Were There", and "Brothers Conflict".
History.
Japanese magazines specifically for girls, known as shōjo magazines, first appeared in 1903 with the founding of Shōjo kai (少女界, "Girls' World")
, and continued with others such as Shōjo Sekai (少女世界, "Girls' World")
 (1906) and the long-running Shōjo no tomo (少女の友, "Girls' Friend")
 (1908).
Simple, single-page manga began to appear in these magazines by 1910, and by the 1930s more sophisticated humor-strips had become an essential feature of most girls' magazines. The most popular manga, Katsuji Matsumoto's "Kurukuru Kurumi-chan" (くるくるクルミちゃん), debuted on the pages of "Shōjo no tomo" (少女の友) in 1938. As World War II progressed, however, "comics, perhaps regarded as frivolous, began to disappear".
Postwar shōjo manga, such as Shosuke Kurakane's popular "Anmitsu Hime",
initially followed the pre-war pattern of simple humor-strips. But Osamu Tezuka's postwar revolution, introducing intense drama and serious themes to children's manga, spread quickly to shōjo manga, particularly after the enormous success of his seminal "Ribon no kishi" (リボンの騎士 "Princess Knight"). Sally the Witch—being the first magical girl genre anime—may (even more broadly) be the first shōjo anime as well.
Until the mid-1960s, males vastly outnumbered the handful of females (for example: Toshiko Ueda, Hideko Mizuno, Masako Watanabe, and Miyako Maki) amongst the artists working on shōjo manga. Many, such as Tetsuya Chiba, functioned as rookies, waiting for an opportunity to move over to shōnen (少年 "boys'") manga. Chiba asked his wife about girls' feelings for research for his manga. At this time, conventional job-opportunities for females did not include becoming a manga artist.
Adapting Tezuka's dynamic style to shōjo manga (which had always been domestic in nature) proved challenging. According to Thorn:
While some chose to simply create longer humor-strips, others turned to popular girls' novels of the day as a model for melodramatic shōjo manga. These manga featured sweet, innocent pre-teen heroines, torn from the safety of family and tossed from one perilous circumstance to another, until finally rescued (usually by a kind, handsome young man) and re-united with their families.
These early shōjo manga almost invariably had pre-adolescent girls as both heroines and readers. Unless they used a fantastic setting (as in "Princess Knight") or a backdrop of a distant time or place, romantic love for the heroine remained essentially taboo. But the average age of the readership rose, and its interests changed. In the mid-1960s one of the few female artists in the field, Yoshiko Nishitani, began to draw stories featuring contemporary Japanese teenagers in love. This signaled a dramatic transformation of the genre.
Between 1950 and 1969, increasingly large audiences for manga emerged in Japan with the solidification of its two main marketing genres, shōnen manga aimed at boys and shōjo manga aimed at girls.
Between roughly 1969 and 1971, a flood of young female manga artists transformed the genre again. Some, including Hagio Moto, Yumiko Oshima, and Keiko Takemiya, became known as the "hana no nijū yon nen gumi" (花の24年組, Year 24 Group, so named from the approximate year of birth many of them shared:Shōwa 24, or 1949). This loosely defined group experimented with content and form, inventing such new subgenres as shōnen-ai, and earning the long-maligned shōjo manga unprecedented critical praise. Other female artists of the same generation, such as Riyoko Ikeda, Yukari Ichijo, and Sumika Yamamoto, garnered unprecedented popular support with such hits (respectively) as "Berusaiyu no bara" (ベルサイユのばら, "The Rose of Versailles"), "Dezainaa" (デザイナー, "Designer"), and "Eesu wo nerae!" (エースをねらえ!, "Aim for the Ace!"). Since the mid-1970s, women have created the vast majority of shōjo manga; notable exceptions include Mineo Maya and Shinji Wada.
From 1975 shōjo manga continued to develop stylistically while simultaneously branching out into different but overlapping subgenres. Yukari Fujimoto feels that during the 1990s, shoujo manga became concerned with self-fulfillment. She feels the Gulf War influenced the development of "girls who fight to protect the destiny of a community", such as "Red River", "Basara", "Magic Knight Rayearth", and "Sailor Moon". She feels that the shōjo manga of the 1990s showed emotional bonds between women were stronger than bonds between a man and a woman. Major subgenres include romance, science fiction, fantasy, magical girls, yaoi, and "Ladies Comics" (in Japanese, "redisu" レディース, "redikomi" レディコミ, and "josei" 女性).
Meaning and spelling.
As shōjo literally means "girl" in Japanese, the equivalent of the western usage will generally include the term: girls' manga (少女漫画 "shōjo manga"), or anime for girls ( "shōjo-muke anime"). The parallel terms shōnen, seinen, and josei also occur in the categorisation of manga and anime, with similar qualification. Though the terminology originates with the Japanese publishers, cultural differences with the West mean that labelling in English tends to vary wildly, with the types often confused and mis-applied.
Due to vagaries in the romanization of Japanese, publishers may transcribe 少女 (written しょうじょ in hiragana) in a wide variety of ways. By far the most common form, "shoujo", follows English phonology, preserves the spelling, and requires only ASCII input. The Hepburn romanization shōjo uses a macron for the long vowel, though the prevalence of Latin-1 fonts often results in a circumflex instead, as in "shôjo". Many English-language texts just ignore long vowels, using "shojo", potentially leading to confusion with 処女 ("shojo", literally: "virgin") as well as other possible meanings. Finally, transliteraters may use Nihon-shiki-type mirroring of the kana spelling: "syôjyo", or "syoujyo".
Western adoption.
Western fans classify a wide variety of titles as shōjo, even though their Japanese creators label them differently. Anything non-offensive and featuring female characters may classify as shōjo manga; including the shōnen comedy "Azumanga Daioh". Similarly, as romance has become a common element of many shōjo works, any title with romance, such as the shōnen "Love Hina"
or the seinen "Oh My Goddess!" tend to get mislabeled. This confusion also extends beyond the fan community; articles aimed at the mainstream also widely misrepresent the terms. In an introduction to anime and manga, Jon Courtenay Grimwood writes: "'Maison Ikkoku' comes from Rumiko Takahashi, one of the best-known of all 'shôjo' writers. Imagine a very Japanese equivalent of 'Sweet Valley High' or 'Melrose Place'. It has Takahashi's usual and highly successful mix of teenagers and romance, with darker clouds of adolescence hovering."
Takahashi is a famous shōnen manga artist, but "Maison Ikkoku", one of her few seinen titles and serialised in "Big Comic Spirits", is aimed at males in their 20s. Matt Thorn, who has made a career out of studying girls' comics, attempts to clarify the matter by explaining that "shôjo manga are manga published in shôjo magazines (as defined by their publishers)". However, English publishers and stores have problems retailing shōjo titles, including its spelling. Licensees such as Dark Horse Comics have misidentified several of the seinen titles, and in particular manga and anime aimed at a younger audience in Japan is often considered "inappropriate" for minors in the US. In this way licensees often either voluntarily censor titles or re-market them towards an older audience. In the less conservative European markets, content that might be heavily edited or cut in an English-language release often remains in French, German and other translated editions.
As one effect of these variations, US companies have moved to use the borrowed words that have gained name-value in fan communities, but to separate them from the Japanese meaning. In their shōjo manga range, publisher VIZ Media attempt a re-appropriation of the term, providing the definition:
 shô·jo (sho'jo) "n." 1. Manga appealing to both female and male readers. 2. Exciting stories with true-to-life characters and the thrill of exotic locales. 3. Connecting the heart and mind through real human relationships.
 — Nasu Yukie, "Here Is Greenwood" vol. 1
The desire to disassociate the word "shōjo" from its meaning, "girl", seems largely driven by fear of putting off potential new readers, particularly male ones.
Manga and anime labeled as "shōjo" need not interest only young girls, and some titles gain a following outside the traditional audience. For instance, Frederik L. Schodt identifies "Banana Fish" by Akimi Yoshida as:
 ...one of the few girls' manga a red-blooded Japanese male adult could admit to reading without blushing. Yoshida, while adhering to the conventions of girls' comics in her emphasis on gay male love, made this possible by eschewing flowers and bug eyes in favor of tight bold strokes, action scenes, and speed lines.
Such successful "crossover" titles remain the exception rather than the rule, however: the archetypal shōjo manga magazine "Hana to Yume" has a 95% female readership, with a majority aged 17 or under. The popularity of romantic shōjo manga in America has encouraged Harlequin to release manga-styled romantic comics.
Circulation figures.
The reported average circulations for some of the top-selling shōjo manga magazines in 2007 included:
For comparison, circulations for the top-selling magazines in other categories for 2007 included:
Shōjo magazines in Japan.
In a strict sense, "shōjo manga" refers to a story serialized in a shōjo manga magazine (a magazine marketed to girls and young women). The list below contains past and current Japanese shōjo manga magazines, grouped according to their publishers. Such magazines can appear on a variety of schedules, including bi-weekly ("Margaret", "Hana to Yume", "Shōjo Comic"), monthly ("Ribon", "Bessatsu Margaret", "Bessatsu Friend", "LaLa"), bi-monthly ("Deluxe Margaret", "LaLa DX", "The Dessert"), and quarterly ("Cookie BOX", "Unpoko"). Weekly shōjo magazines, common in the 1960s and 1970s, had disappeared by the early 1980s.

</doc>
<doc id="28912" url="http://en.wikipedia.org/wiki?curid=28912" title="Srebrenica">
Srebrenica

Srebrenica (Cyrillic: Сребреница, ]) is a town and municipality in easternmost Bosnia and Herzegovina. Srebrenica is a small mountain town, its main industry being salt mining and a nearby spa.
During the Bosnian War, the town was the site of a July 1995 massacre, determined to have been a crime of genocide.
On 24 March 2007, Srebrenica's municipal assembly adopted a resolution demanding independence from the Republika Srpska entity (although not from Bosnia's sovereignty); the Serb members of the assembly did not vote on the resolution.
Local communities.
The municipality (општина or "opština") is further subdivided into the following local communities (мјесне заједнице or "mjesne zajednice"):
Demographics.
The borders of the municipality in the 1953 and 1961 census were different. In 1953, Muslims by nationality had been yet to emerge as an ethnicity leading Slavic Muslims to identify as Yugoslavs. As "Yugoslav" was itself not adopted in 1948, they were all classified as "other". In 2003, Bosnian Serbs comprised 95% of the population of Srebrenica. 
Economy.
Before 1992, there was a metal factory in the town, and lead, zinc, and gold mines nearby. The town's name (Srebrenica) means "silver mine", the same meaning of its old Latin name "Argentaria".
Before the war, Srebrenica also had a big spa and the town prospered from tourism. Nowadays, Srebrenica has some tourism, lot less developed than before the war. Currently a pansion, motel and a hostel are operating in the town.
History.
Roman era.
During the Roman times, there was a settlement of Domavia, known to have been near a mine. Silver ore from there was moved to the mints in Salona in the southwest and Sirmium in the northeast using the Via Argentaria.
Middle Ages.
In the 13th and 14th century the region was part of the Banate of Bosnia, and, subsequently, the Bosnian Kingdom. The earliest reference to the name Srebrenica was in 1376, by which time it was already an important centre for trade in the western Balkans, based especially on the silver mines of the region. By that time, a large number of merchants of the Republic of Ragusa were established there, and they controlled the domestic silver trade and the export by sea, almost entirely via the port of Ragusa (Dubrovnik). During the 14th century, many German miners moved into the area. There were often armed conflicts about Srebrenica because of its mines. According to Czech historian Konstantin Josef Jireček, from 1410 to 1460, Srebrenica switched hands several times, being Serbian five times, Bosnian four times, and Ottoman three times. The mines of Bosnian Podrinje and Usora were part of the Serbian Despotate prior to the Ottoman conquest.
Ottoman period.
With the town coming under Ottoman rule, becoming less influenced by the Republic of Ragusa, the economic importance of Srebrenica went into decline, as did the proportion of Christians in the population. The Franciscan monastery was converted into a mosque, but the large number of Catholics, Ragusa and Saxon, caused the transformation of the town to Islam to be slower than in most of the other towns in the area.
The area of Osat was liberated for a short time during the First Serbian Uprising (1804–13), under the leadership of Kara-Marko Vasić from Crvica. Upon the breakout of the uprising, Metropolitan Hadži Melentije Stevanović contacted Vasić, who met with the rebel leadership. After participated in battles on the Drina (1804), Vasić asked Karađorđe for an army to liberate Osat; Lazar Mutap was dispatched and the region came under rebel rule. In 1808, the Ottomans cleared out Osat, and by 1813, the rebels left the region.
World War II.
During World War II, the Ustaše massacred hundreds of Serbs in villages surrounding Srebrenica. In early January 1941, the Chetniks entered Srebrenica and killed around a thousand Muslim civilians in the town and in nearby villages.
Bosnian War.
The town of Srebrenica came to international prominence as a result of events during the Bosnian War (1992–1995). The strategic objectives proclaimed by the secessionist Bosnian Serb presidency included the creation of a border separating the Serb people from Bosnia's other ethnic communities and the abolition of the border along the river Drina separating Serbia and the Bosnian Serbs' Republika Srpska. The Bosnian Muslim/Bosniak majority population of the Drina Valley posed a major obstacle to the achievement of these objectives. In the early days of the campaign of forcible transfer (ethnic cleansing) that followed the outbreak of war in April 1992 the town of Srebrenica was occupied by Serb/Serbian forces. It was subsequently retaken by Bosniak resistance groups. Refugees expelled from towns and villages across the central Drina valley sought shelter in Srebrenica, swelling the town's population.
The town and its surrounding area was surrounded and besieged by Serb forces. On 16 April 1993, the United Nations declared the Bosnian Muslim/Bosniak enclave a UN safe area, to be "free from any armed attack or any other hostile act", and guarded by a small unit operating under the mandate of United Nations Protection Force (UNPROFOR).
Srebrenica and the other UN safe areas of Žepa and Goražde were isolated pockets of Bosnian government-held territory in eastern Bosnia. In July 1995, despite the town's UN-protected status, it was attacked and captured by the Army of Republika Srpska. Following the town's capture, all men of fighting age who fell into Bosnian Serb hands were massacred in a systematically organised series of summary executions. The women of the town and men below 16 years of age and above 55 were transferred by bus to Tuzla.
The Srebrenica massacre is considered the worst massacre in post-World War II European history to this day.
In 2001, the Srebrenica massacre was determined by judgment of the International Criminal Tribunal for the former Yugoslavia (ICTY) to have been a crime of genocide (confirmed on appeal in 2004). This finding was upheld in 2007 by the International Court of Justice. The decision of the ICTY was followed by an admission to and an apology for the massacre by the Republika Srpska government.
Under the 1995 Dayton Agreement which ended the Bosnian War, Srebrenica was included in the territory assigned to Bosnian Serb control as the Republika Srpska entity of Bosnia and Herzegovina. Although guaranteed under the provisions of the Dayton Agreement, the return of survivors was repeatedly obstructed. In 2007, verbal and physical attacks on returning refugees continued to be reported in the region around Srebrenica.
Fate of Bosnian Muslim villages.
In 1992, Bosniak villages around Srebrenica were under constant attacks by Serb forces. The Bosnian Institute in the United Kingdom has published a list of 296 villages destroyed by Serb forces around Srebrenica three years before the genocide and in the first three months of war (April–June 1992):
 More than three years before the 1995 Srebrenica genocide, Bosnian Serb nationalists - with the logistical, moral and financial support of Serbia and the Yugoslav People's Army (JNA) - destroyed 296 predominantly Bosniak (Bosnian Muslim) villages in the region around Srebrenica, forcibly uprooting some 70,000 Bosniaks from their homes and systematically massacring at least 3,166 Bosniaks (documented deaths) including many women, children and the elderly.
According to the Naser Orić trial judgement:
 "Between April 1992 and March 1993, Srebrenica town and the villages in the area held by Bosnian Muslims were constantly subjected to Serb military assaults, including artillery attacks, sniper fire, as well as occasional bombing from aircraft. Each onslaught followed a similar pattern. Serb soldiers and paramilitaries surrounded a Bosnian Muslim village or hamlet, called upon the population to surrender their weapons, and then began with indiscriminate shelling and shooting. In most cases, they then entered the village or hamlet, expelled or killed the population, who offered no significant resistance, and destroyed their homes. During this period, Srebrenica was subjected to indiscriminate shelling from all directions on a daily basis. Potočari in particular was a daily target for Serb artillery and infantry because it was a sensitive point in the defence line around Srebrenica. Other Bosnian Muslim settlements were routinely attacked as well. All this resulted in a great number of refugees and casualties."

</doc>
<doc id="28913" url="http://en.wikipedia.org/wiki?curid=28913" title="Steve Bracks">
Steve Bracks

Stephen Phillip "Steve" Bracks AC (born 15 October 1954) is a former Australian politician and the 44th Premier of Victoria. He first won the electoral district of Williamstown in 1994 for the Australian Labor Party and was party leader and premier from 1999 to 2007.
Bracks led Labor in Victoria to minority government at the 1999 election, defeating the incumbent Jeff Kennett Liberal and National coalition government. Labor was returned with a majority government after a landslide win at the 2002 election. Labor was elected for a third term at the 2006 election with a substantial but reduced majority. Bracks is the second longest-serving Labor premier in Victorian history. The treasurer, John Brumby, became Labor leader and premier in 2007 when Bracks retired from politics.
Early life.
Steve Bracks was born in Ballarat, where his family owns a fashion business. He is a Lebanese Australian; his paternal grandfather came to Australia as a child from Zahlé in the Beqaa Valley of Lebanon in the 1890s.
Bracks was educated in Ballarat at St Patrick's College and the Ballarat College of Advanced Education (now the University of Ballarat), where he graduated in business studies and education. He became a keen follower of Australian rules football, supporting the Geelong Football Club.
Before politics.
From 1976 to 1981 Bracks was a school commerce teacher. During the 1980s he worked in local government in Ballarat and then as Executive Director of the Ballarat Education Centre. While in these positions he twice (1985 and 1988) contested the seat of Ballarat North in the Victorian Legislative Assembly for the Australian Labor Party.
In 1989 Bracks was appointed statewide manager of Victorian state government employment programs, under the Labor government of John Cain. He then became an adviser to both Cain and Cain's successor as Premier, Joan Kirner. Here he was able to witness from the inside the collapse of the Labor government following the economic and budgetary crisis which began in 1988. This experience gave Bracks a very conservative and cautious view of economic management in government.
Following the defeat of the Kirner government by the Liberal leader Jeff Kennett in late 1992, Bracks became Executive Director of the Victorian Printing Industry Training Board. He quit this post in 1994 when Kirner resigned from Parliament and Bracks was elected for Kirner's seat of Williamstown in the western suburbs of Melbourne, where he now lives with his wife Terry and their three children.
State politics.
Early days.
Bracks was immediately elected to Labor's front bench, as Shadow Minister for Employment, Industrial Relations and Tourism. In 1996, after Labor under John Brumby was again defeated, he became Shadow Treasurer. In March 1999, when it became apparent that Labor was headed for another defeat under Brumby's leadership, Brumby resigned and Bracks was elected Opposition Leader.
First term as Premier.
Political observers were almost unanimous that Bracks had no chance of defeating Liberal premier Jeff Kennett at the September 1999 election: polls gave Kennett a 60% popularity rating. Bracks and his senior colleagues (particularly Brumby, who comes from Bendigo) campaigned heavily in regional areas, accusing Kennett of ignoring regional communities. In response, voters in regional areas deserted the Kennett government. On election night, much to its own surprise, Labor increased its seat count from 29 to 41, with the Liberals and their National Party allies retaining 43, and three falling to rural independents. With the Coalition one seat short of government, the election was to be decided in Frankston East, when the death of incumbent Peter McLellan forced a supplementary election. That supplementary election was won by Labor on a large swing, resulting in a hung parliament. The independents agreed to support a minority Labor government, making Bracks the first Catholic Labor Premier of Victoria since 1932.
Former leader Brumby, appointed Treasurer, was regarded as a major part of the government's success. He and the Deputy Premier and Minister for Health, John Thwaites, and the Attorney-General, Rob Hulls, were regarded as the key ministers in the Bracks government.
Following a pre-1999 election commitment to consider the feasibility of introducing fast rail services to regional centres, in 2000 the government approved funding for the Regional Fast Rail project, upgrading rail lines between Melbourne and Ballarat, Bendigo, Geelong and Traralgon. However, in 2006 the Victorian Auditor General noted that in spite of $750 million spent, "We found that the delivery of more frequent fast rail services in the Geelong, Ballarat, and Bendigo corridors by the agreed dates was not achieved. In total, the journey time outcomes will be more modest than we would have expected with only a minority of travellers likely to benefit from significant journey time improvements. These outcomes occur because giving some passengers full express services means bypassing often large numbers of passengers at intermediate stations along the corridors."
On 14 December 2000, Steve Bracks released a document outlining his government's intent to introduce the Racial and Religious Tolerance Act 2001.
The major criticism of Bracks's first government was that their insistence on consultation stood in the way of effective, proactive government. Bracks, according to critics, achieved little, and lost the excitement of constant change that was characteristic of the Kennett years. The talents of some of the more junior ministers in the government were also questioned. Nevertheless Bracks got through his first term without major mishaps, and his popularity undiminished.
Second term as Premier.
Labor won the 2002 election in a landslide, taking 62 seats out of 88 in the Legislative Assembly—only the third time in Victoria's history that a Labor government had been reelected. In another first, Labor won a slim but clear majority in the Legislative Council as well. While this was the greatest victory Labor had ever had in a Victorian state election, it brought with it considerable risks. With majorities in both houses Bracks could no longer cite his weak parliamentary position as an excuse for inaction.
On 28 August 2002, Bracks, in conjunction with his then New South Wales counterpart, Bob Carr, opened the Mowamba aqueduct between Jindabyne and Dalgety, to divert 38 gigalitres of water a year from Lake Eucumbene to the Snowy and Murray rivers. The ten-year plan cost A$300million with Victoria and NSW splitting the costs. Melbourne Water has stated that within 50 years there will be 20 percent less water going into Victorian reservoirs.
In May 2003 Bracks broke an election promise and announced that the proposed Scoresby Freeway in Melbourne's eastern suburbs would be a tollway rather than a freeway, as promised at the 2002 elections. As well as risking a loss of support in marginal seats in eastern Melbourne, this decision brought about a strong response from the Howard Federal government, which cut off federal funding for the project on the grounds that the Bracks government had reneged on the terms of the federal-state funding agreement. The decision seems to have been on the recommendation of Brumby, who was concerned with the state's budgetary position. Also opposing the decision was the Federal Labor Opposition, which feared anti-Labor reaction at the 2004 Federal election. The then Opposition Leader Mark Latham described a meeting with Bracks and federal shadow ministers, writing:
This backflip, while seen by many as an opportunity for the Liberals to make ground, saw the then leader of the Liberals, Robert Doyle, adopt a much-criticised policy of half tolls, which was later overturned by his successor, Ted Baillieu.
In 2005, Bracks announced that Victorian cattlemen would be banned from using Victoria's "High Plains" to graze cattle, ending a 170-year tradition. Stockmen had been fearing this decision since 1984, when a Labor government excised land to create the Alpine National Park. 300 cattlemen rode horses down Bourke street in protest. Victorian National Party leader Peter Ryan was quoted as saying that Bracks had , a reference to the Banjo Paterson poem "The Man from Snowy River".
Bracks' second government achieved one of Victorian Labor's longest-held goals with a complete reform of the state's system for electing its upper house. It saw the introduction of proportional representation, with eight five-member regions replacing the current single-member constituencies. This system increases the opportunity for minor parties such as the Greens and DLP to win seats in the Legislative Council, giving them a greater chance of holding the balance of power. Illustrating the historic importance Labor assigns to the changes, in a speech to a conference celebrating the 150th anniversary of the Eureka Stockade, Bracks said it was "another victory for the aspirations of Eureka", and has described the changes as "his proudest achievement".
The staging of the 2006 Commonwealth Games, generally viewed as a success (albeit an expensive one), was viewed as a plus for Bracks and the government. With times reasonably good, a perception arguably reinforced by an extensive government advertising campaign selling the virtues of Victoria to Victorians, polls indicated little interest in change, although towards the end of the election campaign polling indicated that the Liberals under Baillieu were closing the gap.
Third term as Premier.
The election campaign was a relatively low-key affair, with the Government and Bracks largely running on their record, as well as their plans to tackle infrastructure issues in their third term. Bracks' image loomed large in Labor's election advertising. Liberal attacks concentrated on the slow process of infrastructure development under Bracks (notably on water supply issues relating to the severe drought affecting Victoria in the election leadup), and new Liberal leader Ted Baillieu promised to start construction on a range of new infrastructure initiatives, including a new dam on the Maribyrnong River and a desalination plant. Labor's broken election promise on Eastlink was also expected to be a factor in some seats in the eastern suburbs of Melbourne.
On 25 November 2006, Steve Bracks won his third election, comfortably defeating Baillieu to secure a third term, with a slightly reduced majority in the Lower House. This marked only the second time that the Victorian Labor Party had won a third term in office. His third term Cabinet was sworn in on 1 December 2006 with Bracks also holding the portfolio of Veterans' Affairs and Multicultural Affairs.
Resignation.
Bracks announced his resignation as Premier on 27 July 2007, saying this was in order to spend more time with his family. He stepped down on 30 July 2007. According to the ABC, Bracks had been under political and personal pressure in the weeks before his resignation. Alone among State Premiers, he had refused to agree to the Federal Government's $10 billion Murray-Darling Basin water conservation plan, and his son had been involved in an accident involving a charge of drink driving. Bracks told a media conference he could no longer give a 100 per cent commitment to politics:
Bracks' deputy John Thwaites announced his resignation on the same day. News of the resignations caused surprise to the general community as well as to politicians. It was revealed that then Federal Labor Leader Kevin Rudd was informed only minutes before the announcement, and tried to talk Bracks out of his decision. Bracks' Treasurer John Brumby was elected unopposed by the Victorian Labor Caucus as Premier, while Attorney-General Rob Hulls was elected Deputy Premier.
One consequence of Bracks leaving politics may have been the introduction of abortion law reform in Victoria. It has been suggested that the resignation of Premier Bracks sowed the seeds for abortion law reform by legislation that parliamentarians previously had refused to support, fearing a backlash from anti-abortion groups led by veteran campaigner Margaret Tighe. Bracks, as a Catholic of Lebanese descent, almost certainly would not have allowed abortion legislation into the parliament, but his successor John Brumby did not share this view, and the Abortion Law Reform Bill introduced by upper house member Candy Broad was passed by the Parliament in 2008.
After politics.
In August 2007, following his resignation as Premier, Bracks announced he would provide a short-term pro bono advising role in East Timor working alongside the newly elected Prime Minister Xanana Gusmão. Bracks was to spend a year travelling between Melbourne and Dili helping with the establishment of Gusmão's administration, the key departments that would need to be involved, and advising on how they would be accountable and reportable to the legislature.
During 2008 Bracks indicated his support for Victorian abortion law reform in Victoria.
In addition to his role advising Gusmão, Bracks also joined several company advisory boards: KPMG, insurance firm Jardine Lloyd Thompson Group, the AIMS Financial Group and the NAB. The KPMG appointment was controversial, as the Victorian government had awarded the firm over 100 contracts during Bracks' time as Premier. On 14 February 2008, the Federal Labor Government appointed Bracks to head an inquiry into the ongoing viability of the Australian car industry.
In 2010, Bracks was appointed a Companion of the Order of Australia for services to the community and the Parliament of Victoria. In recognition of his distinguished services to the Victorian community, he was awarded the degree of Doctor of Laws (honoris causa) – LL.D "(h.c.)" by Deakin University on 27 April 2010. He was also appointed to the Honorary Chair of the Deakin University Foundation.
In February 2013 after the announcement that Nicola Roxon would retire from federal politics, Bracks was cited as a possible candidate for her safe Labor seat of Gellibrand, but he ruled out running for the seat.
Bracks was appointed to the role of Australian Consul-General in New York in May 2013, by the Federal ALP Government of Julia Gillard. At the time, shadow Foreign Minister, the Coalition's Julie Bishop, described the appointment as 'inappropriate' because of the proximity to the upcoming election and 'arrogant' because of a lack of consultation with the then opposition. Following the defeat of the ALP at the 7 September election, incoming foreign minister Julie Bishop reversed the appointment in a decision described as 'petty and vindictive' by acting ALP foreign affairs spokeswoman Tanya Plibersek.

</doc>
<doc id="28915" url="http://en.wikipedia.org/wiki?curid=28915" title="Small Isles">
Small Isles

The Small Isles (Scottish Gaelic: Na h-Eileanan Tarsainn) are a small archipelago of islands in the Inner Hebrides, off the west coast of Scotland. They lie south of Skye and north of Mull and Ardnamurchan – the most westerly point of mainland Scotland.
The four main islands are Canna, Rùm, Eigg and Muck. The largest is Rùm with an area of 105 km2.
The islands now form part of Lochaber, in the Highland council area. Until 1891 Canna, Rùm and Muck were historically part of the counties of Argyll; Eigg was historically part of Inverness-shire, and all of the Small Isles were in Inverness-shire, from 1891 to 1975. The Gaelic name is literally "cross isles" referring to their position between Morar and The Uists.
Smaller islands surrounding the main four include: 
There are also a number of skerries: 
The Small Isles is one of 40 National Scenic Areas in Scotland.

</doc>
<doc id="28916" url="http://en.wikipedia.org/wiki?curid=28916" title="Shetland">
Shetland

Shetland (; Scottish Gaelic: "Sealtainn" ]), also called the Shetland Islands, is a subarctic archipelago of Scotland that lies north-east of the island of Great Britain and forms part of the United Kingdom.
The islands lie some 80 km to the northeast of Orkney and 280 km southeast of the Faroe Islands and form part of the division between the Atlantic Ocean to the west and the North Sea to the east. The total area is 1468 km2 and the population totalled 23,167 in 2011. Comprising the Shetland constituency of the Scottish Parliament, Shetland is also one of the 32 council areas of Scotland; the islands' administrative centre and only burgh is Lerwick.
The largest island, known simply as "Mainland", has an area of 967 km2, making it the third-largest Scottish island and the fifth-largest of the British Isles. There are an additional 15 inhabited islands. The archipelago has an oceanic climate, a complex geology, a rugged coastline and many low, rolling hills.
Humans have lived there since the Mesolithic period, and the earliest written references to the islands date back to Roman times. The early historic period was dominated by Scandinavian influences, especially Norway, and the islands did not become part of Scotland until the 15th century. When Shetland became part of the Kingdom of Great Britain in 1707, trade with northern Europe decreased. Fishing has continued to be an important aspect of the economy up to the present day. The discovery of North Sea oil in the 1970s significantly boosted Shetland incomes, employment and public sector revenues.
The local way of life reflects the joint Norse and Scottish heritage including the Up Helly Aa fire festival, and a strong musical tradition, especially the traditional fiddle style. The islands have produced a variety of writers of prose and poetry, many of whom use the local Shetlandic dialect. There are numerous areas set aside to protect the local fauna and flora, including a number of important seabird nesting sites. The Shetland pony and Shetland Sheepdog are two well-known Shetland animal breeds.
The islands' motto, which appears on the Council's coat of arms, is "Með lögum skal land byggja". This Icelandic phrase is taken from Njáls saga and means "By law shall the land be built up".
Etymology.
The name of Shetland is derived from the Old Norse words, "hjalt" (hilt), and "land" (land).
In AD 43 and 77 the Roman authors Pomponius Mela and Pliny the Elder referred to the seven islands they call "Haemodae" and "Acmodae" respectively, both of which are assumed to be Shetland. Another possible early written reference to the islands is Tacitus' report in AD 98, after describing the discovery and conquest of Orkney, that the Roman fleet had seen "Thule, too". In early Irish literature, Shetland is referred to as "Inse Catt"—"the Isles of Cats", which may have been the pre-Norse inhabitants' name for the islands. The Cat tribe also occupied parts of the northern Scottish mainland and their name can be found in Caithness, and in the Gaelic name for Sutherland ("Cataibh", meaning "among the Cats").
The oldest version of the modern name Shetland is "Hetlandensis", the Latinised adjectival form of the Old Norse name recorded in a letter from Harald count of Shetland in 1190, becoming "Hetland" in 1431 after various intermediate transformations. It is possible that the Pictish "cat" sound forms part of this Norse name. It then became "Hjaltland" in the 16th century.
As Norn was gradually replaced by Scots, "Hjaltland" became "Ȝetland". The initial letter is the Middle Scots letter, "yogh", the pronunciation of which is almost identical to the original Norn sound, "/hj/". When the use of the letter yogh was discontinued, it was often replaced by the similar-looking letter z, hence "Zetland", the misspelt form used to describe the pre-1975 county council.
Most of the individual islands have Norse names, although the derivations of some are obscure and may represent pre-Norse, possibly Pictish or even pre-Celtic names or elements.
Geography and geology.
Shetland is around 170 km north of mainland Scotland, covers an area of 1468 km2 and has a coastline 2702 km long.
Lerwick, the capital and largest settlement, has a population of 6,958 and about half of the archipelago's total population of 23,167 people live within 16 km of the town.
Scalloway on the west coast, which was the capital until 1708, has a population of less than 1,000.
Only 16 of about 100 islands are inhabited. The main island of the group is known as Mainland, and of the next largest, Yell, Unst, and Fetlar lie to the north and Bressay and Whalsay lie to the east. East and West Burra, Muckle Roe, Papa Stour, Trondra and Vaila are smaller islands to the west of Mainland. The other inhabited islands are Foula 28 km west of Walls, Fair Isle 38 km south-west of Sumburgh Head, and the Out Skerries to the east.
The uninhabited islands include Mousa, known for the Broch of Mousa, the finest preserved example in Scotland of these Iron Age round towers, St Ninian's Isle connected to Mainland by the largest active tombolo in the UK, and Out Stack, the northernmost point of the British Isles. Shetland's location means that it provides a number of such records: Muness is the most northerly castle in the United Kingdom and Skaw the most northerly settlement.
The geology of Shetland is complex, with numerous faults and fold axes. These islands are the northern outpost of the Caledonian orogeny, and there are outcrops of Lewisian, Dalriadan and Moine metamorphic rocks with histories similar to their equivalents on the Scottish mainland. There are also Old Red Sandstone deposits and granite intrusions. The most distinctive features are the ultrabasic ophiolite, peridotite and gabbro on Unst and Fetlar, which are remnants of the Iapetus Ocean floor. Much of Shetland's economy depends on the oil-bearing sediments in the surrounding seas. Geological evidence shows that in around 6100 BC a tsunami caused by the Storegga Slides hit Shetland, as well as the rest of the east coast of Scotland, and may have created a wave of up to 25 m high in the voes where modern populations are highest.
The highest point of Shetland is Ronas Hill, which only reaches 450 m. The Pleistocene glaciations entirely covered the islands. During this period, the Stanes of Stofast, a 2000-tonne glacial erratic, came to rest on a prominent hilltop in Lunnasting.
Shetland is a National Scenic Area which, unusually, is made up of a number of discrete locations: Fair Isle, Foula, South West Mainland (including the Scalloway Islands), Muckle Roe, Esha Ness, Fethaland and Herma Ness.
Climate.
Shetland has an oceanic sub-polar climate, with long but cool winters and short mild summers. The climate all year round is moderate due to the influence of the surrounding seas, with average peak temperatures of 7 °C in March and 18 C in July and August. Temperatures over 25 °C are very rare. The highest temperature on record was 28.4 °C in July 1991 and the coldest -8.9 °C in the Januarys of 1952 and 1959. The frost-free period may be as little as three months. In contrast, inland areas of nearby Scandinavia on similar latitudes experience significantly larger temperature differences between summer and winter, with the average highs of regular July days comparable to Shetland's all-time record heat, further demonstrating the moderating effect of the Atlantic Ocean. In contrast, winters are considerably milder than those expected in nearby continental areas, even comparable to winter temperatures of many parts of England and Wales much further south.
The general character of the climate is windy and cloudy with at least 2 mm of rain falling on more than 250 days a year. Average yearly precipitation is 1003 mm, with November and December the wettest months. Snowfall is usually confined to the period November to February, and snow seldom lies on the ground for more than a day. Less rain falls from April to August although no month receives less than 50 mm. Fog is common during summer due to the cooling effect of the sea on mild southerly airflows.
Due to the islands' latitude, on clear winter nights the "northern lights" can sometimes be seen in the sky, while in summer there is almost perpetual daylight, a state of affairs known locally as the "simmer dim". Annual bright sunshine averages 1090 hours and overcast days are common.
Prehistory.
Due to the practice, dating to at least the early Neolithic, of building in stone on virtually treeless islands, Shetland is extremely rich in physical remains of the prehistoric eras and there are over 5,000 archaeological sites all told. A midden site at West Voe on the south coast of Mainland, dated to 4320–4030 BC, has provided the first evidence of Mesolithic human activity on Shetland. The same site provides dates for early Neolithic activity and finds at Scord of Brouster in Walls have been dated to 3400 BC. "Shetland knives" are stone tools that date from this period made from felsite from Northmavine.
Pottery shards found at the important site of Jarlshof also indicate that there was Neolithic activity there although the main settlement dates from the Bronze Age. This includes a smithy, a cluster of wheelhouses and a later broch. The site has provided evidence of habitation during various phases right up until Viking times. Heel-shaped cairns, are a style of chambered cairn unique to Shetland, with a particularly large example on Vementry.
Numerous brochs were erected during the Iron Age. In addition to Mousa there are significant ruins at Clickimin, Culswick, Old Scatness and West Burrafirth, although their origin and purpose is a matter of some controversy. The later Iron Age inhabitants of the Northern Isles were probably Pictish, although the historical record is sparse. Hunter (2000) states in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.” In 2011, the collective site, "The Crucible of Iron Age Shetland", including Broch of Mousa, Old Scatness and Jarlshof, joined the UKs "Tentative List" of World Heritage Sites.
History.
Scandinavian colonisation.
The expanding population of Scandinavia led to a shortage of available resources and arable land there and led to a period of Viking expansion, the Norse gradually shifting their attention from plundering to invasion. Shetland was colonised during the late 8th and 9th centuries, the fate of the existing indigenous population being uncertain. Modern Shetlanders have almost identical proportions of Scandinavian matrilineal and patrilineal genetic ancestry, suggesting that the islands were settled by both men and women in equal measure.
Vikings then made the islands the headquarters of pirate expeditions carried out against Norway and the coasts of mainland Scotland. In response, Norwegian king Harald Hårfagre ("Harald Fair Hair") annexed the Northern Isles (comprising Orkney and Shetland) in 875. Rognvald Eysteinsson received Orkney and Shetland from Harald as an earldom as reparation for the death of his son in battle in Scotland, and then passed the earldom on to his brother Sigurd the Mighty.
The islands were Christianised in the late 10th century. King Olav Tryggvasson summoned the "jarl" Sigurd the Stout during a visit to Orkney and said, "I order you and all your subjects to be baptised. If you refuse, I'll have you killed on the spot and I swear I will ravage every island with fire and steel." Unsurprisingly, Sigurd agreed and the islands became Christian at a stroke. Unusually, from c. 1100 onwards the Norse "jarls" owed allegiance both to Norway and to the Scottish crown through their holdings as Earls of Caithness.
In 1194, when Harald Maddadsson was Earl of Orkney and Shetland, a rebellion broke out against King Sverre Sigurdsson of Norway. The "Øyskjeggs" ("Island Beardies") sailed for Norway but were beaten in the Battle of Florvåg near Bergen. After his victory King Sverre placed Shetland under direct Norwegian rule, a state of affairs that continued for nearly two centuries.
Increased Scottish interest.
From the mid-13th century onwards Scottish monarchs increasingly sought to take control of the islands surrounding the mainland. The process was begun in earnest by Alexander II and was continued by his successor Alexander III. This strategy eventually led to an invasion by Haakon Haakonsson, King of Norway. His fleet assembled in Bressay Sound before sailing for Scotland. After the stalemate of the Battle of Largs, Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. His death halted any further Norwegian expansion in Scotland and following this ill-fated expedition, the Hebrides and Mann were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth, although the Scots recognised continuing Norwegian sovereignty over Orkney and Shetland.
Pawned to Scotland.
In the 14th century, Orkney and Shetland remained a Norwegian province, but Scottish influence was growing. Jon Haraldsson, who was murdered in Thurso in 1231, was the last of an unbroken line of Norse jarls, and thereafter the earls were Scots noblemen of the houses of Angus and St. Clair. On the death of Haakon VI in 1380, Norway formed a political union with Denmark after which the interest of the royal house in the islands declined. In 1469, Shetland was pledged by Christian I, in his capacity as King of Norway, as security against the payment of the dowry of his daughter Margaret, betrothed to James III of Scotland. As the money was never paid, the connection with the crown of Scotland has become perpetual. In 1470, William Sinclair, 1st Earl of Caithness ceded his title to James III and the following year the Northern Isles were directly annexed to the Crown of Scotland, a process confirmed by Parliament in 1472. Nonetheless, Shetland's connection with Norway has proven to be enduring.
From the early 15th century on the Shetlanders sold their goods through the Hanseatic League of German merchantmen. The Hansa would buy shiploads of salted fish, wool and butter and import salt, cloth, beer and other goods. The late 16th century and early 17th century was dominated by the influence of the despotic Robert Stewart, Earl of Orkney, who was granted the islands by his half-sister Mary Queen of Scots, and his son Patrick. The latter commenced the building of Scalloway Castle, but after his imprisonment in 1609 the Crown annexed Orkney and Shetland again until 1643 when Charles I granted them to William Douglas, 7th Earl of Morton. These rights were held on and off by the Mortons until 1766, when they were sold by James Douglas, 14th Earl of Morton to Laurence Dundas.
Early British rule.
The trade with the North German towns lasted until the 1707 Act of Union when high salt duties prohibited the German merchants from trading with Shetland. Shetland then went into an economic depression as the Scottish and local traders were not as skilled in trading with salted fish. However, some local merchant-lairds took up where the German merchants had left off, and fitted out their own ships to export fish from Shetland to the Continent. For the independent farmers of Shetland this had negative consequences, as they now had to fish for these merchant-lairds.
Smallpox afflicted the islands in the 17th and 18th centuries, but as vaccines became common after 1760 the population increased to a maximum of 31,670 in 1861. However, British rule came at price for many ordinary people as well as traders. The Shetlanders nautical skills were sought by the Royal Navy. Some 3,000 served during the Napoleonic wars from 1800 to 1815 and press gangs were rife. During this period 120 men were taken from Fetlar alone and only 20 of them returned home. By the late 19th century 90% of all Shetland was owned by just 32 people, and between 1861 and 1881 more than 8,000 Shetlanders emigrated. With the passing of the Crofters' Act in 1886 the Liberal prime minister William Gladstone emancipated crofters from the rule of the landlords. The Act enabled those who had effectively been landowners' serfs to become owner-occupiers of their own small farms. By this time fishermen from Holland, who had traditionally gathered each year off the coast of Shetland to fish for herring, triggered an industry in the islands that boomed from around 1880 until the 1920s when stocks of the fish began to dwindle.
20th century.
During World War I many Shetlanders served in the Gordon Highlanders, a further 3,000 served in the Merchant Navy and more than 1500 in a special local naval reserve. The 10th Cruiser Squadron was stationed at Swarbacks Minn and during a single year from March 1917 more than 4,500 ships sailed from Lerwick as part of an escorted convoy system. In total, Shetland lost more than 500 men, a higher proportion than any other part of Britain, and there were further waves of emigration in the 1920s and 1930s.
During World War II a Norwegian naval unit nicknamed the "Shetland Bus" was established by the Special Operations Executive in the autumn of 1940 with a base first at Lunna and later in Scalloway to conduct operations around the coast of Norway. About 30 fishing vessels used by Norwegian refugees were gathered and the Shetland Bus conducted covert operations, carrying intelligence agents, refugees, instructors for the resistance, and military supplies. It made over 200 trips across the sea with Leif Larsen, the most highly decorated allied naval officer of the war, making 52 of them. Several RAF bases were also established at Sullom Voe and several lighthouses suffered enemy air attacks.
Oil reserves discovered in the later 20th century in the seas both east and west of Shetland have provided a much needed alternative source of income for the islands. The East Shetland Basin is one of Europe's largest oil fields and as a result of the oil revenue and the cultural links with Norway, a small independence movement developed briefly to recast the constitutional position of Shetland. It saw as its model the Isle of Man, as well as Shetland's closest neighbour, the Faroe Islands, an autonomous dependency of Denmark.
Economy.
Today, the main revenue producers in Shetland are agriculture, aquaculture, fishing, renewable energy, the petroleum industry (crude oil and natural gas production), the creative industries and tourism.
Fishing remains central to the islands' economy today, with the total catch being 75,767 tonne in 2009, valued at over £73.2 million. Mackerel makes up more than half of the catch in Shetland by weight and value, and there are significant landings of haddock, cod, herring, whiting, monkfish and shellfish. Farming is mostly concerned with the raising of Shetland sheep, known for their unusually fine wool. Crops raised include oats and barley; however, the cold, windswept islands make for a harsh environment for most plants. Crofting, the farming of small plots of land on a legally restricted tenancy basis, is still practiced and is viewed as a key Shetland tradition as well as an important source of income.
Oil and gas were first landed in 1978 at Sullom Voe, which has subsequently become one of the largest terminals in Europe. Taxes from the oil have increased public sector spending on social welfare, art, sport, environmental measures and financial development. Three quarters of the islands' workforce is employed in the service sector, and the Shetland Islands Council alone accounted for 27.9% of output in 2003. Shetland's access to oil revenues has funded the Shetland Charitable Trust, which in turn funds a wide variety of local programmes. The balance of the fund in 2011 was £217 million, i.e., about £9,500 per head.
In January 2007, the Shetland Islands Council signed a partnership agreement with Scottish and Southern Energy for the Viking Wind Farm, a 200-turbine wind farm and subsea cable. This renewable energy project would produce about 600 megawatts and contribute about £20 million to the Shetland economy per year. The plan is meeting significant opposition within the islands, primarily resulting from the anticipated visual impact of the development. The PURE project on Unst is a research centre which uses a combination of wind power and fuel cells to create a wind hydrogen system. The project is run by the Unst Partnership, the local community's development trust.
Knitwear is important both to the economy and culture of Shetland, and the Fair Isle design is well-known. However, the industry faces challenges due to plagiarism of the word "Shetland" by manufacturers operating elsewhere, and a certification trademark, "The Shetland Lady", has been registered. 
Shetland is served by a weekly local newspaper, "The Shetland Times" and the online "Shetland News" with radio service being provided by BBC Radio Shetland and the commercial radio station SIBC.
Shetland is a popular destination for cruise ships, and in 2010 the Lonely Planet guide named Shetland as the sixth best region in the world for tourists seeking unspoilt destinations. The islands were described as "beautiful and rewarding" and the Shetlanders as "a fiercely independent and self-reliant bunch". Overall visitor expenditure was worth £16.4 million in 2006, in which year just under 26,000 cruise liner passengers arrived at Lerwick Harbour. In 2009, the most popular visitor attractions were the Shetland Museum, the RSPB reserve at Sumburgh Head, Bonhoga Gallery at Weisdale Mill and Jarlshof.
Transport.
Transport between islands is primarily by ferry, and Shetland Islands Council operates various inter-island services. Shetland is also served by a domestic connection from Lerwick to Aberdeen on mainland Scotland. This service, which takes about 12 hours, is operated by NorthLink Ferries. Some services also call at Kirkwall, Orkney, which increases the journey time between Aberdeen and Lerwick by 2 hours.
Sumburgh Airport, the main airport on Shetland, is located close to Sumburgh Head, 40 km south of Lerwick. Loganair operates flights for FlyBe to other parts of Scotland up to ten times a day, the destinations being Kirkwall, Aberdeen, Inverness, Glasgow and Edinburgh. Lerwick/Tingwall Airport is located 11 km west of Lerwick. Operated by Directflight Ltd. in partnership with Shetland Islands Council, it is devoted to inter-island flights from the Shetland Mainland to most of the inhabited islands.
Scatsta Airport near Sullom Voe allows frequent charter flights from Aberdeen to transport oilfield workers and this small terminal has the fifth largest number of international passengers in Scotland.
Public bus services are operated on Mainland, Whalsay, Burra, Unst and Yell.
The archipelago is exposed to wind and tide, and there are numerous sites of wrecked ships. Lighthouses are sited as an aid to navigation at various locations.
Public services.
The Shetland Islands Council is the Local Government authority for all the islands, based in Lerwick Town Hall.
Shetland is sub-divided into 18 community council areas and into 12 civil parishes that are used for statistical purposes.
Education.
In Shetland there are two High Schools—Anderson and Brae—seven Junior High Schools, and over thirty primary schools.
Shetland is also home to the North Atlantic Fisheries College, the Centre for Nordic Studies and Shetland College, which are all associated with the University of the Highlands and Islands.
Sport.
The islands are represented by the Shetland football team who regularly compete in the Island Games. The islands' senior football league is the G&S Flooring Premier League.
Churches and religion.
The Reformation reached the archipelago in 1560. This was an apparently peaceful transition and there is little evidence of religious intolerance in Shetland's recorded history.
A variety of different religious denominations are represented in the islands.
The Methodist Church has a relatively high membership in Shetland, which is a District of the Methodist Church (with the rest of Scotland comprising a separate District).
The Church of Scotland has a Presbytery of Shetland that includes St. Columba's Church in Lerwick.
The Catholic population is served by the church of St. Margaret and the Sacred Heart in Lerwick. The Parish is part of the Diocese of Aberdeen.
The Scottish Episcopal Church (part of the Anglican Communion) has regular worship at St Magnus' Church, Lerwick, St Colman's Church, Burravoe, and the Chapel of Christ the Encompasser, Fetlar, the last of which is maintained by the Society of Our Lady of the Isles, the most northerly and remote Anglican religious order of nuns.
The Church of Jesus Christ of Latter-day Saints has a congregation in Lerwick. The former print works and offices of the local newspaper, The Shetland Times, has been converted into a chapel.
Politics.
Shetland is represented in the House of Commons as part of the Orkney and Shetland constituency, which elects one Member of Parliament, the current incumbent being Alistair Carmichael. This seat has been held by the Liberal Democrats or their predecessors the Liberal Party since 1950, longer than any other they represent in the UK.
In the Scottish Parliament the Shetland constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system. The current MSP is Tavish Scott of the Scottish Liberal Democrats. Shetland is within the Highlands and Islands electoral region.
The political composition of the Council is 22 Independents. Thus it is one of only three Councils in Scotland with a majority of elected members not representing a political party.
Roy Grönneberg, who founded the local chapter of the Scottish National Party in 1966, designed the flag of Shetland in cooperation with Bill Adams to mark the 500th anniversary of the transfer of the islands from Norway to Scotland. The colours are identical to those of the Flag of Scotland, but are shaped in the Nordic cross. After several unsuccessful attempts, including a plebiscite in 1985, the Lord Lyon King of Arms approved it as the official flag of Shetland in 2005.
Local culture and the arts.
After the islands were transferred to Scotland, thousands of Scots families emigrated to Shetland in the 16th and 17th centuries but studies of the genetic makeup of the islands' population indicate that Shetlanders are just under half Scandinavian in origin. This combination is reflected in many aspects of local life. For example, almost every place name in use can be traced back to the Vikings. The Norn language was a form of Old Norse, which continued to be spoken until the 18th century when it was replaced by an insular dialect of Scots known as Shetlandic, which is in turn being replaced by Scottish English. Although Norn was spoken for hundreds of years it is now extinct and few written sources remain. Shetlandic is used both in local radio and dialect writing, and kept alive by the Shetland Folk Society.
The Lerwick Up Helly Aa is one of a variety of fire festivals held in Shetland annually in the middle of winter, it is always started on the last Tuesday of January. The festival is just over 100 years old in its present, highly organised form. Originally, a festival held to break up the long nights of winter and mark the end of Yule, the festival has become one celebrating the isles' heritage and includes a procession of men dressed as Vikings and the burning of a replica longship.
The cuisine of Shetland is based on locally produced lamb, beef and seafood, much of it organic. Inevitably, the real ale-producing Valhalla Brewery is the most northerly in Britain. The Shetland Black is a variety of blue potato with a dark skin and indigo coloured flesh markings.
Shetland competes in the biennial International Island Games, which it hosted in 2005.
Music.
Shetland's culture and landscapes have inspired a variety of musicians, writers and film-makers. The Forty Fiddlers was formed in the 1950s to promote the traditional fiddle style, which is a vibrant part of local culture today. Notable exponents of Shetland folk music include Aly Bain, Fiddlers' Bid, and the late Tom Anderson and Peerie Willie Johnson. Thomas Fraser was a country musician who never released a commercial recording during his life, but whose work has become popular more than 20 years after his untimely death in 1978.
Writers.
Walter Scott's 1822 novel "The Pirate" is set in "a remote part of Shetland", and was inspired by his 1814 visit to the islands. The name "Jarlshof" meaning "Earl's Mansion" is a coinage of his. Hugh MacDiarmid, the Scots poet and writer lived in Whalsay from the mid-1930s through 1942, and wrote many poems there, including a number that directly address or reflect the Shetland environment such as "On A Raised Beach", which was inspired by a visit to West Linga. The 1975 novel "North Star" by Hammond Innes is largely set in Shetland and Raman Mundair's 2007 book of poetry "A Choreographer's Cartography" offers a British Asian perspective on the landscape. The "Shetland Quartet" by Ann Cleeves, who previously lived in Fair Isle, is a series of crime novels set around the islands. In 2013 her novel "Red Bones" became the basis of BBC crime drama television series "Shetland".
Vagaland, who grew up in Walls, was arguably Shetland's finest poet of the 20th century. Haldane Burgess was a Shetland historian, poet, novelist, violinist, linguist and socialist and Rhoda Bulter (1929 – 1994) is one of the best-known Shetland poets of recent times. Other 20th and 21st century poets and novelists include Christine De Luca, Robert Alan Jamieson who grew up in Sandness, the late Lollie Graham of Veensgarth, Stella Sutherland of Bressay, the late William J Tait from Yell and Laureen Johnson.
There are two monthly magazines in production: "Shetland Life" and "i'i' Shetland". The quarterly The New Shetlander, founded in 1947, is said to be Scotland's longest-running literary magazine. For much of the later 20th century it was the major vehicle for the work of local writers - and others, including early work by George Mackay Brown.
Films.
Michael Powell made "The Edge of the World" in 1937, a dramatisation based on the true story of the evacuation of the last 36 inhabitants of the remote island of St Kilda on 29 August 1930. St Kilda lies in the Atlantic Ocean, 64 km west of the Outer Hebrides but Powell was unable to get permission to film there. Undaunted, he made the film over four months during the summer of 1936 on Foula and the film transposes these events to Shetland. Forty years later, the documentary "Return To The Edge Of The World" was filmed, capturing a reunion of cast and crew of the film as they revisited the island in 1978.
A number of other films have been made on or about Shetland including "A Crofter's Life in Shetland" (1932) "A Shetland Lyric" (1934), "Devil's Gate" (2003) and "It's Nice Up North" (2006), a comedy documentary by Graham Fellows. An annual film festival takes place in the newly built Mareel, a cinema, music and education venue.
Wildlife.
Shetland has three national nature reserves, at the seabird colonies of Hermaness and Noss, and at Keen of Hamar to preserve the serpentine flora. There are a further 81 SSSIs, which cover 66% or more of the land surfaces of Fair Isle, Papa Stour, Fetlar, Noss and Foula. Mainland has 45 separate sites.
Flora.
The landscape in Shetland is marked by the grazing of sheep and the harsh conditions have limited the total number of plant species to about 400. Native trees such as rowan and crab apple are only found in a few isolated places such as cliffs and loch islands. The flora is dominated by Arctic-alpine plants, wild flowers, moss and lichen. Spring squill, buck's-horn plantain, Scots lovage, roseroot and sea campion are abundant, especially in sheltered places. Shetland mouse-ear ("Cerastium nigrescens") is an endemic flowering plant found only in Shetland. It was first recorded in 1837 by botanist Thomas Edmondston. Although reported from two other sites in the 19th century, it currently grows only on two serpentine hills on the island of Unst. The nationally scarce oysterplant is found on several islands and the British Red Listed bryophyte "Thamnobryum alopecurum" has also been recorded.
Fauna.
Shetland has numerous seabird colonies. Birds found on the islands include Atlantic puffin, storm-petrel, red-throated diver, northern gannet and bonxie. Numerous rarities have also been recorded including black-browed albatross and snow goose, and a single pair of snowy owls bred on Fetlar from 1967 to 1975. The Shetland wren, Fair Isle wren and Shetland starling are subspecies endemic to Shetland. There are also populations of various moorland birds such as curlew, snipe and golden plover.
The geographical isolation and recent glacial history of Shetland have resulted in a depleted mammalian fauna and the brown rat and house mouse are two of only three species of rodent present on the islands. The Shetland field mouse is the third and the archipelago's fourth endemic subspecies, of which there are three varieties on Yell, Foula and Fair Isle. They are variants of "Apodemus sylvaticus" and archaeological evidence suggests that this species was present during the Middle Iron Age (around 200 BC to AD 400). It is possible that "Apodemus" was introduced from Orkney where a population has existed since at the least the Bronze Age.
Domesticated animals.
There is a variety of indigenous breeds, of which the diminutive Shetland pony is probably the best known, as well as being an important part of the Shetland farming tradition. The first written record of the pony was in 1603 in the Court Books of Shetland and, for its size, it is the strongest of all the horse breeds. Others are the Shetland Sheepdog or "Sheltie", the endangered Shetland cattle and Shetland Goose and the Shetland sheep which is believed to have originated prior to 1000 AD. The Grice was a breed of semi-domesticated pig that had a habit of attacking lambs, and that became extinct in 1930.
See also.
Lists
About Shetland
Other
References.
General references.
</dl>

</doc>
<doc id="28917" url="http://en.wikipedia.org/wiki?curid=28917" title="Soay, Skye">
Soay, Skye

Soay (Scottish Gaelic: "Sòdhaigh", ]) is an island just off the coast of Skye, in the Inner Hebrides of Scotland.
Geography.
Soay lies to the west of Loch Scavaig on the south-west coast of Skye, from which it is separated by Soay Sound. Unlike its neighbours Skye and Rùm, Soay is low-lying, reaching 141 m at Beinn Bhreac. The dumb-bell shaped island is virtually cut in half by inlets that form Soay Harbour (N) and the main bay, Camas nan Gall (to the S). The main settlement, Mol-chlach is on the shore of Camas nan Gall. It is normally reached by boat from Elgol. The island is part of the Cuillin Hills National Scenic Area, one of 40 in Scotland.
History.
The name derives from Old Norse "so-øy" meaning "Sheep Island". Camas nan Gall (G: "Bay of Foreigners") is probably named after the Norse invaders, after whom the Hebrides ("Na h-Innse Gall") are also named.
The population peaked at 158 in 1851, following eviction of crofters from Skye in the Highland Clearances.
In 1946, author Gavin Maxwell bought the island and established a factory to process shark oil from basking sharks. The enterprise was unsuccessful, lasting just three years. Maxwell wrote about it in his book "Harpoon at a Venture". After the failure of the business the island was sold on to Maxwell's business partner, Tex Geddes. The island had the first solar-powered telephone exchange in the world.
Previously mainly Scottish Gaelic-speaking, most of the population was evacuated to Mull on 20 June 1953, since when the island has been sparsely populated. In 2001 the population was 7. By 2003 this has dwindled to 2 and the usually resident population in 2011 was a single individual.
Stamps.
Local stamps were issued for Soay between 1965 and 1967, all on the Europa theme, some being overprinted to commemorate Sir Winston Churchill. As the stamps were produced without the owner's permission, they are regarded as bogus.

</doc>
<doc id="28918" url="http://en.wikipedia.org/wiki?curid=28918" title="Storytelling game">
Storytelling game

A storytelling game is a game where two or more persons collaborate on telling a spontaneous story. Usually, each player takes care of one or more characters in the developing story. Some games in the tradition of role-playing games require one participant to take the roles of the various supporting characters, as well as introducing non-character forces (for example, a flood), but other systems dispense with this figure and distribute this function among all players.
Since this person usually sets the ground and setting for the story, he or she is often referred to as the "storyteller" (often contracted to "ST") or "narrator". Any number of other alternate forms may be used, many of which are variations on the term "gamemaster"; these variants are especially common in storytelling games derived from or similar to role-playing games.
In contrast to improv theater, storytelling gamers describe the actions of their characters rather than acting them out, except during dialogue or, in some games, monologue. However, "live action" versions exist, which are very much akin to theater except in the crucial absence of a non-participating audience.
Role-playing games.
The most popular modern storytelling games originated as a subgenre of role-playing games, where the game rules and statistics are heavily de-emphasised in favor of creating a believable story and immersive experience for all involved. So while in a conventional game the announcement that one's character is going to leap over a seven-meters-wide canyon will be greeted with the request to roll a number of dice, a player in a storytelling game who wishes to have a character perform a similar feat will have to convince the others (especially the storyteller) why it is both probable and keeping within the established traits of their character to successfully do so. As such, these games are a subclass of diceless role-playing games.
Not all players find the storytelling style of role-playing satisfying. Many role-playing gamers are more comfortable in a system that gives them less freedom, but where they do not need to police themselves; others find it easier to enjoy a system where a more concrete framework of rules is already present. These three types of player are discussed by the GNS theory.
Some role-playing game systems which describe themselves as "storytelling games" nevertheless use randomisers rather than story in the arbitration of the rules, often in the form of a contest of Rock, Paper, Scissors or a card drawn from a deck of cards. Such "storytelling" games are instead simplified or streamlined forms of traditional role-playing games. Conversely, most modern role-playing games encourage gamemasters to ignore their gaming systems if it makes for a more enjoyable story, even though they may not describe themselves as "storytelling" games.
A growing number of websites utilize a bulletin board system, in which the gaming is akin to Collaborative Fiction but known as a "Literary Role-Playing Game" (not to be confused with LARPs). The players contribute to an ongoing story with defined parameters but no narrator or directing force. A 'moderator' may oversee the gamers to ensure that the rules, guidelines and parameters of the gaming "world" are being upheld, but otherwise the writers are free to interact as players in an improvisational play. Many of these "Literary RPGs" are fan-fiction based, such as (most prevalently) Tolkien's Middle-earth, Star Wars, Harry Potter, Twilight, any number of anime and manga sources, or they are simply based in thematic worlds such as the mythologies of Ancient Greece, fairy tales, the Renaissance or science fiction. Most often referred to as "Literary RPGs" and place a greater emphasis on writing skill and storytelling ability than on any sense of competition driven outcome.
White Wolf Game Studio's Storyteller System, which is used in World of Darkness role-playing games such as "" and live-action games under the Mind's Eye Theatre imprint, is the best-known and most popular role-playing game described as a "storytelling game".
Alternate form role-playing games.
An early design of a collaborative storytelling game not based in simulation was created by Chris Engle c. 1988 with his "Matrix Game". In this system, a referee decides the likeliness of the facts proposed by the players, and those facts happen or are rejected according with a dice roll. Players can propose counter-arguments that are resolved in a dice rolling contest. A conflict round can follow to resolve any inconsistencies or further detail new plot points. Matrix Games are now presented in a board game format.
In 1999, game designer Ian Millington developed an early work called "Ergo" which established the basis for collaborative role-playing. It was designed with the rules of the Fudge universal role-playing system in mind but added modifications necessary to get rid of the need for a gamemaster, distributing the responsibility for the game and story equally among all players and undoing the equivalence between player and character.
Modern rule systems (such as the coin system in Universalis) rely less on randomness and more in collaboration between players. This includes rules based on economic systems that force players to negotiate the details of the story, and solve conflicts based on the importance that they give to a given plot element and the resources they're willing to spend to make it into the story.

</doc>
<doc id="28922" url="http://en.wikipedia.org/wiki?curid=28922" title="Scorpion">
Scorpion

Scorpions are predatory arthropod animals of the order Scorpiones within the class Arachnida. They have eight legs and are easily recognised by the pair of grasping pedipalps and the narrow, segmented tail, often carried in a characteristic forward curve over the back, ending with a venomous stinger. Scorpions range in size from 9 mm ("Typhlochactas mitchelli") to 20 cm ("Hadogenes troglodytes").
The evolutionary history of scorpions goes back to the Silurian era 430 million years ago. They have adapted to a wide range of environmental conditions and can now be found on all continents except Antarctica. Scorpions number about 1750 described species, with 13 extant families recognised to date. Only about 25 of these species are known to have venom capable of killing a human being.:1The taxonomy has undergone changes and is likely to change further, as genetic studies are bringing forth new information.
Etymology.
The word "scorpion" is thought to have originated in Middle English between 1175 and 1225 AD from Old French "scorpion", or from Italian "scorpione", both derived from the Latin word "scorpius", which is the romanisation of the Greek word σκορπίος – "skorpíos".
Geographical distribution.
Scorpions are found on all major land masses except Antarctica. Scorpions did not occur naturally in Great Britain, New Zealand and some of the islands in Oceania, but now have been accidentally introduced in some of these places by human trade and commerce.:249 The greatest diversity of scorpions in the Northern Hemisphere is to be found in the subtropical areas lying between latitudes 23° N and 38° N. Above these latitudes, the diversity decreases, with the northernmost natural occurrence of scorpions being the northern scorpion "Paruroctonus boreus" at Medicine Hat, Alberta, Canada 50° N.:251
Today, scorpions are found in virtually every terrestrial habitat, including high-elevation mountains, caves and intertidal zones, with the exception of boreal ecosystems, such as the tundra, high-altitude taiga and the permanently snow-clad tops of some mountains.:251–252 As regards microhabitats, scorpions may be ground-dwelling, tree-living, lithophilic (rock-loving) or psammophilic (sand-loving); some species, such as "Vaejovis janssi", are versatile and found in every type of habitat in Baja California, while others occupy specialised niches such as "Euscorpius carpathicus", which occupies the littoral zone of the shore.
Five colonies of scorpions ("Euscorpius flavicaudis") have established themselves in Sheerness on the Isle of Sheppey in the United Kingdom. This small population has been resident since the 1860s, having probably arrived with imported fruit from Africa. This scorpion species is small and completely harmless to humans. At just over 51° N, this marks the northernmost limit where scorpions live in the wild.
Classification.
There are thirteen families and about 1,750 described species and subspecies of scorpions. In addition, there are 111 described taxa of extinct scorpions.
This classification is based on that of Soleglad & Fet (2003), which replaced the older, unpublished classification of Stockwell. Additional taxonomic changes are from papers by Soleglad et al. (2005).
Systematics.
The following classification covers extant taxa to the rank of family.
Fossil record.
Scorpions have been found in many fossil records, including marine Silurian and estuarine Devonian deposits, coal deposits from the Carboniferous Period and in amber. The oldest known scorpions lived around 430 million years ago in the Silurian period. Though once believed to have lived on the bottom of shallow tropical seas, early scorpions are now believed to have been terrestrial and to have washed into marine settings together with plant matter. These first scorpions were believed to have had gills instead of the present forms' book lungs though this has subsequently been refuted. The oldest Gondwanan scorpiones (Gondwanascorpio) comprise the earliest known terrestrial animals from Gondwana. Currently, 111 fossil species of scorpion are known. Unusually for arachnids, there are more species of Palaeozoic scorpion than Mesozoic or Cenozoic ones.
The "eurypterids", marine creatures that lived during the Palaeozoic era, share several physical traits with scorpions and may be closely related to them. Various species of Eurypterida could grow to be anywhere from 10 cm to 2.5 m in length. However, they exhibit anatomical differences marking them off as a group distinct from their Carboniferous and Recent relatives. Despite this, they are commonly referred to as "sea scorpions". Their legs are thought to have been short, thick, tapering and to have ended in a single strong claw; it appears that they were well-adapted for maintaining a secure hold upon rocks or seaweed against the wash of waves, like the legs of a shore crab.
Morphology.
The body of a scorpion is divided into three parts (tagmata): the head (cephalothorax), the abdomen (mesosoma) and the tail (metasoma).:10
Cephalothorax.
The cephalothorax, also called the "prosoma", comprises the carapace, eyes, chelicerae (mouth parts), pedipalps (the pedipalps of scorpions have chelae, commonly called claws or pincers) and four pairs of walking legs. The scorpion's exoskeleton is thick and durable, providing good protection from predators. Scorpions have two eyes on the top of the cephalothorax, and usually two to five pairs of eyes along the front corners of the cephalothorax. The position of the eyes on the cephalothorax depends in part on the hardness or softness of the soil upon which they spend their lives.
The pedipalp is a segmented, chelate (clawed) appendage used for prey immobilisation, defence and sensory purposes. The segments of the pedipalp (from closest to the body outwards) are coxa, trochanter, femur (humerus), patella, tibia (including the fixed claw and the manus) and tarsus (moveable claw). A scorpion has darkened or granular raised linear ridges, called "keels" or "carinae" on the pedipalp segments and on other parts of the body, which are useful taxonomically.:12
Mesosoma.
The abdomen, also called the "opisthosoma", consists of seven segments (somites), each covered dorsally by a sclerotosed plate (tergum) and also ventrally for segments 3 to 7. The first abdominal segment bears a pair of genital opercula covering the gonopore. Segment 2 consists of the basal plate with the pectines, which are a pair of limbs transformed into sensory organs. Each of the mesosomal segments 3 to 7 have a pair of spiracles, the openings for the scorpion's respiratory organs, known as book lungs. The spiracle openings may be slits, circular, elliptical, or oval.:13–15
Metasoma.
The metasoma, the scorpion's tail, comprises five caudal segments (the first tail segment looks like a last mesosoman segment) and the sixth bearing the telson (the sting). The telson, in turn, consists of the vesicle, which holds a pair of venom glands, and the hypodermic aculeus, the venom-injecting barb.
On rare occasions, scorpions can be born with two metasomata (tails). Two-tailed scorpions are not a different species, merely a genetic abnormality.
Fluorescence.
Scorpions are also known to glow a vibrant blue-green when exposed to certain wavelengths of ultraviolet light such as that produced by a black light, due to the presence of fluorescent chemicals in the cuticle. One fluorescent component is now known to be beta-carboline. A hand-held UV lamp has long been a standard tool for nocturnal field surveys of these animals. Fluorescence occurs as a result of sclerotisation and increases in intensity with each successive instar. This fluorescence may have an active role in scorpion light detection.
Biology.
Scorpions prefer areas where the temperatures range from 20 to, but may survive temperatures ranging from well below freezing to desert heat. Scorpions of the genus "Scorpiops" living in high Asian mountains, bothriurid scorpions from Patagonia and small "Euscorpius" scorpions from Central Europe can all survive winter temperatures of about −25 C. In Repetek (Turkmenistan), there live seven species of scorpions (of which "Pectinibuthus birulai" is endemic) in temperatures varying from -31 to(-).
They are nocturnal and fossorial, finding shelter during the day in the relative cool of underground holes or undersides of rocks, and emerging at night to hunt and feed. Scorpions exhibit photophobic behavior, primarily to evade detection by predators such as birds, centipedes, lizards, mice, possums and rats.
Scorpions are opportunistic predators of small arthropods, although the larger kinds have been known to kill small lizards and mice. The large pincers are studded with highly sensitive tactile hairs, and the moment an insect touches these, they use their chelae (pincers) to catch the prey. Depending on the toxicity of their venom and size of their claws, they will then either crush the prey or inject it with neurotoxic venom. This will kill or paralyze the prey so the scorpion can eat it. Scorpions have an unusual style of eating using chelicerae, small claw-like structures that protrude from the mouth that are unique to the Chelicerata among arthropods. The chelicerae, which are very sharp, are used to pull small amounts of food off the prey item for digestion into a "pre-oral cavity" below the chelicerae and carapace. Scorpions can ingest food only in a liquid form; they have external digestion. The digestive juices from the gut are egested onto the food and the digested food sucked in liquid form. Any solid indigestible matter (fur, exoskeleton, etc.) is trapped by setae in the pre-oral cavity, which is ejected by the scorpion.:296–297
Scorpions can consume huge amounts of food at one sitting. They have a very efficient food storage organ and a very low metabolic rate combined with a relatively inactive lifestyle. This enables scorpions to survive long periods when deprived of food; some are able to survive 6 to 12 months of starvation.:297–298 Scorpions excrete very little; their waste consists mostly of insoluble nitrogenous compounds such as xanthine, guanine and uric acid.
Reproduction.
Most scorpions reproduce sexually, and most species have male and female individuals. However, some species, such as "Hottentotta hottentotta", "Hottentotta caboverdensis", "Liocheles australasiae", "Tityus columbianus", "Tityus metuendus", "Tityus serrulatus", "Tityus stigmurus", "Tityus trivittatus" and "Tityus urugayensis", reproduce through parthenogenesis, a process in which unfertilised eggs develop into living embryos. Parthenogenic reproduction starts following the scorpion's final moult to maturity and continues thereafter.
Sexual reproduction is accomplished by the transfer of a spermatophore from the male to the female; scorpions possess a complex courtship and mating ritual to effect this transfer. Mating starts with the male and female locating and identifying each other using a mixture of pheromones and vibrational communication. Once they have satisfied the other that they are of opposite sex and of the correct species, mating can commence.
The courtship starts with the male grasping the female's pedipalps with his own; the pair then perform a "dance" called the "promenade à deux". In this "dance," the male leads the female around searching for a suitable place to deposit his spermatophore. The courtship ritual can involve several other behaviours, such as juddering and a cheliceral kiss, in which the male's chelicerae – pincers – grasp the female's in a smaller more intimate version of the male's grasping the female's pedipalps and in some cases injecting a small amount of his venom into her pedipalp or on the edge of her cephalothorax, probably as a means of pacifying the female.
When the male has identified a suitable location, he deposits the spermatophore and then guides the female over it. This allows the spermatophore to enter her genital opercula, which triggers release of the sperm, thus fertilising the female. The mating process can take from 1 to 25+ hours and depends on the ability of the male to find a suitable place to deposit his spermatophore. If mating continues too long, the female may lose interest, ending the process.
Once the mating is complete, the male and female will separate. The male will generally retreat quickly, most likely to avoid being cannibalised by the female, although sexual cannibalism is infrequent with scorpions.
Birth and development.
Unlike the majority of species in the class Arachnida, which are oviparous, scorpions seem to be universally ovoviviparous. The young are born one by one after hatching and expelling the embryonic membrane, if any, and the brood is carried about on its mother's back until the young have undergone at least one moult. Before the first moult, scorplings cannot survive naturally without the mother, since they depend on her for protection and to regulate their moisture levels. Especially in species that display more advanced sociability (e.g. "Pandinus" spp.), the young/mother association can continue for an extended period of time. The size of the litter depends on the species and environmental factors, and can range from two to over a hundred scorplings. The average litter however, consists of around 8 scorplings.
The young generally resemble their parents. Growth is accomplished by periodic shedding of the exoskeleton (ecdysis). A scorpion's developmental progress is measured in instars (how many moults it has undergone). Scorpions typically require between five and seven moults to reach maturity. Moulting commences with a split in the old exoskeleton just below the edge of the carapace (at the front of the prosoma). The scorpion then emerges from this split; the pedipalps and legs are first removed from the old exoskeleton, followed eventually by the metasoma. When it emerges, the scorpion's new exoskeleton is soft, making the scorpion highly vulnerable to attack. The scorpion must constantly stretch while the new exoskeleton hardens to ensure that it can move when the hardening is complete. The process of hardening is called sclerotisation. The new exoskeleton does not fluoresce; as sclerotisation occurs, the fluorescence gradually returns.
Relationship with humans.
Sting and venom.
All known scorpion species possess venom and use it primarily to kill or paralyze their prey so that it can be eaten. In general, it is fast-acting, allowing for effective prey capture. However, as a general rule they will kill their prey with brute force if they can, as opposed to using venom. It is also used as a defense against predators. The venom is a mixture of compounds (neurotoxins, enzyme inhibitors, etc.) each not only causing a different effect but possibly also targeting a specific animal. Each compound is made and stored in a pair of glandular sacs and is released in a quantity regulated by the scorpion itself. Of the 1,000+ known species of scorpion, only 25 have venom that is deadly to humans; most of those belong to the family Buthidae (including "Leiurus quinquestriatus", "Hottentotta", "Centruroides" and "Androctonus").
First aid.
First aid for scorpion stings is generally symptomatic. It includes strong analgesia, either systemic (opiates or paracetamol) or locally applied (such as a cold compress). Hypertensive crises are treated with anxiolytics and vasodilators.
Medical use.
Short-chain scorpion toxins constitute the largest group of potassium (K+) channel blocking peptides; an important physiological role of the KCNA3 channel, also known as KV1.3, is to help maintain large electrical gradients for the sustained transport of ions such as Ca2+ that controls T lymphocyte (T cell) proliferation. Thus KV1.3 blockers could be potential immunosuppressants for the treatment of autoimmune disorders (such as rheumatoid arthritis, inflammatory bowel disease and multiple sclerosis).
The venom of "Uroplectes lineatus" is clinically important in dermatology.
Toxins being investigated include:
Consumption.
Fried scorpion is a traditional dish from Shandong, China.
As a part of Chinese medicine, scorpion wine and snake wine are used as analgesic and antidote.
Harvesting.
The beneficial uses in medicine, and culinary demand, lead some larger scorpions to reach prices of $50,000 per scorpion. The venom alone can net up to $39 million (USD) per gallon.

</doc>
<doc id="28923" url="http://en.wikipedia.org/wiki?curid=28923" title="Shriners">
Shriners

Shriners International, previously known as the Ancient Arabic Order of the Nobles of the Mystic Shrine (A.A.O.N.M.S.) and also commonly known as Shriners, was established in 1870, and is an appendant body to Freemasonry.
The name change from the Ancient Arabic Order of the Nobles of the Mystic Shrine, as well as Shriners North America, to Shriners International was facilitated in 2010 across North America, Central America, South America, Europe and Southeast Asia. The organization is best known for the Shriners Hospitals for Children it administers, and the red fezzes that members wear. The organization is headquartered in Tampa, Florida. Shriners International describes itself as a fraternity based on fun, fellowship and the Masonic principles of brotherly love, relief and truth. There are approximately 340,000 members from 195 temples (chapters) in the U.S., Canada, Brazil, Mexico, the Republic of Panama, the Philippines, Puerto Rico, Europe and Australia.
History.
In 1870, there were several thousand Masons in Manhattan, many of whom lunched at the Knickerbocker Cottage at a special table on the second floor. There, the idea of a new fraternity for Masons stressing fun and fellowship was discussed. Walter M. Fleming, M.D., and William J. Florence took the idea seriously enough to act upon it.
Florence, a world-renowned actor, while on tour in Marseille, was invited to a party given by an Arabian diplomat. The entertainment was something in the nature of an elaborately staged musical comedy. At its conclusion, the guests became members of a secret society. Florence took copious notes and drawings at his initial viewing and on two other occasions, once in Algiers and once in Cairo. When he returned to New York in 1870, he showed his material to Fleming.
Fleming took the ideas supplied by Florence and converted them into what would become the "Ancient Arabic Order of the Nobles of the Mystic Shrine (A.A.O.N.M.S.)". Fleming created the ritual, emblem and costumes. Florence and Fleming were initiated August 13, 1870, and initiated 11 other men on June 16, 1871.
The group adopted a Middle Eastern theme and soon established Temples (though the term Temple has now generally been replaced by Shrine Auditorium or Shrine Center). The first Temple established was Mecca Temple (now known as Mecca Shriners), established at the New York City Masonic Hall on September 26, 1872. Fleming was the first Potentate.
In 1875, there were only 43 Shriners in the organization. In an effort to spur membership, at the June 6, 1876 meeting of Mecca Temple, the Imperial Grand Council of the Ancient Order of the Nobles of the Mystic Shrine for North America was created. Fleming was elected the first Imperial Potentate. After some other reworking, by 1878 there were 425 members in 13 temples in eight states, and by 1888, there were 7,210 members in 48 temples in the United States and Canada. By the Imperial Session held in Washington, D.C. in 1900, there were 55,000 members and 82 Temples. By 1938 there were about 340,000 members in the United States. That year "Life" published photographs of its rites for the first time. It described the Shriners as "among secret lodges the No. 1 in prestige, wealth and show", and stated that "[i]n the typical city, especially in the Middle West, the Shriners will include most of the prominent citizens."
Shriners often participate in local parades, sometimes as rather elaborate units: miniature vehicles in themes (all sports cars; all miniature 18-wheeler trucks; all fire engines, and so on), an "Oriental Band" dressed in cartoonish versions of Middle Eastern dress; pipe bands, drummers, motorcycle units, Drum and Bugle Corps, and even traditional brass bands.
Membership.
Despite its theme, the Shrine is not connected to Arab culture or Islam. It is a men's fraternity rather than a religion or religious group. Its only religious requirement is indirect: all Shriners must be Masons (with the exception being in the State of Arkansas), and petitioners to Freemasonry must profess a belief in a Supreme Being. To further minimize confusion with religion, the use of the words "temple" and "mosque" to describe Shriners' buildings has been replaced by "Shrine Center", although individual local chapters are still called temples.
Until 2000, before being eligible for membership in the Shrine, a person had to complete either the Scottish Rite or York Rite degrees of Masonry, but now any Master Mason can join.
Shriners count among their ranks presidents, senators, local business leaders, professional golfers, country music stars, astronauts and racecar drivers.
Women's auxiliaries.
While there are plenty of activities for Shriners and their wives, there are two organizations tied to the Shrine that are for women only: The Ladies' Oriental Shrine and Daughters of the Nile. They both support the Shriners Hospitals and promote sociability, and membership in either organization is open to any woman 18 years of age and older who is related to a Shriner or Master Mason by birth, marriage, or adoption. The Ladies Oriental Shrine of North America was founded in Wheeling, West Virginia, in 1903, and Daughters of the Nile was founded in 1913 in Seattle, Washington. That latter organization has locals called "Temples". There were ten of these in 1922. Among the famous members of the Daughters of the Nile was First Lady Florence Harding, wife of Warren G. Harding.
Architecture.
Some of the earliest Shrine Centers often chose a Moorish Revival style for their Temples. Architecturally notable Shriners Temples include the Shrine Auditorium in Los Angeles, New York City Center, now used as a concert hall, Newark Symphony Hall, the Landmark Theater (formerly The Mosque) in Richmond, Virginia, the Tripoli Shrine Temple in Milwaukee, Wisconsin, the Helena Civic Center (Montana) (formerly the Algeria Shrine Temple), and the Fox Theatre (Atlanta, Georgia) which was jointly built between the Atlanta Shriners and William Fox.
Shriners Hospitals for Children.
The Shrine's charitable arm is the Shriners Hospitals for Children, a network of twenty-two hospitals in the United States, Mexico and Canada. In June 1920, the Imperial Council Session voted to establish a "Shriners Hospital for Crippled Children." The goal of this hospital was to treat orthopedic injuries, diseases, and birth defects in children. After much research and debate, the committee chosen to determine the site of the hospital decided there should be not just one hospital but a network of hospitals spread across North America. The first hospital was opened in 1922 in Shreveport, LA and by the end of the decade thirteen more hospitals were in operation. They now deal with orthopedic care, burn treatment, cleft lip and palate care and spinal cord injury rehabilitation. The rules for all of the Shriners Hospitals are simple and to the point: Any child can be admitted to the hospital if, in the opinion of the surgeons, the child can be treated and is under the age of 18. Until June 2012, all treatment offered at Shriner's Hospitals for Children was offered without any financial obligation to patients and their families. At that time, because the size of their endowment had decreased due to losses in the stock market, the Shriners started billing patients' insurance companies, but still offered free care to those that didn't have insurance. There is no requirement for religion, race, or relationship to a Shriner. Patients must be under the age of eighteen and treatable.
In 2008, Shriners Hospitals had a total budget of $826 million and in 2007 they approved 39,454 new patient applications, attended to the needs of 125,125 patients.
Parade unit.
Most Shrine Temples support several parade units. These units are responsible for promoting a positive Shriner image to the public by participating in local parades. The parade units often include miniature cars powered by lawn mower engines.
An example of a Shrine parade unit is the Heart Shrine Clubs’ Original Fire Patrol of Effingham, Illinois. This unit operates miniature fire engines in honor of a hospital fire that took place in the 1940s in Effingham. They participate in most parades in a 100 mile radius of Effingham. Shriners in Dallas, Texas participate annually in the Twilight Parade at the Texas State Fair. Shriners in St. Louis have several parade motor units, including miniature cars styled after 1932 Ford coupes and 1970s-era Jeep CJ models, and a unit of miniature Indianapolis-styled race cars. Some of these are outfitted with high-performance, alcohol-fueled engines. The drivers' skills are demonstrated during parades with high-speed spinouts.
Other events.
The Shriners are committed to community service and have been instrumental in countless public projects throughout their domain.
Shriners host the annual "East-West Shrine Game", a college football all-star game.
The Shriners originally hosted a golf tournament in concert with singer/actor Justin Timberlake, titled the "Justin Timberlake Shriners Hospitals for Children Open", a PGA Golf Tour golf tournament held in Las Vegas, NV. The relationship between Timberlake and the Shriners ended in 2012, due to the lack of previously agreed participation on Timberlake's part. In July 2012, The PGA TOUR and Shriners Hospitals for Children announced a five-year title sponsorship extension, carrying the commitment to the Shriners Hospitals for Children Open through 2017, now titled "The Shriners Hospitals for Children Open", and is still held in Las Vegas, NV.
Once a year, the fraternity meets for the Imperial Council Session in a major North American city. It is not uncommon for these conventions to have 20,000 participants or more, which generates significant revenue for the local economy.
Many Shrine Centers also hold a yearly "Shrine Circus" as a fundraiser.

</doc>
<doc id="28925" url="http://en.wikipedia.org/wiki?curid=28925" title="Science fiction fandom">
Science fiction fandom

Science fiction fandom or SF fandom is a community or fandom of people actively interested in science fiction and fantasy and in contact with one another based upon that interest. SF fandom has a life of its own, but not much in the way of formal organization (although clubs such as the Futurians [1937–1945], the Los Angeles Science Fantasy Society [1934–present], and the National Fantasy Fan Federation [1941–present] are recognized features of fandom).
Most often called simply "fandom" within the community, it can be viewed as a distinct subculture, with its own rituals and jargon; marriages and other relationships among fans are common, as are multi-generation fannish families.
Origins and history.
Science fiction fandom started through the letter column of Hugo Gernsback's fiction magazines. Not only did fans write comments about the stories—they sent their addresses, and Gernsback published them. Soon, fans were writing letters directly to each other, and meeting in person when they lived close together, or when one of them could manage a trip. In New York City, David Lasser, Gernsback's managing editor, nurtured the birth of a small local club called the Scienceers, which held its first meeting in a Harlem apartment on December 11, 1929. Almost all the members were adolescent boys. Around this time a few other small local groups began to spring up in metropolitan areas around the United States, many of them connecting with fellow enthusiasts via the Science Correspondence Club. In May 1930 the first science fiction fan magazine, "The Comet", was produced by the Chicago branch of the Science Correspondence Club under the editorship of Raymond A. Palmer (later a noted, and notorious, sf magazine editor) and Walter Dennis. In January 1932, the New York City circle, which by then included future comic book editors Julius Schwartz and Mort Weisinger, brought out the first issue of their own publication, "The Time Traveller", with Forrest J Ackerman of the embryonic Los Angeles group as a contributing editor.
In 1934, Gernsback established a correspondence club for fans called the Science Fiction League, the first fannish organization. Local groups across the nation could join by filling out an application. A number of clubs came into being around this time. LASFS (the Los Angeles Science Fantasy Society) was founded at this time as a local branch of the SFL, while several competing local branches sprang up in New York City and immediately began feuding among themselves. 
In 1935, PSFS (the Philadelphia Science Fiction Society, 1935–present) was formed. The next year, half a dozen fans from NYC came to Philadelphia to meet with the PSFS members, as the first Philadelphia Science Fiction Conference, which some claim as the world's first science fiction convention.
Soon after the fans started to communicate directly with each other came the creation of science fiction fanzines. These amateur publications might or might not discuss science fiction and were generally traded rather than sold. They ranged from the utilitarian or inept to professional-quality printing and editing. In recent years, Usenet newsgroups such as rec.arts.sf.fandom, websites and blogs have somewhat supplanted printed fanzines as an outlet for expression in fandom, though many popular fanzines continue to be published. Science-fiction fans have been among the first users of computers, email, personal computers and the Internet.
Many professional science fiction authors started their interest in science fiction as fans, and some still publish their own fanzines or contribute to those published by others.
A widely regarded (though by no means error-free) history of fandom in the 1930s can be found in Sam Moskowitz's "The Immortal Storm: A History of Science Fiction Fandom" Hyperion Press 1988 ISBN 0-88355-131-4 (original edition The Atlanta Science Fiction Organization Press, Atlanta, Georgia 1954). Moskowitz was himself involved in some of the incidents chronicled and has his own point of view, which has often been criticized.
Fandom in Sweden.
Fandom in Sweden ("Sverifandom") emerged in the 1950s. The first Swedish science fiction fanzine was started in the early 1950s. The oldest still existing club, Club Cosmos in Gothenburg, was formed in 1954, and the first Swedish science fiction convention, LunCon, was held in Lund in 1956.
Today, there are a number of science fiction clubs in the country, including Skandinavisk Förening för Science Fiction (whose club fanzine, "Science Fiction Forum", was once edited by Stieg Larsson, a board member and one-time chairman thereof), Linköpings Science Fiction-Förening and Sigma Terra Corps. Between one and four science fiction conventions are held each year in Sweden, among them Swecon, the annual national Swedish con. An annual prize is awarded to someone that has contributed to the national fandom by the Alvar Appeltofft Memorial Fund.
Conventions.
Since the late 1930s, SF fans have organized conventions, non-profit gatherings where the fans (some of whom are also professionals in the field) meet to discuss SF and generally enjoy themselves. (A few fannish couples have held their weddings at conventions.) The 1st World Science Fiction Convention or Worldcon was held in conjunction with the 1939 New York World's Fair, and has been held annually since the end of World War II. Worldcon has been the premier convention in fandom for over half a century; it is at this convention that the Hugo Awards are bestowed, and attendance can approach 8,000 or more.
SF writer Cory Doctorow calls science fiction "perhaps the most social of all literary genres", and states, "Science fiction is driven by organized fandom, volunteers who put on hundreds of literary conventions in every corner of the globe, every weekend of the year."
SF conventions can vary from minimalist "relaxacons" with a hundred or so attendees to heavily programmed events with four to six or more simultaneous tracks of programming, such as WisCon and Worldcons.
Commercial shows dealing with SF-related fields are sometimes billed as 'science fiction conventions,' but are operated as for-profit ventures, with an orientation towards passive spectators, rather than actively involved fans, and a tendency to neglect or ignore written SF in favor of television, film, comics, video games, etc. One of the largest of these is the annual Dragon*Con in Atlanta, Georgia with an attendance of more than 20,000 since 2000.
Science fiction societies.
See also: .
In the United States, many science fiction societies were launched as chapters of the Science Fiction League and, when it faded into history, several of the original League chapters remained viable and were subsequently incorporated as independent organizations. Most notable among the former League chapters which were spun off was the Philadelphia Science Fiction Society, which served as a model for subsequent SF societies formed independent of the League history.
Science fiction societies, more commonly referred to as "clubs" except on the most formal of occasions, form a year-round base of activities for science fiction fans. They are often associated with an SF convention or group of conventions, but maintain a separate existence as cultural institutions within specific geographic regions. Several have purchased property and maintain ongoing collections of SF literature available for research, as in the case of the Los Angeles Science Fantasy Society, the New England Science Fiction Association, and the Baltimore Science Fiction Society. Other SF Societies maintain a more informal existence, meeting at general public facilities or the homes of individual members, such as the Bay Area Science Fiction Association.
Offshoots and subcommunities.
As a community devoted to discussion and exploration of new ideas, fandom has become an incubator for many groups that started out as special interests within fandom, some of which have partially separated into independent intentional communities not directly associated with science fiction. Among these groups are comic book fandom, media fandom, the Society for Creative Anachronism, gaming, and furry fandom, sometimes referred to collectively as "fringe fandoms" with the implication that the original fandom centered on science fiction texts (magazines and later books and fanzines) is the "true" or "core" fandom. Fandom also welcomes and shares interest with other groups involved in new ideas and lifestyles, including LGBT communities, libertarians, neo-pagans, and space activist groups like the L5 Society, among many others. Some groups exist almost entirely within fandom but are distinct and cohesive subcultures in their own rights, such as filkers, costumers, and convention runners (sometimes called "SMOFs").
Fandom encompasses subsets of fans that are principally interested in a single writer or subgenre, such as Tolkien fandom, and ("Trekkies"). Even short-lived television series may have dedicated followings, such as the fans of Joss Whedon's "Firefly" television series and movie "Serenity", known as Browncoats.
Participation in science fiction fandom often overlaps with other similar interests, such as fantasy role-playing games, comic books and anime, and in the broadest sense fans of these activities are felt to be part of the greater community of SF fandom.
There are active SF fandoms around the world. Fandom in non-Anglophone countries is based partially on local literature and media, with cons and other elements resembling those of English-speaking fandom, but with distinguishing local features. For example, Finland's national gathering Finncon is funded by the government, while all conventions and fan activities in Japan are heavily influenced by anime and manga.
Fanspeak.
Science fiction and fantasy fandom has its own slang or jargon, sometimes called "fanspeak" (the term has been in use since at least 1962).
Fanspeak is made up of acronyms, blended words, obscure in-jokes, and standard terms used in specific ways. Some terms used in fanspeak have spread to members of the Society for Creative Anachronism ("Scadians"), Renaissance Fair participants ("Rennies"), hacktivists, and internet gaming and chat fans, due to the social and contextual intersection between the communities. Examples of fanspeak used in these broader fannish communities include gafiate, a term meaning to drop out of SF related community activities, with the implication to Get A Life. The word is derived via the acronym for "get away from it all". A related term is fafiate, for "forced away from it all". The implication is that one would really rather still be involved in fandom, but circumstances make it impossible.
Two other acronyms commonly used in the community are FIAWOL (Fandom Is A Way Of Life) and its opposite FIJAGH (Fandom Is Just A Goddamned Hobby) to describe two ways of looking at the place of fandom in one's life.
Science-fiction fans often refer to themselves using the irregular plural "fen": man/men, fan/fen.
In fiction.
As science fiction fans became professional writers, they started slipping the names of their friends into stories. Wilson "Bob" Tucker slipped so many of his fellow fans and authors into his works that doing so is called tuckerization.
The subgenre of "recursive science fiction" has a fan-maintained bibliography at the New England Science Fiction Association's website; some of it is about science fiction fandom, some not.
In Robert Bloch's 1956 short story, "A Way Of Life", science fiction fandom is the only institution to survive a nuclear holocaust and eventually becomes the basis for the reconstitution of civilization. The science fiction novel "Gather in the Hall of the Planets", by K.M. O'Donnell (aka Barry Malzberg), 1971, takes place at a New York City science fiction convention and features broad parodies of many SF fans and authors. A pair of SF novels by Gene DeWeese and Robert "Buck" Coulson, "Now You See It/Him/Them" and "Charles Fort Never Mentioned Wombats" are set at Worldcons; the latter includes an in-character "introduction" by Wilson Tucker (himself a character in the novel) which is a sly self-parody verging on a self-tuckerization. 
The 1991 SF novel "Fallen Angels" by Larry Niven, Jerry Pournelle and Michael Flynn constitutes a tribute to SF fandom. The story includes a semi-illegal fictional Minneapolis Worldcon in a post-disaster world where science, and thus fandom, is disparaged. Many of the characters are barely tuckerized fans, mostly from the Greater Los Angeles area. 
Mystery writer Sharyn McCrumb's "Bimbos of the Death Sun" and "Zombies of the Gene Pool" are murder mysteries set at a science fiction convention and within the broader culture of fandom respectively. While containing mostly nasty caricatures of fans and fandom, some fans take them with good humor; others consider them vicious and cruel.
In 1994 and 1996, two anthologies of alternate history science fiction involving World Science Fiction Conventions, titled "Alternate Worldcons" and "Again, Alternate Worldcons", edited by Mike Resnick were published.
Fans are slans.
A.E. van Vogt's 1940 novel "Slan" was about a mutant variety of humans who are superior to regular humanity and are therefore hunted down and killed by the normal human population. While the story has nothing to do with fandom, many science fiction fans felt very close to the protagonists, feeling their experience as bright people in a mundane world mirrored that of the mutants; hence, the rallying cry, "Fans Are Slans!"; and the tradition that a building inhabited primarily by fans can be called a slan shack.

</doc>
<doc id="28926" url="http://en.wikipedia.org/wiki?curid=28926" title="Spin">
Spin

Spin or spinning may refer to:

</doc>
<doc id="28927" url="http://en.wikipedia.org/wiki?curid=28927" title="Stellar classification">
Stellar classification

Hertzsprung–Russell diagram
Spectral type
Brown dwarfs
White dwarfs
Red dwarfs
Subdwarfs
Main sequence<br>("dwarfs")
Subgiants
Giants
Bright giants
Supergiants
Hypergiants
absolute
magni-
tude
In astronomy, stellar classification is the classification of stars based on their spectral characteristics. Light from the star is analyzed by splitting it with a prism or diffraction grating into a spectrum exhibiting the rainbow of colors interspersed with absorption lines. Each line indicates an ion of a certain chemical element, with the line strength indicating the abundance of that ion. The relative abundance of the different ions varies with the temperature of the photosphere. The "spectral class" of a star is a short code summarizing the ionization state, giving an objective measure of the photosphere's temperature and density.
Most stars are currently classified under the Morgan–Keenan (MK) system using the letters "O", "B", "A", "F", "G", "K", and "M", a sequence from the hottest ("O" type) to the coolest ("M" type). Each letter class is then subdivided using a numeric digit with "0" being hottest and "9" being coolest (e.g. A8, A9, F0, F1 form a sequence from hotter to cooler). The sequence has been expanded with classes for other stars and star-like objects that do not fit in the classical system, such class "D" for white dwarfs and class "C" for carbon stars.
In the MK system a luminosity class is added to the spectral class using Roman numerals. This is based on the width of certain absorption lines in the star's spectrum which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class "0" or "Ia+" stars for "hypergiants", class "I" stars for "supergiants", class "II" for bright "giants", class "III" for regular "giants", class "IV" for "sub-giants", class "V" for "main-sequence stars", class "sd" for "sub-dwarfs", and class "D" for "white dwarfs". The full spectral class for the Sun is then G2V, indicating a main-sequence star with a temperature around 5,800K.
Modern classification.
The modern classification system is known as the Morgan–Keenan (MK) classification. Each star is assigned a "spectral class" from the older Harvard spectral classification and a "luminosity class" using Roman numerals as explained below, forming the star's spectral type.
Harvard spectral classification.
The Harvard classification system is a one-dimensional classification scheme using single letters of the alphabet, optionally with numeric subdivisions, to group stars according to their spectral characteristics. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000 K, whereas more-evolved stars can have temperatures above 100,000 K. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.
The spectral classes O through M, as well as other more specialized classes discussed later, are subdivided by Arabic numerals (0–9), which can be further divided into half-subtypes. For example, A0 denotes the hottest stars in the A class and A9 denotes the coolest ones. The Sun is classified as G2.
O, B, and A stars are sometimes called "early type", whereas K and M stars are said to be "late type″. This stems from an early 20th-century model of stellar evolution in which stars were powered by gravitational contraction via the Kelvin–Helmholtz mechanism whereby stars start their lives as very hot "early-type" stars, and then gradually cool down, evolving into "late-type″ stars. This mechanism provided ages of the Sun that were much smaller than what is observed, and was rendered obsolete by the discovery that stars are powered by nuclear fusion.
Yerkes spectral classification.
<span id="Luminosity class" /><span id="Luminosity classes" />
The Yerkes spectral classification, also called the MKK system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory. This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity which is related to luminosity (whilst the Harvard classification is based on surface temperature only). Later, in 1953, after some revisions of list of standard stars and classification criteria, the scheme was named the Morgan–Keenan classification, or MK (by William Wilson Morgan and Philip C. Keenan's initials), and this system remains the system in modern use today.
Denser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore differences in the spectrum can be interpreted as "luminosity effects" and a luminosity class can be assigned purely from examination of the spectrum.
A number of different luminosity classes are distinguished
Marginal cases are allowed; for example, a star may be either a supergiant or a bright giant, or may be in between the subgiant and main-sequence classifications. In these cases, two special symbols are used: a slash (/) means that a star is either one class or the other, and a dash (-) means that the star is in between the two classes. For example, a star classified as A3-4III/IV would be in between spectral types A3 and A4, while being either a giant star or a subgiant.
Spectral peculiarities.
Additional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum.
For example, 59 Cygni is listed as spectral type B1.5Vnne, indicating a spectrum with the general classification B1.5V, as well as very broad absorption lines and certain emission lines.
Conventional and apparent colors.
The "conventional color" descriptions are traditional in astronomy, and represent colors relative to the mean color of an "A-class star" which is considered to be white. The "apparent color" descriptions are what the observer would see if trying to describe the stars under a dark sky without aid to the eye, or with binoculars. The table colors used are D65 standard colors calculated for the midpoint of each spectral class (e.g. G5 or A5) and for main-sequence stars. These are accurate representations of the actual color of the disk of a star. Most stars in the sky, except the brightest ones, appear white or bluish white to the unaided eye because they are too dim for color vision to work. Red supergiants are cooler and redder than dwarfs of the same spectral type, and stars with particular spectral features such as carbon stars may be far redder than any black body.
The Sun itself appears white and approximates a black body of 5780 K (see color temperature). It is sometimes called a yellow star (spectroscopically relative to Vega), although it may appear yellow or red when viewed through the atmosphere, or appear white if viewed when too bright for the eye to see any color.
History.
The reason for the odd arrangement of letters in the Harvard classification is historical, having evolved from the earlier Secchi classes and been progressively modified as understanding improved.
Secchi classes.
During the 1860s and 1870s, pioneering stellar spectroscopist Father Angelo Secchi created the Secchi classes in order to classify observed spectra. By 1866, he had developed three classes of stellar spectra:
In 1868, he discovered carbon stars, which he put into a distinct group:
In 1877, he added a fifth class:
In the late 1890s, this classification began to be superseded by the Harvard classification, which is discussed in the remainder of this article.
Draper system.
In the 1880s, the astronomer Edward C. Pickering began to make a survey of stellar spectra at the Harvard College Observatory, using the objective-prism method. A first result of this work was the "Draper Catalogue of Stellar Spectra", published in 1890. Williamina Fleming classified most of the spectra in this catalogue. It used a scheme in which the previously used Secchi classes (I to IV) were divided into more specific classes, given letters from A to N. Also, the letters O, P and Q were used, O for stars whose spectra consisted mainly of bright lines, P for planetary nebulae, and Q for stars not fitting into any other class.
Harvard system.
In 1897, another worker at Harvard, Antonia Maury, placed the Orion subtype of Secchi class I ahead of the remainder of Secchi class I, thus placing the modern type B ahead of the modern type A. She was the first to do so, although she did not use lettered spectral types, but rather a series of twenty-two types numbered from I to XXII.
In 1901, Annie Jump Cannon returned to the lettered types, but dropped all letters except O, B, A, F, G, K, and M, used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so forth. Finally, by 1912, Cannon had changed the types B, A, B5A, F2G, etc. to B0, A0, B5, F2, etc. This is essentially the modern form of the Harvard classification system. A common mnemonic for remembering the spectral type letters is "Oh, Be A Fine Guy/Girl, Kiss Me".
Modern interpretation.
The fact that the Harvard classification of a star indicated its surface or photospheric temperature (or more precisely, its effective temperature) was not fully understood until after its development, though by the time the first Hertzsprung–Russell diagram was formulated (by 1914), this was generally suspected to be true. In the 1920s, the Indian physicist Meghnad Saha derived a theory of ionization by extending well-known ideas in physical chemistry pertaining to the dissociation of molecules to the ionization of atoms. First he applied it to the solar chromosphere, then to stellar spectra. The Harvard astronomer Cecilia Helena Payne (later to become Cecilia Payne-Gaposchkin) then demonstrated that the OBAFGKM spectral sequence is actually a sequence in temperature. Because the classification sequence predates our understanding that it is a temperature sequence, the placement of a spectrum into a given subtype, such as B3 or A7, depends upon (largely subjective) estimates of the strengths of absorption features in stellar spectra. As a result, these subtypes are not evenly divided into any sort of mathematically representable intervals.
Spectral types.
Class O.
Class O stars are very hot and extremely luminous, with most of their radiated output in the ultraviolet range. These are the rarest of all main-sequence stars. About 1 in 3,000,000 (0.00003%) of the main-sequence stars in the solar neighborhood are class O stars. Some of the most massive stars lie within this spectral class. Class O stars frequently have complicated surroundings which make measurement of their spectra difficult.
O stars have dominant lines of absorption and sometimes emission for He II lines, prominent ionized (Si IV, O III, N III, and C III) and neutral helium lines, strengthening from O5 to O9, and prominent hydrogen Balmer lines, although not as strong as in later types. Because they are so massive, class O stars have very hot cores and burn through their hydrogen fuel very quickly, so they are the first stars to leave the main sequence.
When the MKK classification scheme was first described in 1943, the only subtypes of class O used were O5 to O9.5. The MKK scheme was extended to O9.7 in 1971 and O4 in 1978, and new classification schemes have subsequently been introduced which add types O2, O3 and O3.5.
Class B.
Class B stars are very luminous and blue. Their spectra have neutral helium, which are most prominent at the B2 subclass, and moderate hydrogen lines. Ionized metal lines include Mg II and Si II. As O and B stars are so energetic, they only live for a relatively short time. Thus, due to the low probability of kinematic interaction during their lifetime, they do not, and are unable to, stray far from the area in which they were formed.
These stars tend to be found in their originating OB associations, which are associated with giant molecular clouds. The Orion OB1 association occupies a large portion of a spiral arm of the Milky Way and contains many of the brighter stars of the constellation Orion. About 1 in 800 (0.125%) of the main-sequence stars in the solar neighborhood are class B stars.
Class A.
Class A stars are among the more common naked eye stars, and are white or bluish-white. They have strong hydrogen lines, at a maximum by A0, and also lines of ionized metals (Fe II, Mg II, Si II) at a maximum at A5. The presence of Ca II lines is notably strengthening by this point. About 1 in 160 (0.625%) of the main-sequence stars in the solar neighborhood are class A stars.
Class F.
Class F stars have strengthening "H" and "K" lines of Ca II. Neutral metals (Fe I, Cr I) beginning to gain on ionized metal lines by late F. Their spectra are characterized by the weaker hydrogen lines and ionized metals. Their color is white. About 1 in 33 (3.03%) of the main-sequence stars in the solar neighborhood are class F stars.
Class G.
Class G stars are probably the best known, if only for the reason that the Sun is of this class. They make up about 7.5%, nearly one in thirteen, of the main-sequence stars in the solar neighborhood.
Most notable are the "H" and "K" lines of Ca II, which are most prominent at G2. They have even weaker hydrogen lines than F, but along with the ionized metals, they have neutral metals. There is a prominent spike in the G band of CH molecules. G is host to the "Yellow Evolutionary Void". Supergiant stars often swing between O or B (blue) and K or M (red). While they do this, they do not stay for long in the yellow supergiant G classification as this is an extremely unstable place for a supergiant to be.
Class K.
Class K stars are orangish stars that are slightly cooler than the Sun. They make up about 12%, nearly one in eight, of the main-sequence stars in the solar neighborhood. There are also giant K-type stars, which range from hypergiants like RW Cephei, to giants and supergiants, such as Arcturus, whereas orange dwarfs, like Alpha Centauri B, are main-sequence stars.
They have extremely weak hydrogen lines, if they are present at all, and mostly neutral metals (Mn I, Fe I, Si I). By late K, molecular bands of titanium oxide become present. There is a suggestion that K Spectrum stars may potentially increase the chances of life developing on orbiting planets that are within the habitable zone.
Class M.
Class M stars are by far the most common. About 76% of the main-sequence stars in the Solar neighborhood are class M stars. However, because main-sequence stars of spectral class M have such low luminosities, none are bright enough to be visible to see with the unaided eye. The brightest known M-class main-sequence star is M0V Lacaille 8760 at magnitude 6.6 (the fractionally brighter Groombridge 1618 was once considered to be class M0 but is now considered to be as K5) and it is extremely unlikely that any brighter examples will be found.
Although most class M stars are red dwarfs, the class also hosts most giants and some supergiants such as VY Canis Majoris, Antares and Betelgeuse. Furthermore, the late-M group holds hotter brown dwarfs that are above the L spectrum. This is usually in the range of M6.5 to M9.5. The spectrum of a class M star shows lines belonging to oxide molecules, TiO in particular, in the visible and all neutral metals, but absorption lines of hydrogen are usually absent. TiO bands can be strong in class M stars, usually dominating their visible spectrum by about M5. Vanadium monoxide bands become present by late M.
Extended spectral types.
A number of new spectral types have been taken into use from newly discovered types of stars.
Hot blue emission star classes.
Spectra of some very hot and bluish stars exhibit marked emission lines from carbon or nitrogen, or sometimes oxygen.
Class W: Wolf–Rayet.
Class W or WR represents the Wolf–Rayet stars, notable for spectra lacking hydrogen lines. Instead their spectra are dominated by broad emission lines of highly ionised helium, nitrogen, carbon and sometimes oxygen. They are thought to mostly be dying supergiants with their hydrogen layers blown away by stellar winds, thereby directly exposing their hot helium shells. Class W is further divided into subclasses according to the relative strength of nitrogen and carbon emission lines in their spectra (and outer layers).
WR spectra range is listed below:
Although the central stars of most planetary nebulae (CSPNe) show O-type spectra, around 10% are hydrogen-deficient and show WR spectra. These are low mass stars and to distinguish them from the massive Wolf Rayet stars, their spectra are enclosed in square brackets: e.g. [WC]. Most of these show [WC] spectra, some [WO], and very rarely [WN].
Wolf–Rayet examples:
The "Slash" stars.
The slash stars are stars with O-type spectra and WN sequence in their spectra. The name slash comes from their spectra having a slash.
There is a secondary group found with this spectra, a cooler, "intermediate" group with designation of Ofpe/WN9. These stars have also been referred to as WN10 or WN11, but that has become less popular with the realisation of the evolutionary difference to other Wolf–Rayet stars. Recent discoveries of even rarer stars have extended the range of slash stars as far as O2-3.5If*/WN5-7, which are even hotter than the original slash stars.
Cool red and brown dwarf classes.
The new spectral types L, T and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visual spectrum.
Brown dwarfs, whose energy comes from gravitational attraction alone, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes; faster the less massive they are—the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to a degeneracy between mass and age for a given effective temperature and luminosity, no unique values can be assigned to a given spectral type.
Class L.
Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. L does not mean lithium dwarf; a large fraction of these stars do not have lithium in their spectra. Some of these objects have masses large enough to support hydrogen fusion, but some are of substellar mass and do not, so collectively these objects should be referred to as "L dwarfs", not "L stars". They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra. Due to low gravities in giant stars, TiO- and VO-bearing condensates never form. Thus, larger L-type stars can never form in an isolated environment. It may be possible for these L-type supergiants to form through stellar collisions, however, an example of which is V838 Monocerotis.
<span id="Class T"/>
Class T: methane dwarfs.
Class T dwarfs are cool brown dwarfs with surface temperatures between approximately 700 and 1,300 K. Their emission peaks in the infrared. Methane is prominent in their spectra.
Class T and L could be more common than all the other classes combined if recent research is accurate. Study of the number of proplyds (protoplanetary discs, clumps of gas in nebulae from which stars and planetary systems are formed) indicates that the number of stars in the galaxy should be several orders of magnitude higher than what we know about. It is theorized that these proplyds are in a race with each other. The first one to form will become a proto-star, which are very violent objects and will disrupt other proplyds in the vicinity, stripping them of their gas. The victim proplyds will then probably go on to become main-sequence stars or brown dwarfs of the L and T classes, which are quite invisible to us. Because brown dwarfs can live so long, these smaller bodies accumulate over time.
Class Y.
Brown dwarfs of spectral class Y are cooler than those of spectral class T and have qualitatively different spectra from them. A total of 17 objects have been placed in class Y as of . Although such dwarfs have been modelled and detected within forty light years by the Wide-field Infrared Survey Explorer (WISE) there is no well-defined spectral sequence yet with prototypes. Nevertheless, several objects have been assigned spectral classes Y0, Y1, and Y2. The spectra of these objects display absorption around 1.55 micrometers. Delorme et al. has suggested that this feature is due to absorption from ammonia and that this should be taken as indicating the T–Y transition, making these objects of type Y0. In fact, this ammonia-absorption feature is the main criterion that has been adopted to define this class. However, this feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.
The brown dwarf with the latest assigned spectral type, WISE 1828+2650, is a >Y2 dwarf with an effective temperature originally estimated around 300 K, the temperature of the human body. Parallax measurements have however since shown that its luminosity is inconsistent with it being colder than ~400 K; the likely coolest Y dwarf currently known is WD 0806−661B with approximately 350 K.
The mass range for Y dwarfs is 9–25 Jupiter masses, but for young objects might reach below one Jupiter mass, which means that Y class objects straddle the 13 Jupiter mass deuterium-fusion limit that marks the division between brown dwarfs and planets.
Carbon-related late giant star classes.
Carbon-related stars are stars whose spectra indicate production of carbon by helium triple-alpha fusion. With increased carbon abundance, and some parallel s-process heavy element production, the spectra of these stars become increasingly deviant from the usual late spectral classes G, K and M. The giants among those stars are presumed to produce this carbon themselves, but not too few of this class of stars are believed to be double stars whose odd atmosphere once was transferred from a former carbon star companion that is now a white dwarf.
Class C: carbon stars.
Originally classified as R and N stars, these are also known as 'carbon stars'. These are red giants, near the end of their lives, in which there is an excess of carbon in the atmosphere. The old R and N classes ran parallel to the normal classification system from roughly mid G to late M. These have more recently been remapped into a unified carbon classifier C, with N0 starting at roughly C6. Another subset of cool carbon stars are the J-type stars, which are characterized by the strong presence of molecules of 13CN in addition to those of 12CN. A few dwarf (that is, main-sequence) carbon stars are known, but the overwhelming majority of known carbon stars are giants or supergiants.
Class S.
Class S stars have zirconium monoxide lines in addition to (or, rarely, instead of) those of titanium monoxide, and are in between the class M stars and the carbon stars. S stars have excess amounts of zirconium and other elements produced by the s-process, and have their carbon and oxygen abundances closer to equal than is the case for M stars. The latter condition results in both carbon and oxygen being locked up almost entirely in carbon monoxide molecules. For stars cool enough for carbon monoxide to form that molecule tends to "eat up" all of whichever element is less abundant, resulting in "leftover oxygen" (which becomes available to form titanium oxide) in stars of normal composition, "leftover carbon" (which becomes available to form the diatomic carbon molecules) in carbon stars, and "leftover nothing" in the S stars. The relation between these stars and the ordinary M stars indicates a continuum of carbon abundance. Like carbon stars, nearly all known S stars are giants or supergiants.
Classes MS and SC: intermediary carbon-related classes.
In between the M class and the S class, border cases are named MS stars. In a similar way border cases between the S class and the C-N class are named SC or CS. The sequence M → MS → S → SC → C-N is believed to be a sequence of increased carbon abundance with age for carbon stars in the asymptotic giant branch.
White dwarf classifications.
The class D (for Degenerate) is the modern classification used for white dwarfs – low-mass stars that are no longer undergoing nuclear fusion and have shrunk to planetary size, slowly cooling down. Class D is further divided into spectral types DA, DB, DC, DO, DQ, DX, and DZ. The letters are not related to the letters used in the classification of other stars, but instead indicate the composition of the white dwarf's visible outer layer or atmosphere.
The white dwarf types are as follows:
The type is followed by a number giving the white dwarf's surface temperature. This number is a rounded form of 50400/"T"eff, where "T"eff is the effective surface temperature, measured in kelvins. Originally, this number was rounded to one of the digits 1 through 9, but more recently fractional values have started to be used, as well as values below 1 and above 9.
Two or more of the type letters may be used to indicate a white dwarf which displays more than one of the spectral features above. Also, the letter "V" is used to indicate a variable white dwarf.
Extended white dwarf spectral types:
Variable star designations:
Non-stellar spectral types: Classes P and Q.
Finally, the classes P and Q are occasionally used for certain non-stellar objects. Type P objects are planetary nebulae and type Q objects are novae.
Degenerate and exotic stars.
These objects are not stars but are stellar remnants. They are much dimmer and if placed on the HR diagram, would be placed further to the lower left-hand corner.
Stellar classification, habitability, and the search for life.
Stability, luminosity, and lifespan are all factors in stellar habitability. We only know of one star that hosts life, the Sun, a G-class star with an abundance of heavy elements and low variability in brightness. It is also unlike many stellar systems in that there is only one star in its system (see Planetary habitability, under the binary systems section).
Working from these constraints and the problems of having an empirical sample set of only one, the range of stars that are predicted to be able to support life as we know it is limited by a few factors. Of the main-sequence star types, stars more massive than 1.5 times that of the Sun (spectral types O, B, and A) age too quickly for advanced life to develop (using Earth as a guideline). On the other extreme, dwarfs of less than half the mass of the Sun (spectral type M) are likely to tidally lock planets within their habitable zone, along with other problems (see Habitability of red dwarf systems). Although there are many problems facing life on red dwarfs, due to their sheer numbers and longevity many astronomers continue to model these systems.
For these reasons NASA's Kepler Mission has been searching for habitable planets mainly at main-sequence stars that are less massive than spectral type A but more massive than type M—i.e. main-sequence stars of types F, G, and K, the most probable stars to host life.
Variable star classification.
Stars that exhibit change in luminosity are variable stars. There is a variable star classification scheme that encompasses existing stars that are classified in the spectra classification.
Photometric classification.
Stars can also be classified using photometric data from any photometric system. For example, we can calibrate color index diagrams of U−B and B−V in the UBV system according to spectral and luminosity classes. Nevertheless, this calibration is not straightforward, because many effects are superimposed in such diagrams: interstellar reddening, color changes due to metallicity, and the blending of light from binary and multiple stars.
Photometric systems with more colors and narrower passbands allow a star's class, and hence physical parameters, to be determined more precisely. The most accurate determination comes of course from spectral measurements, but there is not always enough time to get qualitative spectra with high signal-to-noise ratio.

</doc>
<doc id="28928" url="http://en.wikipedia.org/wiki?curid=28928" title="Sinope">
Sinope

Sinope may refer to:

</doc>
<doc id="28929" url="http://en.wikipedia.org/wiki?curid=28929" title="Seven Sisters">
Seven Sisters

Seven Sisters is the common name for the Pleiades, a star cluster named for mythological characters.
Seven Sisters may also refer to:

</doc>
<doc id="28930" url="http://en.wikipedia.org/wiki?curid=28930" title="SN 1987A">
SN 1987A

SN 1987A was a supernova in the outskirts of the Tarantula Nebula in the Large Magellanic Cloud (a nearby dwarf galaxy). It occurred approximately 51.4 kiloparsecs from Earth, approximately 168,000 light-years, close enough that it was visible to the naked eye. It could be seen from the Southern Hemisphere. It was the closest observed supernova since SN 1604, which occurred in the Milky Way itself. The light from the new supernova reached Earth on February 23, 1987. As it was the first supernova discovered in 1987, it was labeled “1987A”. Its brightness peaked in May with an apparent magnitude of about 3 and slowly declined in the following months. It was the first opportunity for modern astronomers to see a supernova up close and observations have provided much insight into core-collapse supernovae.
Discovery.
SN 1987A was discovered by Ian Shelton and Oscar Duhalde at the Las Campanas Observatory in Chile on February 24, 1987, and within the same 24 hours independently by Albert Jones in New Zealand. On March 4–12, 1987, it was observed from space by Astron, the largest ultraviolet space telescope of that time.
Progenitor.
Four days after the event was recorded, the progenitor star was tentatively identified as Sanduleak -69° 202, a blue supergiant.
This was an unexpected identification, because at the time a blue supergiant was not considered a possibility for a supernova event in existing models of high mass stellar evolution. Many models of the progenitor have attributed the color to its chemical composition, particularly the low levels of heavy elements, among other factors. There has been some speculation that the star may have merged with a companion star prior to the supernova. However, it is now widely understood that blue supergiants are natural progenitors of supernovae, although there is still speculation that the evolution of such stars requires mass loss involving a binary companion. It is of note that the supernova of the blue giant Sanduleak -69° 202 was about one-tenth as luminous as the average observed type II supernova, which is associated with the denser makeup of the star. Because blue supergiant supernovae are not as bright as those generated by red supergiants, we cannot see them in as large a volume. We would thus not expect to see as many of them, and so they might not be as rare or unusual as previously thought.
Neutrino emissions.
Approximately two to three hours before the visible light from SN 1987A reached Earth, a burst of neutrinos was observed at three separate neutrino observatories. This is likely due to neutrino emission, which occurs simultaneously with core collapse, but preceding the emission of visible light. Transmission of visible light is a slower process that occurs only after the shock wave reaches the stellar surface.
At 07:35 UT, Kamiokande II detected 11 antineutrinos; IMB, 8 antineutrinos; and Baksan, 5 antineutrinos; in a burst lasting less than 13 seconds. Approximately three hours earlier, the Mont Blanc liquid scintillator detected a five-neutrino burst, but this is generally not believed to be associated with SN 1987A.
Although the actual neutrino count was only 24, it was a significant rise from the previously observed background level. This was the first time neutrinos known to be emitted from a supernova had been observed directly, which marked the beginning of neutrino astronomy. The observations were consistent with theoretical supernova models in which 99% of the energy of the collapse is radiated away in the form of neutrinos. The observations are also consistent with the models' estimates of a total neutrino count of 1058 with a total energy of 1046 joules.
The neutrino measurements allowed upper bounds on neutrino mass and charge, as well as the number of flavors of neutrinos and other properties. For example, the data show that within 5% confidence, the rest mass of the electron neutrino is at most 16 eV, 30-millionths the mass of an electron.
The data suggest that the total number of neutrino flavors is at most 8 but other observations and experiments give tighter estimates. Many of these results have since been confirmed or tightened by other neutrino experiments such as more careful analysis of solar neutrinos and atmospheric neutrinos as well as experiments with artificial neutrino sources.
Missing neutron star.
SN 1987A appears to be a core-collapse supernova, which should result in a neutron star given the size of the original star. The neutrino data indicate that a compact object did form at the star's core. However, since the supernova first became visible, astronomers have been searching for the collapsed core but have not detected it. The Hubble Space Telescope has taken images of the supernova regularly since August 1990, but, so far, the images have shown no evidence of a neutron star. A number of possibilities for the 'missing' neutron star are being considered, although none are clearly favored. The first is that the neutron star is enshrouded in dense dust clouds so that it cannot be seen. Another is that a pulsar was formed, but with either an unusually large or small magnetic field. It is also possible that large amounts of material fell back on the neutron star, so that it further collapsed into a black hole. Neutron stars and black holes often give off light when material falls onto them. If there is a compact object in the supernova remnant, but no material to fall onto it, it would be very dim and could therefore avoid detection. Other scenarios have also been considered, such as if the collapsed core became a quark star.
Light curve.
Much of the "light curve," or graph of luminosity as a function of time, after the explosion of a Type II Supernova such as SN 1987A is dominated by radioactive decay processes. The radioactive decay of 56Ni through 56Co to 56Fe produces high-energy photons that dominate the energy output of the ejecta at intermediate (several weeks) to late times (several months). The peak of the light curve was caused by the decay of 56Ni to 56Co (half life 6 days) while the later light curve of SN 1987A in particular fit very closely with the 77.3 day half-life of 56Co decaying to 56Fe.
Because the 56Co has now completely decayed, the luminosity of the SN 1987A ejecta is currently powered by the radioactive decay of 44Ti with a half life of about 60 years. Observations by the INTEGRAL mission showed that the total mass of radioactive 44Ti synthesized during the explosion was 3.1 ± 0.8×10−4 M☉.
Observations of these radioactive decays in the 1987A light curve have measured accurate total masses of the 56Ni, 57Ni, and 44Ti created in the explosion, which provides constraints on the supernova model.
Interaction with circumstellar material.
The three bright rings around SN 1987A are material from the stellar wind of the progenitor. These rings were ionized by the ultraviolet flash from the supernova explosion, and consequently began emitting in various emission lines. These rings did not "turn on" until several months after the supernova; the turn-on process can be very accurately studied through spectroscopy. The rings are large enough that their angular size can be measured accurately: the inner ring is 0.808 arcseconds in radius. Using the distance light must have traveled to light up the inner ring as the base of a right angle triangle and the angular size as seen from the Earth for the local angle, one can use basic trigonometry to calculate the distance to SN1987A, which is about 168,000 light-years. The material from the explosion is catching up with the material expelled during both its red and blue supergiant phases and heating it, so we observe ring structures about the star.
Around 2001, the expanding (>7000 km/s) supernova ejecta collided with the inner ring. This caused its heating and the generation of x-rays — the x-ray flux from the ring increased by a factor of three between 2001 and 2009. A part of the x-ray radiation, which is absorbed by the dense ejecta close to the center, is responsible for a comparable increase in the optical flux from the supernova remnant in 2001–2009. This increase of the brightness of the remnant reversed the trend observed before 2001, when the optical flux was decreasing due to the decaying of 44Ti isotope.

</doc>
<doc id="28931" url="http://en.wikipedia.org/wiki?curid=28931" title="Standard Oil">
Standard Oil

Standard Oil Co. Inc. was an American oil producing, transporting, refining, and marketing company. Established in 1870 as a corporation in Ohio, it was the largest oil refiner in the world. Its controversial history as one of the world's first and largest multinational corporations ended in 1911, when the United States Supreme Court ruled that Standard was an illegal monopoly.
Standard Oil dominated the oil products market initially through horizontal integration in the refining sector, then, in later years vertical integration; the company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened consumers.
John D. Rockefeller was a founder, chairman and major shareholder. With the dissolution of the Standard Oil trust into 33 smaller companies, Rockefeller became the richest man in the world. Other notable Standard Oil principals include Henry Flagler, developer of the Florida East Coast Railway and resort cities, and Henry H. Rogers, who built the Virginian Railway.
Early years.
Standard Oil began as an Ohio partnership formed by the well-known industrialist John D. Rockefeller, his brother William Rockefeller, Henry Flagler, chemist Samuel Andrews, silent partner Stephen V. Harkness, and Oliver Burr Jennings, who had married the sister of William Rockefeller's wife. In 1870 Rockefeller incorporated Standard Oil in Ohio. Of the initial 10,000 shares, John D. Rockefeller received 2,667; Harkness received 1,334; William Rockefeller, Flagler, and Andrews received 1,333 each; Jennings received 1,000; and the firm of Rockefeller, Andrews & Flagler received 1,000. Using highly effective tactics, later widely criticized, it absorbed or destroyed most of its competition in Cleveland in less than two months in 1872 and later throughout the northeastern United States.
In the early years, John D. Rockefeller dominated the combine; he was the single most important figure in shaping the new oil industry. He quickly distributed power and the tasks of policy formation to a system of committees, but always remained the largest shareholder. Authority was centralized in the company's main office in Cleveland, but decisions in the office were made in a cooperative way.
In response to state laws trying to limit the scale of companies, Rockefeller and his associates developed innovative ways of organizing, to effectively manage their fast growing enterprise. On January 2, 1882, they combined their disparate companies, spread across dozens of states, under a single group of trustees. By a secret agreement, the existing thirty-seven stockholders conveyed their shares "in trust" to nine Trustees: John and William Rockefeller, Oliver H. Payne, Charles Pratt, Henry Flagler, John D. Archbold, William G. Warden, Jabez Bostwick, and Benjamin Brewster. This organization proved so successful that other giant enterprises adopted this "trust" form.
The company grew by increasing sales and also through acquisitions. After purchasing competing firms, Rockefeller shut down those he believed to be inefficient and kept the others. In a seminal deal, in 1868, the Lake Shore Railroad, a part of the New York Central, gave Rockefeller's firm a going rate of one cent a gallon or forty-two cents a barrel, an effective 71 percent discount from its listed rates in return for a promise to ship at least 60 carloads of oil daily and to handle the loading and unloading on its own. Smaller companies decried such deals as unfair because they were not producing enough oil to qualify for discounts.
In 1872, Rockefeller joined the South Improvement Co. which would have allowed him to receive rebates for shipping and receive drawbacks on oil his competitors shipped. But when this deal became known, competitors convinced the Pennsylvania Legislature to revoke South Improvement's charter. No oil was ever shipped under this arrangement.
Standard's actions and secret transport deals helped its kerosene price to drop from 58 to 26 cents from 1865 to 1870. Competitors disliked the company's business practices, but consumers liked the lower prices. Standard Oil, being formed well before the discovery of the Spindletop oil field and a demand for oil other than for heat and light, was well placed to control the growth of the oil business. The company was perceived to own and control all aspects of the trade.
In 1885, Standard Oil of Ohio moved its headquarters from Cleveland to its permanent headquarters at 26 Broadway in New York City. Concurrently, the trustees of Standard Oil of Ohio chartered the Standard Oil Co. of New Jersey (SOCNJ) to take advantages of New Jersey's more lenient corporate stock ownership laws.
Also in 1890, Congress passed the Sherman Antitrust Act — a source of American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase "restraint of trade" remained subjective. The Standard Oil group quickly attracted attention from antitrust authorities leading to a lawsuit filed by Ohio Attorney General David K. Watson.
From 1882 to 1906, Standard paid out $548,436,000 in dividends at 65.4 percent payout ratio. The total net earnings from 1882 to 1906 amounted to $838,783,800, exceeding the dividends by $290,347,800, which was used for plant expansions.
1895–1913.
In 1896, John Rockefeller retired from the Standard Oil Co. of New Jersey, the holding company of the group, but remained president and a major shareholder. Vice-president John Dustin Archbold took a large part in the running of the firm. At the same time, state and federal laws sought to counter this development with "antitrust" laws. In 1911, the US Justice Department sued the group under the federal antitrust law and ordered its breakup into 34 companies.
Standard Oil's market position was initially established through an emphasis on efficiency and responsibility. While most companies dumped gasoline in rivers (this was before the automobile was popular), Standard used it to fuel its machines. While other companies' refineries piled mountains of heavy waste, Rockefeller found ways to sell it. For example, Standard created the first synthetic competitor for beeswax and bought the company that invented and produced Vaseline, the Chesebrough Manufacturing Co., which was a Standard company only from 1908 until 1911.
One of the original "Muckrakers" was Ida M. Tarbell, an American author and journalist. Her father was an oil producer whose business had failed due to Rockefeller's business dealings. After extensive interviews with a sympathetic senior executive of Standard Oil, Henry H. Rogers, Tarbell's investigations of Standard Oil fueled growing public attacks on Standard Oil and on monopolies in general. Her work was published in 19 parts in "McClure's" magazine from November 1902 to October 1904, then in 1904 as the book "The History of the Standard Oil Co.".
The Standard Oil Trust was controlled by a small group of families. Rockefeller stated in 1910: "I think it is true that the Pratt family, the Payne-Whitney family (which were one, as all the stock came from Colonel Payne), the Harkness-Flagler family (which came into the company together) and the Rockefeller family controlled a majority of the stock during all the history of the company up to the present time".
These families reinvested most of the dividends in other industries, especially railroads. They also invested heavily in the gas and the electric lighting business (including the giant Consolidated Gas Co. of New York City). They made large purchases of stock in US Steel, Amalgamated Copper, and even Corn Products Refining Co..
British petroleum entrepreneur in Mexico Weetman Pearson, began negotiating with Standard Oil in 1912-13 to sell his "El Aguila" oil company, since Pearson was no longer bound to promises to the Porfirio Díaz regime (1876-1911) to not to sell to U.S. interests. However, the deal fell through and the firm was sold to Royal Dutch Shell.
Standard Oil In China.
Standard Oil's production increased so rapidly it soon exceeded US demand and the company began viewing export markets. In the 1890s, Standard Oil began marketing kerosene to China's large population of close to 400 million as lamp fuel. For its Chinese trademark and brand Standard Oil adopted the name "Mei Foo" (), (which translates to American Trust). Mei Foo also became the name of the tin lamp that Standard Oil produced and gave away or sold cheaply to Chinese farmers, encouraging them to switch from vegetable oil to kerosene. Response was positive, sales boomed and China became Standard Oil's largest market in Asia. Prior to Pearl Harbor, Stanvac was the largest single US investment in SE Asia.
Socony's North China Department operated a subsidiary called Socony River and Coastal Fleet, North Coast Division, which became the North China Division of Stanvac after that company was formed in 1933. To distribute its products, Standard Oil constructed storage tanks, canneries (bulk oil from large ocean tankers was re-packaged into 5-gallon tins), warehouses and offices in key Chinese cities. For inland distribution the company had motor tank trucks and railway tank cars, and for river navigation it had a fleet of low draft steamers and other vessels.
Stanvac's North China Division, based in Shanghai, owned hundreds of river going vessels, including motor barges, steamers, launches, tugboats and tankers. Up to 13 tankers operated on the Yangtze River, the largest of which were "Mei Ping" (1,118 gt), "Mei Hsia" (1,048 gt),and "Mei An" (934 gt). All three were destroyed in the 1937 USS "Panay" incident. "Mei An" was launched in 1901 and was the first vessel in the fleet. Other vessels included "Mei Chuen", "Mei Foo", "Mei Hung", "Mei Kiang", "Mei Lu", "Mei Tan", "Mei Su", "Mei Xia", "Mei Ying", and "Mei Yun". "Mei Hsia", a tanker, was specially designed for river duty and was built by New Engineering and Shipbuilding Works of Shanghai, who also built the 500-ton launch "Mei Foo" in 1912. "Mei Hsia" ("Beautiful Gorges") was launched in 1926 and carried 350 tons of bulk oil in three holds, plus a forward cargo hold and space between decks for carrying general cargo or packed oil. She had a length of 206 ft, a beam of 32 ft, depth of 10 ft and had a bulletproof wheelhouse. "Mei Ping" ("Beautiful Tranquility") launched in 1927, was designed offshore but assembled and finished in Shanghai. Its oil fuel burners came from the U.S. and water tube boilers came from England.
Standard Oil in Arabian Peninsula.
Standard Oil Company and Socony-Vacuum Oil Company became partners in providing markets for the oil reserves in the Middle East. In 1906, SOCONY (later Mobil) opened its first fuel terminals in Alexandria. It explored in Palestine before the World War broke out, but ran into conflict with the British government.
Monopoly charges and anti-trust legislation.
By 1890, Standard Oil controlled 88 percent of the refined oil flows in the United States. The state of Ohio successfully sued Standard, compelling the dissolution of the trust in 1892. But Standard simply separated Standard Oil of Ohio and kept control of it. Eventually, the state of New Jersey changed its incorporation laws to allow a company to hold shares in other companies in any state. So, in 1899, the Standard Oil Trust, based at 26 Broadway in New York, was legally reborn as a holding company, the "Standard Oil Co. of New Jersey" (SOCNJ), which held stock in 41 other companies, which controlled other companies, which in turn controlled yet other companies. This conglomerate was seen by the public as all-pervasive, controlled by a select group of directors, and completely unaccountable.
In 1904, Standard controlled 91 percent of production and 85 percent of final sales. Most of its output was kerosene, of which 55 percent was exported around the world. After 1900 it did not try to force competitors out of business by underpricing them. The federal Commissioner of Corporations studied Standard's operations from the period of 1904 to 1906 and concluded that "beyond question... the dominant position of the Standard Oil Co. in the refining industry was due to unfair practices—to abuse of the control of pipe-lines, to railroad discriminations, and to unfair methods of competition in the sale of the refined petroleum products". Due to competition from other firms, their market share had gradually eroded to 70 percent by 1906 which was the year when the antitrust case was filed against Standard, and down to 64 percent by 1911 when Standard was ordered broken up and at least 147 refining companies were competing with Standard including Gulf, Texaco, and Shell. It did not try to monopolize the exploration and pumping of oil (its share in 1911 was 11 percent).
In 1909, the US Department of Justice sued Standard under federal anti-trust law, the Sherman Antitrust Act of 1890, for sustaining a monopoly and restraining interstate commerce by:
"Rebates, preferences, and other discriminatory practices in favor of the combination by railroad companies; restraint and monopolization by control of pipe lines, and unfair practices against competing pipe lines; contracts with competitors in restraint of trade; unfair methods of competition, such as local price cutting at the points where necessary to suppress competition; [and] espionage of the business of competitors, the operation of bogus independent companies, and payment of rebates on oil, with the like intent."
The lawsuit argued that Standard's monopolistic practices had taken place over the preceding four years:
"The general result of the investigation has been to disclose the existence of numerous and flagrant discriminations by the railroads in behalf of the Standard Oil Co. and its affiliated corporations. With comparatively few exceptions, mainly of other large concerns in California, the Standard has been the sole beneficiary of such discriminations. In almost every section of the country that company has been found to enjoy some unfair advantages over its competitors, and some of these discriminations affect enormous areas."
The government identified four illegal patterns: 1) secret and semi-secret railroad rates; (2) discriminations in the open arrangement of rates; (3) discriminations in classification and rules of shipment; (4) discriminations in the treatment of private tank cars. The government alleged:
"Almost everywhere the rates from the shipping points used exclusively, or almost exclusively, by the Standard are relatively lower than the rates from the shipping points of its competitors. Rates have been made low to let the Standard into markets, or they have been made high to keep its competitors out of markets. Trifling differences in distances are made an excuse for large differences in rates favorable to the Standard Oil Co., while large differences in distances are ignored where they are against the Standard. Sometimes connecting roads prorate on oil—that is, make through rates which are lower than the combination of local rates; sometimes they refuse to prorate; but in either case the result of their policy is to favor the Standard Oil Co. Different methods are used in different places and under different conditions, but the net result is that from Maine to California the general arrangement of open rates on petroleum oil is such as to give the Standard an unreasonable advantage over its competitors"
The government said that Standard raised prices to its monopolistic customers but lowered them to hurt competitors, often disguising its illegal actions by using bogus supposedly independent companies it controlled.
"The evidence is, in fact, absolutely conclusive that the Standard Oil Co. charges altogether excessive prices where it meets no competition, and particularly where there is little likelihood of competitors entering the field, and that, on the other hand, where competition is active, it frequently cuts prices to a point which leaves even the Standard little or no profit, and which more often leaves no profit to the competitor, whose costs are ordinarily somewhat higher."
On May 15, 1911, the US Supreme Court upheld the lower court judgment and declared the Standard Oil group to be an "unreasonable" monopoly under the Sherman Antitrust Act, Section II. It ordered Standard to break up into 90 independent companies with different boards of directors, the biggest two of the companies were Standard Oil of New Jersey (which became Exxon) and Standard Oil of New York (which became Mobil).
Standard's president, John D. Rockefeller, had long since retired from any management role. But, as he owned a quarter of the shares of the resultant companies, and those share values mostly doubled, he emerged from the dissolution as the richest man in the world. The dissolution had actually propelled Rockefeller's personal wealth.
Breakup.
By 1911, with public outcry at a climax, the Supreme Court of the United States ruled, in "Standard Oil Co. of New Jersey v. United States", that Standard Oil must be dissolved under the Sherman Antitrust Act and split into 34 companies. Two of these companies were Jersey Standard ("Standard Oil Co. of New Jersey"), which eventually became Exxon, and Socony ("Standard Oil Co. of New York"), which eventually became Mobil.
Over the next few decades, both companies grew significantly. Jersey Standard, led by Walter C. Teagle, became the largest oil producer in the world. It acquired a 50 percent share in Humble Oil & Refining Co., a Texas oil producer. Socony purchased a 45 percent interest in Magnolia Petroleum Co., a major refiner, marketer and pipeline transporter. In 1931, Socony merged with Vacuum Oil Co., an industry pioneer dating back to 1866, and a growing Standard Oil spin-off in its own right.
In the Asia-Pacific region, Jersey Standard had oil production and refineries in Indonesia but no marketing network. Socony-Vacuum had Asian marketing outlets supplied remotely from California. In 1933, Jersey Standard and Socony-Vacuum merged their interests in the region into a 50–50 joint venture. Standard-Vacuum Oil Co., or "Stanvac", operated in 50 countries, from East Africa to New Zealand, before it was dissolved in 1962.
Other Standard Oil breakup companies include "Standard Oil of Ohio" which became SOHIO, "Standard Oil of Indiana" which became Amoco after other mergers and a name change in the 1980s, and "Standard Oil of California" which became the Chevron Corp.
Legacy and criticism of breakup.
The U.S. Supreme Court ruled in 1911 that antitrust law required Standard Oil to be broken into smaller, independent companies. Among the "baby Standards" that still exist are ExxonMobil and Chevron. Some have speculated that if not for that court ruling, Standard Oil could have possibly been worth more than $1 trillion today. 
Whether the breakup of Standard Oil was beneficial is a matter of some controversy.
Some economists believe that Standard Oil was not a monopoly, and also argue that the intense free market competition resulted in cheaper oil prices and more diverse petroleum products. Critics claimed that success in meeting consumers needs was driving other companies out of the market who were not as successful. An example of this thinking was given in 1890 when Rep. William Mason, arguing in favor of the Sherman Antitrust Act, said: "trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to one cent a barrel, it would not right the wrong done to people of this country by the "trusts" which have destroyed legitimate competition and driven honest men from legitimate business enterprise".
The Sherman Antitrust Act prohibits the restraint of trade. Defenders of Standard Oil insist that the company did not restrain trade; they were simply superior competitors. The federal courts ruled otherwise.
Some economic historians have observed that Standard Oil was in the process of losing its monopoly at the time of its breakup in 1911. Although Standard had 90 percent of American refining capacity in 1880, by 1911 that had shrunk to between 60 and 65 percent, due to the expansion in capacity by competitors. Numerous regional competitors (such as Pure Oil in the East, Texaco and Gulf Oil in the Gulf Coast, Cities Service and Sun in the Midcontinent, Union in California, and Shell overseas) had organized themselves into competitive vertically integrated oil companies, the industry structure pioneered years earlier by Standard itself. In addition, demand for petroleum products was increasing more rapidly than the ability of Standard to expand. The result was that although in 1911 Standard still controlled most production in the older US regions of the Appalachian Basin (78 percent share, down from 92 percent in 1880), Lima-Indiana (90 percent, down from 95 percent in 1906), and the Illinois Basin (83 percent, down from 100 percent in 1906), its share was much lower in the rapidly expanding new regions that would dominate US oil production in the 20th century. In 1911 Standard controlled only 44 percent of production in the Midcontinent, 29 percent in California, and 10 percent on the Gulf Coast.
Some analysts argue that the breakup was beneficial to consumers in the long run, and no one has ever proposed that Standard Oil be reassembled in pre-1911 form. ExxonMobil, however, does represent a substantial part of the original company.
Since the breakup of Standard Oil, several companies, such as General Motors and Microsoft, have come under antitrust investigation for being inherently too large for market competition; however, most of them remained together. The only company since the breakup of Standard Oil that was divided into parts like Standard Oil was AT&T, which after decades as a regulated natural monopoly, was forced to divest itself of the Bell System in 1984.
Successor companies.
The successor companies from Standard Oil's breakup form the core of today's US oil industry. (Several of these companies were considered among the Seven Sisters who dominated the industry worldwide for much of the 20th century.) They include:
Other Standard Oil spin-offs:
Other companies divested in the 1911 breakup:
Note: Standard Oil of Colorado was not a successor company; the name was used to capitalize on the Standard Oil brand in the 1930s. Standard Oil of Connecticut is a fuel oil marketer not related to the Rockefeller companies.
Rights to the name.
Of the 34 "Baby Standards", 11 were given rights to the Standard Oil name, based on the state they were in. Conoco and Atlantic elected to use their respective names instead of the Standard name, and their rights would be claimed by other companies.
By the 1980s, most companies were using their individual brand names instead of the Standard name, with Amoco being the last one to have widespread use of the "Standard" name, as it gave Midwestern owners the option of using the Amoco name or Standard. 
Currently, three supermajor companies own the rights to the Standard name in the United States: ExxonMobil, Chevron Corp., and BP. BP acquired its rights through acquiring Standard Oil of Ohio and Amoco, and has a small handful of stations in the Midwestern United States using the Standard name. Likewise, BP continues to sell marine fuel under the Sohio brand at various marinas throughout Ohio. Chevron has one station in each state it owns the rights to branded as Standard except in Kentucky, which it withdrew from in July 2010. ExxonMobil keeps the Esso trademark alive at stations that sell diesel fuel by selling "Esso Diesel" displayed on the pumps. ExxonMobil has full international rights to the Standard name, and continues to use the Esso name overseas and in Canada.

</doc>
<doc id="28935" url="http://en.wikipedia.org/wiki?curid=28935" title="Seismology">
Seismology

Seismology (; from Greek σεισμός "earthquake" and -λογία "study of") is the scientific study of earthquakes and the propagation of elastic waves through the Earth or through other planet-like bodies. The field also includes studies of earthquake environmental effects, such as tsunamis as well as diverse seismic sources such as volcanic, tectonic, oceanic, atmospheric, and artificial processes (such as explosions). A related field that uses geology to infer information regarding past earthquakes is paleoseismology. A recording of earth motion as a function of time is called a seismogram. A seismologist is a scientist who does research in seismology.
History.
Scholarly interest in earthquakes can be traced back to antiquity. Early speculations on the natural causes of earthquakes were included in the writings of Thales of Miletus (c. 585 BCE), Anaximenes of Miletus (c. 550 BCE), Aristotle (c. 340 BCE) and Zhang Heng (132 CE).
In 132 CE, Zhang Heng of China's Han dynasty designed the first known seismoscope.
In 1664, Athanasius Kircher argued that earthquakes were caused by the movement of fire within a system of channels inside the Earth.
In 1703, Martin Lister (1638 to 1712) and Nicolas Lemery (1645 to 1715) proposed that earthquakes were caused by chemical explosions within the earth.
The Lisbon earthquake of 1755, coinciding with the general flowering of science in Europe, set in motion intensified scientific attempts to understand the behaviour and causation of earthquakes. The earliest responses include work by John Bevis (1757) and John Michell (1761). Michell determined that earthquakes originate within the Earth and were waves of movement caused by "shifting masses of rock miles below the surface."
From 1857, Robert Mallet laid the foundation of instrumental seismology and carried out seismological experiments using explosives.
In 1897, Emil Wiechert's theoretical calculations led him to conclude that the Earth's interior consists of a mantle of silicates, surrounding a core of iron.
In 1906 Richard Dixon Oldham identified the separate arrival of P-waves, S-waves and surface waves on seismograms and found the first clear evidence that the Earth has a central core.
In 1910, after studying the 1906 San Francisco earthquake, Harry Fielding Reid put forward the "elastic rebound theory" which remains the foundation for modern tectonic studies. The development of this theory depended on the considerable progress of earlier independent streams of work on the behaviour of elastic materials and in mathematics.
In 1926, Harold Jeffreys was the first to claim, based on his study of earthquake waves, that below the crust, the core of the Earth is liquid. 
In 1937, Inge Lehmann determined that within the earth's liquid outer core there is a solid "inner" core.
By the 1960s, earth science had developed to the point where a comprehensive theory of the causation of seismic events had come together in the now well-established theory of plate tectonics.
Types of seismic wave.
Seismic waves are elastic waves that propagate in solid or fluid materials. They can be divided into "body waves" that travel through the interior of the materials; "surface waves" that travel along surfaces or interfaces between materials; and "normal modes", a form of standing wave.
Body waves.
There are two types of body waves, P-waves and S-waves. Pressure waves or Primary waves (P-waves), are longitudinal waves that involve compression and rarefaction (expansion) in the direction that the wave is traveling. P-waves are the fastest waves in solids and are therefore the first waves to appear on a seismogram. S-waves, also called shear or secondary waves, are transverse waves that involve perpendicular motion to the direction of propagation. As a result, S-waves appear later than P-waves on a seismogram. Fluids cannot support this perpendicular motion, or shear, so S-waves only travel in solids. P-waves travel in both solids and fluids.
Surface waves.
The two primary types of surface waves are the Rayleigh wave,which has some compressional motion, and the Love wave, which does not. Rayleigh waves can be explained theoretically in terms of interacting P- and S-waves of vertical polarization that are required to satisfy the boundary conditions on the free surface. Love waves can exist in the presence of a subsurface layer, and they are formed by S-waves of horizontal polarization only. Surface waves travel more slowly than P-waves and S-waves; however, because they are guided by the surface of the Earth (and their energy is thus trapped near the Earth's surface) they can be much larger in amplitude than body waves, and can be the largest signals seen in earthquake seismograms. They are particularly strongly excited when their source is close to the surface of the Earth, as in a shallow earthquake or explosion.
Normal modes.
Both body and surface waves are traveling waves; however, large earthquakes can also make the Earth "ring" like a bell. This ringing is a mixture of normal modes with discrete frequencies and periods of an hour or shorter. Motion caused by a large earthquake can be observed for up to a month after the event. The first observations of normal modes were made in the 1960s as the advent of higher fidelity instruments coincided with two of the largest earthquakes of the 20th century - the 1960 Valdivia earthquake and the 1964 Alaska earthquake. Since then, the normal modes of the Earth have given us some of the strongest constraints on the deep structure of the Earth. See also free oscillations of the Earth.
Earthquakes.
One of the first attempts at the scientific study of earthquakes followed the 1755 Lisbon earthquake. Other notable earthquakes that spurred major advancements in the science of seismology include the 1857 Basilicata earthquake, 1906 San Francisco earthquake, the 1964 Alaska earthquake, the 2004 Sumatra-Andaman earthquake, and the 2011 Great East Japan earthquake.
Controlled seismic sources.
Seismic waves produced by explosions or vibrating controlled sources are one of the primary methods of underground exploration in geophysics (in addition to many different electromagnetic methods such as induced polarization and magnetotellurics). Controlled-source seismology has been used to map salt domes, faults, anticlines and other geologic traps in petroleum-bearing rocks, geological faults, rock types, and long-buried giant meteor craters. For example, the Chicxulub Crater, which was caused by an impact that has been implicated in the extinction of the dinosaurs, was localized to Central America by analyzing ejecta in the Cretaceous–Paleogene boundary, and then physically proven to exist using seismic maps from oil exploration.
Detection of seismic waves.
Seismometers are sensors that sense and record the motion of the Earth arising from elastic waves. Seismometers may be deployed at the Earth's surface, in shallow vaults, in boreholes, or underwater. A complete instrument package that records seismic signals is called a seismograph. Networks of seismographs continuously record ground motions around the world to facilitate the monitoring and analysis of global earthquakes and other sources of seismic activity. Rapid location of earthquakes makes tsunami warnings possible because seismic waves travel considerably faster than tsunami waves. Seismometers also record signals from non-earthquake sources ranging from explosions (nuclear and chemical), to local noise from wind or anthropogenic activities, to incessant signals generated at the ocean floor and coasts induced by ocean waves (the global microseism), to cryospheric events associated with large icebergs and glaciers. Above-ocean meteor strikes with energies as high as 4.2 × 1013 J (equivalent to that released by an explosion of ten kilotons of TNT) have been recorded by seismographs, as have a number of industrial accidents and terrorist bombs and events (a field of study referred to as forensic seismology). A major long-term motivation for the global seismographic monitoring has been for the detection and study of nuclear testing.
Mapping the earth's interior.
Because seismic waves commonly propagate efficiently as they interact with the internal structure of the Earth, they provide high-resolution noninvasive methods for studying the planet's interior. One of the earliest important discoveries (suggested by Richard Dixon Oldham in 1906 and definitively shown by Harold Jeffreys in 1926) was that the outer core of the earth is liquid. Since S-waves do not pass through liquids, the liquid core causes a "shadow" on the side of the planet opposite of the earthquake where no direct S-waves are observed. In addition, P-waves travel much slower through the outer core than the mantle.
Processing readings from many seismometers using seismic tomography, seismologists have mapped the mantle of the earth to a resolution of several hundred kilometers. This has enabled scientists to identify convection cells and other large-scale features such as Ultra Low Velocity Zones near the core–mantle boundary.
Seismology and society.
Earthquake prediction.
Forecasting a probable timing, location, magnitude and other important features of a forthcoming seismic event is called earthquake prediction. Various attempts have been made by seismologists and others to create effective systems for precise earthquake predictions, including the VAN method. Most seismologists do not believe that a system to provide timely warnings for individual earthquakes has yet been developed, and many believe that such a system would be unlikely to give useful warning of impending seismic events. However, more general forecasts routinely predict seismic hazard. Such forecasts estimate the probability of an earthquake of a particular size affecting a particular location within a particular time-span, and they are routinely used in earthquake engineering.
Public controversy over earthquake prediction erupted after Italian authorities indicted six seismologists and one government official for manslaughter in connection with a magnitude 6.3 earthquake in L'Aquila, Italy on April 5, 2009. The indictment has been widely perceived as an indictment for failing to predict the earthquake and has drawn condemnation from the American Association for the Advancement of Science and the American Geophysical Union. The indictment claims that, at a special meeting in L'Aquila the week before the earthquake occurred, scientists and officials were more interested in pacifying the population than providing adequate information about earthquake risk and preparedness.
Tools.
Seismological instruments can generate large amounts of data. Systems for processing such data include:

</doc>
<doc id="28936" url="http://en.wikipedia.org/wiki?curid=28936" title="Cyanoacrylate">
Cyanoacrylate

Cyanoacrylates are a family of strong fast-acting adhesives with industrial, medical, and household uses. Cyanoacrylate adhesives have a short shelf life if not used, about one year from manufacture if unopened, one month once opened. They have some minor toxicity.
Cyanoacrylates include methyl 2-cyanoacrylate, ethyl-2-cyanoacrylate (commonly sold under trade names such as "Super Glue" and "Krazy Glue"), n-butyl cyanoacrylate and 2-octyl cyanoacrylate (used in medical, veterinary and first aid applications). Octyl cyanoacrylate was developed to address toxicity concerns and to reduce skin irritation and allergic response. Cyanoacrylate adhesives are sometimes known generically as instant glues, power glues or superglues (although "Super Glue" is a trade name). The abbreviation "CA" is commonly used for industrial grades.
Development.
The original cyanoacrylates (the chemical name for the glue) were discovered in 1942 in a search for materials to make clear plastic gun sights during World War II, when a team of scientists headed by Harry Coover Jr. stumbled upon a formulation that stuck to everything that it came in contact with; it was rejected for the application. In 1951 cyanoacrylates were rediscovered by Eastman Kodak researchers Harry Coover Jr. and Fred Joyner, who recognized their true commercial potential; "Eastman #910" (later "Eastman 910") was the first cyanoacrylate adhesive to be sold, in 1958.
During the 1960s, Eastman Kodak sold cyanoacrylate to Loctite, which in turn repackaged and distributed it under a different brand name "Loctite Quick Set 404". In 1971 Loctite developed its own manufacturing technology and introduced its own line of cyanoacrylate, called "Super Bonder". Loctite quickly gained market share, and by the late 1970s it was believed to have exceeded Eastman Kodak's share in the North American industrial cyanoacrylate market. National Starch and Chemical Company purchased Eastman Kodak’s cyanoacrylate business and combined it with several acquisitions made throughout the 1970s forming Permabond. Other manufacturers of cyanoacrylate include LePage (a Canadian company acquired by Henkel in 1996), the Permabond Division of National Starch and Chemical, Inc., which was a subsidiary of Unilever. Together, Loctite, Eastman and Permabond accounted for approximately 75% of the industrial cyanoacrylate market. s of 2013[ [update]] Permabond continued to manufacture the original 910 formula.
Properties.
In its liquid form, cyanoacrylate consists of monomers of cyanoacrylate molecules. Methyl-2-cyanoacrylate (CH2=C(CN)COOCH3 or C5H5NO2) has a molecular weight equal to 111.1, a flashpoint of 79 °C, and a density of 1.1 g/ml. Ethyl 2-cyanoacrylate (C6H7NO2) has a molecular weight equal to 125 and a flashpoint of >75 °C. To facilitate easy handling, a cyanoacrylate adhesive is frequently formulated with an ingredient such as fumed silica to make it more viscous or gel-like. More recently, formulations are available with additives to increase shear strength, creating a more impact resistant bond. Such additives may include rubber, as in Loctite's "Ultra Gel", or others which are not specified.
In general, cyanoacrylate is an acrylic resin that rapidly polymerises in the presence of water (specifically hydroxide ions), forming long, strong chains, joining the bonded surfaces together. Because the presence of moisture causes the glue to set, exposure to normal levels of humidity in the air causes a thin skin to start to form within seconds, which very greatly slows the reaction. Because of this cyanoacrylate is applied thinly, to ensure that the reaction proceeds rapidly and a strong bond is formed within a reasonable time.
The reaction with moisture can cause a container of glue which has been opened and resealed to become unusable more quickly than if never opened. To minimise this reduction in shelf life cyanoacrylate, once opened, can be stored in an airtight container with a package of silica gel desiccant. Another technique is to insert a hypodermic needle into the opening of a tube. After using the glue, residual glue soon clogs the needle, keeping moisture out. The clog is removed by heating the needle (e.g. with a lighter) before use.
Uses.
Behaviours.
Cyanoacrylates are mainly used as adhesives. They require some care and knowledge for effective use: they do not bond some materials; their shelf life at room temperature is about 12 months unopened and one month once opened; they do not fill spaces, unlike epoxies, and a very thin layer bonds more effectively than a thicker one that does not cure properly; they bond many substances, including human skin and tissues, almost instantly and can cause harm to people. They have an exothermic reaction to natural fibres: cotton, wool, leather, see reaction with cotton below.
Cyanoacrylate glue has a low shearing strength, which has led to its use as a temporary adhesive in cases where the piece needs to be sheared off later. Common examples include mounting a workpiece to a sacrificial glue block on a lathe, and tightening pins and bolts.
Cyanoacrylate is not resistant to friction if applied to a smooth surface. Using either sugar or sandpaper can remove a good amount of Cyanoacrylate from a user's fingertips.
Electronics.
Cyanoacrylates are used to assemble prototype electronics (see wire wrap), flying model aircraft, and as retention dressings for nuts and bolts. Their effectiveness in bonding metal and general versatility have made them popular among modeling and miniatures hobbyists.
Aquaria.
Cyanoacrylate glue's ability to resist water has made it popular with marine aquarium hobbyists for fragging corals. The cut branches of hard corals such as Acropora can be glued to a piece of live rock (harvested reef coral) or Milliput (epoxy putty) to allow the new frag to grow out. It is safe to use directly in the tank, unlike silicone, which must be cured to be safe. However, as a class of adhesives, traditional cyanoacrylates are classified as having "weak" resistance to both moisture and heat although the inclusion of phthalic anhydride reportedly counteracts both of these characteristics.
Bonding smooth surfaces.
Most standard cyanoacrylate adhesives do not bond well with smooth glass, although they can be used as a quick, temporary bond prior to application of an epoxy or cyanoacrylate specifically formulated for use on glass. A mechanical adhesive bond may be formed around glass fibre mat or tissue to reinforce joints or to fabricate small parts.
Filler.
When added to baking soda (sodium bicarbonate), cyanoacrylate glue forms a hard, lightweight adhesive filler (baking soda is first used to fill a gap then the adhesive is dropped onto the baking soda). This works well with porous materials that the glue does not work well with alone. This method is sometimes used by aircraft modelers to assemble or repair polystyrene foam parts. It is also used to repair small nicks in the leading edge of composite propeller blades on light aircraft. The reaction between cyanoacrylate and baking soda is very exothermic (heat-producing) and also produces noxious vapors. See Reaction with cotton below.
Forensics.
Cyanoacrylate is used as a forensic tool to capture latent fingerprints on non-porous surfaces like glass, plastic, etc. Cyanoacrylate is warmed to produce fumes that react with the invisible fingerprint residues and atmospheric moisture to form a white polymer (polycyanoacrylate) on the fingerprint ridges. The ridges can then be recorded. The developed fingerprints are, on most surfaces (except on white plastic or similar), visible to the naked eye. Invisible or poorly visible prints can be further enhanced by applying a luminescent or non-luminescent stain.
Woodworking.
Thin CA glue has application in woodworking. It can be used as a fast-drying, glossy finish. The use of oil (such as boiled linseed oil) may be used to control the rate at which the CA cures. CA glue is also used in combination with sawdust (from a saw or sanding) to fill voids and cracks. These repair methods are used on piano soundboards, wood instruments, and wood furniture.
Medical.
CA glue was in veterinary use for mending bone, hide, and tortoise shell by the early 1970s or before. Harry Coover said in 1966 that a CA spray was used in the Vietnam War to reduce bleeding in wounded soldiers until they could be brought to a hospital. Butyl cyanoacrylate has been used medically since the 1970s. In the US, due to its potential to irritate the skin, the U.S. Food and Drug Administration did not approve its use as a medical adhesive until 1998 with Dermabond. Research has demonstrated the use of cyanoacrylate in wound closure as being safer and more functional than traditional suturing (stitches). The adhesive has demonstrated superior performance in the time required to close a wound, incidence of infection (suture canals through the skin's epidermal, dermal, and subcutaneous fat layers introduce extra routes of contamination), and final cosmetic appearance.
Some rock climbers use cyanoacrylate to repair damage to the skin on their fingertips. Glue covered fingertips do not leave fingerprints. Similarly, stringed-instrument players can form protective finger caps (in addition to calluses) with cyanoacrylates. While the glue is not very toxic and wears off quickly with shed skin, applying large quantities of glue and its fumes directly to the skin can cause chemical burns. 
While standard "superglue" is 100% ethyl cyanoacrylate, many custom formulations ("e.g.,", 91% ECA, 9% poly(methyl methacrylate), <0.5% hydroquinone, and a small amount of organic sulfonic acid and variations on the compound N-butyl-cyanoacrylate's for medical applications) have come to be used for specific applications.
Archery.
Cyanoacrylate is used in archery to glue fletching to arrow shafts. Some special fletching glues are really cyanoacrylate repackaged in special fletching glue kits. Often these tubes have a long thin metal nozzle to aid in better accuracy in the application of the glue to the base of the feather or plastic fletching to ensure a good bond to the arrow shaft.
Cosmetics.
Cyanoacrylate is used in the cosmetology/beauty industry as a "nail glue" for some artificial nail enhancements such as nail tips and nail wraps, and is sometimes mistaken for eye drops causing accidental injury.
Safety issues.
Skin injuries.
CA adhesives may glue skin and eyelids together within seconds. In the course of separating the glued body parts, injuries may occur by ripping off parts of the skin. However, when left alone, the glue will separate from the skin over the course of time (up to four days). The separation process can be accelerated by applying vegetable oil near, on, and around the glue. In the case of glued eyelids, a doctor should be consulted.
Toxicity.
The fumes from CA are a vaporized form of the cyanoacrylate monomer that irritate sensitive membranes in the eyes, nose, and throat. They are immediately polymerized by the moisture in the membranes and become inert. These risks can be minimized by using CA in well ventilated areas. About 5% of the population can become sensitized to CA fumes after repeated exposure, resulting in flu-like symptoms. It may also act as a skin irritant and may cause an allergic skin reaction. The ACGIH assign a Threshold Limit Value exposure limit of 200 parts per billion. On rare occasions, inhalation may trigger asthma. There is no singular measurement of toxicity for all cyanoacrylate adhesives as there is a wide variety of adhesives that contain various cyanoacrylate formulations.
The United States National Toxicology Program and the United Kingdom Health and Safety Executive have concluded that the use of ethyl cyanoacrylate is safe and that additional study is unnecessary. 2-octyl cyanoacrylate degrades much more slowly due to its longer organic backbone that slows the degradation of the adhesive enough to remain below the threshold of tissue toxicity. Due to the toxicity issues of ethyl cyanoacrylate, the use of 2-octyl cyanoacrylate for sutures is preferred.
Reaction with cotton.
Applying cyanoacrylate to some natural materials such as cotton, leather or wool (cotton swabs, cotton balls, and certain yarns or fabrics) results in a powerful, rapid exothermic reaction. The heat released may cause serious burns, ignite the cotton product, or release irritating white smoke. Material Safety Data Sheets for cyanoacrylate instruct users not to wear cotton or wool clothing, especially cotton gloves, when applying or handling cyanoacrylates.
Solvents and debonders.
Acetone, commonly found in nail polish remover, is a widely available solvent capable of softening cured cyanoacrylate. Other solvents include nitromethane, dimethyl sulfoxide, and methylene chloride. gamma-Butyrolactone may also be used to remove cured cyanoacrylate. Commercial debonders are also available.
Shelf life.
Cyanoacrylate adhesives have a short shelf life. Date-stamped containers help to ensure that the adhesive is still viable. One manufacturer supplies the following information and advice: when kept unopened in a cool, dry location such as a refrigerator at a temperature of about 55 °F (13 °C), the shelf life of cyanoacrylate will be extended from about one year from manufacture to at least 15 months. If the adhesive is to be used within six months, it is not necessary to refrigerate it. Cyanoacrylates are moisture-sensitive, and moving from a cool to a hot location will create condensation; after removing from the refrigerator, it is best to let the adhesive reach room temperature before opening. After opening, it should be used within 30 days. Open containers should not be refrigerated. Another manufacturer says that the maximum shelf life of 12 months is obtained for some of their cyanoacrylates if the original containers are stored at 35 to. User forums and some manufacturers say that an almost unlimited shelf life is attainable by storing unopened at -4 °F, the typical temperature of a domestic freezer, and allowing to reach room temperature before use. Rechilling an opened container may cause moisture from the air to condense in the container, however, reports from hobbyist suggest that storing in a freezer can preserve opened cyanoacrylate indefinitely.
As cyanoacrylates age they polymerize, become thicker, and cure more slowly. They can be thinned with a cyanoacrylate of the same chemical composition with lower viscosity. Storing cyanoacrylates below 0 °F will nearly stop the polymerization process and prevent aging.
Further reading.
</dl>

</doc>
<doc id="28938" url="http://en.wikipedia.org/wiki?curid=28938" title="Shell script">
Shell script

A shell script is a computer program designed to be run by the Unix shell, a command line interpreter. The various dialects of shell scripts are considered to be scripting languages.
Typical operations performed by shell scripts include file manipulation, program execution, and printing text.
Capabilities.
Shortcuts.
A shell script can provide a convenient variation of a system command where special environment settings, command options, or post-processing apply automatically, but in a way that allows the new script to still act as a fully normal Unix command.
One example would be to create a version of ls, the command to list files, giving it a shorter command name of l, which would be normally saved in a user's bin directory as /home/"username"/bin/l, and a default set of command options pre-supplied.
Here, the first line (Shebang) indicates which interpreter should execute the rest of the script, and the second line makes a listing with options for file format indicators, columns, all files (none omitted), and a size in blocks. The LC_COLLATE=C sets the default collation order to not fold upper and lower case together, not intermix dotfiles with normal filenames as a side effect of ignoring punctuation in the names (dotfiles are usually only shown if an option like -a is used), and the "$@" causes any parameters given to l to pass through as parameters to ls, so that all of the normal options and other syntax known to ls can still be used.
The user could then simply use l for the most commonly used short listing.
Another example of a shell script that could be used as a shortcut would be to print a list of all the files and directories within a given directory.
In this case, the shell script would start with its normal starting line of #!/bin/sh. Following this, the script executes the command clear which clears the terminal of all text before going to the next line. The following line provides the main function of the script. The ls -al command list the files and directories that are in the directory from which the script is being run. The ls command attributes could be changed to reflect the needs of the user.
Note: If an implementation does not have the clear command, try using the clr command instead.
Batch jobs.
Shell scripts allow several commands that would be entered manually at a command-line interface to be executed automatically, and without having to wait for a user to trigger each stage of the sequence. For example, in a directory with three C source code files, rather than manually running the four commands required to build the final program from them, one could instead create a C shell script, here named build and kept in the directory with them, which would compile them automatically:
The script would allow a user to save the file being edited, pause the editor, and then just run ./build to create the updated program, test it, and then return to the editor. Since the 1980s or so, however, scripts of this type have been replaced with utilities like make which are specialized for building programs.
Generalization.
Simple batch jobs are not unusual for isolated tasks, but using shell loops, tests, and variables provides much more flexibility to users. A Bash (Unix shell) script to convert JPEG images to PNG images, where the image names are provided on the command line—possibly via wildcards—instead of each being listed within the script, can be created with this file, typically saved in a file like /home/"username"/bin/jpg2png
The jpg2png command can then be run on an entire directory full of JPEG images with just /home/"username"/bin/jpg2png *.jpg
Verisimilitude.
A key feature of shell scripts is that the invocation of their interpreters is handled as a core operating system feature. So rather than a user's shell only being able to execute scripts in that shell's language, or a script only having its interpreter directive handled correctly if it was run from a shell (both of which were limitations in the early Bourne shell's handling of scripts), shell scripts are set up and executed by the OS itself. A modern shell script is not just on the same footing as system commands, but rather many system commands are actually shell scripts (or more generally, scripts, since some of them are not interpreted by a shell, but instead by Perl, Python, or some other language). This extends to returning exit codes like other system utilities to indicate success or failure, and allows them to be called as components of larger programs regardless of how those larger tools are implemented.
Like standard system commands, shell scripts classically omit any kind of filename extension unless intended to be read into a running shell through a special mechanism for this purpose (such as sh’s “codice_1”, or csh’s source).
Programming.
Many modern shells also supply various features usually found only in more sophisticated general-purpose programming languages, such as control-flow constructs, variables, comments, arrays, subroutine and so on. With these sorts of features available, it is possible to write reasonably sophisticated applications as shell scripts. However, they are still limited by the fact that most shell languages have little or no support for data typing systems, classes, threading, complex math, and other common full language features, and are also generally much slower than compiled code or interpreted languages written with speed as a performance goal.
The standard Unix tools sed and awk provide extra capabilities for shell programming; Perl can also be embedded in shell scripts as can other scripting languages like Tcl. Perl and Tcl come with graphics toolkits as well.
Other scripting languages.
Many powerful scripting languages have been introduced for tasks that are too large or complex to be comfortably handled with ordinary shell scripts, but for which the advantages of a script are desirable and the development overhead of a full-blown, compiled programming language would be disadvantageous. The specifics of what separates scripting languages from high-level programming languages is a frequent source of debate. But generally speaking a scripting language is one which requires an interpreter.
Life cycle.
Shell scripts often serve as an initial stage in software development, and are often subject to conversion later to a different underlying implementation, most commonly being converted to Perl, Python, or C. The interpreter directive allows the implementation detail to be fully hidden inside the script, rather than being exposed as a filename extension, and provides for seamless reimplementation in different languages with no impact on end users.
Advantages and disadvantages.
Perhaps the biggest advantage of writing a shell script is that the commands and syntax are exactly the same as those directly entered at the command line. The programmer does not have to switch to a totally different syntax, as they would if the script were written in a different language, or if a compiled language was used.
Often, writing a shell script is much quicker than writing the equivalent code in other programming languages. The many advantages include easy program or file selection, quick start, and interactive debugging. A shell script can be used to provide a sequencing and decision-making linkage around existing programs, and for moderately sized scripts the absence of a compilation step is an advantage. Interpretive running makes it easy to write debugging code into a script and re-run it to detect and fix bugs. Non-expert users can use scripting to tailor the behavior of programs, and shell scripting provides some limited scope for multiprocessing.
On the other hand, shell scripting is prone to costly errors. Inadvertent typing errors such as rm -rf * / (instead of the intended rm -rf */) are folklore in the Unix community; a single extra space converts the command from one that deletes everything in the sub-directories to one which deletes everything—and also tries to delete everything in the root directory. Similar problems can transform cp and mv into dangerous weapons, and misuse of the > redirect can delete the contents of a file. This is made more problematic by the fact that many UNIX commands differ in name by only one letter: cp, cd, dd, df, etc.
Another significant disadvantage is the slow execution speed and the need to launch a new process for almost every shell command executed. When a script's job can be accomplished by setting up a pipeline in which efficient filter commands perform most of the work, the slowdown is mitigated, but a complex script is typically several orders of magnitude slower than a conventional compiled program that performs an equivalent task.
There are also compatibility problems between different platforms. Larry Wall, creator of Perl, famously wrote that "It is easier to port a shell than a shell script."
Similarly, more complex scripts can run into the limitations of the shell scripting language itself; the limits make it difficult to write quality code, and extensions by various shells to ameliorate problems with the original shell language can make problems worse.
Many disadvantages of using some script languages are caused by design flaws within the language syntax or implementation, and are not necessarily imposed by the use of a text-based command line; there are a number of shells which use other shell programming languages or even full-fledged languages like Scsh (which uses Scheme).

</doc>
<doc id="28940" url="http://en.wikipedia.org/wiki?curid=28940" title="Subtitle">
Subtitle

A subtitle can refer to:

</doc>
<doc id="28942" url="http://en.wikipedia.org/wiki?curid=28942" title="Solder">
Solder

Solder (, or in North America ) is a fusible metal alloy used to join together metal workpieces and having a melting point below that of the workpiece(s).
Soft solder is typically thought of when solder or soldering is mentioned, with a typical melting range of 90 to. It is commonly used in electronics, plumbing, and assembly of sheet metal parts. Manual soldering uses a soldering iron or soldering gun. Alloys that melt between 180 and are the most commonly used. Soldering performed using alloys with a melting point above 450 C is called 'hard soldering', 'silver soldering', or brazing.
For certain proportions an alloy becomes eutectic and melts at a single temperature; non-eutectic alloys have markedly different "solidus" and "liquidus" temperatures, and within that range they exist as a paste of solid particles in a melt of the lower-melting phase. In electrical work, if the joint is disturbed in the pasty state before it has solidified totally, a poor electrical connection may result; use of eutectic solder reduces this problem. The pasty state of a non-eutectic solder can be exploited in plumbing as it allows molding of the solder during cooling, e.g. for ensuring watertight joint of pipes, resulting in a so-called 'wiped joint'.
For electrical and electronics work solder wire is available in a range of thicknesses for hand-soldering, and with cores containing flux. It is also available as a paste or as a preformed foil shaped to match the workpiece, more suitable for mechanized mass-production. Alloys of lead and tin were universally used in the past, and are still available; they are particularly convenient for hand-soldering. Lead-free solder, somewhat less convenient for hand-soldering, is often used to avoid the environmental effect of lead.
Plumbers often use bars of solder, much thicker than the wire used for electrical applications. Jewelers often use solder in thin sheets which they cut into snippets.
The word solder comes from the Middle English word "soudur", via Old French "solduree" and "soulder", from the Latin "solidare", meaning "to make solid".
With the reduction of the size of circuit board features, the size of interconnects shrinks as well. Current densities above 104 A/cm2 are often achieved and electromigration becomes a concern. At such current densities the Sn63Pb37 solder balls form hillocks on the anode side and voids on the cathode side; the increased content of lead on the anode side suggests lead is the primary migrating species.
Contact with molten solder can cause 'solder embrittlement' of materials, a type of liquid metal embrittlement.
Lead solder.
Tin/lead solders, also called soft solders, are commercially available with tin concentrations between 5% and 70% by weight. The greater the tin concentration, the greater the solder’s tensile and shear strengths. Alloys commonly used for electrical soldering are 60/40 Tin/lead (Sn/Pb) which melts at 183 °C and 63/37 Sn/Pb used principally in electrical/electronic work. The 63/37 is a eutectic alloy, which:
In plumbing, a higher proportion of lead was used, commonly 50/50. This had the advantage of making the alloy solidify more slowly. With the pipes being physically fitted together before soldering, the solder could be wiped over the joint to ensure watertightness. Although lead water pipes were displaced by copper when the significance of lead poisoning began to be fully appreciated, lead solder was still used until the 1980s because it was thought that the amount of lead that could leach into water from the solder was negligible from a properly soldered joint. The electrochemical couple of copper and lead promotes corrosion of the lead and tin. Tin, however, is protected by insoluble oxide. Since even small amounts of lead have been found detrimental to health, lead in plumbing solder was replaced by silver (food grade applications) or antimony, with copper often added, and the proportion of tin was increased (see Lead-free solder.)
The addition of tin—more expensive than lead—improves wetting properties of the alloy; lead itself has poor wetting characteristics. High-tin tin-lead alloys have limited use as the workability range can be provided by a cheaper high-lead alloy.
In electronics, components on printed circuit boards (PCBs) are connected to the printed circuit, and hence to other components, by soldered joints. For miniaturized PCB joints with surface mount components, solder paste has largely replaced solid solder.
Lead-tin solders readily dissolve gold plating and form brittle intermetallics.
Sn60Pb40 solder oxidizes on the surface, forming a complex 4-layer structure: tin(IV) oxide on the surface, below it a layer of tin(II) oxide with finely dispersed lead, followed by a layer of tin(II) oxide with finely dispersed tin and lead, and the solder alloy itself underneath.
Lead, and to some degree tin, as used in solder contains small but significant amounts of radioisotope impurities. Radioisotopes undergoing alpha decay are a concern due to their tendency to cause soft errors. Polonium-210 is especially problematic; lead-210 beta decays to bismuth-210 which then beta decays to polonium-210, an intense emitter of alpha particles. Uranium-238 and thorium-232 are other significant contaminants of alloys of lead.
Lead-free solder.
On July 1, 2006 the European Union Waste Electrical and Electronic Equipment Directive (WEEE) and Restriction of Hazardous Substances Directive (RoHS) came into effect prohibiting the inclusion of significant quantities of lead in most consumer electronics produced in the EU. Manufacturers in the U.S. may receive tax benefits by reducing the use of lead-based solder. Lead-free solders in commercial use may contain tin, copper, silver, bismuth, indium, zinc, antimony, and traces of other metals. Most lead-free replacements for conventional Sn60/Pb40 and Sn63/Pb37 solder have melting points from 5 to 20 °C higher, though solders with much lower melting points are available.
Drop-in replacements for silkscreen with solder paste soldering operations are available. Minor modification to the solder pots (e.g. titanium liners or impellers) used in wave-soldering operations may be desired to reduce maintenance costs associated with the increased tin-scavenging effects of high tin solders. Since the properties of lead-free solders are not as thoroughly known, they may therefore be considered less desirable for critical applications, like certain aerospace or medical projects. "Tin whiskers" were a problem with early electronic solders, and lead was initially added to the alloy in part to eliminate them.
Sn-Ag-Cu (Tin-Silver-Copper) solders are used by two thirds of Japanese manufacturers for reflow and wave soldering, and by about 75% of companies for hand soldering. The widespread use of this popular lead-free solder alloy family is based on the reduced melting point of the Sn-Ag-Cu ternary eutectic behavior (217 ˚C), which is below the Sn-3.5Ag (wt.%) eutectic of 221 °C and the Sn-0.7Cu eutectic of 227 °C (recently revised by P. Snugovsky to Sn-0.9Cu). The ternary eutectic behavior of Sn-Ag-Cu and its application for electronics assembly was discovered (and patented) by a team of researchers from Ames Laboratory, Iowa State University, and from Sandia National Laboratories-Albuquerque.
Much recent research has focused on selection of 4th element additions to Sn-Ag-Cu to provide compatibility for the reduced cooling rate of solder sphere reflow for assembly of ball grid arrays, e.g., Sn-3.5Ag-0.74Cu-0.21Zn (melting range of 217–220 ˚C) and Sn-3.5Ag-0.85Cu-0.10Mn (melting range of 211–215 ˚C).
Tin-based solders readily dissolve gold, forming brittle intermetallics; for Sn-Pb alloys the critical concentration of gold to embrittle the joint is about 4%. Indium-rich solders (usually indium-lead) are more suitable for soldering thicker gold layer as the dissolution rate of gold in indium is much slower. Tin-rich solders also readily dissolve silver; for soldering silver metallization or surfaces, alloys with addition of silvers are suitable; tin-free alloys are also a choice, though their wettability is poorer. If the soldering time is long enough to form the intermetallics, the tin surface of a joint soldered to gold is very dull.
Lead-free solder has a higher Young's modulus than lead-based solder, making it more brittle when deformed. When the PCB on which the electronic components are mounted is subject to bending stress due to warping, the solder joint deteriorates and fractures can appear. This effect is called solder cracking. Another fault is Kirkendall voids which are microscopic cavities in solder. When two different types of metal that are in contact are heated, dispersion occurs (see also Kirkendall effect). Repeated thermal cycling cause the formation of voids which tends to cause solder cracks. Lead-free solder can cause short life cycles of products, as well as planned obsolescence.
Flux-core solder.
Flux is a reducing agent designed to help reduce (return oxidized metals to their metallic state) metal oxides at the points of contact to improve the electrical connection and mechanical strength. The two principal types of flux are acid flux, used for metal mending and plumbing, and rosin flux, used in electronics, where the corrosiveness of acid flux and vapors released when solder is heated would risk damaging delicate circuitry.
Due to concerns over atmospheric pollution and hazardous waste disposal, the electronics industry has been gradually shifting from rosin flux to water-soluble flux, which can be removed with deionized water and detergent, instead of hydrocarbon solvents.
In contrast to using traditional bars or coiled wires of all-metal solder and manually applying flux to the parts being joined, much hand soldering since the mid-20th century has used flux-core solder. This is manufactured as a coiled wire of solder, with one or more continuous bodies of non-acid flux embedded lengthwise inside it. As the solder melts onto the joint, it frees the flux and releases that on it as well.
Hard solder.
Hard solders are used for brazing, and melt at higher temperatures. Alloys of copper with either zinc or silver are the most common.
In silversmithing or jewelry making, special hard solders are used that will pass assay. They contain a high proportion of the metal being soldered and lead is not used in these alloys. These solders vary in hardness, designated as "enameling", "hard", "medium" and "easy". Enameling solder has a high melting point, close to that of the material itself, to prevent the joint desoldering during firing in the enameling process. The remaining solder types are used in decreasing order of hardness during the process of making an item, to prevent a previously soldered seam or joint desoldering while additional sites are soldered. Easy solder is also often used for repair work for the same reason. Flux or rouge is also used to prevent joints from desoldering.
Silver solder is also used in manufacturing to join metal parts that cannot be welded. The alloys used for these purposes contain a high proportion of silver (up to 40%), and may also contain cadmium.
Solder alloys.
Notes on the above table.
Temperature ranges for solidus and liquidus (the boundaries of the mushy state) are listed as solidus/liquidus.
In the Sn-Pb alloys, tensile strength increases with increasing tin content. Indium-tin alloys with high indium content have very low tensile strength.
For soldering semiconductor materials, e.g. die attachment of silicon, germanium and gallium arsenide, it is important that the solder contains no impurities that could cause doping in the wrong direction. For soldering n-type semiconductors, solder may be doped with antimony; indium may be added for soldering p-type semiconductors. Pure tin and pure gold can be used.
Various fusible alloys can be used as solders with very low melting points; examples include Field's metal, Lipowitz's alloy, Wood's metal, and Rose's metal.
Properties.
The thermal conductivity of common solders ranges from 32 to 94 W/(m·K) and the density from 9.25 to 15.00 g/cm3.
Solidifying.
The solidifying behavior depends on the alloy composition. Pure metals solidify at a certain temperature, forming crystals of one phase. Eutectic alloys also solidify at a single temperature, all components precipitating simultaneously in so-called coupled growth. Non-eutectic compositions on cooling start to first precipitate the non-eutectic phase; dendrites when it is a metal, large crystals when it is an intermetallic compound. Such a mixture of solid particles in a molten eutectic is referred to as a mushy state. Even a relatively small proportion of solids in the liquid can dramatically lower its fluidity.
The temperature of total solidification is the solidus of the alloy, the temperature at which all components are molten is the liquidus.
The mushy state is desired where a degree of plasticity is beneficial for creating the joint, allowing filling larger gaps or being wiped over the joint (e.g. when soldering pipes). In hand soldering of electronics it may be detrimental as the joint may appear solidified while it is not yet. Premature handling of such joint then disrupts its internal structure and leads to compromised mechanical integrity.
Alloying element roles.
Different elements serve different roles in the solder alloy:
Impurities in solders.
Impurities usually enter the solder reservoir by dissolving the metals present in the assemblies being soldered. Dissolving of process equipment is not common as the materials are usually chosen to be insoluble in solder.
Intermetallics in solders.
Many different intermetallic compounds are formed during solidifying of solders and during their reactions with the soldered surfaces.
The intermetallics form distinct phases, usually as inclusions in a ductile solid solution matrix, but also can form the matrix itself with metal inclusions or form crystalline matter with different intermetallics. Intermetallics are often hard and brittle. Finely distributed intermetallics in a ductile matrix yield a hard alloy while coarse structure gives a softer alloy. A range of intermetallics often forms between the metal and the solder, with increasing proportion of the metal; e.g. forming a structure of Cu-Cu3Sn-Cu6Sn5-Sn.
Layers of intermetallics can form between the solder and the soldered material. These layers may cause mechanical reliability weakening and brittleness, increased electrical resistance, or electromigration and formation of voids. The gold-tin intermetallics layer is responsible for poor mechanical reliability of tin-soldered gold-plated surfaces where the gold plating did not completely dissolve in the solder.
Gold and palladium readily dissolve in solders. Copper and nickel tend to form intermetallic layers during normal soldering profiles. Indium forms intermetallics as well.
Indium-gold intermetallics are brittle and occupy about 4 times more volume than the original gold. Bonding wires are especially susceptible to indium attack. Such intermetallic growth, together with thermal cycling, can lead to failure of the bonding wires.
Copper plated with nickel and gold is often used. The thin gold layer facilitates good solderability of nickel as it protects the nickel from oxidation; the layer has to be thin enough to rapidly and completely dissolve so bare nickel is exposed to the solder.
Lead-tin solder layers on copper leads can form copper-tin intermetallic layers; the solder alloy is then locally depleted of tin and form a lead-rich layer. The Sn-Cu intermetallics then can get exposed to oxidation, resulting in impaired solderability.
Two processes play a role in a solder joint formation: interaction between the substrate and molten solder, and solid-state growth of intermetallic compounds. The base metal dissolves in the molten solder in an amount depending on its solubility in the solder. The active constituent of the solder reacts with the base metal with a rate dependent on the solubility of the active constituents in the base metal. The solid-state reactions are more complex – the formation of intermetallics can be inhibited by changing the composition of the base metal or the solder alloy, or by using a suitable barrier layer to inhibit diffusion of the metals.
Glass solder.
Glass solders are used to join glasses to other glasses, ceramics, metals, semiconductors, mica, and other materials, in a process called glass frit bonding. The glass solder has to flow and wet the soldered surfaces well below the temperature where deformation or degradation of either of the joined materials or nearby structures (e.g., metallization layers on chips or ceramic substrates) occurs. The usual temperature of achieving flowing and wetting is between 450 and 550 °C.
Two types of glass solders are used: vitreous, and devitrifying. Vitreous solders retain their amorphous structure during remelting, can be reworked repeatedly, and are relatively transparent. Devitrifying solders undergo partial crystallization during solidifying, forming a glass-ceramic, a composite of glassy and crystalline phases. Devitrifying solders usually create a stronger mechanical bond, but are more temperature-sensitive and the seal is more likely to be leaky; due to their polycrystalline structure they tend to be translucent or opaque. Devitrifying solders are frequently "thermosetting", as their melting temperature after recrystallization becomes significantly higher; this allows soldering the parts together at lower temperature than the subsequent bake-out without remelting the joint afterwards. Devitrifying solders frequently contain up to 25% zinc oxide. In production of cathode ray tubes, devitrifying solders based on PbO-B2O3-ZnO are used.
Very low temperature melting glasses, fluid at 200–400 °C, were developed for sealing applications for electronics. They can consist of binary or ternary mixtures of thallium, arsenic and sulfur. Zinc-silicoborate glasses can also be used for passivation of electronics; their coefficient of thermal expansion must match silicon (or the other semiconductors used) and they must not contain alkaline metals as those would migrate to the semiconductor and cause failures.
The bonding between the glass or ceramics and the glass solder can be either covalent, or, more often, van der Waals. The seal can be leak-tight; glass soldering is frequently used in vacuum technology. Glass solders can be also used as sealants; a vitreous enamel coating on iron lowered its permeability to hydrogen 10 times. Glass solders are frequently used for glass-to-metal seals and glass-ceramic-to-metal seals.
Glass solders are available as frit powder with grain size below 60 micrometers. They can be mixed with water or alcohol to form a paste for easy application, or with dissolved nitrocellulose or other suitable binder for adhering to the surfaces until being melted. The eventual binder has to be burned off before melting proceeds, requiring careful firing regime. The solder glass can be also applied from molten state to the area of the future joint during manufacture of the part. Due to their low viscosity in molten state, lead glasses with high PbO content (often 70–85%) are frequently used. The most common compositions are based on lead borates (leaded borate glass or borosilicate glass). Smaller amount of zinc oxide or aluminium oxide can be added for increasing chemical stability. Phosphate glasses can be also employed. Zinc oxide, bismuth trioxide, and copper(II) oxide can be added for influencing the thermal expansion; unlike the alkali oxides, these lower the softening point without increasing of thermal expansion.
Glass solders are frequently used in electronic packaging. CERDIP packagings are an example. Outgassing of water from the glass solder during encapsulation was a cause of high failure rates of early CERDIP integrated circuits. Removal of glass-soldered ceramic covers, e.g., for gaining access to the chip for failure analysis or reverse engineering, is best done by shearing; if this is too risky, the cover is polished away instead.
As the seals can be performed at much lower temperature than with direct joining of glass parts and without use of flame (using a temperature-controlled kiln or oven), glass solders are useful in applications like subminiature vacuum tubes or for joining mica windows to vacuum tubes and instruments (e.g., Geiger tube). Thermal expansion coefficient has to be matched to the materials being joined and often is chosen in between the coefficients of expansion of the materials. In case of having to compromise, subjecting the joint to compression stresses is more desirable than to tensile stresses. The expansion matching is not critical in applications where thin layers are used on small areas, e.g., fireable inks, or where the joint will be subjected to a permanent compression (e.g., by an external steel shell) offsetting the thermally introduced tensile stresses.
Glass solder can be used as an intermediate layer when joining materials (glasses, ceramics) with significantly different coefficient of thermal expansion; such materials cannot be directly joined by diffusion welding. Evacuated glazing windows are made of glass panels soldered together.
A glass solder is used, e.g., for joining together parts of cathode ray tubes and plasma display panels. Newer compositions lowered the usage temperature from 450 to 390 °C by reducing the lead(II) oxide content down from 70%, increasing the zinc oxide content, adding titanium dioxide and bismuth(III) oxide and some other components. The high thermal expansion of such glass can be reduced by a suitable ceramic filler. Lead-free solder glasses with soldering temperature of 450 °C were also developed.
Phosphate glasses with low melting temperature were developed. One of such compositions is phosphorus pentoxide, lead(II) oxide, and zinc oxide, with addition of lithium and some other oxides.
Conductive glass solders can be also prepared.
Preform.
A preform is a pre-made shape of solder specially designed for the application where it is to be used. Many methods are used to manufacture the solder preform, stamping being the most common. The solder preform may include the solder flux needed for the soldering process. This can be an internal flux, inside the solder preform, or external, with the solder preform coated.

</doc>
<doc id="28943" url="http://en.wikipedia.org/wiki?curid=28943" title="Shogun">
Shogun

A shogun (将軍, shōgun, ], literally "military cop
" or "general") was a hereditary military governor in Japan during the shogunate period from 1192 to 1867. In this period, the shoguns were the de facto rulers of the country, though officially they were appointed by the emperor. 
The modern rank of shōgun is equivalent to a generalissimo. Although the original meaning of "shogun" is simply a "general", the title was the short form of sei-i taishōgun (征夷大将軍), the governing individual at various times in the history of Japan, ending when Tokugawa Yoshinobu relinquished the office to the Meiji Emperor in 1867.
A shogun's office or administration is the "shogunate", known in Japanese as the bakufu (幕府, literally "tent office/government"), which originally referred to house of the general and later also suggested a private government under a shogun. The tent symbolized the field commander but also denoted that such an office was meant to be temporary. The shogun's officials were as a collective the "bakufu", and were those who carried out the actual duties of administration while the imperial court retained only nominal authority. In this context, the office of the shōgun was equivalent to that of a viceroy or governor-general, though the shōguns often wielded far greater power than a governor-general would ordinarily have.
Heian period (794–1185).
Originally, the title of "Sei-i Taishōgun" ("Commander-in-Chief of the Expeditionary Force Against the Barbarians") was given to military commanders during the early Heian Period for the duration of military campaigns against the Emishi, who resisted the governance of the Kyoto-based imperial court. Ōtomo no Otomaro was the first "Sei-i Taishōgun". The most famous of these shoguns was Sakanoue no Tamuramaro, who conquered the Emishi in the name of Emperor Kanmu. Eventually, the title was abandoned in the later Heian period after the Ainu had been either subjugated or driven to Hokkaidō.
In the later Heian, one more shogun was appointed. Minamoto no Yoshinaka was named "sei-i taishōgun" during the Gempei War, only to be killed shortly thereafter by Minamoto no Yoshitsune.
Kamakura shogunate (1192–1333).
In the early 11th century, daimyo protected by samurai came to dominate internal Japanese politics. Two of the most powerful families – the Taira and Minamoto – fought for control over the declining imperial court. The Taira family seized control from 1160 to 1185, but was defeated by the Minamoto in the Battle of Dan-no-ura. Minamoto no Yoritomo seized certain powers from the central government and aristocracy and established a feudal system based in Kamakura in which the private military, the samurai, gained some political powers while the emperors of Japan and the aristocracy in Japan remained the "de jure" rulers. In 1192, Yoritomo was awarded the title of "Sei-i Taishōgun" by the emperor and the political system he developed with a succession of shogun at the head became known as a shogunate.
Yoritomo's wife's family, the Hōjō, seized the power from the Kamakura shoguns. When Yoritomo's sons and heirs were assassinated, the shogun became a hereditary figurehead. Real power rested with the Hōjō regents. The Kamakura shogunate lasted for almost 150 years, from 1192 to 1333.
In 1274 and 1281, the Mongol Empire launched invasions against Japan. An attempt by Emperor Go-Daigo to restore imperial rule in 1331 was unsuccessful, but weakened the shogunate significantly and led to its eventual downfall.
The end of the Kamakura shogunate came when Kamakura fell in 1333, and the Hōjō Regency was destroyed. Two imperial families – the senior Northern Court and the junior Southern Court – had a claim to the throne. The problem was solved with the intercession of the Kamakura Shogunate, who had the two lines alternate. This lasted until 1331, when Go-Daigo (of the Southern Court) tried to overthrow the shogunate in order to stop the alternation. As a result, Go-Daigo was exiled. Around 1334–1336, Ashikaga Takauji helped Go-Daigo regain his throne.
The fight against the shogunate left the emperor with too many people claiming a limited supply of land. Ashikaga Takauji turned against the emperor when the discontent about the distribution of land grew great enough. In 1336 the emperor was banished again, in favor of a new emperor.
During the Kemmu Restoration, after the fall of the Kamakura shogunate in 1333, another short-lived shogun arose. Prince Moriyoshi (Morinaga), son of Go-Daigo, was awarded the title of "Sei-i Taishōgun". However, Prince Moriyoshi was later put under house arrest and, in 1335, killed by Ashikaga Tadayoshi.
Ashikaga shogunate (1336–1573).
In 1338 Ashikaga Takauji, like Yoritomo, a descendant of the Minamoto princes, was awarded the title of "sei-i taishōgun" and established the Ashikaga shogunate, which lasted until 1573. The Ashikaga had their headquarters in the Muromachi district of Kyoto, and the time during which they ruled is also known as the Muromachi Period.
Tokugawa Shogunate (1603–1868).
Tokugawa Ieyasu seized power and established a government at Edo (now known as Tokyo) in 1600. He received the title "sei-i taishōgun" in 1603 after he forged a family tree to show he was of Minamoto descent. The Tokugawa shogunate lasted until 1867, when Tokugawa Yoshinobu resigned as shogun and abdicated his authority to Emperor Meiji.
During the Edo period, effective power rested with the Tokugawa shogun, not the emperor in Kyoto, even though the former ostensibly owed his position to the latter. The shogun controlled foreign policy, the military, and feudal patronage. The role of the emperor was ceremonial, similar to the position of the Japanese monarchy after the Second World War.
A shogun was a military leader equivalent to a general, and at various times in the first millennium, shoguns held temporary power, but it became a symbol of military control over the country. The establishment of the shogunate (or "bakufu") at the end of the twelfth century saw the beginning of samurai control of Japan for 700 years until the Meiji Restoration in the middle of the nineteenth century.
Legacy.
Today the head of the Japanese government is the prime minister; the usage of the term "shogun" has nevertheless continued in colloquialisms. A retired prime minister who still wields considerable power and influence behind the scenes is called a "shadow shogun" (闇将軍, yami shōgun), a sort of modern incarnation of the cloistered rule. Examples of "shadow shoguns" are former prime minister Kakuei Tanaka and the politician Ichirō Ozawa.
The label "shogun" is also commonly used, often ironically with the honorific "-sama", to refer to Kim Jong-il, since several of his laudatory titles include the word "general" (: 장군님; : 將軍). It is also a reference to his being essentially a military dictator, as well as to North Korea's isolation, similar to the policy of sakoku.
Shogunate.
The term "bakufu" originally meant the dwelling and household of a shogun, but in time, it came to be used for the system of government of a feudal military dictatorship, exercised in the name of the shogun; this is the broader meaning conveyed by the term "shogunate".
The shogunate system was originally established under the Kamakura shogunate by Minamoto no Yoritomo. Although theoretically the state, and therefore the emperor, held ownership of all land of Japan. The system had some feudal elements, with lesser territorial lords pledging their allegiance to greater ones. Samurai were rewarded for their loyalty with agricultural surplus, usually rice, or labor services from peasants. In contrast to European feudal knights, samurai were not landowners. The hierarchy that held this system of government together was reinforced by close ties of loyalty between samurai and their subordinates.
Each shogunate was dynamic, not static. Power was constantly shifting and authority was often ambiguous. The study of the ebbs and flows in this complex history continues to occupy the attention of scholars. Each shogunate encountered competition. Sources of competition included the emperor and the court aristocracy, the remnants of the imperial governmental systems, the "shōen" system, the great temples and shrines, the "shugo" and the "jitō", the "kokujin" and early modern daimyo. Each shogunate reflected the necessity of new ways of balancing the changing requirements of central and regional authorities.

</doc>
<doc id="28944" url="http://en.wikipedia.org/wiki?curid=28944" title="Short-term memory">
Short-term memory

Short-term memory (or "primary" or "active memory") is the capacity for holding a small amount of information in mind in an active, readily available state for a short period of time. The duration of short-term memory (when rehearsal or active maintenance is prevented) is believed to be in the order of seconds. A commonly cited capacity is 7 ± 2 elements. In contrast, long-term memory can hold an indefinite amount of information. 
Short-term memory should be distinguished from working memory, which refers to structures and processes used for temporarily storing and manipulating information (see details below).
Existence of a separate store.
The idea of the division of memory into short-term and long-term dates back to the 19th century. A classical model of memory developed in the 1960s assumed that all memories pass from a short-term to a long-term store after a small period of time. This model is referred to as the "modal model" and has been most famously detailed by Shiffrin. The exact mechanisms by which this transfer takes place, whether all or only some memories are retained permanently, and indeed the existence of a genuine distinction between the two stores, remain controversial topics among experts.
One form of evidence, cited in favor of the separate existence of a short-term store comes from anterograde amnesia, the inability to learn new facts and episodes. Patients with this form of amnesia, have intact ability to retain small amounts of information over short time scales (up to 30 seconds) but are dramatically impaired in their ability to form longer-term memories (a famous example is patient HM). This is interpreted as showing that the short-term store is spared from amnesia and other brain diseases.
Other evidence comes from experimental studies showing that some manipulations (e.g., a distractor task, such as repeatedly subtracting a single-digit number from a larger number following learning; cf Brown-Peterson procedure) impair memory for the 3 to 5 most recently learned words of a list (it is presumed, still held in short-term memory), while leaving recall for words from earlier in the list (it is presumed, stored in long-term memory) unaffected; other manipulations (e.g., semantic similarity of the words) affect only memory for earlier list words, but do not affect memory for the last few words in a list. These results show that different factors affect short-term recall (disruption of rehearsal) and long-term recall (semantic similarity). Together, these findings show that long-term memory and short-term memory can vary independently of each other.
Not all researchers agree that short-term and long-term memory are separate systems. Some theorists propose that memory is unitary over all time scales, from milliseconds to years. Support for the unitary memory hypothesis comes from the fact that it has been difficult to demarcate a clear boundary between short-term and long-term memory. For instance, Tarnow shows that the recall probability vs. latency curve is a straight line from 6 to 600 seconds (ten minutes), with the probability of failure to recall only saturating after 600 seconds. If there were really two different memory stores operating in this time frame, one could expect a discontinuity in this curve. Other research has shown that the detailed pattern of recall errors looks remarkably similar for recall of a list immediately after learning (it is presumed, from short-term memory) and recall after 24 hours (necessarily from long-term memory).
Further evidence against the existence of a short-term memory store comes from experiments involving continual distractor tasks. In 1974, Robert Bjork and William B. Whitten presented subjects with word pairs to be remembered; however, before and after each word pair, subjects had to do a simple multiplication task for 12 seconds. After the final word-pair, subjects had to do the multiplication distractor task for 20 seconds. In their results, Bjork and Whitten found that the recency effect (the increased probability of recall of the last items studied) and the primacy effect (the increased probability of recall of the first few items) still remained. These results would seem inconsistent with the idea of short-term memory as the distractor items would have taken the place of some of the word-pairs in the buffer, thereby weakening the associated strength of the items in long-term memory. Bjork and Whitten hypothesized that these results could be attributed to the memory processes at work for long-term memory retrieval versus short-term memory retrieval.
Ovid J.L. Tzeng (1973) also found an instance where the recency effect in free recall did not seem to result from the function of a short-term memory store. Subjects were presented with four study-test periods of 10 word lists, with a continual distractor task (20-second period of counting-backward). At the end of each list, participants had to free recall as many words from the list as possible. After free-recall of the fourth list, participants were asked to free recall items from all four lists. Both the initial free recall and the final free recall showed a recency effect. These results went against the predictions of a short-term memory model, where no recency effect would be expected in either initial or final free recall.
Koppenaal and Glanzer (1990) attempted to explain these phenomena as a result of the subjects’ adaptation to the distractor task, which therefore allowed them to preserve at least some of the functions of the short-term memory store. As evidence, they provided the results of their experiment, in which the long-term recency effect disappeared when the distractor after the last item differed from the distractors that preceded and followed all the other items (e.g., arithmetic distractor task and word reading distractor task).
Thapar and Greene challenged this theory. In one of their experiments, participants were given a different distractor task after every item to be studied. According to Koppenaal’s and Glanzer’s theory, there should be no recency effect as subjects would not have had time to adapt to the distractor; yet such a recency effect remained in place in the experiment.
One proposed explanation of the existence of the recency effect in a continual distractor condition, and the disappearance of it in an end-only distractor task is the influence of contextual and distinctive processes. According to this model, recency is a result of the final items’ processing context being similar to the processing context of the other items and the distinctive position of the final items versus items in the middle of the list. In the end distractor task, the processing context of the final items is no longer similar to the processing context of the other list items. At the same time, retrieval cues for these items are no longer as effective as without the distractor. Therefore, the recency effect recedes or vanishes. However, when distractor tasks are placed before and after each item, the recency effect returns, because all the list items once again have similar processing context.
Biological basis.
Synaptic Theory of Short-term memory.
Various researchers have proposed that stimuli are coded in short-term memory using transmitter depletion. According to this hypothesis, a stimulus activates a spatial pattern of activity across neurons in a brain region. As these neurons fire, the available neurotransmitters in their store are depleted and this pattern of depletion is iconic, representing stimulus information and functions as a memory trace. The memory trace decays over time as a consequence of neurotransmitter reuptake mechanisms that restore neurotransmitters to the levels that existed prior to stimulus presentation.
Relationship with working memory.
The relationship between short-term memory and working memory is described differently by various theories, but it is generally acknowledged that the two concepts are distinct. Working memory is a theoretical framework that refers to structures and processes used for temporarily storing and manipulating information. As such, working memory might also be referred to as "working attention". Working memory and attention together play a major role in the processes of thinking. Short-term memory in general refers, in a theory-neutral manner, to the short-term storage of information, and it does not entail the manipulation or organization of material held in memory. Thus, while there are short-term memory components to working memory models, the concept of short-term memory is distinct from these more hypothetical concepts. Within Baddeley's influential 1986 model of working memory there are two short-term storage mechanisms: the phonological loop and the visuospatial sketchpad. Most of the research referred to here involves the phonological loop, because most of the work done on short-term memory has used verbal material. Since the 1990s, however, there has been a surge in research on visual short-term memory, and also increasing work on spatial short-term memory.
Duration of short-term memory.
The limited duration of short-term memory (~18 seconds without a form of memory rehearsal) quickly suggests that its contents spontaneously decay over time. The decay assumption is part of many theories of short-term memory, the most notable one being Baddeley's model of working memory. The decay assumption is usually paired with the idea of rapid covert rehearsal: In order to overcome the limitation of short-term memory, and retain information for longer, information must be periodically repeated or rehearsed — either by articulating it out loud or by mentally simulating such articulation. In this way, the information will re-enter the short-term store and be retained for a further period.
Several researchers, however, dispute that spontaneous decay plays any significant role in forgetting over the short-term, and the evidence is far from conclusive.
Authors doubting that decay causes forgetting from short-term memory often offer as an alternative some form of interference: When several elements (such as digits, words, or pictures) are held in short-term memory simultaneously, their representations compete with each other for recall, or degrade each other. Thereby, new content gradually pushes out older content, unless the older content is actively protected against interference by rehearsal or by directing attention to it.
Capacity of short-term memory.
Whatever the cause or causes of forgetting over the short-term may be, there is consensus that it severely limits the amount of new information that we can retain over brief periods of time. This limit is referred to as the finite capacity of short-term memory. The capacity of short-term memory is often called memory span, in reference to a common procedure of measuring it. In a memory span test, the experimenter presents lists of items (e.g. digits or words) of increasing length. An individual's span is determined as the longest list length that he or she can recall correctly in the given order on at least half of all trials.
In an early and highly influential article, The Magical Number Seven, Plus or Minus Two, the psychologist George Miller suggested that human short-term memory has a forward memory span of approximately seven items plus or minus two and that that was well known at the time (it seems to go back to the 19th-century researcher Wundt). More recent research has shown that this "magical number seven" is roughly accurate for college students recalling lists of digits, but memory span varies widely with populations tested and with material used. For example, the ability to recall words in order depends on a number of characteristics of these words: fewer words can be recalled when the words have longer spoken duration; this is known as the "word-length effect", or when their speech sounds are similar to each other; this is called the "phonological similarity effect". More words can be recalled when the words are highly familiar or occur frequently in the language. Recall performance is also better when all of the words in a list are taken from a single semantic category (such as games) than when the words are taken from same categories. According to the available evidence, the best overall estimate of short-term memory is about four pieces or "chunks" of information. In free recall it has been shown, to the contrary, that there is no such "quantized" limit, rather it is a function of memory decaying with time.
Rehearsal.
Rehearsal is the process where information is kept in short-term memory by mentally repeating it. When the information is repeated each time, that information is reentered into the short-term memory, thus keeping that information for another 10 to 20 seconds (the average storage time for short-term memory).
Chunking.
Chunking is the process by which one can expand his/her ability to remember things in the short term. Chunking is also a process by which a person organizes material into meaningful groups. Although the average person may retain only about four different units in short-term memory, chunking can greatly increase a person's recall capacity. For example, in recalling a phone number, the person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering phone numbers is far more effective than attempting to remember a string of 10 digits.
Practice and the usage of existing information in long-term memory can lead to additional improvements in one's ability to use chunking. In one testing session, an American cross-country runner was able to recall a string of 79 digits after hearing them only once by chunking them into different running times (e.g., the first four numbers were 1518, a three-mile time.)
Factors affecting short-term memory.
It is very difficult to demonstrate the exact capacity of short-term memory (STM) because it will vary depending on the nature of the material to be recalled. There is currently no way of defining the basic unit of information to be stored in the STM store. It is also possible that STM is not the store described by Atkinson and Shiffrin. In that case, the task of defining the task of STM becomes even more difficult.
However, capacity of STM can be affected by the following:
Influence of long-term memory, Reading aloud, Pronunciation time and Individual differences.
Diseases that cause neurodegeneration, such as Alzheimer's Disease can also be a factor in a person's short-term and eventually long-term memory. Damage to certain sections of the brain due to this disease causes a shrinkage in the cerebral cortex which disables the ability to think and recall memories.
Conditions that may impact short-term memory.
Memory loss is a natural process in aging. One study investigated whether or not there were deficits in short-term memory in older adults. This was a previous study which compiled normative French data for three short-term memory tasks (Verbal, visual and spatial). They found impairments present in participants between the ages of 55 and 85 years of age.
Alzheimer’s disease.
Memory distortion in Alzheimer’s disease is a very common disorder found in older adults. Performance of patients with mild to moderate Alzheimer’s disease was compared with the performance of age matched healthy adults. Researchers concluded the study with findings that showed reduced short-term memory recall for Alzheimer’s patients. Episodic memory and semantic abilities deteriorate early in Alzheimer’s disease. Since the cognitive system includes interconnected and reciprocally influenced neuronal networks, one study hypothesized that stimulation of lexical-semantic abilities may benefit semantically structured episodic memory. They found that with Lexical-Semantic stimulation treatment may improve episodic memory in Alzheimer’s Disease patients. It could also be regarded as a clinical option to counteract the cognitive decline typical of the disease
Aphasia.
Aphasias are also seen in many elder adults. Aphasias are responsible for many sentence comprehension deficits. Many language-impaired patients make several complaints about short-term memory deficits, with several family members confirming that patients have trouble recalling previously known names and events. The opinion is supported by many studies showing that many aphasics also have trouble with visual-memory required tasks.
Schizophrenia.
Core symptoms of Schizophrenia patients have been linked to cognitive deficits. One neglected factor that contributes to those deficits is the comprehension of time. In this study, results confirm that cognitive dysfunctions are a major deficit in patients with schizophrenia. The study provided evidence that patients with schizophrenia process temporal information inefficiently.
Advanced age.
Advanced age is associated with decrements in episodic memory. The associative deficit is in which age differences in recognition memory reflect difficulty in binding components of a memory episode and bound units. A previous study used mixed and blocked test designs to examine deficits in short-term memory of older adults and found there was an associative deficit for older adults. This study along with many other previous studies, continue to build evidence of deficits found in older adults short-term memory.
Even when neurological diseases and disorders are not present, there is a progressive and gradual loss of some intellectual functions that become evident in later years. There are several tests used to examine the psychophysical characteristics of the elderly and of them, a well suitable test would be the functional reach (FR) test, and the mini–mental state examination (MMSE). The FR test is an index of the aptitude to maintain balance in an upright position and the MMSE test is a global index of cognitive abilities. These tests were both used by to evaluate the psychophysical characteristics of older adults. They found a loss of physical performance (FR, related to height) as well as a loss of cognitive abilities (MMSE).
Post Traumatic stress disorder.
Post Traumatic stress disorder (PTSD) is associated with altered processing of emotional material with a strong attentional bias toward trauma-related information and interferes with cognitive processing. Aside from trauma processing specificities, a wide range of cognitive impairments have been related to PTSD state with predominant attention and verbal memory deficits.
Short-term memory and intelligence.
There have been few studies done on the relationship between short-term memory and intelligence in PTSD. However, examined whether people with PTSD had equivalent levels of short-term, non-verbal memory on the Benton Visual Retention Test (BVRT), and whether they had equivalent levels of intelligence on the Raven standard Progressive Matrices (RSPM). They found that people with PTSD had worse short-term, non-verbal memory on the BVRT, despite having comparable levels of intelligence on the RSPM, concluding impairments in memory influence intelligence assessments in the subjects.
References.
Bibliography.
</dl>

</doc>
<doc id="28945" url="http://en.wikipedia.org/wiki?curid=28945" title="State supreme court">
State supreme court

In the United States, a state supreme court (known by other names in some states) is the ultimate judicial tribunal in the court system of a particular state ("i.e.", that state's court of last resort).
Generally, the state supreme court, like most appellate tribunals, is exclusively for hearing appeals of legal issues. It does not make any finding of facts, and thus holds no trials. In the case where the trial court made an egregious error in its finding of facts, the state supreme court will remand to the trial court for a new trial. This responsibility of correcting the errors of inferior courts is the origin of a number of the different names for supreme courts in various state court systems.
The court consists of a panel of judges selected by methods outlined in the state constitution. State supreme courts are completely distinct from any United States federal courts located within the geographical boundaries of a state's territory, or the federal United States Supreme Court (although appeals, on some issues, from judgments of a state's highest court can be sought in the U.S. Supreme Court).
Appellate jurisdiction.
Under American federalism, the interpretation of a state supreme court on a matter of state law is normally final and binding and must be accepted in both state and federal courts.
Federal courts may overrule a state court only when there is a federal question, which is to say, a specific issue (such as consistency with the Federal Constitution) that gives rise to federal jurisdiction. Federal appellate review of state supreme court rulings on such matters may be sought by way of a petition for writ of "certiorari" to the Supreme Court of the United States. As the U.S. Supreme Court recognized in "Erie Railroad Co. v. Tompkins" (1938), no part of the federal Constitution actually grants federal courts or the federal Congress the power to directly dictate the content of state law (as distinguished from creating altogether separate federal law that in a particular situation may override state law). Clause 1 of Section 2 of Article Three of the United States Constitution describes the scope of federal judicial power, but only extended it to "the Laws of the United States" and not the laws of the several or individual states. It is this silence on that latter issue that gave rise to the American distinction between state and federal common law not found in other English-speaking common law federations like Australia and Canada.
One of the informal traditions of the American legal system, derived from the common law, is that all litigants are guaranteed at least one appeal after a final judgment on the merits. However, appeal is merely a "privilege" provided by statute in 47 states and in federal judicial proceedings; the U.S. Supreme Court has repeatedly ruled that there is no federal constitutional "right" to an appeal.
Since a few states lack intermediate appellate courts, the state supreme court may operate under "mandatory review", in which it "must" hear all appeals from the trial courts. This is the case, for example, in Nevada. Such judicial systems are usually very congested.
Most state supreme courts have implemented "discretionary review," like their federal counterpart. Under such a system, intermediate appellate courts are entrusted with deciding the vast majority of appeals. Intermediate appellate courts generally focus on the mundane task of what appellate specialists call "error correction," which means their primary task is to decide whether the record reflects that the trial court correctly applied existing law. 
For certain limited categories of cases, the state supreme court still operates under mandatory review, usually with regard to cases involving the interpretation of the state constitution or capital punishment. But for the vast majority, the state supreme court possesses the discretion to grant "certiorari" (known as "review" in states that discourage the use of Latin). These cases usually pertain to issues which different appellate courts within its jurisdiction have decided differently, or highly controversial cases involving a completely new legal issue never seen in that state. In other words, once the state supreme court is able to offload the tedious burden of error correction to intermediate courts, it can then focus on the long-term task (i.e., a policymaking role) of developing a coherent body of case law for the people of its state. 
Iowa and Oklahoma have a unique procedure for appeals. In those states, "all" appeals are filed with the appropriate Supreme Court (Iowa has a single Supreme Court, while Oklahoma has separate civil and criminal Supreme Courts) which then keeps all cases of first impression for itself to decide. It forwards the remaining cases – which deal with points of law it has already addressed – to the intermediate Court of Appeals.
Notably, the Supreme Court of Virginia operates under discretionary review for nearly all cases, but the intermediate Court of Appeals of Virginia hears appeals as a matter of right only in family and administrative cases. The result is that there is "no" first appeal of right for the vast majority of civil and criminal cases in that state. Appellants are still free to petition for review, of course, but such petitions are subject to severe length constraints (6,125 words or 35 pages in Virginia) and necessarily are more narrowly targeted than a long opening appellate brief to an intermediate appellate court (by way of contrast, an opening brief to a California intermediate appellate court can run up to 14,000 words). In turn, the vast majority of decisions of Virginia circuit courts in civil and criminal cases are thereby insulated from appellate review on the merits. 
New Hampshire and West Virginia formerly also provided only discretionary review for nearly all cases even though they had no intermediate appellate court. Both states gradually recognized that even if this arrangement did not offend the federal Constitution, it was unduly harsh for hapless appellants, and transitioned to mandatory review, respectively, in 2004 and 2010.
Influence of the federal Supreme Court on the state supreme courts.
As noted above, the U.S. Supreme Court may hear appeals from state supreme courts "only" if there is a question of law under the United States Constitution (which includes issues arising from federal treaties, statutes, or regulations), and those appeals are heard at the Court's sole discretion (that is, only if the Court grants a petition for writ of certiorari). 
In theory, state supreme courts are bound by the precedent established by the U.S. Supreme Court as to all issues of federal law, but in practice, the Supreme Court reviews very few decisions from state courts. For example, in 2007 the Court reviewed 244 cases appealed from federal courts and only 22 from state courts. Despite the relatively small number of decisions reviewed, Professors Sara Benesh and Wendy Martinek found that state supreme courts follow precedent more closely than federal courts in the area of search and seizure and appear to follow precedent in confessions as well.
Location.
Traditionally, state supreme courts are headquartered in the capital cities of their respective states, though they may occasionally hold oral argument elsewhere. The six main exceptions are:
As for the court's actual facilities, a state supreme court may be housed in the state capitol, in a nearby state office building shared with other courts or state executive branch agencies, or in a small courthouse reserved for its exclusive use. State supreme courts normally require a courtroom for oral argument, private chambers for all justices, a conference room, offices for law clerks and other support staff, a law library, and a lobby with a window where the court clerk can accept filings and release new decisions in the form of "slip opinions" (that is, in looseleaf format held together only by a staple).
Terminology.
Court of Appeals.
Because state supreme courts generally hear only appeals, some courts have names which directly indicate their function – in the states of New York and Maryland, and in the District of Columbia, the highest court is called the "Court of Appeals". In New York, the "Supreme Court" is the trial court of general unlimited jurisdiction and the intermediate appellate court is called the "Supreme Court—Appellate Division". Maryland's jury trial courts are called "Circuit Courts" (non-jury trials are usually conducted by the "District Courts," whose decisions may be appealed to the Circuit Courts), and the intermediate appellate court is called the "Court of Special Appeals". West Virginia mixes the two; its highest court is called the "Supreme Court of Appeals".
Other states' supreme courts have used the term "Appeals": New Jersey's supreme courts under the 1844 constitution and Delaware's supreme court were both the "Court of Errors and Appeals"; The term "Errors" refers to the now-obsolete writ of error, which was used by state supreme courts to correct certain types of egregious errors committed by lower courts.
Older terminology.
Massachusetts and New Hampshire originally named their highest courts the "Superior Court of Judicature." Currently, Massachusetts uses the names "Supreme Judicial Court" (to distinguish itself from the state legislature, which is called the Massachusetts General Court), while New Hampshire uses the name "Supreme Court". Additionally the highest court in Maine is named the "Supreme Judicial Court". This similar terminology is probably a holdover from the time when Maine was part of Massachusetts. In Connecticut, Delaware, New Jersey, and New York, the highest courts formerly used variations of the term "Court of Errors," which indicated that the court's primary purpose was to correct the errors of lower courts.
Dual supreme courts.
Oklahoma and Texas have two separate supreme courts: one for criminal appeals and one for civil cases. In both states, the first is formally called the Court of Criminal Appeals, and the second is called the Supreme Court.
Statistics.
7 years in Maine and 14 in New York.
Texas and Oklahoma have dual supreme courts. In Texas, both have nine justices. In Oklahoma the Supreme Court has nine justices and the Court of criminal appeals has five (assimilated to nine in the above table).
Selection.
Judges are either appointed, selected through a merit process (with an election thereafter in some cases), or elected. The elections may be through partisan or non-partisan elections. A non-partisan election does not mean that the judges run and are selected with no regard to political beliefs. In many cases "non-partisan election" merely means the prospective judges' parties are not printed on the ballot. Political contributions to these campaigns may be allowed, including from trade associations such as the U.S. Chamber of Commerce.

</doc>
<doc id="28946" url="http://en.wikipedia.org/wiki?curid=28946" title="Stability">
Stability

Stability may refer to:

</doc>
<doc id="28951" url="http://en.wikipedia.org/wiki?curid=28951" title="Spyware">
Spyware

Spyware is software that aims to gather information about a person or organization without their knowledge and that may send such information to another entity without the consumer's consent, or that asserts control over a computer without the consumer's knowledge.
"Spyware" is mostly classified into four types: system monitors, trojans, adware, and tracking cookies. Spyware is mostly used for the purposes of tracking and storing Internet users'
movements on the Web and serving up pop-up ads to Internet users.
Whenever spyware is used for malicious purposes, its presence is typically hidden from the user and can be difficult to detect. Some spyware, such as keyloggers, may be installed by the owner of a shared, corporate, or public computer intentionally in order to monitor users.
While the term "spyware" suggests software that monitors a user's computing, the functions of spyware can extend beyond simple monitoring. Spyware can collect almost any type of data, including personal information like Internet surfing habits, user logins, and bank or credit account information. Spyware can also interfere with user control of a computer by installing additional software or redirecting Web browsers. Some spyware can change computer settings, which can result in slow Internet connection speeds, un-authorized changes in browser settings, or changes to software settings.
Sometimes, spyware is included along with genuine software, and may come from a malicious website. In response to the emergence of spyware, a small industry has sprung up dealing in anti-spyware software. Running anti-spyware software has become a widely recognized element of computer security practices, especially for computers running Microsoft Windows. A number of jurisdictions have passed anti-spyware laws, which usually target any software that is surreptitiously installed to control a user's computer.
In German-speaking countries, spyware used or made by the government is sometimes called "govware". Govware is typically a trojan horse software used to intercept communications from the target computer. Some countries like Switzerland and Germany have a legal framework governing the use of such software. In the US, the term policeware has been used for similar purposes. 
Routes of infection.
Spyware does not necessarily spread in the same way as a virus or worm because infected systems generally do not attempt to transmit or copy the software to other computers. Instead, spyware installs itself on a system by deceiving the user or by exploiting software vulnerabilities.
Most spyware is installed without users' knowledge, or by using deceptive tactics. Spyware may try to deceive users by bundling itself with desirable software. Other common tactics are using a Trojan horse. Some spyware authors infect a system through security holes in the Web browser or in other software. When the user navigates to a Web page controlled by the spyware author, the page contains code which attacks the browser and forces the download and installation of spyware.
The installation of spyware frequently involves Internet Explorer. Its popularity and history of security issues have made it a frequent target. Its deep integration with the Windows environment make it susceptible to attack into the Windows operating system. Internet Explorer also serves as a point of attachment for spyware in the form of Browser Helper Objects, which modify the browser's behavior to add toolbars or to redirect traffic.
Effects and behaviors.
A spyware program is rarely alone on a computer: an affected machine usually has multiple infections. Users frequently notice unwanted behavior and degradation of system performance. A spyware infestation can create significant unwanted CPU activity, disk usage, and network traffic. Stability issues, such as applications freezing, failure to boot, and system-wide crashes are also common. Spyware, which interferes with networking software, commonly causes difficulty connecting to the Internet.
In some infections, the spyware is not even evident. Users assume in those situations that the performance issues relate to faulty hardware, Windows installation problems, or another infection. Some owners of badly infected systems resort to contacting technical support experts, or even buying a new computer because the existing system "has become too slow". Badly infected systems may require a clean reinstallation of all their software in order to return to full functionality.
Moreover, some types of spyware disable software firewalls and anti-virus software, and/or reduce browser security settings, which further open the system to further opportunistic infections. Some spyware disables or even removes competing spyware programs, on the grounds that more spyware-related annoyances make it even more likely that users will take action to remove the programs.
Keyloggers are sometimes part of malware packages downloaded onto computers without the owners' knowledge. Some keyloggers software is freely available on the internet while others are commercial or private applications. Most keyloggers allow not only keyboard keystrokes to be captured but also are often capable of collecting screen captures from the computer.
A typical Windows user has administrative privileges, mostly for convenience. Because of this, any program the user runs has unrestricted access to the system. As with other operating systems, Windows users are able to follow the principle of least privilege and use non-administrator accounts. Alternatively, they can also reduce the privileges of specific vulnerable Internet-facing processes such as Internet Explorer.
Since Windows Vista, by default, a computer administrator runs everything under limited user privileges. When a program requires administrative privileges, a User Account Control pop-up will prompt the user to allow or deny the action. This improves on the design used by previous versions of Windows.
Remedies and prevention.
As the spyware threat has worsened, a number of techniques have emerged to counteract it. These include programs designed to remove or block spyware, as well as various user practices which reduce the chance of getting spyware on a system.
Nonetheless, spyware remains a costly problem. When a large number of pieces of spyware have infected a Windows computer, the only remedy may involve backing up user data, and fully reinstalling the operating system. For instance, some spyware cannot be completely removed by Symantec, Microsoft, PC Tools.
Anti-spyware programs.
Many programmers and some commercial firms have released products dedicated to remove or block spyware. Programs such as PC Tools' Spyware Doctor, Lavasoft's "Ad-Aware SE" and Patrick Kolla's "Spybot - Search & Destroy" rapidly gained popularity as tools to remove, and in some cases intercept, spyware programs. On December 16, 2004, Microsoft acquired the "GIANT AntiSpyware" software, rebranding it as "Windows AntiSpyware beta" and releasing it as a free download for Genuine Windows XP and Windows 2003 users. (In 2006 it was renamed Windows Defender).
Major anti-virus firms such as Symantec, PC Tools, McAfee and Sophos have also added anti-spyware features to their existing anti-virus products. Early on, anti-virus firms expressed reluctance to add anti-spyware functions, citing lawsuits brought by spyware authors against the authors of web sites and programs which described their products as "spyware". However, recent versions of these major firms' home and business anti-virus products do include anti-spyware functions, albeit treated differently from viruses. Symantec Anti-Virus, for instance, categorizes spyware programs as "extended threats" and now offers real-time protection against these threats.
How anti-spyware software works.
Anti-spyware programs can combat spyware in two ways:
Such programs inspect the contents of the Windows registry, operating system files, and installed programs, and remove files and entries which match a list of known spyware. Real-time protection from spyware works identically to real-time anti-virus protection: the software scans disk files at download time, and blocks the activity of components known to represent spyware.
In some cases, it may also intercept attempts to install start-up items or to modify browser settings. Earlier versions of anti-spyware programs focused chiefly on detection and removal. Javacool Software's SpywareBlaster, one of the first to offer real-time protection, blocked the installation of ActiveX-based spyware.
Like most anti-virus software, many anti-spyware/adware tools require a frequently updated database of threats. As new spyware programs are released, anti-spyware developers discover and evaluate them, adding to the list of known spyware, which allows the software to detect and remove new spyware. As a result, anti-spyware software is of limited usefulness without regular updates. Updates may be installed automatically or manually.
A popular generic spyware removal tool used by those that requires a certain degree of expertise is HijackThis, which scans certain areas of the Windows OS where spyware often resides and presents a list with items to delete manually. As most of the items are legitimate windows files/registry entries it is advised for those who are less knowledgeable on this subject to post a HijackThis log on the numerous antispyware sites and let the experts decide what to delete.
If a spyware program is not blocked and manages to get itself installed, it may resist attempts to terminate or uninstall it. Some programs work in pairs: when an anti-spyware scanner (or the user) terminates one running process, the other one respawns the killed program. Likewise, some spyware will detect attempts to remove registry keys and immediately add them again. Usually, booting the infected computer in safe mode allows an anti-spyware program a better chance of removing persistent spyware. Killing the process tree may also work.
Security practices.
To detect spyware, computer users have found several practices useful in addition to installing anti-spyware programs. Many users have installed a web browser other than Internet Explorer, such as Mozilla Firefox or Google Chrome. Though no browser is completely safe, Internet Explorer is at a greater risk for spyware infection due to its large user base as well as vulnerabilities such as ActiveX.
Some ISPs—particularly colleges and universities—have taken a different approach to blocking spyware: they use their network firewalls and web proxies to block access to Web sites known to install spyware. On March 31, 2005, Cornell University's Information Technology department released a report detailing the behavior of one particular piece of proxy-based spyware, "Marketscore", and the steps the university took to intercept it. Many other educational institutions have taken similar steps.
Individual users can also install firewalls from a variety of companies. These monitor the flow of information going to and from a networked computer and provide protection against spyware and malware. Some users install a large hosts file which prevents the user's computer from connecting to known spyware-related web addresses. Spyware may get installed via certain shareware programs offered for download. Downloading programs only from reputable sources can provide some protection from this source of attack.
Comparison of spyware, adware, and viruses.
Spyware, adware and trackers.
The term "adware" frequently refers to software that displays advertisements. An example is the Eudora email client display advertisements as an alternative to shareware registration fees. However, these are not considered spyware.
Other spyware behavior, such as reporting websites the user visits, occurs in the background. The data is used for "targeted" advertisement impressions. The prevalence of spyware has cast suspicion on other programs that track Web browsing, even for statistical or research purposes. Many of these adware-distributing companies are backed by millions of dollars of adware-generating revenues. Adware and spyware are similar to viruses in that they can be considered malicious in nature.
Spyware, viruses and worms.
Unlike viruses and worms, spyware does not usually self-replicate. Like many recent viruses, however, spyware—by design—exploits infected computers for commercial gain. Typical tactics include delivery of unsolicited pop-up advertisements, theft of personal information (including financial information such as credit card numbers), monitoring of Web-browsing activity for marketing purposes, and routing of HTTP requests to advertising sites.
"Stealware" and affiliate fraud.
A few spyware vendors, notably 180 Solutions, have written what the "New York Times" has dubbed "stealware", and what spyware researcher Ben Edelman terms "affiliate fraud", a form of click fraud. Stealware diverts the payment of affiliate marketing revenues from the legitimate affiliate to the spyware vendor.
Spyware which attacks affiliate networks places the spyware operator's affiliate tag on the user's activity — replacing any other tag, if there is one. The spyware operator is the only party that gains from this. The user has their choices thwarted, a legitimate affiliate loses revenue, networks' reputations are injured, and vendors are harmed by having to pay out affiliate revenues to an "affiliate" who is not party to a contract. Affiliate fraud is a violation of the terms of service of most affiliate marketing networks. As a result, spyware operators such as 180 Solutions have been terminated from affiliate networks including LinkShare and ShareSale. Mobile devices can also be vulnerable to chargeware, which manipulates users into illegitimate mobile charges.
Identity theft and fraud.
In one case, spyware has been closely associated with identity theft. In August 2005, researchers from security software firm Sunbelt Software suspected the creators of the common CoolWebSearch spyware had used it to transmit "chat sessions, user names, passwords, bank information, etc."; however it turned out that "it actually (was) its own sophisticated criminal little trojan that's independent of CWS." This case is currently under investigation by the FBI.
The Federal Trade Commission estimates that 27.3 million Americans have been victims of identity theft, and that financial losses from identity theft totaled nearly $48 billion for businesses and financial institutions and at least $5 billion in out-of-pocket expenses for individuals.
Digital rights management.
Some copy-protection technologies have borrowed from spyware. In 2005, Sony BMG Music Entertainment was found to be using rootkits in its XCP digital rights management technology Like spyware, not only was it difficult to detect and uninstall, it was so poorly written that most efforts to remove it could have rendered computers unable to function.
Texas Attorney General Greg Abbott filed suit, and three separate class-action suits were filed. Sony BMG later provided a workaround on its website to help users remove it.
Beginning on 25 April 2006, Microsoft's Windows Genuine Advantage Notifications application was installed on most Windows PCs as a "critical security update". While the main purpose of this deliberately uninstallable application is to ensure the copy of Windows on the machine was lawfully purchased and installed, it also installs software that has been accused of "phoning home" on a daily basis, like spyware. It can be removed with the RemoveWGA tool.
Personal relationships.
Spyware has been used to monitor electronic activities of partners in intimate relationships. At least one software package, Loverspy, was specifically marketed for this purpose. Depending on local laws regarding communal/marital property, observing a partner's online activity without their consent may be illegal; the author of Loverspy and several users of the product were indicted in California in 2005 on charges of wiretapping and various computer crimes.
Browser cookies.
Anti-spyware programs often report Web advertisers' HTTP cookies, the small text files that track browsing activity, as spyware. While they are not always inherently malicious, many users object to third parties using space on their personal computers for their business purposes, and many anti-spyware programs offer to remove them.
Examples.
These common spyware programs illustrate the diversity of behaviors found in these attacks. Note that as with computer viruses, researchers give names to spyware programs which may not be used by their creators. Programs may be grouped into "families" based not on shared program code, but on common behaviors, or by "following the money" of apparent financial or business connections. For instance, a number of the spyware programs distributed by Claria are collectively known as "Gator". Likewise, programs that are frequently installed together may be described as parts of the same spyware package, even if they function separately.
History and development.
The first recorded use of the term spyware occurred on 16 October 1995 in a Usenet post that poked fun at Microsoft's business model. "Spyware" at first denoted "software" meant for espionage purposes. However, in early 2000 the founder of Zone Labs, Gregor Freund, used the term in a for the ZoneAlarm Personal Firewall. Later in 2000, a parent using ZoneAlarm was alerted to the fact that "Reader Rabbit," educational software marketed to children by the Mattel toy company, was surreptitiously sending data back to Mattel. Since then, "spyware" has taken on its present sense.
According to a 2005 study by AOL and the National Cyber-Security Alliance, 61 percent of surveyed users' computers were infected with form of spyware. 92 percent of surveyed users with spyware reported that they did not know of its presence, and 91 percent reported that they had not given permission for the installation of the spyware.
s of 2006[ [update]], spyware has become one of the preeminent security threats to computer systems running Microsoft Windows operating systems. Computers on which Internet Explorer (IE) is the primary browser are particularly vulnerable to such attacks, not only because IE is the most widely used, but because its tight integration with Windows allows spyware access to crucial parts of the operating system.
Before Internet Explorer 6 SP2 was released as part of Windows XP Service Pack 2, the browser would automatically display an installation window for any ActiveX component that a website wanted to install. The combination of user ignorance about these changes, and the assumption by Internet Explorer that all ActiveX components are benign, helped to spread spyware significantly. Many spyware components would also make use of exploits in JavaScript, Internet Explorer and Windows to install without user knowledge or permission.
The Windows Registry contains multiple sections where modification of key values allows software to be executed automatically when the operating system boots. Spyware can exploit this design to circumvent attempts at removal. The spyware typically will link itself from each location in the registry that allows execution. Once running, the spyware will periodically check if any of these links are removed. If so, they will be automatically restored. This ensures that the spyware will execute when the operating system is booted, even if some (or most) of the registry links are removed.
Rogue anti-spyware programs.
Malicious programmers have released a large number of rogue (fake) anti-spyware programs, and widely distributed Web banner ads can warn users that their computers have been infected with spyware, directing them to purchase programs which do not actually remove spyware—or else, may add more spyware of their own.
The recent[ [update]] proliferation of fake or spoofed antivirus products that bill themselves as antispyware can be troublesome. Users may receive popups prompting them to install them to protect their computer, when it will in fact add spyware. This software is called rogue software. It is recommended that users do not install any freeware claiming to be anti-spyware unless it is verified to be legitimate. Some known offenders include:
Fake antivirus products constitute 15 percent of all malware.
On January 26, 2006, Microsoft and the Washington state attorney general filed suit against Secure Computer for its Spyware Cleaner product.
Legal issues.
Criminal law.
Unauthorized access to a computer is illegal under computer crime laws, such as the U.S. Computer Fraud and Abuse Act, the U.K.'s Computer Misuse Act, and similar laws in other countries. Since owners of computers infected with spyware generally claim that they never authorized the installation, a "prima facie" reading would suggest that the promulgation of spyware would count as a criminal act. Law enforcement has often pursued the authors of other malware, particularly viruses. However, few spyware developers have been prosecuted, and many operate openly as strictly legitimate businesses, though some have faced lawsuits.
Spyware producers argue that, contrary to the users' claims, users do in fact give consent to installations. Spyware that comes bundled with shareware applications may be described in the legalese text of an end-user license agreement (EULA). Many users habitually ignore these purported contracts, but spyware companies such as Claria say these demonstrate that users have consented.
Despite the ubiquity of EULAs agreements, under which a single click can be taken as consent to the entire text, relatively little caselaw has resulted from their use. It has been established in most common law jurisdictions that this type of agreement can be a binding contract "in certain circumstances." This does not, however, mean that every such agreement is a contract, or that every term in one is enforceable.
Some jurisdictions, including the U.S. states of Iowa and Washington, have passed laws criminalizing some forms of spyware. Such laws make it illegal for anyone other than the owner or operator of a computer to install software that alters Web-browser settings, monitors keystrokes, or disables computer-security software.
In the United States, lawmakers introduced a bill in 2005 entitled the Internet Spyware Prevention Act, which would imprison creators of spyware.
Administrative sanctions.
US FTC actions.
The US Federal Trade Commission has sued Internet marketing organizations under the "unfairness doctrine" to make them stop infecting consumers’ PCs with spyware. In one case, that against Seismic Entertainment Productions, the FTC accused the defendants of developing a program that seized control of PCs nationwide, infected them with spyware and other malicious software, bombarded them with a barrage of pop-up advertising for Seismic’s clients, exposed the PCs to security risks, and caused them to malfunction. Seismic then offered to sell the victims an “antispyware” program to fix the computers, and stop the popups and other problems that Seismic had caused. On November 21, 2006, a settlement was entered in federal court under which a $1.75 million judgment was imposed in one case and $1.86 million in another, but the defendants were insolvent
In a second case, brought against CyberSpy Software LLC, the FTC charged that CyberSpy marketed and sold "RemoteSpy" keylogger spyware to clients who would then secretly monitor unsuspecting consumers’ computers. According to the FTC, Cyberspy touted RemoteSpy as a “100% undetectable” way to “Spy on Anyone. From Anywhere.” The FTC has obtained a temporary order prohibiting the defendants from selling the software and disconnecting from the Internet any of their servers that collect, store, or provide access to information that this software has gathered. The case is still in its preliminary stages. A complaint filed by the Electronic Privacy Information Center (EPIC) brought the RemoteSpy software to the FTC’s attention.
Netherlands OPTA.
An administrative fine, the first of its kind in Europe, has been issued by the Independent Authority of Posts and Telecommunications (OPTA) from the Netherlands. It applied fines in total value of Euro 1,000,000 for infecting 22 million computers. The spyware concerned is called DollarRevenue. The law articles that have been violated are art. 4.1 of the Decision on universal service providers and on the interests of end users; the fines have been issued based on art. 15.4 taken together with art. 15.10 of the Dutch telecommunications law.
Civil law.
Former New York State Attorney General and former Governor of New York Eliot Spitzer has pursued spyware companies for fraudulent installation of software. In a suit brought in 2005 by Spitzer, the California firm Intermix Media, Inc. ended up settling, by agreeing to pay US$7.5 million and to stop distributing spyware.
The hijacking of Web advertisements has also led to litigation. In June 2002, a number of large Web publishers sued Claria for replacing advertisements, but settled out of court.
Courts have not yet had to decide whether advertisers can be held liable for spyware that displays their ads. In many cases, the companies whose advertisements appear in spyware pop-ups do not directly do business with the spyware firm. Rather, they have contracted with an advertising agency, which in turn contracts with an online subcontractor who gets paid by the number of "impressions" or appearances of the advertisement. Some major firms such as Dell Computer and Mercedes-Benz have sacked advertising agencies that have run their ads in spyware.
Libel suits by spyware developers.
Litigation has gone both ways. Since "spyware" has become a common pejorative, some makers have filed libel and defamation actions when their products have been so described. In 2003, Gator (now known as Claria) filed suit against the website PC Pitstop for describing its program as "spyware". PC Pitstop settled, agreeing not to use the word "spyware", but continues to describe harm caused by the Gator/Claria software. As a result, other anti-spyware and anti-virus companies have also used other terms such as "potentially unwanted programs" or greyware to denote these products.
WebcamGate.
In the 2010 WebcamGate case, plaintiffs charged two suburban Philadelphia high schools secretly spied on students by surreptitiously and remotely activating webcams embedded in school-issued laptops the students were using at home, and therefore infringed on their privacy rights. The school loaded each student's computer with LANrev's remote activation tracking software. This included the now-discontinued "TheftTrack". While TheftTrack was not enabled by default on the software, the program allowed the school district to elect to activate it, and to choose which of the TheftTrack surveillance options the school wanted to enable.
TheftTrack allowed school district employees to secretly remotely activate a tiny webcam embedded in the student's laptop, above the laptop's screen. That allowed school officials to secretly take photos through the webcam, of whatever was in front of it and in its line of sight, and send the photos to the school's server. The LANrev software disabled the webcams for all other uses ("e.g.", students were unable to use Photo Booth or video chat), so most students mistakenly believed their webcams did not work at all. In addition to webcam surveillance, TheftTrack allowed school officials to take screenshots, and send them to the school's server. In addition, LANrev allowed school officials to take snapshots of instant messages, web browsing, music playlists, and written compositions. The schools admitted to secretly snapping over 66,000 webshots and screenshots, including webcam shots of students in their bedrooms.

</doc>
<doc id="28952" url="http://en.wikipedia.org/wiki?curid=28952" title="William Jones (philologist)">
William Jones (philologist)

Sir William Jones (28 September 1746 – 27 April 1794) was an Anglo-Welsh philologist and scholar of ancient India, particularly known for his proposition of the existence of a relationship among Indo-European languages. He, along with Henry Thomas Colebrooke and Nathaniel Halhed, founded the Asiatic Society of Bengal, and started a journal called "Asiatick Researches".
Biography.
William Jones was born in London at Beaufort Buildings, Westminster; his father (also named William Jones) was a mathematician from Anglesey in Wales, noted for devising the use of the symbol π. The young William Jones was a linguistic prodigy, learning Greek, Latin, Persian, Arabic, Hebrew and the basics of Chinese writing at an early age. By the end of his life he knew thirteen languages thoroughly and another twenty-eight reasonably well, making him a hyperpolyglot.
Jones' father died when he was aged three. His mother Mary Nix Jones raised him. Jones attended Harrow in September 1753 and then went on to Oxford University. He graduated from University College, Oxford in 1768 and became M.A. in 1773. Too poor, even with his award, to pay the fees, he gained a job tutoring the seven-year-old Lord Althorp, son of Earl Spencer. He embarked on a career as a tutor and translator for the next six years. During this time he published "Histoire de Nader Chah" (1770), a French translation of a work originally written in Persian by Mirza Mehdi Khan Astarabadi. This was done at the request of King Christian VII of Denmark who had visited Jones – who by the age of 24 had already acquired a reputation as an orientalist. This would be the first of numerous works on Persia, Turkey, and the Middle East in general.
In 1770, he joined the Middle Temple and studied law for three years, which would eventually lead him to his life-work in India. He was elected a Fellow of the Royal Society on 30 April 1772. After a spell as a circuit judge in Wales, and a fruitless attempt to resolve the issues of the American colonies in concert with Benjamin Franklin in Paris, he was appointed puisne judge to the Supreme Court of Judicature at Fort William in Calcutta, Bengal on 4 March 1783, and on 20 March he was knighted. In April 1783 he married Anna Maria Shipley, the eldest daughter of Dr. Jonathan Shipley, Bishop of Llandaff and Bishop of St Asaph. Anna Maria used her artistic skills to help Jones document life in India. On 25 September 1783 he arrived in Calcutta. 
Jones was a radical political thinker, a friend of American independence. His work "The principles of government; in a dialogue between a scholar and a peasant" [London?]: printed and distributed gratis by the Society for Constitutional Information, 1783 was the subject of a trial for seditious libel after it was reprinted by his brother-in-law William Shipley.
In the Subcontinent he was entranced by Indian culture, an as-yet untouched field in European scholarship, and on 15 January 1784 he founded the Asiatic Society in Calcutta. Over the next ten years he would produce a flood of works on India, launching the modern study of the subcontinent in virtually every social science. He also wrote on the local laws, music, literature, botany, and geography, and made the first English translations of several important works of Indian literature. He died in Calcutta on 27 April 1794 at the age of 47 and is buried in South Park Street Cemetery.
Sir William Jones sometimes also went by the nom de plume Youns Uksfardi (یونس اوکسفردی). This pen name can be seen on the inner front cover of his "Persian Grammar" published in 1771 (and in subsequent editions as well). The second half of the pen name, Uksfardi, Persian rendition of "from Oxford", can be directly attributed to the deep attachment William Jones had for the University of Oxford. The first name Youns is a rendition of Jones.
Scholarly contributions.
Jones is also known today for making and propagating the observation about languages . In his "Third Anniversary Discourse " to the Asiatic Society (1786) he suggested that Sanskrit, Greek and Latin languages had a common root, and that indeed they may all be further related, in turn, to Gothic and the Celtic languages, as well as to Persian.
Although his name is closely associated with this observation, he was not the first to make it. In the 16th century, European visitors to India became aware of similarities between Indian and European languages and as early as 1653 Van Boxhorn had published a proposal for a proto-language ("Scythian") for Germanic, Romance, Greek, Baltic, Slavic, Celtic and Iranian. Finally, in a memoir sent to the French Academy of Sciences in 1767 Gaston-Laurent Coeurdoux, a French Jesuit who spent all his life in India, had specifically demonstrated the existing analogy between Sanskrit and European languages. In 1786 Jones postulated a proto-language uniting Sanskrit, Iranian, Greek, Latin, Germanic and Celtic, but in many ways his work was less accurate than his predecessors', as he erroneously included Egyptian, Japanese and Chinese in the Indo-European languages, while omitting Hindi.
Nevertheless, Jones' third annual discourse before the Asiatic Society on the history and culture of the Hindus (delivered on 2 February 1786 and published in 1788) with the famed "philologer" passage is often cited as the beginning of comparative linguistics and Indo-European studies.
 The "Sanscrit" language, whatever be its antiquity, is of a wonderful structure; more perfect than the "Greek", more copious than the "Latin", and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and the forms of grammar, than could possibly have been produced by accident; so strong indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists; there is a similar reason, though not quite so forcible, for supposing that both the "Gothic" and the "Celtic", though blended with a very different idiom, had the same origin with the "Sanscrit"; and the old "Persian" might be added to the same family.
This common source came to be known as Proto-Indo-European.
Jones was the first to propose a racial division of India involving an Aryan invasion but at that time there was insufficient evidence to support it. It was an idea later taken up by British administrators such as Herbert Hope Risley but remains disputed today.
Latin chess poem.
In 1763, at the age of 17, Jones wrote the poem "Caissa" in Latin hexameters, based on a 658-line poem called "Scacchia, Ludus" published in 1527 by Marco Girolamo Vida, giving a mythical origin of chess that has become well known in the chess world. He also published an English-language version of the poem.
In the poem the nymph Caissa initially repels the advances of Mars, the god of war. Spurned, Mars seeks the aid of the god of sport, who creates the game of chess as a gift for Mars to win Caissa's favour. Mars wins her over with the game.
Caissa has been since been characterised as the "goddess" of chess, her name being used in several contexts in modern chess playing.
Schopenhauer's citation.
Arthur Schopenhauer referred to one of Sir William Jones's publications in §1 of "The World as Will and Representation" (1819). Schopenhauer was trying to support the doctrine that "everything that exists for knowledge, and hence the whole of this world, is only object in relation to the subject, perception of the perceiver, in a word, representation." He quoted Jones's original English:
... how early this basic truth was recognized by the sages of India, since it appears as the fundamental tenet of the Vedânta philosophy ascribed to Vyasa, is proved by Sir William Jones in the last of his essays: "On the Philosophy of the Asiatics" ("Asiatic Researches", vol. IV, p. 164): "The fundamental tenet of the Vedânta school consisted not in denying the existence of matter, that is solidity, impenetrability, and extended figure (to deny which would be lunacy), but in correcting the popular notion of it, and in contending that it has no essence independent of mental perception; that existence and perceptibility are convertible terms."
Schopenhauer used Jones's authority to relate the basic principle of his philosophy to what was, according to Jones, the most important underlying proposition of Vedânta. He made more passing reference to Sir William Jones's writings elsewhere in his works.
In popular culture.
William Jones appears as a character in Indrajit Hazra's "The Bioscope Man".

</doc>
<doc id="28953" url="http://en.wikipedia.org/wiki?curid=28953" title="Stephen, King of England">
Stephen, King of England

Stephen (c. 1092/6 – 25 October 1154), often referred to as Stephen of Blois, was a grandson of William the Conqueror. He was King of England from 1135 to his death, and also the Count of Boulogne in right of his wife. Stephen's reign was marked by the Anarchy, a civil war with his cousin and rival, the Empress Matilda. He was succeeded by Matilda's son, Henry II, the first of the Angevin kings.
Stephen was born in the County of Blois in middle France; his father, Count Stephen-Henry, died while Stephen was still young, and he was brought up by his mother, Adela. Placed into the court of his uncle, Henry I, Stephen rose in prominence and was granted extensive lands. Stephen married Matilda of Boulogne, inheriting additional estates in Kent and Boulogne that made the couple one of the wealthiest in England. Stephen narrowly escaped drowning with Henry I's son, William Adelin, in the sinking of the "White Ship" in 1120; William's death left the succession of the English throne open to challenge. When Henry I died in 1135, Stephen quickly crossed the English Channel and with the help of his brother Henry of Blois, a powerful ecclesiastic, took the throne, arguing that the preservation of order across the kingdom took priority over his earlier oaths to support the claim of Henry I's daughter, the Empress Matilda.
The early years of Stephen's reign were largely successful, despite a series of attacks on his possessions in England and Normandy by David I of Scotland, Welsh rebels, and the Empress Matilda's husband, Geoffrey of Anjou. In 1138 the Empress's half-brother Robert of Gloucester rebelled against Stephen, threatening civil war. Together with his close advisor, Waleran de Beaumont, Stephen took firm steps to defend his rule, including arresting a powerful family of bishops. When the Empress and Robert invaded in 1139, however, Stephen was unable to crush the revolt rapidly, and it took hold in the south-west of England. Captured at the battle of Lincoln in 1141, Stephen was abandoned by many of his followers and lost control of Normandy. Stephen was freed only after his wife and William of Ypres, one of his military commanders, captured Robert at the Rout of Winchester, but the war dragged on for many years with neither side able to win an advantage.
Stephen became increasingly concerned with ensuring that his son Eustace would inherit his throne. The King tried to convince the Church to agree to crown Eustace to reinforce his claim; Pope Eugene III refused, and Stephen found himself in a sequence of increasingly bitter arguments with his senior clergy. In 1153 the Empress's son, Henry FitzEmpress, invaded England and built an alliance of powerful regional barons to support his claim for the throne. The two armies met at Wallingford, but neither side's barons were keen to fight another pitched battle. Stephen began to examine a negotiated peace, a process hastened by the sudden death of Eustace. Later in the year Stephen and Henry agreed to the Treaty of Winchester, in which Stephen recognised Henry as his heir in exchange for peace, passing over William, Stephen's second son. Stephen died the following year. Modern historians have extensively debated the extent to which Stephen's personality, external events, or the weaknesses in the Norman state contributed to this prolonged period of civil war.
Early life (1096–1135).
Childhood.
Stephen was born in Blois in France, in either 1092 or 1096. His father was Stephen-Henry, Count of Blois and Chartres, an important French nobleman, and an active crusader, who played only a brief part in Stephen's early life. During the First Crusade Stephen-Henry had acquired a reputation for cowardice, and he returned to the Levant again in 1101 to rebuild his reputation; there he was killed at the battle of Ramlah. Stephen's mother, Adela, was the daughter of William the Conqueror and Matilda of Flanders, famous amongst her contemporaries for her piety, wealth and political talent. She had a strong matriarchal influence on Stephen during his early years.
France in the 12th century was a loose collection of counties and smaller polities, under the minimal control of the king of France. The king's power was linked to his control of the rich province of Île-de-France, just to the east of Stephen's home county of Blois. In the west lay the three counties of Maine, Anjou and Touraine, and to the north of Blois was the Duchy of Normandy, from which William the Conqueror had conquered England in 1066. William's children were still fighting over the collective Anglo-Norman inheritance. The rulers across this region spoke a similar language, albeit with regional dialects, followed the same religion, and were closely interrelated; they were also highly competitive and frequently in conflict with one another for valuable territory and the castles that controlled them.
Stephen had at least four brothers and one sister, along with two probable half-sisters. Stephen's eldest brother was William, who under normal circumstances would have ruled the county. William was probably intellectually disabled, and Adela instead had the title passed over him to her second son, Theobald, who went on later to acquire the county of Champagne as well as Blois and Chartres. Stephen's remaining older brother, Odo, died young, probably in his early teens. His younger brother, Henry of Blois, was probably born four years after him. The brothers formed a close-knit family group, and Adela encouraged Stephen to take up the role of a feudal knight, whilst steering Henry towards a career in the church, possibly so that their personal career interests would not overlap. Unusually, Stephen was raised in his mother's household rather than being sent to a close relative; he was taught Latin and riding, and was educated in recent history and Biblical stories by his tutor, William the Norman.
Relationship with Henry I.
Stephen's early life was heavily influenced by his relationship with his uncle Henry I. Henry seized power in England following the death of his elder brother William Rufus. In 1106 he invaded and captured the Duchy of Normandy, controlled by his eldest brother, Robert Curthose, defeating Robert's army at the battle of Tinchebray. Henry then found himself in conflict with Louis VI of France, who took the opportunity to declare Robert's son William Clito the Duke of Normandy. Henry responded by forming a network of alliances with the western counties of France against Louis, resulting in a regional conflict that would last throughout Stephen's early life. Adela and Theobald allied themselves with Henry, and Stephen's mother decided to place him in Henry's court. Henry fought his next military campaign in Normandy, from 1111 onwards, where rebels led by Robert of Bellême were opposing his rule. Stephen was probably with Henry during the military campaign of 1112, when he was knighted by the King, and was definitely present at court during the King's visit to the Abbey of Saint-Evroul in 1113. Stephen probably first visited England in either 1113 or 1115, almost certainly as part of Henry's court.
Henry became a powerful patron of Stephen's; Henry probably chose to support him because Stephen was part of his extended family and a regional ally, yet not sufficiently wealthy or powerful in his own right to represent a threat to either the King or his heir, William Adelin. As a third surviving son, even of an influential regional family, Stephen still needed the support of a powerful patron such as the King to progress in life. With Henry's support, Stephen rapidly began to accumulate lands and possessions. Following the battle of Tinchebray in 1106, Henry confiscated the County of Mortain from William, the Count of Mortain, and the Honour of Eye, a large lordship previously owned by Robert Malet. In 1113, Stephen was granted both the title and the honour, although without the lands previously held by William in England. The gift of the Honour of Lancaster also followed after it was confiscated by Henry from Roger the Poitevin. Stephen was also given lands in Alençon in southern Normandy by Henry, but the local Normans rebelled, seeking assistance from Fulk, the Count of Anjou. Stephen and his older brother Theobald were comprehensively beaten in the subsequent campaign, which culminated in the battle of Alençon, and the territories were not recovered.
Finally, the King arranged for Stephen to marry Matilda in 1125, the daughter and only heiress of the Count of Boulogne, who owned both the important continental port of Boulogne and vast estates in the north-west and south-east of England. In 1127, William Clito, a potential claimant to the English throne, seemed likely to become the Count of Flanders; Stephen was sent by the King on a mission to prevent this, and in the aftermath of his successful election, William Clito attacked Stephen's lands in neighbouring Boulogne in retaliation. Eventually a truce was declared, and William Clito died the following year.
The "White Ship" and succession.
In 1120, the English political landscape changed dramatically. Three hundred passengers embarked on the "White Ship" to travel from Barfleur in Normandy to England, including the heir to the throne, William Adelin, and many other senior nobles. Stephen had intended to sail on the same ship but changed his mind at the last moment and got off to await another vessel, either out of concern for overcrowding on board the ship, or because he was suffering from diarrhoea. The ship foundered en route, and all but two of the passengers died, including William Adelin.
With Adelin dead, the inheritance to the English throne was thrown into doubt. Rules of succession in western Europe at the time were uncertain; in some parts of France, male primogeniture, in which the eldest son would inherit a title, was becoming more popular. It was also traditional for the King of France to crown his successor whilst he himself was still alive, making the intended line of succession relatively clear, but this was not the case in England. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands—usually considered to be the most valuable—and younger sons being given smaller, or more recently acquired, partitions or estates. The problem was further complicated by the sequence of unstable Anglo-Norman successions over the previous sixty years—William the Conqueror had gained England by force, William Rufus and Robert Curthose had fought a war between them to establish their inheritance, and Henry had only acquired control of Normandy by force. There had been no peaceful, uncontested successions.
With William Adelin dead, Henry had only one other legitimate child, Matilda, but as a woman she was at a substantial political disadvantage. Despite Henry taking a second wife, Adeliza of Louvain, it became increasingly unlikely that he would have another legitimate son, and he instead looked to Matilda as his intended heir. Matilda claimed the title of Holy Roman Empress through her marriage to Emperor Henry V, but her husband died in 1125, and she was remarried in 1128 to Geoffrey, the Count of Anjou, whose lands bordered the Duchy of Normandy. Geoffrey was unpopular with the Anglo-Norman elite: as an Angevin ruler, he was a traditional enemy of the Normans. At the same time, tensions continued to grow as a result of Henry's domestic policies, in particular the high level of revenue he was raising to pay for his various wars. Conflict was curtailed, however, by the power of the King's personality and reputation.
Henry attempted to build up a base of political support for Matilda in both England and Normandy, demanding that his court take oaths first in 1127, and then again in 1128 and 1131, to recognise Matilda as his immediate successor and recognise her descendants as the rightful rulers after her. Stephen was amongst those who took this oath in 1127. Nonetheless, relations between Henry, Matilda, and Geoffrey became increasingly strained towards the end of the King's life. Matilda and Geoffrey suspected that they lacked genuine support in England, and proposed to Henry in 1135 that the King should hand over the royal castles in Normandy to Matilda whilst he was still alive and insist on the Norman nobility swearing immediate allegiance to her, thereby giving the couple a much more powerful position after Henry's death. Henry angrily declined to do so, probably out of a concern that Geoffrey would try to seize power in Normandy somewhat earlier than intended. A fresh rebellion broke out in southern Normandy, and Geoffrey and Matilda intervened militarily on behalf of the rebels. In the middle of this confrontation, Henry unexpectedly fell ill and died near Lyons-la-Forêt.
Succession (1135).
Stephen was a well established figure in Anglo-Norman society by 1135. He was extremely wealthy, well-mannered and liked by his peers; he was also considered a man capable of firm action. Chroniclers recorded that despite his wealth and power he was a modest and easy-going leader, happy to sit with his men and servants, casually laughing and eating with them. He was very pious, both in terms of his observance of religious rituals and his personal generosity to the church. Stephen also had a personal Augustinian confessor appointed to him by the Archbishop of Canterbury, who implemented a penitential regime for him, and Stephen encouraged the new order of Cistercians to form abbeys on his estates, winning him additional allies within the church. Rumours of his father's cowardice during the First Crusade, however, continued to circulate, and a desire to avoid the same reputation may have influenced some of Stephen's rasher military actions. His wife, Matilda, played a major role in running their vast English estates, which contributed to the couple being the second-richest lay household in the country after the King. The landless Flemish nobleman William of Ypres had joined Stephen's household in 1133, alongside Faramus of Boulogne, a Flemish relative and friend of Matilda's.
Meanwhile, Stephen's younger brother Henry of Blois had also risen to power under Henry I. Henry of Blois had become a Cluniac monk and followed Stephen to England, where the King made him Abbot of Glastonbury, the richest abbey in England. The King then appointed him Bishop of Winchester, one of the richest bishoprics, allowing him to retain Glastonbury as well. The combined revenues of the two positions made Henry of Winchester the second-richest man in England after the King. Henry of Winchester was keen to reverse what he perceived as encroachment by the Norman kings on the rights of the church. The Norman kings had traditionally exercised a great deal of power and autonomy over the church within their territories. From the 1040s onwards, however, successive popes had put forward a reforming message that emphasised the importance of the church being "governed more coherently and more hierarchically from the centre" and established "its own sphere of authority and jurisdiction, separate from and independent of that of the lay ruler", in the words of historian Richard Huscroft.
When news began to spread of Henry I's death, many of the potential claimants to the throne were not well placed to respond. Geoffrey and Matilda were in Anjou, rather awkwardly supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late king was properly buried, which prevented them from returning to England. Stephen's elder brother Theobald was further south still, in Blois. Stephen, however, was in Bolougne, and when news reached him of Henry's death he left for England, accompanied by his military household. Robert of Gloucester had garrisoned the ports of Dover and Canterbury and some accounts suggest that they refused Stephen access when he first arrived. Nonetheless Stephen probably reached his own estate on the edge of London by 8 December and over the next week he began to seize power in England.
The crowds in London traditionally claimed a right to elect the king of England, and they proclaimed Stephen the new monarch, believing that he would grant the city new rights and privileges in return. Henry of Blois delivered the support of the church to Stephen: Stephen was able to advance to Winchester, where Roger, who was both the Bishop of Salisbury and the Lord Chancellor, instructed the royal treasury to be handed over to Stephen. On 15 December, Henry delivered an agreement under which Stephen would grant extensive freedoms and liberties to the church, in exchange for the Archbishop of Canterbury and the Papal Legate supporting his succession to the throne. There was the slight problem of the religious oath that Stephen had taken to support the Empress Matilda, but Henry convincingly argued that the late king had been wrong to insist that his court take the oath. Furthermore, the late king had only insisted on that oath to protect the stability of the kingdom, and in light of the chaos that might now ensue, Stephen would be justified in ignoring it. Henry was also able to persuade Hugh Bigod, the late King's royal steward, to swear that the King had changed his mind about the succession on his deathbed, nominating Stephen instead. Stephen's coronation was held a week later at Westminster Abbey on 22 December.
Meanwhile, the Norman nobility gathered at Le Neubourg to discuss declaring Theobald king, probably following the news that Stephen was gathering support in England. The Normans argued that the count, as the eldest grandson of William the Conqueror, had the most valid claim over the kingdom and the duchy, and was certainly preferable to Matilda. Theobald met with the Norman barons and Robert of Gloucester at Lisieux on 21 December, but their discussions were interrupted by the sudden news from England that Stephen's coronation was to occur the next day. Theobald then agreed to the Normans' proposal that he be made king, only to find that his former support immediately ebbed away: the barons were not prepared to support the division of England and Normandy by opposing Stephen. Stephen subsequently financially compensated Theobald, who in return remained in Blois and supported his brother's succession.
Early reign (1136–39).
Initial years (1136–37).
Stephen's new Anglo-Norman kingdom had been shaped by the Norman conquest of England in 1066, followed by the Norman expansion into south Wales over the coming years. Both the kingdom and duchy were dominated by a small number of major barons who owned lands on both sides of the English Channel, with the lesser barons beneath them usually having more localised holdings. The extent to which lands and positions should be passed down through hereditary right or by the gift of the king was still uncertain, and tensions concerning this issue had grown during the reign of Henry I. Certainly lands in Normandy, passed by hereditary right, were usually considered more important to major barons than those in England, where their possession was less certain. Henry had increased the authority and capabilities of the central royal administration, often bringing in "new men" to fulfil key positions rather than using the established nobility. In the process he had been able to maximise revenues and contain expenditures, resulting in a healthy surplus and a famously large treasury, but also increasing political tensions.
Stephen had to intervene in the north of England immediately after his coronation. David I of Scotland invaded the north on the news of Henry's death, taking Carlisle, Newcastle and other key strongholds. Northern England was a disputed territory at this time, with the Scottish kings laying a traditional claim to Cumberland, and David also claiming Northumbria by virtue of his marriage to the daughter of the former Anglo-Saxon earl Waltheof. Stephen rapidly marched north with an army and met David at Durham. An agreement was made under which David would return most of the territory he had taken, with the exception of Carlisle. In return, Stephen confirmed David's son Prince Henry's possessions in England, including the Earldom of Huntingdon.
Returning south, Stephen held his first royal court at Easter 1136. A wide range of nobles gathered at Westminster for the event, including many of the Anglo-Norman barons and most of the higher officials of the church. Stephen issued a new royal charter, confirming the promises he had made to the church, promising to reverse Henry's policies on the royal forests and to reform any abuses of the royal legal system. Stephen portrayed himself as the natural successor to Henry I's policies, and reconfirmed the existing seven earldoms in the kingdom on their existing holders. The Easter court was a lavish event, and a large amount of money was spent on the event itself, clothes and gifts. Stephen gave out grants of land and favours to those present and endowed numerous church foundations with land and privileges. Stephen's accession to the throne still needed to be ratified by the Pope, however, and Henry of Blois appears to have been responsible for ensuring that testimonials of support were sent both from Stephen's elder brother Theobald and from the French king Louis VI, to whom Stephen represented a useful balance to Angevin power in the north of France. Pope Innocent II confirmed Stephen as king by letter later that year, and Stephen's advisers circulated copies widely around England to demonstrate Stephen's legitimacy.
Troubles continued across Stephen's kingdom. After the Welsh victory at the battle of Llwchwr in January 1136 and the successful ambush of Richard Fitz Gilbert de Clare in April, south Wales rose in rebellion, starting in east Glamorgan and rapidly spreading across the rest of south Wales during 1137. Owain Gwynedd and Gruffydd ap Rhys successfully captured considerable territories, including Carmarthen Castle. Stephen responded by sending Richard's brother Baldwin and the Marcher Lord Robert Fitz Harold of Ewyas into Wales to pacify the region. Neither mission was particularly successful, and by the end of 1137 the King appears to have abandoned attempts to put down the rebellion. Historian David Crouch suggests that Stephen effectively "bowed out of Wales" around this time to concentrate on his other problems. Meanwhile, Stephen had put down two revolts in the south-west led by Baldwin de Redvers and Robert of Bampton; Baldwin was released after his capture and travelled to Normandy, where he became an increasingly vocal critic of the King.
The security of Normandy was also a concern. Geoffrey of Anjou invaded in early 1136 and, after a temporary truce, invaded later the same year, raiding and burning estates rather than trying to hold the territory. Events in England meant that Stephen was unable to travel to Normandy himself, so Waleran de Beaumont, appointed by Stephen as the lieutenant of Normandy, and Theobald led the efforts to defend the duchy. Stephen himself only returned to the duchy in 1137, where he met with Louis VI and Theobald to agree to an informal regional alliance, probably brokered by Henry, to counter the growing Angevin power in the region. As part of this deal, Louis recognised Stephen's son Eustace as Duke of Normandy in exchange for Eustace giving fealty to the French king. Stephen was less successful, however, in regaining the Argentan province along the Normandy and Anjou border, which Geoffrey had taken at the end of 1135. Stephen formed an army to retake it, but the frictions between his Flemish mercenary forces led by William of Ypres and the local Norman barons resulted in a battle between the two halves of his army. The Norman forces then deserted the King, forcing Stephen to give up his campaign. Stephen agreed to another truce with Geoffrey, promising to pay him 2,000 marks a year in exchange for peace along the Norman borders.
In the years following his succession, Stephen's relationship with the church became gradually more complex. The royal charter of 1136 had promised to review the ownership of all the lands that had been taken by the crown from the church since 1087, but these estates were now typically owned by nobles. Henry of Blois's claims, in his role as Abbot of Glastonbury, to extensive lands in Devon resulted in considerable local unrest. In 1136, Archbishop of Canterbury William de Corbeil died. Stephen responded by seizing his personal wealth, which caused some discontent amongst the senior clergy. Stephen's brother Henry wanted to succeed to the post, but Stephen instead supported Theobald of Bec, who was eventually appointed, while the papacy named Henry papal legate, possibly as consolation for not receiving Canterbury.
Stephen's first few years as king can be interpreted in different ways. From a positive perspective, he stabilised the northern border with Scotland, contained Geoffrey's attacks on Normandy, was at peace with Louis VI, enjoyed good relations with the church and had the broad support of his barons. There were significant underlying problems, nonetheless. The north of England was now controlled by David and Prince Henry, Stephen had abandoned Wales, the fighting in Normandy had considerably destabilised the duchy, and an increasing number of barons felt that Stephen had given them neither the lands nor the titles they felt they deserved or were owed. Stephen was also rapidly running out of money: Henry's considerable treasury had been emptied by 1138 due to the costs of running Stephen's more lavish court and the need to raise and maintain his mercenary armies fighting in England and Normandy.
Defending the kingdom (1138–39).
Stephen was attacked on several fronts during 1138. First, Robert of Gloucester rebelled against the King, starting the descent into civil war in England. An illegitimate son of Henry I and the half-brother of the Empress Matilda, Robert was one of the most powerful Anglo-Norman barons, controlling estates in Normandy as well as the Earldom of Gloucester. He was known for his qualities as a statesman, his military experience, and leadership ability. Robert had tried to convince Theobald to take the throne in 1135; he did not attend Stephen's first court in 1136 and it took several summonses to convince him to attend court at Oxford later that year. In 1138, Robert renounced his fealty to Stephen and declared his support for Matilda, triggering a major regional rebellion in Kent and across the south-west of England, although Robert himself remained in Normandy. In France, Geoffrey of Anjou took advantage of the situation by re-invading Normandy. David of Scotland also invaded the north of England once again, announcing that he was supporting the claim of his niece the Empress Matilda to the throne, pushing south into Yorkshire.
Anglo-Norman warfare during the reign of Stephen was characterised by attritional military campaigns, in which commanders tried to seize key enemy castles in order to allow them to take control of their adversaries' territory and ultimately win a slow, strategic victory. The armies of the period centred on bodies of mounted, armoured knights, supported by infantry and crossbowmen. These forces were either feudal levies, drawn up by local nobles for a limited period of service during a campaign, or, increasingly, mercenaries, who were expensive but more flexible and often more skilled. These armies, however, were ill-suited to besieging castles, whether the older motte-and-bailey designs or the newer, stone-built keeps. Existing siege engines were significantly less powerful than the later trebuchet designs, giving defenders a substantial advantage over attackers. As a result, slow sieges to starve defenders out, or mining operations to undermine walls, tended to be preferred by commanders over direct assaults. Occasionally pitched battles were fought between armies but these were considered highly risky endeavours and were usually avoided by prudent commanders. The cost of warfare had risen considerably in the first part of the 12th century, and adequate supplies of ready cash were increasingly proving important in the success of campaigns.
Stephen's personal qualities as a military leader focused on his skill in personal combat, his capabilities in siege warfare and a remarkable ability to move military forces quickly over relatively long distances. In response to the revolts and invasions, Stephen rapidly undertook several military campaigns, focusing primarily on England rather than Normandy. His wife Matilda was sent to Kent with ships and resources from Boulogne, with the task of retaking the key port of Dover, under Robert's control. A small number of Stephen's household knights were sent north to help the fight against the Scots, where David's forces were defeated later that year at the battle of the Standard in August by the forces of Thurstan, the Archbishop of York. Despite this victory, however, David still occupied most of the north. Stephen himself went west in an attempt to regain control of Gloucestershire, first striking north into the Welsh Marches, taking Hereford and Shrewsbury, before heading south to Bath. The town of Bristol itself proved too strong for him, and Stephen contented himself with raiding and pillaging the surrounding area. The rebels appear to have expected Robert to intervene with support that year, but he remained in Normandy throughout, trying to persuade the Empress Matilda to invade England herself. Dover finally surrendered to the queen's forces later in the year.
Stephen's military campaign in England had progressed well, and historian David Crouch describes it as "a military achievement of the first rank". The King took the opportunity of his military advantage to forge a peace agreement with Scotland. Stephen's wife Matilda was sent to negotiate another agreement between Stephen and David, called the treaty of Durham; Northumbria and Cumbria would effectively be granted to David and his son Prince Henry, in exchange for their fealty and future peace along the border. Unfortunately, the powerful Ranulf, Earl of Chester, considered himself to hold the traditional rights to Carlisle and Cumberland and was extremely displeased to see them being given to the Scots. Nonetheless, Stephen could now focus his attention on the anticipated invasion of England by Robert and Matilda's forces.
Road to civil war (1139).
Stephen prepared for the Angevin invasion by creating a number of additional earldoms. Only a handful of earldoms had existed under Henry I and these had been largely symbolic in nature. Stephen created many more, filling them with men he considered to be loyal, capable military commanders, and in the more vulnerable parts of the country assigning them new lands and additional executive powers. Stephen appears to have had several objectives in mind, including both ensuring the loyalty of his key supporters by granting them these honours, and improving his defences in key parts of the kingdom. Stephen was heavily influenced by his principal advisor, Waleran de Beaumont, the twin brother of Robert of Leicester. The Beaumont twins and their younger brother and cousins received the majority of these new earldoms. From 1138 onwards, Stephen gave them the earldoms of Worcester, Leicester, Hereford, Warwick and Pembroke, which—especially when combined with the possessions of Stephen's new ally, Prince Henry, in Cumberland and Northumbria—created a wide block of territory to act as a buffer zone between the troubled south-west, Chester, and the rest of the kingdom. With their new lands, the power of the Beamounts grew to the point where David Crouch suggests that it became "dangerous to be anything other than a friend of Waleran" at Stephen's court.
Stephen took steps to remove a group of bishops he regarded as a threat to his rule. The royal administration under Henry I had been headed by Roger, the Bishop of Salisbury, supported by Roger's nephews, Alexander and Nigel, the Bishops of Lincoln and Ely respectively, and Roger's son, Roger le Poer, who was the Lord Chancellor. These bishops were powerful landowners as well as ecclesiastical rulers, and they had begun to build new castles and increase the size of their military forces, leading Stephen to suspect that they were about to defect to the Empress Matilda. Roger and his family were also enemies of Waleran, who disliked their control of the royal administration. In June 1139, Stephen held his court in Oxford, where a fight between Alan of Brittany and Roger's men broke out, an incident probably deliberately created by Stephen. Stephen responded by demanding that Roger and the other bishops surrender all of their castles in England. This threat was backed up by the arrest of the bishops, with the exception of Nigel who had taken refuge in Devizes Castle; the bishop only surrendered after Stephen besieged the castle and threatened to execute Roger le Poer. The remaining castles were then surrendered to the King.
Stephen's brother, Henry of Blois, was alarmed by this, both as a matter of principle, since Stephen had previously agreed in 1135 to respect the freedoms of the church, and more pragmatically because he himself had recently built six castles and had no desire to be treated in the same way. As the papal legate, he summoned the King to appear before an ecclesiastical council to answer for the arrests and seizure of property. Henry asserted the Church’s right to investigate and judge all charges against members of the clergy. Stephen sent Aubrey de Vere as his spokesman to the council, who argued that Roger of Salisbury had been arrested not as a bishop, but rather in his role as a baron who had been preparing to change his support to the Empress Matilda. The King was supported by Hugh, Archbishop of Rouen, who challenged the bishops to show how canon law entitled them to build or hold castles. Aubrey threatened that Stephen would complain to the pope that he was being harassed by the English church, and the council let the matter rest following an unsuccessful appeal to Rome. The incident successfully removed any military threat from the bishops, but it may have damaged Stephen's relationship with the senior clergy, and in particular with his brother Henry.
Civil war (1139–54).
Initial phase of the war (1139–40).
The Angevin invasion finally arrived in 1139. Baldwin de Redvers crossed over from Normandy to Wareham in August in an initial attempt to capture a port to receive the Empress Matilda's invading army, but Stephen's forces forced him to retreat into the south-west. The following month, however, the Empress was invited by the Dowager Queen Adeliza to land at Arundel instead, and on 30 September Robert of Gloucester and the Empress arrived in England with 140 knights. The Empress stayed at Arundel Castle, whilst Robert marched north-west to Wallingford and Bristol, hoping to raise support for the rebellion and to link up with Miles of Gloucester, a capable military leader who took the opportunity to renounce his fealty to the King. Stephen promptly moved south, besieging Arundel and trapping Matilda inside the castle.
Stephen then agreed to a truce proposed by his brother, Henry of Blois; the full details of the truce are not known, but the results were that Stephen first released Matilda from the siege and then allowed her and her household of knights to be escorted to the south-west, where they were reunited with Robert of Gloucester. The reasoning behind Stephen's decision to release his rival remains unclear. Contemporary chroniclers suggested that Henry argued that it would be in Stephen's own best interests to release the Empress and concentrate instead on attacking Robert, and Stephen may have seen Robert, not the Empress, as his main opponent at this point in the conflict. Stephen also faced a military dilemma at Arundel—the castle was considered almost impregnable, and he may have been worried that he was tying down his army in the south whilst Robert roamed freely in the west. Another theory is that Stephen released Matilda out of a sense of chivalry; Stephen was certainly known for having a generous, courteous personality and women were not normally expected to be targeted in Anglo-Norman warfare.
Having released the Empress, Stephen focused on pacifying the south-west of England. Although there had been few new defections to the Empress, his enemies now controlled a compact block of territory stretching out from Gloucester and Bristol south-west into Devon and Cornwall, west into the Welsh Marches and east as far as Oxford and Wallingford, threatening London. Stephen started by attacking Wallingford Castle, held by the Empress's childhood friend Brien FitzCount, only to find it too well defended. Stephen left behind some forces to blockade the castle and continued west into Wiltshire to attack Trowbridge, taking the castles of South Cerney and Malmesbury en route. Meanwhile, Miles of Gloucester marched east, attacking Stephen's rearguard forces at Wallingford and threatening an advance on London. Stephen was forced to give up his western campaign, returning east to stabilise the situation and protect his capital.
At the start of 1140, Nigel, the Bishop of Ely, whose castles Stephen had confiscated the previous year, rebelled against Stephen as well. Nigel hoped to seize East Anglia and established his base of operations in the Isle of Ely, then surrounded by protective fenland. Stephen responded quickly, taking an army into the fens and using boats lashed together to form a causeway that allowed him to make a surprise attack on the isle. Nigel escaped to Gloucester, but his men and castle were captured, and order was temporarily restored in the east. Robert of Gloucester's men retook some of the territory that Stephen had taken in his 1139 campaign. In an effort to negotiate a truce, Henry of Blois held a peace conference at Bath, to which Stephen sent his wife. The conference collapsed over the insistence by Henry and the clergy that they should set the terms of any peace deal, which Stephen found unacceptable.
Ranulf of Chester remained upset over Stephen's gift of the north of England to Prince Henry. Ranulf devised a plan for dealing with the problem by ambushing Henry whilst the prince was travelling back from Stephen's court to Scotland after Christmas. Stephen responded to rumours of this plan by escorting Henry himself north, but this gesture proved the final straw for Ranulf. Ranulf had previously claimed that he had the rights to Lincoln Castle, held by Stephen, and under the guise of a social visit, Ranulf seized the fortification in a surprise attack. Stephen marched north to Lincoln and agreed to a truce with Ranulf, probably to keep him from joining the Empress's faction, under which Ranulf would be allowed to keep the castle. Stephen returned to London but received news that Ranulf, his brother and their family were relaxing in Lincoln Castle with a minimal guard force, a ripe target for a surprise attack of his own. Abandoning the deal he had just made, Stephen gathered his army again and sped north, but not quite fast enough—Ranulf escaped Lincoln and declared his support for the Empress, and Stephen was forced to place the castle under siege.
Second phase of the war (1141–42).
While Stephen and his army besieged Lincoln Castle at the start of 1141, Robert of Gloucester and Ranulf of Chester advanced on the King's position with a somewhat larger force. When the news reached Stephen, he held a council to decide whether to give battle or to withdraw and gather additional soldiers: Stephen decided to fight, resulting in the battle of Lincoln on 2 February 1141. The King commanded the centre of his army, with Alan of Brittany on his right and William of Aumale on his left. Robert and Ranulf's forces had superiority in cavalry and Stephen dismounted many of his own knights to form a solid infantry block; he joined them himself, fighting on foot in the battle. Stephen was not a gifted public speaker, and delegated the pre-battle speech to Baldwin of Clare, who delivered a rousing declaration. After an initial success in which William's forces destroyed the Angevins' Welsh infantry, the battle went badly for Stephen. Robert and Ranulf's cavalry encircled Stephen's centre, and the king found himself surrounded by the enemy army. Many of Stephen's supporters, including Waleran de Beaumont and William of Ypres, fled from the field at this point but Stephen fought on, defending himself first with his sword and then, when that broke, with a borrowed battle axe. Finally, he was overwhelmed by Robert's men and taken away from the field in custody.
Robert took Stephen back to Gloucester, where the King met with the Empress Matilda, and was then moved to Bristol Castle, traditionally used for holding high-status prisoners. He was initially left confined in relatively good conditions, but his security was later tightened and he was kept in chains. The Empress now began to take the necessary steps to have herself crowned queen in his place, which would require the agreement of the church and her coronation at Westminster. Stephen's brother Henry summoned a council at Winchester before Easter in his capacity as papal legate to consider the clergy's view. He had made a private deal with the Empress Matilda that he would deliver the support of the church, if she agreed to give him control over church business in England. Henry handed over the royal treasury, rather depleted except for Stephen's crown, to the Empress, and excommunicated many of Stephen's supporters who refused to switch sides. Archbishop Theobald of Canterbury was unwilling to declare Matilda queen so rapidly, however, and a delegation of clergy and nobles, headed by Theobald, travelled to see Stephen in Bristol and consult about their moral dilemma: should they abandon their oaths of fealty to the King? Stephen agreed that, given the situation, he was prepared to release his subjects from their oath of fealty to him, and the clergy gathered again in Winchester after Easter to declare the Empress "Lady of England and Normandy" as a precursor to her coronation. When Matilda advanced to London in an effort to stage her coronation in June, though, she faced an uprising by the local citizens in support of Stephen that forced her to flee to Oxford, uncrowned.
Once news of Stephen's capture reached him, Geoffrey of Anjou invaded Normandy again and, in the absence of Waleran of Beaumont, who was still fighting in England, Geoffrey took all the duchy south of the river Seine and east of the river Risle. No help was forthcoming from Stephen's brother Theobald this time either, who appears to have been preoccupied with his own problems with France—the new French king, Louis VII, had rejected his father's regional alliance, improving relations with Anjou and taking a more bellicose line with Theobald, which would result in war the following year. Geoffrey's success in Normandy and Stephen's weakness in England began to influence the loyalty of many Anglo-Norman barons, who feared losing their lands in England to Robert and the Empress, and their possessions in Normandy to Geoffrey. Many started to leave Stephen's faction. His friend and advisor Waleran was one of those who decided to defect in mid-1141, crossing into Normandy to secure his ancestral possessions by allying himself with the Angevins, and bringing Worcestershire into the Empress's camp. Waleran's twin brother, Robert of Leicester, effectively withdrew from fighting in the conflict at the same time. Other supporters of the Empress were restored in their former strongholds, such as Bishop Nigel of Ely, and others still received new earldoms in the west of England. The royal control over the minting of coins broke down, leading to coins being struck by local barons and bishops across the country.
Stephen's wife Matilda played a critical part in keeping the King's cause alive during his captivity. Queen Matilda gathered Stephen's remaining lieutenants around her and the royal family in the south-east, advancing into London when the population rejected the Empress. Stephen's long-standing commander William of Ypres remained with the queen in London; William Martel, the royal steward, commanded operations from Sherborne in Dorset, and Faramus of Boulogne ran the royal household. The queen appears to have generated genuine sympathy and support from Stephen's more loyal followers. Henry's alliance with the Empress proved short-lived, as they soon fell out over political patronage and ecclesiastical policy; the bishop met Stephen's wife Queen Matilda at Guildford and transferred his support to her.
The King's eventual release resulted from the Angevin defeat at the rout of Winchester. Robert of Gloucester and the Empress besieged Henry in the city of Winchester in July. Queen Matilda and William of Ypres then encircled the Angevin forces with their own army, reinforced with fresh troops from London. In the subsequent battle the Empress's forces were defeated and Robert of Gloucester himself was taken prisoner. Further negotiations attempted to deliver a general peace agreement but Queen Matilda was unwilling to offer any compromise to the Empress, and Robert refused to accept any offer to encourage him to change sides to Stephen. Instead, in November the two sides simply exchanged Robert and the King, and Stephen began re-establishing his authority. Henry held another church council, which this time reaffirmed Stephen's legitimacy to rule, and a fresh coronation of Stephen and Matilda occurred at Christmas 1141.
At the beginning of 1142 Stephen fell ill, and by Easter rumours had begun to circulate that he had died. Possibly this illness was the result of his imprisonment the previous year, but he finally recovered and travelled north to raise new forces and to successfully convince Ranulf of Chester to change sides once again. Stephen then spent the summer attacking some of the new Angevin castles built the previous year, including Cirencester, Bampton and Wareham. In September, he spotted an opportunity to seize the Empress Matilda herself in Oxford. Oxford was a secure town, protected by walls and the river Isis, but Stephen led a sudden attack across the river, leading the charge and swimming part of the way. Once on the other side, the King and his men stormed into the town, trapping the Empress in the castle. Oxford Castle, however, was a powerful fortress and, rather than storming it, Stephen had to settle down for a long siege, albeit secure in the knowledge that Matilda was now surrounded. Just before Christmas, the Empress left the castle unobserved, crossed the icy river on foot and made her escape to Wallingford. The garrison surrendered shortly afterwards, but Stephen had lost an opportunity to capture his principal opponent.
Stalemate (1143–46).
The war between the two sides in England reached a stalemate in the mid-1140s, while Geoffrey of Anjou consolidated his hold on power in Normandy. 1143 started precariously for Stephen when he was besieged by Robert of Gloucester at Wilton Castle, an assembly point for royal forces in Herefordshire. Stephen attempted to break out and escape, resulting in the battle of Wilton. Once again, the Angevin cavalry proved too strong, and for a moment it appeared that Stephen might be captured for a second time. On this occasion, however, William Martel, Stephen's steward, made a fierce rear guard effort, allowing Stephen to escape from the battlefield. Stephen valued William's loyalty sufficiently to agree to exchange Sherborne Castle for his safe release—this was one of the few instances where Stephen was prepared to give up a castle to ransom one of his men.
In late 1143, Stephen faced a new threat in the east, when Geoffrey de Mandeville, the Earl of Essex, rose up in rebellion against the King in East Anglia. Stephen had disliked the baron for several years, and provoked the conflict by summoning Geoffrey to court, where the King arrested him. Stephen threatened to execute Geoffrey unless the baron handed over his various castles, including the Tower of London, Saffron Walden and Pleshey, all important fortifications because they were in, or close to, London. Geoffrey gave in, but once free he headed north-east into the Fens to the Isle of Ely, from where he began a military campaign against Cambridge, with the intention of progressing south towards London. With all of his other problems and with Hugh Bigod still in open revolt in Norfolk, Stephen lacked the resources to track Geoffrey down in the Fens and made do with building a screen of castles between Ely and London, including Burwell Castle.
For a period, the situation continued to worsen. Ranulf of Chester revolted once again in the summer of 1144, splitting up Stephen's Honour of Lancaster between himself and Prince Henry. In the west, Robert of Gloucester and his followers continued to raid the surrounding royalist territories, and Wallingford Castle remained a secure Angevin stronghold, too close to London for comfort. Meanwhile, Geoffrey of Anjou finished securing his hold on southern Normandy and in January 1144 he advanced into Rouen, the capital of the duchy, concluding his campaign. Louis VII recognised him as Duke of Normandy shortly after. By this point in the war, Stephen was depending increasingly on his immediate royal household, such as William of Ypres and others, and lacked the support of the major barons who might have been able to provide him with significant additional forces; after the events of 1141, Stephen made little use of his network of earls.
After 1143 the war ground on, but progressing slightly better for Stephen. Miles of Gloucester, one of the most talented Angevin commanders, had died whilst hunting over the previous Christmas, relieving some of the pressure in the west. Geoffrey de Mandeville's rebellion continued until September 1144, when he died during an attack on Burwell. The war in the west progressed better in 1145, with the King recapturing Faringdon Castle in Oxfordshire. In the north, Stephen came to a fresh agreement with Ranulf of Chester, but then in 1146 repeated the ruse he had played on Geoffrey de Mandeville in 1143, first inviting Ranulf to court, before arresting him and threatening to execute him unless he handed over a number of castles, including Lincoln and Coventry. As with Geoffrey, the moment Ranulf was released he immediately rebelled, but the situation was a stalemate: Stephen had few forces in the north with which to prosecute a fresh campaign, whilst Ranulf lacked the castles to support an attack on Stephen. By this point, however, Stephen's practice of inviting barons to court and arresting them had brought him into some disrepute and increasing distrust.
Final phases of the war (1147–52).
England had suffered extensively from the war by 1147, leading later Victorian historians to call the period of conflict "the Anarchy". The contemporary "Anglo-Saxon Chronicle" recorded how "there was nothing but disturbance and wickedness and robbery". Certainly in many parts of the country, such as Wiltshire, Berkshire, the Thames Valley and East Anglia, the fighting and raiding had caused serious devastation. Numerous "adulterine", or unauthorised, castles had been built as bases for local lords—the chronicler Robert of Torigny complained that as many as 1,115 such castles had been built during the conflict, although this was probably an exaggeration as elsewhere he suggested an alternative figure of 126. The previously centralised royal coinage system was fragmented, with Stephen, the Empress and local lords all minting their own coins. The royal forest law had collapsed in large parts of the country. Some parts of the country, though, were barely touched by the conflict—for example, Stephen's lands in the south-east and the Angevin heartlands around Gloucester and Bristol were largely unaffected, and David I ruled his territories in the north of England effectively. The King's overall income from his estates, however, declined seriously during the conflict, particularly after 1141, and royal control over the minting of new coins remained limited outside of the south-east and East Anglia. With Stephen often based in the south-east, increasingly Westminster, rather than the older site of Winchester, was used as the centre of royal government.
The character of the conflict in England gradually began to shift; as historian Frank Barlow suggests, by the late 1140s "the civil war was over", barring the occasional outbreak of fighting. In 1147 Robert of Gloucester died peacefully, and the next year the Empress Matilda left south-west England for Normandy, both of which contributed to reducing the tempo of the war. The Second Crusade was announced, and many Angevin supporters, including Waleran of Beaumont, joined it, leaving the region for several years. Many of the barons were making individual peace agreements with each other to secure their lands and war gains. Geoffrey and Matilda's son, the future King Henry II, mounted a small mercenary invasion of England in 1147 but the expedition failed, not least because Henry lacked the funds to pay his men. Surprisingly, Stephen himself ended up paying their costs, allowing Henry to return home safely; his reasons for doing so are unclear. One potential explanation is his general courtesy to a member of his extended family; another is that he was starting to consider how to end the war peacefully, and saw this as a way of building a relationship with Henry.
The young Henry FitzEmpress returned to England again in 1149, this time planning to form a northern alliance with Ranulf of Chester. The Angevin plan involved Ranulf agreeing to give up his claim to Carlisle, held by the Scots, in return for being given the rights to the whole of the Honour of Lancaster; Ranulf would give homage to both David and Henry Fitzempress, with Henry having seniority. Following this peace agreement, Henry and Ranulf agreed to attack York, probably with help from the Scots. Stephen marched rapidly north to York and the planned attack disintegrated, leaving Henry to return to Normandy, where he was declared duke by his father.
Although still young, Henry was increasingly gaining a reputation as an energetic and capable leader. His prestige and power increased further when he unexpectedly married Eleanor of Aquitaine in 1152; Eleanor was the attractive Duchess of Aquitaine and the recently divorced wife of Louis VII of France, and the marriage made Henry the future ruler of a huge swathe of territory across France.
In the final years of the war, Stephen began to focus on the issue of his family and the succession. Stephen's eldest son was Eustace and the King wanted to confirm him as his successor, although chroniclers recorded that Eustace was infamous for levying heavy taxes and extorting money from those on his lands. Stephen's second son, William, was married to the extremely wealthy heiress Isabel de Warenne. In 1148, Stephen built the Cluniac Faversham Abbey as a resting place for his family. Both Stephen's wife, Queen Matilda, and his older brother Theobald died in 1152.
Argument with the church (1145–52).
Stephen's relationship with the church deteriorated badly towards the end of his reign. The reforming movement within the church, which advocated greater autonomy from royal authority for the clergy, had continued to grow, while new voices such as the Cistercians had gained additional prestige within the monastic orders, eclipsing older orders such as the Cluniacs. Stephen's dispute with the church had its origins in 1140, when Archbishop Thurstan of York died. An argument then broke out between a group of reformers based in York and backed by Bernard of Clairvaux, the head of the Cistercian order, who preferred William of Rievaulx as the new archbishop, and Stephen and his brother Henry of Blois, who preferred various Blois family relatives. The row between Henry and Bernard grew increasingly personal, and Henry used his authority as legate to appoint his nephew William of York to the post in 1144 only to find that, when Pope Innocent II died in 1145, Bernard was able to get the appointment rejected by Rome. Bernard then convinced Pope Eugene III to overturn Henry's decision altogether in 1147, deposing William, and appointing Henry Murdac as archbishop instead.
Stephen was furious over what he saw as potentially precedent-setting papal interference in his royal authority, and initially refused to allow Murdac into England. When Theobald, the Archbishop of Canterbury, went to consult with the Pope on the matter against Stephen's wishes, the King refused to allow him back into England either, and seized his estates. Stephen also cut his links to the Cistercian order, and turned instead to the Cluniacs, of which Henry was a member.
Nonetheless, the pressure on Stephen to get Eustace confirmed as his legitimate heir continued to grow. The King gave Eustace the County of Boulogne in 1147, but it remained unclear whether Eustace would inherit England. Stephen's preferred option was to have Eustace crowned while he himself was still alive, as was the custom in France, but this was not the normal practice in England, and Celestine II, during his brief tenure as pope between 1143 and 1144, had banned any change to this practice. Since the only person who could crown Eustace was Archbishop Theobald, who refused to do so without agreement from the current pope, Eugene III, the matter reached an impasse. At the end of 1148, Stephen and Theobald came to a temporary compromise that allowed Theobald to return to England. Theobald was appointed a papal legate in 1151, adding to his authority. Stephen then made a fresh attempt to have Eustace crowned at Easter 1152, gathering his nobles to swear fealty to Eustace, and then insisting that Theobald and his bishops anoint him king. When Theobald refused yet again, Stephen and Eustace imprisoned both him and the bishops and refused to release them unless they agreed to crown Eustace. Theobald escaped again into temporary exile in Flanders, pursued to the coast by Stephen's knights, marking a low point in Stephen's relationship with the church.
Treaties and peace (1153–54).
Henry FitzEmpress returned to England again at the start of 1153 with a small army, supported in the north and east of England by Ranulf of Chester and Hugh Bigod. Stephen's castle at Malmesbury was besieged by Henry's forces, and the King responded by marching west with an army to relieve it. Stephen unsuccessfully attempted to force Henry's smaller army to fight a decisive battle along the river Avon. In the face of the increasingly wintry weather, Stephen agreed to a temporary truce and returned to London, leaving Henry to travel north through the Midlands where the powerful Robert de Beaumont, Earl of Leicester, announced his support for the Angevin cause. Despite only modest military successes, Henry and his allies now controlled the south-west, the Midlands and much of the north of England.
Over the summer, Stephen intensified the long-running siege of Wallingford Castle in a final attempt to take this major Angevin stronghold. The fall of Wallingford appeared imminent and Henry marched south in an attempt to relieve the siege, arriving with a small army and placing Stephen's besieging forces under siege themselves. Upon news of this, Stephen gathered up a large force and marched from Oxford, and the two sides confronted each other across the River Thames at Wallingford in July. By this point in the war, the barons on both sides seem to have been eager to avoid an open battle. As a result, instead of a battle ensuing, members of the church brokered a truce, to the annoyance of both Stephen and Henry.
In the aftermath of Wallingford, Stephen and Henry spoke together privately about a potential end to the war; Stephen's son Eustace, however, was furious about the peaceful outcome at Wallingford. He left his father and returned home to Cambridge to gather more funds for a fresh campaign, where he fell ill and died the next month. Eustace's death removed an obvious claimant to the throne and was politically convenient for those seeking a permanent peace in England. It is possible, however, that Stephen had already begun to consider passing over Eustace's claim; historian Edmund King observes that Eustace's claim to the throne was not mentioned in the discussions at Wallingford, for example, and this may have added to Stephen's son's anger.
Fighting continued after Wallingford, but in a rather half-hearted fashion. Stephen lost the towns of Oxford and Stamford to Henry while the King was diverted fighting Hugh Bigod in the east of England, but Nottingham Castle survived an Angevin attempt to capture it. Meanwhile, Stephen's brother Henry of Blois and Archbishop Theobald of Canterbury were for once unified in an effort to broker a permanent peace between the two sides, putting pressure on Stephen to accept a deal. The armies of Stephen and Henry FitzEmpress met again at Winchester, where the two leaders would ratify the terms of a permanent peace in November. Stephen announced the Treaty of Winchester in Winchester Cathedral: he recognised Henry FitzEmpress as his adopted son and successor, in return for Henry doing homage to him; Stephen promised to listen to Henry's advice, but retained all his royal powers; Stephen's remaining son, William, would do homage to Henry and renounce his claim to the throne, in exchange for promises of the security of his lands; key royal castles would be held on Henry's behalf by guarantors, whilst Stephen would have access to Henry's castles; and the numerous foreign mercenaries would be demobilised and sent home. Stephen and Henry sealed the treaty with a kiss of peace in the cathedral.
Death.
Stephen's decision to recognise Henry as his heir was, at the time, not necessarily a final solution to the civil war. Despite the issuing of new currency and administrative reforms, Stephen might potentially have lived for many more years, whilst Henry's position on the continent was far from secure. Although Stephen's son William was young and unprepared to challenge Henry for the throne in 1153, the situation could well have shifted in subsequent years—there were widespread rumours during 1154 that William planned to assassinate Henry, for example. Historian Graham White describes the treaty of Winchester as a "precarious peace", capturing the judgement of most modern historians that the situation in late 1153 was still uncertain and unpredictable.
Certainly many problems remained to be resolved, including re-establishing royal authority over the provinces and resolving the complex issue of which barons should control the contested lands and estates after the long civil war. Stephen burst into activity in early 1154, travelling around the kingdom extensively. He began issuing royal writs for the south-west of England once again and travelled to York where he held a major court in an attempt to impress upon the northern barons that royal authority was being reasserted. After a busy summer in 1154, however, Stephen travelled to Dover to meet the Count of Flanders; some historians believe that the King was already ill and preparing to settle his family affairs. Stephen fell ill with a stomach disorder and died on 25 October at the local priory, being buried at Faversham Abbey with his wife Matilda and son Eustace.
Legacy.
Aftermath.
After Stephen's death, Henry II succeeded to the throne of England. Henry vigorously re-established royal authority in the aftermath of the civil war, dismantling castles and increasing revenues, although several of these trends had begun under Stephen. The destruction of castles under Henry was not as dramatic as once thought, and although he restored royal revenues, the economy of England remained broadly unchanged under both rulers. Stephen's remaining son William I of Blois was confirmed as the Earl of Surrey by Henry, and prospered under the new regime, with the occasional point of tension with Henry. Stephen's daughter Marie I of Boulogne also survived her father; she had been placed in a convent by Stephen, but after his death left and married. Stephen's middle son, Baldwin, and second daughter, Matilda, had died before 1147 and were buried at Holy Trinity Priory, Aldgate. Stephen probably had three illegitimate sons, Gervase, Ralph and Americ, by his mistress Damette; Gervase became Abbot of Westminster in 1138, but after his father's death Gervase was removed by Henry in 1157 and died shortly afterwards.
Historiography.
Much of the modern history of Stephen's reign is based on accounts of chroniclers who lived in, or close to, the middle of the 12th century, forming a relatively rich account of the period. All of the main chronicler accounts carry significant regional biases in how they portray the disparate events. Several of the key chronicles were written in the south-west of England, including the "Gesta Stephani", or "Acts of Stephen", and William of Malmesbury's "Historia Novella", or "New History". In Normandy, Orderic Vitalis wrote his "Ecclesiastical History", covering Stephen's reign until 1141, and Robert of Torigni wrote a later history of the rest of the period. Henry of Huntingdon, who lived in the east of England, produced the "Historia Anglorum" that provides a regional account of the reign. The "Anglo-Saxon Chronicle" was past its prime by the time of Stephen, but is remembered for its striking account of conditions during "the Anarchy". Most of the chronicles carry some bias for or against Stephen, Robert of Gloucester or other key figures in the conflict. Those writing for the church after the events of Stephen's later reign, such as John of Salisbury for example, paint the King as a tyrant due to his argument with the Archbishop of Canterbury; by contrast, clerics in Durham regarded Stephen as a saviour, due to his contribution to the defeat of the Scots at the battle of the Standard. Later chronicles written during the reign of Henry II were generally more negative: Walter Map, for example, described Stephen as "a fine knight, but in other respects almost a fool." A number of charters were issued during Stephen's reign, often giving details of current events or daily routine, and these have become widely used as sources by modern historians.
Historians in the "Whiggish" tradition that emerged during the Victorian period traced a progressive and universalist course of political and economic development in England over the medieval period. William Stubbs focused on these constitutional aspects of Stephen's reign in his 1874 volume the "Constitutional History of England", beginning an enduring interest in Stephen and his reign. Stubbs' analysis, focusing on the disorder of the period, influenced his student John Round to coin the term "the Anarchy" to describe the period, a label that, whilst sometimes critiqued, continues to be used today. The late-Victorian scholar Frederic William Maitland also introduced the possibility that Stephen's reign marked a turning point in English legal history—the so-called "tenurial crisis".
Stephen remains a popular subject for historical study: David Crouch suggests that after King John he is "arguably the most written-about medieval king of England". Modern historians vary in their assessments of Stephen as a king. Historian R. H. Davis's influential biography paints a picture of a weak king: a capable military leader in the field, full of activity and pleasant, but "beneath the surface ... mistrustful and sly", with poor strategic judgement that ultimately undermined his reign. Stephen's lack of sound policy judgement and his mishandling of international affairs, leading to the loss of Normandy and his consequent inability to win the civil war in England, is also highlighted by another of his biographers, David Crouch. Historian and biographer Edmund King, whilst painting a slightly more positive picture than Davis, also concludes that Stephen, while a stoic, pious and genial leader, was also rarely, if ever, his own man, usually relying upon stronger characters such as his brother or wife. Historian Keith Stringer provides a more positive portrayal of Stephen, arguing that his ultimate failure as king was the result of external pressures on the Norman state, rather than the result of personal failings.
Popular representations.
Stephen and his reign have been occasionally used in historical fiction. Stephen and his supporters appear in Ellis Peters' historical detective series "Brother Cadfael", set between 1137 and 1145. Peters' depiction of Stephen's reign is an essentially local narrative, focused on the town of Shrewsbury and its environs. Peters paints Stephen as a tolerant man and a reasonable ruler, despite his execution of the Shrewsbury defenders after the taking of the city in 1138. In contrast, Stephen is depicted unsympathetically in both Ken Follett's historical novel "The Pillars of the Earth" and the TV mini-series adapted from it.
Issue.
Stephen of Blois married Matilda of Boulogne in 1125. They had the following issue:
King Stephen's illegitimate children by a certain Damette were:
Bibliography.
</dl>

</doc>
<doc id="28954" url="http://en.wikipedia.org/wiki?curid=28954" title="Space Battleship Yamato">
Space Battleship Yamato

Space Battleship Yamato (Japanese: 宇宙戦艦ヤマト, Hepburn: Uchū Senkan Yamato, also called Cosmoship Yamato) is a Japanese science fiction anime series featuring an eponymous spacecraft. It is also known to English-speaking audiences as Space Cruiser Yamato; an English-dubbed and heavily edited version of the series was broadcast on North American and Australian television as "Star Blazers". The first two seasons ("The Quest for Iscandar" and "The Comet Empire") of this version were broadcast in Greece in 1981-82 as "Διαστημόπλοιο " ("Spaceship Argo"). An Italian-language version was also broadcast under the name "Star Blazers" in Italy, and a Portuguese-language version was successfully shown in Brazil under the title "Patrulha Estelar" ("Star Patrol") and "Viaje a la Ultima Galaxia" ("Voyage to the Final Galaxy") or "Astronave Intrepido" ("Starship Intrepid") in Spain and Latin America. 
It is a seminal series in the history of anime, marking a turn towards more complex serious works and influencing works such as "Mobile Suit Gundam" and "Neon Genesis Evangelion"; Hideaki Anno has ranked "Yamato" his favorite anime and credited it with sparking his interest in anime.
"Yamato" was the first anime series or movie to win the Seiun Award, a feat not repeated until the 1985 "Nausicaä of the Valley of the Wind".
Development.
Conceived in 1973 by producer Yoshinobu Nishizaki, the project underwent heavy revisions. Originally intended to be an outer-space variation on "Lord of the Flies," the project at first was titled "Asteroid Ship Icarus" and had a multinational teenage crew journeying through space in a hollowed-out asteroid in search of the planet Iscandar. There was to be much discord among the crew; many of them acting purely out of self-interest and for personal gain. The enemy aliens were originally called Rajendora.
When Leiji Matsumoto was brought onto the project, many of these concepts were discarded. It is his art direction, ship designs and unique style that accredit him in fans' eyes as the true creator of "Space Battleship Yamato", even though Nishizaki retains legal rights to the work.
Plot.
The first season began airing in Japan on October 6, 1974. Set in the year 2199, an alien race known as the "Gamilas" ("Gamilons" in the English "Star Blazers" dub) unleash radioactive meteorite bombs on Earth, rendering the planet's surface uninhabitable. Humanity has retreated into deep underground cities, but the radioactivity is slowly affecting them as well, with humanity's extinction estimated in one year. Earth's space fleet is hopelessly outclassed by the Gamilas and all seems lost until a message capsule from a mysterious crashed spaceship is retrieved on Mars. The capsule yields blueprints for a faster-than-light engine and an offering of help from Queen Starsha of the planet Iscandar in the Large Magellanic Cloud. She says that her planet has a device, the Cosmo-Cleaner D (Cosmo DNA), which can cleanse Earth of its radiation damage.
The inhabitants of Earth secretly build a massive spaceship inside the ruins of the gigantic Japanese battleship "Yamato" which lies exposed at the former bottom of the ocean location where she was sunk in World War II. This becomes the "Space Battleship Yamato" for which the story is titled. Using Starsha's blueprints, they equip the new ship with a space warp drive, called the "wave motion engine", and a new, incredibly powerful weapon at the bow called the "Wave Motion Gun". The Wave Motion Engine (波動エンジン, hadō enjin) is capable of converting the vacuum of space into tachyon energy, as well as functioning like a normal rocket engine, and providing essentially infinite power to the ship, it enables the "Yamato" to "ride" the wave of tachyons and travel faster than light. The Wave Motion Gun (波動砲, hadō hō), also called the Dimensional Wave Motion Explosive Compression Emitter, is the "trump card" of the Yamato that functions by connecting the Wave Motion Engine to the enormous firing gate at the ship's bow, enabling the tachyon energy power of the engine to be fired in a stream directly forwards. Enormously powerful, it can vaporize a fleet of enemy ships—or a small continent (as seen in the first season, fifth episode)—with one shot; however, it takes a brief but critical period to charge before firing.
In the English "Star Blazers" dub, the ship is noted as being the historical "Yamato", but is then renamed the "Argo" (after the ship of Jason and the Argonauts). A crew of 114 departs for Iscandar in the "Yamato" to retrieve the radiation-removing device and return to Earth within the one-year deadline. Along the way, they discover the motives of their blue-skinned adversaries: the planet Gamilas, sister planet to Iscandar, is dying; and its leader, Lord Desslar ("Desslok" in the "Star Blazers" dub), is trying to irradiate Earth enough for his people to move there, at the expense of the "barbarians" he considers humanity to be.
The first season contained 26 episodes, following the "Yamato"‍ '​s voyage out of the Milky Way Galaxy and back again. A continuing story, it features the declining health of "Yamato"‍ '​s Captain Okita (Avatar in the "Star Blazers" dub), and the transformation of the brash young orphan Susumu Kodai (Derek Wildstar) into a mature officer, as well as his budding romance with female crewmember Yuki Mori (Nova). The foreign edits tend to play up the individual characters, while the Japanese original is often more focused on the ship itself. In a speech at the 1995 Anime Expo, series episode director Noboru Ishiguro said low ratings and high production expenses forced producer Yoshinobu Nishizaki to trim down the episode count from the original 39 episodes to only 26. The 13 episodes would have introduced Captain Harlock as a new series character.
Movie edition.
The series was condensed into a 130-minute-long movie by combining elements from a few key episodes of the first season. Additional animation was created for the movie (such as the scenes on Iscandar) or recycled from the series' test footage (such as the opening sequence). The movie, which was released in Japan on August 6, 1977, was edited down further and dubbed into English in 1978; entitled "Space Cruiser Yamato" or simply "Space Cruiser", it was only given a limited theatrical release in Europe & Latin America, where it was called "Patrulha Estelar" ("Star Patrol", in Brazilian Portuguese) or "Astronave Intrepido" ("Starship Intrepid", in Spanish), though it was later released on video in most countries.
Sequels.
Farewell to Space Battleship Yamato (1978).
The success of the "Yamato" movie in Japan eclipsed that of the local release of "", leading to the production of a second movie that would end the story. Also going by the name "Arrivederci Yamato", "Farewell to Space Battleship Yamato", set in the year 2201, shows the "Yamato" crew going up against the White Comet Empire, a mobile city fortress called Gatlantis, from the Andromeda Galaxy. A titanic space battle results in the crew going out on a suicide mission to save humanity. The film has been considered as a non-canonical, alternate timeline.
Space Battleship Yamato II (1978-79).
Viewer dissatisfaction with the ending of "Farewell to Space Battleship Yamato" prompted the production of a second "Yamato" television season which retconned the film and presented a slightly different plot against Zōdah (Prince Zordar in the "Star Blazers" dub) and his Comet Empire, and ended without killing off the "Yamato" or its primary characters. Like "Farewell", the story is set in the year 2201, and expands the film story to 26 episodes. This second season featured additional plots such as a love story between Teresa (Trelaina) and "Yamato" crew member Daisuke Shima (Mark Venture), and an onboard antagonism between Kodai and Saito (Knox), leader of a group of space marines.
Footage from "Farewell to Space Battleship Yamato" was reused in the second season, particularly in the opening titles. The sequence of the "Yamato" launching from water was also reused in two of the subsequent movies.
Yamato: The New Voyage (1979).
The television movie, "Yamato: The New Voyage" (aka "Yamato: The New Journey"), came next, featuring a new enemy, the Black Nebula Empire. The story opens in late 2201. In the film, later modified into a theatrical movie, Desslar sees his homeworld, Gamilas, destroyed by the grey-skinned aliens, and its twin planet Iscandar next in line for invasion. He finds an eventual ally in the "Yamato", then on a training mission under deputy captain Kodai.
Be Forever Yamato (1980).
The theatrical movie "Be Forever Yamato", set in the year 2202, sees the Black Nebula Empire launch a powerful weapon at Earth, a hyperon bomb which will annihilate humanity if they resist a full-scale invasion. The "Yamato", under new captain, Yamanami, travels to the aliens' home galaxy only to discover what appears to be a future Earth—defeated and ruled by the enemy. Appearing in this film is Sasha, the daughter of Queen Starsha of Iscandar and Mamoru Kodai (Susumu's older brother).
Space Battleship Yamato III (1980-81).
Following these movies, a third season of the television series was produced, broadcast on Japanese television in 1980. Its date was not mentioned in the broadcast, but design documents, as well as anime industry publications, cited the year 2205. In the story, the Sun is hit by a stray proton missile from a nearby battle between forces of the Galman Empire and Bolar Federation. This missile greatly accelerates nuclear fusion in the Sun, and humanity must either evacuate to a new home or find a means of preventing a supernova. During the course of the story, it is learned that the people of the Galman Empire are actually the forebears of Desslar and the Gamilas race. Desslar and the remnants of his space fleet have found and liberated Galman from the Bolar Federation. Originally conceived as a 52-episode story, funding cuts meant the season had to be truncated to 25 episodes, with a corresponding loss of overall story development. This third season was adapted into English several years after the original "Star Blazers" run and, to the dissatisfaction of fans, used different voice actors than did the earlier seasons.
Final Yamato (1983).
Premiering in Japanese theaters on March 19, 1983, "Final Yamato" reunites the crew one more time to combat the threat of the Denguilu, a militaristic alien civilization that intends to use the water planet, Aquarius, to flood Earth and resettle there (having lost their home planet to a galactic collision). Captain Okita, who was found to be in cryogenic sleep since the first season, returns to command the "Yamato" and sacrifices himself to stop the Denguili's plan. Susumu and Yuki also get married.
The story is set in the year 2203, contradicting earlier assumptions that its predecessor, "Yamato III", took place in 2205. Having a running time of 165 minutes, Final Yamato retains the record of being the Japanese longest animated film ever made.
Yamato 2520 (1994).
In the mid-1990s, Nishizaki attempted to create a sequel to "Yamato", set hundreds of years after the original. "Yamato 2520" was to chronicle the adventures of the eighteenth starship to bear the name, and its battle against the Seiren Federation. Much of the continuity established in the original series (including the destruction of Earth's moon) is ignored in this sequel.
In place of Leiji Matsumoto, American artist Syd Mead ("∀ Gundam", "Blade Runner", "Tron" and "") provided the conceptual art.
Due to the bankruptcy of Nishizaki's company Office Academy, and legal disputes with Matsumoto over the ownership of the "Yamato" copyrights, the series was never finished and only four episodes were produced.
Space Battleship Great Yamato (2000).
"Space Battleship Great Yamato" (新宇宙戦艦ヤマト, Shin Uchū Senkan Yamato, lit. "New Space Battleship Yamato") is a graphic novel comic created by the animator Leiji Matsumoto. For a time it was streaming online. However this has since stopped.
Dai Yamato Zero-go (2004).
"Dai Yamato Zero-go" (大ヤマト零号, lit. "The Great Yamato Zero") is the second original animated video based on Space Battleship Yamato
The story begins in 3199, when a mighty enemy attacks the Milky Way from a neighbouring galaxy, and defeats the Milky Way Alliance, reducing them to just six fleets. After the Alliance headquarters is destroyed, and when the collapse of the central Milky Way Alliance is imminent, the Great Yamato "Zero" embarks on a mission to assist the Milky Way Alliance in one last great battle.
New Space Battleship Yamato, (2004, cancelled).
In March 2002, a Tokyo court ruled that Yoshinobu Nishizaki legally owned the "Yamato" copyrights. Nishizaki and Matsumoto eventually settled, and Nishizaki pushed ahead with developing a new Yamato television series. Project proposals for a 26-episode television series were drawn up in early 2004, but no further work was done with Tohoku Shinsha not backing the project. American series expert Tim Eldred was able to secure a complete package of art, mecha designs, and story outline at an auction over Japanese store Mandarake in April 2014.
Set 20 years after "Final Yamato", the series would have shown Susumu Kodai leading a salvage operation for the remains of the Yamato. The ship is rebuilt as the Earth Defense Force builds a second Space Battleship Yamato to combat the Balbard Empire, an alien race that has erected a massive honeycombed cage called Ru Sak Gar, over Earth in a bid to stop the human race's spacefaring efforts. A feature film to be released after the series ended would have featured the original space battleship fighting the Balbards' attempt to launch a black hole at Earth. Kodai, Yuki, and Sanada are the only original series characters who would have returned in the series.
Yamato: Resurrection (2009).
Although New Space Battleship Yamato was sent to the discard pile, Nishizaki began work on a new movie titled "Yamato: Rebirth" (宇宙戦艦ヤマト 復活篇 "Uchū Senkan Yamato: Fukkatsu hen") (set after the original series), while Matsumoto planned a new "Yamato series". However, additional legal conflicts stalled both projects until August 2008, when Nishizaki announced plans for the release of his film on December 12, 2009.
Set 17 years after the events of "Final Yamato", "Resurrection" brings together some members of the "Yamato" crew, who lead Earth's inhabitants to resettle in a far-flung star system after a black hole is discovered, which will destroy the solar system in three months.
Remakes.
Live-action film (2010).
Released on December 1, 2010, "Space Battleship Yamato" is the franchise's first live-action film. Directed by Takashi Yamazaki, the movie stars Takuya Kimura as Susumu Kodai and Meisa Kuroki as Yuki. It was revealed originally that the plot would be based on that of the 1974 series. However, an official trailer released during June 2010 on Japanese television has also shown elements from the series' second season (1978).
2199 (2012).
Debuting in Japanese cinemas on April 7, 2012, "2199" is a remake of the 1974 series. Yutaka Izubuchi serves as supervising director, with character designs by Nobuteru Yuki, and Junichiro Tamamori and Makoto Kobayashi in charge of mecha and conceptual designs. The series is a joint project of Enagio, Xebec and AIC. Hideaki Anno designed the new series' opening sequence.
Timeline(s).
With the retelling of "Arrivederci Yamato" as the open-ended "Yamato II" television series (ending in late 2201), "Arrivederci Yamato" was redesignated as a discardable, alternate timeline. The follow-on film, "Yamato: New Journey", took place in late 2201; and its successor, "Be Forever Yamato", in early 2202. "Yamato III" was commonly believed to be set in 2205 (several printed publications used this date, although it was never stated in the show's broadcast). But the following film, "Final Yamato", was set in 2203. The opening narration of "Final" mentioned the Bolar/Galman conflict, implying that the date for "Yamato III" was to be regarded as some time between 2202 and 2203 (making for an unrealistic and compressed timeline).
It is not known if this change was due to the lackluster response to "Yamato III", the production staff's dissatisfaction with the truncated series (additionally, Nishizaki and Matsumoto had limited involvement with it), or a mere oversight.
In 2220, the ship is rebuilt following the events of "Final Yamato". The new captain of the ship is Susumu Kodai, who was the main character in the previous movies. This is canon, as it is set 17 years after "Final Yamato". It is not known what happened between 2203 and 2220.
Space Battleship Yamato arcade game.
Space Battleship Yamato was a 1985 Japanese exclusive Laserdisc video game designed by Taito which was based on the television series of the same name.
Characters.
The "Space Battleship Yamato" series generally involves themes of brave sacrifice, noble enemies, and respect for heroes lost in the line of duty. This can be seen as early as the second episode of the first season, which recounts the defeat of the original battleship "Yamato" while sailors and pilots from both sides salute her as she sinks (this scene was cut from the English dub, but later included on the "Star Blazers" DVD release). The movies spend much time showing the crew visiting monuments to previous missions and recalling the bravery of their fallen comrades. Desslar, the enemy defeated in the first season and left without a home or a people, recognizes that his foes are fighting for the same things he fought for and, eventually, becomes Earth's most important ally.
English title.
For many years, English-language releases of the anime bore the title "Space Cruiser Yamato". This romanization has appeared in Japanese publications because Nishizaki, a sailing enthusiast who owned a cruiser yacht, ordered that this translation be used out of love for his boat. However, in reference to naval nomenclature, it is technically inaccurate, as 戦艦 "senkan" means "battleship" and not "cruiser" (which in Japanese would be 巡洋艦 "jun'yōkan"). Leiji Matsumoto's manga adaptation was titled "Cosmoship Yamato". Today, "Yamato" releases, including the Voyager Entertainment DVD, are marketed either as "Star Blazers" or "Space Battleship Yamato".
"Star Blazers" (1979) is a heavily edited dubbed version for the United States market produced by Westchester Film Corporation. Voyager Entertainment released DVD volumes and comic adaptations of the anime years later.

</doc>
<doc id="28957" url="http://en.wikipedia.org/wiki?curid=28957" title="Southern blot">
Southern blot

A Southern blot is a method used in molecular biology for detection of a specific DNA sequence in DNA samples. Southern blotting combines transfer of electrophoresis-separated DNA fragments to a filter membrane and subsequent fragment detection by probe hybridization.
The method is named after its inventor, the British biologist Edwin Southern. Other blotting methods (i.e., western blot, northern blot, eastern blot, southwestern blot) that employ similar principles, but using RNA or protein, have later been named in reference to Edwin Southern's name. As the label is eponymous, Southern is capitalised, as is conventional for proper nouns. The names for other blotting methods may follow this convention, by analogy.
Result.
Hybridization of the probe to a specific DNA fragment on the filter membrane indicates that this fragment contains DNA sequence that is complementary to the probe.
The transfer step of the DNA from the electrophoresis gel to a membrane permits easy binding of the labeled hybridization probe to the size-fractionated DNA. It also allows for the fixation of the target-probe hybrids, required for analysis by autoradiography or other detection methods.
Southern blots performed with restriction enzyme-digested genomic DNA may be used to determine the number of sequences (e.g., gene copies) in a genome. A probe that hybridizes only to a single DNA segment that has not been cut by the restriction enzyme will produce a single band on a Southern blot, whereas multiple bands will likely be observed when the probe hybridizes to several highly similar sequences (e.g., those that may be the result of sequence duplication). Modification of the hybridization conditions (for example, increasing the hybridization temperature or decreasing salt concentration) may be used to increase specificity and decrease hybridization of the probe to sequences that are less than 100% similar.
Applications.
Southern transfer may be used for homology-based cloning on the basis of amino acid sequence of the protein product of the target gene. Oligonucleotides are designed that are similar to the target sequence. The oligonucleotides are chemically synthesised, radiolabeled, and used to screen a DNA library, or other collections of cloned DNA fragments. Sequences that hybridize with the hybridization probe are further analysed, for example, to obtain the full length sequence of the targeted gene.
Second, Southern blotting can also be used to identify methylated sites in particular genes. Particularly useful are the restriction nucleases "MspI" and "HpaII", both of which recognize and cleave within the same sequence. However, "HpaII" requires that a C within that site be methylated, whereas "MspI" cleaves only DNA unmethylated at that site. Therefore, any methylated sites within a sequence analyzed with a particular probe will be cleaved by the former, but not the latter, enzyme.

</doc>
<doc id="28961" url="http://en.wikipedia.org/wiki?curid=28961" title="Standard gauge">
Standard gauge

The standard gauge (also Stephenson gauge after George Stephenson, International gauge or normal gauge) is a widely used railway track gauge. Approximately 60% of lines in the world are this gauge (see the list of countries that use the standard gauge). Except for Russia, Uzbekistan, and Finland, all high-speed lines are this gauge.
The distance between the inside edges of the rails is defined to be 1435 mm (but in the United States and Canada, it is still defined as 4 ft 8½ in which is 1435 mm).
It is also called UIC gauge, or UIC track gauge, the European gauge in the EU and Russia, or uniform gauge in Queensland.
History.
As railways developed and expanded one of the key issues was track gauge (the distance, or width, between the inner sides of the rails) to be used. The result was the adoption throughout a large part of the world of a “standard gauge” of 4 ft 8 1⁄2 in allowing inter-connectivity and inter-operability.
In England some early lines in colliery (coal mining) areas in the northeast were ; and in Scotland some early lines were (Scotch gauge). By 1846, in both countries, these lines were widened to standard gauge. Parts of the United States, mainly in the Northeast, adopted the same gauge because some early trains were purchased from Britain. However, until well into the second half of the 19th century Britain and the USA had several different track gauges. The American gauges converged as the advantages of equipment interchange became increasingly apparent; notably, the South's broad gauge was converted to be compatible with standard gauge over the course of two days beginning 31 May 1886. "See" Track gauge in the United States.
With the advent of metrication, standard gauge was redefined as 1,435 mm, a reduction of 0.1 mm, but well within existing tolerance limits. The exception is the United States, where standard gauge continues to be defined in terms of customary units.
Origins.
A popular legend that has been around since at least 1937 traces the origin of the 4 ft 8 1⁄2 in gauge even further back than the coalfields of northern England, pointing to the evidence of rutted roads marked by chariot wheels dating from the Roman Empire. Snopes categorized this legend as false but commented that “... it is perhaps more fairly labelled as 'True, but for trivial and unremarkable reasons.'" The historical tendency to place the wheels of horse-drawn vehicles approximately 5 ft apart probably derives from the width needed to fit a carthorse in between the shafts. In addition, while road-traveling vehicles are typically measured from the outermost portions of the wheel rims (and there is some evidence that the first railroads were measured in this way as well), it became apparent that for vehicles travelling on rails it was better to have the wheel flanges located "inside" the rails, and thus the distance measured on the inside of the wheels (and, by extension, the inside faces of the rail heads), was the important one.
There was no standard gauge for horse railways, but there were rough groupings: in the north of England none were less than . Wylam colliery's system, built before 1763, was ; as was John Blenkinsop's Middleton Railway, the old plateway was relaid to so that Blenkinsop's engine could be used. Others were Beamish or 4 ft 7 1⁄2 in (Bigges Main and Kenton and Coxlodge).
The English railway pioneer George Stephenson spent much of his early engineering career working for the coal mines of County Durham. He favoured for wagonways in Northumberland and Durham and used it on his Killingworth line. The Hetton and Springwell wagonways also used this gauge.
Stephenson's Stockton and Darlington railway (S&DR) was built primarily to transport coal from mines near Shildon to the port at Stockton-on-Tees. The initial gauge of was set to accommodate the existing gauge of hundreds of horse-drawn chaldron wagons that were already in use on the wagonways in the mines. The railway used this gauge for 15 years before a change was made to 4 ft 8 1⁄2 in gauge. The historic Mount Washington Cog Railway, the world's first mountain-climbing rack railway, is still in operation in the 21st century, and has used the earlier 4 ft 8 in gauge since its inauguration in 1868.
The beginnings of the gauge.
George Stephenson used the gauge (including a belated extra 1/2 in of free movement to reduce binding on curves) for the Liverpool and Manchester Railway, authorised in 1826 and opened 30 September 1830. The success of this project led to George Stephenson and his son Robert being employed to engineer several other larger railway projects.
Standard gauge.
During the "gauge war" with the Great Western Railway, standard gauge was called "narrow gauge". The modern use of narrow gauge for gauges less than standard did not arise for 20 years, until the first such locomotive-hauled passenger railway, the Festiniog.
The Royal Commission.
In 1845, in the United Kingdom of Great Britain and Ireland, a Royal Commission on Railway Gauges reported in favour of a standard gauge. In Great Britain, Stephenson's gauge was chosen on the grounds that lines of this gauge were eight times longer than those of the rival gauge adopted principally by the Great Western Railway. The subsequent Gauge Act ruled that new passenger-carrying railways in Great Britain should be built to a standard gauge of , and those in Ireland to a new standard gauge of . It allowed the broad-gauge companies in Great Britain to continue with their tracks and expanding their networks within the "Limits of Deviation" and the exceptions defined in the Act. After an intervening period of mixed-gauge operation (tracks were laid with three rails), the Great Western Railway finally converted its entire network to standard gauge in 1892.
The Royal Commission made no comment about small to-be-called "narrow"-gauge lines, such as the Festiniog Railway, which allowed a future multiplicity of small gauges in the UK; it also made no comments about future gauges in British colonies.
Regrets.
Robert Stephenson was reported to have said that if he had a second chance to choose a standard gauge, he would choose one wider than . "I would take a few inches more, but a very few".
Road vehicles.
Several states in the United States had laws requiring road vehicles to have a consistent gauge to allow them to follow ruts in the road. These gauges were similar to railway standard gauge.
References.
Works cited.
</dl>

</doc>
<doc id="28962" url="http://en.wikipedia.org/wiki?curid=28962" title="Sodium laureth sulfate">
Sodium laureth sulfate

Sodium laureth sulfate, or sodium lauryl ether sulfate (SLES), is an anionic detergent and surfactant found in many personal care products (soaps, shampoos, toothpaste etc.). SLES is an inexpensive and very effective foaming agent. SLES, SLS, ALS and sodium pareth sulfate are surfactants that are used in many cosmetic products for their cleansing and emulsifying properties. They behave similarly to soap.
Chemical structure.
Its chemical formula is CH3(CH2)10CH2(OCH2CH2)"n"OSO3Na. Sometimes the number represented by "n" is specified in the name, for example laureth-2 sulfate. The product is heterogeneous in the number of ethoxyl groups, where "n" is the mean. It is common for commercial products for "n"= 3. SLES is prepared by ethoxylation of dodecyl alcohol. The resulting ethoxylate is converted to a half ester of sulfuric acid, which is neutralized by conversion to the sodium salt. The related surfactant sodium lauryl sulfate (also known as sodium dodecyl sulfate or SDS) is produced similarly, but without the ethoxylation step. SLS and ammonium lauryl sulfate (ALS) are commonly used alternatives to SLES in consumer products.
Toxicology.
Irritation.
SLES is an irritant like many other detergents, with the irritation increasing with concentration. It has also been shown that SLES causes eye or skin irritation in experiments done on animals and humans. The related surfactant SLS is a known irritant, and research suggests that SLES can also cause irritation after extended exposure in some people.
Other.
It has long been speculated that the SLS would be carcinogenic, but this is a myth. Toxicology research by the U.S. OSHA and IARC supports the conclusions of the Cosmetic, Toiletry, and Fragrance Association (CTFA) and the American Cancer Society that SLES is not a carcinogen. The Australian government's Department of Health and Ageing and its National Industrial Chemicals Notification and Assessment Scheme (NICNAS) have determined SLS does not react with DNA .
1,4-Dioxane contamination.
Some products containing SLES have been found to also contain traces (up to 279 ppm) of 1,4-dioxane. The U.S. Food and Drug Administration recommends that these levels be monitored. The U.S. Environmental Protection Agency classifies 1,4-dioxane to be a probable human carcinogen (not observed in epidemiological studies of workers using the compound, but resulting in more cancer cases in controlled animal studies), and a known irritant with a no-observed-adverse-effects level of 400 milligrams per cubic meter at concentrations significantly higher than those found in commercial products. Under Proposition 65, 1,4-dioxane is classified in the U.S. state of California to cause cancer. The FDA encourages manufacturers to remove 1,4-dioxane, though it is not required by federal law.

</doc>
<doc id="28965" url="http://en.wikipedia.org/wiki?curid=28965" title="Saraswati River (disambiguation)">
Saraswati River (disambiguation)

Saraswati River can refer to:

</doc>
<doc id="28967" url="http://en.wikipedia.org/wiki?curid=28967" title="Simpson Desert">
Simpson Desert

The Simpson Desert is a large area of dry, red sandy plain and dunes in Northern Territory, South Australia and Queensland in central Australia. It is the fourth largest Australian desert, with an area of 176,500 km2 (68,100 sq mi) and is the world's largest sand dune desert.
The desert is underlain by the Great Artesian Basin, one of the largest inland drainage areas in the world. Water from the basin rises to the surface at numerous natural springs, including Dalhousie Springs, and at bores drilled along stock routes, or during petroleum exploration. As a result of exploitation by such bores, the flow of water to springs has been steadily decreasing in recent years. It is also part of the Lake Eyre basin. 
The Simpson Desert is an erg which contains the world's longest parallel sand dunes. These north-south oriented dunes are static, held in position by vegetation. They vary in height from 3 metres in the west to around 30 metres on the eastern side. The largest dune, Nappanerica, is 40 metres in height.
History.
The explorer Charles Sturt, who visited the region from 1844–1846, was the first European to see the desert. In 1880 Augustus Poeppel, a surveyor with the South Australian Survey Department determined the border between Queensland and South Australia to the west of Haddon Corner and in doing so marked the corner point where the States of Queensland and South Australia meet the Northern Territory. After he returned to Adelaide, it was discovered that the links in his surveyor's chain had stretched. Poeppel’s border post was too far west by 300 metres. In 1884, surveyor Larry Wells moved the post to its proper position on the eastern bank of Lake Poeppel. The tri-state border is now known as Poeppel Corner. In January 1886 surveyor David Lindsay ventured into the desert from the western edge, in the process discovering and documenting, with the help of a Wangkangurru Aboriginal man, nine native wells and travelling as far east as the Queensland/Northern Territory border. In 1936 Ted Colson became the first non-indigenous person to cross the desert in its entirety, riding camels. The name Simpson Desert was coined by Cecil Madigan, after Alfred Allen Simpson, an Australian industrialist, philanthropist, geographer, and president of the South Australian branch of the Royal Geographical Society of Australasia. Mr Simpson was the owner of the Simpson washing machine company. In 2006 Lucas Trihey was the first non-indigenous person to walk across the desert through the geographical centre away from vehicle tracks and unsupported. He carried all his equipment in a two wheeled cart and crossed from East Bore on the western edge of the desert to Birdsville in the east. In 2008, Belgian Louis-Philippe Loncke became the first non-indigenous person to complete a north-south crossing of the desert on foot and unsupported and through the geographical centre. 
In 1967, the Queensland Government established the Simpson Desert National Park.
Access.
No maintained roads cross the desert. The Donohue Highway is an unpaved outback track passing from near Boulia towards the Northern Territory border in the north of the desert. There are tracks that were created during seismic surveys in the search for gas and oil during the 1960s and 1970s. These include the French Line, the Rig Road, and the QAA Line. Such tracks are still navigable by well-equipped four-wheel-drive vehicles which must carry extra fuel and water. Towns providing access to the South Australian edge of the Simpson Desert include Innamincka to the south and Oodnadatta to the southwest; and from the eastern (Queensland) side include Birdsville, Bedourie, Thargomindah and Windorah. Last fuel on the western side is at the Mount Dare hotel and store. Before 1980, a section of the Commonwealth Railways Central Australian line passed along the western side of the Simpson Desert.
Visitor attraction.
The desert is popular with tourists, particularly in winter, and popular landmarks include the ruins and mound springs at Dalhousie Springs, Purnie Bore wetlands, Approdinna Attora Knoll and Poeppel Corner (where Queensland, South Australia and Northern Territory meet). Because of the excessive heat and inadequately experienced drivers attempting to access the desert in the past, the Department of Environment and Natural Resources has decided since 2008-2009 to close the Simpson Desert during the summer — to save unprepared "adventurers" from themselves.
Climate.
The area has an extremely hot, dry desert climate. Rainfall is minimal, averaging only about 150 mm per year and falling mainly in summer. Temperatures in summer can approach 50 °C and large sand storms are common. Winters are generally cool however heatwaves even in the middle of July are not unheard of. 
Some of the heaviest rain in decades occurred during 2009-2010, and has seen the Simpson Desert burst into life and colour. Birdsville recorded more rain in 24 hours than is usual in a whole year in early March 2010. Rain inundated Queensland’s north-west and Gulf regions. In total, 17 million megalitres of water entered the State’s western river systems leading to Lake Eyre. In 2010, researchers uncovered the courses of ancient river systems under the desert.
Ecology.
The Simpson Desert is also a large part of the World Wildlife Fund ecoregion of the same name which consists of the Channel Country and the Simpson Strzelecki Dunefields bioregions of the Interim Biogeographic Regionalisation for Australia (IBRA). 
The flora of the Simpson Desert ecoregion is limited to drought-resistant shrubs and grasses especially "Zygochloa paradoxa" grass that holds the dunes together and the spinifex and other tough grasses of sides slopes and sandy desert floor between the dunes. The Channel Country section of the ecoregion lies to the northeast of the desert proper around the towns of Bedourie and Windorah in Queensland, and consists of low hills covered with Mitchell grass cut through with rivers lined with coolabah trees. The ecoregion also includes areas of rocky upland and seasonally wet clay and salt pans, particularly Lake Eyre, the centre of one of the largest inland drainage systems in the world, including the Georgina and Diamantina Rivers. 
Wildlife adapted to this hot, dry environment and seasonal flooding includes the water-holding frog ("Litoria platycephala") and a number of reptiles that inhabit the desert grasses. Endemic mammals of the desert include the kowari ("Dasycercus byrnei") while birds include the grey grasswren ("Amytornis barbatus") and Eyrean grasswren ("Amytornis goyderi"). Lake Eyre and the other seasonal wetlands are important habitats for fish and birds, especially as a breeding ground for waterbirds while the rivers are home to birds, bats and frogs. The seasonal wetlands of the ecoregion include Lake Eyre and the Coongie Lakes as well as the swamps that emerge when Cooper Creek, Strzelecki Creek and the Diamantina River are in flood. The birds that use these wetlands include the freckled duck ("Stictonetta naevosa"), musk duck ("Biziura lobata"), silver gull ("Larus novaehollandiae"), Australian pelican ("Pelecanus conspicillatus"), great egret ("Ardea alba"), glossy ibis ("Plegadis falcinellus"), and banded stilt ("Cladorhynchus leucocephalus"). Finally the mound springs of the Great Artesian Basin are important habitat for a number of plants, fish, snails and other invertebrates.
Native vegetation is largely intact as the desert is uninhabitable. Therefore, habitats are not threatened by agriculture, but are damaged by introduced species, particularly rabbits and feral camels. The only human activitiy in the desert proper has been the construction of the gas pipelines, while the country on its fringes has been used for cattle grazing and contains towns such as Innamincka. Mound springs and other waterholes are vulnerable to overuse and damage. Protected areas of the ecoregion include the Simpson Desert, Goneaway, Lochern, Bladensburg, Witjira and Kati Thanda-Lake Eyre National Parks as well as the Simpson Desert Conservation Park, Innamincka Regional Reserve and Simpson Desert Regional Reserve. Ethabuka Reserve is a nature reserve in the north of the desert owned and managed by Bush Heritage Australia.
Dunefields.
The extensive dunefields of the Simpson Desert display a range of colours from brilliant white to dark red and include pinks and oranges.
Morphology.
The sand ridges have a trend of SSE-NNW and continue parallel for kilometers. This pattern is seen throughout the deserts of Australia. Some of the ridges continue unbroken for up to 200 km. The height and the spacing between the ridges are directly related. Where there are 5-6 ridges in a kilometer, the height is around 15 meters but when there is one or two ridges per kilometer the height jumps to 35–38 meters. (Twidale, 1980) In cross section, the Lee side is the eastern slope with an incline of 34-38 degrees, while the Stoss side is the western slope with an incline of only 10-20 degrees. In cross section, the cross beds are planar with foresets alternating between east and west. The foresets have incline angles of 10-30 degrees. (Twidale, 1980)
Sediment.
The sand is predominately made up of quartz grains. The grains are rounded and sub angular. They range in size from 0.05 mm to 1.2 mm with 0.5 mm being the average size for the crests and 0.3 mm being the average size on the dune flanks. The active crests have well sorted sand sediment but on the interdunes, the sediment is not as well sorted (Twidale, 1980). The sediment varies in color from pink to brick red, but by the rivers and playas the sediment color is light grey. The progression of the color from grey to red is due to the release of iron oxide from the sediment when weathered (Twidale, 1980).

</doc>
<doc id="28968" url="http://en.wikipedia.org/wiki?curid=28968" title="Skycar">
Skycar

Skycar may refer to:

</doc>
<doc id="28969" url="http://en.wikipedia.org/wiki?curid=28969" title="Silesian Voivodeship">
Silesian Voivodeship

Silesian Voivodeship, or Silesia Province (in Polish, "województwo śląskie" ; Czech: "Slezské vojvodství"; Slovak: "Sliezske vojvodstvo"), is a voivodeship, or province, in southern Poland, centering on the historic region known as Upper Silesia ("Górny Śląsk"), with the capital in Katowice. Contrary to the name, however, the eastern half of the Silesian Voivodeship is not identical to the historical region of Silesia, but rather that of Lesser Poland.
It was created on January 1, 1999, out of the former Katowice, Częstochowa and Bielsko-Biała Voivodeships, pursuant to the Polish local government reforms adopted in 1998.
History.
For the first time Silesian Voivodeship was appointed in Second Polish Republic. It had much wider range of power autonomy, than other contemporary Polish Voivodeships and it covered all historical lands of Upper Silesia, which ended up in the Interwar period Poland (among them: Katowice (Kattowitz), Rybnik (Rybnik), Pszczyna (Pleß), Wodzisław (Loslau), Żory (Sohrau), Mikołów (Nikolai), Tychy (Tichau), Królewska Huta (Königshütte), Tarnowskie Góry (Tarnowitz), Miasteczko Śląskie (Georgenberg), Woźniki (Woischnik), Lubliniec (Lublinitz), Cieszyn (Teschen), Skoczów (Skotschau), Bielsko (Bielitz)). This Voivodeship did not include – as opposed to the present one – lands and cities of old pre-Partition Polish–Lithuanian Commonwealth. Among the last ones the Southern part was included in Kraków Voivodeship Żywiec (Saybusch), Wilamowice (Wilmesau), Biała Krakowska (Biala) oraz Jaworzno), and the North Western part Będzin (Bendzin), Dąbrowa Górnicza (Dombrowa), Sosnowiec (Sosnowitz), Częstochowa (Tschenstochau), Myszków, Szczekociny (Schtschekotzin), Zawiercie, Sławków) belonged to Kielce Voivodeship.
After aggression of Nazi Germany (Invasion of Poland), on 8 October 1939, Hitler published a decree “About division and administration of Eastern Territories”. A Silesian Province ("Gau Schlesien") was created, with a seat in Breslau (Wrocław). It consisted of four districts: Kattowitz, Oppeln, Breslau and Liegnitz.
The following counties were included in Kattowitz District: Kattowitz, Königshütte, Tarnowitz, Beuthen Hindenburg, Gleiwitz, Freistadt, Teschen, Biala, Bielitz, Saybusch, Pleß, Sosnowitz, Bendzin and parts of the following counties: Kranau, Olkusch, Riebnich and Wadowitz. However, according to Hitler’s dectee from 12 October 1939 about establishing General Government ("Generalgouvernement"), Tschenstochau (Częstochowa) belonged to GG.
In 1941 the Silesian Province ("Provinz Schlesien") underwent new administrative division and as a result Upper Silesian Province was created ("Provinz Oberschlesien"):
After the War during 1945 - 1950 there existed a Silesian Voivodeship, commonly known as Śląsko-Dąbrowskie Voivodeship, which included a majior part of today’s Silesian Vivodeship. In 1950 Śląsko-Dąbrowskie Voivodeship was divided into Opole and Katowice Voivodeships. The latter one had borders similar to the borders of modern Silesian Voivodeship.
The present Silesian Voivodeship was formed in 1999 from the following voivodeships of the previous adminisrtrative division:
Geography.
The Silesian Voivodeship borders both the Czech Republic and Slovakia to the south. It is also bordered by four other Polish voivodeships: those of Opole (to the west), Łódź (to the north), Świętokrzyskie (to the north-east), and Lesser Poland (to the east).
The region includes the Silesian Upland ("Wyżyna Śląska") in the centre and north-west, and the Krakowsko-Częstochowska Upland ("Jura Krakowsko-Częstochowska") in the north-east. The southern border is formed by the Beskidy Mountains (Beskid Śląski and Beskid Żywiecki).
The current administrative unit of Silesian Voivodeship is just a fraction of the historical Silesia which is within the borders of today's Poland (there are also fragments of Silesia in the Czech Republic and Germany). Other parts of today's Polish Silesia are administered as the Opole, the Lower Silesian Voivodeships and the Lubusz Voivodeship. On the other hand, a large part of the current administrative unit of the Silesian Voivodeship is not part of historical Silesia (e.g., Częstochowa, Zawiercie, Myszków, Jaworzno, Sosnowiec, Żywiec, Dąbrowa Górnicza, Będzin and east part of Bielsko-Biała, which are historically parts of Lesser Poland).
Demography.
Silesian Voivodeship has the highest population density in the country (379 people per square kilometre, compared to the national average of 124). The region's considerable industrialisation gives it the lowest unemployment rate nationally (6.2%). The Silesian region is the most industrialized and the most urbanized region in Poland: 78% of its population live in towns and cities.
Tourism.
Both northern and southern Silesia is surrounded by a green belt. Bielsko-Biała is enveloped by the magnificent Beskidy Mountains which are particularly popular with winter sports fans. This genuine skier's paradise offers over 150 ski lifts and 200 kilometres of ski routes. More and more slopes are illuminated and equipped with artificial snow generators. Szczyrk, Brenna, Wisła and Ustroń are the most popular winter mountain resorts. Rock climbing sites can be found in admirable corners of Jura Krakowsko-Częstochowska. The ruins of castles forming the Eagle Nests Trail are a famous attraction of the region. While in Silesia, one cannot miss the Black Madonna's Jasna Góra Sanctuary in Częstochowa - the annual destination of over 4 million pilgrims from all over the world. In south-western Silesia is popular place for visit parks, palaces and old monastery (Rudy Raciborskie, Wodzisław Śląski). Along Odra river are interesing natural reserve and at summer places for swimming.
With its more than two centuries of industrialisation history, region has a number of technical heritage memorials. These include narrow and standard gauge railways, coal and silver mines, shafts and its equipment from 19th and 20th century.
Cities and towns.
Due to its industrial and urban nature, the voivodeship has many cities and large towns. Of Poland's 40 most-populous cities, 12 are in Silesian Voivodeship. 19 of the cities in the voivodeship have the legal status of "city-county" (see powiat). In all it has 71 cities and towns (with legal city rights), listed below in descending order of population (according to official figures for 2006):
Economy.
The Silesian voivodship is predominantly an industrial region. Most of the
mining is derived from one of the world's largest bituminous coalfields of the Upper Silesian Industrial District ("Górnośląski Okręg Przemysłowy") and the Rybnik Coal District ("Rybnicki Okręg Węglowy") with its major cities Rybnik, Jastrzębie Zdrój, Żory and Wodzisław Śląski. Lead and zinc can be found near Bytom, Zawiercie and Tarnowskie Góry; iron ore and raw materials for building - near Częstochowa. The most important regional industries are: mining, iron, lead and zinc metallurgy, power industry, engineering, automobile, chemical, building materials and textile. In the past, the Silesian economy was determined by coal mining. Now, considering the investment volume, car manufacturing is becoming more and more important. The most profitable company in the region is Fiat Auto-Poland S.A. in Bielsko-Biała with a revenue of PLN 6.2 billion in 1997. Recently a new car factory has been opened by GM Opel in Gliwice. There are two Special Economic Zones in the area: Katowice and Częstochowa. The voivodship's economy consists of about 323,000, mostly small and medium-sized, enterprises employing over 3 million people. The biggest Polish steel-works "Huta Katowice" is situated in Dąbrowa Górnicza.
Silesian Voivodship is also one of the richest regions in Poland. Average monthly salary is about 3,800 zlotys (over 1,200 EUR).
Transport.
Katowice International Airport (in Tarnowskie Góry County) is used for domestic and international flights, Other Nearby Airports are John Paul II International Airport Kraków-Balice and Warsaw Frédéric Chopin Airport. The Silesian agglomeration railway network has the largest concentration in the country. The voivodship capital enjoys good railway and road connections with Gdańsk (motorway A1) and Ostrava (motorway A1), Kraków (motorway A4), Wrocław (motorway A4), Łódź (motorway A1) and Warsaw. It is also the crossing point for many international routes like E40 connecting Calais, Brussels, Cologne, Dresden, Wrocław, Kraków and Kiev and E75 from Scandinavia to the Balkans. A relatively short distance to Vienna facilitates cross-border co-operation and may positively influence the process of European integration.
Linia Hutnicza Szerokotorowa (known by its acronym "LHS", English: "Broad gauge metallurgy line") in Sławków is the longest broad gauge railway line in Poland. The line runs on a single track for almost 400 km from the Polish-Ukrainian border, crossing it just east of Hrubieszów. It is the westernmost broad gauge railway line in Europe that is connected to the broad gauge rail system of the countries of the former Soviet Union.
Education.
There are eleven public universities in the voivodship. The biggest university is the University of Silesia in Katowice, with 43,000 students. The region's capital boasts the Medical University, The Karol Adamiecki University of Economics in Katowice, the University of Music in Katowice, the Physical Education Academy and the Academy of Fine Arts. Częstochowa is the seat of the Częstochowa University of Technology and Pedagogic University. The Silesian University of Technology in Gliwice is nationally renowned. Bielsko-Biała is home of the Technical-Humanistic Academy. In addition, 17 new private schools have been established in the region.
There are more than 300,000 people studying in the voivodship.
Politics.
The Silesian voivodeship's government is headed by the province's voivode "(governor)" who is appointed by the Polish Prime Minister. The voivode is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the sejmik "(provincial assembly)". The current voivode of Silesia is Zygmunt Łukaszczyk, whilst the present marshal is Bogusław Śmigielski.
The Sejmik of Silesia consists of 48 members.
Administrative division.
Silesian Voivodeship is divided into 36 counties (powiats). These include 19 city counties (far more than any other voivodeship) and 17 land counties. The counties are further divided into 167 gminas.
The counties are listed in the following table (ordering within categories is by decreasing population).
Protected areas.
Protected areas in Silesian Voivodeship include eight areas designated as Landscape Parks:

</doc>
<doc id="28970" url="http://en.wikipedia.org/wiki?curid=28970" title="SECD machine">
SECD machine

The SECD machine is a highly influential virtual machine and abstract machine intended as a target for functional programming language compilers. The letters stand for Stack, Environment, Control, Dump, the internal registers of the machine. These registers point to linked lists in memory. 
The machine was the first to be specifically designed to evaluate lambda calculus expressions. It was originally described by Peter J. Landin as part of his ISWIM programming language definition in 1963. The description published by Landin was fairly abstract, and left many implementation choices open (like an operational semantics). Hence the SECD machine is often presented in a more detailed form, such as Peter Henderson's Lispkit Lisp compiler, which has been distributed since 1980. Since then it has been used as the target for several other experimental compilers.
In 1989 researchers at the University of Calgary worked on a hardware implementation of the machine.
Informal description.
When evaluation of an expression begins, the expression is loaded as the only element of C. The environment E, stack S and dump D begin empty.
During evaluation of C it is converted to reverse Polish notation with ap (for apply) being the only operator. For example, the expression F (G X) (a single list element) is changed to the list X:G:ap:F:ap.
Evaluation of C proceeds similarly to other RPN expressions. If the first item in C is a value, it is pushed onto the stack S. More exactly, if the item is an identifier, the value pushed onto the stack will be the binding for that identifier in the current environment E. If the item is an abstraction, a closure is constructed to preserve the bindings of its free variables (which are in E), and it is this closure which is pushed onto the stack.
If the item is ap, two values are popped off the stack and the application done (first applied to second). If the result of the application is a value, it is pushed onto the stack.
If the application is of an abstraction to a value, however, it will result in a lambda calculus expression which may itself be an application (rather than a value), and so cannot be pushed onto the stack. In this case, the current contents of S, E, and C are pushed onto D (which is a stack of these triples), S is reinitialised to empty, and C is reinitialised to the application result with E containing the environment for the free variables of this expression, augmented with the binding that resulted from the application. Evaluation then proceeds as above.
Completed evaluation is indicated by C being empty, in which case the result will be on the stack S. The last saved evaluation state on D is then popped, and the result of the completed evaluation is pushed onto the stack contents restored from D. Evaluation of the restored state then continues as above.
If C and D are both empty, overall evaluation has completed with the result on the stack S.
Registers and memory.
The SECD machine is stack-based. Functions take their arguments from the stack. The arguments to built-in instructions are encoded immediately after them in the instruction stream.
Like all internal data-structures, the stack is a list, with the S register pointing at the list's "head" or beginning. Due to the list structure, the stack need not be a continuous block of memory, so stack space is available as long as there is a single free memory cell. Even when all cells have been used, garbage collection may yield additional free memory. Obviously, specific implementations of the SECD structure can implement the stack as a canonical stack structure, so improving the overall efficiency of the virtual machine, provided that a strict bound be put on the dimension of the stack.
The C register points at the head of the code or instruction list that will be evaluated. Once the instruction there has been executed, the C is pointed at the next instruction in the list—it is similar to an "instruction pointer" (or program counter) in conventional machines, except that subsequent instructions are always specified during execution and are not by default contained in subsequent memory locations, as it is the case with the conventional machines.
The current variable environment is managed by the E register, which points at a list of lists. Each individual list represents one environment level: the parameters of the current function are in the head of the list, variables that are free in the current function, but bound by a surrounding function, are in other elements of E.
The dump, at whose head the D register points, is used as temporary storage for values of the other registers, for example during function calls. It can be likened to the return stack of other machines.
The memory organization of the SECD machine is similar to the model used by most functional language interpreters: a number of memory cells, each of which can hold either an "atom" (a simple value, for example "13"), or represent an empty or non-empty list. In the latter case, the cell holds two pointers to other cells, one representing the first element, the other representing the list except for the first element. The two pointers are traditionally named "car" and "cdr" respectively—but the more modern terms "head" and "tail" are often used instead. The different types of values that a cell can hold are distinguished by a "tag". Often different types of atoms (integers, strings, etc.) are distinguished as well.
So a list holding the numbers "1", "2", and "3", usually written as "(1 2 3)", could be represented as follows:
 Address Tag Content (value for integers, car & cdr for lists)
 9 [ integer | 2 ]
 8 [ integer | 3 ]
 7 [ list | 8 | 0 ]
 6 [ list | 9 | 7 ]
 2 [ list | 1 | 6 ]
 1 [ integer | 1 ]
 0 [ nil ]
The memory cells 3 to 5 do not belong to our list, the cells of which can be distributed randomly over the memory. Cell 2 is the head of the list, it points to cell 1 which holds the first element's value, and the list containing only "2" and "3" (beginning at cell 6). Cell 6 points at a cell holding 2 and at cell 7, which represents the list containing only "3". It does so by pointing at cell 8 containing the value "3", and pointing at an empty list ("nil") as cdr. In the SECD machine, cell 0 always implicitly represents the empty list, so no special tag value is needed to signal an empty list (everything needing that can simply point to cell 0).
The principle that the cdr in a list cell must point at another list is just a convention. If both car and cdr point at atoms, that will yield a pair, usually written like "(1 . 2)"
Instructions.
A number of additional instructions for basic functions like car, cdr, list construction, integer addition, I/O, etc. exist. They all take any necessary parameters from the stack.

</doc>
<doc id="28971" url="http://en.wikipedia.org/wiki?curid=28971" title="Stratego">
Stratego

Stratego is a strategy board game for two players on a 10×10 square board. Each player controls 40 pieces representing individual officers and soldiers in an army. The objective of the game is to find and capture the opponent's Flag, or to capture so many enemy pieces that the opponent cannot make any further moves. Players cannot see the ranks of one another's pieces, so disinformation and discovery are important facets to gameplay.
Gameplay.
Typically, one player uses red pieces, and the other uses blue pieces. Pieces are colored on both sides, so players can easily distinguish between their own and their opponent's. Ranks are printed on one side only and placed so that players cannot identify specific opponent's pieces. Each player moves one piece per turn. If a piece is moved onto a square occupied by an opposing piece, their identities are revealed; the weaker piece (there are exceptions; see below) is removed from the board. If the weaker piece was the attacker that piece is removed from the board; if the attacker is the stronger piece, it will remove the weaker piece and occupy its square. If the engaging pieces are of equal rank, both are removed. A piece may not move onto a square already occupied unless it attacks.
Two zones in the middle of the board, each 2×2, cannot be entered by either player's pieces at any time. They are shown as lakes on the battlefield and serve as choke points to make frontal assaults less direct. The object is to capture the opponent's flag or cause a surrender. In the event of no movable pieces for a player, the opponent is the winner. The average game has 381 moves. The number of legal positions is 10115. The number of possible games is 10535.
Setup.
Players arrange their 40 pieces in a 4×10 configuration at either end of the board. Such pre-play distinguishes the fundamental strategy of particular players, and influences the outcome of the game.
Pieces.
For most pieces, rank alone determines the outcome, but there are special pieces. The most numerous special piece is the "Bomb" (each player has six; note that the Bombs cannot move) which only Miners can defuse. It immediately eliminates any other piece striking it, without itself being destroyed. Each player also has one "Spy", which succeeds only if it attacks the Marshal, or the Flag. If the Spy attacks any other piece, or is attacked by any piece (including the Marshal), the Spy is defeated.
Classic version.
From highest rank to lowest the pieces are:
Ice vs Fire.
In the new Hasbro version, though just a variant and not intended to replace the original game:
Generalities.
Each player has six Bombs and one Flag. The Flag and Bombs are the only pieces that cannot attack another piece due to being unable to move. The Bombs remain on the board, unless removed by a Miner.
All movable pieces, with the exception of the Scout, may move only one step to any adjacent space vertically or horizontally. The Scout may move any number of spaces in a straight line (such as the rook in chess). In older versions of "Stratego" the Scout could attack only if it began its turn adjacent to an enemy piece. In more recent versions of the game the Scout can move several squares, ending with attacking an enemy piece. No piece can move diagonally, or back and forth between the same two spaces for more than three consecutive turns.
Some versions (primarily those released since 2000) make 10 (the Marshal) the highest rank, while others (versions prior to 2000, as well as the Nostalgia version released in 2002) have the Marshal piece ranked at 1.
Strategy.
In contrast to chess, "Stratego" is a game with incomplete information. In this respect it resembles somewhat such chess variants as Kriegspiel or Dark chess. Collecting the information, planning, and strategic thinking play an important role in "Stratego". Psychological aspects are very important too.
Basic strategies.
Overall strategy in "Stratego" involves:
Placing the Spy too far forward, for example, makes it more likely to be captured early on, but placing it too far back may make it inaccessible when the enemy Marshal is identified. Likewise, Miners are weak, but their ability to defuse Bombs may be needed early (although some players prefer to leave Bombs "unexploded" as long as possible, particularly if they hamper an opponent's movements). The placement of "reserve troops" in the rearmost row and deployment of Scouts, which can move in an unimpeded straight line, is also a strategic point.
During game play, players must identify Bombs without sacrificing too many troops, determine the probable location of the enemy Flag, and form an attack plan that takes into account the likely ranks of the troops and exact location of the Bombs that usually surround the Flag.
Flag placement.
Since one of the win criteria is to capture the Flag, its placement is vital. It is commonly placed on the back row surrounded by two or three Bombs for protection. Some players will use this generalisation to their advantage and place the Flag somewhere unprotected, for example the Shoreline Bluff (also called "the Lakeside Bluff"), i.e. placing the Flag directly adjacent to one of the lakes where the opponent may not think to look for it.
Inexperienced players may accidentally alert an opponent to the location of their Flag by calling too much attention to it when they initially position their pieces on the board. This is often done by simply placing their Flag down first and then constructing their defenses around it. One counter measure for this is to place all the pieces on the board randomly and then rearrange them into the desired setup. This tactic became obsolete when some newer versions came supplied with a cardboard privacy screen.
Bluffing.
Some common bluffs are:
Effective scouting.
Scouts are very useful towards the end of the game, once the board is more clear. They can be used to identify bombs on the back row, reveal bluffs, or even capture the flag. Scouts are most effective when they are moved one space at a time until necessary, as the moment they move multiple spaces, they are identified as a scout.
Since they can move along a whole line, they are also effective for catching a spy daring to move into hostile territory, even when across the gameboard.
Spy strategies.
In most games, it is advisable to have the Spy shadow a General or a Colonel. These pieces are normally vulnerable to attack by the opposing Marshal. Keeping a General or Colonel in the same vicinity as the spy allows an effective retreat to where the opponent's Marshal can be ambushed by the Spy.
Spy bluffs are also effective. For example, using a Sergeant to shadow a Colonel might confuse an opponent, who might be reluctant to have their Marshal attack the Colonel.
Miner strategies.
Sophisticated players might identify opposing Bombs, but leave them in place, interfering with the enemy's movement. To do this, it is vital to memorize the location of all the opponent's Bombs as they are identified. By keeping the Miners unmoved in their territory during the early game, a player can create the Bomb bluff, in which the opposing player may mistake those unmoved Miners for Bombs.
Protecting pieces.
One of the most important concepts of "Stratego" is the incomplete knowledge and misdirection, so the manual recommends taking a piece with one that is not much stronger than it, for example take a Captain with a Major. In the same manner, one strategy is to protect with an "evens and odds" system, where a piece is protected by one two levels stronger than it, an odd piece protecting another odd piece, for example protecting the Captain with a Colonel.
Enforcing an advantage.
If a player has gained an advantage over the opponent, the advantage can be protected by attacking a high-ranked piece with one of the same rank, such as attacking a Major with another Major. This would be a significant loss for an opponent lacking a Colonel, General, or Marshal.
Attacking unknown pieces.
A risky strategy, which might be necessary when losing, is to attack unknown, unmoved pieces with a strong piece. This strategy relies on odds, for example if a player attacks an unknown, unmoved piece with a General, it would lose to the Marshal, a Bomb, or the other General. The odds are 7 in 40 of losing the General, but the odds can be improved by not attacking pieces likely to be Bombs, or by keeping track of the pieces already identified.
History.
Chinese predecessors.
The origins of "Stratego" can be traced back to traditional Chinese board game "Jungle" also known as "Game of the Fighting Animals" (Dou Shou Qi) or "Animal Chess". The game Jungle also has pieces (but of animals rather than soldiers) with different ranks and pieces with higher rank capture the pieces with lower rank. The board, with two lakes in the middle, is also remarkably similar to that in "Stratego". The major differences between the two games is that in Jungle, the pieces are not hidden from the opponent, and the initial setup is fixed.
A modern, more elaborate, Chinese game known as Land Battle Chess (Lu Zhan Qi) or Army Chess (Lu Zhan Jun Qi) is a descendant of Jungle, and a cousin of "Stratego": the initial setup is not fixed, both players keep their pieces hidden from their opponent, and the objective is to capture the enemy's flag. Lu Zhan Jun Qi's basic gameplay is similar, though differences include "missile" pieces and a Chinese Chess-style board layout with the addition of railroads and defensive "camps". A third player is also typically used as a neutral referee to decide battles between pieces without revealing their identities. An expanded version of the Land Battle Chess game also exists, adding naval and aircraft pieces and is known as Sea-Land-Air Battle Chess (Hai Lu Kong Zhan Qi).
European predecessors.
In its present form "Stratego" appeared in Europe before World War I as a game called L'attaque. Thierry Depaulis writes on "Ed's Stratego Site":
It was in fact designed by a lady, Mademoiselle Hermance Edan, who filed a patent for a 'jeu de bataille avec pièces mobiles sur damier' (a battle game with mobile pieces on a gameboard) on 1908-11-26. The patent was released by the French Patent Office in 1909 (patent #396.795 ). Hermance Edan had given no name to her game but a French manufacturer named "Au Jeu Retrouvé" was selling the game as L'Attaque as early as 1910.
Depaulis further notes that the 1910 version divided the armies into red and blue colors. The rules of L'attaque were basically the same as the game we know as "Stratego". It featured standing cardboard rectangular pieces, color printed with soldiers who wore contemporary (to 1900) uniforms, not Napoleonic uniforms.
Classic Stratego.
The modern game of Stratego, with its Napoleonic imagery, was originally manufactured in the Netherlands by Jumbo. Pieces were originally made of printed cardboard. After World War II, painted wood pieces became standard. The game was licensed by the Milton Bradley Company for American distribution, and introduced in the United States in 1961 (although it was trademarked in 1960). 
Starting in the late 1960s all versions switched plastic pieces. The change from wood to plastic was made for economical reasons, as was the case with many products during that period, but with Stratego the change also served a structural function: Unlike the wooden pieces, the plastic pieces were designed with a small base. The wooden pieces had none, often resulting in pieces tipping over. This, of course, was disastrous for that player, since it often immediately revealed the piece's rank, as well as unleashing a literal domino effect by having a falling piece knock over other pieces. European versions introduced cylindrical castle-shaped pieces that proved to be popular. American variants later introduced new rectangular pieces with a more stable base and colorful stickers, not images directly imprinted on the plastic.
The game is particularly popular in the Netherlands, Germany, and Belgium, where regular world and national championships are organized. The international Stratego scene has, more recently, been dominated by players from the Netherlands.
European versions of the game give the Marshal the highest number (10), while the initial American versions give the Marshal the lowest number (1) to show the highest value (i.e. it is the #1 or most powerful tile). More recent American versions of the game, which adopted the European system, caused considerable complaint among American players who grew up in the 1960s and 1970s. This may have been a factor in the release of a Nostalgic edition, in a wooden box, reproducing the Classic edition of the early 1970s.
Modern Stratego variations.
The Jumbo Company continues to release European editions, including a three- and four-player version, and a new "Cannon" piece (which jumps two squares to capture any piece, but loses to any attack against it). It also included some alternate rules such as "Barrage" (a quicker two-player game with fewer pieces) and "Reserves" (reinforcements in the three- and four-player games). The four-player version appeared in America in the 1990s.
Electronic Stratego was introduced by Milton Bradley in 1982. It has features that make many aspects of the game strikingly different from those of classic Stratego. Each type of playing piece in Electronic Stratego has a unique series of bumps on its bottom that are read by the game's battery-operated touch-sensitive "board". When attacking another piece a player hits their Strike button, presses their piece and then the targeted piece: the game either rewards a successful attack or punishes a failed strike with an appropriate bit of music. In this way the players never know for certain the rank of the piece that wins the attack, only whether the attack wins, fails, or ties (similar to the role of the referee in the Chinese game of Luzhanqi). Instead of choosing to move a piece, a player can opt to "probe" an opposing piece by hitting the Probe button and pressing down on the enemy piece: the game then beeps out a rough approximation of the strength of that piece. There are no bomb pieces: bombs are set using pegs placed on a touch-sensitive "peg board" that is closed from view prior to the start of the game. Hence, it is possible for a player to have their piece occupying a square with a bomb on it. If an opposing piece lands on the seemingly-empty square, the game plays the sound of an explosion and that piece is removed from play. As in classic Stratego, only a Miner can remove a bomb from play. A player who successfully captures the opposing Flag is rewarded with a triumphant bit of music from the "1812 Overture".
Variants.
Release versions.
Official Modern Version: Also known as Stratego Original. Redesigned pieces and game art. The pieces now use stickers attached to new "castle-like" plastic pieces. The stickers must be applied by the player after purchase, though the box does not mention any assembly being required. Ranking order is reversed to adopt European play style (higher numbers equals higher rank).
Science Fiction Version: Jumbo / Spin Master version of Stratego, Common in North America department stores. The game has a futuristic science fiction theme. Played on a smaller 8x10 board, with 30 pieces per player. Unique spotter unit is one of the pieces.
Nostalgia Game Series Edition: Traditional stamped plastic pieces, although the metallic paint is dull and less reflective than some older versions, and the pieces are not engraved as some previous editions were. Wooden box, traditional board and game art.
Library Edition: Hasbro's Library Series puts what appears to be the classic Stratego of the Nostalgia Edition into a compact, book-like design. The box approximates the size of a book and is made to fit in a bookcase in one's library. In this version, the scout may not move and strike in the same turn.
Stratego Onyx: Stratego Onyx is sold exclusively by Barnes & Noble. It includes foil stamped wooden game pieces and a raised gameboard with a decorative wooden frame.
Franklin Mint Civil War Collector's Edition: The Franklin Mint created a luxury version of Stratego with a Civil War theme and gold and silver plated pieces. Due to a last-minute licensing problem, the set was never officially released and offered for sale. The only remaining copies are those sent to the company's retail stores for display.
Ultimate Stratego: No longer in production, this version can still be found at some online stores and specialty gaming stores. This version is a variant of traditional Stratego and can accommodate up to 4 players simultaneously. The Ultimate Stratego board game contained four different Stratego versions: "Ultimate Lightning", "Alliance Campaign", "Alliance Lightning" and "Ultimate Campaign".
Stratego CD-ROM: No longer in production, this version can still be found in many online stores. Produced by Hasbro Interactive this game combined Classic and Ultimate Stratego to give a choice of five different versions.
Promotional.
Hertog Jan, a Dutch brand of beer, released a promotional version of Stratego with variant rules. It includes substantially fewer pieces, including only one Bomb and no Miners. Since each side has only about 18 pieces, the pieces are far more mobile. The scout in this version is allowed to move three squares in any combination of directions (including L-shapes) and there is a new piece called the archer, which is defeated by anything, but can defeat any piece other than the Bomb by shooting it from a two-square distance, in direct orthogonal, or straight, directions only. If one player is unable to move any more of his pieces, the game results in a tie because neither player's flag was captured.
Commercial.
These variants are produced by the company with pop culture themed pieces.
Produced by Avalon Hill:
Produced by USAopoly:
Competition.
Stratego is a very competitive game and this competition has increased over the years. There are now many Stratego competitions held throughout the world.
Competitive Stratego competitions are now held in all four versions of the game.

</doc>
<doc id="28972" url="http://en.wikipedia.org/wiki?curid=28972" title="Sindh">
Sindh

Sindh (Sindhi: سنڌ सिंध ; Urdu: سندھ‎ ; Latin: "Indus" ; Ancient Greek: Ἰνδός ; Sanskrit: सिंधु ) is one of the five provinces of Pakistan and historically home to the Sindhi people. It is also locally known as the Mehran. The name "Sindh" is derived from the Sanskrit "Sindhu", a reference to the Indus River that passes almost through the middle of the entire province. This river was known to the ancient Iranians in Avestan as "Hindu", in Sanskrit as "Sindhu", to Assyrians (as early as the seventh century BC) as "Sinda", to the Persians as "Ab-e-sind", to the Greeks as "Indos", to the Romans as "Indus", to the Pashtuns as "Abasind", to the Arabs as "Al-Sind", to the Chinese as "Sintow", and to the Javanese as "Santri".
Sindh is bounded to the west by Balochistan, to the north by Punjab, to the east by the Indian states of Gujarat and Rajasthan and to the south by the Arabian Sea. The capital and largest city of the province is Karachi, which is also Pakistan's largest city and the country's only financial hub. Most of the population in the province is Muslim, with sizable Hindu, Ahmadiyya, Christian, Parsi and Sikh minorities.
Origin of the name.
Sindh Province and the people inhabiting the region are named after the river known as the Sindhu before independence and now called the Indus River. In Sanskrit, "síndhu" means "river, stream", and refers to the Indus river in particular. The Greeks who conquered Sindh in 325 BC under the command of Alexander the Great rendered it as "Indós", hence the modern "Indus". The ancient Iranians referred to everything east of the river Indus as "hind" from the word "Sindh." In Persian S is sounded H. When the British arrived in the 17th century in India, they followed that regional example and applied the Greek name for Sindh to the entire South Asia, calling it "India". The word 'Sindhu' is in the Indian national anthem, as it was part of India before partition of the country.
History.
Prehistoric period.
Sindh's first known village settlements date as far back as 7000 BCE. Permanent settlements at Mehrgarh, currently in Balochistan, to the west expanded into Sindh. This culture blossomed over several millennia and gave rise to the Indus Valley Civilization around 3000 BCE. The Indus Valley Civilization rivaled the contemporary civilizations of Ancient Egypt and Mesopotamia in size and scope, numbering nearly half a million inhabitants at its height with well-planned grid cities and sewer systems.
The Indus Valley Civilization is the farthest known outpost of archaeology in prehistoric times. Evidence of a new element of pre-Harappan culture has been traced here. When the primitive village communities in Balochistan were still struggling against a difficult highland environment, a highly cultured people were trying to assert themselves at Kot Diji. This was one of the most developed urban civilizations of the ancient world. It flourished between the 25th century BC and 1500 BC in the Indus valley sites of Mohenjo-daro and Harappa. The people had a high standard of art and craftsmanship and a well-developed system of quasi-pictographic writing which remains un-deciphered despite ceaseless efforts. The remarkable ruins of the beautifully planned towns, the brick buildings of the common people, roads, public baths and the covered drainage system suggest a highly organized community.
According to some accounts, there is no evidence of large palaces or burial grounds for the elite. The grand and presumably holy site might have been the great bath, which is built upon an artificially created elevation. This indigenous civilization collapsed around 1700 BC. The cause is hotly debated, and may have been a massive earthquake, which dried up the Ghaggar River. Skeletons discovered in the ruins of Mohen Jo Daro ("mount of dead") indicate that the city was suddenly attacked and the population was wiped out.
Sindh finds mention in the Hindu epic "Mahabharata" as being part of Bharatvarsha. Sindh was conquered by the Persian Achaemenid Empire in the sixth century BC. In the late 300s BC, Sindh was conquered by a mixed army led by Macedonian Greeks under Alexander the Great, Alexander described his encounters with these trans-Indus tribes of Sindh: "I am involved in the land of a lions and brave people, where every foot of the ground is like a well of steel, confronting my soldier. You have brought only one son into the world, but everyone in this land can be called an Alexander". The region remained under control of Greek satraps for only a few decades. After Alexander's death, there was a brief period of Seleucid rule, before Sindh was traded to the Mauryan Empire led by Chandragupta in 305 BC. During the rule of the Mauryan Emperor Ashoka, the Buddhist religion spread to Sindh.
Mauryan rule ended in 185 BC with the overthrow of the last king by the Sunga Dynasty. In the disorder that followed, Greek rule returned when Demetrius I of Bactria led a Greco-Bactrian invasion of India and annexed most of northwestern lands, including Sindh. Demetrius was later defeated and killed by a usurper, but his descendants continued to rule Sindh and other lands as the Indo-Greek Kingdom. Under the reign of Menander I many Indo-Greeks followed his example and converted to Buddhism.
In the late 100s BC, Scythian tribes shattered the Greco-Bactrian empire and invaded the Indo-Greek lands. Unable to take the Punjab region, they invaded South Asia through Sindh, where they became known as Indo-Scythians (later Western Satraps). By the first century AD, the Tocharian Kushan Empire annexed Sindh. Though the Kushans were Zoroastrian and Shamanists, they were tolerant of the local Buddhist tradition and sponsored many building projects for local beliefs. Ahirs were also found in large numbers in Sindh.
Abiria country of Abhira tribe was located in southern Sindh.
The Kushan Empire was defeated in the mid 200s AD by the Sassanid Empire of Persia, who installed vassals known as the Kushanshahs in these far eastern territories. These rulers were defeated by the Kidarites in the late 300s. It then came under the Gupta Empire after dealing with the Kidarites. By the late 400s, attacks by Hephthalite tribes known as the Indo-Hephthalites or "Hunas" (Huns) broke through the Gupta Empire's northwestern borders and overran much of northwestern India. Sindh came under the rule of Emperor Harshavardhan, then the Rai Dynasty around 478 AD. The Rais were overthrown by Chachar of Alor around 632. The Brahman dynasty ruled a vast territory that stretched from Multan in the north to the Rann of Kutch, Alor was their capital.
Arrival of Islam.
In AD 711, Muhammad bin Qasim conquered the Sindh and Indus Valley, bringing South Asian societies into contact with Islam, Dahir was an unpopular Hindu king that ruled over a Buddhist majority and that Chach of Alor and his kin were regarded as usurpers of the earlier Buddhist Rai Dynasty, a view questioned by those who note the diffuse and blurred nature of Hindu and Buddhist practices in the region, especially that of the royalty to be patrons of both and those who believe that Chach himself may have been a Buddhist. The forces of Muhammad bin Qasim defeated Raja Dahir in alliance with the Jats and other regional governors.
In 711 AD, Muhammad bin Qasim led an Umayyad force of 20,000 cavalry and 5 catapults. Muhammad bin Qasim defeated the Raja Dahir, and captured the cities of Alor, Multan and Debal. Sindh became the easternmost province of the Umayyad Caliphate and was referred to as "Al-Sindh" on Arab maps, with lands further east known as "Hind". Muhammad bin Qasim built the city of Mansura as his capital; the city then produced famous historical figures such as Abu Mashar Sindhi, Abu Ata al-Sindhi, Abu Raja Sindhi and Sind ibn Ali. At the port city of Debal most of the Bawarij embraced Islam and became known as Sindhi Sailors; they became famous due to their skills in navigation, geography and languages. After Bin Qasim left, the Umayyads ruled Sindh through the Habbari dynasty.
By the year 750 AD, Debal was second only to Basra; Sindhi sailors from the port city of Debal voyaged to Basra, Bushehr, Musqat, Aden, Kilwa, Zanzibar, Sofala, Malabar, Sri Lanka and Java (where Sindhi merchants were known as the Santri). During the power struggle between the Umayyads and the Abbasids. The Habbari Dynasty became semi independent and was eliminated and Mansura was invaded by Mahmud Ghaznavi. Sindh then became an eastern most province of the Abbasid Caliphate ruled by the Soomro Dynasty until the Siege of Baghdad (1258). It should be noted that "Mansura" was the first capital of the "Soomra Dynasty" and the last of the "Habbari dynasty". Muslim geographers, historians and travelers such as al-Masudi, Ibn Hawqal, Istakhri, Ahmed ibn Sahl al-Balkhi, al-Tabari, Baladhuri, Nizami, al-Biruni, Saadi Shirazi, Ibn Battutah and Katip Çelebi wrote about or visited the region, sometimes using the name "Sindh" for the entire area from the Arabian Sea to the Hindu Kush.
Soomro period.
When Sindh was under the Umayyad Caliphate, the Habbari dynasty was in control. The Umayyads appointed "Aziz al Habbari" as the governor of Sindh. Habbaris ruled Sindh until Mahmud Ghaznavi defeated the Habbaris in 1120. Mahmud Ghaznavi viewed the Abbasids to be the Caliphs thus he removed the remaining influence of the Umayyad Caliphate in the region and Sindh fell to Abbasid control following the defeat of the Habbaris. The Abbasid Caliphate then made Al Khafif from Samarra; the term 'Soomro' means 'of Samarra' in Sindhi. The new governor of Sindh was to create a better, stronger and stable government. Once he became the governor he allotted several key positions to his family and friends; thus Al-Khafif or Sardar Khafif Soomro formed the Soomro Dynasty in Sindh and became its first king. Until the Siege of Baghdad (1258) the Soomro dynasty was the Abbasid Caliphate's functionary in Sindh, but after that it became independent. Since then some Soomras intermarried with local women and adopted some local customs as well. They were the first Muslims to translate the Quran into the Sindhi language.
When the Soomro Dynasty lost ties with the Abbasid Caliphate after the Siege of Baghdad (1258) the Soomra King Dodo-I, established their rule from the shores of the Arabian Sea to the punjab in the north and in the east to Rajistan and in the west to Pakistani Balochistan. The Soomros were one of the first Muslims in Sindh. They created a chivalrous culture in Sindh which eventually facilitated their rule centered at Mansura. Puran was later abandoned due to changes in the course of the Puran River; they ruled for the next 95 years until 1351 AD. During this period, Kutch was ruled by the Samma Dynasty, who enjoyed good relations with the Soomras in Sindh. Since the Soomro Dynasty lost its support from the Abbasid Caliphate, the Sultans of Delhi wanted a piece of Sindh. The Soomros successfully defended their kingdom for about 36 years but their dynasties soon fell to the might of the massive Sultans of Delhi armies such as the Tughluks and the Khiljis.
Samma period.
In 1339 Jam Unar founded a Sindhi Muslim Samma Dynasty and challenged the Sultans of Delhi. he used the title of the "Sultan of Sindh". The Samma tribe reached its peak during the reign of Jam Nizamuddin II (also known by the nickname Jám Nindó). During his reign from 1461 to 1509, Nindó greatly expanded the new capital of Thatta and its Makli hills, which replaced Debal. He also patronized Sindhi art, architecture and culture. The Samma had left behind a popular legacy especially in architecture, music and art. Important court figures included the famous poet Kazi Kadal, Sardar Darya Khan, Moltus Khan, Makhdoom Bilwal and the theologian Kazi Kaadan. However, Thatta was a port city; unlike garrison towns, it could not mobilize large armies against the Arghun and Tarkhan Mongol invaders, who killed many regional Sindhi Mirs and Amirs loyal to the Samma. Some Parts of sindh still remained under the Sultans of Delhi and the ruthless Arghuns and the Tarkhans sacked Thatta during the rule of Jam Ferozudin.
Kalhoro and Mughal period.
In the year 1524, the few remaining Sindhi Amirs welcomed the Mughal Empire and Babur dispatched his forces to defeat the Arghuns and the Tarkhans, who had violated the liberties of the inhabitants of the province. In the coming centuries Sindh became a region fiercely loyal to the Mughals. A network of forts manned by cavalry and musketeers further extended Mughal power in Sindh. In 1540 a deadly mutiny by Sher Shah Suri forced the Mughal Emperor Humayun to withdraw to Sindh, where he joined the Sindhi Emir Hussein Umrani. In 1541 Humayun married Hamida Banu Begum. She gave birth to the infant Akbar at Umarkot in the year 1542. In 1556 the Ottoman Admiral Seydi Ali Reis visited Humayun; various regions of the South Asia including Sindh (Makran coast and the Mehran delta) are mentioned in his book "Mirat ul Memalik". The Portuguese navigator Fernão Mendes Pinto claims that Sindhi sailors joined the Ottoman Admiral Kurtoğlu Hızır Reis on his expedetion to Aceh in 1565.
During the reign of Akbar, Sindh produced various scholars such as and others such as Mir Ahmed Nasrallah Thattvi, Tahir Muhammad Thattvi and Mir Ali Sir Thattvi and the Mughal chronicler Abu'l-Fazl ibn Mubarak and his brother the poet Faizi was a descendant of a Sindhi Shaikh family from Rel, Siwistan in Sindh. Abu'l-Fazl ibn Mubarak was the author of "Akbarnama" (an official biographical account of Akbar) and the "Ain-i-Akbari" (a detailed document recording the administration of the Mughal Empire). It was also during the Mughal period when Sindhi literature began to flourish and historical figures such as Shah Abdul Latif Bhittai, Sulatn-al-Aoliya Muhammad Zaman and Sachal Sarmast became prominent throughout the land. In the year 1603 Shah Jahan visited the province of Sindh; at Thatta he was generously welcomed by the locals after the death of his father Jahangir. Shah Jahan ordered the construction of the Shahjahan Mosque, which was completed during the early years of his rule under the supervision of Mirza Ghazi Beg. Also during his reign, in the year 1659 in the Mughal Empire, Muhammad Salih Tahtawi of Thatta created a seamless celestial globe with Arabic and Persian inscriptions using a wax casting method. Sindh was also home to very famous wealthy merchant-rulers such as Mir Bejar of Sindh, whose great wealth had attracted the alliance of Sultan bin Ahmad of Oman. After the death of Aurangzeb, the Mughal Empire and its institutions began to decline. Various warring Nawabs took control of vast territories; they ruled independently of the Mughal Emperor.
In the year 1701 the Nawab Kalhora were authorized in a firman by the Mughal Emperor Aurangzeb to administer the province of Sindh. In 1739, Main Noor Mohammad Kalhoro challenged the powerful invader Nadir Shah but failed according to legend. To avenge the massacre of his allies, the capture of Main Noor Mohammad Kalhoro and the abduction of his sons, Main Noor Mohammad Kalhoro sent a small force to assassinate Nadir Shah and turn events in favor of the Mughal Emperor during the Battle of Karnal in 1739, but remained unsuccessful. From 1752 to 1762, Marathas collected Chauth or tributes from Sindh. Maratha power greatly declined in Sindh after Panipat in 1761. In 1762, Mian Ghulam Shah Kalhoro brought stability in Sindh, he reorganized the province and independently defeated the Marathas and their prominent vassal the "Rao of Kuch" in the Thar Desert and returned victoriously. After the Sikhs annexed Multan, the Kalhora Dynasty supported counterattacks against the Sikhs and defined their borders, however due to the lack of internal stability the Kalhoras could not continue further conquests. In the year 1783 Akbar Shah II issued a firman, which designated Mir Fateh Ali Khan Talpur baloch as the new "Nawab of Sindh", and mediated peace particularly after the ferocious Battle of Halani and the defeat of the ruling Kalhora by the Talpur baloch tribes.
British Raj.
In 1802, when Mir Ghulam Ali Khan Talpur Baloch succeeded as the Nawab, internal tension broke out in the province. As a result, the following year the Maratha Empire declared war on Sindh and Berar Subah, during which Arthur Wellesley took a leading role — causing much early suspicion between the Emirs of Sindh and the British Empire. The British East India Company made its first contacts in the Sindhi port city of Thatta, which according to a report was:
"a city as large as London containing 50,000 houses which were made of stone and mortar with large verandahs some three or four stories high ... the city has 3,000 looms ... the textiles of Sindh were the flower of the whole produce of the East, the international commerce of Sindh gave it a place among that of Nations, Thatta has 400 schools and 4,000 Dhows at its docks, the city is guarded by well armed Sepoys".
British and Bengal Presidency forces under General Charles James Napier arrived in Sindh in the nineteenth century and conquered Sindh in 1843. The Baloch coalition led by Talpur Balochs under Mir Nasir Khan Talpur Baloch was defeated at the Battle of Miani, during which 5,000 Talpur Baloch were killed. Shortly afterward, Hoshu Sheedi commanded another army at the Battle of Dubbo, where 5,000 Baloch were killed. The first Agha Khan helped the British in their conquest of Sindh. As result he was granted a lifetime pension. A British journal by Thomas Postans mentions the captive Sindhi Amirs: "The Amirs as being the prisoners of "Her Majesty"... they are maintained in strict seclusion; they are described as Broken-Hearted and Miserable men, maintaining much of the dignity of fallen greatness, and without any querulous or angry complaining at this unlivable source of sorrow, refusing to be comforted".
Within weeks, Charles Napier and his forces occupied Sindh. After 1853, the British divided Sindh into districts and later made it part of British India's Bombay Presidency. Sindh became a separate province in 1935.
Sibghatullah Shah Rashidi pioneered the Sindhi Muslim Hur Movement against the British Raj. He was hanged on 20March 1943 in Hyderabad, Sindh. His burial place is not known. During the British period, railways, printing presses and bridges were introduced in the province. Writers like Mirza Kalich Beg compiled and traced the literary history of Sindh.
Pakistan Resolution in the Sindh Assembly.
The Sindh assembly was the first British Indian legislature to pass the resolution in favour of Pakistan. Influential Sindhi activists under the supervision of G.M. Syed and other important leaders at the forefront of the provincial autonomy movement joined the Muslim League in 1938 and presented the Pakistan resolution in the Sindh Assembly. In 1890 Sindh acquired representation for the first time in the Bombay Legislative Assembly. Four members represented Sindh. Those leaders and many others from Sindh played an important role in ensuring the separation of Sindh from the Bombay Presidency, which took place on 1April 1936.
The newly created province of Sindh secured a legislative assembly of its own, elected on the basis of communal and minorities' representation. Sir Lancelot Graham was appointed as the first governor of Sindh by the British government on 1April 1936. He was also the Head of the Council, which comprised 25 members, including two advisors from the Bombay Council to administer the affairs of Sindh until 1937. The British ruled the area for a century. According to English explorer Richard Burton, Sindh was one of the most restive provinces during the British Raj and was, at least originally, home to many prominent Muslim leaders such as Ubaidullah Sindhi and Muhammad Ali Jinnah (neither of whom were Sindhi) who strove for greater Muslim autonomy. At the 27th Session of the Muslim League at Lahore on March23, 1940, Sir Haji Abdullah Haroon was among those who spoke and endorsed the Pakistan Resolution.
Creation of Pakistan.
On 14 August 1947, Pakistan was created according to the two nation theory. The province of Sindh attained self-rule for the first time since the defeat of Sindhi Talpur Amirs in the Battle of Miani on 17 February 1843. The first major challenge faced by the Government of Pakistan was the settlement of over 2 million Muhajirs from India who began migrating into newly created Pakistan. In 1947 Sindh joined Pakistan by vote of members of legislature.
Geography and climate.
Sindh is located on the western corner of South Asia, bordering the Iranian plateau in the west. Geographically it is the third largest province of Pakistan, stretching about 579 km from north to south and 442 km (extreme) or 281 km (average) from east to west, with an area of 140915 km2 of Pakistani territory. Sindh is bounded by the Thar Desert to the east, the Kirthar Mountains to the west, and the Arabian Sea in the south. In the centre is a fertile plain around the Indus River.
Sindh lies in a tropical to subtropical region; it is hot in the summer and mild to warm in winter. Temperatures frequently rise above 46 °C between May and August, and the minimum average temperature of 2 °C occurs during December and January in the northern and higher elevated regions. The annual rainfall averages about seven inches, falling mainly during July and August. The southwest monsoon wind begins to blow in mid-February and continues until the end of September, whereas the cool northerly wind blows during the winter months from October to January.
Sindh lies between the two monsoons—the southwest monsoon from the Indian Ocean and the northeast or retreating monsoon, deflected towards it by the Himalayan mountains—and escapes the influence of both. The region's scarcity of rainfall is compensated by the inundation of the Indus twice a year, caused by the spring and summer melting of Himalayan snow and by rainfall in the monsoon season.
Sindh is divided into three climatic regions: Siro (the upper region, centred on Jacobabad), Wicholo (the middle region, centred on Hyderabad), and Lar (the lower region, centred on Karachi). The thermal equator passes through upper Sindh, where the air is generally very dry. Central Sindh's temperatures are generally lower than those of upper Sindh but higher than those of lower Sindh. Dry hot days and cool nights are typical during the summer. Central Sindh's maximum temperature typically reaches 43 -. Lower Sindh has a damper and humid maritime climate affected by the southwestern winds in summer and northeastern winds in winter, with lower rainfall than Central Sindh. Lower Sindh's maximum temperature reaches about 35 -. In the Kirthar range at 1800 m and higher at Gorakh Hill and other peaks in Dadu District, temperatures near freezing have been recorded and brief snowfall is received in the winters.
Flora and fauna.
The province is mostly arid with scant vegetation except for the irrigated Indus Valley. The dwarf palm, "Acacia Rupestris" (kher), and "Tecomella undulata" (lohirro) trees are typical of the western hill region. In the Indus valley, the "Acacia nilotica" (babul) (babbur) is the most dominant and occurs in thick forests along the Indus banks. The "Azadirachta indica" (neem) (nim), "Zizyphys vulgaris" (bir) (ber), "Tamarix orientalis" (jujuba lai) and "Capparis aphylla" (kirir) are among the more common trees.
Mango, date palms, and the more recently introduced banana, guava, orange, and chiku are the typical fruit-bearing trees. The coastal strip and the creeks abound in semi-aquatic and aquatic plants, and the inshore Indus delta islands have forests of "Avicennia tomentosa" (timmer) and "Ceriops candolleana" (chaunir) trees. Water lilies grow in abundance in the numerous lake and ponds, particularly in the lower Sindh region.
Among the wild animals, the "Sindh ibex" (sareh), blackbuck, wild sheep (urial or gadh) and black bear are found in the western rocky range, whereas the leopard is now rare and the Asiatic cheetah already extinct. The pirrang (large tiger cat or fishing cat) of the eastern desert region is also disappearing. Deer occur in the lower rocky plains and in the eastern region, as do the striped hyena (charakh), jackal, fox, porcupine, common gray mongoose, and hedgehog. The Sindhi phekari, ped lynx or Caracal cat, is found in some areas. Phartho (hog deer) and wild bear occur particularly in the central inundation belt. There are a variety of bats, lizards, and reptiles, including the cobra, lundi (viper), and the mysterious Sindh krait of the Thar region, which is supposed to suck the victim's breath in his sleep. Crocodiles are rare and inhabit only the backwaters of the Indus, eastern Nara channel and Karachi backwater. Besides a large variety of marine fish, the plumbeous dolphin, the beaked dolphin, rorqual or blue whale, and a variety of skates frequent the seas along the Sindh coast. The pallo (sable fish), a marine fish, ascends the Indus annually from February to April to spawn. The Indus river dolphin is among the most endangered species in Pakistan and is found in the part of the Indus river in northern Sindh. Hog deer and wild bear occur particularly in the central inundation belt.
There are also varieties of bats, lizards, and reptiles, including the cobra, lundi (viper). Some unusual sightings of Asian cheetah occurred in 2003 near the Balochistan Border in Kirthar mountains. The pirrang (large tiger cat or fishing cat) of the eastern desert region is also disappearing. Deer occur in the lower rocky plains and in the eastern region, as do the striped hyena (charakh), jackal, fox, porcupine, common gray mongoose, and hedgehog. Crocodiles are rare and inhabit only the backwaters of the Indus, the eastern Nara channel and some population of marsh crocodiles can be very easily seen in the waters of Haleji Lake near Karachi. Besides a large variety of marine fish, the plumbeous dolphin, the beaked dolphin, rorqual or blue whale, and a variety of skates frequent the seas along the Sindh coast. The pallo (sable fish), though a marine fish, ascends the Indus annually from February to April to spawn. The rare houbara bustard also find Sindh's warm climate suitable to rest and mate. Unfortunately, it is being hunted by locals and foreigners alike.
Although Sindh has a semi arid climate, through its coastal and riverine forests, its huge fresh water lakes and mountains and deserts, Sindh supports a large amount of varied wildlife. Due to the semi arid climate of Sindh the left out forests support an average population of jackals and snakes. The national parks established by the Government of Pakistan in collaboration with many organizations such as World Wide Fund for Nature and Sindh Wildlife Department support a huge variety of animals and birds. The Kirthar National Park in the Kirthar range spreads over more than 3000 km2 of desert, stunted tree forests and a lake. The KNP supports Sindh ibex, wild sheep (urial) and black bear along with the rare leopard. There are also occasional sightings of The Sindhi phekari, ped lynx or Caracal cat. There is a project to introduce tigers and Asian elephants too in KNP near the huge Hub Dam Lake. Between July and November when the monsoon winds blow onshore from the ocean, giant olive ridley turtles lay their eggs along the seaward side. The turtles are protected species. After the mothers lay and leave them buried under the sands the SWD and WWF officials take the eggs and protect them until they are hatched to protect them from predators.
Demographics and society.
Sindh has the 2nd highest Human Development Index out of all of Pakistan's provinces at 0.628. The 1998 Census of Pakistan indicated a population of 30.4 million. Just under half of the population are urban dwellers, mainly found in Karachi, Hyderabad, Sukkur, Mirpurkhas, Nawabshah District, Umerkot and Larkana. Sindhi is the sole official language of Sindh since the 19th century.
The Sindhis as a whole are composed of original descendants of an ancient population known as Sammaat, various sub-groups related to the Baloch origin are found in interior Sindh and to a lesser extent Sindhis of Pashtun origins. Sindhis of Balochi origins make up about 30% of the total Sindhi population (they however speak Sindhi Saraiki as their native tongue), while Urdu-speaking Muhajirs make up over 19% of the total population of the province while Punjabi are 10% and Pashtuns represent 7%.
A significant number of Hindu Sindhis can be also found in India, they emigrated to Republic of India following religious violence during independence.
Languages.
According to the 1998 Population Census of Pakistan following are the Major Languages of the Province 
Other languages include Kashmiri, Gujarati, Memoni, Dari, Kutchi, Khowar, Shina, Kashmiri, Bangali , Lari (dialect), and Brahui.
Sindhi language.
Sindhī (Arabic script: سنڌي) is spoken by more than 35 million people(in 2011) in the province of Sindh. However 25% people are Sindhi-speaking in the largest city of Karachi Sindh, Pakistan. Karachi is also populated by migrants from India who speak Urdu . The other migrated inhabitants of the city are Biharis from Bangladesh, Pashtuns from Khyber Pakhtunkhwa, Punjabis from various parts of Punjab and other linguistic groups of Pakistan. Most of these Urdu-speaking people sought refuge in the city from India during the independence of Pakistan, and they settled in Karachi, Hyderabad, Sukkar and other cities in Sindh.
Sindhi is an Indo-European language, linguistically considered to be the sister language of Sanskrit. Balochi, Gujarati, Rajasthani language have influences of Sindhi and Sanskrit however accommodating substantial Persian, Turkish and Arabic words.
In Pakistan Sindhi is written in a modified Arabic script, where the majority of the Sindhi population is Muslim. Hindu Sindhis who migrated to India after independence (currently are about 6 million) still register their mother tongue as Sindhi, meanwhile 7 million Hindu Sindhis are living in Pakistan.
Key dialects: Kutchi, Lasi, Parkari, Memoni, Lari, Vicholi, Utradi, Macharia, Dukslinu (spoken by Hindu Sindhi) and Siraki. During British colonial period, Siraiki evolved as a separate language.
Parkari Koli language.
Parkari Koli (sometimes called just "Parkari") is a language mainly spoken in the province of Sindh, Pakistan. It has 250,000 speakers (1995).
Religion.
Sindh's population is mainly Muslim (81.32%), and Sindh is also home to nearly all (93%) of Pakistan's Hindus, who form 16% of the province's population. The majority of Muslims are Sunni Hanafi followed by Shia Ithnā‘ashariyyah. The non-Muslim communities includes Hindus, Ahmadiyya, Christians, and Zoroastrians 2% of Sindh. A large number of Hindus migrated to India after independence of Pakistan in 1947 while Muslim refugees, Muhajirs, arrived from India.
Government.
The Provincial Assembly of Sindh is unicameral and consists of 168 seats, of which 5% are reserved for non-Muslims and 17% for women. The provincial capital of Sindh is Karachi. The government is presided over by the Chief Minister of Sindh. Most of the Sindhi tribes in the province are involved in Pakistan's politics. Sindh is a stronghold of the centre-left Pakistan Peoples Party (PPP), which is the largest political party in the province.
Districts.
There are 29 districts in Sindh, Pakistan.
Major cities.
Many of the settlements in Sindh are located on, or close to the River Indus. This is similar to Egypt, where many settlements are located on the Nile.
Economy.
Sindh has the second largest economy in Pakistan. Its GDP per capita was $1,400 in 2010 which is 50 per cent more than the rest of the nation or 35 per cent more than the national average. Historically, Sindh's contribution to Pakistan's GDP has been between 30% to 32.7%. Its share in the service sector has ranged from 21% to 27.8% and in the agriculture sector from 21.4% to 27.7%. Performance wise, its best sector is the manufacturing sector, where its share has ranged from 36.7% to 46.5%. Since 1972, Sindh's GDP has expanded by 3.6 times.
Endowed with coastal access, Sindh is a major centre of economic activity in Pakistan and has a highly diversified economy ranging from heavy industry and finance centred in and around Karachi to a substantial agricultural base along the Indus. Manufacturing includes machine products, cement, plastics, and various other goods.
Sindh is Pakistan's most natural gas producing province.
Agriculture is very important in Sindh with cotton, rice, wheat, sugar cane, dates, bananas, and mangoes as the most important crops. Sindh is the richest province of Pakistan in natural resources of gas, petrol, and coal.
Education.
This is a chart of the education market of Sindh estimated by the government in 1998.
Major public and private educational institutes of Sindh include:
Culture.
Sindhi culture is known all over the world for its arts, crafts and heritage.
Arts and crafts.
The traditions of Sindhi craftwork reflect the cumulative influence of 5000 years of invaders and settlers, whose various modes of art were eventually assimilated into the culture. The elegant floral and geometrical designs that decorate everyday objects—whether of clay, metal, wood, stone or fabric—can be traced to Muslim influence.
Though chiefly an agricultural and pastoral province, Sindh has a reputation for ajraks, pottery, leatherwork, carpets, textiles, and silk cloths which, in design and finish, are matchless. The chief articles produced are blankets, coarse cotton cloth (soosi), camel fittings, metalwork, lacquered work, enamel, gold and silver embroidery. Hala is famous for pottery and tiles; Boobak for carpets; Nasirpur, Gambat and Thatta for cotton lungees and khes. Other popular crafts include the earthenware of Johi, the metal vessels of Shikarpur, the ralli quilt, embroidery and leather articles of Tharparkar, and the lacquered work of Kandhkot.
Prehistoric finds from archaeological sites like Mohenjo-daro, engravings in various graveyards, and the architectural designs of Makli and other tombs have provided ample evidence of the people's literary and musical traditions.
Painting and calligraphy have also developed in recent times. Some young trained men have taken up commercial art.
Cultural heritage.
Sindh has a rich heritage of traditional handicraft that has evolved over the centuries. Perhaps the most professed exposition of Sindhi culture is in the handicrafts of Hala, a town some 30 kilometres from Hyderabad. Hala's artisans manufacture high-quality and impressively priced wooden handicrafts, textiles, paintings, handmade paper products, and blue pottery. Lacquered wood works known as Jandi, painting on wood, tiles, and pottery known as Kashi, hand woven textiles including "khadi", "susi", and "ajraks" are synonymous with Sindhi culture preserved in Hala's handicraft.
The Small and Medium Enterprises Authority (SMEDA) is planning to set up an organization of artisans to empower the community. SMEDA is also publishing a directory of the artisans so that exporters can directly contact them. Hala is the home of a remarkable variety of traditional crafts and traditional handicrafts that carry with them centuries of skill that has woven magic into the motifs and designs used.
Sindh is known the world over for its various handicrafts and arts. The work of Sindhi artisans was sold in ancient markets of Damascus, Baghdad, Basra, Istanbul, Cairo and Samarkand. Referring to the lacquer work on wood locally known as Jandi, T. Posten (an English traveller who visited Sindh in the early 19th century) asserted that the articles of Hala could be compared with exquisite specimens of China. Technological improvements such as the spinning wheel (charkha) and treadle (pai-chah) in the weaver's loom were gradually introduced and the processes of designing, dyeing and printing by block were refined. The refined, lightweight, colourful, washable fabrics from Hala became a luxury for people used to the woolens and linens of the age.
The ajrak has existed in Sindh since the birth of its civilization. The colour blue is predominantly used for ajraks. Sindh was traditionally a large producer of indigo and cotton cloth and both used to be exported to the Middle East. The ajrak is a mark of respect when it is given to an honoured guest or friend. In Sindh, it is most commonly given as a gift at Eid, at weddings, or on other special occasions like homecoming.
The ralli (also known as rilli, rehli, rallee, gindi or other names), or patchwork quilt, is another Sindhi icon and part of the heritage and culture. Most Sindhi homes have many rallis—one for each member of the family and a few spare for guests. The ralli is made with small pieces of cloth of different geometrical shapes sewn together to create intricate designs. They may be used as a bedspread or a blanket, and are often given as gifts to friends and guests.
Many women in rural Sindh are skilled in the production of caps. Sindhi caps are manufactured commercially on a small scale at New Saeedabad and Hala New. These are in demand with visitors from Karachi and other places; however, these manufacturing units have a limited production capacity.
Sindhi people began celebrating Sindhi Topi Day on December6, 2009 to preserve the historical culture of Sindh by wearing Ajrak and Sindhi topi.
Different non-governmental organisations (NGOs) such as the World Wildlife Fund, Pakistan, also play an important role to promote the culture of Sindh. They also provide training to women artisans in the interior of Sindh so these females get a source of generating income for themselves. For this purpose they are promoting their products under the name of "Crafts Forever".
Places of interest.
Sindh has numerous tourist sites. Modern tourist sites include certain recent resorts, amusement parks, water parks and golf clubs. The most commonly known are Arena, Aladdin Amusement Park, Go-Aish and Sindbad.
Historical tourist sites include the ruins of Mohenjo-daro near the city of Larkana, Runi Kot, Jamshoro, Kot Deji, the Jain temples of Nangar Parker and the historic temple of Sadhu Bela, Sukkur. Islamic architecture is quite prominent in the province; its numerous mausoleums include the ancient Shahbaz Qalander mausoleum.

</doc>
<doc id="28975" url="http://en.wikipedia.org/wiki?curid=28975" title="Super Bowl III">
Super Bowl III

Super Bowl III was the third AFL-NFL Championship Game in professional American football, the first to officially bear the name "Super Bowl". The game, played on January 12, 1969, at the Orange Bowl in Miami, Florida, is regarded as one of the greatest upsets in American sports history. The heavy underdog American Football League (AFL) champion New York Jets defeated the National Football League (NFL) champion Baltimore Colts by a score of 16–7. This was the first Super Bowl victory for the AFL.
Entering Super Bowl III most sports writers and fans believed that AFL teams were less talented than NFL clubs, and expected the Colts to defeat the Jets by a wide margin. Baltimore posted a 13–1 record during the 1968 NFL season before defeating the Cleveland Browns, 34–0, in the 1968 NFL Championship Game. The Jets finished the 1968 AFL season at 11–3, and defeated the Oakland Raiders, 27–23, in the 1968 AFL Championship Game.
Undaunted, Jets quarterback Joe Namath made an appearance three days before the Super Bowl at the Miami Touchdown Club and brashly guaranteed a victory. His team backed up his words by controlling most of the game, and built a 16–0 lead through the fourth quarter off of a touchdown run by Matt Snell and three field goals by Jim Turner. Colts quarterback Earl Morrall threw three interceptions before being replaced by Johnny Unitas, who then led Baltimore to its only touchdown during the last few minutes of the game. Namath, who completed 17 out of 28 passes for 206 yards, was named the Super Bowl's Most Valuable Player, despite not throwing a touchdown pass in the game or any passes at all in the fourth quarter.
This was the first Super Bowl to feature famous celebrities during its ceremonies instead of just college bands; comedian Bob Hope led a pregame ceremony honoring the Apollo astronauts.
A Super Bowl between the Colts and Jets cannot occur again, barring a future shift to the National Football Conference (NFC) by either one. After the AFL-NFL Merger in 1970, the Colts were one of three teams moved to the newly formed American Football Conference (AFC), the same conference as the Jets. The former Super Bowl combatants became divisional rivals in the AFC East until the 2002 realignment shifted the Colts, who by then had moved to Indianapolis, to the new AFC South.
Background.
The game was awarded to Miami on May 14, 1968 at the owners meetings held in Atlanta. 
Professional football.
The National Football League had dominated professional football from its origins after World War I. Rival leagues had crumbled or merged with it, and when the American Football League began to play in 1960, it was the fourth to hold that similar name to challenge the older NFL. Unlike its earlier namesakes, however, this AFL was able to command sufficient financial resources to survive; one factor in this was becoming the first league to sign a television contract—previously, individual franchises had signed agreements with networks to televise games. The junior league proved successful enough, in fact, to make attractive offers to players. After the 1964 season, in fact, there had been a well-publicized bidding war which culminated with the signing, by the AFL's New York Jets (formerly New York Titans), of University of Alabama quarterback Joe Namath for an unprecedented contract. Fearing that bidding wars over players would become the norm, greatly increasing labor costs, NFL owners, ostensibly led by league Commissioner Pete Rozelle, obtained a merger with the AFL. That merger agreement provided for a single draft, interleague play in the pre-season, a championship game to follow each season, and the integration of the two leagues into one in a way to be agreed at a future date. As the two leagues had an unequal number of teams (under the new merger agreement, the NFL expanded by one team to 16, and the AFL by one to 10), realignment was advocated by some owners, but was opposed. Eventually, three NFL teams (Cleveland Browns, Pittsburgh Steelers, and the Baltimore Colts) agreed to move over to join the original AFL franchises of 1960 in what became the American Football Conference.
Despite the ongoing merger, it was a commonly held view that the NFL was a far superior league. This was seemingly confirmed by the results of the first two interleague championship games, in January 1967 and 1968, in which the NFL champion Green Bay Packers, coached by the legendary Vince Lombardi, easily defeated the AFL's Kansas City Chiefs and Oakland Raiders. Although publicized as the inter-league championship games, it wasn't until later that the moniker for this championship contest between the now two conferences (National and American) began having the nickname of "Super Bowl" applied to it by the media and later began being counted by using roman numerals, the creation of the term being credited to then-owner of the Kansas Chiefs Lamar Hunt.
Baltimore Colts.
The Baltimore Colts had won the 1958 and 1959 NFL championships under Coach Weeb Ewbank. In the following years, however, the Colts failed to make the playoffs, and the Colts dismissed Ewbank after a 7–7 record in 1962. He was soon hired by New York's new AFL franchise, which had just changed its name from the Titans to the Jets. In Ewbank's place, Baltimore hired an untested young head coach, Don Shula, who would also go on to become one of the game's greatest coaches. The Colts did well under Shula, despite losing to the Cleveland Browns in the 1964 NFL Championship Game and, in 1965, losing in overtime to the Green Bay Packers in a tie-breaking game to decide the NFL Western Division championship. The Colts finished a distant second in the West to the Packers in 1966, and in 1967, with the NFL divided into four divisions of four teams each, went undefeated with two ties through their first 13 games, but lost the game and the Coastal Division championship to the Los Angeles Rams on the final Sunday of the season—under newly instituted tiebreakers procedures, L. A. won the division championship as it had better net points in the two games the teams played (the Rams win and an earlier tie). The Colts finished 11–1–2, out of the playoffs. In 1968, Shula and the Colts were considered a favorite to win the NFL championship again, which carried with it an automatic berth what was now becoming popularly known as the "Super Bowl" against the champion of the younger AFL. The NFL champion, in both cases the Green Bay Packers, had easily won the first two Super Bowls (1967 and 1968) over the AFL winner, establishing for a while then the superiority of the older NFL circuit.
Baltimore's quest for a championship seemed doomed from the start when long-time starting quarterback Johnny Unitas suffered a pre-season injury to his throwing arm and was replaced by Earl Morrall, a veteran who had started inconsistently over the course of his 12 seasons with four different teams. But Morrall would go on to have the best year of his career, leading the league in passer rating (93.2) during the regular season. His performance was so impressive that Colts coach Don Shula decided to keep Morrall in the starting lineup after Unitas was healthy enough to play. The Colts had won ten games in a row, including four shutouts, and finished the season with an NFL-best 13–1 record. In those ten games, they had allowed only seven touchdowns. Then, the Colts avenged their sole regular-season loss against the Cleveland Browns by crushing them, 34–0, in the NFL Championship Game.
The Colts offense ranked second in the NFL in points scored (402). Wide receivers Jimmy Orr (29 receptions, 743 yards, 6 touchdowns) and Willie Richardson (37 receptions, 698 yards, 8 touchdowns) provided Baltimore with two deep threats, with Orr averaging 25.6 yards per catch, and Richardson averaging 18.9. Tight end John Mackey also recorded 45 receptions for 644 yards and 5 touchdowns. Pro Bowl running back Tom Matte was the team's top rusher with 662 yards and 9 touchdowns. He also caught 25 passes for 275 yards and another touchdown. Running backs Terry Cole and Jerry Hill combined for 778 rushing yards and 236 receiving yards.
The Colts defense led the NFL in fewest points allowed (144, tying the then all-time league record), and ranked third in total rushing yards allowed (1,339). Bubba Smith, a 6'7" 295-pound defensive end considered the NFL's best pass rusher, anchored the line. Linebacker Mike Curtis was considered one of the top linebackers in the NFL. Baltimore's secondary consisted of defensive backs Bobby Boyd (8 interceptions), Rick Volk (6 interceptions), Lenny Lyles (5 interceptions), and Jerry Logan (3 interceptions). The Colts were the only NFL team to routinely play a zone defense. That gave them an advantage in the NFL because the other NFL teams were inexperienced against a zone defense. (This would not give them an advantage over the upstart New York Jets, however, because zone defenses were common in the AFL and the Jets knew how to attack them.)
New York Jets.
The New York Jets, led by head coach Weeb Ewbank (who was the head coach of the Colts when they won the famous 1958 NFL Championship game and later the '59 title also), finished the season with an 11–3 regular season record (one of the losses was to the Oakland Raiders in the infamous "Heidi Game") and had to rally to defeat those same Raiders, 27–23, in a thrilling AFL Championship Game.
Jets quarterback Joe Namath threw for 3,147 yards during the regular season, but completed just 49.2 percent of his passes, and threw more interceptions (17) than touchdowns (15). Still, he led the offense effectively enough for them to finish the regular season with more total points scored (419) than Baltimore. More importantly, Namath usually found ways to win. For example, late in the fourth quarter of the AFL championship game, Namath threw an interception that allowed the Raiders to take the lead. But he then made up for his mistake by completing 3 consecutive passes on the ensuing drive, advancing the ball 68 yards in just 55 seconds to score a touchdown to regain the lead for New York. Future Hall of Fame wide receiver Don Maynard caught the game-winning pass in the end zone but strained his hamstring on the play.
The Jets had a number of offensive weapons that Namath used. Maynard had the best season of his career, catching 57 passes for 1,297 yards (an average of 22.8 yards per catch) and 10 touchdowns. Wide receiver George Sauer, Jr. recorded 66 receptions for 1,141 yards and 3 touchdowns. The Jets rushing attack was also effective. Fullback Matt Snell, a power runner, was the top rusher on the team with 747 yards and 6 touchdowns, while elusive halfback Emerson Boozer contributed 441 yards and 5 touchdowns. Meanwhile, kicker Jim Turner made 34 field goals and 43 extra points for a combined total of 145 points.
The Jets defense led the AFL in total rushing yards allowed (1,195). Gerry Philbin, John Elliott, and Verlon Biggs anchored the defensive line. The Jets linebacking core was led by middle linebacker Al Atkinson. The secondary was led by defensive backs Johnny Sample (a former Colt who played on their 1958 NFL Championship team) who recorded 7 interceptions, and Jim Hudson, who recorded 5.
Several of the Jets' players had been cut by NFL teams. Maynard had been cut by the New York Giants after they lost the 1958 NFL Championship to the Colts. "I kept a little bitterness in me," he says. Sample had been cut by the Colts. "I was almost in a frenzy by the time the game arrived," he says. "I held a private grudge against the Colts. I was really ready for that game. All of us were." Offensive tackle Winston Hill had been cut five years earlier by the Colts as a rookie in training camp. "Ordell Braase kept making me look bad in practice," he says. Hill would be blocking Braase in Super Bowl III.
At an all-night party to celebrate the Jets victory over the Raiders at Namath's nightclub, Bachelors III, Namath poured champagne over Johnny Carson as the talk show host commented, "First time I ever knew you to waste the stuff."
Super Bowl pregame news and notes.
After winning the AFL championship, Namath was asked by a reporter to state his opinion of Earl Morrall. Namath said that at least four AFL quarterbacks were better than Morrall: Himself, his backup (38-year old Babe Parilli), John Hadl of the San Diego Chargers, and Bob Griese of the Dolphins.
"The Guarantee".
Despite the Jets' accomplishments, AFL teams were generally not regarded as having the same caliber of talent as NFL teams. However, three days before the game, an intoxicated Namath appeared at the Miami Touchdown Club and boldly predicted to the audience, "We're gonna win the game. I guarantee it." Jets' head coach Weeb Ewbank, in an NFL Films segment, once joked that he "could have shot" Namath for the statement. Namath made his famous "guarantee" in response to a rowdy Colts supporter at the club, who boasted the Colts would easily defeat the Jets. Namath said he never intended to make such a public prediction, and never would have done so if he had not been confronted by the fan. Nevertheless, his comments and subsequent performance in the game itself are one of the more famous instances in NFL lore.
Some analysts suggested that the Jets' record in the NFL might have been 9–5, which would have made them unlikely to have made the 1968 NFL playoffs altogether, let alone competitive against the dominant Colts.
Despite this, the AFL champions shared the confident feelings of their quarterback. According to Matt Snell, all of the Jets, not just Namath, were insulted and angry that they were 18-point underdogs. Most of the Jets considered the Raiders, whom they barely beat (27–23) in the AFL title game, a better team than the Colts. However, watching films of the Colts and in preparation for the game, Jets coaching staff and offensive players noted that their offense was particularly suited against the Colts defense. The Colts defensive schemes relied on frequent blitzing, which covered up weak points in pass coverage. The Jets had an automatic contingency for such blitzes by short passing to uncovered tight ends or backs. After a film session the Wednesday prior to the game, Jets tight end Pete Lammons, a Crockett, Texas, native, was heard to drawl, "Damn, y'all, we gotta stop watching these films. We gonna get overconfident."
Television and entertainment.
The game was broadcast in the United States by NBC Sports – at the time, still a "Service of NBC News" – with Curt Gowdy handling the play-by-play duties and joined by color commentators Al DeRogatis and Kyle Rote in the broadcast booth. Also helping with NBC's coverage were Jim Simpson (reporting from the sidelines) and Pat Summerall, on loan from CBS (helping conduct player interviews for the pregame show, along with Rote). In an interview later done with NFL Films, Gowdy called it the most memorable game he ever called because of its historical significance.
While the Orange Bowl was sold out for the game, the live telecast was not shown in Miami due to both leagues' unconditional blackout rules at the time.
"Mr. Football" was the title of the pregame show, which featured marching bands playing "Mr. Touchdown U.S.A." as people in walking footballs representing all NFL and AFL teams except the Jets and Colts were paraded, after which performers representing a Jets player and a Colts player appeared on top of a large, multi-layered, smoke topped cake. Astronauts of the "Apollo 8" mission (Frank Borman, Jim Lovell, and William Anders), the first manned flight around the Moon, which had returned to Earth just 18 days prior to the game, then led the Pledge of Allegiance. Lloyd Geisler, first trumpeter of the Washington National Symphony Orchestra, performed the national anthem. The Florida A&M University band was featured during the "America Thanks" halftime show.
This game is thought to be the earliest surviving Super Bowl game preserved on videotape in its entirety, save for a portion of the Colts' fourth quarter scoring drive. The original NBC broadcast was aired as part of the NFL Network "Super Bowl Classics" series.
Game summary.
New York entered the game with their primary deep threat, wide receiver Don Maynard, playing with a pulled hamstring. But his 112-yard, two touchdown performance against the Oakland Raiders in the AFL championship game made the Colts defense pay special attention to him, not realizing he was injured. Using Maynard as a decoy—he had no receptions in the game—Joe Namath was able to take advantage of single coverage on wide receiver George Sauer, Jr.. (After studying the Colts' zone defense, Ewbank had told his receivers, "Find the dead spots in the zone, hook up, and Joe will hit you.") The Jets had a conservative game plan, emphasizing the run as well as short high-percentage passes to minimize interceptions. Meanwhile, with the help of many fortunate plays, the Jets defense kept the Colts offense from scoring for most of the game.
1st half.
The Jets, led by captains Namath and Johnny Sample, and Colts, led by captains Preston Pearson, Johnny Unitas and Lenny Lyles, met at midfield where referee Tom Bell announced the Jets had won the coin toss and had elected to receive the football. The coin toss had been conducted an hour prior to kickoff but this was done for the benefit of the spectators. Colts kicker Lou Michaels kicked the ball off to Earl Christy who returned the ball 25 yards to the Jets' 23-yard line. Namath handed the ball off to Snell on first down who carried it 3 yards. On second down, Snell carried the ball for 9 yards, earning the Jets' first first down of the game. Colts free safety Rick Volk sustained a concussion when he tackled Snell and was subsequently lost for the game. On the ensuing play, Emerson Boozer lost four yards when he was tackled behind the line of scrimmage by Don Shinnick. Namath threw his first pass to Snell that gained 9 yards on 2nd and 14, but a 2-yard loss by Snell on the following play forced the Jets to punt the ball.
The Colts began their first offensive series on their own 27-yard line. Quarterback Earl Morrall completed a 19-yard pass to tight end John Mackey and then running back Tom Matte ran for 10 yards to place the ball on the Jets' 44-yard line. Jerry Hill's runs of 7 and 5 yards picked up another Colts first down, then Morrall's pass to tight end Tom Mitchell gained 15 yards on third and thirteen and saw the ball placed at the Jets' 19-yard line. In scoring position, Morrall attempted to score quickly against a reeling Jets defense. Receiver Willie Richardson dropped Morrall's pass on first down followed by an incompletion on second down after Mitchell was overthrown. On third down, none of his receivers were open and Morrall was tackled at the line of scrimmage by Al Atkinson. Michaels was brought out to attempt a 27-yard field goal, but it was wide left. "You could almost feel the steam go out of them," said Snell.
On the Jets' second possession, Namath threw deep to Maynard, who, despite his pulled hamstring, was open by a step. The ball was overthrown, but this one play helped change the outcome of the game. Fearing the speedy Maynard, the Colts decided to rotate their zone defense to help cover Maynard, leaving Sauer covered one-on-one by Lenny Lyles, helping Sauer catch 8 passes for 133 yards, including a crucial third quarter 39-yard reception that kept a scoring drive alive. The Jets kept rushing Snell to their strong left, rushing off tackle with Boozer blocking the linebacker, and gained first down after first down as the Colts defense gave ground. The Colts defense was more concerned about Maynard, the passing game, and the deep threat of a Namath to Maynard touchdown. Although the Colts were unaware of Maynard's injury, the Jets "were" aware that Lyles had been weakened by tonsillitis all week, causing them great glee when they saw the one-on-one matchup with Sauer.
With less than two minutes left in the period, Colts punter David Lee booted a 51-yard kick that pinned the Jets back at their own 4-yard line. Three plays later, Sauer caught a 3-yard pass from Namath, but fumbled while being tackled by Lyles, and Baltimore linebacker Ron Porter recovered it at New York's 12-yard line. However, on third down (the second play of the second quarter), Morrall's pass was tipped by Jets linebacker Al Atkinson, bounced crazily, high into the air off tight end Tom Mitchell, and was intercepted by Jets cornerback Randy Beverly in the end zone for a touchback. "That was the game in a nutshell," says Matte. Starting from their own 20-yard line, Snell rushed on the next 4 plays, advancing the ball 26 yards. The Jets would have success all day running off left tackle behind the blocking of Winston Hill, who, according to Snell, was overpowering 36-year-old defensive end Ordell Braase, the man who had tormented the rookie Hill in Colts' training camp. Said Snell, "Braase pretty much faded out." Namath later completed 3 consecutive passes, moving the ball to the Colts 23-yard line. Boozer gained just 2 yards on the next play, but Snell followed it up with a 12-yard reception at the 9-yard line and a 5-yard run to the 4-yard line, and capped the drive with a 4-yard touchdown run, once again off left tackle. The score gave the Jets a 7–0 lead, and marked the first time in history that an AFL team led in the Super Bowl.
On Baltimore's ensuing drive, a 30-yard completion from Morrall to running back Tom Matte helped the Colts advance to the New York 42-yard line, but they once again failed to score as Jets cornerback Johnny Sample broke up Morrall's third down pass and Michaels missed his second field goal attempt, this time from 46 yards. Two plays after the Jets took over following the missed field goal, Namath's 36-yard completion to Sauer enabled New York to eventually reach the Baltimore 32-yard line. But Namath then threw two incompletions, and was sacked on third down by Colts linebacker Dennis Gaubatz for a 2-yard loss. New York kicker Jim Turner tried to salvage the drive with a 41-yard field goal attempt, but he missed.
On their ensuing possession, Baltimore went from their own 20-yard line to New York's 15-yard line in three plays, aided by Matte's 58-yard run. But with 2 minutes left in the half, Morrall was intercepted again, by Sample at the Jets' 2-yard line, deflating the Colts considerably. The Jets then were forced to punt on their ensuing drive, and the Colts advanced the ball to New York's 41-yard line. What followed is one of the most famous plays in Super Bowl history. Baltimore tried a flea flicker play, which had a huge impact on the momentum of the game. Matte ran off right tackle after taking a handoff, then pitched the ball back to Morrall. The play completely fooled the NBC Camera Crew, and the Jets defense, leaving receiver Jimmy Orr wide open near the end zone. However, Morrall failed to spot him and instead threw a pass intended for running back Jerry Hill that was intercepted by Jets safety Jim Hudson as time expired, maintaining the Jets' 7–0 lead at halftime. Earlier in the season, against the Atlanta Falcons, on the same play, Morrall had completed the same pass for a touchdown to Orr, the play's intended target. "I was the primary receiver," Orr said later. "Earl said he just didn't see me. I was open from here to Tampa." "I'm just a lineman, but I looked up and saw Jimmy open," added center Bill Curry. "I don't know what happened." Some speculated that Morrall couldn't see Orr because the Florida A&M marching band (in blue uniforms similar to the Colts) was gathering behind the end zone for the halftime show.
2nd half.
The third quarter belonged to the Jets, who controlled the ball for all but three minutes of the period. Baltimore ran only seven offensive plays all quarter, gaining only 11 yards. Matte lost a fumble on the first play from scrimmage in the second half, yet another demoralizing event, which led to Turner's 32-yard field goal to increase the Jets' lead, 10–0. Then, after forcing the Colts to punt again, Namath completed 4 passes for 40 yards to set up Turner's 30-yard field goal to increase the lead, 13–0. On that drive, Namath temporarily went out of the game after injuring his right thumb, and was replaced by backup quarterback Babe Parilli for a few plays. Namath returned by the end of the third quarter, but the Jets would not run a pass play for the entire fourth quarter.
Matt Snell said, "By this time, the Colts were pressing. You saw the frustration and worry on all their faces." After Turner's second field goal, with 4 minutes left in the third quarter, Colts head coach Don Shula took Morrall out of the game and put in the sore-armed Johnny Unitas to see if he could provide a spark to Baltimore's offense. Unitas could not get the Colts offense moving on their next drive and they were forced to punt again after 3 plays. Then, aided by a 39-yard pass from Namath to Sauer, the Jets drove all the way to the Colts 2-yard line. Baltimore's defense wouldn't quit, and kept them out of the end zone. Turner kicked his third field goal early in the final period to make the score 16–0.
On Baltimore's next possession, they managed to drive all the way to the Jets' 25-yard line. However, Beverly ended the drive by intercepting a pass from Unitas in the end zone, the Jets' fourth interception of the game. New York then drove to the Colts 35-yard line with 7 consecutive running plays, but ended up with no points after Turner missed a 42-yard field goal attempt.
Unitas started out the next drive with 3 incomplete passes, but completed a key 17-yard pass to Orr on fourth down. Ten plays later, aided by three Jets penalties, Baltimore finally scored a touchdown on a 1-yard run by Hill to cut their deficit to 16–7, but with only 3:19 left in the game. The Colts then recovered an onside kick and drove to the Jets 19-yard line with 3 consecutive completions by Unitas, but his next 3 passes fell incomplete. Instead of kicking a field goal and attempting another onside kick (which would have been necessary in the end), they opted to throw on 4th down, and the pass fell incomplete, turning the ball over on downs. That ended any chance of a Baltimore comeback, as the Jets ran the ball for 6 plays before being forced to punt.
When the Colts got the ball back, only 8 seconds remained in the game. The Colts then attempted two final passes before time ran out, and the game was over. Matt Snell said, "Leaving the field, I saw the Colts were exhausted and in a state of shock. I don't remember any Colt coming over to congratulate me." As he ran off the field, Namath, in a spontaneous show of defiance held up his index finger, signaling "number one."
Years later Morrall said, "I thought we would win handily. We'd only lost twice in our last 30 games. I'm still not sure what happened that day at the Orange Bowl, however; it's still hard to account for." Wrote Matt Snell, "The most distinct image I have from that whole game is of Ordell Braase and some other guys—not so much Mike Curtis--having a bewildered look."
Namath finished the game having completed 17 of his 28 passes. He is the only quarterback to win Super Bowl MVP without throwing a touchdown pass. Snell rushed for 121 yards on 30 carries with a touchdown, and caught 4 passes for 40 yards. Sauer caught eight passes for 133 yards. Beverly became the first player in Super Bowl history to record 2 interceptions. Morrall had a terrible day—just 6 of 17 completions for 71 yards, and was intercepted 3 times. Despite not being put into the game until late in the third quarter, Unitas finished with more pass completions (11) and passing yards (110) than Morrall, but he also threw one interception. Matte was the Colts' top rusher with 116 yards on just 11 carries, an average of 10.5 yards per run, and caught 2 passes for 30 yards. The Colts were minus-4 in turnovers throwing four interceptions, all of which were deep in Jet territory.
Box score.
"at Miami Orange Bowl, Miami, Florida
Final Statistics.
Source: 
Individual leaders.
1Completions/Attempts
2Carries
3Long gain
4Receptions

</doc>
<doc id="28976" url="http://en.wikipedia.org/wiki?curid=28976" title="Super Bowl XX">
Super Bowl XX

Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46–10, capturing their first NFL championship since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans, Louisiana.
To date, it is the fourth, and most recent, Super Bowl where both teams were making their Super Bowl debuts. The Bears entered the game after becoming the second team in NFL history to win 15 regular season games. With their then-revolutionary 46 defense, Chicago led the league in several defensive categories, outscored their opponents with a staggering margin of 456–198, and recorded two postseason shutouts. The Patriots were considered a cinderella team during the 1985 season, and posted an 11–5 regular season record, but entered the playoffs as a wild card because of tiebreakers. But defying the odds, New England posted three road playoff wins to advance to Super Bowl XX.
In their victory over the Patriots, the Bears set Super Bowl records for sacks (seven), fewest rushing yards allowed (seven), and margin of victory (36 points). New England took what was – until Super Bowl XLI in 2007 – the quickest lead in Super Bowl history, with Tony Franklin's 36-yard field goal 1:19 into the first quarter after a Chicago fumble. But the Patriots were eventually held to negative yardage (−19) throughout the entire first half, and finished with just 123 total yards from scrimmage, the second lowest total yards in Super Bowl history, behind the Minnesota Vikings (119 total yards) in Super Bowl IX. Bears defensive end Richard Dent, who had 1.5 quarterback sacks, forced two fumbles, and blocked a pass, was named the game's Most Valuable Player.
The telecast of the game on NBC was watched by an estimated 92.57 million viewers. To commemorate the 20th Super Bowl, all previous Super Bowl MVPs were honored during the pregame ceremonies.
Background.
NFL owners awarded the hosting of Super Bowl XX to New Orleans, Louisiana on December 14, 1982 at an owners meeting held in Dallas. This was the sixth time that New Orleans hosted the Super Bowl. Tulane Stadium was the site of Super Bowls IV, VI, and IX; while the Louisiana Superdome previously hosted XII and XV.
Today, Super Bowl XX remains the last Super Bowl to feature two teams both making their first appearance in the game. It was the fourth overall following Super Bowl I, Super Bowl III, and Super Bowl XVI. Any future Super Bowl that would have such a combination would have to have the Detroit Lions playing either the Cleveland Browns, Houston Texans, or Jacksonville Jaguars in the game. Both the Lions and Browns won NFL championships before the Super Bowl era. It was also the last Super Bowl until Super Bowl XXXIV between the St. Louis Rams and Tennessee Titans to feature two teams who had never previously won a Super Bowl.
The nation's recognition of the Bears' accomplishment was overshadowed by the Space Shuttle "Challenger" disaster two days later, an event which caused the cancellation of the Bears' post-Super Bowl White House visit; the surviving members of the team eventually would be invited to the White House in 2011.
Chicago Bears.
Under head coach Mike Ditka, who won the 1985 NFL Coach of the Year Award, the Bears went 15–1 in the regular season, becoming the second NFL team to win 15 regular season games, while outscoring their opponents with a staggering margin of 456–198.
The Bears' defense, the "46 defense", allowed the fewest points (198), fewest total yards (4,135), and fewest rushing of any team during the regular season (1,319). They also led the league in interceptions (34) and ranked third in sacks (64).
Pro Bowl quarterback Jim McMahon provided the team with a solid passing attack, throwing for 2,392 yards and 15 touchdowns, while also rushing for 252 yards and three touchdowns. Running back Walter Payton, who was then the NFL's all-time leading rusher with 14,860 yards, rushed for 1,551 yards. He also caught 49 passes for 483 yards, and scored 11 touchdowns. Linebacker Mike Singletary won the NFL Defensive Player of the Year Award by recording three sacks, three fumble recoveries, and one interception.
But one of the most distinguishable players on defense was a large rookie lineman named William "The Refrigerator" Perry. Perry came into training camp before the season weighing over 380 pounds. But after Bears defensive coach Buddy Ryan told the press that the team "wasted" their first round draft pick on him, Perry lost some weight and ended up being an effective defensive tackle. He got even more attention when Ditka started putting him in the game at the fullback position during offensive plays near the opponent's goal line. During the regular season, Perry rushed for 2 touchdowns, caught a pass for another touchdown, and was frequently a lead blocker for Payton during goal line plays.
The Bears "46 defense" also had the following impact players: On the defensive line, Pro Bowler and future Hall of Famer Richard Dent led the NFL in sacks for the second year in a row with 17, while Pro Bowler and future Hall of Famer Dan Hampton recorded 6.5 sacks, and nose tackle Steve McMichael compiled 8. In addition to Singletary, linebacker Otis Wilson had 10.5 sacks and 3 interceptions, while Wilber Marshall recorded 4 interceptions. In the secondary, defensive back Leslie Frazier had 6 interceptions, Mike Richardson recorded 4 interceptions, Dave Duerson had 5 interceptions, and Gary Fencik recorded 5 interceptions and 118 tackles.
Chicago's main offensive weapon was Payton and the running game. A big reason for Payton's success was fullback Matt Suhey as the primary lead blocker. Suhey was also a good ball carrier, rushing for 471 yards and catching 33 passes for 295 yards. The team's rushing was also aided by Pro Bowlers Jim Covert and Jay Hilgenberg and the rest of the Bears' offensive line.
In their passing game, the Bears' primary deep threat was wide receiver Willie Gault, who caught 33 passes for 704 yards, an average of 21.3 yards per catch, and returned 22 kickoffs for 557 yards and a touchdown. Tight end Emery Moorehead was another key contributor, catching 35 passes for 481 yards. Wide receiver Dennis McKinnon was another passing weapon, recording 31 receptions, 555 yards, and 7 touchdowns. On special teams, Kevin Butler set a rookie scoring record with 144 points, making 31 of 37 field goals (83%) and 51 of 51 extra points.
Meanwhile, the players brought their characterizations to the national stage with the "Super Bowl Shuffle", a rap song the Bears recorded during the season. Even though it was in essence a novelty song, it actually peaked at #41 on the Billboard charts and received a Grammy nomination for best R&B song by a group.
New England Patriots.
The Patriots were a cinderella team during the 1985 season because many sports writers and fans thought they were lucky to make the playoffs at all. New England began the season losing three of their first five games, but won six consecutive games to finish with an 11–5 record. However, the 11–5 mark only earned them third place in the AFC East behind the Miami Dolphins and the New York Jets.
Quarterback Tony Eason, in his third year in the NFL, was inconsistent during the regular season, completing 168 out of 299 passes for 2,156 yards and 11 touchdowns, but also 17 interceptions. Eason suffered an injury midway through the season and was replaced by backup Steve Grogan, who was considered one of the best reserve quarterbacks in the league. Grogan was the starter in six of the Patriots' games, and finished the regular season with 85 out of 156 completions for 1,311 yards, 7 touchdowns, and 5 interceptions.
Wide receiver Stanley Morgan provided the team with a good deep threat, catching 39 passes for 760 yards and 5 touchdowns. On the other side of the field, multi-talented wide receiver Irving Fryar was equally effective, catching 39 passes for 670 yards, while also rushing for 27 yards, gaining another 559 yards returning punts and kickoffs, and scoring 10 touchdowns. But like the Bears, the Patriots' main strength on offense was their rushing attack. Halfback Craig James rushed for 1,227 yards, caught 27 passes for 370 yards, and scored 7 touchdowns. Fullback Tony Collins rushed for 657 yards, recorded a team-leading 52 receptions for 549 yards, and scored 5 touchdowns. The Patriots also had an outstanding offensive line, led by Pro Bowl tackle Brian Holloway and future Hall of Fame guard John Hannah.
New England's defense ranked 5th in the league in fewest yards allowed (5,048). Pro Bowl linebacker Andre Tippett led the AFC with 16.5 sacks and recovered 3 fumbles. Pro Bowl linebacker Steve Nelson was also a big defensive weapon, excelling at pass coverage and run stopping. Also, the Patriots' secondary only gave up 14 touchdown passes during the season, second fewest in the league. Pro Bowl defensive back Raymond Clayborn recorded 6 interceptions for 80 return yards and 1 touchdown, while Pro Bowler Fred Marion had 7 interceptions for 189 return yards.
Playoffs.
In the playoffs, the Patriots qualified as the AFC's second wild card.
But the Patriots, under head coach Raymond Berry, defied the odds, beating the New York Jets 26–14, Los Angeles Raiders 27–20, and the Dolphins 31–14 – all on the road – to make it to the Super Bowl. The win against Miami had been especially surprising, not only because Miami was the only team to beat Chicago in the season, but also because New England had not won in the Orange Bowl (Miami's then-home field) since 1966, the Dolphins' first season (then in the AFL). The Patriots had lost to Miami there 18 consecutive times, including a 30–27 loss in their 15th game of the season. But New England dominated the Dolphins in the AFC Championship Game, recording two interceptions from quarterback Dan Marino and recovering 4 fumbles. New England remains the only team to finish third in their division and qualify for the Super Bowl in the same season.
Meanwhile, the Bears became the first and only team in NFL history to shut out both of their opponents in the playoffs, beating the New York Giants 21–0 and the Los Angeles Rams 24–0.
Super Bowl pregame hype.
Much of the Super Bowl pregame hype centered on Bears quarterback Jim McMahon. First, he was fined by the NFL during the playoffs for a violation of the league's dress code, wearing a head band on which he had handwritten "Adidas". He then started to wear a head band saying "Rozelle", after then-league commissioner Pete Rozelle.
McMahon suffered a strained glute as the result of a hit taken in the NFC Championship Game and flew his acupuncturist into New Orleans to get treatment. During practice four days before the Super Bowl, he wore a hand band reading "Acupuncture". During a Bears practice before the Super Bowl, McMahon mooned a helicopter that was hovering over the practice.
Another anecdote involving McMahon during the Super Bowl anticipation was the New Orleans’ press reporting quote attributed McMahon, which referred to the women of New Orleans as "sluts".
This caused wide controversy among the women of New Orleans and McMahon publicly apologized on sports radio, denouncing the claim as false, indicating that he couldn’t have said such things simply because he’s a late-sleeper, and wouldn’t have been up that early in the morning to publicly smear the women of New Orleans.
Television and entertainment.
The NBC telecast of the game, with play-by-play announcer Dick Enberg and color commentators Merlin Olsen and Bob Griese (who was not in the booth with Enberg and Olsen), garnered the third highest Nielsen rating of any Super Bowl to date, a 48.3 but it ended up being the first Super Bowl to garner over 90 million viewers the highest to date up to that point. While Dick Enberg, Merlin Olsen and Bob Griese called the game, Bob Costas and his "NFL '85" castmates, Ahmad Rashad and Pete Axthelm anchored the pregame, halftime and postgame coverage. Other contributors included Charlie Jones (recapping Super Bowl I) and Bill Macatee. Also, the pregame coverage included what became known as "the silent minute"; a 60-second countdown over a black screen (a concept devised by then-NBC Sports executive Michael Weisman); a skit featuring comedian Rodney Dangerfield and an interview by NBC Nightly News anchor Tom Brokaw of United States President Ronald Reagan at the White House (this would not become a regular Super Bowl pregame feature until Super Bowl XLIII; when Today show host Matt Lauer interviewed U.S. President Barack Obama).
To celebrate the 20th Super Bowl game, the Most Valuable Players of the previous Super Bowls were featured during the pregame festivities. After trumpeter Wynton Marsalis performed the national anthem, Bart Starr, MVP of Super Bowl I and Super Bowl II, tossed the coin.
The performance event group Up with People performed during the halftime show titled "Beat of the Future". Up with People dancers portrayed various scenes into the future. This was the last Super Bowl to feature Up with People as a halftime show, though they later performed in the Super Bowl XXV pregame show. The halftime show was dedicated to the memory of Dr. Martin Luther King, Jr. (the first observance of Martin Luther King, Jr. Day had been held the previous Monday).
"The Last Precinct" debuted on NBC after the game.
Super Bowl XX was simulcast in Canada on CTV and also broadcast on Channel 4 in the United Kingdom.
Super Bowl XX is featured on "NFL's Greatest Games" under the title "Super Bears" with narration by Don LaFontaine.
Game summary.
The Patriots took the then-quickest lead in Super Bowl history after linebacker Larry McGrew recovered a fumble from Walter Payton at the Chicago 19-yard line on the second play of the game (the Bears themselves would break this record in Super Bowl XLI when Devin Hester ran back the opening kickoff for a touchdown). Bears quarterback Jim McMahon took responsibility for this fumble after the game, saying he had called the wrong play. This set up Tony Franklin's 36-yard field goal 1:19 into the first quarter after three incomplete passes by Tony Eason (the first of which starting tight end Lin Dawson went down with torn ligaments in his knee). "I looked up at the message board," said Chicago linebacker Mike Singletary, "and it said that 15 of the 19 teams that scored first won the game. I thought, yeah, but none of those 15 had ever played the Bears." Chicago struck back with a 7-play, 59-yard drive, featuring a 43-yard pass completion from McMahon to wide receiver Willie Gault, to set up a field goal from Kevin Butler, tying the score 3–3.
After both teams traded punts, Richard Dent and linebacker Wilber Marshall shared a sack on Eason, forcing a fumble that lineman Dan Hampton recovered on the Patriots 13-yard line. Chicago then drove to the 3-yard line, but had to settle for another field goal from Butler after rookie defensive lineman William "The Refrigerator" Perry was tackled for a 1-yard loss while trying to throw his first NFL pass on a halfback option play. On the Patriots' ensuing drive, Dent forced running back Craig James to fumble, which was recovered by Singletary at the 13-yard line. Two plays later, Bears fullback Matt Suhey scored on an 11-yard touchdown run to increase the lead to 13–3.
New England took the ensuing kickoff and ran one play before the first quarter ended, which resulted in positive yardage for the first time in the game (a 3-yard run by James). But after an incomplete pass and a 4-yard loss, they had to send in punter Rich Camarillo again, and receiver Keith Ortego returned the ball 12 yards to the 41-yard line. The Bears subsequently drove 59 yards in 10 plays, featuring a 24-yard reception by Suhey, to score on McMahon's 2-yard touchdown run to increase their lead, 20–3. After the ensuing kickoff, New England lost 13 yards in 3 plays and had to punt again, but got the ball back with great field position when defensive back Raymond Clayborn recovered a fumble from Suhey at their own 46-yard line. On the punt, Ortego forgot what the play call was for the punt return, and the ensuing chaos resulted in him being penalized for running after a fair catch and teammate Leslie Frazier suffering a knee injury, which ended his career.
Patriots coach Raymond Berry then replaced Eason with Steve Grogan, who had spent the previous week hoping he would have the opportunity to step onto NFL's biggest stage. "I probably won't get a chance," he had told reporters a few days before the game. "I just hope I can figure out some way to get on the field. I could come in on the punt-block team and stand behind the line and wave my arms, or something." But on his first drive, Grogan could only lead them to the 37-yard line, and they decided to punt rather than risk a 55-yard field goal attempt. The Bears then marched 72 yards in 11 plays, moving the ball inside the Patriots 10-yard line. New England kept them out of the end zone, but Butler kicked his third field goal on the last play of the half to give Chicago a 23–3 halftime lead.
The end of the half was controversial. With 21 seconds left in the first half, McMahon scrambled to the Patriots' 3-yard line and was stopped inbounds. With the clock ticking down, players from both teams were fighting, and the Bears were forced to snap the ball before the officials formally put it back into play, allowing McMahon to throw the ball out of bounds and stop the clock with three seconds left. The Bears were penalized five yards for delay of game, but according to NFL rules, 10 seconds should have also been run off the clock during such a deliberate clock-stopping attempt in the final two minutes of a half. In addition, a flag should have been thrown for fighting (also according to NFL rules). This would have likely resulted in offsetting penalties, which would still allow for a field goal attempt. As far as the illegal snap is concerned, the non-call was promptly acknowledged by the officials and reported by NBC sportscasters during halftime, but the resulting three points were not taken away from the Bears (because of this instance, the NFL instructed officials to strictly enforce the 10-second run-off rule at the start of the 1986 season).
The Bears had dominated New England in the first half, holding them to 21 offensive plays (only four of which resulted in positive yardage), −19 total offensive yards, two pass completions, one first down, and 3 points. While Eason was in the game, the totals were six possessions, one play of positive yardage out of 15 plays, no first downs, 3 points, 3 punts, 2 turnovers, no pass completions, and -36 yards of total offense. Meanwhile, Chicago gained 236 yards and scored 23 points themselves.
After the Patriots received the second-half kickoff, they managed to get one first down, but then had to punt after Grogan was sacked twice. Camarillo, who punted four times in the first half, managed to pin the Bears back at their own 4-yard line with a then-Super Bowl record 62-yard punt. But the Patriots' defense still had no ability to stop Chicago's offense. On their very first play, McMahon faked a handoff to Payton, then threw a 60-yard completion to Gault. Eight plays later, McMahon finished the Super Bowl-record 96-yard drive with a 1-yard touchdown run to increase the Bears' lead to 30–3. On New England's second drive of the quarter, Chicago cornerback Reggie Phillips (who replaced Frazier) intercepted a pass from Grogan and returned it 28 yards for a touchdown to increase the lead to 37–3.
On the second play of their ensuing possession, the Patriots turned the ball over again, when receiver Cedric Jones lost a fumble after catching a 19-yard pass from Grogan, and Wilber Marshall returned the fumble 13 yards to New England's 37-yard line. A few plays later, McMahon's 27-yard completion to receiver Dennis Gentry moved the ball to the 1-yard line, setting up perhaps the most memorable moment of the game. "Refrigerator" Perry was brought on to score on offense, as he had done twice in the regular season. His touchdown made the score 44–3. The Bears' 21 points in the third quarter is still a record for the most points scored in that period.
Perry's surprise touchdown cost Las Vegas sports books hundreds of thousands of dollars in losses from prop bets. The Patriots finally scored a touchdown early in the fourth quarter, advancing the ball 76 yards in 12 plays and scoring on an 8-yard fourth-down pass from Grogan to receiver Irving Fryar. But the Bears' defense dominated New England for the rest of the game, forcing another fumble, another interception, and defensive lineman Henry Waechter's sack on Grogan in the end zone for a safety to make the final score 46–10.
One oddity in the Bears' victory was that Walter Payton had a relatively poor performance running the ball and never scored a touchdown in Super Bowl XX, his first and only Super Bowl appearance during his Hall of Fame career (Many people including Mike Ditka have claimed that the reason for this was due to the fact that the Patriots' defensive scheme was centered on stopping Payton). Although Payton was ultimately the Bears' leading rusher during the game, the Patriots' defense held him to only 61 yards on 22 carries, with his longest run being only 7 yards. He was given several opportunities to score near the goal line, but New England stopped him every time before he reached the end zone (such as his 2-yard loss from the New England 3-yard line a few plays before Butler's second field goal, and his 2-yard run from the 4-yard line right before McMahon's first rushing touchdown). Thus, Chicago head coach Mike Ditka opted to go for other plays to counter the Patriots' defense. Perry's touchdown and McMahon's rushing touchdowns are scoring opportunities that were denied to Payton. Ditka has since gone on record stating that his biggest regret of his career was not creating a scoring opportunity for Payton during the game.
McMahon, who completed 12 out of 20 passes for 256 yards, became the first quarterback in a Super Bowl to score 2 rushing touchdowns. Bears receiver Willie Gault finished the game with 129 receiving yards on just 4 receptions, an average of 32.3 yards per catch. He also returned 4 kickoffs for 49 yards. Suhey had 11 carries for 52 yards and a touchdown, and caught a pass for 24 yards. Singletary tied a Super Bowl record with 2 fumble recoveries.
Eason became the first Super Bowl starting quarterback to fail to complete a pass, going 0 for 6 attempts. Grogan completed 17 out of 30 passes for 177 yards and 1 touchdown, with 2 interceptions. Although fullback Tony Collins was the Patriots' leading rusher, he was limited to just 4 yards on 3 carries, and caught 2 passes for 19 yards. New England receiver Stephen Starring returned 7 kickoffs for 153 yards and caught 2 passes for 39 yards. The Patriots, as a team, only recorded 123 total offensive yards, the second-lowest total in Super Bowl history.
Box score.
"at Louisiana Superdome, New Orleans, Louisiana
! Game information
Second Quarter
Third Quarter
Fourth Quarter
Final statistics.
Sources: , 
Individual leaders.
1Completions/attempts
2Carries
3Long gain
4Receptions
Starting lineups.
Source:

</doc>
<doc id="28977" url="http://en.wikipedia.org/wiki?curid=28977" title="Salute">
Salute

A salute is a gesture or other action used to display respect. Salutes are primarily associated with armed forces, but other organisations and civilians also use salutes.
Military salutes.
In military traditions of various times and places, there have been numerous methods of performing salutes, using hand gestures, cannon or rifle shots, hoisting of flags, removal of headgear, or other means of showing respect or deference. In the Commonwealth of Nations, only officers are saluted, and the salute is to the commission they carry from their respective commanders-in-chief representing the Monarch, not the officers themselves.
The French salute is almost identical to the British Army's. The customary salute in the Polish Armed Forces is the two-fingers salute, a variation of the British military salute with only two fingers extended. In the Russian military, the right hand, palm down, is brought to the right temple, almost, but not quite, touching; the head has to be covered. In the Swedish armed forces, the salute is identical to that of the U.S. armed forces and the Royal Navy. In the Hellenic Army salute the palm is facing down and the fingers point to the coat of arms.
In the United States Navy, United States Marine Corps, United States Coast Guard, United States Public Health Service Commissioned Corps and Colombian Army, as well as in all branches of the British Armed Forces, Polish Armed Forces, Canadian Forces, Turkish Armed Forces, Swedish Armed Forces, Norwegian Armed Forces, Italian Armed Forces and Hellenic Armed Forces, Russian and all former Soviet republic forces, hand salutes are only given when a cover (protection for the head, usually a hat) is worn.
The United States Army and United States Air Force give salutes both covered and uncovered, but saluting indoors is forbidden except when formally reporting to a superior officer or during an indoor ceremony. It should be noted that when outdoors, a cover is to be worn at all times when wearing Battle Dress Uniforms/Army Combat Uniforms, but is not required when wearing PTs.
When the presence of enemy snipers is suspected, military salutes are generally forbidden, since the enemy may use them to recognize officers as valuable targets.
Saluting with Left hand:
A soldier can salute with the left hand when the right hand is encumbered in some way, for example, a soldier with a rifle at Right Shoulder Arms; if movement of a weapon would be encumbered when making the armed salute; if the performance of duty requires the right hand for use or operation of equipment such as riding a motorcycle; if it is not possible to use the hand due to injury or amputation; when escorting a woman and it is not possible to walk on her right side.
Origin.
According to some modern military manuals, the modern Western salute originated when knights greeted each other to show friendly intentions by raising their visors to show their faces, using a salute. Others also note that the raising of one's visor was a way to identify oneself saying "This is who I am, and I am not afraid." Medieval visors were, to this end, equipped with a protruding spike that allowed the visor to be raised using a saluting motion.
According to the US Army Quartermaster School, the following explanation of the origin of the hand salute is: It was a long-established military custom for subordinates to remove their headgear in the presence of superiors. As late as the American Revolution, a British Army soldier saluted by removing his hat. With the advent of increasingly cumbersome headgear in the 18th and 19th centuries, however, the act of removing one's hat was gradually converted into the simpler gesture of grasping or touching the visor and issuing a courteous salutation.
As early as 1745, a British order book stated that: "The men are ordered not to pull off their hats when they pass an officer, or to speak to them, but only to clap up their hands to their hats and bow as they pass." Over time, it became conventionalized into something resembling our modern hand salute.
The naval salute, with the palm downwards is said to have evolved because the palms of naval ratings, particularly deckhands, were often dirty through working with lines and was deemed insulting to present a dirty palm to an officer; thus the palm was turned downwards. During the Napoleonic Wars, British crews saluted officers by touching a clenched fist to the brow as though grasping a hat-brim between fingers and thumb.
Small arms salutes.
When carrying a sword (which is still done on ceremonial occasions), European military forces and their cultural descendants use a two-step gesture. The sword is first raised, in the right hand, to the level of and close to the front of the neck. The blade is inclined forward and up 30 degrees from the vertical; the true edge is to the left. Then the sword is slashed downward to a position with the point close to the ground in front of the right foot. The blade is inclined down and forward with the true edge to the left. This gesture originated in the Crusades. The hilt of a sword formed a cross with the blade, so if a crucifix was not available, a Crusader could kiss the hilt of his sword when praying, before entering battle, for oaths and vows, and so on. The lowering of the point to the ground is a traditional act of submission.
In fencing, the fencers salute each other before putting their masks on to begin a bout. There are several methods of doing this, but the most common is to bring the sword in front of the face so that the blade is pointing up in front of the nose. The fencers also salute the referee and the audience.
When armed with a rifle, two methods are available when saluting. The usual method is called "present arms"; the rifle is brought to the vertical, muzzle up, in front of center of the chest with the trigger away from the body. The hands hold the stock close to the positions they would have if the rifle were being fired, though the trigger is not touched. Less formal salutes include the "order arms salute" and the "shoulder arms salutes." These are most often given by a sentry to a low-ranking superior who does not rate the full "present arms" salute. In the "order arms salute," the rifle rests on its butt by the sentry's right foot, held near the muzzle by the sentry's right hand, and does not move. The sentry brings his flattened left hand across his body and touches the rifle near its muzzle. When the rifle is being carried on the shoulder, a similar gesture is used in which the flattened free hand is brought across the body to touch the rifle near the rear of the receiver.
A different type of salute with a rifle is a ritual firing performed during military funerals, known as a three-volley salute. In this ceremonial act, an odd number of rifleman fire three blank cartridges in unison into the air over the casket. This originates from an old European tradition wherein a battle was halted to remove the dead and wounded, then three shots were fired to signal readiness to reengage.
Heavy arms: gun salutes.
Naval cannon fire.
The custom of firing cannon salutes originated in the Royal Navy. When a cannon was fired, it partially disarmed the ship, so needlessly firing a cannon showed respect and trust. As a matter of courtesy a warship would fire her guns harmlessly out to sea, to show that she had no hostile intent. At first, ships were required to fire seven guns, and forts, with their more numerous guns and a larger supply of gunpowder, to fire 21 times. Later, as the quality of gunpowder improved, the British increased the number of shots required from ships to match the forts.
The system of odd numbered rounds is said to have been originated by Samuel Pepys, Secretary to the Navy in the Restoration, as a way of economising on the use of powder, the rule until that time having been that all guns had to be fired. Odd numbers were chosen, as even numbers indicated a death.
As naval customs evolved, the 21-gun salute came to be reserved for heads of state, with fewer rounds used to salute lower-ranking officials. Today, heads of government and cabinet ministers (e.g., the Vice President, U.S. cabinet members, and service secretaries), and military officers with five-star rank receive 19 rounds; four-stars receive 17 rounds; three-stars receive 15; two-stars receive 13; and a one-star general or admiral receives 11. These same standards are currently adhered to by ground-based saluting batteries.
Multiples of 21-gun salutes may be fired for particularly important celebrations. In monarchies this is often done at births of members of the royal family of the country and other official celebrations associated with the royal family.
United States Army Presidential Salute Battery.
A specialty platoon of the 3rd US Infantry Regiment (The Old Guard), the Presidential Salute Battery is based at Fort Myer, Virginia. The Guns Platoon (as it is known for short) has the task of rendering military honors in the National Capital Region, including armed forces full-honors funerals; state funerals; presidential inaugurations; full-honors wreath ceremonies at the Tomb of the Unknowns in Arlington National Cemetery; state arrivals at the White House and Pentagon, and retirement ceremonies for general-grade officers in the Military District of Washington, which are normally conducted at Fort Myer.
The Presidential Salute Battery also participates in A Capitol Fourth, the Washington Independence Day celebration; the guns accompany the National Symphony Orchestra in performing the "1812 Overture".
The platoon maintains its battery of ten ceremonially-modified World War II-vintage M-5 anti-tank guns at the Old Guard regimental motor pool.
Aerial salutes.
A ceremonial or celebratory form of aerial salute is the flypast (known as a "flyover" in the United States), which often follows major parades such as the annual Trooping the Colour in the United Kingdom or the French "défilé du 14 juillet". It is seen in other countries as well, notably Singapore and Canada.
Gun salute by aircraft, primarily displayed during funerals, began with simple flypasts during World War I and have evolved into the missing man formation, where either a formation of aircraft is conspicuously missing an element, or where a single plane abruptly leaves a formation
A casual salute by an aircraft, somewhat akin to waving to a friend, is the custom of "waggling" the wings by partially rolling the aircraft first to one side, and then the other.
Military salutes in different countries.
Australia and New Zealand.
In both countries, the right-hand salute is generally identical to, and drawn from the traditions of, the British armed forces. The salute of the Australian or New Zealand Army is best described as the right arm taking the path of the longest way up and then the shortest way down. Similar in many ways, the salute of the Royal Australian Air Force and Royal New Zealand Air Force takes the longest way up and the shortest way down. The Royal Australian Navy and Royal New Zealand Navy, however, take the shortest way up, palm down, and the shortest way down. The action of the arm rotating up is slower than the action of the conclusion of the salute which is the arm being quickly "snapped" down to the saluter's side. Junior members are required to salute first and the senior member is obliged to return the compliment. Protocol dictates that the Monarch, members of the Royal Family, the Governor-General and State Governors are to be saluted at all times by all ranks. Except where a Drill Manual (or parade) protocol dictates otherwise, the duration of the salute is timed at three beats of the quick-time march (approximately 1.5 seconds), timed from the moment the senior member first returns it. In situations where cover (or "headdress", as it is called in the Australian Army) is not being worn, the salute is given verbally; the junior party (or at least the senior member thereof) will first come to attention, then offer the salute "Good morning/afternoon Your Majesty/Your Royal Highness/Prime Minister/Your Grace/Sir/Ma'am", etc., as the case may be. It is this, rather than the act of standing to attention, which indicates that a salute is being offered. If either party consists of two or more members, all will come to attention, but only the most senior member of the party will offer (or return) the physical or verbal salute. The party which is wearing headdress must always offer, or respond with, a full salute. At the Forward Edge of the Battle Area (FEBA) no salutes of any kind are given, under any circumstances; it is always sensible to assume that there are snipers in the area. In this case, parties personally known to each other are addressed familiarly by their first or given names, regardless of rank; senior officers are addressed as one might address a stranger, courteously, but without any naming or mark of respect.
British military.
British Army.
Since 1917, the British Army's salute has been given with the right hand palm facing forwards with the fingers almost touching the cap or beret. Before 1917, the salute was given with whichever hand was furthest from the person being saluted, whether that was the right or the left. The salute is given to acknowledge the Queen's commission. A salute may not be given unless a soldier is wearing his regimental headdress, for example a Beret, Caubeen, Tam o' Shanter, Glengarry, field service cap or peaked cap. This does not apply to members of The Blues and Royals (RHG/1stD) The Household Cavalry who, after The Battle of Waterloo were allowed to salute without headdress. If a soldier or officer is not wearing headdress then he or she must come to attention instead of giving/returning the salute. The subordinate salutes first and maintains the salute until the superior has responded in kind.
There is a widespread though erroneous belief that it is statutory for "all ranks to salute a bearer of the Victoria Cross". There is no official requirement that appears in the official Warrant of the VC, nor in Queen's Regulations and Orders, but tradition dictates that this occurs and as such the Chiefs of Staff will salute a Private awarded a VC or GC.
It is also said that the style of salute may also signify that he/she (that is saluting) bears no arms (weapons) as those that bear arms salute with their rifle or sword, at the present arms for a royal salute or general salute.
Royal Air Force.
The custom of saluting commissioned officers relates wholly to the commission given by Her Majesty the Queen to that officer, not the person. Therefore, when a subordinate airman salutes an officer, he is indirectly acknowledging Her Majesty as Head of State. A salute returned by the officer is on behalf of the Queen.
The RAF salute is essentially the same as that of the Army. When RAF personnel hand salute they display an open hand, positioned such that the finger tips almost, but not quite, touch the hat band.
Royal Navy.
The Naval salute differs in that the palm of the hand faces down towards the shoulder. This dates back to the days of sailing ships, when tar and pitch were used to seal a ship's timbers from seawater. To protect their hands, officers wore white gloves and it was considered most undignified to present a dirty palm in the salute, so the hand was turned through 90 degrees. A common story is that Queen Victoria, having been saluted by an individual with a dirty palm, decreed that in future sailors of the fleet would salute palm down, with the palm facing the ground..
In the colonial context.
In the British Empire (originally in the maritime and hinterland sphere of influence of the East India Company, HEIC, later transformed into crown territories), mainly in British India, the numbers of guns fired as a "gun salute" to the ruler of a so-called princely state became a politically highly significant indicator of his status, not governed by objective rules, but awarded (and in various cases increased) by the British paramount power, roughly reflecting his state's socio-economic, political and/or military weight, but also as a prestigious reward for loyalty to the Raj, in classes (always odd numbers) from three to twenty-one (seven lacking), for the "vassal" indigenous rulers (normally hereditary with a throne, sometimes raised as a personal distinction for an individual ruling prince). Two sovereign monarchies officially outside the Empire were granted a higher honour: thirty-one guns for the royal houses of Afghanistan (under British and Russian influence), and Siam (which was then ruled by the Rattanakosin Kingdom).
In addition, the right to style himself "Highness" (Majesty, which since its Roman origin expresses the sovereign authority of the state, was denied to all "vassals"), a title of great importance in international relations, was formally restricted to rulers of relatively high salute ranks (originally only those with eleven guns or more, later also those with nine guns).
Indonesia Salute.
In Indonesia, the salute is a very common gesture amongst every part of the country, starting from school to military, the salute is a gesture that every person must known and is commonly used for the flag raising ceremony. In the military, this gesture is known as Present Arms or in Indonesian: "Hormat Senjata, Gerak".
Canadian military.
Much as the British salute, described above, the Canadian military salutes to demonstrate a mark of respect and courtesy for the commissioned ranks. When in uniform and not wearing headdress one does not salute. Instead, compliments shall be paid by standing at attention. If on the march, arms shall be swung and the head turned to the left or right as required.
On Remembrance Day, 2009, The Prince of Wales attended the national ceremony in Ottawa with Governor General Michaëlle Jean—both wearing Canadian military dress. CBC live television coverage of the event noted that, when Prince Charles saluted, he performed the Canadian form of the salute with a cupped hand (the British "naval salute"—appropriate, as he did his military service as an officer in the Royal Navy), adopted by all elements of the Canadian Forces after unification in 1968, rather than the British (Army) form with the palm facing forward.
French military.
Much as the British salute, described above, the French military salutes to demonstrate a mark of respect and courtesy for the commissioned ranks. Until 1992, salutes were not performed if a member is not wearing a headdress. The salute is performed with a flat hand, palm facing forwards; the upper arm is horizontal and the tip of the fingers come near the corner of the eyes. A crisp tension may be given when the salute is broken.
German military.
In the German Bundeswehr, the salute is performed with a flat hand, with the thumb resting on the index finger. The hand is slightly tilted to the front so that the thumb can not be seen. The upper arm is horizontal and the fingers point to the temple but do not touch it or the headgear. Every soldier saluting another uniformed soldier is entitled to be saluted in return. Soldiers below the rank of Feldwebel are not permitted to speak while saluting. Since the creation of the Bundeswehr, soldiers are required to salute with and without headgear. Originally, in the Reichswehr it was not permitted to perform the salute when the soldier is not wearing uniform headgear. In the Wehrmacht, the traditional military salute was required when wearing headgear, but the Nazi salute was performed when not wearing headgear. East German National People's Army followed the Reichswehr protocol.
Israel Defense Forces.
In the Israel Defense Forces, saluting is normally reserved for special ceremonies. Unlike the United States, saluting is not a constant part of day to day barracks life.
People's Republic of China.
Military personnel of the People's Liberation Army salute according to Chinese standards and similar to the Royal Navy salute.
Polish military.
In Polish military forces, militarymen use two fingers to salute, and when they wear headdress (including helmet) only (an expression of the Polish struggle towards the ultimate egalitarian society: not saluting the person per se). This is called the Two-finger salute. There are some exceptions in Polish regulations when salute is not demonstrated, for instance after proclaiming alert in military unit area. As above, salute is marking respect for higher rank or command. Untrained recruits are obliged to salute as without headdress, i.e. to stand at attention (or—during walking—to march at attention).
Swiss Armed Forces.
Swiss soldiers are required to salute any higher-ranking military personnel whenever they encounter them. The salute is given like that of the British Navy with the palm pointing towards the shoulder.
Turkish Armed Forces.
In the Turkish military, the head has to be covered. When the head is not covered, or when holding a weapon, the head is nodded forward slightly while maintaining erect posture. In case of two military personnel approaching each other walking, the lower ranking individual initiates the salute six paces prior to crossing paths with the ranking individual, and finishes it one pace past.
Zogist salute.
The Zogist salute is a military salute that was instituted by Zog I of Albania. It is a gesture whereby the right hand is placed over the heart, with the palm facing downwards. It was first widely used by Zog's personal police force and was later adopted by the Royal Albanian Army.
US Armed Forces.
In the US Army, soldiers may only salute left-handed while bearing a military standard.
Non-military services.
Canadian police.
Most police forces salute similar to the Canadian Forces standard, with the exception of the Royal Canadian Mounted Police and the Royal Newfoundland Constabulary, which follow the British Army standard of saluting with the full palm facing forward, touching the brim of the hat, if worn.
Non-police.
Similar salutes are used by honour guards for non-police services (e.g. Toronto Fire Services, Toronto Transit Commission) during funerals or ceremonial events.
Hong Kong.
All uniform branches of the Hong Kong (Police, Police Auxiliary, Police Pipeband, Fire (including Ambulance service members), Immigration, Customs, Correctional Services, Government Flying Service, Civil Aid Service) salute according to British Army traditions. Personnel stationed with the People's Liberation Army in Hong Kong salute using the Chinese military standards and similar to those used by the Royal Navy.
Non-government organizations like Hong Kong Air Cadet Corps, Hong Kong Adventure Corps, the Boys' Brigade, Hong Kong, Hong Kong Sea Cadet Corps and St. John Ambulance all follow the same military salutes due to their ties with the British Armed Forces.
Civilian salutes.
In most countries, such as the United Kingdom and Canada, civilians do not salute the flag, although some may stand at attention when a national anthem is played or the national flag raised or lowered. In the United Kingdom, certain civilian individuals, such as officers of HM Revenue and Customs, salute the quarterdeck of Royal Navy vessels on boarding.
In many countries, gestures such as tipping one's hat when passing another on the street can be considered appropriate civilian salutes. A more formal hat tip-and-lift is common in Britain, especially by doormen in hotels.
In the United States, civilians may salute the American flag by placing their right hand over their heart or by standing at attention during the playing of the national anthem or while reciting the American Pledge of Allegiance, or when the flag is passing by, as in a parade, etc. Men and boys remove their hats and other headgear during the salute—excepting religious headdress (and military headdress worn by veterans in uniform, who are otherwise civilians). The nature of the headgear determines whether it is held in the left or right hand, tucked under the left arm, etc. However, if it is held in the right hand, the headgear is not held over the heart but the hand is placed in the same position it would be if it were not holding anything.
The Defense Authorization Act of 2009, signed by President Bush, contained a provision that gave veterans and active-duty service members not in uniform the right to salute during the playing of the national anthem. Previous legislation authorized saluting when not in uniform during the raising, lowering and passing of the flag. However, because a salute is a form of communication protected by the Free Speech clause of the First Amendment, legislative authorization is not required for any civilian—veteran or non-veteran—to salute the American flag.
Civilians in some other countries, like Italy and Nigeria, also render the same civilian salute as their American counterparts when hearing their respective national anthems.
In Iran a salute similar to the United States is given. In ancient times a salute would be given by raising a flat hand in front of the chest with the thumb facing the saluters face.
In Latin America, especially in Mexico, a salute similar to the United States military's salute (see below) is used, but the hand is placed across the left chest with the palm facing the ground.
The same salute was instituted in Albania as the "Zog salute" by King Zog I.
In the Philippines, civilians salute to the national flag during flag raising and upon hearing the Philippine National Anthem by standing at attention and doing the same hand-to-heart salute as their American, Italian, Nigerian, and South African counterparts. People wearing hats or caps must bare their heads and hold the headwear over their heart. Members of the Armed Forces of the Philippines "(including Philippine Air Force)" and Philippine National Police meanwhile do the traditional salutes if they are on duty. If a member of the military or police is not on duty, they must place their hands over their left chest.
Boy Scouts and Girl Scouts meanwhile have their own form of salutes.
In Indonesia, civilians may either place their right hand to their left-breast (heart) or salute the flag as per a military salute, which may be that of the "PETA" Revolutionaries, or as per modern military drill. All persons present regardless of nationality are expected to stand silently and respectfully during its raising and lowering. It is a severe criminal offense in Indonesia to dishonour the national flag (known in Indonesian as "Sang Saka Merah Putih", "The Red and White") and its ceremonies, or the national anthem, "Indonesia Raya".
Thailand also has the same rule like Indonesia wherein all persons present regardless of nationality are expected to stand at attention and respectfully during the flag raising and lowering and upon hearing the Thai National Anthem every 8:00 a.m. and 6:00 p.m. or hearing the Sansoen Phra Barami. The Lèse majesté in Thailand says that it is a serious criminal offense to dishonor the flag of Thailand or National/Royal Anthem.
Roman salute.
The Roman salute is a gesture in which the arm is held out forward straight, with palm down and fingers extended straight and touching. Sometimes the arm is raised upward at an angle, sometimes it is held out parallel to the ground. A well known symbol of fascism, it is commonly perceived to be based on a classical Roman custom.p. 2 but no known Roman work of art displays this salute, nor does any known Roman text describe it.
Beginning with Jacques-Louis David's painting "The Oath of the Horatii" (1784), an association of the gesture with Roman republican and imperial culture emerged through 18th-century French art.:42–56 The association with ancient Roman traditions was further developed in popular culture through late 19th- and early 20th-century plays and films.:pp. 70–101 These include the epic "Cabiria" (1914), whose screenplay was attributed to Italian nationalist Gabriele d'Annunzio. In a case of life imitating art, d'Annunzio appropriated the salute as a neo-imperial ritual when he led the occupation of Fiume in 1919. It was soon adopted by the Italian Fascist party and from them the Nazi party. However, the armed forces ("Wehrmacht") of the Third Reich used the traditional European military salute until, in the wake of the July 20 plot on Hitler's life in 1944, the Nazi salute or "Hitlergruss" was imposed on them. The Bellamy salute was a similar gesture and was the civilian salute of the United States from 1892 to 1942.
In Germany showing the Roman salute is prohibited. Those rendering similar salutes, for example raising the left instead of the right hand, or raising only three fingers, are liable to prosecution. The punishment derives from § 86a of the German Criminal Code and can be up to three years imprisonment or a fine (in minor cases).
Airline Industry.
According to SOPs (Standard Operating Procedures) of most airlines, ground crew that handles departure of an aircraft from a gate (such handling normally includes: disconnecting of required for engine start pneumatic generators or aircraft power and ventilation utilities, aircraft push-back, icing inspection, etc.), is required to salute the Captain before the aircraft is released for taxi. Captain normally returns the salute. Since a large percentage of airline pilots are ex-military pilots, this practice was transferred to the airline industry from the military. Exactly the same saluting practice is appropriate to most military aircraft operations, including Air Force, Navy and Army.
Clenched fist salute.
The raised clenched fist, symbolizing unity in struggle, was popularized in the 19th century by the socialist, communist and anarchist movements, and is still used today.
In the United States, the raised fist was associated with the Black Power movement, symbolized in the 1968 Olympics Black Power salute; a clenched-fist salute is also proper in many African nations, including South Africa. However, the two salutes are somewhat different: in the Black Power salute, the arm is held straight, while in the salute of leftist movements the arm is bent slightly at the elbow.
Greetings.
Many different gestures are used throughout the world as simple greetings. In Western cultures the handshake is very common, though it has numerous subtle variations in the strength of grip, the vigour of the shake, the dominant position of one hand over the other, and whether or not the left hand is used. 
Historically, when men normally wore hats out of doors, male greetings to people they knew, and sometimes those they did not, involved touching, raising slightly ("tipping"), or removing their hat in a variety of gestures. This basic gesture remained normal in very many situations from the Middle Ages until men typically ceased wearing hats in the mid-20th century. Hat-raising began with an element of recognition of superiority, where only the socially inferior party might perform it, but gradually lost this element; King Louis XIV of France made a point of at least touching his hat to all women he encountered. However the gesture was never used by women, for whom their head-covering included considerations of modesty. When a man was not wearing a hat he might touch his hair to the side of the front of his head to replicate a hat tipping gesture. This was typically performed by lower class men to social superiors, such as peasants to the land-owner, and is known as "tugging the forelock", which still sometimes occurs as a metaphor for submissive behaviour. 
In Europe, the formal style of upper-class greeting used by a man to a woman in the Early Modern Period was to hold the woman's presented hand (usually the right) with his right hand and kiss it while bowing. This style has not been widespread for a century or more. In cases of a low degree of intimacy, the hand is held but not kissed. The ultra-formal style, with the man's right knee on the floor, is now only used in marriage proposals, as a romantic gesture.
The Arabic term "salaam" (literally "peace", from the spoken greeting that accompanies the gesture), refers to the practice of placing the right palm on the heart, before and after a handshake.
A Chinese martial arts greeting features the right fist placed in the palm of the left hand, and a head nod or bow.
In India, it is common to see the Namaste greeting (or "Sat Sri Akal" for Sikhs) where the palms of the hands are pressed together and held near the heart with the head gently bowed.
In Indonesia, a nation with a huge variety of cultures and religions, many greetings are expressed, from the formalized greeting of the highly stratified and hierarchical Javanese to the more egalitarian and practical greetings of outer islands.
Javanese, Batak and other ethnicities currently or formerly involved in the armed forces will salute a Government-employed superior, and follow with a deep bow from the waist or short nod of the head and a passing, loose handshake. Hand position is highly important; the superior's hand must be higher than the inferior's.
Muslim men will clasp both hands, palms together at the chest and utter the correct Islamic "slametan" (greeting) phrase, which may be followed by cheek-to-cheek contact, a quick hug or loose handshake. Pious Muslim women rotate their hands from a vertical to perpendicular prayer-like position in order to barely touch the finger tips of the male greeter and may opt out of the cheek-to-cheek contact.
If the male is an "Abdi Dalem" royal servant, courtier or particularly "peko-peko" (taken directly from Japanese to mean obsequious) or even a highly formal individual, he will retreat backwards with head downcast, the left arm crossed against the chest and the right arm hanging down, never showing his side or back to his superior. His head must always be lower than that of his superior.
Younger Muslim males and females will clasp their elder's or superior's outstretched hand to the forehead as a sign of respect and obeisance.
If a manual worker or a person with obviously dirty hands salutes or greets an elder or superior, he will show deference to his superior and avoid contact by bowing, touching the right forehead in a very quick salute or a distant "slamet" gesture.
The traditional Javanese "Sungkem" involves clasping the palms of both hands together, aligning the thumbs with the nose, turning the head downwards and bowing deeply, bending from the knees. In a royal presence, the one performing "sungkem" would kneel at the base of the throne.
A gesture called a "wai" is used in Thailand, where the hands are placed together palm to palm, approximately at nose level, while bowing. The "wai" is similar in form to the gesture referred to by the Japanese term "gassho" by Buddhists. In Thailand, the men and women would usually press two palms together and bow a little while saying "Sawadee ka" (female speaker) or "Sawadee krap" (male speaker).
Some cultures use hugs and kisses (regardless of the sex of the greeters), but those gestures show an existing degree of intimacy and are not used between total strangers. All of these gestures are being supplemented or completely displaced by the handshake in areas with large amounts of business contact with the West.
These bows indicate respect and acknowledgment of social rank, but do not necessarily imply obeisance.
Obeisances.
An "obeisance" is a gesture not only of respect but also of submission. Such gestures are rarer in cultures that do not have strong class structures; citizens of the Western World, for example, often react with hostility to the idea of bowing to an authority figure. The distinction between a formally polite greeting and an obeisance is often hard to make; for example, "proskynesis" (from the words πρός "pros" (towards) and κυνέω "kyneo" (to kiss)) is described by the Greek researcher Herodotus of Halicarnassus, who lived in the 5th century BC in his "Histories" 1.134:
After his conquest of Persia, Alexander the Great introduced Persian etiquette into his own court, including the practice of proskynesis. Visitors, depending on their ranks, would have to prostrate themselves, bow to, kneel in front of, or kiss the king. His Greek countrymen objected to this practice, as they considered these rituals only suitable to the gods.
In countries with recognized social classes, bowing to nobility and royalty is customary. Standing bows of obeisance all involve bending forward from the waist with the eyes downcast, though variations in the placement of the arms and feet are seen. In western European cultures, women do not bow, they "curtsey" (a contraction of "courtesy" that became its own word), a movement in which one foot is moved back and the entire body lowered to a crouch while the head is bowed.
The European formal greeting used from men to women can be transformed into an obeisance gesture by holding the suzerain's hand with both hands. This kind of respect is due to kings, princes, sovereigns (in their kingdoms), archbishops (in their metropolitan province) or the Pope (everywhere). In ultra-formal ceremonies (a coronation, oath of allegiance or episcopal inauguration) the right knee shall touch the ground.
In South Asia traditions, obeisance also involves prostrating oneself before a king.
Many religious believers kneel in prayer, and some (Roman Catholics, and Anglicans) "genuflect", bending one knee to touch the ground, at various points during religious services; the Orthodox Christian equivalent is a deep bow from the waist, and as an especially solemn obeisance the Orthodox make prostrations, bending down on both knees and touching the forehead to the floor. Roman Catholics also employ prostrations on Good Friday and at ordinations. During Islamic prayer, a kneeling bow called "sajdah" is used, with forehead, nose, hands, knees, and toes all touching the ground. Jews bow from the waist many times during prayer. Four times during the Yom Kippur service, and once on each day of Rosh Hashanah, many Jews will kneel and then prostrate. With the Salvation Army, when becoming a soldier, at a christening or other official event, underneath the flag, a salute is often used. This involves holding the hand, palm forwards, with all the fingers held in a clenched fist position. The index finger is left raised pointing towards God, and the hand is often held at chest height, in a similar position to that of Girl Guides.
Marching bands and Drum & Bugle Corps.
Hand salutes similar to those used in the military are rendered by the Drum Major of a marching band or drum corps just prior to beginning their performance (after the show announcer asks if the group is ready), as well as following completion of the performance, both rendered to the audience.
The classic "corps style" salute is often known as the "punch" type, where the saluting party will first punch their right arm straight forward from their body, arm parallel to the ground, hand in a fist, followed by the more traditional salute position with the right hand, left arm akimbo. Dropping the salute typically entails snapping the saluting hand to the side and clenching the fist, then dropping both arms to the sides.
In the US, a Drum Major carrying a large baton or mace will often salute by bringing the right hand, holding the mace with the head upward, to the left shoulder.
There are occasional, more flashy variations, such as the windmill action of the saluting arm for the Madison Scouts drum major, or running the saluting hand around the brim of the aussie instead of snapping it down from the Cavaliers.
Military salutes in popular culture.
Many artefacts of popular culture have created military salutes for fictional purposes, more often than not with a cynical or sarcastic purpose.
In his 1953 comic book album Le Dictateur et le Champignon, which is part of the Spirou et Fantasio series, Belgian artist Franquin creates a silly salute, used in a fictional Latin American country named Palombia. When saluting, subordinates of General Zantas must raise their hands over their heads, with the palm facing forward, then point to the top of their heads with their thumbs.
Franquin repeats this idea in his 1957 comic book album Z comme Zorglub, another episode of the Spirou et Fantasio series. Here, almighty science wizard Zorglub's conscripted soldiers salute their leader by pointing to their heads with their index fingers to cynically underline how much of a genius they consider him to be.
In the 1987 parodic science fiction film Spaceballs, directed by Mel Brooks, all subordinates of Supreme leader President Skroob salute him by first bending their forearms over their opposed hands as though they are about to give him the arm of honor salute, but at the last moment, use their raised hands to wave him good bye, rather than showing him the middle finger.
In the Japanese Animated Series Shingeki No Kyojin (Attack on Titan) the members of the armed forces (and sometimes civilians in a show of respect towards military) salute by bending their arms and placing their clenched fist over their hearts. The gesture, known as "offering hearts" is meant to demonstrate that the soldiers are willing to give their bodies and lives to protect humanity and to ensure its survival. This is an apt gesture as the casualty rate among these soldiers is extreme due to the overwhelming power of their enemies.

</doc>
<doc id="28979" url="http://en.wikipedia.org/wiki?curid=28979" title="Scopolamine">
Scopolamine

Scopolamine (USAN), hyoscine (BAN) also known as levo-duboisine or burundanga, sold as Scopoderm, is a tropane alkaloid drug with muscarinic antagonist effects. It is among the secondary metabolites of plants from Solanaceae (nightshade) family of plants, such as henbane, jimson weed ("Datura"), angel's trumpets ("Brugmansia"), and corkwood ("Duboisia"). Scopolamine exerts its effects by acting as a competitive antagonist at muscarinic acetylcholine receptors; it is thus classified as an anticholinergic, antimuscarinic drug. Although it is , there is indirect evidence for m1-receptor subtype specificity. (See the article on the parasympathetic nervous system for details of this physiology.)
Its use in medicine is relatively limited, with its chief uses being in the treatment of motion sickness and postoperative nausea and vomiting.
Scopolamine is named after the plant genus "Scopolia". The name "hyoscine" is from the scientific name for henbane, "Hyoscyamus niger".
Medical use.
Scopolamine has a number of uses in medicine, where it is used to treat:
It is sometimes used as a premedication (especially to reduce respiratory tract secretions) to surgery, mostly commonly by injection.
Adverse effects.
Adverse effect incidence:
Overdose.
Physostigmine is an acetylcholinesterase inhibitor that readily crosses the blood-brain barrier, and has been used as an antidote to treat the CNS depression symptoms of scopolamine overdose. Other than this supportive treatment, gastric lavage and induced emesis (vomiting) are usually recommended as treatments for overdoses. The symptoms of overdose include:
Medication interactions.
Due to interactions with metabolism of other drugs, scopolamine can cause significant unwanted side effects when taken with other medications. Specific attention should be paid to other medications in the same pharmacologic class as scopolamine, also known as anticholinergics. The following medications could potentially interact with the metabolism of scopolamine: analgesics/pain medications, ethanol, zolpidem, thiazide diuretics, buprenorphine, anticholinergic drugs such as tiotropium, etc.
Biosynthesis in plants.
The steps of the biosynthesis of scopolamine are:
History.
One of the earlier alkaloids isolated from plant sources, scopolamine has been in use in its purified forms (such as various salts, including hydrochloride, hydrobromide, hydroiodide and sulfate), since its isolation by the German scientist Albert Ladenburg in 1880, and as various preparations from its plant-based form since antiquity and perhaps prehistoric times. Following the description of the structure and activity of scopolamine by Ladenburg, the search for synthetic analogues of and methods for total synthesis of scopolamine and/or atropine in the 1930s and 1940s resulted in the discovery of diphenhydramine, an early antihistamine and the prototype of its chemical subclass of these drugs, and pethidine, the first fully synthetic opioid analgesic, known as Dolatin and Demerol amongst many other trade names.
Scopolamine was used in conjunction with morphine, oxycodone, or other opioids from before 1900 into the 1960s to put mothers in labor into a kind of "twilight sleep". The analgesia from scopolamine plus a strong opioid is deep enough to allow higher doses to be used as a form of anaesthesia.
Scopolamine mixed with oxycodone (Eukodal) and ephedrine was marketed by Merck as SEE (from the German initials of the ingredients) and Scophedal starting in 1928, and the mixture is sometimes mixed on site on rare occasions in the area of its greatest historical usage, namely Germany and Central Europe.
Scopolamine was also one of the active ingredients in Asthmador, an over-the-counter (OTC) smoking preparation marketed in the 1950s and '60s claiming to combat asthma and bronchitis. In November 1990, the US Food and Drug Administration forced OTC products with scopolamine and several hundred other ingredients that had allegedly not been proved effective off the market. Scopolamine shared a small segment of the OTC sleeping pill market with diphenhydramine, phenyltoloxamine, pyrilamine, doxylamine, and other first-generation antihistamines, many of which are still used for this purpose in drugs such as Sominex, Tylenol PM, NyQuil, etc.
Methods of administration.
Scopolamine can be administered orally, subcutaneously, ophthalmically and intravenously, as well as via a transdermal patch. The transdermal patch ("e.g.," Transderm Scōp) for prevention of nausea and motion sickness employs scopolamine base, and is effective for up to three days. The oral, ophthalmic, and intravenous forms have shorter half-lives and are usually found in the form scopolamine hydrobromide (for example in Scopace, soluble 0.4-mg tablets or Donnatal).
NASA is currently developing a nasal administration method. With a precise dosage, the NASA spray formulation has been shown to work faster and more reliably than the oral form.
Use in pregnancy.
Scopolamine crosses the placenta and is a pregnancy category C medication, meaning a risk to the fetus cannot be ruled out. Either studies in animals have revealed adverse effects on the fetus (teratogenic or embryocidal effects or other) and no controlled studies in women have been made, or studies in women and animals are not available. Drugs should be given only if the potential benefits justify the potential risk to the fetus. It may cause respiratory depression and/or neonatal hemorrhage when used during pregnancy, and some animal studies did report adverse events. Transdermal scopolamine has been used as an adjunct to epidural anesthesia for Caesarean delivery without adverse CNS effects on the newborn. Except when used prior to Caesarean section, use it during pregnancy only if the benefit to the mother outweighs the potential risk to the fetus.
Use in breastfeeding.
Scopolamine enters breast milk by secretion. Although no human studies exist to document the safety of scopolamine while nursing, the manufacturer recommends caution be used if scopolamine be administered to a nursing woman.
Recreational use.
While it is occasionally used recreationally for its hallucinogenic properties, the experiences are often mentally and physically extremely unpleasant, and frequently physically dangerous, so repeated use is rare.
Use in the elderly population.
Scopolamine use in the elderly can increase the likelihood of experiencing adverse effects from the drug. This phenomenon is especially true of the elder population who are also concurrently on several other medications. Avoid scopolamine use in this age group due to potent anticholinergic adverse effects and uncertain effectiveness.
Scopolamine-related hospitalizations.
About one in five emergency room admissions for poisoning in Bogotá, Colombia, have been attributed to scopolamine. In June 2008, more than 20 people were hospitalized with psychosis in Norway after ingesting counterfeit rohypnol tablets containing scopolamine.
Interrogation and criminal use.
The effects of scopolamine were studied by criminologists in the early 20th century. In 2009, it was proved that Czechoslovak communist state security secret police used scopolamine at least three times to obtain confessions from alleged antistate conspirators. Because of a number of undesirable side effects, scopolamine was shortly disqualified as a truth serum.
In 1910, scopolamine was detected in the remains believed to be those of Cora Crippen, wife of Dr. Hawley Harvey Crippen, and was accepted at the time as the cause of her death, since her husband was known to have bought some at the start of the year.
Per the United States State Department (March 4, 2012): "One common and particularly dangerous method that criminals use in order to rob a victim is through the use of drugs. The most common has been scopolamine. Unofficial estimates put the number of annual scopolamine incidents in Colombia at approximately 50,000. Scopolamine can render a victim unconscious for 24 hours or more. In large doses, it can cause respiratory failure and death. It is most often administered in liquid or powder form in foods and beverages. The majority of these incidents occur in night clubs and bars, and usually men, perceived to be wealthy, are targeted by young, attractive women. To avoid becoming a victim of scopolamine, one should never accept food or beverages offered by strangers or new acquaintances or leave food or beverages unattended. Victims of scopolamine or other drugs should seek immediate medical attention."
Research.
Scopolamine has been shown in multiple studies to be a potent antidepressant and anxiolytic medication. Two methods of administration have been studied. The first is in-patient sessions where the patients receive an intravenous infusion of a relatively large quantity of scopolamine during a session that lasts 1–2 hours. The patients are monitored during the infusion and released soon after as the effects wear off quickly. In research assessing intravenous infusions, scopolamine has been found to produce rapid and robust antidepressant effects in patients with major depressive disorder.
The second route of administration is oral scopolamine in a pill; it has been studied for depression.

</doc>
<doc id="28981" url="http://en.wikipedia.org/wiki?curid=28981" title="Society for Creative Anachronism">
Society for Creative Anachronism

The Society for Creative Anachronism (SCA) is an international living history group with the aim of studying and recreating mainly Medieval European cultures and their histories before the 17th century. A quip often used within the SCA describes it as a group devoted to the Middle Ages "as they ought to have been," choosing to "selectively recreate the culture, choosing elements of the culture that interest and attract us." Founded in 1966, the non-profit educational corporation has over 30,000 paid members as of 2014[ [update]] with about 60,000 total participants in the society (including members and non-member participants).
History.
The SCA's roots can be traced to a backyard graduation party of a UC Berkeley medieval studies graduate, the author Diana Paxson, in Berkeley, California, on May Day in 1966. The party began with a "Grand Tournament" in which the participants wore helmets, fencing masks, and usually some semblance of a costume, and whacked away at each other with weapons including plywood swords, padded maces, and even a fencing foil. It ended with a parade down Telegraph Avenue with everyone singing "Greensleeves". It was styled as a "protest against the 20th century." The SCA still measures dates within the society from the date of that party, calling the system "Anno Societatis" (Latin for "in the Year of the Society"). For example, 2009 May 1 to 2010 April 30 was A.S. XLIV (44). The name "Society for Creative Anachronism" was coined by science fiction author Marion Zimmer Bradley, an early participant, when the nascent group needed an official name in order to reserve a park for a tournament.
In 1968, Bradley moved to Staten Island, New York and founded the Kingdom of the East, holding a tournament that summer to determine the first Eastern King of the SCA. That September, a tournament was held at the 26th World Science Fiction Convention, which was in Berkeley that year. The SCA had produced a book for the convention called "A Handbook for the Current Middle Ages", which was a how-to book for people wanting to start their own SCA chapters. Convention goers purchased the book and the idea spread. Soon, other local chapters began to form. In October 1968, the SCA was incorporated as a 501(c)(3) non-profit corporation in California. By the end of 1969, the SCA's three original kingdoms had been established: West Kingdom, East, and Middle. All SCA kingdoms trace their roots to these original three. The number of SCA kingdoms has continued to grow by the expansion and division of existing kingdoms; for example, the kingdoms now called the Outlands, Artemisia, Ansteorra, Gleann Abhann, Meridies, and Trimaris all are made up of lands originally belonging to the fourth kingdom, Atenveldt, which began as a branch of the West Kingdom.
Activities.
The SCA engages in a broad range of activities, including SCA armoured combat, SCA fencing, archery, equestrian activities, feasting, medieval dance and recreating medieval arts and sciences, including a broad range of crafts as well as medieval music and theatre. Other activities include the study and practice of heraldry and scribal arts (calligraphy and illumination). Members are afforded opportunities to register a medieval personal name and coat of arms (often colloquially called a "device" in SCA parlance). SCA scribes produce illuminated scrolls to be given by SCA royalty as awards for various achievements.
Most local groups in the SCA hold weekly fighter practices, and many also hold regular archery practices, dance practices, A&S (Arts & Science) nights and other regular gatherings. Some kingdoms and regions also have occasional war practices, where fighters practice formations and group tactics in preparation for large scale "war" events.
Events.
Some local groups participate in nearby Renaissance fairs, though the main focus of activity is organized through the SCA's own events. Each kingdom in the SCA runs its own schedule of events which are announced in the kingdom newsletter (and usually posted on the kingdom web site), but some of the largest SCA-sanctioned events, called "wars", attract members from many kingdoms. Pennsic War, fought annually between the East Kingdom and Middle Kingdom, is the biggest event in the SCA, but other annual SCA wars include Estrella War in Atenveldt, Gulf Wars in Gleann Abhann (formerly Meridies), Great Western War in Caid, War of the Lillies in Calontir and others. Other annual or semi-annual Kingdom-level events held analogously by most or all SCA kingdoms include Crown Tournament, Coronation, Kingdom Arts and Sciences competition and Queen's Prize. Additionally, most baronies in the SCA have their own traditional annual events such as Baronial Arts and Sciences competition, a championship tournament, and often a Yule or Twelfth Night feast. Various SCA groups also sometimes host collegia or symposia, where members gather for a raft of classes on various medieval arts and sciences and other SCA-related topics.
Publications.
The SCA produces two quarterly publications, "The Compleat Anachronist" and "Tournaments Illuminated", and each kingdom publishes a monthly newsletter.
"The Compleat Anachronist" is a quarterly monographic series, each issue focused on a specific topic related to the period of circa 600–1600 in European history. Issues are written by SCA members and have covered a wide range of topics.
"Tournaments Illuminated" is a quarterly magazine, each issue covering a range of topics and including several features such as news, a humor column, book reviews, war reports and various articles on SCA-related topics of interest.
Organization.
Corporate organization.
The SCA is incorporated as a 501(c)(3) non-profit corporation in California, with its current headquarters in the city of Milpitas. It is headed by a board of directors, each of which is nominated by the membership of the SCA, selected by sitting directors, and elected to serve for 3.5 years. Each director serves as an ombudsman for various kingdoms and society officers. The BoD, as it is called, is responsible for handling the corporate affairs of the SCA and is also in charge of certain disciplinary actions, such as revoking the membership status of participants who have broken Corpora regulations or modern law while participating in SCA activities.
Because the SCA now has groups all over the world, it has also been incorporated in other countries, e.g. SCAA in Australia and SCANZ in New Zealand. These affiliated bodies work with the US board of directors with regards to societal issues, but make all decisions affected by local law independently of the US parent body. Although they agree to work in unity with the US SCA board of directors, they are autonomous and are not bound by any ruling of the US body.
Branches.
The SCA is divided into administrative regions which it calls "kingdoms". Smaller branches within those kingdoms include "Principalities", "Regions", "Baronies", and "Provinces", and local chapters are known as "Cantons", "Ridings", "Shires", "Colleges", "Strongholds", and "Ports". Kingdoms, Principalities, and Baronies have ceremonial rulers who preside over activities and issue awards to individuals and groups. Colleges, Strongholds, and Ports are local chapters (like a shire) that are associated with an institution, such as a school, military base, or even a military ship at sea.
All SCA branches are organized in descending order as follows:
Groups are active all over the United States, Canada, Europe, Australia, South Africa, and New Zealand, with scattered groups elsewhere, including Panama and Thailand. At one time there was even a group on the aircraft carrier USS "Nimitz", known as the "Shire of Curragh Mor" (anglicized Irish for "Big Boat"), and the shire's arms played on the "Nimitz's" ship's badge. There is also an active chapter in South Korea, the Stronghold of Warrior's Gate, with a mix of active duty military personnel from the several services and military-connected civilians. There are also non-territorial, usually called "households", which are not part of the Society's formal organization, the largest of which is the Mongol Empire-themed Great Dark Horde.
Kingdoms.
The twenty SCA Kingdoms and the geograpic areas they cover are (in order of founding)
Officers.
The Society as a whole, each kingdom, and each local group within a kingdom, all have a standard group of officers with titles loosely based on medieval equivalents.
Culture of the group.
Members of the SCA study and take part in a variety of activities, including combat and chivalry, archery, heraldry, equestrian activities, costuming, cooking, metalwork, woodworking, music, dance, calligraphy, fiber arts, and others as practiced during the member's time period.
Personae.
To aid historical recreation, participants in the SCA create historically plausible characters known individually as "personae". To new members, a persona can simply be a costume and a name used for weekend events, while other members may study and create an elaborate personal history. The goal of a well-crafted persona is a historically accurate person who might have lived in a particular historical time and place. The SCA has onomastic students who assist members in creating a persona name appropriate to a particular time and place within the SCA's studied period. However, claiming to be a specific historical individual, especially a very familiar one (e.g. Genghis Khan, Julius Caesar, Henry Plantagenet, Queen Elizabeth I), is not permitted. Likewise, one is not allowed to claim the "persona" of a fellow SCA member, alive or dead. Nor is one allowed to take on the persona of a sufficiently familiar fictional character (e.g. Robin of Locksley/Robin Hood).
Heraldry.
A major dimension to the SCA is its internal system of Heraldry. Any member of the society may apply to register a name and device for their persona, which are checked by the heralds for uniqueness and period authenticity, before being blazoned and recorded in the society's Armorial. The system has evolved since the formation of the society; and now has three Sovereigns of Arms, with "Principal Heralds" for each Kingdom, who oversee deputy officers for matters such as heraldic education and processing registrations, and local officers (generally one for each local chapter) who assist the local participants. In addition to design of arms, heralds in the Society also provide services such as voice heraldry (similar to a master of ceremonies) at tournaments and official functions, and organizing tournament brackets or "lists."
Royalty.
The SCA has ceremonial rulers chosen by winning tournaments (Kings/Queens, Princes/Princesses) in SCA armored combat. Barons and Baronesses are appointed by Royalty, although some baronies hold elections or competitions to choose their preferred Baron and/or Baroness. One of the primary functions of state for reigning monarchs is to recognize participant achievement through awards. Most awards denote excellence in a specific pursuit such as local service, arts and sciences, and combat. Some awards change the precedence and title of the recipient, giving him or her the privilege of being known as "Lord"/"Lady", "Baron", "Duchess", "Master", and so forth. High level awards are often given with the consultation of the other people who have received the award, such as peerages and consulting orders.
Rule by right of arms.
Each SCA kingdom is "ruled" by a king and queen chosen by winning a Crown Tournament in armored combat. Corpora require this to be held as a "properly constituted armored combat" tournament. The winner of the Crown Tournament and his/her Consort are styled "Crown Prince and Princess" and serve an advisory period (three to six months, depending upon the scheduling of the Crown Tournament) under the current King and Queen prior to acceding to the throne and ruling in their turn.
Peerage orders.
The highest ranking titles in the SCA belong to the royalty, followed by the former royalty. Former kings and queens become counts and countesses (dukes and duchesses if they have reigned more than once), and former princes and princesses of Principalities become viscounts and viscountesses. This system is not historically based, but was developed out of practical necessity early in the Society's history.
Directly beneath this "landed" nobility (current and former royalty) rank the highest awards, the Peerages. The SCA has four orders of peerage: the Order of Chivalry, awarded for skill at arms in Armored Combat; the Order of the Laurel, awarded for skill in the arts and sciences; the Order of the Pelican, awarded for outstanding service to the Society; and the Order of the Masters of Defense, awarded for skill at arms in Rapier Combat. In Several of the Kingdoms the Order of the Rose, made up of former Queens, is considered a peerage equal to the other four.
Elevation to peerage.
Peerages are bestowed by the Crown (the Sovereign and Consort) of a Kingdom. In most cases, this is done with the consent of the members of a given peerage, often at their suggestion. The Society's Bylaws state that "the Crown may elevate subjects to the Peerage by granting membership in one of the Orders conferring a Patent of Arms, after consultation with the members of the Order within the Kingdom, and in accordance with the laws and customs of the kingdom. Restriction: to advance a candidate to the Order of Knighthood, a Knight of the Society (usually the King) must bestow the accolade".
Cultural impact.
In May 1999, "The Onion" ran a front-page article headlined "Society For Creative Anachronism Seizes Control Of Russia" featuring photos of actual SCA participants from the Barony of Jaravellir (Madison, Wisconsin).
Members of the SCA are given pivotal roles in S.M. Stirling's Emberverse series, where their skills in pre-industrial technology and warfare become invaluable in helping humanity adapt when all modern technology (including firearms) ceases working.
In his conclusion to the "Space Odyssey" series, ""; Arthur C. Clarke portrays the SCA as still being active in the year 3001.
The novel "Murder at the War" ("Knightfall" in paperback edition) by Mary Monica Pulver is a murder mystery set entirely at the SCA's largest annual event, Pennsic War.
In David Weber's science fiction novel "Honor Among Enemies", main character Honor Harrington mentions that her uncle is a member of the SCA and that he taught her to shoot from the hip (the time the SCA covers having been moved up to the 19th century in the future era in which the novel is set, to include cowboy and Civil War reenactors).
In Christopher Stasheff's "Warlock" series the inhabitants of the planet Gramarye are revealed to be descended from SCA participants. A prequel, "Escape Velocity", describes how the Scadians first came to Gramarye, and how lands were assigned to the royal peers.
In "Ariel" (1983), a post-apocalyptic fantasy by Steven R. Boyett, technology suddenly stops working and sorcery and swordfight take over. Several characters who are former SCA members attribute their survival to their SCA experience.
The fantasy novel "The Folk of the Air" by Peter S. Beagle was written after the author attended a few early SCA events "circa" 1968; but he has repeatedly stated that he then studiously avoided any contact with the actual SCA itself for almost two decades, so that his description of a fictitious "League for Archaic Pleasures" would not be "contaminated" by contact with the actual real-life organization.
In "Number of the Beast", Robert A. Heinlein portrayed an SCA tournament where live weapons were used and the battles actually fought to the 'death'. The defeated combatants were either transported to an alternate reality where medical technology was advanced enough that they could be revived from any wound or transported to the alternate reality that was Valhalla. The contestants' desires were placed in sealed envelopes prior to the tournament, which were destroyed if the competitor won and obeyed if a competitor lost.
In John Ringo's "Council Wars" science-fiction series, characters with SCA or SCA-like experience help their society recover from a catastrophic loss of technology.
Critiques and criticism of the SCA.
Authenticity.
Tensions regarding the desirable degree of authenticity at SCA functions are highlighted by David Friedman in his articles "A Dying Dream" and "Concerning the C in SCA".
The accepted minimum standard for attendance at an SCA event is "an attempt at pre-17th century clothing", which leads to numerous discussions of the definition of "attempt". Some SCA events have been dedicated to particular historic events or have portions of their camping sectioned off for only strict reenactment, sometimes called "Enchanted Ground", in which much more strenuous attempts are made to keep anachronistic objects and actions out.
The distinction between the goals of fun and authenticity is an ongoing philosophical conflict within the Society. See, for example, the debates from rec.org.sca, the SCA newsgroup on USENET.
SCA members use modern elements when necessary for personal comfort or medical needs, or to promote safety (i.e. using rattan for swords or shear thickening substances for padding. Unlike some other living history groups, most SCA gatherings do not reenact a specific time or place in history. Consequently, SCA events are more self-referential to individual members' personas where several cultures and historic periods are represented at an event. Thus the SCA may be more of a subculture than a reenactment group. For instance, the discussions of the Grand Council of the SCA, an advisory group to the Board of Directors, debated this at length.
One argument in the SCA is the meaning of "Creative Anachronism". An oft-quoted though unofficial SCA motto is "The Middle Ages as they "should" have been".
Despite such criticisms, there is some educational quality to the group's activities and they have helped to foster a good deal of valuable research, especially in the area of medieval crafts.
Extent of royal influence.
The extent of the Crown's authority varies from kingdom to kingdom. Argument over the extent of royal influence is another strong element of the SCA's internal culture. One view of this can be found in Mike Woodford's "Trends of Change in the SCA".
Authenticity of determining a "king" by combat.
Actual medieval monarchs were not chosen by Tournament combat. There are, however, literary and historical bases for the custom, most famously the tournament in Sir Walter Scott's "Ivanhoe". In the Middle Ages, there were a number of different "mock king" games, some of which involved some form of combat, such as King of the Mountain or the King of Archers. In the 17th century the Cotswold Games were developed, the winner of which was declared to be "king". Also, the medieval sagas contain accounts of uniting petty kingdoms under a single king through "actual" combat.
The SCA's first event did not choose a "king". Fighters vied for the right to declare their ladies (only men fought at the first event) "fairest", later called the "Queen of Love and Beauty".

</doc>
<doc id="28982" url="http://en.wikipedia.org/wiki?curid=28982" title="Snowball Earth">
Snowball Earth

Proterozoic snowball periods</div Scale><div id=ScaleBar style="width:1px; float:left; height:550px; padding:0; background-color:#242020" />px;
 height:px;
 margin-left:0em;
 width:14em;
">px; 
">Tonianpx;
 height:px;
 margin-left:0em;
 width:14em;
">px; 
">Cryogenianpx;
 height:px;
 margin-left:0em;
 width:14em;
">px; 
">Ediacaranpx;
 height:px;
 margin-left:0em;
 width:2.1em;
">px; 
">px;
 height:24px;
 margin-left:1.4em;
 width:11.2em;
">px; 
"> Sturtian px;
 height:32px;
 margin-left:1.4em;
 width:11.2em;
">px; 
"> Marinoan px;
 height:1.56113537118px;
 margin-left:1.4em;
 width:11.2em;
">px; 
">Gaskierspx;
 height:54px;
 margin-left:1.4em;
 width:11.2em;
">px; 
"> Kaigas? px;
 height:12px;
 margin-left:0em;
 width:14em;
">px; 
"></div Timeline>px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
">px;
"></div containsTSN>  Neoproterozoic era
 Snowball Earth</div Legend>A recent estimate of the timing and duration of Proterozoic glacial periods. Note that great uncertainty surrounds the dating of pre-Gaskiers glaciations. The status of the Kaigas is not clear; its dating is very insecure and many workers do not recognise it as a glaciation. From Smith 2009. An earlier and longer possible snowball phase, the Huronian glaciation, is not shown here.
</div caption></div Container>
The Snowball Earth hypothesis posits that the Earth's surface became entirely or nearly entirely frozen at least once, sometime earlier than 650 Mya (million years ago). Proponents of the hypothesis argue that it best explains sedimentary deposits generally regarded as of glacial origin at tropical paleolatitudes, and other otherwise enigmatic features in the geological record. Opponents of the hypothesis contest the implications of the geological evidence for global glaciation, the geophysical feasibility of an ice- or slush-covered ocean, and the difficulty of escaping an all-frozen condition. There are a number of unanswered questions, including whether the Earth was a full snowball, or a "slushball" with a thin equatorial band of open (or seasonally open) water.
The geological time frames under consideration come before the sudden appearance of multicellular life forms on Earth known as the Cambrian explosion, and the most recent snowball episode may have triggered the evolution of multi-cellular life on Earth. Another, much earlier and longer, snowball episode, the Huronian glaciation, which occurred 2400 to 2100 Ma may have been triggered by the first appearance of oxygen in the atmosphere, the "Great Oxygenation Event."
History.
Sir Douglas Mawson (1882–1958), an Australian geologist and Antarctic explorer, spent much of his career studying the Neoproterozoic stratigraphy of South Australia where he identified thick and extensive glacial sediments and late in his career speculated about the possibility of global glaciation.
Mawson's ideas of global glaciation, however, were based on the mistaken assumption that the geographic position of Australia, and that of other continents where low-latitude glacial deposits are found, has remained constant through time. With the advancement of the continental drift hypothesis, and eventually plate tectonic theory, came an easier explanation for the glaciogenic sediments—they were deposited at a point in time when the continents were at higher latitudes.
In 1964, the idea of global-scale glaciation reemerged when W. Brian Harland published a paper in which he presented palaeomagnetic data showing that glacial tillites in Svalbard and Greenland were deposited at tropical latitudes. From this palaeomagnetic data, and the sedimentological evidence that the glacial sediments interrupt successions of rocks commonly associated with tropical to temperate latitudes, he argued for an ice age that was so extreme that it resulted in the deposition of marine glacial rocks in the tropics.
In the 1960s, Mikhail Budyko, a Russian climatologist, developed a simple energy-balance climate model to investigate the effect of ice cover on global climate. Using this model, Budyko found that if ice sheets advanced far enough out of the polar regions, a feedback loop ensued where the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the entire Earth was covered in ice and stabilized in a new ice-covered equilibrium. While Budyko's model showed that this ice-albedo "stability" could happen, he concluded that it had never happened, because his model offered no way to escape from such a scenario. In 1971, Aron Faegre, an American physicist, showed that a similar simple energy balance model predicted three stable global climates, one of which was snowball earth.
This model introduced Edward Norton Lorenz concept of intransitivity indicating that there could be a major jump from one climate to another, including to snowball earth.
The term "snowball Earth" was coined by Joseph Kirschvink in a short paper published in 1992 within a lengthy volume concerning the biology of the Proterozoic eon. The major contributions from this work were: (1) the recognition that the presence of banded iron formations is consistent with such a glacial episode and (2) the introduction of a mechanism with which to escape from an ice-covered Earth—the accumulation of CO2 from volcanic outgassing leading to an ultra-greenhouse effect.
Franklyn Van Houten's discovery of a consistent geological pattern in which lake levels rose and fell is now known as the "Van Houten cycle." His studies of phosphorus deposits and banded iron formations in sedimentary rocks made him an early adherent of the "snowball Earth" hypothesis postulating that the planet's surface froze more than 650 million years ago.
Interest in the snowball Earth increased dramatically after Paul F. Hoffman applied Kirschvink's ideas to a succession of Neoproterozoic sedimentary rocks in Namibia and elaborated upon the hypothesis by incorporating such observations as the occurrence of cap carbonates in the journal "Science" in 1998.
In 2010, Francis MacDonald reported evidence that Pangaea was at equatorial latitude during the Cryogenian with glacial ice at or below sea level and the associated Sturtian glaciation was global.
Evidence.
The snowball Earth hypothesis was originally devised to explain the apparent presence of glaciers at tropical latitudes. An ice-albedo feedback would result in the ice rapidly advancing to the equator once glaciers spread to within 25° to 30° of the equator, according to modelling. Therefore, the presence of glacial deposits seemingly within the tropics suggests global ice cover.
Critical to an assessment of the validity of the theory, therefore, is an understanding of the reliability and significance of the evidence that led to the belief that ice ever reached the tropics. This evidence must prove two things:
During a period of global glaciation, it must also be demonstrated that glaciers were active at different global locations at the same time, and that no other deposits of the same age are in existence.
This last point is very difficult to prove. Before the Ediacaran, the biostratigraphic markers usually used to correlate rocks are absent; therefore there is no way to prove that rocks in different places across the globe were deposited at the same time. The best that can be done is to estimate the age of the rocks using radiometric methods, which are rarely accurate to better than a million years or so.
The first two points are often the source of contention on a case-to-case basis. Many glacial features can also be created by non-glacial means, and estimating the latitude of landmasses even as little as million years ago can be riddled with difficulties.
Palaeomagnetism.
The snowball Earth hypothesis was first posited in order to explain what were then considered to be glacial deposits near the equator.
Since tectonic plates move in time, ascertaining their position at a given point in history is not easy. In addition to considerations of how the recognisable landmasses could have fit together, the latitude at which a rock was deposited can be constrained by palaeomagnetism.
When sedimentary rocks form, magnetic minerals within them tend to align themselves with the Earth's magnetic field. Through the precise measurement of this palaeomagnetism, it is possible to estimate the latitude (but not the longitude) where the rock matrix was deposited. Paleomagnetic measurements have indicated that some sediments of glacial origin in the Neoproterozoic rock record were deposited within 10 degrees of the equator, although the accuracy of this reconstruction is in question.
This palaeomagnetic location of apparently glacial sediments (such as dropstones) has been taken to suggest that glaciers extended to sea-level in the tropical latitudes.
It is not clear whether this can be taken to imply a global glaciation, or the existence of localised, possibly land-locked, glacial regimes. Others have even suggested that most data do not constrain any glacial deposits to within 25° of the equator.
Skeptics suggest that the palaeomagnetic data could be corrupted if the Earth's magnetic field was substantially different from today's. Depending on the rate of cooling of the Earth's core, it is possible that during the Proterozoic, its magnetic field did not approximate a dipolar distribution, with a North and South pole roughly aligning with the planet's axis as they do today. Instead, a hotter core may have circulated more vigorously and given rise to 4, 8 or more poles. Paleomagnetic data would then have to be re-interpreted as particles could align pointing to a 'West Pole' rather than the North Pole. Alternatively, the Earth's dipolar field could have oriented such that the poles were close to the equator. This hypothesis has been posited to explain the extraordinarily rapid motion of the magnetic poles implied by the Ediacaran palaeomagnetic record; the alleged motion of the north pole would occur around the same time as the Gaskiers glaciation.
Another weakness of reliance on palaeomagnetic data is the difficulty in determining whether the magnetic signal recorded is original, or whether it has been reset by later activity. For example, a mountain-building orogeny releases hot water as a by-product of metamorphic reactions; this water can circulate to rocks thousands of kilometers away and reset their magnetic signature. This makes the authenticity of rocks older than a few million years difficult to determine without painstaking mineralogical observations. Moreover, further evidence is accumulating that large-scale remagnetization events have taken place, that may require revision of the position of the paleomagnetic poles.
There is currently only one deposit, the Elatina deposit of Australia, that was indubitably deposited at low latitudes; its depositional date is well constrained, and the signal is demonstrably original.
Low-latitude glacial deposits.
Sedimentary rocks that are deposited by glaciers have distinctive features that enable their identification. Long before the advent of the "snowball Earth" hypothesis many Neoproterozoic sediments had been interpreted as having a glacial origin, including some apparently at tropical latitudes at the time of their deposition. However, it is worth remembering that many sedimentary features traditionally associated with glaciers can also be formed by other means. Thus the glacial origin of many of the key occurrences for snowball Earth has been contested.
As of 2007, there was only one "very reliable" – still challenged – datum point identifying tropical tillites, which makes statements of equatorial ice cover somewhat presumptuous. However evidence of sea-level glaciation in the tropics during the Sturtian is accumulating.
Evidence of possible glacial origin of sediment includes:
Open-water deposits.
It appears that some deposits formed during the snowball period could only have been formed in the presence of an active hydrological cycle. Bands of glacial deposits up to 5,500 meters thick, separated by small (meters) bands of non-glacial sediments, demonstrate that glaciers were melting and re-forming repeatedly for tens of millions of years; solid oceans would not permit this scale of deposition. It is considered possible that ice streams such as seen in Antarctica today could be responsible for these sequences.
Further, sedimentary features that could only form in open water, for example wave-formed ripples, far-traveled ice-rafted debris and indicators of photosynthetic activity, can be found throughout sediments dating from the snowball Earth periods. While these may represent 'oases' of meltwater on a completely frozen Earth, computer modelling suggests that large areas of the ocean must have remained ice free arguing that a "hard" snowball is not plausible in terms of energy balance and general circulation models.
Carbon isotope ratios.
There are two stable isotopes of carbon in sea water: carbon-12 (12C) and the rare carbon-13 (13C), which makes up about 1.109 percent of carbon atoms.
Biochemical processes, of which photosynthesis is one, tend to preferentially incorporate the lighter 12C isotope. Thus ocean-dwelling photosynthesizers, both protists and algae, tend to be very slightly depleted in 13C, relative to the abundance found in the primary volcanic sources of the Earth's carbon. Therefore, an ocean with photosynthetic life will have a lower 13C/12C ratio within organic remains, and a lower ratio in corresponding ocean water. The organic component of the lithified sediments will forever remain very slightly, but measurably, depleted in 13C.
During the proposed episode of snowball Earth, there are rapid and extreme negative excursions in the ratio of 13C to 12C. This is consistent with a deep freeze that killed off most or nearly all photosynthetic life – although other mechanisms, such as clathrate release, can also cause such perturbations. Close analysis of the timing of 13C 'spikes' in deposits across the globe allows the recognition of four, possibly five, glacial events in the late Neoproterozoic.
Banded iron formations.
Banded iron formations (BIF) are sedimentary rocks of layered iron oxide and iron-poor chert. In the presence of oxygen, iron naturally rusts and becomes insoluble in water. The banded iron formations are commonly very old and their deposition is often related to the oxidation of the Earth's atmosphere during the Paleoproterozoic era, when dissolved iron in the ocean came in contact with photosynthetically produced oxygen and precipitated out as iron oxide.
The bands were produced at the tipping point between an anoxic and an oxygenated ocean. Since today's atmosphere is oxygen rich (nearly 21 percent by volume) and in contact with the oceans, it is not possible to accumulate enough iron oxide to deposit a banded formation. The only extensive iron formations that were deposited after the Paleoproterozoic (after 1.8 billion years ago) are associated with Cryogenian glacial deposits.
For such iron-rich rocks to be deposited there would have to be anoxia in the ocean, so that much dissolved iron (as ferrous oxide) could accumulate before it met an oxidant that would precipitate it as ferric oxide. For the ocean to become anoxic it must have limited gas exchange with the oxygenated atmosphere. Proponents of the hypothesis argue that the reappearance of BIF in the sedimentary record is a result of limited oxygen levels in an ocean sealed by sea ice, while opponents suggest that the rarity of the BIF deposits may indicate that they formed in inland seas.
Being isolated from the oceans, such lakes may have been stagnant and anoxic at depth, much like today's Black Sea; a sufficient input of iron could provide the necessary conditions for BIF formation. A further difficulty in suggesting that BIFs marked the end of the glaciation is that they are found interbedded with glacial sediments. BIFs are also strikingly absent during the Marinoan glaciation.
Cap carbonate rocks.
 Around the top of Neoproterozoic glacial deposits there is commonly a sharp transition into a chemically precipitated sedimentary limestone or dolostone metres to tens of metres thick. These cap carbonates sometimes occur in sedimentary successions that have no other carbonate rocks, suggesting that their deposition is result of a profound aberration in ocean chemistry.
These cap carbonates have unusual chemical composition, as well as strange sedimentary structures that are often interpreted as large ripples.
The formation of such sedimentary rocks could be caused by a large influx of positively charged ions, as would be produced by rapid weathering during the extreme greenhouse following a snowball Earth event. The δ13C isotopic signature of the cap carbonates is near −5 ‰, consistent with the value of the mantle — such a low value is usually/could be taken to signify an absence of life, since photosynthesis usually acts to raise the value; alternatively the release of methane deposits could have lowered it from a higher value, and counterbalance the effects of photosynthesis.
The precise mechanism involved in the formation of cap carbonates is not clear, but the most cited explanation suggests that at the melting of a snowball Earth, water would dissolve the abundant CO2 from the atmosphere to form carbonic acid, which would fall as acid rain. This would weather exposed silicate and carbonate rock (including readily attacked glacial debris), releasing large amounts of calcium, which when washed into the ocean would form distinctively textured layers of carbonate sedimentary rock. Such an abiotic "cap carbonate" sediment can be found on top of the glacial till that gave rise to the snowball Earth hypothesis.
However, there are some problems with the designation of a glacial origin to cap carbonates. Firstly, the high carbon dioxide concentration in the atmosphere would cause the oceans to become acidic, and dissolve any carbonates contained within — starkly at odds with the deposition of cap carbonates. Further, the thickness of some cap carbonates is far above what could reasonably be produced in the relatively quick deglaciations. The cause is further weakened by the lack of cap carbonates above many sequences of clear glacial origin at a similar time and the occurrence of similar carbonates within the sequences of proposed glacial origin. An alternative mechanism, which may have produced the Doushantuo cap carbonate at least, is the rapid, widespread release of methane. This accounts for incredibly low — as low as −48 ‰ — δ13C values — as well as unusual sedimentary features which appear to have been formed by the flow of gas through the sediments.
Changing acidity.
Isotopes of the element boron suggest that the pH of the oceans dropped dramatically before and after the Marinoan glaciation.
This may indicate a buildup of carbon dioxide in the atmosphere, some of which would dissolve into the oceans to form carbonic acid. Although the boron variations may be evidence of extreme climate change, they need not imply a global glaciation.
Space dust.
The Earth's surface is very depleted in the element iridium, which primarily resides in the Earth's core. The only significant source of the element at the surface is cosmic particles that reach Earth. During a snowball Earth, iridium would accumulate on the ice sheets, and when the ice melted the resulting layer of sediment would be rich in iridium. An iridium anomaly has been discovered at the base of the cap carbonate formations, and has been used to suggest that the glacial episode lasted for at least 3 million years, but this does not necessarily imply a "global" extent to the glaciation; indeed, a similar anomaly could be explained by the impact of a large meteorite.
Cyclic climate fluctuations.
Using the ratio of mobile cations to those that remain in soils during chemical weathering (the chemical index of alteration), it has been shown that chemical weathering varied in a cyclic fashion within a glacial succession, increasing during interglacial periods and decreasing during cold and arid glacial periods. This pattern, if a true reflection of events, suggests that the "snowball Earths" bore a stronger resemblance to Pleistocene ice age cycles than to a completely frozen Earth.
What's more, glacial sediments of the Portaskaig formation in Scotland clearly show interbedded cycles of glacial and shallow marine sediments. The significance of these deposits is highly reliant upon their dating. Glacial sediments are difficult to date, and the closest dated bed to the Portaskaig group is 8 km stratigraphically above the beds of interest. Its dating to 600 Ma means the beds can be tentatively correlated to the Sturtian glaciation, but they may represent the advance or retreat of a snowball Earth.
Mechanisms.
The initiation of a snowball Earth event would involve some initial cooling mechanism, which would result in an increase in the Earth's coverage of snow and ice. The increase in Earth's coverage of snow and ice would in turn increase the Earth's albedo, which would result in positive feedback for cooling. If enough snow and ice accumulates, runaway cooling would result. This positive feedback is facilitated by an equatorial continental distribution, which would allow ice to accumulate in the regions closer to the equator, where solar radiation is most direct.
Many possible triggering mechanisms could account for the beginning of a snowball Earth, such as the eruption of a supervolcano, a reduction in the atmospheric concentration of greenhouse gases such as methane and/or carbon dioxide, changes in solar energy output, or perturbations of the Earth's orbit. Regardless of the trigger, initial cooling results in an increase in the area of the Earth's surface covered by ice and snow, and the additional ice and snow reflects more solar energy back to space, further cooling the Earth and further increasing the area of the Earth's surface covered by ice and snow. This positive feedback loop could eventually produce a frozen equator as cold as modern-day Antarctica.
Global warming associated with large accumulations of carbon dioxide in the atmosphere over millions of years, emitted primarily by volcanic activity, is the proposed trigger for melting a snowball Earth. Due to positive feedback for melting, the eventual melting of the snow and ice covering most of the Earth's surface would require as little as 1,000 years.
Continental distribution.
A tropical distribution of the continents is, perhaps counter-intuitively, necessary to allow the initiation of a snowball Earth.
Firstly, tropical continents are more reflective than open ocean, and so absorb less of the Sun's heat: most absorption of solar energy on Earth today occurs in tropical oceans.
Further, tropical continents are subject to more rainfall, which leads to increased river discharge — and erosion.
When exposed to air, silicate rocks undergo weathering reactions which remove carbon dioxide from the atmosphere. These reactions proceed in the general form: Rock-forming mineral + CO2 + H2O → cations + bicarbonate + SiO2. An example of such a reaction is the weathering of wollastonite:
The released calcium cations react with the dissolved bicarbonate in the ocean to form calcium carbonate as a chemically precipitated sedimentary rock. This transfers carbon dioxide, a greenhouse gas, from the air into the geosphere, and, in steady-state on geologic time scales, offsets the carbon dioxide emitted from volcanoes into the atmosphere.
A paucity of suitable sediments for analysis makes precise continental distribution during the Neoproterozoic difficult to establish. Some reconstructions point towards polar continents — which have been a feature of all other major glaciations, providing a point upon which ice can nucleate. Changes in ocean circulation patterns may then have provided the trigger of snowball Earth.
Additional factors that may have contributed to the onset of the Neoproterozoic snowball include the introduction of atmospheric free oxygen, which may have reached sufficient quantities to react with methane in the atmosphere, oxidizing it to carbon dioxide, a much weaker greenhouse gas, and a younger — thus fainter — Sun, which would have emitted 6 percent less radiation in the Neoproterozoic.
Normally, as the Earth gets colder due to natural climatic fluctuations and changes in incoming solar radiation, the cooling slows these weathering reactions. As a result, less carbon dioxide is removed from the atmosphere and the Earth warms as this greenhouse gas accumulates — this 'negative feedback' process limits the magnitude of cooling. During the Cryogenian period, however, the Earth's continents were all at tropical latitudes, which made this moderating process less effective, as high weathering rates continued on land even as the Earth cooled. This let ice advance beyond the polar regions. Once ice advanced to within 30° of the equator, a positive feedback could ensue such that the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the whole Earth is ice-covered.
Polar continents, due to low rates of evaporation, are too dry to allow substantial carbon deposition — restricting the amount of atmospheric carbon dioxide that can be removed from the Carbon cycle. A gradual rise of the proportion of the isotope carbon-13 relative to carbon-12 in sediments pre-dating "global" glaciation indicates that CO2 draw-down before snowball Earths was a slow and continuous process.
The start of snowball Earths are always marked by a sharp downturn in the δ13C value of sediments, a hallmark that may be attributed to a crash in biological productivity as a result of the cold temperatures and ice-covered oceans.
During the frozen period.
Global temperature fell so low that the equator was as cold as modern-day Antarctica. This low temperature was maintained by the high albedo of the ice sheets, which reflected most incoming solar energy into space. A lack of heat-retaining clouds, caused by water vapor freezing out of the atmosphere, amplified this effect.
Breaking out of global glaciation.
The carbon dioxide levels necessary to unfreeze the Earth have been estimated as being 350 times what they are today, about 13% of the atmosphere. Since the Earth was almost completely covered with ice, carbon dioxide could not be withdrawn from the atmosphere by release of alkaline metal ions weathering out of siliceous rocks. Over 4 to 30 million years, enough CO2 and methane, mainly emitted by volcanoes, would accumulate to finally cause enough greenhouse effect to make surface ice melt in the tropics until a band of permanently ice-free land and water developed; this would be darker than the ice, and thus absorb more energy from the Sun — initiating a "positive feedback".
On the continents, the melting of glaciers would release massive amounts of glacial deposit, which would erode and weather. The resulting sediments supplied to the ocean would be high in nutrients such as phosphorus, which combined with the abundance of CO2 would trigger a cyanobacteria population explosion, which would cause a relatively rapid reoxygenation of the atmosphere, which may have contributed to the rise of the Ediacaran biota and the subsequent Cambrian explosion — a higher oxygen concentration allowing large multicellular lifeforms to develop. This positive feedback loop would melt the ice in geological short order, perhaps less than 1,000 years; replenishment of atmospheric oxygen and depletion of the CO2 levels would take further millennia.
Destabilization of substantial deposits of methane hydrates locked up in low-latitude permafrost may also have acted as a trigger and/or strong positive feedback for deglaciation and warming.
It is possible that carbon dioxide levels fell enough for Earth to freeze again; this cycle may have repeated until the continents had drifted to more polar latitudes.
More recent evidence suggests that with colder oceanic temperatures, the resulting higher ability of the oceans to dissolve gases led to the carbon content of sea water being more quickly oxidized to carbon dioxide. This leads directly to an increase of atmospheric carbon dioxide, enhanced greenhouse warming of the surface of the Earth, and the prevention of a total snowball state.
Slushball Earth hypothesis.
While the presence of glaciers is not disputed, the idea that the entire planet was covered in ice is more contentious, leading some scientists to posit a "slushball Earth", in which a band of ice-free, or ice-thin, waters remains around the equator, allowing for a continued hydrologic cycle.
This hypothesis appeals to scientists who observe certain features of the sedimentary record that can only be formed under open water, or rapidly moving ice (which would require somewhere ice-free to move to). Recent research observed geochemical cyclicity in clastic rocks, showing that the "snowball" periods were punctuated by warm spells, similar to ice age cycles in recent Earth history. Attempts to construct computer models of a snowball Earth have also struggled to accommodate global ice cover without fundamental changes in the laws and constants which govern the planet.
A less extreme snowball earth hypothesis involves continually evolving continental configurations and changes in ocean circulation. Synthesised evidence has produced models indicating a "slushball Earth", where the stratigraphic record does not permit postulating complete global glaciations. Kirschivink's original hypothesis had recognised that warm tropical puddles would be expected to exist in a snowball earth.
The snowball Earth hypothesis does not explain the alternation of glacial and interglacial events, nor the oscillation of glacial sheet margins.
Scientific dispute.
The argument against the hypothesis is evidence of fluctuation in ice cover and melting during "snowball Earth" deposits. Evidence for such melting comes from evidence of glacial dropstones, geochemical evidence of climate cyclicity, and interbedded glacial and shallow marine sediments. A longer record from Oman, constrained to 13°N, covers the period from 712 to 545 million years ago — a time span containing the Sturtian and Marinoan glaciations — and shows both glacial and ice-free deposition.
There have been difficulties in recreating a snowball Earth with global climate models. Simple GCMs with mixed-layer oceans can be made to freeze to the equator; a more sophisticated model with a full dynamic ocean (though only a primitive sea ice model) failed to form sea ice to the equator. In addition, the levels of CO2 necessary to melt a global ice cover have been calculated to be 130,000 ppm, which is considered by some to be unreasonably large.
Strontium isotopic data have been found to be at odds with proposed snowball Earth models of silicate weathering shutdown during glaciation and rapid rates immediately post-glaciation. Therefore, methane release from permafrost during marine transgression was proposed to be the source of the large measured carbon excursion in the time immediately after glaciation.
"Zipper rift" hypothesis.
Nick Eyles suggest that the Neoproterozoic Snowball Earth was in fact no different from any other glaciation in Earth's history, and that efforts to find a single cause are likely to end in failure. The "Zipper rift" hypothesis proposes two pulses of continental "unzipping" — first, the breakup of the supercontinent Rodinia, forming the proto-Pacific ocean; then the splitting of the continent Baltica from Laurentia, forming the proto-Atlantic — coincided with the glaciated periods.
The associated tectonic uplift would form high plateaus, just as the East African Rift is responsible for high topography; this high ground could then host glaciers.
Banded iron formations have been taken as unavoidable evidence for global ice cover, since they require dissolved iron ions and anoxic waters to form; however, the limited extent of the Neoproterozoic banded iron deposits means that they may not have formed in frozen oceans, but instead in inland seas. Such seas can experience a wide range of chemistries; high rates of evaporation could concentrate iron ions, and a periodic lack of circulation could allow anoxic bottom water to form.
Continental rifting, with associated subsidence, tends to produce such landlocked water bodies. This rifting, and associated subsidence, would produce the space for the fast deposition of sediments, negating the need for an immense and rapid melting to raise the global sea levels.
High-obliquity hypothesis.
A competing hypothesis to explain the presence of ice on the equatorial continents was that the Earth's axial tilt was quite high, in the vicinity of 60°, which would place the Earth's land in high "latitudes", although supporting evidence is scarce. A less extreme possibility would be that it was merely the Earth's magnetic pole that wandered to this inclination, as the magnetic readings which suggested ice-filled continents depends on the magnetic and rotational poles being relatively similar. In either of these two situations, the freeze would be limited to relatively small areas, as is the case today; severe changes to the Earth's climate are not necessary.
Inertial interchange true polar wander.
The evidence for low-latitude glacial deposits during the supposed snowball Earth episodes has been reinterpreted via the concept of inertial interchange true polar wander (IITPW).
This hypothesis, created to explain palaeomagnetic data, suggests that the Earth's axis of rotation shifted one or more times during the general timeframe attributed to snowball Earth. This could feasibly produce the same distribution of glacial deposits without requiring any of them to have been deposited at equatorial latitude. While the physics behind the proposition is sound, the removal of one flawed data point from the original study rendered the application of the concept in these circumstances unwarranted.
Several alternative explanations for the evidence have been proposed.
Survival of life through frozen periods.
A tremendous glaciation would curtail photosynthetic life on Earth, thus letting the atmospheric oxygen be drastically depleted and perhaps even disappear, and thus allow non-oxidized iron-rich rocks to form.
Detractors argue that this kind of glaciation would have made life extinct entirely. However, microfossils such as stromatolites and oncolites prove that, in shallow marine environments at least, life did not suffer any perturbation. Instead life developed a trophic complexity and survived the cold period unscathed. Proponents counter that it may have been possible for life to survive in these ways:
However, organisms and ecosystems, as far as it can be determined by the fossil record, do not appear to have undergone the significant change that would be expected by a mass extinction. With the advent of more precise dating, a phytoplankton extinction event which had been associated with snowball Earth was shown to precede glaciations by 16 million years. Even if life were to cling on in all the ecological refuges listed above, a whole-Earth glaciation would result in a biota with a noticeably different diversity and composition. This change in diversity and composition has not yet been observed– in fact, the organisms which should be most susceptible to climatic variation emerge unscathed from the snowball Earth.
Implications.
A snowball Earth has profound implications in the history of life on Earth. While many refugia have been postulated, global ice cover would certainly have ravaged ecosystems dependent on sunlight. Geochemical evidence from rocks associated with low-latitude glacial deposits have been interpreted to show a crash in oceanic life during the glacials.
The melting of the ice may have presented many new opportunities for diversification, and may indeed have driven the rapid evolution which took place at the end of the Cryogenian period.
Effect on early evolution.
The Neoproterozoic was a time of remarkable diversification of multicellular organisms, including animals. Organism size and complexity increased considerably after the end of the snowball glaciations. This development of multicellular organisms may have been the result of increased evolutionary pressures resulting from multiple icehouse-hothouse cycles; in this sense, snowball Earth episodes may have "pumped" evolution. Alternatively, fluctuating nutrient levels and rising oxygen may have played a part. Interestingly, another major glacial episode may have ended just a few million years before the Cambrian explosion.
Mechanistically, the impact of snowball Earth (in particular the later glaciations) on complex life is likely to have occurred through the process of kin selection. Organ-scale differentiation, in particular the terminal (irreversible) differentiation present in animals, requires the individual cell (and the genes contained within it) to "sacrifice" their ability to reproduce, so that the colony is not disrupted. From the short-term perspective of the gene, more offspring will be gained by causing the cell in which it is contained to ignore any signals received from the colony, and to reproduce at the maximum rate, regardless of the implications for the wider group. Today, this incentive explains the formation of tumours in animals and plants.
It has been argued that because snowball Earth would undoubtedly have decimated the population size of any given species, the extremely small populations that resulted would all have been descended from a small number of individuals (see founder effect), and consequently the average relatedness between any two individuals (in this case individual cells) would have been exceptionally high as a result of glaciations. Altruism is known to increase from rarity when relatedness (R) exceeds the ratio of the cost (C) to the altruist (in this case, the cell giving up its own reproduction by differentiating), to the benefit (B) to the recipient of altruism (the germ line of the colony, that reproduces as a result of the differentiation), i.e. R > C/B (see Hamilton's rule). The evolutionary pressure of the high relatedness in the context of a post-glaciation population boom may have been sufficient to overcome the reproductive cost of forming a complex animal, for the first time in Earth's history.
There is also a rival hypothesis which has been gaining currency in recent years: that early snowball Earths did not so much "affect" the evolution of life on Earth as result from it. In fact the two hypotheses are not mutually exclusive. The idea is that Earth's life forms affect the global carbon cycle and so major evolutionary events alter the carbon cycle, redistributing carbon within various reservoirs within the biosphere system and in the process temporarily lowering the atmospheric (greenhouse) carbon reservoir until the revised biosphere system settled into a new state. The Snowball I episode (of the Huronian glaciation 2.4 to 2.1 billion years) and Snowball II (of the Precambrian's Cryogenian between 580 – 850 million years and which itself had a number of distinct episodes) are respectively thought to be caused by the evolution of oxygenic photosynthesis and then the rise of more advanced multicellular animal life and life's colonization of the land.
Effects on ocean circulation.
Global ice cover, if it existed, may – in concert with geothermal heating – have led to a lively, well mixed ocean with great vertical convective circulation.
Occurrence and timing of snowball Earths.
Neoproterozoic.
There are three or four significant ice ages during the late Neoproterozoic. Of these, the Marinoan was the most significant, and the Sturtian glaciations were also truly widespread. Even the leading snowball proponent Hoffman agrees that the ~million-year-long Gaskiers glaciation did not lead to global glaciation, although it was probably as intense as the late Ordovician glaciation. The status of the Kaigas "glaciation" or "cooling event" is currently unclear; some workers do not recognise it as a glacial, others suspect that it may reflect poorly dated strata of Sturtian association, and others believe it may indeed be a third ice age. It was certainly less significant than the Sturtian or Marinoan glaciations, and probably not global in extent. Emerging evidence suggests that the Earth underwent a number of glaciations during the Neoproterozoic, which would stand strongly at odds with the snowball hypothesis.
Paleoproterozoic.
The snowball Earth hypothesis has been invoked to explain glacial deposits in the Huronian Supergroup of Canada, though the palaeomagnetic evidence that suggests ice sheets at low latitudes is contested. The glacial sediments of the Makganyene formation of South Africa are slightly younger than the Huronian glacial deposits (~2.25 billion years old) and were deposited at tropical latitudes. It has been proposed that rise of free oxygen that occurred during the Great Oxygenation Event removed methane in the atmosphere through oxidation. As the Sun was notably weaker at the time, the Earth's climate may have relied on methane, a powerful greenhouse gas, to maintain surface temperatures above freezing.
In the absence of this methane greenhouse, temperatures plunged and a snowball event could have occurred.
Karoo Ice Age.
Before the theory of continental drift, glacial deposits in Carboniferous strata in tropical continents areas such as India and South America led to speculation that the Karoo Ice Age glaciation reached into the tropics. However, a continental reconstruction shows that ice was in fact constrained to the polar parts of the supercontinent Gondwana.

</doc>
<doc id="28984" url="http://en.wikipedia.org/wiki?curid=28984" title="S.S. Lazio">
S.S. Lazio

Società Sportiva Lazio, (BIT: ) commonly referred to as Lazio (]), is a professional Italian sports club based in Rome, most known for its football activity. The society, founded in 1900, play in the Serie A and have spent most of their history in the top tier of Italian football. Lazio have been Italian champions twice, and have won the Coppa Italia six times, the Supercoppa Italiana three times, and both the UEFA Cup Winners' Cup and UEFA Super Cup on one occasion.
The club had their first major success in 1958, winning the domestic cup. In 1974 they won their first Serie A title. The past fifteen years have been the most successful period in Lazio's history, seeing them win the UEFA Cup Winners' Cup and UEFA Super Cup in 1999, the Serie A title in 2000, several domestic cups and reaching their first UEFA Cup final in 1998.
Lazio's traditional kit colours are Sky Blue shirts and white shorts with white socks, the colours are reminiscent of Rome's ancient Hellenic legacy. Their home is the 72,689 capacity Stadio Olimpico in Rome, which they share with A.S. Roma until the year 2016 when the latter will leave for the Stadio della Roma.
Lazio have a long-standing rivalry with Roma, with whom they have contested the "Derby della Capitale" (in English "Derby of the capital" or Rome derby) since 1929.
Lazio is also a sports club that participate in forty sports disciplines in total, more than any other sports association in the world.
History.
"Società Podistica Lazio" was founded on 9 January 1900 in the Prati district of Rome. Until 1910 the club played at an amateur level, until it officially joined the league competition in 1912 as soon as the Italian Football Federation began organising championships in the center and south of Italy, and reached the final of the national championship playoff three times, but never won, losing in 1913 to Pro Vercelli, in 1914 to Casale and in 1923 to Genoa 1893.
In 1927, Lazio was the only major Roman club which resisted the Fascist regime's attempts to merge all the city's teams into what would become A.S. Roma the same year.
The club played in the first organised Serie A in 1929 and, led by legendary Italian striker Silvio Piola, achieved a second place finish in 1937 – its highest pre-war result.
The 1950s produced a mix of mid and upper table results with an Italian Cup win in 1958. Lazio was relegated for the first time in 1961 to the Serie B, but returned in the top flight two years later. After a number of mid-table placements, another relegation followed in 1970–71. Back to Serie A in 1972–73, Lazio immediately emerged as surprise challengers for the Scudetto to Milan and Juventus in 1972–1973, only losing out on the final day of the season, with a team comprising captain Giuseppe Wilson, as well as midfielders Luciano Re Cecconi and Mario Frustalupi, striker Giorgio Chinaglia, and head coach Tommaso Maestrelli. Lazio improved such successes the following season, ensuring its first title in 1973–74. However, tragic deaths of Luciano Re Cecconi and scudetto trainer Tommaso Maestrelli, as well as the departure of Chinaglia, would be a triple blow for Lazio. The emergence of Bruno Giordano during this period provided some relief as he finished League top scorer in 1979, when Lazio finished 8th.
Lazio were forcibly relegated to Serie B in 1980 due to a remarkable scandal concerning illegal bets on their own matches, along with AC Milan. They remained in Italy's second division for three seasons in what would mark the darkest period in Lazio's history. They would return in 1983 and manage a last-day escape from relegation the following season. The 1984–85 season would prove harrowing, with a pitiful 15 points and bottom place finish.
In 1986, Lazio was hit with a 9-point deduction (a true deathblow back in the day of the two-point win) for a betting scandal involving player Claudio Vinazzani. An epic struggle against relegation followed the same season in Serie B, with the club led by trainer Eugenio Fascetti only avoiding relegation to the Serie C after play-off wins over Taranto and Campobasso. This would prove a turning point in the club's history, with Lazio returning to Serie A in 1988 and, under the careful financial management of Gianmarco Calleri, the consolidation of the club's position as a solid top-flight club.
The arrival of Sergio Cragnotti, in 1992, changed the club's history due to his long-term investments in new players to make the team a "scudetto" competitor. A notable early transfer during his tenure was the capture of English midfielder Paul Gascoigne from Tottenham Hotspur for £5.5million. Gascoigne's transfer to Lazio is credited with the increase of interest in Serie A in the United Kingdom during the 1990s. Cragnotti repeatedly broke transfer records in pursuit of players who were considered major stars – Juan Sebastian Veron for £18million, Christian Vieri for £19million and breaking the world transfer record, albeit only for a matter of weeks, to sign Hernan Crespo from Parma for £35million.
Lazio were Serie A runners-up in 1995, third in 1996, and fourth in 1997, then losing the championship just by one point to Milan on the last championship's match in 1999 before, with the likes of Siniša Mihajlović, Alessandro Nesta, Marcelo Salas and Pavel Nedvěd in the side, finally winning its second "scudetto" in 2000, as well as the Italian Cup in an impressive and rare (by Italian standards) "double" with Sven-Göran Eriksson (1997–2001) as manager.
Lazio had two more Coppa Italia triumphs in 1998 and 2004, as well as the last ever UEFA Cup Winners' Cup in 1999. They also reached the UEFA Cup, but lost 0–3 against Internazionale.
In addition, Lazio won the Italian Super Cup twice and defeated Manchester United in 1999 to win the European Super Cup.
In 2000, Lazio became also the first Italian football club to be quoted on the Italian "Piazza Affari" stock market.
However, with money running out, Lazio's results slowly worsened in the years; in 2002, a financial scandal involving Cragnotti and his food products multinational Cirio forced him to leave the club, and Lazio was controlled until 2004 by caretaker financial managers and a bank pool. This forced the club to sell their star players and even fan favourite captain Alessandro Nesta. In 2004 entrepreneur Claudio Lotito acquired the majority of the club.
In 2006, the club qualified to the 2006–07 UEFA Cup under coach Delio Rossi. The club was however excluded from European competitions due to their involvement in match-fixing scandal.
In the 2006–07 season, despite a later-reduced points deduction, Lazio achieved a third place finish, thus gaining qualification to the UEFA Champions League third qualifying round, where they defeated Dinamo Bucharest to get into the group phase, and ended fourth place in the group composed of Real Madrid, Werder Bremen and Olympiacos. Things in the league did not go much better with the team spending most of the season in the bottom half of the table, sparking the protests of the fans, and eventually ending the Serie A season in 12th place. In the 2008–09 season, Lazio won their fifth Coppa Italia, beating Sampdoria in the final.
Lazio started the 2009–10 season playing the Supercoppa against Inter in Beijing, and winning the match 2–1 with goals from Matuzalém and Rocchi.
Colours, badge and nicknames.
First kit ever worn by the club.
Lazio's colours of white and sky blue were inspired by the national emblem of Greece, due to the fact that Lazio is a mixed sports club this was chosen in recognition of the fact that the Ancient Olympic Games and along with it the sporting tradition in Europe is linked to Greece.
Originally Lazio wore a shirt which was divided into white and sky blue quarters, with black shorts and socks. After a while of wearing a plain white shirt very early on, Lazio reverted to the colours which they wear today. Some seasons Lazio have used a sky blue and white shirt with stripes, but usually it is sky blue with a white trim, with the white shorts and socks. The club's colours have led to their Italian nickname of "biancocelesti".
Lazio's traditional club badge and symbol is the eagle, which was chosen by founding member Luigi Bigiarelli. It is an acknowledgment to the emblem of Zeus (the god of sky and thunder in Greek mythology) commonly known as the Aquila; Lazio's use of the symbol has led to two of their nicknames; "le Aquile" (the Eagles) and "Aquilotti" (Eaglets). The current club badge features a golden eagle above a white shield with a blue border; inside the shield is the club's name and a smaller tripartite shield with the colours of the club.
Stadium.
Stadio Olimpico, located on the Foro Italico, is the major stadium of Rome, Italy. It is the home of the Italian national football team, as well as of both local teams S.S. Lazio and Roma. It was opened in 1937 and after its latest renovation in 2008, the stadium has a capacity of 72,689 seats. It was the site of the 1960 Summer Olympics, but has also served as the location of the 1987 World Athletics Championships, the 1980 European Championship final, the 1990 World Cup and the Champions League Final in 1996 and 2009.
Also on the Foro Italico lies the Stadio dei Marmi, or "marble stadium", which was built in 1932 and designed by Enrico Del Debbio. It has tiers topped by 60 white marble statues that were gifts from Italian cities in commemoration of 60 athletes.
During the 1989–90 season, Lazio and Roma played their games at the Stadio Flaminio of Rome, located in the district Flaminio, because of the renovation works carried out at the Stadio Olimpico.
Supporters and rivalries.
Lazio is the sixth most supported football club in Italy and the second in Rome, with around 2% of Italian football fans supporting the club (according to "La Repubblica's" research of August 2008).
Historically the largest section of Lazio supporters in the city of Rome has come from the far northern section, creating an arch-like shape across Rome with affluent areas such as Parioli, Prati, Flaminio, Cassia and Monte Mario.
Founded in 1987, "Irriducibili Lazio" were the club's biggest ultras group for over 20 years. Usually the only time they create traditional Italian ultras displays is for the "Derby della Capitale", the match between Lazio and their main rivals, Roma. Known in English speaking countries as the "Rome derby," it is amongst the most heated and emotional footballing rivalries in the world. Lazio fan Vincenzo Paparelli was killed at one of the derby games during the 1979–80 season after being hit in the eye by an emergency rocket thrown by a Roma fan. Lazio also have a strong rivalry with Napoli and Livorno. Conversely the ultras have friendly relationships with Inter, Triestina and Hellas Verona.
Statistics and records.
Giuseppe Favalli holds Lazio's official appearance record, having made 401 over the course of 16 years from 1992 until 2004. The record for a goalkeeper is held by Luca Marchegiani, with 229 appearances, while the record for league appearances is held by Aldo Puccinelli with 339.
The all-time leading goalscorer for Lazio is Silvio Piola, with 148 goals scored. Piola, who played also with Pro Vercelli, Torino, Juventus and Novara, is also the highest goalscorer in Serie A history, with 274 goals, 49 ahead of anyone else. Simone Inzaghi (still in activity) is the all-time top goalscorer in the European Competitions, with 20 goals. He is also one of the five players who scored four goals in a single UEFA Champions League match. Tommaso Rocchi is the latest top scorer at the club.
Officially, Lazio's highest home attendance is approximately 80,000 for a Serie A match against Foggia on 12 May 1974, the match that awarded to Lazio the first "Scudetto". This is also the record for the Stadio Olimpico, including A.S. Roma and Italy national football team's matches.
Players.
Retired numbers.
12 – In the season 2003–2004, Curva Nord of Stadio Olimpico, as a sign of recognition towards the Curva Nord, considered the 12th man in the field.
Notable managers.
The following managers have all won at least one trophy when in charge of Lazio:
Società Sportiva Lazio as a company.
In 1998, during Sergio Cragnotti's period in charge, Società Sportiva Lazio became a joint stock company: Lazio were the first Italian club to do so. Currently, the Lazio shares are distributed between Claudio Lotito, who holds 66.692%, and other shareholders who own the remaining 33.308%. Along with Juventus and Roma, Lazio is one of only three Italian clubs listed on the Borsa Italiana (Italian stock exchange). Unlike the other two Italian clubs on the stock exchange there is only one significantly large share holder in Lazio.
According to The Football Money League published by consultants Deloitte, in the 2004–05 season Lazio was the twentieth highest earning football club in the world with an estimated revenue of €83 million.
Lazio was one of the few clubs that self-sustain from the financial support of shareholder, made an aggregate profit in recent seasons: 2005–06 €16,790,826; 2006–07: €99,693,224 (due to extraordinary income by the creation of S.S. Lazio Marketing & Communication spa.); 2007–08, €6,263,202; 2008–09, €1,336,576; 2009–10, €300,989.; 2010–11, €670,862; 2011–12, €580,492.
References.
</dl>

</doc>
<doc id="28990" url="http://en.wikipedia.org/wiki?curid=28990" title="Saint Ninian">
Saint Ninian

Saint Ninian (traditionally 4th-5th century) is a Christian saint first mentioned in the 8th century as being an early missionary among the Pictish peoples of what is now Scotland. For this reason he is known as the Apostle to the Southern Picts, and there are numerous dedications to him in those parts of Scotland with a Pictish heritage, throughout the Scottish Lowlands, and in parts of Northern England with a Northumbrian heritage. In Scotland, Ninian is also known as Ringan, and as Trynnian in Northern England.
Ninian's major shrine was at Whithorn in Galloway, where he is associated with the Candida Casa (Latin for 'White House'). Nothing is known about his teachings, and there is no unchallenged authority for information about his life.
A link between the Ninian of tradition and a person who actually appears in the historical record is not yet confirmed, though Finnian of Moville has gained traction as a leading candidate. This article discusses the particulars and origins of what has come to be known as the "traditional" stories of Saint Ninian.
Background.
The Southern Picts, for whom Ninian is held to be the apostle, are the Picts south of the mountains known as the Mounth, which cross Scotland north of the Firths of Clyde and Forth. That they had once been Christian is known from a 5th-century mention of them by Saint Patrick in his "Letter to Coroticus", where he refers to them as 'apostate Picts'. Patrick could not have been referring to the Northern Picts who were converted by Saint Columba in the 6th century because they were not yet Christian, and thus could not be called 'apostate'. Northumbria had established a bishopric among the Southern Picts at Abercorn in 681, under Bishop Trumwine. This effort was abandoned shortly after the Picts defeated the Northumbrians at the Battle of Dun Nechtain in 685.
Christianity had flourished in Galloway in the 6th century. by the time of Bede's account in 731, the Northumbrians had enjoyed an unbroken relationship with Galloway for a century or longer, beginning with the Northumbrian predecessor state of Bernicia. The full nature of the relationship is uncertain. Also at this time, Northumbria was establishing bishoprics in its sphere of influence, to be subordinate to the Northumbrian Archbishop of York. One such bishopric was established at Whithorn in 731, and Bede's account serves to support the legitimacy of the new Northumbrian bishopric. The Bernician name "hwit ærn" is Old English for the Latin "candida casa", or 'white house' in modern English, and it has survived as the modern name of Whithorn.
There is as yet no unchallenged connection of the historical record to the person who was Bede's Ninian. However, the unlikelihood that the reputable historian Bede invented Ninian without some basis in the historical record, combined with an increased knowledge of Ireland's early saints and Whithorn's early Christian connections, has led to serious scholarly efforts to find Bede's basis. James Henthorn Todd, in his 1855 publication of the "Leabhar Imuinn" (The Book of Hymns of the Ancient Church of Ireland), suggested that it was Finnian of Moville, and that view has gained traction among modern scholars.
Traditional story.
The earliest mention of Ninian of Whithorn is in a short passage of "The Ecclesiastical History of the English People" by the Northumbrian monk Bede in ca. 731. The 9th-century poem "Miracula Nyniae Episcopi" records some of the miracles attributed to him. A "Life of Saint Ninian" ("Vita Sancti Niniani") was written around 1160 by Ailred of Rievaulx, and in 1639 James Ussher discusses Ninian in his "Brittanicarum Ecclesiarum Antiquitates". These are the sources of information about Ninian of Whithorn, and all provide seemingly innocuous personal details about his life. However, there is no unchallenged historical evidence to support any of their stories, and all sources had political and religious agendas that were served by their accounts of Saint Ninian (discussed below).
Tradition holds that Ninian was a Briton who had studied in Rome, that he established an episcopal see at the "Candida Casa" in Whithorn, that he named the see for Saint Martin of Tours, that he converted the southern Picts to Christianity, and that he is buried at Whithorn. Variations of the story add that he had actually met Saint Martin, that his father was a Christian king, and that he was buried in a stone sarcophagus near the altar of his church. Further variations assert that he left for Ireland, and died there in 432. Dates for his birth are derived from the traditional mention of Saint Martin, who died in 397.
Bede's contribution (ca. 731).
Bede says that Ninian (whose name he only renders in the ablative case "Nynia") was a Briton who had been instructed in Rome; that he made his church of stone, which was unusual among the Britons; that his episcopal see was named after Saint Martin of Tours; that he preached to and converted the southern Picts; that his base was in a place called " Ad Candidam Casam", which was in the province of the Bernicians; and that he was buried there, along with many other saints.
Bede's information is minimal and he does not claim it as fact, asserting only that he is passing on "traditional" information. He provides the first historical reference to Saint Ninian, in a passing reference contained in the final part of a single paragraph.
Aelred's contribution (ca. 1160).
Leaving aside the tales regarding miracles, in the "Vita Sancti Niniani" Aelred includes the following incidental information regarding Saint Ninian: that his father was a Christian king; that he was consecrated a bishop in Rome and that he met Saint Martin in Tours; that Saint Martin sent masons with him on his homeward journey, at his request; that these masons built a church of stone, situated on the shore, and that on learning of Saint Martin's death, Ninian dedicated the church to him; that a certain rich and powerful "King Tuduvallus" was converted by him; that he died after having converted the Picts and returned home, being buried in a stone sarcophagus near the altar of his church; and that he had once travelled with his brother, named "Plebia".
Aelred said that in addition to finding information about Ninian in Bede, he took much additional information for his "Life of S. Ninian" from a source written in a "barbarous language"; there is no further information about this text. Aelred wrote his "Life of S. Ninian" sometime after spending ten years at the Scottish court and thus had close connections both to the Scottish royal family and to Fergus of Galloway (who would resurrect the Bishopric of Galloway), all of whom would have been pleased to have a manuscript with such a glowing description of a Galwegian and Scottish saint. His work is what Thomas Heffernan refers to as a "sacred biography," probably intended for a politically ambitious audience.
Ussher's contribution (1639).
Ussher says that Ninian left Candida Casa for "Cluayn-coner" in Ireland, and eventually died in Ireland; that his mother was a Spanish princess; that his father wished to regain him after having assented to his training for an ecclesiastical state; that a bell comes from heaven to call together his disciples; that a wooden church was raised by him, with beams delivered by stags; and that a harper with no experience at architecture was the builder of the church. He adds that a smith and his son, named respectively "Terna" and "Wyn", witnessed a miracle by Ninian and that the saint was granted lands to be called "Wytterna".
In addition, Skene attributes the "traditional" date of Ninian's death (16 September 432) ultimately to Ussher's "Life of Ninian," noting that the date is "without authority."
Ussher's contribution is often disparaged, as he both invented fictitious histories and misquoted legitimate manuscripts to suit his own purposes. Still, he had access to legitimate manuscripts, and he has contributed to some versions of the traditional stories.
Other sources.
Others who wrote of Saint Ninian used the accounts of Bede, Aelred, or Ussher, or used derivatives of them in combination with information from various manuscripts. This includes John Capgrave (1393 – 1464), John of Tinmouth (fl. ca. 1366), John Colgan (d. ca. 1657), and many others, up to the present day.
The anonymously written 8th century hagiographic "Miracula Nynie Episcopi" ("Miracles of Bishop Ninian") is discounted as a non-historical account, and copies are not widely extant.
Dedications to St Ninian.
Dedications to Saint Ninian are expressions of respect for the good works that are attributed to him, and the authenticity of the stories about him are not relevant to that point. Almost all of the dedications have their origins in the medieval era, after Aelred wrote his account.
The dedications are found throughout the lands of the ancient Picts of Scotland, throughout Scotland south of the Firths of Clyde and Forth, in Orkney and Shetland, and in parts of northern England.
Dedications on the Isle of Man date from the time of medieval Scottish dominance, and are not natively inspired.
There are dedications to St. Ninian in East Donegal and Belfast; and a spot formerly on the shore of Belfast Lough was traditionally known as St. Ninian's point, where the missionary reputedly landed after a voyage from Scotland. These connections reflect a strong Ulster-Scots heritage in both areas of Ulster.
There are also dedications elsewhere in the world where there is a Scottish heritage, such as Nova Scotia. St. Ninian's Cathedral is located in Antigonish, Nova Scotia.
There is a noticeable lack of dedications in the Scottish Highlands and Isles. 
Bibliography.
</dl>

</doc>
<doc id="28994" url="http://en.wikipedia.org/wiki?curid=28994" title="Standard Generalized Markup Language">
Standard Generalized Markup Language

The Standard Generalized Markup Language (SGML; ISO 8879:1986) is for defining generalized markup languages for documents. ISO 8879 Annex A.1 defines generalized markup:
Generalized markup is based on two postulates:
HTML was theoretically an example of an SGML-based language until HTML 5, which admits that browsers can't parse it as SGML (for compatibility reasons) and codifies exactly what they must do instead.
DocBook SGML and LinuxDoc are better examples, as they were used almost exclusively with actual SGML tools.
Standard versions.
SGML is an ISO standard: "ISO 8879:1986 Information processing — Text and office systems — Standard Generalized Markup Language (SGML)", of which there are three versions:
SGML is part of a trio of enabling ISO standards for electronic documents developed by ISO/IEC JTC1/SC34 (ISO/IEC Joint Technical Committee 1, Subcommittee 34 – Document description and processing languages) :
SGML is supported by various technical reports, in particular
History.
SGML descended from IBM's Generalized Markup Language (GML), which Charles Goldfarb, Edward Mosher, and Raymond Lorie developed in the 1960s. Goldfarb, editor of the international standard, coined the “GML” term using their surname initials. Goldfarb also wrote the definitive work on SGML syntax in "The SGML Handbook". The syntax of SGML is closer to the COCOA format. As a document markup language, SGML was originally designed to enable the sharing of machine-readable large-project documents in government, law, and industry. Many such documents must remain readable for several decades—a long time in the information technology field. SGML also was extensively applied by the military, and the aerospace, technical reference, and industrial publishing industries. The advent of the XML profile has made SGML suitable for widespread application for small-scale, general-purpose use.
Document validity.
SGML (ENR+WWW) defines two kinds of validity. According to the revised Terms and Definitions of ISO 8879 (from the public draft):
A conforming SGML document must be either a type-valid SGML document, a tag-valid SGML document, or both. Note: A user may wish to enforce additional constraints on a document, such as whether a document instance is integrally-stored or free of entity references.
A type-valid SGML document is defined by the standard as
An SGML document in which, for each document instance, there is an associated document type declaration (DTD) to whose DTD that instance conforms.
A tag-valid SGML document is defined by the standard as
An SGML document, all of whose document instances are fully tagged. There need not be a document type declaration associated with any of the instances. Note: If there is a document type declaration, the instance can be parsed with or without reference to it.
Terminology.
"Tag-validity" was introduced in SGML (ENR+WWW) to support XML which allows documents with no DOCTYPE declaration but which can be parsed without a grammar or documents which have a DOCTYPE declaration that makes no XML Infoset contributions to the document. The standard calls this "fully tagged". "Integrally stored" reflects the XML requirement that elements end in the same entity in which they started. "Reference-free" reflects the HTML requirement that entity references are for special characters and do not contain markup. SGML validity commentary, especially commentary that was made before 1997 or that is unaware of SGML (ENR+WWW), covers "type-validity" only.
The SGML emphasis on validity supports the requirement for generalized markup that "markup should be rigorous." (ISO 8879 A.1)
Syntax.
An SGML document may have three parts:
An SGML document may be composed from many entities (discrete pieces of text). In SGML, the entities and element types used in the document may be specified with a DTD, the different character sets, features, delimiter sets, and keywords are specified in the SGML Declaration to create the "concrete syntax" of the document.
Although full SGML allows implicit markup and some other kinds of tags, the XML specification (s4.3.1) states:
Each XML document has both a logical and a physical structure. Physically, the document is composed of units called entities. An entity may refer to other entities to cause their inclusion in the document. A document begins in a "root" or document entity. Logically, the document is composed of declarations, elements, comments, character references, and processing instructions, all of which are indicated in the document by explicit markup.
For introductory information on a basic, modern SGML syntax, see XML. The following material concentrates on features not in XML and is not a comprehensive summary of SGML syntax.
Optional features.
SGML generalizes and supports a wide range of markup languages as found in the mid 1980s. These ranged from terse Wiki-like syntaxes to RTF-like bracketed languages to HTML-like matching-tag languages. SGML did this by a relatively simple default "reference concrete syntax" augmented with a large number of optional features that could be enabled in the SGML Declaration. Not every SGML parser can necessarily process every SGML document. Because each processor's "System Declaration" can be compared to the document's "SGML Declaration" it is always possible to know whether a document is supported by a particular processor.
Many SGML features relate to markup minimization. Other features relate to concurrent (parallel) markup (CONCUR), to linking processing attributes (LINK), and to embedding SGML documents within SGML documents (SUBDOC).
The notion of customizable features was not appropriate for Web use, so one goal of XML was to minimize optional features. However XML's well-formedness rules cannot support Wiki-like languages, leaving them unstandardized and difficult to integrate with non-text information systems.
Concrete and abstract syntaxes.
The usual (default) SGML "concrete syntax" resembles this example, which is the default HTML concrete syntax:
SGML provides an "abstract syntax" that can be implemented in many different types of "concrete syntax". Although the markup norm is using angle brackets as start- and end- tag delimiters in an SGML document (per the standard-defined "reference concrete syntax"), it is possible to use other characters—provided a suitable "concrete syntax" is defined in the document's SGML declaration. For example, an SGML interpreter might be programmed to parse GML, wherein the tags are delimited with a left colon and a right full stop, thus, an ":e" prefix denotes an end tag: codice_1. According to the reference syntax, letter-case (upper- or lower-) is not distinguished in tag names, thus the three tags: (i) codice_2, (ii) codice_3, and (iii) codice_4 are equivalent. ("NOTE:" A concrete syntax might "change" this rule via the NAMECASE NAMING declarations).
Markup Minimization.
SGML has features for reducing the number of characters required to mark up a document, which must be enabled in the SGML Declaration. SGML processors need not support every available feature, thus allowing applications to tolerate many types of inadvertent markup omissions; however, SGML systems usually are intolerant of invalid structures. XML is intolerant of syntax omissions, and does not require a DTD for validation.
OMITTAG.
Both start tags and end tags may be omitted from a document instance, provided:
For example, if OMITTAG YES is specified in the SGML Declaration (enabling the OMITTAG feature), and the DTD includes the following declarations:
then this excerpt:
which omits two codice_6 tags and two codice_7 tags, would represent valid markup.
Note also that omitting tags is optional – the same excerpt could be tagged like this:
and would still represent valid markup.
Note: The OMITTAG feature is unrelated to the tagging of elements whose declared content is codice_8 as defined in the DTD:
Elements defined like this have no end tag, and specifying one in the document instance would result in invalid markup. This is syntactically different than XML empty elements in this regard.
SHORTREF.
Tags can be replaced with delimiter strings, for a terser markup, via the SHORTREF feature. This markup style is now associated with wiki markup, e.g. wherein two equals-signs (==), at the start of a line, are the “heading start-tag”, and two equals signs (==) after that are the “heading end-tag”.
SHORTTAG.
SGML markup languages whose concrete syntax enables the SHORTTAG VALUE feature, do not require attribute values containing only alphanumeric characters to be enclosed within quotation marks—either double codice_9 (LIT) or single codice_10 (LITA)—so that the previous markup example could be written:
One feature of SGML markup languages is the "presumptuous empty tagging", such that the empty end tag codice_11 in codice_12 "inherits" its value from the nearest previous full start tag, which, in this example, is codice_13 (in other words, it closes the most recently opened item). The expression is thus equivalent to codice_14.
NET.
Another feature is the "NET" (Null End Tag) construction: codice_15, which is structurally equivalent to codice_14.
Other features.
Additionally, the SHORTTAG NETENABL IMMEDNET feature allows shortening tags surrounding an empty text value, but forbids shortening full tags:
can be written as
wherein the first slash ( / ) stands for the NET-enabling “start-tag close” (NESTC), and the second slash stands for the NET. NOTE: XML defines NESTC with a /, and NET with an > (angled bracket)—hence the corresponding construct in XML appears as <QUOTE/>.
The third feature is 'text on the same line', allowing a markup item to be ended with a line-end; especially useful for headings and such, requiring using either SHORTREF or DATATAG minimization. For example, if the DTD includes the following declarations:
(and "&#RE;&#RS;" is a short-reference delimiter in the concrete syntax), then:
is equivalent to:
Formal characterization.
SGML has many features that defied convenient description with the popular formal automata theory and the contemporary parser technology of the 1980s and the 1990s. The standard warns in Annex H:
The SGML "model group" notation was deliberately designed to resemble the regular expression notation of automata theory, because automata theory provides a theoretical foundation for some aspects of the notion of conformance to a content model. No assumption should be made about the general applicability of automata to content models.
A report on an early implementation of a parser for basic SGML, the Amsterdam SGML Parser, notes the DTD-grammar in SGML must conform to a notion of unambiguity which closely resembles the LL(1) conditions
 and specifies various differences.
There appears to be no definitive classification of full SGML against a known class of formal grammar. Plausible classes may include tree-adjoining grammars and adaptive grammars.
XML is described as being generally parsable like a two-level grammar for non-validated XML and a Conway-style pipeline of coroutines (lexer, parser, validator) for valid XML. The SGML productions in the ISO standard are reported to be LL(3) or LL(4). XML-class subsets are reported to be expressible using a W-grammar. According to one paper, and probably considered at an "information set" or parse tree level rather than a character or delimiter level:
 The class of documents that conform to a given SGML document grammar forms an LL(1) language. ... The SGML document grammars by themselves are, however, not LL(1) grammars. 
The SGML standard does not define SGML with formal data structures, such as parse trees, however, an SGML document is constructed of a rooted directed acyclic graph (RDAG) of physical storage units known as “entities”, which is parsed into a RDAG of structural units known as “elements”. The physical graph is loosely characterized as an "entity tree", but entities might appear multiple times. Moreover, the structure graph is also loosely characterized as an "element tree", but the ID/IDREF markup allows arbitrary arcs.
The results of parsing can also be understood as a data tree in different notations; where the document is the root node, and entities in other notations (text, graphics) are child nodes. SGML provides apparatus for linking to and annotating external non-SGML entities.
The SGML standard describes it in terms of "maps" and "recognition modes" (s9.6.1). Each entity, and each element, can have an associated "notation" or "declared content type", which determines the kinds of references and tags which will be recognized in that entity and element. Also, each element can have an associated "delimiter map" (and "short reference map"), which determines which characters are treated as delimiters in context. The SGML standard characterizes parsing as a state machine switching between recognition modes. During parsing, there is a stack of maps that configure the scanner, while the tokenizer relates to the recognition modes.
Parsing involves traversing the dynamically-retrieved entity graph, finding/implying tags and the element structure, and validating those tags against the grammar. An unusual aspect of SGML is that the grammar (DTD) is used both passively — to "recognize" lexical structures, and actively — to "generate" missing structures and tags that the DTD has declared optional. End- and start- tags can be omitted, because they can be inferred. Loosely, a series of tags can be omitted only if there is a single, possible path in the grammar to imply them. It was this active use of grammars that made concrete SGML parsing difficult to formally characterize.
SGML uses the term "validation" for both recognition and generation. XML does not use the grammar (DTD) to change delimiter maps or to inform the parse modes, and does not allow tag omission; consequently, XML validation of elements is not active in the sense that SGML validation is active. SGML "without" a DTD (e.g. simple XML), is a grammar or a language; SGML "with" a DTD is a metalanguage. SGML with an SGML declaration is, perhaps, a meta-metalanguage, since it is a metalanguage whose declaration mechanism "is" a metalanguage.
SGML has an abstract syntax implemented by many possible concrete syntaxes, however, this is not the same usage as in an abstract syntax tree and as in a concrete syntax tree. In the SGML usage, a concrete syntax is a set of specific delimiters, while the abstract syntax is the set of names for the delimiters. The XML Infoset corresponds more to the programming language notion of abstract syntax introduced by John McCarthy.
Derivatives.
XML.
The W3C XML (Extensible Markup Language) is a profile (subset) of SGML designed to ease the implementation of the parser compared to a full SGML parser, primarily for use on the World Wide Web. In addition to disabling many SGML options present in the reference syntax (such as omitting tags and nested subdocuments) XML adds a number of additional restrictions on the kinds of SGML syntax. For example, despite enabling SGML shortened tag forms, XML does not allow unclosed start or end tags. It also relied on many of the additions made by the WebSGML Annex. XML currently is more widely used than full SGML. XML has lightweight internationalization based on Unicode. Applications of XML include XHTML, XQuery, XSLT, XForms, XPointer, JSP, SVG, RSS, Atom, XML-RPC, RDF/XML, and SOAP.
HTML.
While HTML was developed partially independently and in parallel with SGML, its creator Tim Berners-Lee, intended it to be an application of SGML. The design of HTML (Hyper Text Markup Language) was therefore inspired by SGML tagging, but, since no clear expansion and parsing guidelines were established, most actual HTML documents are not valid SGML documents. Later, HTML was reformulated (version 2.0) to be more of an SGML application, however, the HTML markup language has many legacy- and exception- handling features that differ from SGML's requirements. HTML 4 is an SGML application that fully conforms to ISO 8879 – SGML.
The charter for the recently revived World Wide Web Consortium HTML Working Group says, "the Group will not assume that an SGML parser is used for 'classic HTML'". Although HTML syntax closely resembles SGML syntax with the default "reference concrete syntax", HTML5 abandons any attempt to define HTML as an SGML application, explicitly defining its own which more closely match existing implementations and documents. It does, however, define an alternative XHTML serialization, which conforms to XML and therefore to SGML as well.
OED.
The second edition of the "Oxford English Dictionary" (OED) is entirely marked up with an SGML-based markup language.
The third edition is marked up as XML.
Others.
Other document markup languages are partly related to SGML and XML, but — because they cannot be parsed or validated or other-wise processed using standard SGML and XML tools — they are not considered either SGML or XML languages; the Z Format markup language for typesetting and documentation is an example.
Several modern programming languages support tags as primitive token types, or now support Unicode and regular expression pattern-matching. An example is the Scala programming language.
Applications.
Document markup languages defined using SGML are called "applications" by the standard; many pre-XML SGML applications were proprietary property of the organizations which developed them, and thus unavailable in the World Wide Web. The following list is of pre-XML SGML applications.
Open source implementations.
Significant open source implementations of SGML have included:
SP and Jade, the associated DSSSL processors, are maintained by the project, and are common parts of Linux distributions. A general archive of SGML software and materials resides at . The original HTML parser class, in Sun System's implementation of Java, is a limited-features SGML parser, using SGML terminology and concepts.

</doc>
<doc id="29000" url="http://en.wikipedia.org/wiki?curid=29000" title="Speciation">
Speciation

Speciation is the evolutionary process by which new biological species arise. The biologist Orator F. Cook was the first to coin the term 'speciation' for the splitting of lineages or "cladogenesis," as opposed to "anagenesis" or "phyletic evolution" occurring within lineages. Whether genetic drift is a minor or major contributor to speciation is the subject matter of much ongoing discussion.
There are four geographic modes of speciation in nature, based on the extent to which speciating populations are isolated from one another: allopatric, peripatric, parapatric, and sympatric. Speciation may also be induced artificially, through animal husbandry, agriculture, or laboratory experiments.
Natural speciation.
All forms of natural speciation have taken place over the course of evolution; however, debate persists as to the relative importance of each mechanism in driving biodiversity.
One example of natural speciation is the diversity of the three-spined stickleback, a marine fish that, after the last ice age, has undergone speciation into new freshwater colonies in isolated lakes and streams. Over an estimated 10,000 generations, the sticklebacks show structural differences that are greater than those seen between different genera of fish including variations in fins, changes in the number or size of their bony plates, variable jaw structure, and color differences.
Speciation rate.
There is debate as to the rate at which speciation events occur over geologic time. While some evolutionary biologists claim that speciation events have remained relatively constant over time, some palaeontologists such as Niles Eldredge and Stephen Jay Gould have argued that species usually remain unchanged over long stretches of time, and that speciation occurs only over relatively brief intervals, a view known as "punctuated equilibrium". (See last subheading under Darwin's Dilemma below.)
Allopatric.
During allopatric (from the ancient Greek "allos", "other" + Greek "patrā", "fatherland") speciation, a population splits into two geographically isolated populations (for example, by habitat fragmentation due to geographical change such as mountain building). The isolated populations then undergo genotypic and/or phenotypic divergence as: (a) they become subjected to dissimilar selective pressures; (b) they independently undergo genetic drift; (c) different mutations arise in the two populations. When the populations come back into contact, they have evolved such that they are reproductively isolated and are no longer capable of exchanging genes. Island genetics is the term associated with the tendency of small, isolated genetic pools to produce unusual traits. Examples include insular dwarfism and the radical changes among certain famous island chains, for example on Komodo. The Galápagos islands are particularly famous for their influence on Charles Darwin. During his five weeks there he heard that Galápagos tortoises could be identified by island, and noticed that finches differed from one island to another, but it was only nine months later that he reflected that such facts could show that species were changeable. When he returned to England, his speculation on evolution deepened after experts informed him that these were separate species, not just varieties, and famously that other differing Galápagos birds were all species of finches. Though the finches were less important for Darwin, more recent research has shown the birds now known as Darwin's finches to be a classic case of adaptive evolutionary radiation.
Peripatric.
In peripatric speciation, a subform of allopatric speciation, new species are formed in isolated, smaller peripheral populations that are prevented from exchanging genes with the main population. It is related to the concept of a founder effect, since small populations often undergo bottlenecks. Genetic drift is often proposed to play a significant role in peripatric speciation.
Parapatric.
In parapatric speciation, there is only partial separation of the zones of two diverging populations afforded by geography; individuals of each species may come in contact or cross habitats from time to time, but reduced fitness of the heterozygote leads to selection for behaviours or mechanisms that prevent their inter-breeding. Parapatric speciation is modelled on continuous variation within a "single", connected habitat acting as a source of natural selection rather than the effects of isolation of habitats produced in peripatric and allopatric speciation.
Parapatric speciation may be associated with differential landscape-dependent selection. Even if there is a gene flow between two populations, strong differential selection may impede assimilation and different species may eventually develop. Habitat differences may be more important in the development of reproductive isolation than the isolation time. Caucasian rock lizards "Darevskia rudis", "D. valentini" and "D. portschinskii" all hybridize with each other in their hybrid zone; however, hybridization is stronger between "D. portschinskii" and "D. rudis", which separated earlier but live in similar habitats than between "D. valentini" and two other species, which separated later but live in climatically different habitats.
Ecologists refer to parapatric and peripatric speciation in terms of ecological niches. A niche must be available in order for a new species to be successful.
Sympatric.
Sympatric speciation refers to the formation of two or more descendant species from a single ancestral species all occupying the same geographic location.
Often-cited examples of sympatric speciation are found in insects that become dependent on different host plants in the same area. However, the existence of sympatric speciation as a mechanism of speciation is still hotly contested. Scientists have argued that the evidences of sympatric speciation are in fact examples of micro-allopatric, or heteropatric speciation.
The best illustrated example of sympatric speciation is that of the cichlids of East Africa inhabiting the Rift Valley lakes, particularly Lake Victoria, Lake Malawi and Lake Tanganyika. There are over 800 described species, and according to estimate, there could be well over 1,600 species in the region. All the species have diversified from a common ancestral fish ("Oryzias latipes") about 113 million years ago. Their evolution is cited as an example of both natural and sexual selection. A 2008 study suggests that sympatric speciation has occurred in Tennessee cave salamanders. Sympatric speciation driven by ecological factors may also account for the extraordinary diversity of crustaceans living in the depths of Siberia's Lake Baikal.
Example of three-spined sticklebacks.
Freshwater three-spined sticklebacks, which have been studied by Dolph Schluter, were once thought to provide an intriguing example best explained by sympatric speciation. Schluter and colleagues found two different species of three-spined sticklebacks in each of five different lakes. Each lake contained a large benthic species with a large mouth that feeds on large prey in the littoral zone, as well as a smaller limnetic species with a smaller mouth that feeds on the small plankton in open water. DNA analysis indicated that each lake was colonized independently, presumably by a marine ancestor, after the last ice age. It also showed that the two species in each lake are more closely related to each other than they are to any of the species in the other lakes.The two species in each lake are reproductively isolated; neither mates with the other. However, aquarium tests showed that the benthic species from one lake is able to mate with the benthic species from the other lakes. Likewise the limnetic species from the different lakes are able to mate with each other. These benthic and limnetic species even display their mating preferences when presented with sticklebacks from Japanese lakes. A Canadian benthic prefers a Japanese benthic over its close limnetic relative from its own lake.
The researchers concluded that in each lake there had been great competition within a single original species for limited resources. This led to disruptive selection — competition favoring fishes at either extreme of body size and mouth size over those nearer the mean, as well as assortative mating — each size preferred mates like it. The result was a divergence into two subpopulations exploiting different food in different parts of the lake.The fact that this pattern of speciation occurred the same way on three separate occasions suggests strongly that ecological factors in a sympatric population can cause speciation.
However, the DNA evidence cited above is from mitochondrial DNA (mtDNA), which can often move easily between closely related species ("introgression") when they hybridize or engage in despeciation. A more recent study, using genetic markers from the nuclear genome, shows that limnetic forms in different lakes are more closely related to each other (and to marine lineages) than to benthic forms in the same lake. The three-spine stickleback is now usually considered an example of "double invasion" (a form of allopatric speciation) in which repeated invasions of marine forms have subsequently differentiated into benthic and limnetic forms. The three-spine stickleback provides an example of how molecular biogeographic studies that rely solely on mtDNA can be misleading, and that consideration of the genealogical history of alleles from multiple unlinked markers (i.e. nuclear genes) is necessary to infer speciation histories.
Speciation via polyploidization.
Polyploidy is a mechanism that has caused many rapid speciation events in sympatry because offspring of, for example, tetraploid x diploid matings often result in triploid sterile progeny. However, not all polyploids are reproductively isolated from their parental plants, and gene flow may still occur for example through triploid hybrid x diploid matings that produce tetraploids, or matings between meiotically unreduced gametes from diploids and gametes from tetraploids (see also hybrid speciation).
It has been suggested that many of the existing plant and most animal species have undergone an event of polyploidization in their evolutionary history. Reproduction of successful polyploid species is sometimes asexual, by parthenogenesis or apomixis, as for unknown reasons many asexual organisms are polyploid. Rare instances of polyploid mammals are known, but most often result in prenatal death.
Hawthorn fly.
One example of evolution at work is the case of the hawthorn fly, "Rhagoletis pomonella", also known as the apple maggot fly, which appears to be undergoing sympatric speciation. Different populations of hawthorn fly feed on different fruits. A distinct population emerged in North America in the 19th century some time after apples, a non-native species, were introduced. This apple-feeding population normally feeds only on apples and not on the historically preferred fruit of hawthorns. The current hawthorn feeding population does not normally feed on apples. Some evidence, such as the fact that six out of thirteen allozyme loci are different, that hawthorn flies mature later in the season and take longer to mature than apple flies; and that there is little evidence of interbreeding (researchers have documented a 4-6% hybridization rate) suggests that sympatric speciation is occurring. The emergence of the new hawthorn fly is an example of evolution in progress.
Reinforcement.
Reinforcement, also called the "Wallace effect", is the process by which natural selection increases reproductive isolation. It may occur after two populations of the same species are separated and then come back into contact. If their reproductive isolation was complete, then they will have already developed into two separate incompatible species. If their reproductive isolation is incomplete, then further mating between the populations will produce hybrids, which may or may not be fertile. If the hybrids are infertile, or fertile but less fit than their ancestors, then there will be further reproductive isolation and speciation has essentially occurred (e.g., as in horses and donkeys.)
The reasoning behind this is that if the parents of the hybrid offspring each have naturally selected traits for their own certain environments, the hybrid offspring will bear traits from both, therefore would not fit either ecological niche as well as either parent. The low fitness of the hybrids would cause selection to favor assortative mating, which would control hybridization. This is sometimes called the Wallace effect after the evolutionary biologist Alfred Russel Wallace who suggested in the late 19th century that it might be an important factor in speciation. 
Conversely, if the hybrid offspring are more fit than their ancestors, then the populations will merge back into the same species within the area they are in contact.
Reinforcement favoring reproductive isolation is required for both parapatric and sympatric speciation. Without reinforcement, the geographic area of contact between different forms of the same species, called their "hybrid zone," will not develop into a boundary between the different species. Hybrid zones are regions where diverged populations meet and interbreed. Hybrid offspring are very common in these regions, which are usually created by diverged species coming into secondary contact. Without reinforcement, the two species would have uncontrollable inbreeding. Reinforcement may be induced in artificial selection experiments as described below.
Artificial speciation.
New species have been created by domesticated animal husbandry, but the initial dates and methods of the initiation of such species are not clear. For example, domestic sheep were created by hybridisation, and no longer produce viable offspring with "Ovis orientalis", one species from which they are descended. Domestic cattle, on the other hand, can be considered the same species as several varieties of wild ox, gaur, yak, etc., as they readily produce fertile offspring with them.
The best-documented creations of new species in the laboratory were performed in the late 1980s. William Rice and G.W. Salt bred fruit flies, "Drosophila melanogaster", using a maze with three different choices of habitat such as light/dark and wet/dry. Each generation was placed into the maze, and the groups of flies that came out of two of the eight exits were set apart to breed with each other in their respective groups. After thirty-five generations, the two groups and their offspring were isolated reproductively because of their strong habitat preferences: they mated only within the areas they preferred, and so did not mate with flies that preferred the other areas. The history of such attempts is described in Rice and Hostert (1993).
Diane Dodd used a laboratory experiment to show how reproductive isolation can evolve in "Drosophila pseudoobscura" fruit flies after several generations by placing them in different media, starch- and maltose-based media.
Dodd's experiment has been easy for many others to replicate, including with other kinds of fruit flies and foods. Research in 2005 has shown that this rapid evolution of reproductive isolation may in fact be a relic of infection by "Wolbachia" bacteria.
Alternatively, these observations are consistent with the notion that sexual creatures are inherently reluctant to mate with individuals whose appearance or behavior is different from the norm. The risk that such deviations are due to heritable maladaptations is very high. Thus, if a sexual creature, unable to predict natural selection's future direction, is conditioned to produce the fittest offspring possible, it will avoid mates with unusual habits or features. Sexual creatures will then inevitably tend to group themselves into reproductively isolated species.
Genetics.
Few speciation genes have been found. They usually involve the reinforcement process of late stages of speciation. In 2008 a speciation gene causing reproductive isolation was reported. It causes hybrid sterility between related subspecies.
Hybrid speciation.
Hybridization between two different species sometimes leads to a distinct phenotype. This phenotype can also be fitter than the parental lineage and as such natural selection may then favor these individuals. Eventually, if reproductive isolation is achieved, it may lead to a separate species. However, reproductive isolation between hybrids and their parents is particularly difficult to achieve and thus hybrid speciation is considered an extremely rare event. The Mariana Mallard is thought to have arisen from hybrid speciation.
Hybridisation is an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis. Polyploids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.
Hybridization without change in chromosome number is called homoploid hybrid speciation. It is considered very rare but has been shown in "Heliconius" butterflies and sunflowers. Polyploid speciation, which involves changes in chromosome number, is a more common phenomenon, especially in plant species.
Gene transposition as a cause.
Theodosius Dobzhansky, who studied fruit flies in the early days of genetic research in 1930s, speculated that parts of chromosomes that switch from one location to another might cause a species to split into two different species. He mapped out how it might be possible for sections of chromosomes to relocate themselves in a genome. Those mobile sections can cause sterility in inter-species hybrids, which can act as a speciation pressure. In theory, his idea was sound, but scientists long debated whether it actually happened in nature. Eventually a competing theory involving the gradual accumulation of mutations was shown to occur in nature so often that geneticists largely dismissed the moving gene hypothesis.
However, 2006 research shows that jumping of a gene from one chromosome to another can contribute to the birth of new species. This validates the reproductive isolation mechanism, a key component of speciation.
Human speciation.
Humans have genetic similarities with chimpanzees and bonobos, their closest relatives, suggesting common ancestors. The central idea of biological evolution is that all life on Earth shares a common ancestor. Evolution means that we are all distant cousins: humans and oak trees, hummingbirds and whales. Human beings are all descendants of a LUCA, the last universal common ancestor. The LUCA splits into branches of different species. This split is called a speciation event. Over a large number of years, evolution produces diversity in forms of life due to evolutionary changes, such as: gene flow, mutations, migration, genetic drift, and natural selection.
The variants in shared ancestral species is said to be due to multiple genetic lineages. Roughly one-quarter of our genome shares no immediate ancestry with chimpanzees. It was determined that the human genetic lineage must have started evolving before the differentiation of humans, chimps, and gorillas.
Analysis of genetic drift and recombination using a Markov model suggests humans and chimpanzees speciated apart 4.1 million years ago. Even though there are similarities an article demonstrates that the human genome is a mosaic with respect to evolutionary history. The variants in shared ancestral species is said to be due to multiple genetic lineages.
Darwin's dilemma.
In addressing the question of the origin of species, there are two key issues: (1) what are the evolutionary mechanisms of speciation, and (2) what accounts for the separateness and individuality of species in the biota? Since Darwin's time, efforts to understand the nature of species have primarily focused on the first aspect, and it is now widely agreed that the critical factor behind the origin of new species is reproductive isolation. Next we focus on the second aspect of the origin of species.
Darwin's dilemma: Why do species exist?
In "The Origin of Species", Charles Darwin interpreted biological evolution in terms of natural selection, but was perplexed by the clustering of organisms into species. Chapter 6 of Darwin's book is entitled "Difficulties of the Theory". In discussing these "difficulties" he noted "First, why, if species have descended from other species by fine gradations, do we not everywhere see innumerable transitional forms? Why is not all nature in confusion, instead of the species being, as we see them, well defined?" This dilemma can be referred to as the absence or rarity of transitional varieties in habitat space.
Another dilemma, related to the first one, is the absence or rarity of transitional varieties in time (see diagram at the bottom of the page). Darwin pointed out that by the theory of natural selection "innumerable transitional forms must have existed", and wondered "why do we not find them embedded in countless numbers in the crust of the earth." That clearly defined species actually do exist in nature in both space and time implies that some fundamental feature of natural selection operates to generate and maintain species.
The effect of sexual reproduction on species formation.
It has been argued that the resolution of Darwin's dilemmas lies in the fact that out-crossing sexual reproduction has an intrinsic cost of rarity. The cost of rarity arises as follows. If, on a resource gradient, a large number of separate species evolve, each exquisitely adapted to a very narrow band on that gradient, each species will, of necessity, consist of very few members. Finding a mate under these circumstances may present difficulties when many of the individuals in the neighborhood belong to other species. Under these circumstances, any species that happens, by chance, to increase in population size (at the expense of one or other of its neighboring species, if the environment is saturated), this will immediately make it easier for its members to find sexual partners. The members of the neighboring species, whose population sizes have decreased, will experience greater difficulty in finding mates, and therefore form pairs less frequently than in the larger species. This has a snowball effect, with large species growing at the expense of the smaller, rarer species, eventually driving them to extinction. Eventually, only a few species remain, each distinctly different from the other. The cost of rarity not only involves the costs of failure to find a mate, but also indirect costs such as the cost of communication in seeking out a partner at low population densities.
Bernstein "et al." argue furthermore that if an environmental gradient is populated by a single species which is perfectly adapted to only a small portion of that environment, it will be difficult for better-adapted individuals to pass their adaptation on to others in region. Such advantageous characteristics are unlikely to be due to a single altered gene, but rather to a combination of several altered genes, each of which, on its own, imparts little or no benefit to its carrier. If such an individual mates with a randomly selected mate, the advantageous combination of genes will be broken up, and the advantage lost, unless it happens to mate with another individual with the same advantageous combination of altered genes. This will be an exceptionally rare event, the consequence of which is that the species will be resistant to change over time or to the budding off of new species.
Rarity brings with it other costs. A rare or unusual feature is very seldom advantageous. In most instances, it will be indicative of a (non-silent) mutation, which is almost certain to be deleterious. It therefore behooves sexual creatures to avoid mates sporting rare or unusual features. Should this be the case, then sexual populations will rapidly shed rare or peripheral phenotypic features, thus canalizing the entire external appearance, as illustrated in the accompanying illustration of the African pygmy kingfisher, "Ispidina picta". This remarkable uniformity of all the members of a sexual species has stimulated the proliferation of field guides on birds, mammals, reptiles, insects, and many other taxons, in which each species can be described in full by means of a single illustration (or a pair of illustrations if there is sufficient sexual dimorphism). Once a population has become as homogeneous in appearance as is typical of most species (and is illustrated in the photograph of the African pygmy kingfisher), its members will avoid mating with members of other populations that look different from themselves. Thus, the avoidance of mates displaying rare and unusual phenotypic features inevitably leads to reproductive isolation, one of the hallmarks of speciation.
In the contrasting case of organisms that reproduce asexually, there is no cost of rarity; consequently, there are only benefits to fine-scale adaptation. Thus, asexual organisms very frequently show the continuous variation in form (often in many different directions) that Darwin expected evolution to produce, making their classification into "species" (more correctly, "morphospecies") very difficult.
Punctuated evolution.
Evolution is imposed on species or groups. It is not planned or striven for in some Lamarckist way. The mutations on which the process depends are random events, and, except for the "silent mutations" which do not affect the functionality or appearance of the carrier, are thus usually disadvantageous, and their chance of proving to be useful in the future is vanishingly small. 
Therefore, while a species or group might benefit from being able to adapt to a new environment by accumulating a wide range of genetic variation, this is to the detriment of the "individuals" who have to carry these mutations until a small, unpredictable minority of them ultimately contributes to such an adaptation. Thus, the "capability" to evolve is a group adaptation, a concept discredited by (for example) George C. Williams, John Maynard Smith and Richard Dawkins as selectively disadvantageous to the individual.
If sexual creatures avoid mutant mates with strange or unusual characteristics, then mutations that affect the external appearance of their carriers will seldom be passed on to the next and subsequent generations. They will therefore seldom be tested by natural selection. Evolution is, therefore, effectively halted or slowed down considerably. The only mutations that can accumulate in a population are ones that have no noticeable effect on the outward appearance and functionality of their bearers (i.e., they are "silent" or "neutral mutations", which can be, and are, used to trace the relatedness and age of populations and species.)
This implies that evolution can only occur if mutant mates cannot be avoided, as a result of a severe scarcity of potential mates. This is most likely to occur in small, isolated communities. These occur most commonly on small islands, in remote valleys, lakes, river systems, or caves, or during the aftermath of a mass extinction. Under these circumstances, not only is the choice of mates severely restricted but population bottlenecks, founder effects, genetic drift and inbreeding cause rapid, random changes in the isolated population's genetic composition. Furthermore, hybridization with a related species trapped in the same isolate might introduce additional genetic changes. If an isolated population such as this survives its genetic upheavals, and subsequently expands into an unoccupied niche, or into a niche in which it has an advantage over its competitors, a new species, or subspecies, will have come in being. In geological terms this will be an abrupt event. A resumption of avoiding mutant mates will, thereafter, result, once again, in evolutionary stagnation.
Thus the fossil record of an evolutionary progression typically consists of species that suddenly appear, and ultimately disappear, in many cases close to a million years later, without any change in external appearance. Graphically, these fossil species are represented by horizontal lines, whose lengths depict how long each of them existed. The horizontality of the lines illustrates the unchanging appearance of each of the fossil species depicted on the graph. During each species' existence new species appear at random intervals, each also lasting many hundreds of thousands of years before disappearing without a change in appearance. The exact relatedness of these concurrent species is generally impossible to determine. This is illustrated in the following diagram depicting the evolution of modern humans from the time that the Hominins separated from the line that led to the evolution of our closest living primate relatives, the chimpanzees.
For similar evolutionary time lines see, for instance, the paleontological list of African dinosaurs, Asian dinosaurs, the Lampriformes and Amiiformes.

</doc>
<doc id="29004" url="http://en.wikipedia.org/wiki?curid=29004" title="SQL">
SQL

SQL (, or ; Structured Query Language) is a special-purpose programming language designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).
Originally based upon relational algebra and tuple relational calculus, SQL consists of a data definition language and a data manipulation language. The scope of SQL includes data insert, query, update and delete, schema creation and modification, and data access control. Although SQL is often described as, and to a great extent is, a declarative language (4GL), it also includes procedural elements.
SQL was one of the first commercial languages for Edgar F. Codd's relational model, as described in his influential 1970 paper, "A Relational Model of Data for Large Shared Data Banks." Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language.
SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987.<ref name="ISO/IEC"></ref> Since then, the standard has been revised to include a larger set of features. Despite the existence of such standards, though, most SQL code is not completely portable among different database systems without adjustments.
History.
SQL was initially developed at IBM by Donald D. Chamberlin and Raymond F. Boyce in the early 1970s. This version, initially called "SEQUEL" ("Structured English QUEry Language"), was designed to manipulate and retrieve data stored in IBM's original quasi-relational database management system, System R, which a group at IBM San Jose Research Laboratory had developed during the 1970s. The acronym SEQUEL was later changed to SQL because "SEQUEL" was a trademark of the UK-based Hawker Siddeley aircraft company.
In the late 1970s, Relational Software, Inc. (now Oracle Corporation) saw the potential of the concepts described by Codd, Chamberlin, and Boyce, and developed their own SQL-based RDBMS with aspirations of selling it to the U.S. Navy, Central Intelligence Agency, and other U.S. government agencies. In June 1979, Relational Software, Inc. introduced the first commercially available implementation of SQL, Oracle V2 (Version2) for VAX computers.
After testing SQL at customer test sites to determine the usefulness and practicality of the system, IBM began developing commercial products based on their System R prototype including System/38, SQL/DS, and DB2, which were commercially available in 1979, 1981, and 1983, respectively.
Syntax.
Language elements.
The SQL language is subdivided into several language elements, including:
Operators.
Conditional (CASE) expressions.
SQL has the codice_1 expression, which was introduced in SQL-92. In its most general form, which is called a "searched case" in the SQL standard, it works like else if in other programming languages:
SQL tests codice_2 conditions in the order they appear in the source. If the source does not specify an codice_3 expression, SQL defaults to codice_4. An abbreviated syntax—called "simple case" in the SQL standard—mirrors switch statements:
This syntax uses implicit equality comparisons, with the usual caveats for comparing with NULL.
For the Oracle-SQL dialect, the latter can be shortened to an equivalent codice_5 construct:
The last value is the default; if none is specified, it also defaults to codice_6.
However, unlike the standard's "simple case", Oracle's codice_5 considers two codice_6s equal with each other.
Queries.
The most common operation in SQL is the query, which is performed with the declarative codice_9 statement. codice_9 retrieves data from one or more tables, or expressions. Standard codice_9 statements have no persistent effects on the database. Some non-standard implementations of codice_9 can have persistent effects, such as the codice_13 syntax that exists in some databases.
Queries allow the user to describe desired data, leaving the database management system (DBMS) responsible for planning, optimizing, and performing the physical operations necessary to produce that result as it chooses.
A query includes a list of columns to include in the final result, immediately following the codice_9 keyword. An asterisk ("codice_15") can also be used to specify that the query should return all columns of the queried tables. codice_9 is the most complex statement in SQL, with optional keywords and clauses that include:
The following is an example of a codice_9 query that returns a list of expensive books. The query retrieves all rows from the "Book" table in which the "price" column contains a value greater than 100.00. The result is sorted in ascending order by "title". The asterisk (*) in the "select list" indicates that all columns of the "Book" table should be included in the result set.
The example below demonstrates a query of multiple tables, grouping, and aggregation, by returning a list of books and the number of authors associated with each book.
Example output might resemble the following:
 Title Authors
 SQL Examples and Guide 4
 The Joy of SQL 1
 An Introduction to SQL 2
 Pitfalls of SQL 1
Under the precondition that "isbn" is the only common column name of the two tables and that a column named "title" only exists in the "Books" table, the above query could be rewritten in the following form:
However, many vendors either do not support this approach, or require certain column naming conventions for natural joins to work effectively.
SQL includes operators and functions for calculating values on stored values. SQL allows the use of expressions in the "select list" to project data, as in the following example, which returns a list of books that cost more than 100.00 with an additional "sales_tax" column containing a sales tax figure calculated at 6% of the "price".
Subqueries.
Queries can be nested so that the results of one query can be used in another query via a relational operator or aggregation function. A nested query is also known as a "subquery". While joins and other table operations provide computationally superior (i.e. faster) alternatives in many cases, the use of subqueries introduces a hierarchy in execution that can be useful or necessary. In the following example, the aggregation function codice_33 receives as input the result of a subquery:
A subquery can use values from the outer query, in which case it is known as a correlated subquery.
Since 1999 the SQL standard allows named subqueries called common table expression (named and designed after the IBM DB2 version 2 implementation; Oracle calls these subquery factoring). CTEs can also be recursive by referring to themselves; the resulting mechanism allows tree or graph traversals (when represented as relations), and more generally fixpoint computations.
Inline View.
An Inline view is the use of referencing a SQL subquery in a FROM clause. Essentially, the inline view is a subquery that can be selected from or joined to. Inline View functionality allows the user to reference the subquery as a table. The inline view also is referred to as a derived table or a subselect. Inline view functionality was introduced in Oracle 9i. <ref name="Oracle 9i - Inline View/Derived Table"></ref>
In the following example, the SQL statement involves a join from the initial Books table to the Inline view "Sales". This inline view captures associated book sales information using the ISBN to join to the Books table. As a result, the inline view provides the result set with additional columns (the number of items sold and the company that sold the books):
Null or three-valued logic (3VL).
The concept of Null was introduced into SQL to handle missing information in the relational model. The word codice_6 is a reserved keyword in SQL, used to identify the Null special marker. Comparisons with Null, for instance equality (=) in WHERE clauses, results in an Unknown truth value. In SELECT statements SQL returns only results for which the WHERE clause returns a value of True; i.e., it excludes results with values of False and also excludes those whose value is Unknown.
Along with True and False, the Unknown resulting from direct comparisons with Null thus brings a fragment of three-valued logic to SQL. The truth tables SQL uses for AND, OR, and NOT correspond to a common fragment of the Kleene and Lukasiewicz three-valued logic (which differ in their definition of implication, however SQL defines no such operation).
There are however disputes about the semantic interpretation of Nulls in SQL because of its treatment outside direct comparisons. As seen in the table above direct equality comparisons between two NULLs in SQL (e.g. codice_35) returns a truth value of Unknown. This is in line with the interpretation that Null does not have a value (and is not a member of any data domain) but is rather a placeholder or "mark" for missing information. However, the principle that two Nulls aren't equal to each other is effectively violated in the SQL specification for the codice_36 and codice_37 operators, which do identify nulls with each other. Consequently, these set operations in SQL may produce results not representing sure information, unlike operations involving explicit comparisons with NULL (e.g. those in a codice_20 clause discussed above). In Codd's 1979 proposal (which was basically adopted by SQL92) this semantic inconsistency is rationalized by arguing that removal of duplicates in set operations happens "at a lower level of detail than equality testing in the evaluation of retrieval operations." However, computer science professor Ron van der Meyden concluded that "The inconsistencies in the SQL standard mean that it is not possible to ascribe any intuitive logical semantics to the treatment of nulls in SQL."
Additionally, since SQL operators return Unknown when comparing anything with Null directly, SQL provides two Null-specific comparison predicates: codice_39 and codice_40 test whether data is or is not Null. Universal quantification is not explicitly supported by SQL, and must be worked out as a negated existential quantification. There is also the "<row value expression> IS DISTINCT FROM <row value expression>" infixed comparison operator, which returns TRUE unless both operands are equal or both are NULL. Likewise, IS NOT DISTINCT FROM is defined as "NOT (<row value expression> IS DISTINCT FROM <row value expression>)". also introduced codice_41 type variables, which according to the standard can also hold Unknown values. In practice, a number of systems (e.g. PostgreSQL) implement the BOOLEAN Unknown as a BOOLEAN NULL.
Data manipulation.
The Data Manipulation Language (DML) is the subset of SQL used to add, update and delete data:
Transaction controls.
Transactions, if available, wrap DML operations:
codice_53 and codice_54 terminate the current transaction and release data locks. In the absence of a codice_48 or similar statement, the semantics of SQL are implementation-dependent.
The following example shows a classic transfer of funds transaction, where money is removed from one account and added to another. If either the removal or the addition fails, the entire transaction is rolled back.
Data definition.
The Data Definition Language (DDL) manages table and index structure. The most basic items of DDL are the codice_61, codice_62, codice_63, codice_64 and codice_65 statements:
Data types.
Each column in an SQL table declares the type(s) that column may contain. ANSI SQL includes the following data types.
Numbers.
For example, the number 123.45 has a precision of 5 and a scale of 2. The precision is a positive integer that determines the number of significant digits in a particular radix (binary or decimal). The scale is a non-negative integer. A scale of 0 indicates that the number is an integer. For a decimal number with scale S, the exact numeric value is the integer value of the significant digits divided by 10S.
SQL provides a function to round numerics or dates, called codice_89 (in Informix, DB2, PostgreSQL, Oracle and MySQL) or codice_90 (in Informix, SQLite, Sybase, Oracle, PostgreSQL and Microsoft SQL Server)
Date and time.
SQL provides several functions for generating a date / time variable out of a date / time string (codice_105, codice_106, codice_107), as well as for extracting the respective members (seconds, for instance) of such variables. The current system date / time of the database server can be called by using functions like codice_108.
The IBM Informix implementation provides the codice_109 and the codice_110 functions to increase the accuracy of time, for systems requiring sub-second precision.
Data control.
The Data Control Language (DCL) authorizes users to access and manipulate data.
Its two main statements are:
Example:
Procedural extensions.
SQL is designed for a specific purpose: to query data contained in a relational database. SQL is a set-based, declarative query language, not an imperative language like C or BASIC. However, extensions to Standard SQL add procedural programming language functionality, such as control-of-flow constructs. These include:
In addition to the standard SQL/PSM extensions and proprietary SQL extensions, procedural and object-oriented programmability is available on many SQL platforms via DBMS integration with other languages. The SQL standard defines SQL/JRT extensions (SQL Routines and Types for the Java Programming Language) to support Java code in SQL databases. SQL Server 2005 uses the SQLCLR (SQL Server Common Language Runtime) to host managed .NET assemblies in the database, while prior versions of SQL Server were restricted to unmanaged extended stored procedures primarily written in C. PostgreSQL lets users write functions in a wide variety of languages—including Perl, Python, Tcl, and C.
Criticism.
SQL deviates in several ways from its theoretical foundation, the relational model and its tuple calculus. In that model, a table is a set of tuples, while in SQL, tables and query results are lists of rows: the same row may occur multiple times, and the order of rows can be employed in queries (e.g. in the LIMIT clause). Whether this is a common practical concern, it is also a subject of debate. Furthermore, additional features (such as NULL and views) were introduced without founding them directly on the relational model, which makes them more difficult to interpret.
Critics argue that SQL should be replaced with a language that strictly returns to the original foundation: for example, see "The Third Manifesto". Other critics suggest that Datalog has two advantages over SQL: it has cleaner semantics, which facilitates program understanding and maintenance, and it is more expressive, in particular for recursive queries.
Another criticism is that SQL implementations are incompatible between vendors and do not necessarily completely follow standards. In particular date and time syntax, string concatenation, codice_6s, and comparison case sensitivity vary from vendor to vendor. A particular exception is PostgreSQL, which strives for standards compliance.
Popular implementations of SQL commonly omit support for basic features of Standard SQL, such as the codice_91 or codice_93 data types. The most obvious such examples, and incidentally the most popular commercial and proprietary SQL DBMSs, are Oracle (whose codice_91 behaves as codice_117, and lacks a codice_93 type) and MS SQL Server (before the 2008 version). As a result, SQL code can rarely be ported between database systems without modifications.
There are several reasons for this lack of portability between database systems:
Standardization.
SQL was adopted as a standard by the American National Standards Institute (ANSI) in 1986 as SQL-86 and the International Organization for Standardization (ISO) in 1987. Nowadays the standard is subject to continuous improvement by the Joint Technical Committee "ISO/IEC JTC 1, Information technology, Subcommittee SC 32, Data management and interchange", which affiliate to ISO as well as IEC. It is commonly denoted by the pattern: "ISO/IEC 9075-n:yyyy Part n: title", or, as a shortcut, "ISO/IEC 9075".
"ISO/IEC 9075" is complemented by "ISO/IEC 13249: SQL Multimedia and Application Packages" (SQL/MM), which defines SQL based interfaces and packages to widely spread applications like video, audio and spatial data.
Until 1996, the National Institute of Standards and Technology (NIST) data management standards program certified SQL DBMS compliance with the SQL standard. Vendors now self-certify the compliance of their products.
The original standard declared that the official pronunciation for "SQL" was an initialism: ("es queue el"). Regardless, many English-speaking database professionals (including Donald Chamberlin himself) use the acronym-like pronunciation of ("sequel"), mirroring the language's pre-release development name of "SEQUEL".
The SQL standard has gone through a number of revisions:
Interested parties may purchase SQL standards documents from ISO, IEC or ANSI. A draft of SQL:2008 is freely available as a zip archive.
The SQL standard is divided into nine parts.
ISO/IEC 9075 is complemented by ISO/IEC 13249 "SQL Multimedia and Application Packages". This closely related but separate standard is developed by the same committee. It defines interfaces and packages based on SQL. The aim is a unified access to typical database applications like text, pictures, data mining or spatial data.
Alternatives.
A distinction should be made between alternatives to SQL as a language, and alternatives to the relational model itself. Below are proposed relational alternatives to the SQL language. See navigational database and NoSQL for alternatives to the relational model.
References.
</dl>

</doc>
<doc id="29005" url="http://en.wikipedia.org/wiki?curid=29005" title="Strait of Hormuz">
Strait of Hormuz

The Strait of Hormuz Persian: تنگه هرمز‎ "Tangeh-ye Hormoz"   , Arabic: مَضيق هُرمُز‎ "Maḍīq Hurmuz") is a strait between the Gulf of Oman and the Persian Gulf. It is the only sea passage from the Persian Gulf to the open ocean and is one of the world's most strategically important choke points. On the north coast is Iran, and on the south coast is the United Arab Emirates and Musandam, an exclave of Oman. At its narrowest, the strait is 21 nmi wide.
About 20% of the world's petroleum, and about 35% of the petroleum traded by sea, passes through the strait making it a highly important strategic location for international trade.
Etymology.
The opening to the Persian Gulf was described, but not given a name, in the "Periplus of the Erythraean Sea", a 1st-century mariner's guide:
"At the upper end of these Calaei islands is a range of mountains called Calon, and there follows not far beyond, the mouth of the Persian Gulf, where there is much diving for the pearl-mussel. To the left of the straits are great mountains called Asabon and to the right there rises in full view another round and high mountain called Semiramis; between them the passage across the strait is about six hundred stadia; beyond which that very great and broad sea, the Persian Gulf, reaches far into the interior. At the upper end of this gulf there is a market-town designated by law called Apologus, situated near Charaex Spasini and the River Euphrates."—Periplus of the Erythraean Sea, Chapter 35
In the 10th to 17th centuries AD, the Kingdom of Ormus, which seems to have given the strait its name, was located here. Scholars, historians and linguists derive the name "Ormuz" from the local Persian word هورمغ "Hur-mogh" meaning date palm. In the local dialects of Hurmoz and Minab this strait is still called Hurmogh and has the aforementioned meaning.
The resemblance of this word with the name of the Persian god هرمز "Hormoz" (a variant of "Ahura Mazda") has resulted in the popular belief that these words are related.
Navigation.
To reduce the risk of collision, ships moving through the Strait follow a Traffic Separation Scheme (TSS): inbound ships use one lane, outbound ships another, each lane being two miles wide. The lanes are separated by a two-mile-wide "median".
To traverse the Strait, ships pass through the territorial waters of Iran and Oman under the transit passage provisions of the United Nations Convention on the Law of the Sea. Although not all countries have ratified the convention, most countries, including the U.S., accept these customary navigation rules as codified in the Convention.
Oman has a radar site Link Quality Indicator (LQI) to monitor the TSS in the Strait of Hormuz. This site is on a small island on the peak of Musandam Peninsula.
Traffic statistics.
According to the U.S. Energy Information Administration, in the year 2011, an average of 14 tankers per day passed out of the Persian Gulf through the Strait carrying 17 Moilbbl of crude oil. This was said to represent 35% of the world's seaborne oil shipments and 20% of oil traded worldwide. The report stated that more than 85% of these crude oil exports went to Asian markets, with Japan, India, South Korea and China the largest destinations.
A 2007 report from the Center for Strategic and International Studies also stated that 17 million barrels passed out of the Gulf daily, but that oil flows through the Strait accounted for roughly 40% of all world-traded oil.
Events.
Operation Praying Mantis.
On 18 April 1988, the U.S. Navy waged a one-day battle against Iranian forces in and around the strait. The battle, dubbed Operation Praying Mantis by the U.S., was launched in retaliation for the USS "Samuel B. Roberts" striking a mine laid in the channel by Iran on 14 April. U.S. forces sank one frigate, one gunboat, and up to six armed speedboats, as well as seriously damaging a second frigate.
The downing of Iran Air 655.
On 3 July 1988, 290 people were killed when an Iran Air Airbus A300 passenger jet was shot down over the strait by the United States Navy guided missile cruiser USS "Vincennes" when it was mistakenly identity as an attacking warplane.
Collision between USS "Newport News" and tanker "Mogamigawa".
On 8 January 2007, the nuclear submarine USS "Newport News", traveling submerged, struck , a 300,000-ton Japanese-flagged very large crude tanker, south of the strait. There were no injuries, and no oil leaked from the tanker.
Tensions in 2008.
2008 US-Iranian naval dispute.
A series of naval stand-offs between Iranian speedboats and U.S. warships in the Strait of Hormuz occurred in December 2007 and January 2008. U.S. officials accused Iran of harassing and provoking their naval vessels, but Iranian officials denied the allegations. On 14 January 2008, U.S. Navy officials appeared to contradict the Pentagon version of the 16th of January event, in which the Pentagon had reported that U.S. vessels had almost fired on approaching Iranian boats. The Navy's regional commander, Vice Admiral Kevin Cosgriff, said the Iranians had "neither anti-ship missiles nor torpedoes" and he "wouldn't characterize the posture of the US 5th Fleet as afraid of these small boats".
Iranian defence policy.
On 29 June 2008, the commander of Iran's Revolutionary Guard, Mohammad Ali Jafari, said that if either Israel or the United States attacked Iran, it would seal off the Strait of Hormuz to wreak havoc in the oil markets. This followed more ambiguous threats from Iran's oil minister and other government officials that an attack on Iran would result in turmoil in the world's oil supply.
Vice Admiral Kevin Cosgriff, commander of the U.S. 5th Fleet stationed in Bahrain across the Persian Gulf from Iran, warned that such Iranian action by would be considered an act of war, and the U.S. would not allow Iran to hold hostage nearly a third of the world's oil supply.
On 8 July 2008, Ali Shirazi, a mid-level clerical aide to Iran's Supreme Leader Ayatollah Ali Khamenei, was quoted by the student news agency ISNA as telling the Revolutionary Guards, "The Zionist regime is pressuring White House officials to attack Iran. If they commit such a stupidity, Tel Aviv and U.S. shipping in the Persian Gulf will be Iran's first targets and they will be burned."
Naval activity in 2008.
In the last week of July 2008, in the Operation Brimstone, dozens of U.S. and foreign naval ships came to undergo joint exercises for possible military activity in the shallow waters off the coast of Iran.
As of 11 August 2008, more than 40 U.S. and allied ships reportedly were en route to the Strait of Hormuz. One U.S. carrier battle group from Japan would complement the two which are already in the Persian Gulf, for a total of five battle groups, not including the submarines.
Collision between USS "Hartford" and USS "New Orleans".
On 20 March 2009, United States Navy "Los Angeles"-class submarine USS "Hartford" collided with the "San Antonio"-class amphibious transport dock USS "New Orleans" in the strait. The collision, which slightly injured 15 sailors aboard the Hartford, ruptured a fuel tank aboard the New Orleans, spilling 25000 USgal of marine diesel fuel.
U.S. Iran tensions in 2011–2012.
On 27 December 2011, Iranian Vice President Mohammad-Reza Rahimi threatened to cut off oil supply from the Strait of Hormuz should economic sanctions limit, or cut off, Iranian oil exports. A U.S. Fifth Fleet spokeswoman said in response that the Fleet was "always ready to counter malevolent actions", whilst Admiral Habibollah Sayyari of the Iranian navy claimed that cutting off oil shipments would be "easy". Despite an initial 2% rise in oil prices, oil markets ultimately did not react significantly to the Iranian threat, with oil analyst Thorbjoern Bak Jensen of Global Risk Management concluding that "they cannot stop the flow for a longer period due to the amount of U.S. hardware in the area".
On 3 January 2012, Iran threatened to take action if the U.S. Navy moves an aircraft carrier back into the Persian Gulf. Iranian Army chief Ataollah Salehi said the United States had moved an aircraft carrier out of the Gulf because of Iran's naval exercises, and Iran would take action if the ship returned. "Iran will not repeat its warning...the enemy's carrier has been moved to the Gulf of Oman because of our drill. I recommend and emphasize to the American carrier not to return to the Persian Gulf", he said.
The U.S. Navy spokesman Commander Bill Speaks quickly responded that deployment of U.S. military assets would continue as has been the custom stating: "The U.S. Navy operates under international maritime conventions to maintain a constant state of high vigilance in order to ensure the continued, safe flow of maritime traffic in waterways critical to global commerce."
While earlier statements from Iran had little effect on global oil markets, coupled with the new sanctions, these comments from Iran are driving crude futures higher, up over 4%. Pressure on prices reflect a combination of uncertainty driven further by China's recent response – reducing oil January 2012 purchases from Iran by 50% compared to those made in 2011.
The U.S. led sanctions may be "beginning to bite" as Iranian currency has recently lost some 12% of its value. Further pressure on Iranian currency was added by French Foreign Minister Alain Juppé who was quoted as calling for more "strict sanctions" and urged EU countries to follow the US in freezing Iranian central bank assets and imposing an embargo on oil exports.
On 7 January 2012, the British government announced that it would be sending the Type 45 destroyer HMS "Daring" to the Persian Gulf. "Daring", which is the lead ship of her class is one of the "most advanced warships" in the world, and will undertake its first mission in the Persian Gulf. The British Government however have said that this move has been long-planned, as "Daring" will replace another Armilla patrol frigate.
On 9 January 2012, Iranian Defense Minister Ahmad Vahidi denied that Iran had ever claimed that it would close the Strait of Hormuz, saying that "the Islamic Republic of Iran is the most important provider of security in the strait... if one threatens the security of the Persian Gulf, then all are threatened."
The Iranian Foreign Ministry confirmed on 16 January 2012 that it has received a letter from the United States concerning the Strait of Hormuz, "via three different channels." Authorities were considering whether to reply, although the contents of the letter were not divulged. The United States had previously announced its intention to warn Iran that closing the Strait of Hormuz is a "red line" that would provoke an American response. Gen. Martin E. Dempsey, the chairman of the Joint Chiefs of Staff, said this past weekend that the United States would "take action and re-open the strait,” which could be accomplished only by military means, including minesweepers, warship escorts and potentially airstrikes. Defense Secretary Leon E. Panetta told troops in Texas that the United States would not tolerate Iran's closing of the strait. Nevertheless Iran continued to discuss the impact of shutting the Strait on world oil markets, saying that any disruption of supply would cause a shock to markets that "no country" could manage.
By 23 January, a flotilla had been established by countries opposing Iran's threats to close the Hormuz Strait. These ships operated in the Persian Gulf and Arabian Sea off the coast of Iran. The flotilla included three American aircraft carriers (the USS "Carl Vinson", the USS "Enterprise" and USS "Abraham Lincoln") and three destroyers (USS "Momsen", USS "Sterett", USS "Halsey"), seven British warships, including the destroyer HMS "Daring" and a number of Type 23 frigates (HMS "Westminster", HMS "Argyll", HMS "Somerset" and HMS "St Albans"), and a French warship, the frigate "La Motte-Picquet" .
On 24 January, tensions rose further after the European Union imposed sanctions on Iranian oil. A senior member of Iran's parliament said that the Islamic Republic would close the entry point to the Gulf if new sanctions block its oil exports. "If any disruption happens regarding the sale of Iranian oil, the Strait of Hormuz will definitely be closed," Mohammad Kossari, deputy head of parliament's foreign affairs and national security committee, told the semi-official Fars News Agency.
2015 seizure of MV "Maersk Tigris".
On April 28, 2015, IRGCN patrol boats contacted the Marshall Islands-flagged container ship "Maersk Tigris", which was westbound through the strait, and directed the ship to proceed further into Iranian territorial waters, according to a spokesman for the U.S. Defense Department. When the ship's master declined, one of the Iranian craft fired shots across the bridge of the "Maersk Tigris". The master complied and proceeded into Iranian waters near Larak Island. The U.S. Navy sent aircraft and a destroyer, USS "Farragut", to monitor the situation. 
Maersk says they have agreed to pay an Iranian company $163,000 over a dispute about 10 container boxes transported to Dubai in 2005. The court ruling allegedly ordered a fine of $3.6 million.
Ability of Iran to hinder shipping.
Millennium Challenge 2002 was a major war game exercise conducted by the United States armed forces in 2002. According to a 2012 article in The Christian Science Monitor, it simulated an attempt by Iran to close the strait. The assumptions and results were controversial.
A 2008 article in "International Security" contended that Iran could seal off or impede traffic in the Strait for a month, and an attempt by the U.S. to reopen it would be likely to escalate the conflict. In a later issue, however, the journal published a response which questioned some key assumptions and suggested a much shorter timeline for re-opening.
In December 2011, Iran's navy began a ten-day exercise in international waters along the strait. The Iranian Navy Commander, Rear Admiral Habibollah Sayyari, stated that the strait would not be closed during the exercise; Iranian forces could easily accomplish that but such a decision must be made at a political level.
Captain John Kirby, a Pentagon spokesman, was quoted in a December 2011 Reuters article: "Efforts to increase tension in that part of the world are unhelpful and counter-productive. For our part, we are comfortable that we have in the region sufficient capabilities to honor our commitments to our friends and partners, as well as the international community." In the same article, Suzanne Maloney, an Iran expert at the Brookings Institution, said, "The expectation is that the U.S. military could address any Iranian threat relatively quickly."
General Martin Dempsey, Chairman of the Joint Chiefs of Staff, said in January 2012 that Iran "has invested in capabilities that could, in fact, for a period of time block the Strait of Hormuz." He also stated, "We've invested in capabilities to ensure that if that happens, we can defeat that."
Alternative shipping routes.
In June 2012, Saudi Arabia reopened the Iraq Pipeline through Saudi Arabia (IPSA), which was confiscated from Iraq in 2001 and travels from Iraq across Saudi Arabia to a Red Sea port. It will have a capacity of 1.65 million barrels per day.
In July 2012, the UAE began using a new pipeline from the Habshan fields in Abu Dhabi to the Fujairah oil terminal on the Gulf of Oman, effectively bypassing the Strait of Hormuz. It was constructed by China and will have a maximum capacity of around 2 million barrels per day, over three-fourths of the UAE's 2012 production rate. The UAE is also increasing Fujairah's storage and off-loading capacities.
In a July 2012 Foreign Policy article, Gal Luft compared Iran and the Strait of Hormuz to the Ottoman Empire and the Dardanelles, a choke point for shipments of Russian grain a century ago. He indicated that tensions involving the Strait of Hormuz are leading those currently dependent on shipments from the Gulf to find alternative shipping capabilities. He stated that Saudi Arabia was considering building new pipelines to Oman and Yemen, and that Iraq might revive the disused Iraq-Syria pipeline to ship crude to the Mediterranean. Luft stated that reducing Hormuz traffic "presents the West with a new opportunity to augment its current Iran containment strategy."

</doc>
<doc id="29006" url="http://en.wikipedia.org/wiki?curid=29006" title="Space observatory">
Space observatory

A space observatory is any instrument (such as a telescope) in outer space that is used for observation of distant planets, galaxies and other outer space objects. This category is distinct from other observatories located in space that are pointed toward Earth for the purpose of reconnaissance and other types of information gathering.
Introduction.
Performing astronomy from Earth's surface is limited by the filtering and distortion of electromagnetic radiation (scintillation or twinkling) due to the atmosphere. Some terrestrial telescopes (such as the Very Large Telescope) can reduce atmospheric effects with adaptive optics. A telescope orbiting Earth outside the atmosphere is subject neither to twinkling nor to light pollution from artificial light sources on Earth.
Space-based astronomy is even more important for frequency ranges which are outside the optical window and the radio window, the only two wavelength ranges of the electromagnetic spectrum that are not severely attenuated by the atmosphere. For example, X-ray astronomy is nearly impossible when done from Earth, and has reached its current importance in astronomy only due to orbiting X-ray telescopes such as the Chandra observatory and the XMM-Newton observatory. Infrared and ultraviolet are also greatly blocked.
Space observatories can generally be divided into two classes: missions which map the entire sky (surveys), and observatories which make observations of chosen parts of the sky.
Many space observatories have already completed their missions, while others continue operating, and still others are planned for the future. Satellites have been launched and operated by NASA, ISRO of India, ESA, Japanese Space Agency and the Soviet space program later succeeded by Roskosmos of Russia.
History.
In 1946, American theoretical astrophysicist Lyman Spitzer was the first to conceive the idea of a telescope in outer space, a decade before the Soviet Union launched the first satellite, "Sputnik 1".
Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 70s for such a system to be built, Spitzer's vision ultimately materialized into the Hubble Space Telescope, which was launched on April 20, 1990 by the Space Shuttle "Discovery" (STS-31).

</doc>
<doc id="29007" url="http://en.wikipedia.org/wiki?curid=29007" title="Saint David">
Saint David

Saint David (Welsh: Dewi Sant; c. 500 – c. 589) was a Welsh bishop of Menevia during the 6th century; he was later regarded as a saint. He is the patron saint of Wales. David was a native of Wales, and a relatively large amount of information is known about his life. However, his birth date is uncertain: suggestions range from 462 to 512. The Welsh annals placed his death 569 years after the birth of Christ, but Phillimore's dating revised this to 601.
Monasticism.
Many of the traditional tales about David are found in the "Buchedd Dewi", a hagiography written by Rhygyfarch in the late 11th century. Rhygyfarch claimed it was based on documents found in the cathedral archives. Modern historians are sceptical of some of its claims: one of Rhygyfarch's aims was to establish some independence for the Welsh church, which had refused the Roman rite until the 8th century and now sought a metropolitan status equal to that of Canterbury. (This may apply to the supposed pilgrimage to Jerusalem where he was anointed as an archbishop by the patriarch).
The tradition that he was born at Henvynyw (Vetus-Menevia) in Cardiganshire is not improbable. He became renowned as a teacher and preacher, founding monastic settlements and churches in Wales, Dumnonia, and Brittany. St David's Cathedral stands on the site of the monastery he founded in the Glyn Rhosyn valley of Pembrokeshire. Around 550, he attended the Synod of Brefi, where his eloquence in opposing Pelagianism caused his fellow monks to elect him primate of the region. As such he presided over the synod of Caerleon (the "Synod of Victory") around 569.
Legend.
His best-known miracle is said to have taken place when he was preaching in the middle of a large crowd at the Synod of Brefi: the village of Llanddewi Brefi stands on the spot where the ground on which he stood is reputed to have risen up to form a small hill. A white dove, which became his emblem, was seen settling on his shoulder. John Davies notes that one can scarcely "conceive of any miracle more superfluous" in that part of Wales than the creation of a new hill. David is said to have denounced Pelagianism during this incident and he was declared archbishop by popular acclaim according to Rhygyfarch, bringing about the retirement of Dubricius. St David's metropolitan status as an archbishopric was later supported by Bernard, Bishop of St David's, Geoffrey of Monmouth and Gerald of Wales.
The Monastic Rule of David prescribed that monks had to pull the plough themselves without draught animals, must drink only water and eat only bread with salt and herbs, and spend the evenings in prayer, reading and writing. No personal possessions were allowed: even to say "my book" was considered an offence. He lived a simple life and practised asceticism, teaching his followers to refrain from eating meat and drinking beer. His symbol, also the symbol of Wales, is the leek (this largely comes from a reference in Shakespeare's Henry V, Act V scene 1) :
"Fluellen: "If your Majesty is remembered of it, the Welshmen did good service in a garden where leeks did grow, wearing leeks in their Monmouth caps, which your Majesty knows, to this hour is an honourable badge of the service, and I do believe, your Majesty takes no scorn to wear the leek upon Saint Tavy's day". King Henry: "I wear it for a memorable honour; for I am Welsh, you know, good countryman"."
Connections to Glastonbury.
Rhygyfarch counted Glastonbury Abbey among the churches David founded. Around forty years later William of Malmesbury, believing the Abbey older, said that David visited Glastonbury only to rededicate the Abbey and to donate a travelling altar including a great sapphire. He had had a vision of Jesus who said that "the church had been dedicated long ago by Himself in honour of His Mother, and it was not seemly that it should be re-dedicated by human hands". So David instead commissioned an extension to be built to the abbey, east of the Old Church. (The dimensions of this extension given by William were verified archaeologically in 1921). One manuscript indicates that a sapphire altar was among the items King Henry VIII confiscated from the abbey at its dissolution a thousand years later.
Death.
It is claimed that David lived for over 100 years, and that he died on a Tuesday 1 March (now Saint David's Day). It is generally accepted that this was around 589, and 1 March fell on a Tuesday in 589. The monastery is said to have been "filled with angels as Christ received his soul." His last words to his followers were in a sermon on the previous Sunday. The Welsh Life of St David gives these as: "Bydwch lawen a chedwch ych ffyd a'ch cret, a gwnewch y petheu bychein a glywyssawch ac a welsawch gennyf i. A mynheu a gerdaf y fford yd aeth an tadeu idi", which translates as, "Be joyful, and keep your faith and your creed, and do the little things that you have seen me do and heard about. I will walk the path that our fathers have trod before us." "Do ye the little things in life" ("Gwnewch y pethau bychain mewn bywyd") is today a very well known phrase in Welsh.
David was buried at St David's Cathedral at St David's, Pembrokeshire, where his shrine was a popular place of pilgrimage throughout the Middle Ages. During the 10th and 11th centuries the Cathedral was regularly raided by Vikings, who removed the shrine from the church and stripped off the precious metal adornments. In 1275 a new shrine was constructed, the ruined base of which remains to this day (see photo), which was originally surmounted by an ornamental wooden canopy with murals of St David, St Patrick and St Denis of France. The relics of St David and St Justinian were kept in a portable casket on the stone base of the shrine. It was at this shrine that Edward I came to pray in 1284. During the reformation Bishop Barlow (1536–48), a staunch Protestant, stripped the shrine of its jewels and confiscated the relics of David and Justinian.
Veneration.
Unlike many contemporary "saints" of Wales, David was officially recognized at the Vatican by Pope Callixtus II in 1120, thanks to the work of Bernard, Bishop of St David's. Music for his office has been edited by O.T. Edwards in "Matins, Lauds and Vespers for St David's Day: the Medieval Office of the Welsh Patron Saint in National Library of Wales MS 20541 E" (Cambridge, 1990).
Over 50 churches in South Wales were dedicated to him in pre-Reformation days.
In the 2004 edition of the Roman Martyrology, David is listed under 1 March with the Latin name "Dávus". He is recognised as bishop of Menevia in Wales who governed his monastery following the example of the Eastern Fathers. Through his leadership, many monks went forth to evangelise Wales, Ireland, Cornwall and Armorica (Brittany and surrounding provinces).
The restored Shrine of St David was unveiled and re-dedicated by the Right Reverend Wyn Evans, Bishop of St Davids, at a Choral Eucharist on St David's Day 2012.
Iconography.
He is usually represented standing on a little hill, with a dove on his shoulder.
Reputation.
David's popularity in Wales is shown by the Armes Prydein Fawr, of around 930, a popular poem which prophesied that in the future, when all might seem lost, the Cymry (the Welsh people) would unite behind the standard of David to defeat the English; "A lluman glân Dewi a ddyrchafant" ("And they will raise the pure banner of Dewi").
Although David's role in spreading of Christianity on the continent belongs to the realm of legend, he inspired numerous place names in Brittany including Saint-Divy, Saint-Yvy and Landivy.
David's life and teachings have inspired a choral work by Welsh composer Karl Jenkins, "Dewi Sant". It is a seven-movement work best known for the classical crossover series Adiemus, which intersperses movements reflecting the themes of David's last sermon with those drawing from three Psalms. An oratorio by another Welsh composer Arwel Hughes, also entitled "Dewi Sant", was composed in 1950.
Saint David is also thought to be associated with corpse candles, lights that would warn of the imminent death of a member of the community. The story goes that David prayed for his people to have some warning of their death, so that they could prepare themselves. In a vision, David's wish was granted and told that from then on, people who lived in the land of Dewi Sant (Saint David) "would be forewarned by the dim light of mysterious tapers when and where the death might be expected." The color and/or size of the tapers indicated whether the person to die would be a woman, man, or child.

</doc>
