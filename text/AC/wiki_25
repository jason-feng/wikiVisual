<doc id="29238" url="http://en.wikipedia.org/wiki?curid=29238" title="Systems theory">
Systems theory

Systems theory is the interdisciplinary study of systems in general, with the goal of elucidating principles that can be applied to all types of systems at all nesting levels in all fields of research. The term does not yet have a well-established, precise meaning, but systems theory can reasonably be considered a specialization of systems thinking; alternatively as a goal output of systems science and systems engineering, with an emphasis on generality useful across a broad range of systems (versus the particular models of individual fields).
The term originates from Bertalanffy's general system theory (GST) and is used in later efforts in other fields, such as the action theory of Talcott Parsons and the social systems theory of Niklas Luhmann.
A central topic of systems theory is self-regulating systems, i.e. systems self-correcting through feedback. Self-regulating systems are found in nature, including the physiological systems of our body, in local and global ecosystems, and in climate—and in human learning processes (from the individual on up through international organizations like the UN).
Overview.
Contemporary ideas from systems theory have grown with diverse areas, exemplified by the work of biologist Ludwig von Bertalanffy, linguist Béla H. Bánáthy, sociologist Talcott Parsons, ecological systems with Howard T. Odum, Eugene Odum and Fritjof Capra, organizational theory and management with individuals such as Peter Senge, interdisciplinary study with areas like Human Resource Development from the work of Richard A. Swanson, and insights from educators such as Debora Hammond and Alfonso Montuori. As a transdisciplinary, interdisciplinary and multiperspectival domain, the area brings together principles and concepts from ontology, philosophy of science, physics, computer science, biology, and engineering as well as geography, sociology, political science, psychotherapy (within family systems therapy) and economics among others. Systems theory thus serves as a bridge for interdisciplinary dialogue between autonomous areas of study as well as within the area of systems science itself.
In this respect, with the possibility of misinterpretations, von Bertalanffy believed a general theory of systems "should be an important regulative device in science," to guard against superficial analogies that "are useless in science and harmful in their practical consequences." Others remain closer to the direct systems concepts developed by the original theorists. For example, Ilya Prigogine, of the Center for Complex Quantum Systems at the University of Texas, Austin, has studied emergent properties, suggesting that they offer analogues for living systems. The theories of autopoiesis of Francisco Varela and Humberto Maturana represent further developments in this field. Important names in contemporary systems science include Russell Ackoff, Béla H. Bánáthy, Anthony Stafford Beer, Peter Checkland, Brian Wilson, Robert L. Flood, Fritjof Capra, Michael C. Jackson, and Edgar Morin among others.
With the modern foundations for a general theory of systems following the World War, Ervin Laszlo, in the preface for Bertalanffy's book "Perspectives on General System Theory", maintains that the translation of "general system theory" from German into English has "wrought a certain amount of havoc". The preface explains that the original concept of a general system theory, ""Allgemeine Systemtheorie" (or "Lehre")", pointed out that "Theorie" (or "Lehre"), just as "Wissenschaft" (translated Scholarship), "has a much broader meaning in German than the closest English words ‘theory’ and ‘science'". These ideas refer to an organized body of knowledge and "any systematically presented set of concepts, whether empirically, axiomatically, or philosophically" represented, while many associate "Lehre" with theory and science in the etymology of general systems, though it also does not translate from the German very well; its "closest equivalent" translates as "teaching", but "sounds dogmatic and off the mark". The idea of a "general systems theory" might have lost many of its root meanings in the translation and many people started to believe that the systems theorists had articulated nothing but a pseudoscience, systems theory transfer into the name used by early investigators for the interdependence of relationships created in organizations by defining a new way of thinking about science and scientific paradigms.
A system in this frame of reference can contain regularly interacting or interrelating groups of activities. For example, in noting the influence in organizational psychology as the field evolved from "an individually oriented industrial psychology to a systems and developmentally oriented organizational psychology", some theorists recognize that organizations have complex social systems; separating the parts from the whole reduces the overall effectiveness of organizations. This difference, from conventional models that center on individuals, structures, departments and units, separates in part from the whole, instead of recognizing the interdependence between groups of individuals, structures and processes that enable an organization to function. Laszlo explains that the new systems view of organized complexity went "one step beyond the Newtonian view of organized simplicity" which reduced the parts from the whole, or understood the whole without relation to the parts. The relationship between organisations and their environments can be seen as the foremost source of complexity and interdependence. In most cases, the whole has properties that cannot be known from analysis of the constituent elements in isolation. Béla H. Bánáthy, who argued — along with the founders of the systems society — that "the benefit of humankind" is the purpose of science, has made significant and far-reaching contributions to the area of systems theory. For the Primer Group at ISSS, Bánáthy defines a perspective that iterates this view:
The systems view is a world-view that is based on the discipline of SYSTEM INQUIRY. Central to systems inquiry is the concept of SYSTEM. In the most general sense, system means a configuration of parts connected and joined together by a web of relationships. The Primer group defines system as a family of relationships among the members acting as a whole. Von Bertalanffy defined system as "elements in standing relationship"—
Similar ideas are found in learning theories that developed from the same fundamental concepts, emphasising how understanding results from knowing concepts both in part and as a whole. In fact, Bertalanffy’s organismic psychology paralleled the learning theory of Jean Piaget. Some consider interdisciplinary perspectives critical in breaking away from industrial age models and thinking, wherein history represents history and math represents math, while the arts and sciences specialization remain separate and many treat teaching as behaviorist conditioning. The contemporary work of Peter Senge provides detailed discussion of the commonplace critique of educational systems grounded in conventional assumptions about learning, including the problems with fragmented knowledge and lack of holistic learning from the "machine-age thinking" that became a "model of school separated from daily life." In this way some systems theorists attempt to provide alternatives to, and evolved ideation from orthodox theories which have grounds in classical assumptions, including individuals such as Max Weber and Émile Durkheim in sociology and Frederick Winslow Taylor in scientific management. The theorists sought holistic methods by developing systems concepts that could integrate with different areas.
Some may view the contradiction of reductionism in conventional theory (which has as its subject a single part) as simply an example of changing assumptions. The emphasis with systems theory shifts from parts to the organization of parts, recognizing interactions of the parts as not static and constant but dynamic processes. Some questioned the conventional closed systems with the development of open systems perspectives. The shift originated from absolute and universal authoritative principles and knowledge to relative and general conceptual and perceptual knowledge and still remains in the tradition of theorists that sought to provide means to organize human life. In other words, theorists rethought the preceding history of ideas; they did not lose them. Mechanistic thinking was particularly critiqued, especially the industrial-age mechanistic metaphor for the mind from interpretations of Newtonian mechanics by Enlightenment philosophers and later psychologists that laid the foundations of modern organizational theory and management by the late 19th century.
Examples of applications.
Systems biology.
Systems biology is a movement that draws on several trends in bioscience research. Proponents describe systems biology as a biology-based inter-disciplinary study field that focuses on complex interactions in biological systems, claiming that it uses a new perspective (holism instead of reduction). Particularly from year 2000 onwards, the biosciences use the term widely and in a variety of contexts. An often stated ambition of systems biology is the modelling and discovery of emergent properties which represents properties of a system whose theoretical description requires the only possible useful techniques to fall under the remit of systems biology. It is thought that Ludwig von Bertalanffy may have created the term systems biology in 1928.
Systems engineering.
Systems engineering is an interdisciplinary approach and means for enabling the realisation and deployment of successful systems. It can be viewed as the application of engineering techniques to the engineering of systems, as well as the application of a systems approach to engineering efforts. Systems engineering integrates other disciplines and specialty groups into a team effort, forming a structured development process that proceeds from concept to production to operation and disposal. Systems engineering considers both the business and the technical needs of all customers, with the goal of providing a quality product that meets the user needs.
Systems psychology.
Systems psychology is a branch of psychology that studies human behaviour and experience in complex systems. It received inspiration from systems theory and systems thinking, as well as the basics of theoretical work from Roger Barker, Gregory Bateson, Humberto Maturana and others. It makes an approach in psychology in which groups and individuals receive consideration as systems in homeostasis. Systems psychology "includes the domain of engineering psychology, but in addition seems more concerned with societal systems and with the study of motivational, affective, cognitive and group behavior that holds the name engineering psychology." In systems psychology, "characteristics of organizational behaviour, for example individual needs, rewards, expectations, and attributes of the people interacting with the systems, considers this process in order to create an effective system".
History.
Whether considering the first systems of written communication with Sumerian cuneiform to Mayan numerals, or the feats of engineering with the Egyptian pyramids, systems thinking can date back to antiquity. Differentiated from Western rationalist traditions of philosophy, C. West Churchman often identified with the I Ching as a systems approach sharing a frame of reference similar to pre-Socratic philosophy and Heraclitus. Von Bertalanffy traced systems concepts to the philosophy of G.W. Leibniz and Nicholas of Cusa's "coincidentia oppositorum". While modern systems can seem considerably more complicated, today's systems may embed themselves in history.
Figures like James Joule and Sadi Carnot represent an important step to introduce the "systems approach" into the (rationalist) hard sciences of the 19th century, also known as the energy transformation. Then, the thermodynamics of this century, by Rudolf Clausius, Josiah Gibbs and others, established the "system" reference model as a formal scientific object.
The Society for General Systems Research specifically catalyzed systems theory as an area of study, which developed following the World Wars from the work of Ludwig von Bertalanffy, Anatol Rapoport, Kenneth E. Boulding, William Ross Ashby, Margaret Mead, Gregory Bateson, C. West Churchman and others in the 1950s, had specifically catalyzed by collaboration in. Cognizant of advances in science that questioned classical assumptions in the organizational sciences, Bertalanffy's idea to develop a theory of systems began as early as the interwar period, publishing "An Outline for General Systems Theory" in the "British Journal for the Philosophy of Science", Vol 1, No. 2, by 1950. Where assumptions in Western science from Greek thought with Plato and Aristotle to Newton's "Principia" have historically influenced all areas from the hard to social sciences (see David Easton's seminal development of the "political system" as an analytical construct), the original theorists explored the implications of twentieth century advances in terms of systems.
People have studied subjects like complexity, self-organization, connectionism and adaptive systems in the 1940s and 1950s. In fields like cybernetics, researchers such as Norbert Wiener, William Ross Ashby, John von Neumann and Heinz von Foerster, examined complex systems mathematically. John von Neumann discovered cellular automata and self-reproducing systems, again with only pencil and paper. Aleksandr Lyapunov and Jules Henri Poincaré worked on the foundations of chaos theory without any computer at all. At the same time Howard T. Odum, known as a radiation ecologist, recognized that the study of general systems required a language that could depict energetics, thermodynamics and kinetics at any system scale. Odum developed a general system, or universal language, based on the circuit language of electronics, to fulfill this role, known as the Energy Systems Language. Between 1929-1951, Robert Maynard Hutchins at the University of Chicago had undertaken efforts to encourage innovation and interdisciplinary research in the social sciences, aided by the Ford Foundation with the interdisciplinary Division of the Social Sciences established in 1931. Numerous scholars had actively engaged in these ideas before (Tectology by Alexander Bogdanov, published in 1912-1917, is a remarkable example), but in 1937, von Bertalanffy presented the general theory of systems at a conference at the University of Chicago.
The systems view was based on several fundamental ideas. First, all phenomena can be viewed as a web of relationships among elements, or a system. Second, all systems, whether electrical, biological, or social, have common patterns, behaviors, and properties that the observer can analyze and use to develop greater insight into the behavior of complex phenomena and to move closer toward a unity of the sciences. System philosophy, methodology and application are complementary to this science. By 1956, theorists established the Society for General Systems Research, which they renamed the International Society for Systems Science in 1988. The Cold War affected the research project for systems theory in ways that sorely disappointed many of the seminal theorists. Some began to recognize that theories defined in association with systems theory had deviated from the initial General Systems Theory (GST) view. The economist Kenneth Boulding, an early researcher in systems theory, had concerns over the manipulation of systems concepts. Boulding concluded from the effects of the Cold War that abuses of power always prove consequential and that systems theory might address such issues. Since the end of the Cold War, a renewed interest in systems theory emerged, combined with efforts to strengthen an ethical view on the subject.
Developments.
General systems research and systems inquiry.
Many early systems theorists aimed at finding a general systems theory that could explain all systems in all fields of science. The term goes back to Bertalanffy's book titled "General System theory: Foundations, Development, Applications" from 1968. According to Von Bertalanffy, he developed the "allgemeine Systemlehre" (general systems teachings) first via lectures beginning in 1937 and then via publications beginning in 1946.
Von Bertalanffy's objective was to bring together under one heading the organismic science that he had observed in his work as a biologist. His desire was to use the word "system" for those principles that are common to systems in general. In GST, he writes:
...there exist models, principles, and laws that apply to generalized systems or their subclasses, irrespective of their particular kind, the nature of their component elements, and the relationships or "forces" between them. It seems legitimate to ask for a theory, not of systems of a more or less special kind, but of universal principles applying to systems in general.—
Ervin Laszlo in the preface of von Bertalanffy's book "Perspectives on General System Theory":
Thus when von Bertalanffy spoke of Allgemeine Systemtheorie it was consistent with his view that he was proposing a new perspective, a new way of doing science. It was not directly consistent with an interpretation often put on "general system theory", to wit, that it is a (scientific) "theory of general systems." To criticize it as such is to shoot at straw men. Von Bertalanffy opened up something much broader and of much greater significance than a single theory (which, as we now know, can always be falsified and has usually an ephemeral existence): he created a new paradigm for the development of theories.
Ludwig von Bertalanffy outlines systems inquiry into three major domains: Philosophy, Science, and Technology. In his work with the Primer Group, Béla H. Bánáthy generalized the domains into four integratable domains of systemic inquiry:
These operate in a recursive relationship, he explained. Integrating Philosophy and Theory as Knowledge, and Method and Application as action, Systems Inquiry then is knowledgeable action.
Cybernetics.
Cybernetics is the study of the communication and control of regulatory feedback both in living and lifeless systems (organisms, organizations, machines), and in combinations of those. Its focus is how anything (digital, mechanical or biological) controls its behavior, processes information, reacts to information, and changes or can be changed to better accomplish those three primary tasks.
The terms "systems theory" and "cybernetics" have been widely used as synonyms. Some authors use the term "cybernetic" systems to denote a proper subset of the class of general systems, namely those systems that include feedback loops. However Gordon Pask's differences of eternal interacting actor loops (that produce finite products) makes general systems a proper subset of cybernetics. According to Jackson (2000), von Bertalanffy promoted an embryonic form of general system theory (GST) as early as the 1920s and 1930s but it was not until the early 1950s it became more widely known in scientific circles.
Threads of cybernetics began in the late 1800s that led toward the publishing of seminal works (e.g., Wiener's "Cybernetics" in 1948 and von Bertalanffy's "General Systems Theory" in 1968). Cybernetics arose more from engineering fields and GST from biology. If anything it appears that although the two probably mutually influenced each other, cybernetics had the greater influence. Von Bertalanffy (1969) specifically makes the point of distinguishing between the areas in noting the influence of cybernetics: "Systems theory is frequently identified with cybernetics and control theory. This again is incorrect. Cybernetics as the theory of control mechanisms in technology and nature is founded on the concepts of information and feedback, but as part of a general theory of systems;" then reiterates: "the model is of wide application but should not be identified with 'systems theory' in general", and that "warning is necessary against its incautious expansion to fields for which its concepts are not made." (17-23). Jackson (2000) also claims von Bertalanffy was informed by Alexander Bogdanov's three volume "Tectology" that was published in Russia between 1912 and 1917, and was translated into German in 1928. He also states it is clear to Gorelik (1975) that the "conceptual part" of general system theory (GST) had first been put in place by Bogdanov. The similar position is held by Mattessich (1978) and Capra (1996). Ludwig von Bertalanffy never even mentioned Bogdanov in his works, which Capra (1996) finds "surprising".
Cybernetics, catastrophe theory, chaos theory and complexity theory have the common goal to explain complex systems that consist of a large number of mutually interacting and interrelated parts in terms of those interactions. Cellular automata (CA), neural networks (NN), artificial intelligence (AI), and artificial life (ALife) are related fields, but they do not try to describe general (universal) complex (singular) systems. The best context to compare the different "C"-Theories about complex systems is historical, which emphasizes different tools and methodologies, from pure mathematics in the beginning to pure computer science now. Since the beginning of chaos theory when Edward Lorenz accidentally discovered a strange attractor with his computer, computers have become an indispensable source of information. One could not imagine the study of complex systems without the use of computers today.
Complex adaptive systems.
Complex adaptive systems are special cases of complex systems. They are "complex" in that they are diverse and composed of multiple, interconnected elements; they are "adaptive" in that they have the capacity to change and learn from experience. The term "complex adaptive system" was coined at the interdisciplinary Santa Fe Institute (SFI), by John H. Holland, Murray Gell-Mann and others. An alternative conception of complex adaptive (and learning) systems, methodologically at the interface between natural and social science, has been presented by Kristo Ivanov in terms of hypersystems. This concept intends to offer a theoretical basis for understanding and implementing participation of "users", decisions makers, designers and affected actors, in the development or maintenance of self-learning systems.

</doc>
<doc id="29240" url="http://en.wikipedia.org/wiki?curid=29240" title="Lists of stars">
Lists of stars

The following are lists of stars. These are astronomical objects that spend some portion of their existence generating energy through thermonuclear fusion.
Other star listings.
Other stars.
The following is a list of particularly notable actual or hypothetical stars that have their own articles in Wikipedia, but are not included in the lists above.

</doc>
<doc id="29247" url="http://en.wikipedia.org/wiki?curid=29247" title="Sulfuric acid">
Sulfuric acid

Sulfuric acid (alternative spelling sulphuric acid) is a highly corrosive strong mineral acid with the molecular formula H2SO4. It is a pungent-ethereal, colorless to slightly yellow viscous liquid which is soluble in water at all concentrations. Sometimes, it is dyed dark brown during production to alert people to its hazards. The historical name of this acid is oil of vitriol.
Sulfuric acid is a diprotic acid and shows different properties depending upon its concentration. Its corrosiveness on other materials, like metals, living tissues or even stones, can be mainly ascribed to its strong acidic nature and, if concentrated, strong dehydrating and oxidizing properties. Sulfuric acid at a high concentration can cause very serious damage upon contact, since not only does it cause chemical burns via hydrolysis, but also secondary thermal burns through dehydration. It can lead to permanent blindness if splashed onto eyes and irreversible damage if swallowed. Accordingly, safety precautions should be strictly observed when handling it. Moreover, it is hygroscopic, readily absorbing water vapour from the air.
Sulfuric acid has a wide range of applications including domestic acidic drain cleaner, electrolyte in lead-acid batteries and various cleaning agents. It is also a central substance in the chemical industry. Principal uses include mineral processing, fertilizer manufacturing, oil refining, wastewater processing, and chemical synthesis. It is widely produced with different methods, such as contact process, wet sulfuric acid process and some other methods.
History.
The study of vitriol, a category of glassy minerals from which the acid can be derived, began in ancient times. Sumerians had a list of types of vitriol that they classified according to the substances' color. Some of the earliest discussions on the origin and properties of vitriol is in the works of the Greek physician Dioscorides (first century AD) and the Roman naturalist Pliny the Elder (23–79 AD). Galen also discussed its medical use. Metallurgical uses for vitriolic substances were recorded in the Hellenistic alchemical works of Zosimos of Panopolis, in the treatise "Phisica et Mystica", and the Leyden papyrus X.
Persian alchemists Jābir ibn Hayyān (c. 721 – c. 815 AD), Razi (865 – 925 AD), and Jamal Din al-Watwat (d. 1318, wrote the book "Mabāhij al-fikar wa-manāhij al-'ibar"), included vitriol in their mineral classification lists. Ibn Sina focused on its medical uses and different varieties of vitriol.
Sulfuric acid was called "oil of vitriol" by medieval European alchemists because it was prepared by roasting "green vitriol" (iron (II) sulfate) in an iron retort. There are references to it in the works of Vincent of Beauvais and in the "Compositum de Compositis" ascribed to Saint Albertus Magnus. A passage from Pseudo-Geber´s "Summa Perfectionis" was long considered to be the first recipe for sulfuric acid, but this was a misinterpretation.
In the seventeenth century, the German-Dutch chemist Johann Glauber prepared sulfuric acid by burning sulfur together with saltpeter (potassium nitrate, KNO3), in the presence of steam. As saltpeter decomposes, it oxidizes the sulfur to SO3, which combines with water to produce sulfuric acid. In 1736, Joshua Ward, a London pharmacist, used this method to begin the first large-scale production of sulfuric acid.
In 1746 in Birmingham, John Roebuck adapted this method to produce sulfuric acid in lead-lined chambers, which were stronger, less expensive, and could be made larger than the previously used glass containers. This process allowed the effective industrialization of sulfuric acid production. After several refinements, this method, called the lead chamber process or "chamber process", remained the standard for sulfuric acid production for almost two centuries.
Sulfuric acid created by John Roebuck's process approached a 65% concentration. Later refinements to the lead chamber process by French chemist Joseph Louis Gay-Lussac and British chemist John Glover improved concentration to 78%. However, the manufacture of some dyes and other chemical processes require a more concentrated product. Throughout the 18th century, this could only be made by dry distilling minerals in a technique similar to the original alchemical processes. Pyrite (iron disulfide, FeS2) was heated in air to yield iron(II) sulfate, FeSO4, which was oxidized by further heating in air to form iron(III) sulfate, Fe2(SO4)3, which, when heated to 480 °C, decomposed to iron(III) oxide and sulfur trioxide, which could be passed through water to yield sulfuric acid in any concentration. However, the expense of this process prevented the large-scale use of concentrated sulfuric acid.
In 1831, British vinegar merchant Peregrine Phillips patented the contact process, which was a far more economical process for producing sulfur trioxide and concentrated sulfuric acid. Today, nearly all of the world's sulfuric acid is produced using this method.
Physical properties.
Grades of sulfuric acid.
Although nearly 99% sulfuric acid can be made, the subsequent loss of SO3 at the boiling point brings the concentration to 98.3% acid. The 98% grade is more stable in storage, and is the usual form of what is described as "concentrated sulfuric acid." Other concentrations are used for different purposes. Some common concentrations are:
"Chamber acid" and "tower acid" were the two concentrations of sulfuric acid produced by the lead chamber process, chamber acid being the acid produced in lead chamber itself (<70% to avoid contamination with nitrosylsulfuric acid) and tower acid being the acid recovered from the bottom of the Glover tower. They are now obsolete as commercial concentrations of sulfuric acid, although they may be prepared in the laboratory from concentrated sulfuric acid if needed. In particular, "10M" sulfuric acid (the modern equivalent of chamber acid, used in many titrations) is prepared by slowly adding 98% sulfuric acid to an equal volume of water, with good stirring: the temperature of the mixture can rise to 80 °C (176 °F) or higher.
Sulfuric acid reacts with its anhydride, SO3, to form H2S2O7, called "pyrosulfuric acid", "fuming sulfuric acid", "Disulfuric acid" or "oleum" or, less commonly, "Nordhausen acid". Concentrations of oleum are either expressed in terms of % SO3 (called % oleum) or as % H2SO4 (the amount made if H2O were added); common concentrations are 40% oleum (109% H2SO4) and 65% oleum (114.6% H2SO4). Pure H2S2O7 is a solid with melting point of 36 °C.
Pure sulfuric acid has a vapor pressure of <0.001 torr at 25 °C and 1 torr at 145.8 °C, and 98% sulfuric acid has a <1 mmHg vapor pressure at 40 °C.
Pure sulfuric acid is a viscous clear liquid, like oil, and this explains the old name of the acid ('oil of vitriol').
Commercial sulfuric acid is sold in several different purity grades. Technical grade H2SO4 is impure and often colored, but is suitable for making fertilizer. Pure grades, such as United States Pharmacopeia (USP) grade, are used for making pharmaceuticals and dyestuffs. Analytical grades are also available.
There are nine hydrates known, but three of them were confirmed to be tetrahydrate (H2SO4·4H2O), hemihexahydrate (H2SO4·6 1⁄2H2O) and octahydrate (H2SO4·8H2O).
Polarity and conductivity.
Anhydrous H2SO4 is a very polar liquid, having a dielectric constant of around 100. It has a high electrical conductivity, caused by dissociation through protonating itself, a process known as autoprotolysis.
The equilibrium constant for the autoprotolysis is
The comparable equilibrium constant for water, Kw is 10−14, a factor of 1010 (10 billion) smaller.
In spite of the viscosity of the acid, the effective conductivities of the H3SO4+ and HSO4- ions are high due to an intra-molecular proton-switch mechanism (analogous to the Grotthuss mechanism in water), making sulfuric acid a good conductor of electricity. It is also an excellent solvent for many reactions.
Chemical properties.
Reaction with water and dehydrating property.
Because the hydration reaction of sulfuric acid is highly exothermic, dilution should always be performed by adding the acid to the water rather than the water to the acid. Because the reaction is in an equilibrium that favors the rapid protonation of water, addition of acid to the water ensures that the "acid" is the limiting reagent. This reaction is best thought of as the formation of hydronium ions:
HSO4- is the "bisulfate" anion and SO42- is the "sulfate" anion. K1 and K2 are the acid dissociation constants.
Because the hydration of sulfuric acid is thermodynamically favorable and the affinity of it for water is sufficiently strong, sulfuric acid is an excellent dehydrating agent. Concentrated sulfuric acid has a very powerful dehydrating property, removing water (H2O) from other compounds including sugar and other carbohydrates and producing carbon, heat, steam, and a more dilute acid containing increased amounts of hydronium and bisulfate ions.
In laboratory, this is often demonstrated by mixing table sugar (sucrose) into sulfuric acid. The sugar changes from white to dark brown and then to black as carbon is formed. A rigid column of black, porous carbon will emerge as well. The carbon will smell strongly of caramel due to the heat generated.
Similarly, mixing starch into concentrated sulfuric acid will give elemental carbon and water as absorbed by the sulfuric acid (which becomes slightly diluted). The effect of this can be seen when concentrated sulfuric acid is spilled on paper which is composed of cellulose; the cellulose reacts to give a burnt appearance, the carbon appears much as soot would in a fire.
Although less dramatic, the action of the acid on cotton, even in diluted form, will destroy the fabric.
The reaction with copper(II) sulfate can also demonstrate the dehydration property of sulfuric acid. The blue crystal is changed into white powder as water is removed.
Acid-base properties.
As an acid, sulfuric acid reacts with most bases to give the corresponding sulfate. For example, the blue copper salt copper(II) sulfate, commonly used for electroplating and as a fungicide, is prepared by the reaction of copper(II) oxide with sulfuric acid:
Sulfuric acid can also be used to displace weaker acids from their salts. Reaction with sodium acetate, for example, displaces acetic acid, CH3COOH, and forms sodium bisulfate:
Similarly, reacting sulfuric acid with potassium nitrate can be used to produce nitric acid and a precipitate of potassium bisulfate. When combined with nitric acid, sulfuric acid acts both as an acid and a dehydrating agent, forming the nitronium ion NO2+, which is important in nitration reactions involving electrophilic aromatic substitution. This type of reaction, where protonation occurs on an oxygen atom, is important in many organic chemistry reactions, such as Fischer esterification and dehydration of alcohols.
When allowed to react with superacids, sulfuric acid can act as a base and be protonated, forming the [H3SO4]+ ion. Salt of [H3SO4]+ have been prepared using the following reaction in liquid HF:
The above reaction is thermodynamically favored due to the high bond enthalpy of the Si–F bond in the side product. Protonation using simply HF/SbF5, however, have met with failure, as pure sulfuric acid undergoes self-ionization to give [H3O]+ ions, which prevents the conversion of H2SO4 to [H3SO4]+ by the HF/SbF5 system:
Reactions with metals and strong oxidizing property.
Dilute sulfuric acid reacts with metals via a single displacement reaction as with other typical acids, producing hydrogen gas and salts (the metal sulfate). It attacks reactive metals (metals at positions above copper in the reactivity series) such as iron, aluminium, zinc, manganese, magnesium and nickel.
However, concentrated sulfuric acid is a strong oxidizing agent and does not react with metals in the same way as other typical acids. Sulfur dioxide, water and SO42− ions are evolved instead of the hydrogen and salts.
It can oxidize non-active metals such as tin and copper, depending upon the temperature.
Lead and tungsten, however, are resistant to sulfuric acid.
Reactions with non-metals.
Hot concentrated sulfuric acid oxidizes non-metals such as carbon (as bituminous coal) and sulfur.
Reaction with sodium chloride.
It reacts with sodium chloride, and gives hydrogen chloride gas and sodium bisulfate:
Electrophilic aromatic substitution.
Benzene undergoes electrophilic aromatic substitution with sulfuric acid to give the corresponding sulfonic acids:
Occurrence.
Pure sulfuric acid is not encountered naturally on Earth in anhydrous form, due to its great affinity for water. Dilute sulfuric acid is a constituent of acid rain, which is formed by atmospheric oxidation of sulfur dioxide in the presence of water – i.e., oxidation of sulfurous acid. Sulfur dioxide is the main byproduct produced when sulfur-containing fuels such as coal or oil are burned.
Sulfuric acid is formed naturally by the oxidation of sulfide minerals, such as iron sulfide. The resulting water can be highly acidic and is called acid mine drainage (AMD) or acid rock drainage (ARD). This acidic water is capable of dissolving metals present in sulfide ores, which results in brightly colored, toxic streams. The oxidation of pyrite (iron sulfide) by molecular oxygen produces iron(II), or Fe2+:
The Fe2+ can be further oxidized to Fe3+:
The Fe3+ produced can be precipitated as the hydroxide or hydrous oxide:
The iron(III) ion ("ferric iron") can also oxidize pyrite:
When iron(III) oxidation of pyrite occurs, the process can become rapid. pH values below zero have been measured in ARD produced by this process.
ARD can also produce sulfuric acid at a slower rate, so that the acid neutralizing capacity (ANC) of the aquifer can neutralize the produced acid. In such cases, the total dissolved solids (TDS) concentration of the water can be increased from the dissolution of minerals from the acid-neutralization reaction with the minerals.
Sulfuric acid is used as a defence by certain marine species, for example, the phaeophyte alga "Desmarestia munda" (order Desmarestiales) concentrates sulfuric acid in cell vacuoles.
Extraterrestrial sulfuric acid.
Venus.
Sulfuric acid is produced in the upper atmosphere of Venus by the Sun's photochemical action on carbon dioxide, sulfur dioxide, and water vapor. Ultraviolet photons of wavelengths less than 169 nm can photodissociate carbon dioxide into carbon monoxide and atomic oxygen. Atomic oxygen is highly reactive. When it reacts with sulfur dioxide, a trace component of the Venusian atmosphere, the result is sulfur trioxide, which can combine with water vapor, another trace component of Venus's atmosphere, to yield sulfuric acid. In the upper, cooler portions of Venus's atmosphere, sulfuric acid exists as a liquid, and thick sulfuric acid clouds completely obscure the planet's surface when viewed from above. The main cloud layer extends from 45–70 km above the planet's surface, with thinner hazes extending as low as 30 km and as high as 90 km above the surface. The permanent Venusian clouds produce a concentrated acid rain, as the clouds in the atmosphere of Earth produce water rain.
The atmosphere exhibits a sulfuric acid cycle. As sulfuric acid rain droplets fall down through the hotter layers of the atmosphere's temperature gradient, they are heated up and release water vapor, becoming more and more concentrated. When they reach temperatures above 300 °C, sulfuric acid begins to decompose into sulfur trioxide and water, both in the gas phase. Sulfur trioxide is highly reactive and dissociates into sulfur dioxide and atomic oxygen, which oxidizes traces of carbon monoxide to form carbon dioxide. Sulfur dioxide and water vapor rise on convection currents from the mid-level atmospheric layers to higher altitudes, where they will be transformed again into sulfuric acid, and the cycle repeats.
Europa.
Infrared spectra from NASA's Galileo mission show distinct absorptions on Jupiter's moon Europa that have been attributed to one or more sulfuric acid hydrates. Sulfuric acid in solution with water causes significant freezing-point depression of water's melting point, down to 210 K, and this would make more likely the existence of liquid solutions beneath Europa's icy crust.The interpretation of the spectra is somewhat controversial. Some planetary scientists prefer to assign the spectral features to the sulfate ion, perhaps as part of one or more minerals on Europa's surface.
Manufacture.
Sulfuric acid is produced from sulfur, oxygen and water via the conventional contact process (DCDA) or the wet sulfuric acid process (WSA).
Contact process.
In the first step, sulfur is burned to produce sulfur dioxide.
This is then oxidized to sulfur trioxide using oxygen in the presence of a vanadium(V) oxide catalyst. This reaction is reversible and the formation of the sulfur trioxide is exothermic.
The sulfur trioxide is absorbed into 97–98% H2SO4 to form oleum (H2S2O7), also known as fuming sulfuric acid. The oleum is then diluted with water to form concentrated sulfuric acid.
Note that directly dissolving SO3 in water is not practical due to the highly exothermic nature of the reaction between sulfur trioxide and water. The reaction forms a corrosive aerosol that is very difficult to separate, instead of a liquid.
Wet sulfuric acid process.
In the first step, sulfur is burned to produce sulfur dioxide:
or, alternatively, hydrogen sulfide (H2S) gas is incinerated to SO2 gas:
This is then oxidized to sulfur trioxide using oxygen with vanadium(V) oxide as catalyst.
The sulfur trioxide is hydrated into sulfuric acid H2SO4:
The last step is the condensation of the sulfuric acid to liquid 97–98% H2SO4:
Other methods.
Another method is the less well-known metabisulfite method, in which metabisulfite is placed at the bottom of a beaker, and 12.6 molar concentration hydrochloric acid is added. The resulting gas is bubbled through nitric acid, which will release brown/red vapors. The completion of the reaction is indicated by the ceasing of the fumes. This method does not produce an inseparable mist, which is quite convenient.
Sulfuric acid can be produced in the laboratory by burning sulfur in air and dissolving the gas produced in a hydrogen peroxide solution.
Prior to 1900, most sulfuric acid was manufactured by the lead chamber process. As late as 1940, up to 50% of sulfuric acid manufactured in the United States was produced by chamber process plants.
In early to mid nineteenth century "vitriol" plants existed, among other places, in Prestonpans in Scotland, Shropshire and the Lagan Valley in County Antrim Ireland where it was used as a bleach for linen. Early bleaching of linen was done using milk but this was a slow process and the use of vitriol sped up the bleaching process.
Uses.
Sulfuric acid is a very important commodity chemical, and indeed, a nation's sulfuric acid production is a good indicator of its industrial strength. World production in 2004 was about 180 million tonnes, with the following geographic distribution: Asia 35%, North America (including Mexico) 24%, Africa 11%, Western Europe 10%, Eastern Europe and Russia 10%, Australia and Oceania 7%, South America 7%. Most of this amount (~60%) is consumed for fertilizers, particularly superphosphates, ammonium phosphate and ammonium sulfates. About 20% is used in chemical industry for production of detergents, synthetic resins, dyestuffs, pharmaceuticals, petroleum catalysts, insecticides and antifreeze, as well as in various processes such as oil well acidicizing, aluminium reduction, paper sizing, water treatment. About 6% of uses are related to pigments and include paints, enamels, printing inks, coated fabrics and paper, and the rest is dispersed into a multitude of applications such as production of explosives, cellophane, acetate and viscose textiles, lubricants, non-ferrous metals and batteries.
Industrial production of chemicals.
The major use for sulfuric acid is in the "wet method" for the production of phosphoric acid, used for manufacture of phosphate fertilizers. In this method, phosphate rock is used, and more than 100 million tonnes are processed annually. This raw material is shown below as fluorapatite, though the exact composition may vary. This is treated with 93% sulfuric acid to produce calcium sulfate, hydrogen fluoride (HF) and phosphoric acid. The HF is removed as hydrofluoric acid. The overall process can be represented as:
Ammonium sulfate, an important nitrogen fertilizer, is most commonly produced as a byproduct from coking plants supplying the iron and steel making plants. Reacting the ammonia produced in the thermal decomposition of coal with waste sulfuric acid allows the ammonia to be crystallized out as a salt (often brown because of iron contamination) and sold into the agro-chemicals industry.
Another important use for sulfuric acid is for the manufacture of aluminium sulfate, also known as paper maker's alum. This can react with small amounts of soap on paper pulp fibers to give gelatinous aluminium carboxylates, which help to coagulate the pulp fibers into a hard paper surface. It is also used for making aluminium hydroxide, which is used at water treatment plants to filter out impurities, as well as to improve the taste of the water. Aluminium sulfate is made by reacting bauxite with sulfuric acid:
Sulfuric acid is also important in the manufacture of dyestuffs solutions.
Sulfur-iodine cycle.
The sulfur-iodine cycle is a series of thermo-chemical processes used to obtain hydrogen. It consists of three chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen.
The sulfur and iodine compounds are recovered and reused, hence the consideration of the process as a cycle. This process is endothermic and must occur at high temperatures, so energy in the form of heat has to be supplied.
The sulfur-iodine cycle has been proposed as a way to supply hydrogen for a hydrogen-based economy. It does not require hydrocarbons like current methods of steam reforming. But note that all of the available energy in the hydrogen so produced is supplied by the heat used to make it.
The sulfur-iodine cycle is currently being researched as a feasible method of obtaining hydrogen, but the concentrated, corrosive acid at high temperatures poses currently insurmountable safety hazards if the process were built on a large scale.
Industrial cleaning agent.
Sulfuric acid is used in large quantities by the iron and steelmaking industry to remove oxidation, rust and scaling from rolled sheet and billets prior to sale to the automobile and major appliances industry. Used acid is often recycled using a spent acid regeneration (SAR) plant. These plants combust spent acid with natural gas, refinery gas, fuel oil or other fuel sources. This combustion process produces gaseous sulfur dioxide (SO2) and sulfur trioxide (SO3) which are then used to manufacture "new" sulfuric acid. SAR plants are common additions to metal smelting plants, oil refineries, and other industries where sulfuric acid is consumed in bulk, as operating a SAR plant is much cheaper than the recurring costs of spent acid disposal and new acid purchases.
Catalyst.
Sulfuric acid is used for a variety of other purposes in the chemical industry. For example, it is the usual acid catalyst for the conversion of cyclohexanone oxime to caprolactam, used for making nylon. It is used for making hydrochloric acid from salt via the Mannheim process. Much H2SO4 is used in petroleum refining, for example as a catalyst for the reaction of isobutane with isobutylene to give isooctane, a compound that raises the octane rating of gasoline (petrol).
Electrolyte.
Sulfuric acid acts as the electrolyte in lead-acid (car) batteries (lead-acid accumulator):
At anode:
At cathode:
Overall:
Domestic uses.
Sulfuric acid at high concentrations is frequently the major ingredient in acidic drain cleaners which are used to remove grease, hair, tissue paper, etc. Similar to their alkaline versions, such drain openers can dissolve fats and proteins via hydrolysis. Moreover, as concentrated sulfuric acid has a strong dehydrating property, it can remove tissue paper via dehydrating process as well. Since the acid may react with water vigorously, such acidic drain openers should be added slowly into the pipe to be cleaned.
Health.
Sulfuric acid and sulfonated phenolics are the primary ingredients in Debacterol, a liquid topical agent that is used in the treatment of recurrent aphthous stomatitis (canker sores/mouth ulcers) or for any procedures in the oral cavity which require controlled, focal debridement of necrotic tissues.
Safety.
Laboratory hazards.
Sulfuric acid is capable of causing very severe burns, especially when it is at high concentrations. In common with other corrosive acids and alkali, it readily decomposes proteins and lipids through amide and ester hydrolysis upon contact with living tissues, such as skin and flesh. In addition, it exhibits a strong dehydrating property on carbohydrates, liberating extra heat and causing secondary thermal burns. Accordingly, it rapidly attacks the cornea and can induce permanent blindness if splashed onto eyes. If ingested, it damages internal organs irreversibly and may even be fatal. Protective equipment should hence always be used when handling it. Moreover, its strong oxidizing property makes it highly corrosive to many metals and may extend its destruction on other materials. Because of such reasons, damage posed by sulfuric acid is potentially more severe than that by other comparable strong acids, such as hydrochloric acid and nitric acid.
Sulfuric acid must be stored carefully in containers made of nonreactive material (such as glass). Solutions equal to or stronger than 1.5 M are labeled "CORROSIVE", while solutions greater than 0.5 M but less than 1.5 M are labeled "IRRITANT". However, even the normal laboratory "dilute" grade (approximately 1 M, 10%) will char paper if left in contact for a sufficient time.
The standard first aid treatment for acid spills on the skin is, as for other corrosive agents, irrigation with large quantities of water. Washing is continued for at least ten to fifteen minutes to cool the tissue surrounding the acid burn and to prevent secondary damage. Contaminated clothing is removed immediately and the underlying skin washed thoroughly.
Preparation of the diluted acid can also be dangerous due to the heat released in the dilution process. The concentrated acid is always added to water and not the other way around, to take advantage of the relatively high heat capacity of water. Addition of water to concentrated sulfuric acid leads to the dispersal of a sulfuric acid aerosol or worse, an explosion. Preparation of solutions greater than 6 M (35%) in concentration is most dangerous, as the heat produced may be sufficient to boil the diluted acid: efficient mechanical stirring and external cooling (such as an ice bath) are essential.
On a laboratory scale, sulfuric acid can be diluted by pouring concentrated acid onto crushed ice made from de-ionized water. The ice melts in an endothermic process while dissolving the acid. The amount of heat needed to melt the ice in this process is greater than the amount of heat evolved by dissolving the acid so the solution remains cold. After all the ice has melted, further dilution can take place using water.
Industrial hazards.
Although sulfuric acid is non-flammable, contact with metals in the event of a spillage can lead to the liberation of hydrogen gas. The dispersal of acid aerosols and gaseous sulfur dioxide is an additional hazard of fires involving sulfuric acid.
The main occupational risks posed by this acid are skin contact leading to burns (see above) and the inhalation of aerosols. Exposure to aerosols at high concentrations leads to immediate and severe irritation of the eyes, respiratory tract and mucous membranes: this ceases rapidly after exposure, although there is a risk of subsequent pulmonary edema if tissue damage has been more severe. At lower concentrations, the most commonly reported symptom of chronic exposure to sulfuric acid aerosols is erosion of the teeth, found in virtually all studies: indications of possible chronic damage to the respiratory tract are inconclusive as of 1997. In the United States, the permissible exposure limit (PEL) for sulfuric acid is fixed at 1 mg/m3: limits in other countries are similar. There have been reports of sulfuric acid ingestion leading to vitamin B12 deficiency with subacute combined degeneration. The spinal cord is most often affected in such cases, but the optic nerves may show demyelination, loss of axons and gliosis.
Legal restrictions.
International commerce of sulfuric acid is controlled under the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances, 1988, which lists sulfuric acid under Table II of the convention as a chemical frequently used in the illicit manufacture of narcotic drugs or psychotropic substances.

</doc>
<doc id="29248" url="http://en.wikipedia.org/wiki?curid=29248" title="Space colonization">
Space colonization

Space colonization (also called space settlement, or extraterrestrial colonization) is permanent human habitation off planet Earth.
Many arguments have been made for space colonization. The two most common are survival of human civilization and the biosphere in case of a planetary-scale disaster (natural or man-made), and the vast resources in space for expansion of human society.
No space colonies have been built so far. Currently, the building of a space colony would present a set of huge challenges both technological and economic. Space settlements would have to provide for nearly all (or all) the material needs of hundreds or thousands of humans, in an environment out in space that is very hostile to human life. They would involve technologies, such as controlled ecological life support systems, that have yet to be developed in any meaningful way. They would also have to deal with the as yet unknown issue of how humans would behave and thrive in such places long-term. Because of the huge cost of sending anything from the surface of the Earth into orbit (roughly $20,000 USD per kilogram) a space colony would be a massively expensive project.
There are no plans for building one by any large-scale organization, either government or private. However, there have been many proposals, speculations, and designs for space settlements that have been made, and there are a considerable number of space colonization advocates and groups. Several famous scientists, such as Freeman Dyson, have come out in favor of space settlement.
On the technological front, there is ongoing progress in making access to space cheaper, and in creating automated manufacturing and construction techniques. This could in the future lead to widespread space tourism, which could be a stepping stone to space colonization.
Reasons.
Survival of human civilization.
The primary argument that calls for space colonization as a first-order priority is as insurance of the survival of human civilization, by developing alternative locations off Earth where humankind could continue in the event of natural and man-made disasters.
Theoretical physicist and cosmologist Stephen Hawking has argued for space colonization as a means of saving humanity, in 2001 and 2006. In 2001 he predicted that the human race would become extinct within the next thousand years, unless colonies could be established in space. The more recent one in 2006 stated that mankind faces two options: Either we colonize space within the next two hundred years and build residential units on other planets or we will face the prospect of long-term extinction.
In 2005, then NASA Administrator Michael Griffin identified space colonization as the ultimate goal of current spaceflight programs, saying:
Louis J. Halle, formerly of the United States Department of State, wrote in "Foreign Affairs" (Summer 1980) that the colonization of space will protect humanity in the event of global nuclear warfare. The physicist Paul Davies also supports the view that if a planetary catastrophe threatens the survival of the human species on Earth, a self-sufficient colony could "reverse-colonize" Earth and restore human civilization. The author and journalist William E. Burrows and the biochemist Robert Shapiro proposed a private project, the Alliance to Rescue Civilization, with the goal of establishing an off-Earth "backup" of human civilization.
J. Richard Gott has estimated, based on his Copernican principle, that the human race could survive for another 7.8 million years, but it isn't likely to ever colonize other planets. However, he expressed a hope to be proven wrong, because "colonizing other worlds is our best chance to hedge our bets and improve the survival prospects of our species".
Survival of the biosphere.
Many of the same existential risks to humankind would destroy parts or all of Earth's biosphere as well. An example would be a very large asteroid impact. And although many have speculated about life and intelligence existing in other parts of space, Earth is the only place in the universe currently known to harbor either of these.
But even if these threats are averted, eventually Earth is to become uninhabitable. This is due to the Sun's increasing luminosity over its lifetime: the Sun is estimated to have been 70 percent as bright as it is now when it first formed 4.5 billion years ago, and will be 10 percent brighter in about a billion years. It has been suggested that approximately 800 million years from now, that Earth will cease to be able to sustain multi-cellular life. Later on in several billion years, the brightening Sun will cause a runaway greenhouse effect, extinguishing all life on Earth.
Vast resources in space.
Resources in space, both in materials and energy, are enormous. The Solar System alone has, according to different estimates, enough material and energy to support anywhere from several thousand to over a billion times that of the current Earth-based human population. Outside the Solar System in the Milky Way are anywhere up to several hundred billion other stellar systems. Outside the Milky Way are up to several hundred billion other galaxies in the observable universe. 
All these planets and other bodies offer a virtually endless supply of resources providing limitless growth potential. Harnessing these resources can lead to much economic development.
Expansion with fewer negative consequences.
Expansion of humans and technological progress has usually resulted in some form of environmental devastation, and destruction of ecosystems and their accompanying wildlife.The Earth right now has entered another mass extinction event, similar to the one 65 million years ago that wiped out the dinosaurs. The reason for this new mass extinction: us (humans).
Outside the Earth, there are no currently known biospheres to destroy in space.
Expansion has also often come at the expense of displacing many indigenous peoples, the resulting treatment of these peoples ranging anywhere from encroachment to full-blown genocide. Since space has no indigenous peoples this need not be a consequence, as some space settlement advocates have pointed out.
Could help Earth.
Another argument for space colonization is to mitigate the negative effects of overpopulation. If the resources of space were opened to use and viable life-supporting habitats were built, Earth would no longer define the limitations of growth. Although Earth's resources do not grow, humans more and more learn to exploit them effectively, and sometimes even almost completely. As extraterrestrial resources become available, demand on terrestrial ones would decline.
Other arguments.
Additional goals cite the innate human drive to explore and discover, a quality recognized at the core of progress and thriving civilizations.
Nick Bostrom has argued that from a utilitarian perspective, space colonization should be a chief goal as it would enable a very large population to live for a very long period of time (possibly billions of years), which would produce an enormous amount of utility (or happiness). He claims that it is more important to reduce existential risks to increase the probability of eventual colonization than to accelerate technological development so that space colonization could happen sooner. In his paper, he assumes that the created lives will have positive ethical value despite the problem of suffering, or that future technology could solve it.
In a 2001 interview with Freeman Dyson, J. Richard Gott and Sid Goldstein, they were asked for reasons why some humans should live in space. Their answers were:
Goals.
There will be a very high initial investment cost for space colonies and any other permanent space infrastructure due to the high cost of getting into space. However, proponents argue that the long-term vision of developing space infrastructure will provide long-term benefits far in excess of the initial start-up costs. 
Because current space launch costs are so high ($4,000 to $40,000 per kilogram), any serious plans for space colonization must include developing low-cost access to space followed by developing in-situ resource utilization. Therefore, the initial investments must be made in the development of low-cost access to space followed by an initial capacity to provide these necessities: materials, energy, propellant, communication, life support, radiation protection, self-replication, and population.
Although some items of the infrastructure requirements above can already be easily produced on Earth and would therefore not be very valuable as trade items (oxygen, water, base metal ores, silicates, etc.), other high value items are more abundant, more easily produced, of higher quality, or can only be produced in space. These would provide (over the long-term) a very high return on the initial investment in space infrastructure.
Some of these high-value trade goods include precious metals, gem stones, power, solar cells, ball bearings, semi-conductors, and pharmaceuticals.
Space colonization is seen as a long-term goal of some national space programs. Since the advent of the 21st-century commercialization of space, which opened cooperation between NASA and the private sector, several private companies have announced plans toward the colonization of Mars. Among entrepreneurs leading the call for space colonization are Elon Musk, Dennis Tito and Bas Lansdorp.
Potential sites for space colonies include the Moon, Mars, asteroids and free-floating space habitats. Ample quantities of all the necessary materials, such as solar energy and water, are available from or on the Moon, Mars, near-Earth asteroids or other planetary bodies.
The main impediments to commercial exploitation of these resources are the very high cost of initial investment, the very long period required for the expected return on those investments ("The Eros Project" plans a 50 year development.), and the fact that the thing has never been done before — the high-risk nature of the investment.
Major governments and well-funded corporations have announced plans for new categories of activities: space tourism and hotels, prototype space-based solar-power satellites, heavy-lift boosters and asteroid mining—that create needs and capabilities for humans to be in space.
In particular, progresses with the annihilation of matter could render spaceflight and colonization more efficient and affordable, to a revolutionary degree,
and nuclear engineering.
Space colony types.
There are two main types of space colonies:
There is considerable debate among space settlement advocates as to which type (and associated locations) represents the better option for expanding humanity into space.
Space habitats.
Locations in space would necessitate a space habitat, also called space colony and orbital colony, or a space station which would be intended as a permanent settlement rather than as a simple waystation or other specialized facility. They would be literal "cities" in space, where people would live and work and raise families. Many designs have been proposed with varying degrees of realism by both science fiction authors and scientists. Such a space habitat could be isolated from the rest of humanity but near enough to Earth for help. This would test if thousands of humans can survive on their own before sending them beyond the reach of help.
Method.
Building colonies in space would require access to water, food, space, people, construction materials, energy, transportation, communications, life support, simulated gravity, radiation protection and capital investment. It is likely the colonies would be located by proximity to the necessary physical resources. The practice of space architecture seeks to transform spaceflight from a heroic test of human endurance to a normality within the bounds of comfortable experience. As is true of other frontier opening endeavors, the capital investment necessary for space colonization would probably come from the state, an argument made by John Hickman and Neil deGrasse Tyson.
Materials.
Colonies on the Moon, Mars, or asteroids could extract local materials. The Moon is deficient in volatiles such as argon, helium and compounds of carbon, hydrogen and nitrogen. The LCROSS impacter was targeted at the Cabeus crater which was chosen as having a high concentration of water for the Moon. A plume of material erupted in which some water was detected. Anthony Colaprete estimated that the Cabeus crater contains material with 1% water or possibly more. Water ice should also be in other permanently shadowed craters near the lunar poles. Although helium is present only in low concentrations on the Moon, where it is deposited into regolith by the solar wind, an estimated million tons of He-3 exists over all. It also has industrially significant oxygen, silicon, and metals such as iron, aluminum, and titanium.
Launching materials from Earth is expensive, so bulk materials for colonies could come from the Moon, a near-Earth object, Phobos, or Deimos. The benefits of using such sources include: a lower gravitational force, there is no atmospheric drag on cargo vessels, and there is no biosphere to damage. Many NEOs contain substantial amounts of metals. Underneath a drier outer crust (much like oil shale), some other NEOs are inactive comets which include billions of tons of water ice and kerogen hydrocarbons, as well as some nitrogen compounds.
Farther out, Jupiter's Trojan asteroids are thought to be high in water ice and other volatiles.
Recycling of some raw materials would almost certainly be necessary.
Energy.
Solar energy in orbit is abundant, reliable, and is commonly used to power satellites today. There is no night in free space, and no clouds or atmosphere to block sunlight. Light intensity obeys an inverse-square law. So the solar energy available at distance "d" from the Sun is "E" = 1367/"d"2 W/m2, where "d" is measured in astronomical units (AU) and 1367 watts/m2 is the energy available at the distance of Earth's orbit from the Sun, 1 AU.
In the weightlessness and vacuum of space, high temperatures for industrial processes can easily be achieved in solar ovens with huge parabolic reflectors made of metallic foil with very lightweight support structures. Flat mirrors to reflect sunlight around radiation shields into living areas (to avoid line-of-sight access for cosmic rays, or to make the Sun's image appear to move across their "sky") or onto crops are even lighter and easier to build.
Large solar power photovoltaic cell arrays or thermal power plants would be needed to meet the electrical power needs of the settlers' use. In developed nations on Earth, electrical consumption can average 1 kilowatt/person (or roughly 10 megawatt-hours per person per year.) These power plants could be at a short distance from the main structures if wires are used to transmit the power, or much farther away with wireless power transmission.
A major export of the initial space settlement designs was anticipated to be large solar power satellites that would use wireless power transmission (phase-locked microwave beams or lasers emitting wavelengths that special solar cells convert with high efficiency) to send power to locations on Earth, or to colonies on the Moon or other locations in space. For locations on Earth, this method of getting power is extremely benign, with zero emissions and far less ground area required per watt than for conventional solar panels. Once these satellites are primarily built from lunar or asteroid-derived materials, the price of SPS electricity could be lower than energy from fossil fuel or nuclear energy; replacing these would have significant benefits such as elimination of greenhouse gases and nuclear waste from electricity generation.
However, the value of SPS power delivered wirelessly to other locations in Space will typically be far higher than to locations on Earth. Otherwise, the means of generating the power would need to be included with these projects and pay the heavy penalty of Earth launch costs. Therefore, other than proposed demonstration projects for power delivered to Earth, the first priority for SPS electricity is likely to be locations in space, such as communications satellites, fuel depots or "orbital tugboat" boosters transferring cargo and passengers between Low-Earth Orbit (LEO) and other orbits such as Geosynchronous orbit (GEO), lunar orbit or Highly-Eccentric Earth Orbit (HEEO).:132
The Moon has nights of two Earth weeks in duration. Mars has nights, relatively high gravity, and an atmosphere with dust storms to cover and degrade solar panels. Also, its greater distance from the Sun (1.5 astronomical units, AU) translates into "E/(1.52 = 2.25)" only ½-⅔ the solar energy of Earth orbit. For these reasons, nuclear power is sometimes proposed for colonies in these locations. Another alternative would be transmitting energy wirelessly to the lunar or Martian colonies from solar power satellites (SPSs) as described above—note again that the difficulties of generating power in these locations make the relative advantages of SPSs much greater there than for power beamed to locations on Earth.
For both solar thermal and nuclear power generation in airless environments, such as the Moon and space, and to a lesser extent the very thin Martian atmosphere, one of the main difficulties is dispersing the inevitable heat generated. This requires fairly large radiator areas.
Transportation.
Space access.
Transportation to orbit is often the limiting factor in space endeavours. To settle space, much cheaper launch vehicles are required, as well as a way to avoid serious damage to the atmosphere from the thousands, perhaps millions, of launches required. One possibility is the air-breathing hypersonic spaceplane under development by NASA and other organizations, both public and private. Other proposed projects include skyhooks, space elevators, mass drivers, launch loops, and StarTrams.
Cislunar and Solar-System travel.
Transportation of large quantities of materials from the Moon, Phobos, Deimos, and near-Earth asteroids to orbital settlement construction sites is likely to be necessary.
Transportation using off-Earth resources for propellant in conventional rockets would be expected to massively reduce in-space transportation costs compared to the present day. Propellant launched from the Earth is likely to be prohibitively expensive for space colonization, even with improved space access costs.
Other technologies such as tether propulsion, VASIMR, ion drives, solar thermal rockets, solar sails, magnetic sails, and nuclear thermal propulsion can all potentially help solve the problems of high transport cost once in space.
For lunar materials, one well-studied possibility is to build mass drivers to launch bulk materials to waiting settlements. Alternatively, lunar space elevators might be employed.
Local transport.
Lunar rovers and Mars rovers are common features of proposed colonies for those bodies. Space suits would likely be needed for excursions, maintenance, and safety.
Communication.
Compared to the other requirements, communication is easy for orbit and the Moon. A great proportion of current terrestrial communications already passes through satellites. Yet, as colonies further from the Earth are considered, communication becomes more of a burden. Transmissions to and from Mars suffer from significant delays due to the speed of light and the greatly varying distance between conjunction and opposition—the lag will range between 7 and 44 minutes—making real-time communication impractical. Other means of communication that do not require live interaction such as e-mail and voice mail systems should pose no problem.
Life support.
In space settlements, a life support system must recycle or import all the nutrients without "crashing." The closest terrestrial analogue to space life support is possibly that of a nuclear submarine. Nuclear submarines use mechanical life support systems to support humans for months without surfacing, and this same basic technology could presumably be employed for space use. However, nuclear submarines run "open loop"—extracting oxygen from seawater, and typically dumping carbon dioxide overboard, although they recycle existing oxygen. Recycling of the carbon dioxide has been approached in the literature using the Sabatier process or the Bosch reaction.
Although a fully mechanistic life support system is conceivable, a closed ecological system is generally proposed for life support. The Biosphere 2 project in Arizona has shown that a complex, small, enclosed, man-made biosphere can support eight people for at least a year, although there were many problems. A year or so into the two-year mission oxygen had to be replenished, which strongly suggests that they achieved atmospheric closure.
The relationship between organisms, their habitat and the non-Earth environment can be:
A combination of the above technologies is also possible.
Radiation protection.
Cosmic rays and solar flares create a lethal radiation environment in space. In Earth orbit, the Van Allen belts make living above the Earth's atmosphere difficult. To protect life, settlements must be surrounded by sufficient mass to absorb most incoming radiation, unless magnetic or plasma radiation shields were developed.
Passive mass shielding of four metric tons per square meter of surface area will reduce radiation dosage to several mSv or less annually, well below the rate of some populated high natural background areas on Earth. This can be leftover material (slag) from processing lunar soil and asteroids into oxygen, metals, and other useful materials. However, it represents a significant obstacle to maneuvering vessels with such massive bulk (mobile spacecraft being particularly likely to use less massive active shielding). Inertia would necessitate powerful thrusters to start or stop rotation, or electric motors to spin two massive portions of a vessel in opposite senses. Shielding material can be stationary around a rotating interior.
"See also:" Health threat from cosmic rays
Self-replication.
Space manufacturing could enable self-replication. Some think it the ultimate goal because it allows an exponential increase in colonies, while eliminating costs to and dependence on Earth. It could be argued that the establishment of such a colony would be Earth's first act of self-replication (see Gaia spore). Intermediate goals include colonies that expect only information from Earth (science, engineering, entertainment) and colonies that just require periodic supply of light weight objects, such as integrated circuits, medicines, genetic material and tools.
"See also:" von Neumann probe, clanking replicator, molecular nanotechnology
Psychological adjustment.
The monotony and loneliness that comes from a prolonged space mission can leave astronauts susceptible to cabin fever or having a psychotic break. Moreover, lack of sleep, fatigue, and work overload can affect an astronaut's ability to perform well in an environment such as space where every action is critical.
Population size.
In 2002, the anthropologist John H. Moore that a population of 150–180 would permit a stable society to exist for 60 to 80 generations — equivalent to 2000 years.
A much smaller initial population of as little as two women should be viable as long as human embryos are available from Earth. Use of a sperm bank from Earth also allows a smaller starting base with negligible inbreeding.
Researchers in conservation biology have tended to adopt the "50/500" rule of thumb initially advanced by Franklin and Soule. This rule says a short-term effective population size ("N"e) of 50 is needed to prevent an unacceptable rate of inbreeding, whereas a long‐term "N"e of 500 is required to maintain overall genetic variability. The "N"e = 50 prescription corresponds to an inbreeding rate of 1% per generation, approximately half the maximum rate tolerated by domestic animal breeders. The "N"e = 500 value attempts to balance the rate of gain in genetic variation due to mutation with the rate of loss due to genetic drift.
Location.
Location is a frequent point of contention between space colonization advocates. The location of colonization can be on a physical body or free-flying:
Near-Earth space.
Earth orbit.
Compared to other locations, Earth orbit has substantial advantages and one major, but solvable, problem. Orbits close to Earth can be reached in hours, whereas the Moon is days away and trips to Mars take months. There is ample continuous solar power in high Earth orbits. The level of (pseudo-) gravity can be controlled at any desired level by rotating an orbital colony.
The main disadvantage of orbital colonies is lack of materials. These may be expensively imported from the Earth, or more cheaply from extraterrestrial sources, such as the Moon (which has ample metals, silicon, and oxygen), near-Earth asteroids, comets, or elsewhere. As of 2014, the International Space Station provides a temporary, yet still non-autonomous, human presence in low Earth orbit.
The Moon.
Due to its proximity and familiarity, Earth's Moon is discussed as a target for colonization. It has the benefits of proximity to Earth and lower escape velocity, allowing for easier exchange of goods and services. A drawback of the Moon is its low abundance of volatiles necessary for life such as hydrogen, nitrogen, and carbon. Water-ice deposits that exist in some polar craters could serve as a source for these elements. An alternative solution is to bring hydrogen from near-Earth asteroids and combine it with oxygen extracted from lunar rock.
The Moon's low surface gravity is also a concern, as it is unknown whether 1/6g is enough to maintain human health for long periods. 
Lagrange points.
Another near-Earth possibility are the five Earth–Moon Lagrange points. Although they would generally also take a few days to reach with current technology, many of these points would have near-continuous solar power because their distance from Earth would result in only brief and infrequent eclipses of light from the Sun. However, the fact that the Earth–Moon Lagrange points L4 and L5 tend to collect dust and debris, whereas L1-L3 require active station-keeping measures to maintain a stable position, make them somewhat less suitable places for habitation than was originally believed. Additionally, the orbit of L2–L5 takes them out of the protection of the Earth's magnetosphere for approximately two-thirds of the time, exposing them to the health threat from cosmic rays.
The five Earth–Sun Lagrange points would totally eliminate eclipses, but only L1 and L2 would be reachable in a few days' time. The other three Earth–Sun points would require months to reach.
Near-Earth asteroids.
Many small asteroids in orbit around the Sun have the advantage that they pass closer than Earth's moon several times per decade. In between these close approaches to home, the asteroid may travel out to a furthest distance of some 350,000,000 kilometers from the Sun (its aphelion) and 500,000,000 kilometers from Earth.
The inner planets.
Mars.
The surface of Mars is about the same size as the dry land surface of Earth. The ice in Mars' south polar cap, if spread over the planet, would be a layer 12 m thick and there is carbon (locked as carbon dioxide in the atmosphere).
Mars may have gone through similar geological and hydrological processes as Earth and therefore might contain valuable mineral ores. Equipment is available to extract "in situ" resources (e.g. water, air) from the Martian ground and atmosphere. There is interest in colonizing Mars in part because life could have existed on Mars at some point in its history, and may even still exist in some parts of the planet.
However, its atmosphere is very thin (averaging 800 Pa or about 0.8% of Earth sea-level atmospheric pressure); so the pressure vessels necessary to support life are very similar to deep-space structures. The climate of Mars is colder than Earth's. The dust storms block out most of the sun's light for a month or more at a time. Its gravity is only around a third that of Earth's; it is unknown whether this is sufficient to support human beings for extended periods (all long-term human experience to date has been at around Earth gravity, or one g).
The atmosphere is thin enough, when coupled with Mars' lack of magnetic field, that radiation is more intense on the surface, and protection from solar storms would require radiation shielding.
Terraforming Mars would make life outside pressure vessels on the surface possible. There is some discussion of it actually being done.
"See also: Exploration of Mars, Martian terraforming"
Phobos and Deimos.
The moons of Mars may be a target for space colonization. Low delta-v is needed to reach Earth from Phobos and Deimos, allowing delivery of material to cislunar space, as well as transport around the Martian system. The moons themselves may be suitable for habitation, with methods similar to those for asteroids.
Venus.
While the surface of Venus is far too hot and features atmospheric pressure at least 90 times that at sea level on Earth, its massive atmosphere offers a possible alternate location for colonization. At an altitude of approximately 50 km, the pressure is reduced to a few atmospheres, and the temperature would be between 40–100 °C, depending on the altitude. This part of the atmosphere is probably within dense clouds which contain some sulfuric acid. Even these may have a certain benefit to colonization, as they present a possible source for the extraction of water.
Mercury.
There is a suggestion that Mercury could be colonized using the same technology, approach and equipment that is used in colonizing the Moon. Such colonies would almost certainly be restricted to the polar regions due to the extreme daytime temperatures elsewhere on the planet.
Observations of Mercury's polar regions by radar from Earth and the ongoing observations of the Messenger Probe have been consistent with water ice and/or other frozen volatiles being present in permanently shadowed areas of craters in Mercury's polar regions. Measurements of Mercury's exosphere, which is practically a vacuum, revealed more ions derived from water than scientists had expected. All of these observations are consistent with water ice and/or other volatiles being available to hypothetical future colonists of Mercury.
The asteroid belt.
Colonization of asteroids would require space habitats. The asteroid belt has significant overall material available, the largest object being Ceres, although it is thinly distributed as it covers a vast region of space. Unmanned supply craft should be practical with little technological advance, even crossing 1/2 billion kilometers of cold vacuum. The colonists would have a strong interest in assuring that their asteroid did not hit Earth or any other body of significant mass, but would have extreme difficulty in moving an asteroid of any size. The orbits of the Earth and most asteroids are very distant from each other in terms of delta-v and the asteroidal bodies have enormous momentum. Rockets or mass drivers can perhaps be installed on asteroids to direct their path into a safe course.
Ceres.
Ceres is a dwarf planet in the asteroid belt, comprising about one third the mass of the whole belt and being the sixth largest body in the inner Solar System by mass and volume. Ceres has a surface area somewhat larger than Argentina. Being the largest body in the asteroid belt, Ceres could become the main base and transport hub for future asteroid mining infrastructure, allowing mineral resources to be transported further to Mars, the Moon and Earth. See further: Main-Belt Asteroids. It may be possible to paraterraform Ceres, making life easier for the colonists. Given its low gravity and fast rotation, a space elevator would also be practical.
Moons of outer planets.
Jovian moons — Europa, Callisto and Ganymede.
The Artemis Project designed a plan to colonize Europa, one of Jupiter's moons. Scientists were to inhabit igloos and drill down into the Europan ice crust, exploring any sub-surface ocean. This plan discusses possible use of "air pockets" for human inhabitation. Europa is considered one of the more habitable bodies in the Solar System and so merits investigation as a possible abode for life.
Ganymede is the largest moon in the Solar System. It may be attractive as Ganymede is the only moon with a magnetosphere and so is less irradiated at the surface. The presence of magnetosphere, likely indicates a convecting molten core within Ganymede, which may in turn indicate a rich geologic history for the moon.
NASA performed a study called "HOPE" (Revolutionary Concepts for Human Outer Planet Exploration) regarding the future exploration of the Solar System. The target chosen was Callisto. It could be possible to build a surface base that would produce fuel for further exploration of the Solar System.
The three out of four largest moons of Jupiter (Europa, Ganymede and Callisto) have an abundance of volatiles making future colonization possible.
Moons of Saturn — Titan, Enceladus, and others.
Titan is suggested as a target for colonization, because it is the only moon in the Solar System to have a dense atmosphere and is rich in carbon-bearing compounds. Robert Zubrin identified Titan as possessing an abundance of all the elements necessary to support life, making Titan perhaps the most advantageous locale in the outer Solar System for colonization, and saying "In certain ways, Titan is the most hospitable extraterrestrial world within our solar system for human colonization".
Enceladus is a small, icy moon orbiting close to Saturn, notable for its extremely bright surface and the geyser-like plumes of ice and water vapor that erupt from its southern polar region. If Enceladus has liquid water, it joins Mars and Jupiter's moon Europa as one of the prime places in the Solar System to look for extraterrestrial life and possible future settlements.
Other large satellites: Rhea, Iapetus, Dione, Tethys, and Mimas, all have large quantities of volatiles, which can be used to support settlement.
Moons of Uranus and Neptune.
The five large moons of Uranus (Miranda, Ariel, Umbriel, Titania and Oberon) and Triton—Neptune's largest moon—, although very cold, have large amounts of frozen water and other volatiles and could potentially be settled, only they would require a lot of nuclear power to sustain the habitats. Triton's thin atmosphere also contains some nitrogen and even some frozen nitrogen on the surface (the surface temperature is 38 K or about -391°Fahrenheit). Pluto is estimated to have a very similar structure to Triton.
The Kuiper belt and Oort cloud.
The Kuiper belt is estimated to have 70,000 bodies of 100 km or larger.
Freeman Dyson has suggested that within a few centuries human civilization will have relocated to the Kuiper belt.
The Oort cloud is estimated to have up to a trillion comets.
Other Solar System locations.
Statites.
Statites or "static satellites" employ solar sails to position themselves in orbits that gravity alone could not accomplish. Such a solar sail colony would be free to ride solar radiation pressure and travel off the ecliptic plane. Navigational computers with an advanced understanding of flocking behavior could organize several statite colonies into the beginnings of the true "swarm" concept of a Dyson sphere.
Surfaces of giant planets.
It may be possible to colonize the three farthest giant planets with floating cities in their atmospheres. By heating hydrogen balloons, large masses can be suspended underneath at roughly Earth gravity. A human colony on Jupiter would be less practical due to its high gravity, escape velocity, and radiation. Such colonies could export helium-3 for use in fusion reactors if they ever become practical.
Escape from the giant planets, especially Jupiter, seems well beyond current or near-term foreseeable chemical-rocket technology due to the combination of large velocity and high acceleration needed to even achieve low orbit.
Outside the Solar System.
Looking beyond the Solar System, there are up to several hundred billion potential stars with possible colonization targets. The main difficulty is the vast distances to other stars: roughly a hundred thousand times further away than the planets in the Solar System. This means that some combination of very high speed (some percentage of the speed of light), or travel times lasting centuries or millennia, would be required. These speeds are far beyond what current spacecraft propulsion systems can provide.
Many scientific papers have been published about interstellar travel. Given sufficient travel time and engineering work, both unmanned and generational voyages seem possible, though representing a very considerable technological and economic challenge unlikely to be met for some time, particularly for manned probes.
Space colonization technology could in principle allow human expansion at high, but sub-relativistic speeds, substantially less than the speed of light, "c".  An interstellar colony ship would be similar to a space habitat, with the addition of major propulsion capabilities and independent energy generation.
Hypothetical starship concepts proposed both by scientists and in hard science fiction include:
The above concepts all appear limited to high, but still sub-relativistic speeds, due to fundamental energy and reaction mass considerations, and all would entail trip times which might be enabled by space colonization technology, permitting self-contained habitats with lifetimes of decades to centuries. Yet human interstellar expansion at average speeds of even 0.1% of "c"  would permit settlement of the entire Galaxy in less than one half of a galactic rotation period of ~250,000,000 years, which is comparable to the timescale of other galactic processes. Thus, even if interstellar travel at near relativistic speeds is never feasible (which cannot be clearly determined at this time), the development of space colonization could allow human expansion beyond the Solar System without requiring technological advances that cannot yet be reasonably foreseen. This could greatly improve the chances for the survival of intelligent life over cosmic timescales, given the many natural and human-related hazards that have been widely noted.
If humanity does gain access to a large amount of energy, on the order of the mass-energy of entire planets, it may eventually become feasible to construct Alcubierre drives. These are one of the few methods of superluminal travel which may be possible under current physics.
Intergalactic travel.
Looking beyond the Milky Way, there are about 100 billion other galaxies in the observable universe. The distances between galaxies are on the order of a million times further than those between the stars. Because of the speed of light limit on how fast any material objects can travel in space, intergalactic travel would either have to involve voyages lasting millions of years, or a possible faster than light propulsion method based on speculative physics, such as the Alcubierre drive. There are, however, no scientific reasons for stating that intergalactic travel is impossible in principle.
Funding.
Space colonization can roughly be said to be possible when the necessary methods of space colonization become cheap enough (such as space access by cheaper launch systems) to meet the cumulative funds that have been gathered for the purpose.
Although there are no immediate prospects for the large amounts of money required for space colonization to be available given traditional launch costs,
there is some prospect of a radical reduction to launch costs in the 2010s, which would consequently lessen the cost of any efforts in that direction. With a published price of US$ per launch of up to 13150 kg payload to low Earth orbit, SpaceX Falcon 9 rockets are already the "cheapest in the industry". Advancements currently being developed as part of the SpaceX reusable launch system development program to enable reusable Falcon 9s "could drop the price by an order of magnitude, sparking more space-based enterprise, which in turn would drop the cost of access to space still further through economies of scale." If SpaceX is successful in developing the reusable technology, it would be expected to "have a major impact on the cost of access to space", and change the increasingly competitive market in space launch services.
The President's Commission on Implementation of United States Space Exploration Policy suggested that an inducement prize should be established, perhaps by government, for the achievement of space colonization, for example by offering the prize to the first organization to place humans on the Moon and sustain them for a fixed period before they return to Earth.
Terrestrial analogues to space colonies.
The most famous attempt to build an analogue to a self-sufficient colony is Biosphere 2, which attempted to duplicate Earth's biosphere. BIOS-3 is another closed ecosystem, completed in 1972 in Krasnoyarsk, Siberia.
Many space agencies build testbeds for advanced life support systems, but these are designed for long duration human spaceflight, not permanent colonization.
Remote research stations in inhospitable climates, such as the Amundsen–Scott South Pole Station or Devon Island Mars Arctic Research Station, can also provide some practice for off-world outpost construction and operation. The Mars Desert Research Station has a habitat for similar reasons, but the surrounding climate is not strictly inhospitable.
Nuclear submarines provide an example of conditions encountered in artificial space environment. Crews of these vessels often spend long periods (6 months or more) submerged during their deployments. However, the submarine environment provides a somewhat open life support system since the vessel can replenish supplies of fresh water and oxygen from seawater.
Other examples of small groups in isolated living conditions are record long-distance flights, long-distance (single-handed) sails, oil platforms, prisons, bunkers, small islands and underground bases.
The study of terrestrial analogues is also a central focus in space architecture.
History.
The first known work on space colonization was "The Brick Moon", a work of fiction published in 1869 by Edward Everett Hale, about an inhabited artificial satellite.
The Russian schoolmaster and physicist Konstantin Tsiolkovsky foresaw elements of the space community in his book "Beyond Planet Earth" written about 1900. Tsiolkovsky had his space travelers building greenhouses and raising crops in space. Tsiolkovsky believed that going into space would help perfect human beings, leading to immortality and peace.
Others have also written about space colonies as Lasswitz in 1897 and Bernal, Oberth, Von Pirquet and Noordung in the 1920s. Wernher von Braun contributed his ideas in a 1952 "Colliers" article. In the 1950s and 1960s, Dandridge M. Cole published his ideas.
Another seminal book on the subject was the book "The High Frontier: Human Colonies in Space" by Gerard K. O'Neill in 1977 which was followed the same year by "Colonies in Space " by T. A. Heppenheimer.
M. Dyson wrote "Home on the Moon; Living on a Space Frontier" in 2003; Peter Eckart wrote "Lunar Base Handbook" in 2006 and then Harrison Schmitt's "Return to the Moon" written in 2007.
, Bigelow Aerospace is the only private commercial spaceflight company that has launched two experimental space station modules, Genesis I (2006) and Genesis II (2007), into Earth-orbit, and has indicated that their first production model of the space habitat, the BA 330, could be launched by 2017.
Objections.
A corollary to the Fermi paradox—"nobody else is doing it"—is the argument that because no evidence of alien colonization technology exists, it is statistically unlikely to even be possible using that same level of technology ourselves.
Colonizing space would require massive amounts of financial, physical and human capital devoted to research, development, production, and deployment. Earth's natural resources do not increase to a noteworthy extent (which is in keeping with the "only one Earth" position of environmentalists). Thus, considerable efforts in colonizing places outside Earth would appear as a hazardous waste of the Earth's limited resources for an aim without a clear end.
The fundamental problem of public things, needed for survival, such as space programs, is the free rider problem. Convincing the public to fund such programs would require additional self-interest arguments: If the objective of space colonization is to provide a "backup" in case everyone on Earth is killed, then why should someone on Earth pay for something that is only useful after they are dead? This assumes that space colonization is not widely acknowledged as a sufficiently valuable social goal.
Although seen as a relief to the problem of overpopulation, others have argued that space colonization is an impractical solution; in 1999, science fiction author Arthur C. Clarke said that "the population battle must be fought or won here on Earth".
Other objections include concern about creating a culture in which humans are no longer seen as human, but rather as material assets. The issues of human dignity, morality, philosophy, culture, bioethics, and the threat of megalomaniac leaders in these new "societies" would all have to be addressed in order for space colonization to meet the psychological and social needs of people living in isolated colonies.
As an alternative or addendum for the future of the human race, many science fiction writers have focused on the realm of the 'inner-space', that is the computer-aided exploration of the human mind and human consciousness—possibly en route developmentally to a Matrioshka Brain.
Robotic exploration is proposed as an alternative to gain many of the same scientific advantages without the limited mission duration and high cost of life support and return transportation involved in manned missions.
Another objection is the potential to cause interplanetary contamination on planets that may harbor hypothetical extraterrestrial life.
Involved organizations.
Organizations that contribute to space colonization include:
In fiction.
Although established space colonies are a stock element in science fiction stories, fictional works that explore the themes, social or practical, of the settlement and occupation of a habitable world are much rarer.

</doc>
<doc id="29250" url="http://en.wikipedia.org/wiki?curid=29250" title="Second Council of Nicaea">
Second Council of Nicaea

The Second Council of Nicaea is recognized as the seventh of the first seven ecumenical councils by both West and East. Orthodox, Catholics, and Old Catholics unanimously recognize it; Protestant opinions on it are varied. 
It met in AD 787 in Nicaea (site of the First Council of Nicaea; present-day İznik in Turkey) to restore the use and veneration of icons (or, holy images), which had been suppressed by imperial edict inside the Byzantine Empire during the reign of Leo III (717–741). His son, Constantine V (741–775), had held the Council of Hieria to make the suppression official.
Background.
The veneration of icons had been banned by Byzantine Emperor Constantine V and supported by his Council of Hieria (754 AD), which had described itself as the seventh ecumenical council. The Council of Hieria was overturned by the Second Council of Nicaea only 33 years later, and has also been rejected by Catholic and Orthodox churches, since none of the five major patriarchs were represented. The emperor's vigorous enforcement of the ban included persecution of those who worshiped icons and monks in general. There were also political overtones to the persecution—images of emperors were still allowed by Constantine, which some opponents saw as an attempt to give wider authority to imperial power than to the saints and bishops. Constantine's iconoclastic tendencies were shared by Constantine's son, Leo IV. After the latter's early death, his widow, Irene of Athens, as regent for her son, began its restoration, moved thereto by personal inclination and political considerations.
In 784 the imperial secretary Patriarch Tarasius was appointed successor to the Patriarch Paul IV—he accepted on the condition that intercommunion with the other churches should be reestablished; that is, that the images should be restored. However, a council, claiming to be ecumenical, had abolished the veneration of icons, so psychologically another ecumenical council was necessary for its restoration. 
Pope Adrian I was invited to participate, and gladly accepted. However, the invitation intended for the oriental patriarchs could not even be delivered to them. The Roman legates were an archbishop and an abbot, both named Peter.
In 786, the council met in the Church of the Holy Apostles in Constantinople. However, soldiers in collusion with the opposition entered the church, and broke up the assembly. As a result, the government resorted to a stratagem. Under the pretext of a campaign, the iconoclastic bodyguard was sent away from the capital — disarmed and disbanded.
The council was again summoned to meet, this time in Nicaea, since Constantinople was still distrusted. The council assembled on September 24, 787 at the church of Hagia Sophia. It numbered about 350 members; 308 bishops or their representatives signed. Tarasius presided, and seven sessions were held in Nicaea.
Proceedings of the Council.
"First Session" (September 24, 787) — Three bishops, Basilius of Ancyra, Theodore of Myra, and Theodosius of Amorium begged for pardon for the heresy of iconoclasm.
"Second Session" (September 26, 787) — Papal legates read the letters of Pope Hadrian I asking for agreement with veneration of images, to which question the bishops of the council answered: "We follow, we receive, we admit".
"Third Session" (September 28, 787) — Other bishops having made their abjuration, were received into the council.
"Fourth Session" (October 1, 787) — Proof of the lawfulness of the veneration of icons was drawn from Exodus 25:19 sqq.; Numbers 7:89; Hebrews 9:5 sqq.; Ezekiel 41:18, and Genesis 31:34, but especially from a series of passages of the Church Fathers; the authority of the latter was decisive. 
"Fifth Session" (October 4, 787) — It was claimed that the iconoclast heresy came originally from Jews, Saracens, and Manicheans.
"Sixth Session" (October 6, 787) — The definition of the pseudo-Seventh council (754) was read and condemned.
"Seventh Session" (October 13, 787) — The council issued a declaration of faith concerning the veneration of holy images.
It was determined that "As the sacred and life-giving cross is everywhere set up as a symbol, so also should the images of Jesus Christ, the Virgin Mary, the holy angels, as well as those of the saints and other pious and holy men be embodied in the manufacture of sacred vessels, tapestries, vestments, etc., and exhibited on the walls of churches, in the homes, and in all conspicuous places, by the roadside and everywhere, to be revered by all who might see them. For the more they are contemplated, the more they move to fervent memory of their prototypes. Therefore, it is proper to accord to them a fervent and reverent adoration, not, however, the veritable worship which, according to our faith, belongs to the Divine Being alone — for the honor accorded to the image passes over to its prototype, and whoever adores the image adores in it the reality of what is there represented."
"Eighth Session" (October 23, 787) — The last session was held in Constantinople at the Magnaura Palace. The Empress Irene and her son were present and they signed the document.
The clear distinction between the adoration offered to God, and that accorded to the images may well be looked upon as a result of the iconoclastic reform. The twenty-two canons drawn up in Constantinople also served ecclesiastical reform. Careful maintenance of the ordinances of the earlier councils, knowledge of the scriptures on the part of the clergy, and care for Christian conduct are required, and the desire for a renewal of ecclesiastical life is awakened.
The council also decreed that every altar should contain a relic, which remains the case in modern Catholic and Orthodox regulations (Canon VII), and made a number of decrees on clerical discipline, especially for monks when mixing with women. 
The papal legates voiced their approval of the restoration of the veneration of icons in no uncertain terms, and the patriarch sent a full account of the proceedings of the council to Pope Hadrian I, who had it translated (the translation Anastasius later replaced with a better one).
This council is celebrated in the Eastern Orthodox Church, and Eastern Catholic Churches of Byzantine Rite as "The Sunday of the Triumph of Orthodoxy" each year on the first Sunday of Great Lent—the fast that leads up to Pascha (Easter)—and again on the Sunday closest to October 11 (the Sunday on or after October 8). The former celebration commemorates the council as the culmination of the Church's battles against heresy, while the latter commemorates the council itself.

</doc>
<doc id="29252" url="http://en.wikipedia.org/wiki?curid=29252" title="Sexual orientation">
Sexual orientation

Sexual orientation is an enduring pattern of romantic or sexual attraction (or a combination of these) to persons of the opposite sex or gender, the same sex or gender, or to both sexes or more than one gender. These attractions are generally subsumed under heterosexuality, homosexuality, and bisexuality, while asexuality (the lack of sexual attraction to others) is sometimes identified as the fourth category. These categories are aspects of the more nuanced nature of sexual identity. For example, people may use other labels, such as "pansexual" or "polysexual", or none at all. According to the American Psychological Association, sexual orientation "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions". 
The term "sexual preference" largely overlaps with sexual orientation, but is generally distinguished in psychological research. A person who identifies as bisexual, for example, may sexually prefer one sex over the other. "Sexual preference" may also suggest a degree of voluntary choice, whereas the scientific consensus is that sexual orientation is not a choice.
There is no consensus among scientists about why a person develops a particular sexual orientation. Many scientists think that nature and nurture – a combination of genetic, hormonal, and environmental influences – factor into the cause of sexual orientation. They favor biologically-based theories, which point to genetic factors, the early uterine environment, both, or the inclusion of genetic and social factors. There is no substantive evidence which suggests parenting or early childhood experiences play a role when it comes to sexual orientation; when it comes to same-sex sexual behavior, shared or familial environment plays no role for men and minor role for women. Research over several decades has demonstrated that sexual orientation ranges along a continuum, from exclusive attraction to the opposite sex to exclusive attraction to the same sex.
Sexual orientation is reported primarily within biology and psychology (including sexology), but it is also a subject area in anthropology and history (including social constructionism), and there are other explanations that relate to sexual orientation and culture.
Definitions and distinguishing from sexual identity and behavior.
General.
Sexual orientation is traditionally defined as including heterosexuality, bisexuality, and homosexuality, while asexuality is considered the fourth category of sexual orientation by some researchers and has been defined as the absence of a traditional sexual orientation. An asexual has little to no sexual attraction to males or females. It may be considered a lack of a sexual orientation, and there is significant debate over whether or not it is a sexual orientation.
Most definitions of sexual orientation include a psychological component, such as the direction of an individual's erotic desires, or a behavioral component, which focuses on the sex of the individual's sexual partner/s. Some people prefer simply to follow an individual's self-definition or identity. Scientific and professional understanding is that "the core attractions that form the basis for adult sexual orientation typically emerge between middle childhood and early adolescence". Sexual orientation differs from sexual identity in that it encompasses relationships with others, while sexual identity is a concept of self.
The American Psychological Association states that "[s]exual orientation refers to an enduring pattern of emotional, romantic, and/or sexual attractions to men, women, or both sexes" and that "[t]his range of behaviors and attractions has been described in various cultures and nations throughout the world. Many cultures use identity labels to describe people who express these attractions. In the United States, the most frequent labels are lesbians (women attracted to women), gay men (men attracted to men), and bisexual people (men or women attracted to both sexes). However, some people may use different labels or none at all". They additionally state that sexual orientation "is distinct from other components of sex and gender, including biological sex (the anatomical, physiological, and genetic characteristics associated with being male or female), gender identity (the psychological sense of being male or female), and social gender role (the cultural norms that define feminine and masculine behavior)". According to psychologists, sexual orientation also refers to a person’s choice of sexual partners, who may be homosexual, heterosexual, or bisexual.
Sexual identity and sexual behavior are closely related to sexual orientation, but they are distinguished, with sexual identity referring to an individual's conception of themselves, behavior referring to actual sexual acts performed by the individual, and orientation referring to "fantasies, attachments and longings." Individuals may or may not express their sexual orientation in their behaviors. People who have a homosexual sexual orientation that does not align with their sexual identity are sometimes referred to as 'closeted'. The term may, however, reflect a certain cultural context and particular stage of transition in societies which are gradually dealing with integrating sexual minorities. In studies related to sexual orientation, when dealing with the degree to which a person's sexual attractions, behaviors and identity match, scientists usually use the terms "concordance" or "discordance." Thus, a woman who is attracted to other women, but calls herself heterosexual and only has sexual relations with men, can be said to experience discordance between her sexual orientation (homosexual or lesbian) and her sexual identity and behaviors (heterosexual).<ref name="Concordance/discordance in SO"></ref>
"Sexual identity" may also be used to describe a person's perception of his or her own "sex", rather than sexual orientation. The term "sexual preference" has a similar meaning to "sexual orientation", and the two terms are often used interchangeably, but "sexual preference" suggests a degree of voluntary choice. The term has been a listed by the American Psychological Association's Committee on Gay and Lesbian Concerns as a wording that advances a "heterosexual bias".
Androphilia, gynephilia and other terms.
"Androphilia" and "gynephilia" (or "gynecophilia") are terms used in behavioral science to describe sexual attraction, as an alternative to a homosexual and heterosexual conceptualization. They are used for identifying a subject's object of attraction without attributing a sex assignment or gender identity to the subject. Related terms such as "pansexual" and polysexual do not make any such assignations to the subject. People may also use terms such as "queer", "pansensual," "polyfidelitous," "ambisexual," or personalized identities such as "byke" or "biphilic".
SGL (Same Gender Loving) is considered to be more than a different term for gay; it introduces the concept of love into the discussion. SGL also acknowledges relationships between people of like identities, for example third gender individuals who may be oriented toward each other, and expands the discussion of sexuality beyond the original man/woman gender duality. The complexity of transgender orientation is also more completely understood within this perspective.
Using "androphilia" and "gynephilia" can avoid confusion and offense when describing people in non-western cultures, as well as when describing intersex and transgender people. Psychiatrist Anil Aggrawal explains that androphilia, along with gynephilia, "is needed to overcome immense difficulties in characterizing the sexual orientation of trans men and trans women. For instance, it is difficult to decide whether a trans man erotically attracted to males is a heterosexual female or a homosexual male; or a trans woman erotically attracted to females is a heterosexual male or a lesbian female. Any attempt to classify them may not only cause confusion but arouse offense among the affected subjects. In such cases, while defining sexual attraction, it is best to focus on the object of their attraction rather than on the sex or gender of the subject." Sexologist Milton Diamond writes, "The terms heterosexual, homosexual, and bisexual are better used as adjectives, not nouns, and are better applied to behaviors, not people. This usage is particularly advantageous when discussing the partners of transsexual or intersexed individuals. These newer terms also do not carry the social weight of the former ones."
Some researchers advocate use of the terminology to avoid bias inherent in Western conceptualizations of human sexuality. Writing about the Samoan fa'afafine demographic, sociologist Johanna Schmidt writes that in cultures where a third gender is recognized, a term like "homosexual transsexual" does not align with cultural categories.
Some researchers, such as Bruce Bagemihl, have criticized the labels "heterosexual" and "homosexual" as confusing and degrading. Bagemihl writes, "...the point of reference for 'heterosexual' or 'homosexual' orientation in this nomenclature is solely the individual's genetic sex prior to reassignment (see for example, Blanchard et al. 1987, Coleman and Bockting, 1988, Blanchard, 1989). These labels thereby ignore the individual's personal sense of gender identity taking precedence over biological sex, rather than the other way around." Bagemihl goes on to take issue with the way this terminology makes it easy to claim transsexuals are really homosexual males seeking to escape from stigma.
Gender, transgender, cisgender, and conformance.
The earliest writers on sexual orientation usually understood it to be intrinsically linked to the subject's own sex. For example, it was thought that a typical female-bodied person who is attracted to female-bodied persons would have masculine attributes, and vice versa. This understanding was shared by most of the significant theorists of sexual orientation from the mid nineteenth to early twentieth century, such as Karl Heinrich Ulrichs, Richard von Krafft-Ebing, Magnus Hirschfeld, Havelock Ellis, Carl Jung, and Sigmund Freud, as well as many gender-variant homosexual people themselves. However, this understanding of homosexuality as sexual inversion was disputed at the time, and, through the second half of the twentieth century, gender identity came to be increasingly seen as a phenomenon distinct from sexual orientation. Transgender and cisgender people may be attracted to men, women, or both, although the prevalence of different sexual orientations is quite different in these two populations (see sexual orientation of trans women). An individual homosexual, heterosexual or bisexual person may be masculine, feminine, or androgynous, and in addition, many members and supporters of lesbian and gay communities now see the "gender-conforming heterosexual" and the "gender-nonconforming homosexual" as negative stereotypes. Nevertheless, studies by J. Michael Bailey and KJ Zucker found a majority of the gay men and lesbians sampled reporting various degrees of gender-nonconformity during their childhood years.
Transgender people today identify with the sexual orientation that corresponds with their gender; meaning that a trans woman who is solely attracted to women would often identify as a lesbian. A trans man solely attracted to women would be a straight man.
Sexual orientation sees greater intricacy when non-binary understandings of both sex (male, female, or intersex) and gender (man, woman, transgender, third gender, etc. are considered. Sociologist Paula Rodriguez Rust (2000) argues for a more multifaceted definition of sexual orientation:
 ...Most alternative models of sexuality... define sexual orientation in terms of dichotomous biological sex or gender... Most theorists would not eliminate the reference to sex or gender, but instead advocate incorporating more complex nonbinary concepts of sex or gender, more complex relationships between sex, gender, and sexuality, and/or additional nongendered dimensions into models of sexuality.
 — Paula C. Rodriguez Rust
Relationships outside of orientation.
Gay and lesbian people can have sexual relationships with someone of the opposite sex for a variety of reasons, including the desire for a perceived traditional family and concerns of discrimination and religious ostracism. While some LGBT people hide their respective orientations from their spouses, others develop positive gay and lesbian identities while maintaining successful marriages. Coming out of the closet to oneself, a spouse of the opposite sex, and children can present challenges that are not faced by gay and lesbian people who are not married to people of the opposite sex or do not have children.
Fluidity.
General aspects.
According to the American Psychological Association and the Royal College of Psychiatrists' Gay and Lesbian Mental Health Special Interest Group, there is no sound scientific evidence that sexual orientation can be changed. The American Psychological Association states "most people experience little or no sense of choice about their sexual orientation." The American Psychiatric Association states that "some people believe that sexual orientation is innate and fixed; however, sexual orientation develops across a person's lifetime. Individuals maybe become aware at different points in their lives that they are heterosexual, gay, lesbian, or bisexual." People realizing their sexual orientation at any point in their lives is thoroughly documented in research.
Often, sexual orientation and sexual identity are not distinguished, which can impact accurately assessing sexual identity and whether or not sexual orientation is able to change; sexual identity can change throughout a person's life and may at times not align with actual sexual orientation. At least one study suggests that self-reported sexual orientation in a community may change over time in response to differing social trends. A report from the Centre for Addiction and Mental Health states, "For some people, sexual orientation is continuous and fixed throughout their lives. For others, sexual orientation may be fluid and change over time." Other research suggests that "[f]or some [people] the focus of sexual interest will shift at various points through the life span..."
"There... [was, as of 1995,] essentially no research on the longitudinal stability of sexual orientation over the adult life span... It [was]... still an unanswered question whether... [the] measure [of 'the complex components of sexual orientation as differentiated from other aspects of sexual identity at one point in time'] will predict future behavior or orientation. Certainly, it is... not a good predictor of past behavior and self-identity, given the developmental process common to most gay men and lesbians (i.e., denial of homosexual interests and heterosexual experimentation prior to the coming-out process)." Some studies report that "[a number of] lesbian women, and some heterosexual women as well, perceive choice as an important element in their sexual orientations."
Born bisexual, then monosexualizing.
Innate bisexuality is an idea introduced by Sigmund Freud. According to this theory, all humans are born bisexual in a very broad sense of the term, that of incorporating general aspects of both sexes. In Freud's view, this was true anatomically and therefore also psychologically, with sexual attraction to both sexes being one part of this psychological bisexuality. Freud believed that in the course of sexual development the masculine side would normally become dominant in men and the feminine side in women, but that as adults everyone still has desires derived from both the masculine and the feminine sides of their natures. Freud did not claim that everyone is bisexual in the sense of feeling the same level of sexual attraction to both genders.
Causes.
The exact causes for the development of a particular sexual orientation have yet to be established. To date, a lot of research has been conducted to determine the influence of genetics, hormonal action, development dynamics, social and cultural influences—which has led many to think that biology and environment factors play a complex role in forming it. It was once thought that homosexuality was the result of faulty psychological development, resulting from childhood experiences and troubled relationships, including childhood sexual abuse. It has been found that this was based on prejudice and misinformation.
Biology.
Research has identified several biological factors which may be related to the development of sexual orientation, including genes, prenatal hormones, and brain structure. No single controlling cause has been identified, and research is continuing in this area.
Though researchers generally believe that sexual orientation is not determined by any one factor but by a combination of genetic, hormonal, and environmental influences, with biological factors involving a complex interplay of genetic factors and the early uterine environment, they favor biological models for the cause. They believe that sexual orientation is not a choice, and some of them believe that it is established at conception. That is, individuals do not choose to be homosexual, heterosexual, bisexual, or asexual. 
While current scientific investigation usually seeks to find biological explanations for the adoption of a particular sexual orientation, there are yet no replicated scientific studies supporting any specific biological etiology for sexual orientation. However, scientific studies have found a number of statistical biological differences between gay people and heterosexuals, which may result from the same underlying cause as sexual orientation itself.
Genetic factors.
Genes may be related to the development of sexual orientation. At one time, studies of twins appeared to point to a major genetic component, but problems in experimental design of the available studies have made their interpretation difficult, and one recent study appears to exclude genes as a major factor.
Hormones.
The hormonal theory of sexuality holds that, just as exposure to certain hormones plays a role in fetal sex differentiation, such exposure also influences the sexual orientation that emerges later in the adult. Fetal hormones may be seen as either the primary influence upon adult sexual orientation or as a co-factor interacting with genes or environmental and social conditions.
As female fetuses have two X chromosomes and male ones a XY pair, the chromosome Y is the responsible for producing male differentiation on the defect female development. The differentiation process is driven by androgen hormones, mainly testosterone and dihydrotestosterone (DHT). The newly formed testicles in the fetus are responsible for the secretion of androgens, that will cooperate in driving the sexual differentiation of the developing fetus, included its brain. This results in sexual differences between males and females. This fact has led some scientists to test in various ways the result of modifying androgen exposure levels in mammals during fetus and early life.
Birth order.
Recent studies found an increased chance of homosexuality in men whose mothers previously carried to term many male children. This effect is nullified if the man is left-handed.
Known as the "fraternal birth order" (FBO) effect, this theory has been backed up by strong evidence of its prenatal origin, although no evidence thus far has linked it to an exact prenatal mechanism. However, research suggests that this may be of immunological origin, caused by a maternal immune reaction against a substance crucial to male fetal development during pregnancy, which becomes increasingly likely after every male gestation. As a result of this immune effect, alterations in later-born males' prenatal development have been thought to occur. This process, known as the maternal immunization hypothesis (MIH), would begin when cells from a male fetus enter the mother's circulation during pregnancy or while giving birth. These Y-linked proteins would not be recognized in the mother's immune system due to the fact that she is female, causing her to develop antibodies which would travel through the placental barrier into the fetal compartment. From here, the anti-male bodies would then cross the blood/brain barrier (BBB) of the developing fetal brain, altering sex-dimorphic brain structures relative to sexual orientation, causing the exposed son to be more attracted to men over women.
Environmental factors.
There is no substantive evidence to support the suggestion that early childhood experiences, parenting, sexual abuse, or other adverse life events influence sexual orientation; however, studies do find that aspects of sexuality expression have an experiential basis and that parental attitudes towards a particular sexual orientation may affect how children of the parents experiment with behaviors related to a certain sexual orientation.
Influences: professional organizations' statements.
The American Academy of Pediatrics in 2004 stated:
The mechanisms for the development of a particular sexual orientation remain unclear, but the current literature and most scholars in the field state that one's sexual orientation is not a choice; that is, individuals do not choose to be homosexual or heterosexual. A variety of theories about the influences on sexual orientation have been proposed. Sexual orientation probably is not determined by any one factor but by a combination of genetic, hormonal, and environmental influences. In recent decades, biologically based theories have been favored by experts. Although there continues to be controversy and uncertainty as to the genesis of the variety of human sexual orientations, there is no scientific evidence that abnormal parenting, sexual abuse, or other adverse life events influence sexual orientation. Current knowledge suggests that sexual orientation is usually established during early childhood.
The American Psychological Association, the American Psychiatric Association, and the National Association of Social Workers in 2006 stated:
Currently, there is no scientific consensus about the specific factors that cause an individual to become heterosexual, homosexual, or bisexual – including possible biological, psychological, or social effects of the parents' sexual orientation. However, the available evidence indicates that the vast majority of lesbian and gay adults were raised by heterosexual parents and the vast majority of children raised by lesbian and gay parents eventually grow up to be heterosexual.
The Royal College of Psychiatrists in 2007 stated:
Despite almost a century of psychoanalytic and psychological speculation, there is no substantive evidence to support the suggestion that the nature of parenting or early childhood experiences play any role in the formation of a person's fundamental heterosexual or homosexual orientation. It would appear that sexual orientation is biological in nature, determined by a complex interplay of genetic factors and the early uterine environment. Sexual orientation is therefore not a choice, though sexual behaviour clearly is.
The American Psychiatric Association stated:
No one knows what causes heterosexuality, homosexuality, or bisexuality. Homosexuality was once thought to be the result of troubled family dynamics or faulty psychological development. Those assumptions are now understood to have been based on misinformation and prejudice.
A legal brief dated September 26, 2007, and presented on behalf of the American Psychological Association, California Psychological Association, American Psychiatric Association, National Association of Social Workers, and National Association of Social Workers, California Chapter, stated:
Although much research has examined the possible genetic, hormonal, developmental, social, and cultural influences on sexual orientation, no findings have emerged that permit scientists to conclude that sexual orientation – heterosexuality, homosexuality, or bisexuality – is determined by any particular factor or factors. The evaluation of "amici" is that, although some of this research may be promising in facilitating greater understanding of the development of sexual orientation, it does not permit a conclusion based in sound science at the present time as to the cause or causes of sexual orientation, whether homosexual, bisexual, or heterosexual.
Efforts to change sexual orientation.
"Sexual orientation change efforts" are methods that aim to change a same-sex sexual orientation. They may include behavioral techniques, cognitive behavioral techniques, "reparative therapy", psychoanalytic techniques, medical approaches, and religious and spiritual approaches.
No major mental health professional organization has sanctioned efforts to change sexual orientation and virtually all of them have adopted policy statements cautioning the profession and the public about treatments that purport to change sexual orientation. These include the American Psychiatric Association, American Psychological Association, American Counseling Association, National Association of Social Workers in the USA, the Royal College of Psychiatrists, and the Australian Psychological Society.
In 2009, the American Psychological Association Task Force on Appropriate Therapeutic Responses to Sexual Orientation conducted a systematic review of the peer-reviewed journal literature on sexual orientation change efforts (SOCE) and concluded:
Efforts to change sexual orientation are unlikely to be successful and involve some risk of harm, contrary to the claims of SOCE practitioners and advocates. Even though the research and clinical literature demonstrate that same-sex sexual and romantic attractions, feelings, and behaviors are normal and positive variations of human sexuality, regardless of sexual orientation identity, the task force concluded that the population that undergoes SOCE tends to have strongly conservative religious views that lead them to seek to change their sexual orientation. Thus, the appropriate application of affirmative therapeutic interventions for those who seek SOCE involves therapist acceptance, support, and understanding of clients and the facilitation of clients' active coping, social support, and identity exploration and development, without imposing a specific sexual orientation identity outcome.
In 2012, the Pan American Health Organization (the North and South American branch of the World Health Organization) released a statement cautioning against services that purport to "cure" people with non-heterosexual sexual orientations as they lack medical justification and represent a serious threat to the health and well-being of affected people, and noted that the global scientific and professional consensus is that homosexuality is a normal and natural variation of human sexuality and cannot be regarded as a pathological condition. The Pan American Health Organization further called on governments, academic institutions, professional associations and the media to expose these practices and to promote respect for diversity. The World Health Organization affiliate further noted that gay minors have sometimes been forced to attend these "therapies" involuntarily, being deprived of their liberty and sometimes kept in isolation for several months, and that these findings were reported by several United Nations bodies. Additionally, the Pan American Health Organization recommended that such malpractices be denounced and subject to sanctions and penalties under national legislation, as they constitute a violation of the ethical principles of health care and violate human rights that are protected by international and regional agreements.
The National Association for Research & Therapy of Homosexuality (NARTH), which describes itself as a "professional, scientific organization that offers hope to those who struggle with unwanted homosexuality," disagrees with the mainstream mental health community's position on conversion therapy. The American Psychological Association and the Royal College of Psychiatrists expressed concerns that the positions espoused by NARTH are not supported by the science and create an environment in which prejudice and discrimination can flourish.
Assessment and measurement.
Varying definitions and strong social norms about sexuality can make sexual orientation difficult to quantify.
Early classification schemes.
One of the earliest sexual orientation classification schemes was proposed in the 1860s by Karl Heinrich Ulrichs in a series of pamphlets he published privately. The classification scheme, which was meant only to describe males, separated them into three basic categories: "dionings, urnings" and "uranodionings". An "urning" can be further categorized by degree of effeminacy. These categories directly correspond with the categories of sexual orientation used today: "heterosexual", "homosexual", and "bisexual". In the series of pamphlets, Ulrichs outlined a set of questions to determine if a man was an "urning". The definitions of each category of Ulrichs' classification scheme are as follows:
From at least the late nineteenth century in Europe, there was speculation that the range of human sexual response looked more like a continuum than two or three discrete categories. Berlin sexologist Magnus Hirschfeld published a scheme in 1896 that measured the strength of an individual's sexual desire on two independent 10-point scales, A (homosexual) and B (heterosexual). A heterosexual individual may be A0, B5; a homosexual individual may be A5, B0; an asexual would be A0, B0; and someone with an intense attraction to both sexes would be A9, B9.
Kinsey scale.
The Kinsey scale, also called the Heterosexual-Homosexual Rating Scale was first published in "Sexual Behavior in the Human Male" (1948) by Alfred Kinsey, Wardell Pomeroy, and Clyde Martin and also featured in "Sexual Behavior in the Human Female" (1953). The scale was developed to combat the assumption at the time that people are either heterosexual or homosexual and that these two types represent antitheses in the sexual world. Recognizing that a large portion of population is not completely heterosexual or homosexual and people can experience both heterosexual and homosexual behavior and psychic responses, Kinsey et al., stated:
Males do not represent two discrete populations, heterosexual and homosexual. The world is not to be divided into sheep and goats. Not all things are black nor all things white... The living world is a continuum in each and every one of its aspects. The sooner we learn this concerning human sexual behavior, the sooner we shall reach a sound understanding of the realities of sex.—Kinsey et al. (1948) pp. 639.
The Kinsey scale provides a classification of sexual orientation based on the relative amounts of heterosexual and homosexual experience or psychic response in one's history at a given time. The classification scheme works such that individuals in the same category show the same balance between the heterosexual and homosexual elements in their histories. The position on the scale is based on the relation of heterosexuality to homosexuality in one's history, rather than the actual amount of overt experience or psychic response. An individual can be assigned a position on the scale in accordance with the following definitions of the points of the scale:
The Kinsey scale has been praised for dismissing the dichotomous classification of sexual orientation and allowing for a new perspective on human sexuality. However, the scale has been criticized because it is still not a true continuum. Despite seven categories being able to provide a more accurate description of sexual orientation than a dichotomous scale it is still difficult to determine which category individuals should be assigned to. In a major study comparing sexual response in homosexual males and females, Masters and Johnson discuss the difficulty of assigning the Kinsey ratings to participants. Particularly, they found it difficult to determine the relative amount heterosexual and homosexual experience and response in a person's history when using the scale. They report finding it difficult to assign ratings 2-4 for individuals with a large number of heterosexual and homosexual experiences. When, there is a lot of heterosexual and homosexual experiences in one's history it becomes difficult for that individual to be fully objective in assessing the relative amount of each.
Weinrich et al. (1993) and Weineberg et al. (1994) criticized the scale for lumping individuals who are different based on different dimensions of sexuality into the same categories. When applying the scale, Kinsey considered two dimensions of sexual orientation: overt sexual experience and psychosexual reactions. Valuable information was lost by collapsing the two values into one final score. A person who has only predominantly same sex reactions is different from someone with relatively little reaction but lots of same sex experience. It would have been quite simple for Kinsey to have measured the two dimensions separately and report scores independently to avoid loss of information. Furthermore, there are more than two dimensions of sexuality to be considered. Beyond behavior and reactions, one could also assess attraction, identification, lifestyle etc. This is addressed by the Klein Sexual Orientation Grid.
A third concern with the Kinsey scale is that it inappropriately measures heterosexuality and homosexuality on the same scale, making one a tradeoff of the other. Research in the 1970s on masculinity and femininity found that concepts of masculinity and femininity are more appropriately measured as independent concepts on a separate scale rather than as a single continuum, with each end representing opposite extremes. When compared on the same scale, they act as tradeoffs such, whereby to be more feminine one had to be less masculine and vice versa. However, if they are considered as separate dimensions one can be simultaneously very masculine and very feminine. Similarly, considering heterosexuality and homosexuality on separate scales would allow one to be both very heterosexual and very homosexual or not very much of either. When they are measured independently, the degree of heterosexual and homosexual can be independently determined, rather than the balance between heterosexual and homosexual as determined using the Kinsey Scale.
Klein Sexual Orientation Grid.
In response to the criticism of the Kinsey scale only measuring two dimensions of sexual orientation, Fritz Klein developed the Klein sexual orientation grid (KSOG), a multidimensional scale for describing sexual orientation. Introduced in Klein's book "The Bisexual Option", the KSOG uses a 7-point scale to assess seven different dimensions of sexuality at three different points in an individual's life: past (from early adolescence up to one year ago), present (within the last 12 months), and ideal (what would you choose if it were completely your choice).
The Sell Assessment of Sexual Orientation.
The Sell Assessment of Sexual Orientation (SASO) was developed to address the major concerns with the Kinsey Scale and Klein Sexual Orientation Grid and as such, measures sexual orientation on a continuum, considers various dimensions of sexual orientation, and considers homosexuality and heterosexuality separately. Rather than providing a final solution to the question of how to best measure sexual orientation, the SASO is meant to provoke discussion and debate about measurements of sexual orientation.
The SASO consists of 12 questions. Six of these questions assess sexual attraction, four assess sexual behavior, and two assess sexual orientation identity. For each question on the scale that measures homosexuality there is a corresponding question that measures heterosexuality giving six matching pairs of questions. Taken all together, the six pairs of questions and responses provide a profile of an individual's sexual orientation. However, results can be further simplified into four summaries that look specifically at responses that correspond to either homosexuality, heterosexuality, bisexuality or asexuality.
Of all the questions on the scale, Sell considered those assessing sexual attraction to be the most important as sexual attraction is a better reflection of the concept of sexual orientation which he defined as "extent of sexual attractions toward members of the other, same, both sexes or neither" than either sexual identity or sexual behavior. Identity and behavior are measured as supplemental information because they are both closely tied to sexual attraction and sexual orientation. Major criticisms of the SASO have not been established, but a concern is that the reliability and validity remains largely unexamined.
Difficulties with assessment.
Research focusing on sexual orientation uses scales of assessment to identify who belongs in which sexual population group. It is assumed that these scales will be able to reliably identify and categorize people by their sexual orientation. However, it is difficult to determine an individual's sexual orientation through scales of assessment, due to ambiguity regarding the definition of sexual orientation. Generally, there are three components of sexual orientation used in assessment. Their definitions and examples of how they may be assessed are as follows:
Though sexual attraction, behavior, and identity are all components of sexual orientation, if a person defined by one of these dimensions were congruent with those defined by another dimension it would not matter which was used in assessing orientation, but this is not the case. There is "little coherent relationship between the amount and mix of homosexual and heterosexual behavior in a person's biography and that person's choice to label himself or herself as bisexual, homosexual, or heterosexual". Individuals typically experience diverse attractions and behaviors that may reflect curiosity, experimentation, social pressure and is not necessarily indicative of an underlying sexual orientation. For example, a woman may have fantasies or thoughts about sex with other women but never act on these thoughts and only have sex with opposite gender partners. If sexual orientation was being assessed based on one's sexual attraction then this individual would be considered homosexual, but her behavior indicates heterosexuality.
As there is no research indicating which of the three components is essential in defining sexual orientation, all three are used independently and provide different conclusions regarding sexual orientation. Savin Williams (2006) discusses this issue and notes that by basing findings regarding sexual orientation on a single component, researchers may not actually capture the intended population. For example if homosexual is defined by same sex behavior, gay virgins are omitted, heterosexuals engaging in same sex behavior for other reasons than preferred sexual arousal are miscounted, and those with same sex attraction who only have opposite-sex relations are excluded. Because of the limited populations that each component captures, consumers of research should be cautious in generalizing these findings.
One of the uses for scales that assess sexual orientation is determining what the prevalence of different sexual orientations are within a population. Depending on subject's age, culture and sex, the prevalence rates of homosexuality vary depending on which component of sexual orientation is being assessed: sexual attraction, sexual behavior, or sexual identity. Assessing sexual attraction will yield the greatest prevalence of homosexuality in a population whereby the proportion of individuals indicating they are same sex attracted is two to three times greater than the proportion reporting same sex behavior or identify as gay, lesbian, or bisexual. Furthermore, reports of same sex behavior usually exceed those of gay, lesbian, or bisexual identification. The following chart demonstrates how widely the prevalence of homosexuality can vary depending on what age, location and component of sexual orientation is being assessed:
The variance in prevalence rates is reflected in people's inconsistent responses to the different components of sexual orientation within a study and the instability of their responses over time. Laumann et al., (1994) found that among U.S. adults 20% of those who would be considered homosexual on one component of orientation were homosexual on the other two dimensions and 70% responded in a way that was consistent with homosexuality on only one of the three dimensions. Furthermore, sexuality is fluid such that one's sexual orientation is not necessarily stable or consistent over time but is subject to change throughout life. Diamond (2003) found that over 7 years 2/3 of the women changed their sexual identity at least once, with many reporting that the label was not adequate in capturing the diversity of their sexual or romantic feelings. Furthermore, women who relinquished bisexual and lesbian identification did not relinquish same sex sexuality and acknowledged the possibility for future same sex attractions and/or behaviour. One woman stated "I'm mainly straight but I'm one of those people who, if the right circumstance came along, would change my viewpoint". Therefore, individuals classified as homosexual in one study might not be identified the same way in another depending on which components are assessed and when the assessment is made making it difficult to pin point who is homosexual and who is not and what the overall prevalence within a population may be.
Implications.
Depending on which component of sexual orientation is being assessed and referenced, different conclusions can be drawn about the prevalence rate of homosexuality which has real world consequences. Knowing how much of the population is made up of homosexual individuals influences how this population may be seen or treated by the public and government bodies. For example, if homosexual individuals constitute only 1% of the general population they are politically easier to ignore or than if they are known to be a constituency that surpasses most ethnic and ad minority groups. If the number is relatively minor then it is difficult to argue for community based same sex programs and services, mass media inclusion of gay role models, or Gay/Straight Alliances in schools. For this reason, in the 1970s Bruce Voeller, the chair of the National Gay and Lesbian Task Force perpetuated a common myth that the prevalence of homosexuality is 10% for the whole population by averaging a 13% number for men and a 7% number for women. Voeller generalized this finding and used it as part of the modern gay rights movement to convince politicians and the public that "we [gays and lesbians] are everywhere".
Proposed solutions.
In the paper "Who's Gay? Does It Matter?", Ritch Savin-Williams proposes two different approaches to assessing sexual orientation until well positioned and psychometrically sound and tested definitions are developed that would allow research to reliably identify the prevalence, causes, and consequences of homosexuality. 
He first suggests that greater priority should be given to sexual arousal and attraction over behaviour and identity because it is less prone to self- and other-deception, social conditions and variable meanings. To measure attraction and arousal he proposed that biological measures should be developed and used. There are numerous biological/physiological measures that exist that can measure sexual orientation such as sexual arousal, brain scans, eye tracking, body odour preference, and anatomical variations such as digit-length ratio and right or left handedness. 
Secondly, Savin-Williams suggests that researchers should forsake the general notion of sexual orientation altogether and assess only those components that are relevant for the research question being investigated. For example:
Means of assessment.
Means typically used include surveys, interviews, cross-cultural studies, physical arousal measurements sexual behavior, sexual fantasy, or a pattern of erotic arousal. The most common is verbal self-reporting or self-labeling, which depend on respondents being accurate about themselves.
Sexual arousal.
Studying human sexual arousal has proved a fruitful way of understanding how men and women differ as genders and in terms of sexual orientation. A clinical measurement may use penile or vaginal photoplethysmography, where genital engorgement with blood is measured in response to exposure to different erotic material.
Some researchers who study sexual orientation argue that the concept may "not" apply similarly to men and women. A study of sexual arousal patterns found that women, when viewing erotic films which show female-female, male-male and male-female sexual activity (oral sex or penetration), have patterns of arousal which do "not" match their declared sexual orientations as well as men's. That is, heterosexual and lesbian women's sexual arousal to erotic films do "not" differ significantly by the genders of the participants (male or female) or by the type of sexual activity (heterosexual or homosexual). On the contrary, men's sexual arousal patterns tend to be more in line with their stated orientations, with heterosexual men showing more penis arousal to female-female sexual activity and less arousal to female-male and male-male sexual stimuli, and homosexual and bisexual men being more aroused by films depicting male-male intercourse and less aroused by other stimuli.
Another study on men and women's patterns of sexual arousal confirmed that men and women have different patterns of arousal, independent of their sexual orientations. The study found that women's genitals become aroused to both human and nonhuman stimuli from movies showing humans of both genders having sex (heterosexual and homosexual) and from videos showing non-human primates (bonobos) having sex. Men did "not" show any sexual arousal to non-human visual stimuli, their arousal patterns being in line with their specific sexual interest (women for heterosexual men and men for homosexual men).
These studies suggest that men and women are different in terms of sexual arousal patterns and that this is also reflected in how their genitals react to sexual stimuli of both genders or even to non-human stimuli. Sexual orientation has many dimensions (attractions, behavior, identity), of which sexual arousal is the only product of sexual attractions which can be measured at present with some degree of physical precision. Thus, the fact that women are aroused by seeing non-human primates having sex does not mean that women's sexual orientation includes this type of sexual interest. Some researchers argue that women's sexual orientation depends less on their patterns of sexual arousal than men's and that other components of sexual orientation (like emotional attachment) must be taken into account when describing women's sexual orientations. In contrast, men's sexual orientations tend to be primarily focused on the physical component of attractions and, thus, their sexual feelings are more exclusively oriented according to sex.
More recently, scientists have started to focus on measuring changes in brain activity related to sexual arousal, by using brain-scanning techniques. A study on how heterosexual and homosexual men's brains react to seeing pictures of naked men and women has found that both hetero- and homosexual men react positively to seeing their preferred sex, using the same brain regions. The only significant group difference between these orientations was found in the amygdala, a brain region known to be involved in regulating fear.
Although these findings have contributed to understanding how sexual arousal can differentiate between genders and sexual orientations, it is still a matter of debate whether these results reflect differences which are the result of social learning or genetic or biological factors. Further studies are needed to clarify how much of people's reactions to sexual stimuli of their preferred gender are due to learned or innate factors.
Culture.
Research suggests that sexual orientation is independent of cultural and other social influences, but that open identification of one's sexual orientation may be hindered by homophobic/hetereosexist settings. Social systems such as religion, language and ethnic traditions can have a powerful impact on realization of sexual orientation. Influences of culture may complicate the process of measuring sexual orientation. The majority of empirical and clinical research on LGBT populations are done with largely white, middle-class, well-educated samples, however there are pockets of research that document various other cultural groups, although these are frequently limited in diversity of gender and sexual orientation of the subjects. Integration of sexual orientation with sociocultural identity may be a challenge for LGBT individuals. Individuals may or may not consider their sexual orientation to define their sexual identity, as they may experience various degrees of fluidity of sexuality, or may simply identify more strongly with another aspect of their identity such as family role. American culture puts a great emphasis on individual attributes, and views the self as unchangeable and constant. In contrast, East Asian cultures put a great emphasis on a person's social role within social hierarchies, and view the self as fluid and malleable. These differing cultural perspectives have many implications on cognitions of the self, including perception of sexual orientation.
Language.
One major obstacle when comparing cultures is problems of translation. Many English terms lack equivalents in other languages, while concepts and words from other languages fail to be reflected in the English language. Translation and vocabulary obstacles are not limited to the English language. Language can be limiting in that it forces individuals to identify with a label that may or may not accurately reflect their true sexual orientation. Language can also be used to signal sexual orientation to others. The meaning of words referencing categories of sexual orientation are negotiated in the mass media in relation to social organization. New words may be brought into use to describe new terms or better describe complex interpretations of sexual orientation. Other words may pick up new layers or meaning. For example, the heterosexual Spanish terms "marido" and "mujer" for "husband" and "wife", respectively, have recently been replaced in Spain by the gender-neutral terms "cónyuges" or "consortes" meaning "spouses".
Perceptions.
One person may presume knowledge of another person's sexual orientation based upon perceived characteristics, such as appearance, clothing, tone of voice, and accompaniment by and behavior with other people. The attempt to detect sexual orientation in social situations is known as gaydar; some studies have found that guesses based on face photos perform better than chance.
Perceived sexual orientation may affect how a person is treated. For instance, in the United States, the FBI reported that 15.6% of hate crimes reported to police in 2004 were "because of a sexual-orientation bias". Under the UK Employment Equality (Sexual Orientation) Regulations 2003, as explained by Advisory, Conciliation and Arbitration Service, "workers or job applicants must not be treated less favourably because of their sexual orientation, their perceived sexual orientation or because they associate with someone of a particular sexual orientation".
In Euro-American cultures, sexual orientation is defined by the gender(s) of the people a person is romantically or sexually attracted to. Euro-American culture generally assumes heterosexuality, unless otherwise specified. Cultural norms, values, traditions and laws facilitate heterosexuality, including constructs of marriage and family. Efforts are being made to change these attitudes, and legislation is being passed to promote equality.
Some other cultures do not recognize a homosexual/heterosexual/bisexual distinction. It is common to distinguish a person's sexuality according to their sexual role (active/passive; insertive/penetrated). In this distinction, the passive role is typically associated with femininity and/or inferiority, while the active role is typically associated with masculinity and/or superiority. For example, an investigation of a small Brazilian fishing village revealed three sexual categories for men: men who have sex only with men (consistently in a passive role), men who have sex only with women, and men who have sex with women and men (consistently in an active role). While men who consistently occupied the passive role were recognized as a distinct group by locals, men who have sex with only women, and men who have sex with women and men, were not differentiated. Little is known about same-sex attracted females, or sexual behavior between females in these cultures.
Racism and ethnically relevant support.
In the United States, non-Caucasian LGBT individuals may find themselves in a double minority, where they are neither fully accepted or understood by mainly Caucasian LGBT communities, nor are they accepted by their own ethnic group. Many people experience racism in the dominant LGBT community where racial stereotypes merge with gender stereotypes, such that Asian-American LGBTs are viewed as more passive and feminine, while African-American LGBTs are viewed as more masculine and aggressive. There are a number of culturally specific support networks for LGBT individuals active in the United States. For example, "Ô-Môi" for Vietnamese American queer females.
Religion.
Sexuality in the context of religion is often a controversial subject, especially that of sexual orientation. In the past, various sects have viewed homosexuality from a negative point of view and had punishments for same-sex relationships. In modern times, an increasing number of religions and religious denominations accept homosexuality. It is possible to integrate sexual identity and religious identity, depending on the interpretation of religious texts.
Internet and media.
The internet has influenced sexual orientation in two ways: it is a common mode of discourse on the subject of sexual orientation and sexual identity, and therefore shapes popular conceptions; and it allows anonymous attainment of sexual partners, as well as facilitates communication and connection between greater numbers of people.
Demographics.
The multiple aspects of sexual orientation and the boundary-drawing problems already described create methodological challenges for the study of the demographics of sexual orientation. Determining the frequency of various sexual orientations in real-world populations is difficult and controversial.
Most modern scientific surveys find that the majority of people report a mostly heterosexual orientation. However, the relative percentage of the population that reports a homosexual orientation varies with differing methodologies and selection criteria. Most of these statistical findings are in the range of 2.8 to 9% of males, and 1 to 5% of females for the United States – this figure can be as high as 12% for some large cities and as low as 1% for rural areas.
Estimates for the percentage of the population that are bisexual vary widely, at least in part due to differing definitions of bisexuality. Some studies only consider a person bisexual if they are nearly equally attracted to both sexes, and others consider a person bisexual if they are "at all" attracted to the same sex (for otherwise mostly heterosexual persons) or to the opposite sex (for otherwise mostly homosexual persons). A small percentage of people are not sexually attracted to anyone (asexuality). A study in 2004 placed the prevalence of asexuality at 1%.
Kinsey data.
In the oft-cited and oft-criticized "Sexual Behavior in the Human Male" (1948) and "Sexual Behavior in the Human Female" (1953), by Alfred C. Kinsey et al., people were asked to rate themselves on a scale from completely heterosexual to completely homosexual. Kinsey reported that when the individuals' behavior as well as their identity are analyzed, most people appeared to be at least somewhat bisexual — i.e., most people have some attraction to either sex, although usually one sex is preferred. According to Kinsey, only a minority (5–10%) can be considered fully heterosexual or homosexual. Conversely, only an even smaller minority can be considered fully bisexual (with an equal attraction to both sexes). Kinsey's methods have been criticized as flawed, particularly with regard to the randomness of his sample population, which included prison inmates, male prostitutes and those who willingly participated in discussion of previously taboo sexual topics. Nevertheless, Paul Gebhard, subsequent director of the Kinsey Institute for Sex Research, reexamined the data in the Kinsey Reports and concluded that removing the prison inmates and prostitutes barely affected the results.
Social constructionism and Western societies.
Because sexual orientation is complex and multi-dimensional, some academics and researchers, especially in queer studies, have argued that it is a historical and social construction. In 1976, philosopher and historian Michel Foucault argued that homosexuality as an identity did not exist in the eighteenth century; that people instead spoke of "sodomy," which referred to sexual acts. Sodomy was a crime that was often ignored, but sometimes punished severely (see sodomy law). He wrote, "'Sexuality' is an invention of the modern state, the industrial revolution, and capitalism."
Sexual orientation is argued as a concept that evolved in the industrialized West, and there is a controversy as to the universality of its application in other societies or cultures. Non-westernized concepts of male sexuality differ essentially from the way sexuality is seen and classified under the Western system of sexual orientation. The validity of the notion of sexual orientation as defined in the West, as a biological phenomenon rather than a social construction specific to a region and period, has also been questioned within the industrialized Western society).
Heterosexuality and homosexuality are terms often used in European and American cultures to encompass a person's entire social identity, which includes self and personality. In Western cultures, some people speak meaningfully of gay, lesbian, and bisexual identities and communities. In other cultures, homosexuality and heterosexual labels do not emphasize an entire social identity or indicate community affiliation based on sexual orientation.
Some historians and researchers argue that the emotional and affectionate activities associated with sexual-orientation terms such as "gay" and "heterosexual" change significantly over time and across cultural boundaries. For example, in many English-speaking nations, it is assumed that same-sex kissing, particularly between men, is a sign of homosexuality, whereas various types of same-sex kissing are common expressions of friendship in other nations. Also, many modern and historic cultures have formal ceremonies expressing long-term commitment between same-sex friends, even though homosexuality itself is taboo within the cultures.
Politics and theology.
Two researchers, raising (1995) 'serious doubt whether sexual orientation is a valid concept at all,' warned against increasing politicization of this area.
Professor Michael King stated, "The conclusion reached by scientists who have investigated the origins and stability of sexual orientation is that it is a human characteristic that is formed early in life, and is resistant to change. Scientific evidence on the origins of homosexuality is considered relevant to theological and social debate because it undermines suggestions that sexual orientation is a choice."

</doc>
<doc id="29253" url="http://en.wikipedia.org/wiki?curid=29253" title="Spandrel">
Spandrel

A spandrel, less often spandril or splaundrel, is the space between two arches or between an arch and a rectangular enclosure. 
There are four or five accepted and cognate meanings of "spandrel" in architectural and art history, mostly relating to the space between a curved figure and a rectangular boundary - such as the space between the curve of an arch and a rectilinear bounding moulding, or the wallspace bounded by adjacent arches in an arcade and the stringcourse or moulding above them, or the space between the central medallion of a carpet and its rectangular corners, or the space between the circular face of a clock and the corners of the square revealed by its hood. Also included is the space under a flight of stairs, if it is not occupied by another flight of stairs. This is a common location to find storage space in residential structures.
In a building with more than one floor, the term spandrel is also used to indicate the space between the top of the window in one story and the sill of the window in the story above. The term is typically employed when there is a sculpted panel or other decorative element in this space, or when the space between the windows is filled with opaque or translucent glass, in this case called "spandrel glass". In concrete or steel construction, an exterior beam extending from column to column usually carrying an exterior wall load is known as a spandrel beam.
The spandrels over doorways in Perpendicular work are generally richly decorated. At Magdalen College, Oxford is one which is perforated. The spandrel of doors is sometimes ornamented in the Decorated period, but seldom forms part of the composition of the doorway itself, being generally over the label.
Bridges.
Arches are commonly used in bridge construction and so spandrels may also appear in those structures. Historically, most arch spans had solid spandrels, meaning that the areas between arches were completely filled in — usually with masonry — until the advent of steel and reinforced concrete in the 19th and 20th centuries. 
Where a river is prone to repeated flooding, the increased pressure of flowing water against the spandrel may cause the bridge to be washed away. Some bridges thus had deliberate openings, usually tubular, in their spandrels to allow floodwater to pass through.
Open-spandrel bridges later became fairly common, where thin ribs were used to connect the upper deck to the bridge arches, resulting in significant savings in material and weight, and therefore in cost. The Roman Trajan's Bridge across the Danube is one of the oldest examples. Reinforced-concrete open-spandrel bridges were fairly common for crossing large distances in the 1920s and 1930s.
Domes.
Spandrels can also occur in the construction of domes and are typical in grand architecture from the medieval period onwards. Where a dome needed to rest on a square or rectangular base, the dome was raised above the level of the supporting pillars, with three-dimensional spandrels called pendentives taking the weight of the dome and concentrating it onto the pillars.
References.
 

</doc>
<doc id="29257" url="http://en.wikipedia.org/wiki?curid=29257" title="SimpleText">
SimpleText

SimpleText is the native text editor for the Apple Macintosh OS in versions before OS X. SimpleText allows editing including text formatting (underline, italic, bold, etc.), fonts, and sizes. It was developed to integrate the features included in the different versions of TeachText that were created by various software development groups within Apple. 
It can be considered similar to Windows' WordPad application. In later versions it also gained additional read only display capabilities for PICT files, as well as other Mac OS built-in formats like Quickdraw GX and QTIF, 3DMF and even QuickTime movies. SimpleText can even record short sound samples and, using Apple's PlainTalk speech system, read out text in English. Users who wanted to add sounds longer than 24 seconds, however, needed to use a separate program to create the sound and then paste the desired sound into the document using ResEdit.
SimpleText evolved from TeachText, which was derived from the Edit (application), a simple text editor distributed with the earliest Macintosh operating systems to demonstrate the use of the Macintosh interface and the TextEdit application programming interface. The need for SimpleText arose after Apple stopped bundling MacWrite, to ensure that every user could open and read Readme documents.
The key improvement between SimpleText and TeachText was the addition of text styling. SimpleText could support multiple fonts and font sizes, while TeachText supported only a single font per document. Adding text styling features made SimpleText WorldScript-savvy, meaning that it can use Simplified and Traditional Chinese characters. Like TeachText, SimpleText was also limited to only 32 kB of text in a document, although images could increase the total file size beyond this limit. SimpleText style information was stored in the file's resource fork in such a way that if the resource fork was stripped (such as by uploading to a non-Macintosh server), the text information would be retained.
In Mac OS X, SimpleText is replaced by the more powerful TextEdit application, which reads and writes more document formats as well as including word processor-like features such as a ruler and spell checking. TextEdit's styled text format is RTF, which is able to survive a single-forked file system intact.
Apple has released the source code for a Carbon version of SimpleText in the Panther (10.3) Developer Tools. If the Developer Tools are installed, it can be found at /Developer/Examples/Carbon/SimpleText.

</doc>
<doc id="29263" url="http://en.wikipedia.org/wiki?curid=29263" title="Statute of Westminster 1931">
Statute of Westminster 1931

The Statute of Westminster, 1931 is an Act of the Parliament of the United Kingdom and separate versions of it are now domestic law within some of the other Commonwealth realms, to the extent that they have not been implicitly repealed by subsequent laws. Passed on 11 December 1931, the act, either immediately or upon ratification, effectively both established the legislative independence of the self-governing Dominions of the British Empire from the United Kingdom and bound them all to seek each other's approval for changes to monarchical titles and the common line of succession. It thus became a statutory embodiment of the principles of equality and common allegiance to the Crown set out in the Balfour Declaration of 1926. It thus had the effect of making the Dominions sovereign nations.
The Statute of Westminster's relevance today is that it sets the basis for the continuing relationship between the Commonwealth realms and the Crown.
Application.
The Statute of Westminster gave effect to certain political resolutions passed by the Imperial Conferences of 1926 and 1930; in particular, the Balfour Declaration of 1926. The main effect was the removal of the ability of the British parliament to legislate for the Dominions, part of which also required the repeal of the Colonial Laws Validity Act 1865 in its application to the Dominions. King George V expressed his want for the laws of succession to be exempt from the statute's provisions, but, it was determined that would be contrary to the principles of equality set out in the Balfour Declaration. Both Canada and the Irish Free State pushed for the ability to amend the succession laws themselves and section 2(2) (allowing a Dominion to amend or repeal laws of paramount force, such as the succession laws, insofar as they are part of the law of that Dominion) was included in the Statute of Westminster at Canada's insistence. After the statute was passed, the British parliament could no longer make laws for the Dominions, other than with the request and consent of the government of that Dominion. Before then, the Dominions had legally been self-governing colonies of the United Kingdom. However, the statute had the effect of making them sovereign nations once they adopted it.
The Statute of Westminster provides that:
No Act of Parliament of the United Kingdom passed after the commencement of this Act shall extend or be deemed to extend, to a Dominion as part of the law of that Dominion, unless it is expressly declared in that Act that that Dominion has requested, and consented to, the enactment thereof. 
It also states:
No law and no provision of any law made after the commencement of this Act by the Parliament of a Dominion shall be void or inoperative on the ground that it is repugnant to the Law of England, or to the provisions of any existing or future Act of Parliament of the United Kingdom, or to any order, rule, or regulation made under any such Act, and the powers of the Parliament of a Dominion shall include the power to repeal or amend any such Act, order, rule, or regulation insofar as the same is part of the law of the Dominion.
The statute applied to Canada, the Irish Free State, and the Union of South Africa without the need for any acts of ratification; the governments of those countries gave their consent to the application of the law to their respective jurisdiction. Section 10 required the parliaments of the other three Dominions—Australia, New Zealand, and Newfoundland—to adopt the statute before it would apply to them as part of their domestic laws.
Since 1931, over a dozen new Commonwealth realms have been created, all of which now hold the same powers as the United Kingdom, Canada, Australia, and New Zealand over matters of change to the monarchy. Ireland and South Africa are now republics and Newfoundland is part of Canada.
Australia.
The Parliament of Australia passed the Statute of Westminster Adoption Act in 1942. To clarify its war powers, this adoption was backdated to 3 September 1939, the beginning of the Second World War. Sections eight and nine preserved the provisions of the Australian constitution and of the limitations on the powers of the Australian government.
However, section nine of the Statute of Westminster allowed the Colonial Laws Validity Act 1865 to have continued application in the six Australian states and the Australian Capital Territory; this allowed the British parliament to continue to pass legislation concerning the states and territory, although "in accordance with the [existing] constitutional practice". This lasted until the Australia Act 1986 came into effect, though, in practice, those powers were never exercised. For example, in a referendum on secession in Western Australia in April 1933, 68% of voters favoured leaving the Commonwealth of Australia and becoming a separate Dominion of the British Empire. The state government sent a delegation to Westminster to request that this result be enacted into law, but the British government refused to intervene on the grounds that this was a matter for the Commonwealth of Australia to be concerned with. As a result of this decision in London, no action was taken in Canberra or Perth.
Canada.
Despite the fact that the Statute of Westminster applied to Canada without any need for ratification in its parliament, the British North America Acts—the written elements (in 1931) of the Canadian constitution—were excluded from the application of the statute. This was the result of disagreements between the Canadian provinces and the federal government over how the British North America Acts could be amended, otherwise. These disagreements were resolved only in time for the passage of the Canada Act 1982, thus completing the patriation of the Canadian constitution to Canada.
As a consequence of the Statute's adoption, the Parliament of Canada gained the ability to abolish appeals to the Judicial Committee of the Privy Council. Criminal appeals were abolished in 1933, while civil appeals continued until 1949. As such abolition did not affect active appeals, the last Privy Council ruling did not take place until 1959, in "Ponoka-Calmar Oils v Wakefield". The last Privy Council ruling of constitutional significance occurred in 1954, in "Winner v. S.M.T. (Eastern) Limited". Otherwise, the Supreme Court of Canada effectively became the final court of appeal.
Irish Free State.
The Irish Free State never formally adopted the Statute of Westminster, its Executive Council taking the view that the Anglo-Irish Treaty of 1921 had already ended Westminster's right to legislate for the Free State. The Free State's constitution gave the Oireachtas "sole and exclusive power of making laws". Hence, even before 1931, the Free State did not arrest British Army and Royal Air Force deserters on its territory, even though the UK believed post-1922 British laws gave the Free State's Garda Síochána the power to do so. The UK's Irish Free State Constitution Act 1922 said, however, "[n]othing in the [Free State] Constitution shall be construed as prejudicing the power of [the British] Parliament to make laws affecting the Irish Free State in any case where, in accordance with constitutional practice, Parliament would make laws affecting other self-governing Dominions".
Motions of approval of the Report of the Commonwealth Conference had been passed by the Dáil and Seanad in May 1931 and the final form of the Statute of Westminster included the Irish Free State among the Dominions the British parliament could not legislate for without the Dominion's request and consent. Originally, the UK government had wanted to exclude from the Statute of Westminster the legislation underpinning the 1921 treaty, from which the Free State's constitution had emerged. President W. T. Cosgrave objected, although he promised the Executive Council would not amend the legislation unilaterally. The other Dominions backed Cosgrave and, when an amendment to similar effect was proposed at Westminster by John Gretton, parliament duly voted it down. When the statute became law in the UK, Patrick McGilligan, the Free State Minister for External Affairs, stated: "It is a solemn declaration by the British people through their representatives in Parliament that the powers inherent in the Treaty position are what we have proclaimed them to be for the last ten years." He went on to present the statute as largely the fruit of the Free State's efforts to secure for the other Dominions the same benefits it already enjoyed under the treaty.
After Éamon de Valera led Fianna Fáil to victory in the Free State election of 1932, he began removing the monarchical elements of the constitution, beginning with the Oath of Allegiance. De Valera initially considered invoking the Statute of Westminster in making these changes, but John J. Hearne advised him not to. Abolishing the Oath of Allegiance in effect abrogated the 1921 treaty. Generally, the British thought that this was morally objectionable but legally permitted by the Statute of Westminster. Robert Lyon Moore, a southern unionist from County Donegal, challenged the legality of the abolition in the Free State courts and then appealed to the Judicial Committee of the Privy Council (JCPC) in London. However, the Free State had also abolished the right of appeal to the JCPC. In 1935, the JCPC ruled that both abolitions were valid under the Statute of Westminster.
New Zealand.
The Parliament of New Zealand adopted the Statute of Westminster by passing its Statute of Westminster Adoption Act 1947 in November 1947. The New Zealand Constitution Amendment Act, passed the same year, empowered the New Zealand parliament to change the constitution, but did not remove the ability of the British parliament to legislate regarding the New Zealand constitution. The remaining role of the British parliament was removed by the New Zealand Constitution Act 1986.
Newfoundland.
The Dominion of Newfoundland never adopted the Statute of Westminster, especially because of financial troubles and corruption there. By request of the Dominion's government, the United Kingdom established the Commission of Government in 1934, resuming direct rule of Newfoundland. That arrangement remained until Newfoundland became a province of Canada in 1949.
Union of South Africa.
Although the Union of South Africa was not among the Dominions that needed to adopt the Statute of Westminster for it to take effect, two laws—the Status of the Union Act, 1934, and the Royal Executive Functions and Seals Act of 1934—were passed to confirm South Africa's status as a sovereign state.
Implications for succession to the throne.
The preamble to the Statute of Westminster sets out conventions which affect attempts to change the rules of succession to the Crown. The second paragraph of the preamble to the statute reads:
And whereas it is meet and proper to set out by way of preamble to this Act that, inasmuch as the Crown is the symbol of the free association of the members of the British Commonwealth of Nations, and as they are united by a common allegiance to the Crown, it would be in accord with the established constitutional position of all the members of the Commonwealth in relation to one another that any alteration in the law touching the Succession to the Throne or the Royal Style and Titles shall hereafter require the assent as well of the Parliaments of all the Dominions as of the Parliament of the United Kingdom:
This means, for example, that any change in any realm to the Act of Settlement's provisions barring Roman Catholics from the throne would require the unanimous assent of the parliaments of all the other Commonwealth realms if the unity of the Crown is to be retained. The preamble does not itself contain enforceable provisions, it merely expresses a constitutional convention, albeit one fundamental to the basis of the relationship between the Commonwealth realms. (As sovereign nations, each is free to withdraw from the arrangement, using their respective process for constitutional amendment, and no longer be united through common allegiance to the Crown.) Additionally, per section 4, if a realm wished for a British act amending the Act of Settlement in the UK to become part of that realm's laws, thereby amending the Act of Settlement in that realm, it would have request and consent to the British act and the British act would have to state such request and consent had been given. Section 4 of the Statute of Westminster has been repealed in a number of realms, however, and replaced by other constitutional clauses absolutely disallowing the British parliament from legislating for those realms.
This has raised some logistical concerns, as it would mean multiple parliaments would all have to assent to any future changes in any realm to its line of succession, as with the Perth Agreement's proposals to abolish male-preference primogeniture.
Abdication of King Edward VIII.
During the abdication crisis in 1936, British Prime Minister Stanley Baldwin consulted the Commonwealth prime ministers at the request of King Edward VIII. The King wanted to marry Wallis Simpson, whom Baldwin and other British politicians considered unacceptable as queen, as she was an American divorcée. Baldwin was able to get the then five Dominion prime ministers to agree with this and thus register their official disapproval at the King's planned marriage. The King later requested the Commonwealth prime ministers be consulted on a compromise plan, in which he would wed Simpson under a morganatic marriage pursuant to which she would not become queen. Under Baldwin's pressure, this plan was also rejected by the Dominions. All of these negotiations occurred at a diplomatic level and never went to the Commonwealth parliaments. However, the enabling legislation that allowed for the actual abdication (His Majesty's Declaration of Abdication Act 1936) did require the assent of each Dominion parliament to be passed and the request and consent of the Dominion governments so as to allow it to be part of the law of each Dominion. For expediency and to avoid embarrassment, the British government had suggested the Dominion governments regard whomever is monarch of the UK to automatically be their monarch. However, the Dominions rejected this; Prime Minister of Canada William Lyon Mackenzie King pointed out that the Statute of Westminster required Canada's request and consent to any legislation passed by the British parliament before it could become part of Canada's laws and affect the line of succession in Canada. The text of the British act states that Canada requested and consented (the only Dominion to formally do both) to the act applying in Canada under the Statute of Westminster, while Australia, New Zealand, and the Union of South Africa simply assented.
In February 1937, the South African parliament formally gave its assent by passing His Majesty King Edward the Eighth's Abdication Act, 1937, which declared that Edward had abdicated on 10 December 1936; that he and his descendants, if any, would have no right of succession to the throne; and that the Royal Marriages Act 1772 would not apply to him or his descendants, if any. The move was largely done for symbolic purposes, in an attempt by Prime Minister J. B. M. Hertzog to assert South Africa's independence from Britain. In Canada, the federal parliament passed the Succession to the Throne Act 1937, to assent to His Majesty's Declaration of Abdication Act and ratify the government's request and consent to it. In the Irish Free State, Prime Minister Éamon de Valera used the departure of Edward as an opportunity to remove all explicit mention of the monarch from the constitution of the Irish Free State, through the Constitution (Amendment No. 27) Act 1936, passed on 11 December 1936. The following day, the External Relations Act provided for the king to carry out certain diplomatic functions, if authorised by law. A new Constitution of Ireland, with a president, was approved by Irish voters in 1937, with the Irish Free State becoming simply "Ireland", or, in the Irish language, "Éire". However, the head of state of Ireland remained unclear until 1949, when Ireland unambiguously became a republic outside the Commonwealth of Nations by enacting the Republic of Ireland Act 1948.
Commemoration.
In some countries where the Statute of Westminster forms a part of the constitution, the anniversary of the date of the passage of the original British statute is commemorated as Statute of Westminster Day. In Canada, it is mandated that, on 11 December, the Royal Union Flag (as the Union Jack is called by law in Canada) is to be flown at properties owned by the federal Crown, where the requisite second flag pole is available.

</doc>
<doc id="29265" url="http://en.wikipedia.org/wiki?curid=29265" title="Serbia">
Serbia

Serbia (), officially the Republic of Serbia (Serbian: Republika Srbija, Република Србија, ]), is a sovereign state situated at the crossroads between Central and Southeast Europe, covering the southern part of the Pannonian Plain and the central Balkans. Serbia is landlocked and borders Hungary to the north; Romania and Bulgaria to the east; Macedonia to the south; and Croatia, Bosnia, and Montenegro to the west; it also claims a border with Albania through the disputed territory of Kosovo. The capital of Serbia, Belgrade, is one of the largest cities in Southeast Europe. As of a 2011 census, Serbia (excluding Kosovo) had a total population of 7.2 million.
Following the Slavic migrations to the Balkans from the 6th century onwards, Serbs established several states in the early Middle Ages. The Serbian Kingdom obtained recognition by Rome and Constantinople in 1217; it reached its peak in 1346 as a relatively short-lived Serbian Empire. By the mid-16th century, the entire territory of modern-day Serbia was annexed by the Ottoman Empire, at times interrupted by the Habsburgs. In the early 19th century, the Serbian Revolution established the nation-state as the region's first constitutional monarchy, which subsequently expanded its territory. Following disastrous casualties in World War I, and subsequent unification of Habsburg crownland of Vojvodina with Serbia, the country co-founded Yugoslavia with other South Slavic peoples, which would exist in various formations until the Yugoslav Wars of the 1990s, which had devastating effects for the region. As a result, Serbia formed a union with Montenegro in 1992, which broke apart in 2006, when Serbia again became an independent country. In 2008 the parliament of Kosovo, Serbia's southern province with an Albanian ethnic majority, declared independence, with mixed responses from the international community.
Serbia is a member of the UN, CoE, OSCE, PfP, BSEC, and CEFTA. It is also an official candidate for membership in the European Union, which is negotiating its EU accession, acceding country to the WTO and is a militarily neutral state. Serbia is an upper-middle income economy with dominant service sector, followed by the industrial sector and agriculture. It has a high Human Development Index, ranked 77th in the world in 2014 and a medium-high Global Peace Index, ranked 52nd.
Etymology.
The name "Serbia" was first mentioned as , meaning "land of the Serbs". There are many theories regarding the origin of the name of the Serbs. The most likely is that it is derived from the Old Slavic root "*serb-", meaning "same". Another proposed etymology is that of the Indo-European root "*ser-" "to watch over, protect", akin to Latin "servare" "to keep, guard, protect, preserve, observe".
History.
Early history.
Approximately 8,500 years ago, during the Neolithic Era, Neolithic, Starčevo, and Vinča cultures existed in or near modern-day Belgrade and dominated the Balkans, (as well as parts of Central Europe and Asia Minor). Two important local archeological sites from this era, Lepenski Vir and Vinča-Belo Brdo, still exist near the banks of the Danube. During the Iron Age, Thracians, Dacians, and Illyrians were encountered by the Ancient Greeks during their expansion into the south of modern Serbia in the 4th century BC; the northwesternmost point of Alexander the Great's empire being the town of Kale-Krševica. The Greek influx was followed shortly after by the Celtic tribe of Scordisci, who settled throughout the area in the 3rd century BC. The Scordisci formed their own tribal state in this area, and built several fortifications, including their state capital at Singidunum (present-day Belgrade) and Naissos (present-day Niš).
The Romans conquered much of modern-day Serbia in the 2nd century BC. In 167 BC the Roman province of Illyricum was established; the remainder of central present-day Serbia was conquered around 75 BC, forming the Roman province of Moesia Superior; the modern-day Srem region was conquered in 9 BC; and Bačka and Banat in 106 AD after the Dacian wars. As a result of this, contemporary Serbia extends fully or partially over several former Roman provinces, including Moesia, Pannonia, Praevalitana, Dalmatia, Dacia and Macedonia. The chief towns of Upper Moesia (and wider) were: Singidunum (Belgrade), Viminacium (now Old Kostolac), Remesiana (now Bela Palanka), Naissos (Niš), and Sirmium (now Sremska Mitrovica), the latter of which served as a Roman capital during the Tetrarchy. Seventeen Roman Emperors were born in the area of modern-day Serbia, second only to contemporary Italy. The most famous of these was Constantine the Great, the first Christian Emperor, who issued an edict ordering religious tolerance throughout the Empire. When the Roman Empire was divided in 395, most of Serbia remained under the Eastern Roman Empire, while its western parts were included in the Western Roman Empire.
By the early 6th century, Southern Slavs were present throughout the Byzantine Empire in large numbers.
Medieval Serbia.
The Serbs in the Byzantine world lived in the so-called "Slav lands", lands initially out of Byzantine control and independent. The Vlastimirović dynasty established the Serbian Principality in the 8th century. In 822, the Serbs "inhabited the greater part of Dalmatia", and Christianity was adopted as the state religion in 870. In the mid-10th century the state had emerged into a tribal confederation that stretched to the shores of the Adriatic Sea by the Neretva, the Sava, the Morava, and Skadar. The state disintegrated after the death of the last known Vlastimirid ruler; the Byzantines annexed the region and held it for a century, until 1040 when the Serbs under the leadership of what would become the Vojislavljević dynasty revolted in "Duklja", a maritime region. In 1091, the Vukanović dynasty established the Serbian Grand Principality, based in Raška ("Rascia"). The two-halves were reunited in 1142.
In 1166, Stefan Nemanja assumed the throne, marking the beginning of a prospering Serbia, henceforth under the rule of the Nemanjić dynasty. Nemanja's son Rastko ( "Saint Sava"), gained autocephaly for the Serbian Church in 1217 and authored the oldest known constitution, and at the same time Stefan the First-Crowned established the Serbian Kingdom. Medieval Serbia reached its peak during the reign of Dušan the Mighty, who took advantage of the Byzantine civil war and doubled the size of the state by conquering territories to the south and east at the expense of Byzantium, reaching as far as the Peloponnese, also being crowned Emperor of Serbs and Greeks along the way. The Battle of Kosovo in 1389 marks a turning point and is considered as a beginning of the fall of the medieval Serbian state. The magnate families Lazarević and Branković ruled the suzerain Serbian Despotate afterwards (in the 15th and 16th centuries).
After the fall of Constantinople to the Ottomans in 1453 and the Siege of Belgrade, the Serbian Despotate fell in 1459 following the siege of the provisional capital of Smederevo. The Smederevo Fortress is the largest medieval lowland type of fortresses in Europe. By 1455, central Serbia was completely conquered by the Ottoman Empire. After repelling Ottoman attacks for over 70 years, Belgrade finally fell in 1521, opening the way for Ottoman expansion into Central Europe. Vojvodina, as a part of Habsburg Empire, resisted Ottoman rule until well into the 16th century.
Ottoman and Habsburg rule.
After the loss of independence to the Kingdom of Hungary and the Ottoman Empire, Serbia briefly regained sovereignty under Jovan Nenad in the 16th century. Three Habsburg invasions and numerous rebellions constantly challenged Ottoman rule. One famous incident was the Banat Uprising in 1595, which was part of the Long War between the Ottomans and the Habsburgs. The area of modern Vojvodina endured a century-long Ottoman occupation before being ceded to the Habsburg Empire at the end of the 17th century under the Treaty of Karlowitz.
In all Serb lands south of the rivers Danube and Sava, the nobility was eliminated and the peasantry was enserfed to Ottoman masters, while much of the clergy fled or were confined to the isolated monasteries. Under the Ottoman system, Serbs, as Christians, were considered an inferior class of people and subjected to heavy taxes, and a small portion of the Serbian populace experienced Islamisation. Ottomans abolished the Serbian patriarchate (1459), but reestablished it in 1555, providing for limited continuation of Serbian cultural traditions within the empire.
As the Great Serb Migrations depopulated most of southern Serbia, the Serbs sought refuge across the Danube River in Vojvodina to the north and the Military Frontier in the west, where they were granted rights by the Austrian crown under measures such as the "Statuta Wallachorum" of 1630. The ecclesiastical center of the Serbs also moved northwards, to the Metropolitanate of Sremski Karlovci, as the Patriarchate of Peć was once-again abolished by the Ottomans in 1766. Following several petitions, the Holy Roman Emperor Leopold I formally granted Serbs who wished to leave the right to their autonomous crownland.
In 1717–1739, Austrian Empire also ruled most of Central Serbia as Kingdom of Serbia (1718–1739). This is the period when the most widespread Serbian word (one which has entered most world languages—vampire—was introduced to the West for the first time. Apart from Vojvodina and Northern Belgrade which were absorbed into the Habsburg Empire, Central Serbia was also included into Austrian territory between 1688–1692 and 1788–1793. Southwestern Serbia (Sanjak of Novi Pazar) remained under a "de facto" Austro-Hungarian occupation between 1878–1913, when it was temporarily returned to the Ottomans, and annexed by Kingdom of Serbia the following year.
Revolution and independence.
The Serbian Revolution for independence from the Ottoman Empire lasted eleven years, from 1804 until 1815. The revolution comprised two separate uprisings which gained autonomy from the Ottoman Empire that eventually evolved towards full independence (1835–1867).
During the First Serbian Uprising, led by Duke Karađorđe Petrović, Serbia was independent for almost a decade before the Ottoman army was able to reoccupy the country. Shortly after this, the Second Serbian Uprising began. Led by Miloš Obrenović, it ended in 1815 with a compromise between Serbian revolutionaries and Ottoman authorities. Likewise, Serbia was one of the first nations in the Balkans to abolish feudalism. The Convention of Ackerman in 1826, the Treaty of Adrianople in 1829 and finally, the Hatt-i Sharif, recognized the suzerainty of Serbia. The first Serbian Constitution was adopted on 15 February 1835.
Following the clashes between the Ottoman army and Serbs in Belgrade in 1862, and under pressure from the Great Powers, by 1867 the last Turkish soldiers left the Principality. By enacting a new constitution without consulting the Porte, Serbian diplomats confirmed the "de facto" independence of the country. In 1876, Serbia declared war on the Ottoman Empire, proclaiming its unification with Bosnia. The formal independence of the country was internationally recognized at the Congress of Berlin in 1878, which formally ended the Russo-Turkish War; this treaty, however, prohibited Serbia from uniting with Bosnia by placing the latter under Austro-Hungarian occupation, alongside the occupation of Raška (Sandžak). From 1815 to 1903, the Principality of Serbia was ruled by the House of Obrenović, except from 1842 to 1858, when it was led by Prince Aleksandar Karađorđević. In 1882, Serbia became a Kingdom, ruled by King Milan I. In 1903, following the May Overthrow, the House of Karađorđević, descendants of the revolutionary leader Karađorđe Petrović, assumed power. The 1848 revolution in Austria lead to the establishment of the autonomous territory of Serbian Vojvodina. By 1849, the region was transformed into the Voivodeship of Serbia and Banat of Temeschwar.
Balkan Wars, World War I and the First Yugoslavia.
In the course of the First Balkan War in 1912, the Balkan League defeated the Ottoman Empire and captured its European territories, which enabled territorial expansion into Raška and Kosovo. The Second Balkan War soon ensued when Bulgaria turned on its former allies, but was defeated, resulting in the Treaty of Bucharest. In two years, Serbia enlarged its territory by 80% and its population by 50%; it also suffered high casualties on the eve of World War I, with around 20,000 dead.
The assassination on 28 June 1914 of Archduke Franz Ferdinand of Austria in Sarajevo by Gavrilo Princip, a member of the Young Bosnia organization, led to Austria-Hungary declaring war on Serbia. In defense of its ally Serbia, Russia mobilized its troops, which resulted in Austria-Hungary's ally Germany declaring war on Russia. The retaliation by Austria-Hungary against Serbia activated a series of military alliances that set off a chain reaction of war declarations across the continent, leading to the outbreak of World War I within a month. Serbia won the first major battles of World War I, including the Battle of Cer and Battle of Kolubara – marking the first Allied victories against the Central Powers in World War I. Despite initial success, it was eventually overpowered by the Central Powers in 1915. Most of its army and some people went into exile to Greece and Corfu, where they recovered, regrouped and returned to the Macedonian front to lead a final breakthrough through enemy lines on 15 September 1918, liberating Serbia and defeating the Austro-Hungarian Empire and Bulgaria. Serbia, with its campaign, was a major Balkan Entente Power which contributed significantly to the Allied victory in the Balkans in November 1918, especially by helping France force Bulgaria's capitulation. Serbia was classified as a "minor Entente power". Serbia's casualties accounted for 8% of the "total" Entente military deaths; 58% (243,600) soldiers of the Serbian army perished in the war. The total number of casualties is placed around 700,000, more than 16% of Serbia's prewar size, and a majority (57%) of its overall male population.
As the Austro-Hungarian Empire collapsed, the territory of Syrmia united with Serbia on 24 November 1918, followed by Banat, Bačka and Baranja a day later, thereby bringing the entire Vojvodina into the Serb Kingdom. On 26 November 1918, the Podgorica Assembly deposed the House of Petrović-Njegoš and united Montenegro with Serbia. On 1 December 1918, Serbian Prince Regent Alexander of Serbia proclaimed the Kingdom of the Serbs, Croats, and Slovenes under King Peter I of Serbia.
King Peter was succeeded by his son, Alexander, in August 1921. Serb centralists and Croat autonomists clashed in the parliament, and most governments were fragile and short-lived. Nikola Pašić, a conservative prime minister, headed or dominated most governments until his death. King Alexander changed the name of the country to Yugoslavia and changed the internal divisions from the 33 oblasts to nine new banovinas. The effect of Alexander's dictatorship was to further alienate the non-Serbs from the idea of unity. Alexander was assassinated in Marseille, during an official visit in 1934 by Vlado Chernozemski, member of the IMRO. Alexander was succeeded by his eleven-year-old son Peter II and a regency council was headed by his cousin, Prince Paul. In August 1939 the Cvetković–Maček Agreement established an autonomous Banate of Croatia as a solution to Croatian concerns.
World War II and the Second Yugoslavia.
In 1941, in spite of Yugoslav attempts to remain neutral in the war, the Axis powers invaded Yugoslavia.
The territory of modern Serbia was divided between Hungary, Bulgaria, Independent State of Croatia and Italy (greater Albania and Montenegro), while the remaining part of Serbia was placed under German Military administration, with a Serbian puppet governments led by Milan Aćimović and Milan Nedić. The occupied territory was the scene of a civil war between royalist Chetniks commanded by Draža Mihailović and communist partisans commanded by Josip Broz Tito. Against these forces were arrayed Axis auxiliary units of the Serbian Volunteer Corps and the Serbian State Guard. Draginac and Loznica massacre of 2,950 villagers in Western Serbia in 1941 was the first large execution of civilians in occupied Serbia by Nazis, with Kragujevac massacre and Novi Sad Raid of Jews and Serbs by Hungarian fascists being the most notorious, with over 3,000 victims in each case. After one year of occupation, around 16,000 Serbian Jews were murdered in the area, or around 90% of its pre-war Jewish population. Many concentration camps were established across the area. Banjica concentration camp was the largest concentration camp, with primary victims being Serbian Jews, Roma, and Serb political prisoners.
The Axis puppet state of the Independent State of Croatia committed large-scale persecution and genocide of Serbs, Jews, and Roma. The estimate of the United States Holocaust Memorial Museum indicates that between 320,000 and 340,000 ethnic Serb residents of Croatia, Bosnia and northern Serbia were murdered during the Ustaše genocide campaign; the same figures are supported by the Jewish Virtual Library. Official Yugoslav sources used to estimate more than victims, mostly Serbs. The Jasenovac memorial so far lists 82,085 names killed at the this concentration camp alone, out of around 100,000 estimated victims (75% of whom were of Serbian origin). Out of roughly 1 million casualties in all of Yugoslavia up until 1944, around 250,000 were citizens of Serbia of different ethnicities.
The Republic of Užice was a short-lived liberated territory established by the Partisans and the first liberated territory in World War II Europe, organized as a military mini-state that existed in the autumn of 1941 in the west of occupied Serbia. By late 1944, the Belgrade Offensive swung in favour of the partisans in the civil war; the partisans subsequently gained control of Yugoslavia. Following the Belgrade Offensive, the Syrmian Front was the last major military action of World War II in Serbia.
The victory of the Communist Partisans resulted in the abolition of the monarchy and a subsequent constitutional referendum. A single-party state was soon established in Yugoslavia by the League of Communists of Yugoslavia, between 60,000 and 70,000 people were killed in Serbia during the communist takeover. All opposition was suppressed and people deemed to be promoting opposition to socialism or promoting separatism were imprisoned or executed for sedition. Serbia became a constituent republic within the SFRY known as the Socialist Republic of Serbia, and had a republic-branch of the federal communist party, the League of Communists of Serbia. Serbia's most powerful and influential politician in Tito-era Yugoslavia was Aleksandar Ranković, one of the "big four" Yugoslav leaders, alongside Tito, Edvard Kardelj, and Milovan Đilas. Ranković was later removed from the office because of the disagreements regarding Kosovo's nomenklatura and the unity of Serbia. Ranković's dismissal was highly unpopular amongst Serbs. Pro-decentralization reformers in Yugoslavia succeeded in the late 1960s in attaining substantial decentralization of powers, creating substantial autonomy in Kosovo and Vojvodina, and recognizing a Yugoslav Muslim nationality. As a result of these reforms, there was a massive overhaul of Kosovo's nomenklatura and police, that shifted from being Serb-dominated to ethnic Albanian-dominated through firing Serbs on a large scale. Further concessions were made to the ethnic Albanians of Kosovo in response to unrest, including the creation of the University of Pristina as an Albanian language institution. These changes created widespread fear amongst Serbs of being treated as second-class citizens.
Breakup of Yugoslavia and political transition.
In 1989, Slobodan Milošević rose to power in Serbia. Milošević promised a reduction of powers for the autonomous provinces of Kosovo and Vojvodina, where his allies subsequently took over power, during the Anti-bureaucratic revolution. This ignited tensions with the communist leadership of the other republics, and awoke nationalism across the country, that eventually resulted in the Breakup of Yugoslavia, with Slovenia, Croatia, Bosnia and Herzegovina, Macedonia and Kosovo declaring independence. Serbia and Montenegro remained together as the Federal Republic of Yugoslavia (FRY).
Fueled by ethnic tensions, the Yugoslav Wars erupted, with the most severe conflicts taking place in Croatia and Bosnia, where ethnic Serb populations opposed independence from Yugoslavia. The FRY remained outside the conflicts, but provided logistic, military and financial support to Serb forces in Croatia and Bosnia and Herzegovina. In response, the UN imposed sanctions against the Federal Republic of Yugoslavia in May 1992, which led to political isolation and the collapse of the economy. Multiparty democracy was introduced in Serbia in 1990, officially dismantling the single-party system. Critics of Milošević claimed that the government continued to be authoritarian despite constitutional changes, as Milošević maintained strong political influence over the state media and security apparatus. When the ruling SPS refused to accept its defeat in municipal elections in 1996, Serbians engaged in large protests against the government. Between 1998 and 1999, peace was broken again, when the situation in Kosovo worsened with continued clashes between Yugoslav security forces and the KLA. The confrontations led to the Kosovo War.
In September 2000, opposition parties accused Milošević of electoral fraud. A campaign of civil resistance followed, led by the Democratic Opposition of Serbia (DOS), a broad coalition of anti-Milošević parties. This culminated on 5 October when half a million people from all over the country congregated in Belgrade, compelling Milošević to concede defeat. The fall of Milošević ended Yugoslavia's international isolation. Milošević was sent to the ICTY. The DOS announced that FR Yugoslavia would seek to join the European Union. In 2003, the Federal Republic of Yugoslavia was renamed Serbia and Montenegro; the EU opened negotiations with the country for the Stabilization and Association Agreement. Serbia's political climate has remained tense and in 2003, the prime minister Zoran Đinđić was assassinated as result of a plot originating from circles of organized crime and former security officials.
On 21 May 2006, Montenegro held a referendum to determine whether to end its union with Serbia. The results showed 55.4% of voters in favor of independence, which was just above the 55% required by the referendum. On 5 June 2006, the National Assembly of Serbia declared Serbia to be the legal successor to the former state union.
The province of Kosovo unilaterally declared independence from Serbia on 17 February 2008. Serbia immediately condemned the declaration and continues to deny any statehood to Kosovo. The declaration has sparked varied responses from the international community, some welcoming it, while others condemned the unilateral move. Status neutral talks between Serbia and Kosovo-Albanian authorities are held in Brussels, mediated by the EU.
In April 2008 Serbia was invited to join the Intensified Dialogue programme with NATO despite the diplomatic rift with the alliance over Kosovo. Serbia officially applied for membership in the European Union on 22 December 2009, and received candidate status on 1 March 2012, following a delay in December 2011. Following a positive recommendation of the European Commission and European Council in June 2013, negotiations to join the EU commenced in January 2014.
Geography.
Located at the crossroads between Central and Southern Europe, Serbia is found in the Balkan peninsula and the Pannonian Plain. Serbia lies between latitudes 41° and 47° N, and longitudes 18° and 23° E. The country covers a total of 88,361 km2 (including Kosovo), which places it at 113th place in the world; with Kosovo excluded, the total area is 77,474 km2, which would make it 117th. Its total border length amounts to 2,027 km (Albania 115 km, Bosnia and Herzegovina 302 km, Bulgaria 318 km, Croatia 241 km, Hungary 151 km, Macedonia 221 km, Montenegro 203 km and Romania 476 km). All of Kosovo's border with Albania (115 km), Macedonia (159 km) and Montenegro (79 km) are under control of the Kosovo border police. Serbia treats the 352 km long border between Kosovo and Central Serbia as an "administrative line"; it is under shared control of Kosovo border police and Serbian police forces, and there are 11 crossing points.
The Pannonian Plain covers the northern third of the country (mainly Vojvodina and Mačva) while the easternmost tip of Serbia extends into the Wallachian Plain. The terrain of central part of the country, with the region of Šumadija at its heart, consists chiefly of hills traversed by the rivers. Mountains dominate the southern third of Serbia. Dinaric Alps stretch in the west and the southwest following the flow of the rivers Drina and Ibar. Carpathian Mountains and Balkan Mountains stretch in north–south direction in the eastern Serbia. Ancient mountains in the southeast corner of the country belong to Rilo-Rhodope Mountain system. Elevation ranges from the Midžor peak of the Balkan Mountains at 2,169 m (highest peak in Serbia, excluding Kosovo) to the lowest point of just 17 m near Danube river at Prahovo.
Climate.
The climate of Serbia is under the influences of the landmass of Eurasia and Atlantic Ocean and Mediterranean Sea. With mean January temperatures around 0 C, and mean July temperatures of 22 C, it can be classified into humid subtropical climate.
In the north, the climate is more continental, with cold winters, and hot, humid summers along with well distributed rainfall patterns. In the south, summers and autumns are drier, and winters are relatively cold, with heavy inland snowfall in the mountains. Differences in elevation, proximity to the Adriatic Sea and large river basins, as well as exposure to the winds account for climate variations. Southern Serbia is subject to Mediterranean influences. However, the Dinaric Alps and other mountain ranges contribute to the cooling of most of the warm air masses. Winters are quite harsh in the Pešter plateau, because of the mountains which encircle it. One of the climatic features of Serbia is Košava, a cold and very squally southeastern wind which starts in the Carpathian Mountains and follows the Danube northwest through the Iron Gate where it gains a jet effect and continues to Belgrade and can spread as far south as Niš.
The average annual air temperature for the period 1961–1990 for the area with an altitude of up to 300 m is 10.9 °C. The areas with an altitude of 300 to have an average annual temperature of around 10.0 °C, and over 1000 m of altitude around 6.0 °C. The lowest recorded temperature in Serbia was −39.5 °C on 13 January 1985, Karajukića Bunari in Pešter, and the highest was 44.9 C, on 24 July 2007, recorded in Smederevska Palanka.
Serbia is one of few European countries with "very high risk" exposure to the natural hazards (earthquakes, storms, floods, droughts). It is estimated that potential floods, particularly in areas of Central Serbia, threaten over 500 larger settlements and an area of 16,000 square kilometers. The most disastrous were the floods in May 2014, when 57 people died and a damage of over a 1.5 billion euro was incited.
Hydrology.
Almost all of Serbia's rivers drain to the Black Sea, by way of the Danube river. The Danube, second largest European river, passes through Serbia with 588 kilometers (21% of its overall length) and represents country's largest source of fresh water. It is joined by its biggest tributaries, the Great Morava (longest river entirely in Serbia with 493 km of length), Sava and Tisza rivers. One notable exception is the Pčinja which flows into the Aegean.
Due to configuration of the terrain, natural lakes are sparse and small; most of them are located in the lowlands of Vojvodina, like the glacial lake Palić (covering 6 square kilometers, country's largest natural lake) or numerous oxbow lakes along river flows (like Zasavica and Carska Bara). However, there are numerous artificial lakes, mostly due to hydroelectric dams, the biggest being Đerdap (Iron Gates) on the Danube with 163 square kilometers on the Serbian side (a total area of 253 square kilometers is shared with Romania) as well as the deepest (with maximum depth of 92 meters); Perućac on the Drina, and Vlasina. The largest waterfall, Jelovarnik, located in Kopaonik, is 71 meters high.
Abundance of relatively unpolluted surface waters and numerous underground natural and mineral water sources of high water quality presents a chance for export and economy improvement; however, more extensive exploitation and production of bottled water began only recently.
Environment.
With 29.1% of its territory covered by forest, Serbia is considered to be a middle-forested country. Forest coverage is, when compared on a global scale, similar to world forest coverage which accounts for 30%, but it is somewhat lower than the European average of 35%. The total forest area in Serbia is 2,252,000 hа (1,194,000 hа or 53% are state-owned, and 1,058,387 hа or 47% are privately owned) or 0.3 ha per inhabitant. The most common trees are oak, beech, pines and firs.
Serbia is a country of rich ecosystem and species diversity – covering only 1.9% of the whole European territory Serbia is home to 39% of European vascular flora, 51% of European fish fauna, 40% of European reptile and amphibian fauna, 74% of European bird fauna, 67% European mammal fauna. Its abundance of mountains and rivers make it an ideal environment for a variety of animals, many of which are protected including wolves, lynx, bears, foxes and stags. Mountain of Tara in western Serbia is one of the last regions in Europe where bears can still live in absolute freedom. Serbia is also home to about 380 species of bird, including the imperial eagle, the great bustard, the corn crake and the Madagascar pochard. In Carska Bara, there are over 300 bird species on just a few square kilometers. Uvac Gorge is considered one of the last habitats of white-head vulture in Europe.
There are 377 protected areas of Serbia, encompassing 4,947 square kilometers or 6.4% of the country. The "Spatial plan of the Republic of Serbia" states that the total protected area should be increased to 12% by 2021. Those protected areas include 5 national parks (Đerdap, Tara, Kopaonik, Fruška Gora and Šar Mountain), 15 nature parks, 15 "landscapes of outstanding features", 61 nature reserves, and 281 natural monuments.
Air pollution is a significant problem in Bor area, due to work of large copper mining and smelting complex, and Pančevo where oil and petrochemical industry is based. Some cities suffer from water supply problems, due to mismanagement and low investments in the past, as well as water pollution (like the pollution of the Ibar River from the Trepča zinc-lead combinate, affecting the city of Kraljevo, or the presence of natural arsenic in underground waters in Zrenjanin). Poor waste management has been identified as one of the most important environmental problems in Serbia and the recycling is a fledgling activity, with only 15% of its waste being turned back for reuse. The 1999 NATO bombing caused serious damage to the environment, with several thousand tons of toxic chemicals stored in targeted factories and refineries released into the soil and water basins.
Politics.
Serbia is a parliamentary republic. Government in Serbia is divided into legislative, executive and judiciary branches.
Serbia had one of the first modern constitutions in Europe, the 1835 Constitution (known as "Sretenje Constitution"), which was at the time considered among the most progressive and liberal constitutions in the world. Since then it has adopted 10 different constitutions. The current constitution was adopted on 8 November 2006 in the aftermath of Montenegro independence referendum which by consequence renewed the independence of Serbia itself. The Constitutional Court rules on matters regarding the Constitution.
The President of the Republic ("Predsednik Republike") is the head of state, is elected by popular vote to a five-year term and is limited by the Constitution to a maximum of two terms. In addition to being the commander in chief of the armed forces, the president has the procedural duty of appointing the prime minister with the consent of the parliament, and has some influence on foreign policy. Tomislav Nikolić is the current president following the 2012 presidential election. Seat of the presidency is Novi Dvor.
The Government ("Vlada") is composed of the prime minister and cabinet ministers. The Government is responsible for proposing legislation and a budget, executing the laws, and guiding the foreign and internal policies. The current prime minister is Aleksandar Vučić of the Serbian Progressive Party.
The National Assembly ("Narodna skupština") is a unicameral legislative body. The National Assembly has the power to enact laws, approve the budget, schedule presidential elections, select and dismiss the Prime Minister and other ministers, declare war, and ratify international treaties and agreements. It is composed of 250 proportionally elected members who serve four-year terms. The largest political parties in Serbia are the centre-right Serbian Progressive Party, leftist Socialist Party of Serbia and centre-left Democratic Party.
Serbia has a three-tiered judicial system, made up of the Supreme Court of Cassation as the court of the last resort, Courts of Appeal as the appellate instance, and Basic and High courts as the general jurisdictions at first instance. Courts of special jurisdictions are the Administrative Court, commercial courts (including the Commercial Court of Appeal at second instance) and misdemeanour courts (including High Misdemeanor Court at second instance). The judiciary is overseen by the Ministry of Justice. Serbia has a typical civil law legal system.
Law enforcement is the responsibility of the Serbian Police, which is subordinate to the Ministry of the Interior. Serbian Police fields 26,527 uniformed officers. National security and counterintelligence are the responsibility of the Security Information Agency (BIA).
Foreign relations.
Serbia has established diplomatic relations with 188 UN member states, the Holy See, the Sovereign Military Order of Malta, and the European Union. Foreign relations are conducted through the Ministry of Foreign Affairs. Serbia has a network of 65 embassies and 23 consulates internationally. There are 65 foreign embassies, 5 consulates and 4 liaison offices in Serbia.
Serbian foreign policy is focused on achieving the strategic goal of becoming a member state of the European Union (EU). Serbia started the process of joining the EU by signing of the Stabilisation and Association Agreement on 29 April 2008 and officially applied for membership in the European Union on 22 December 2009. It received a full candidate status on 1 March 2012 and started accession talks on 21 January 2014.
The province of Kosovo unilaterally declared independence from Serbia on 17 February 2008. The declaration of independence has sparked varied responses from the international community, some welcoming it, while others condemn the unilateral move. Serbia has consistently recalled its ambassadors from states which have recognized Kosovo, in protest.
Military.
The Serbian Armed Forces are subordinate to the Ministry of Defence, and are composed of the Army and the Air Force. Although a landlocked country, Serbia operates a river flotilla which patrols on the Danube, Sava, and Tisza rivers. The Serbian Chief of the General Staff reports to the Defence Minister. The Chief of Staff is appointed by the President, who is the Commander-in-chief. As of 2012, Serbia defence budget amounts to $612 million or an estimated 1.6% of the country's GDP.
Traditionally relying on a large number of conscripts, Serbian Armed Forces went through a period of downsizing, restructuring and professionalisation. Conscription was abolished on 1 January 2011. Serbian Armed Forces have 28,000 active troops, supplemented by the "active reserve" which numbers 20,000 members and "passive reserve" with about 170,000.
Serbia participates in the NATO Individual Partnership Action Plan program, but has shown no intention of joining NATO in the near future, due to significant popular rejection, largely derived from the NATO bombing of Yugoslavia in 1999. It is an observer member of the Collective Securities Treaty Organization (CSTO) The country also signed the Stability Pact for South Eastern Europe. The Serbian Armed Forces take part in several multinational peacekeeping missions, including deployments in Lebanon, Cyprus, Ivory Coast, and Liberia.
Serbia is a large producer and exporter of military equipment in the region. Defence exports totaled around $250 million in 2011. Serbia exports across the world, notably to the Middle East, Africa, Southeast Asia, and North America. The defence industry has seen significant growth over the years and it continues to grow on a yearly basis.
Administrative divisions.
Serbia is a unitary state composed of municipalities/cities, districts, and two autonomous provinces.
In Serbia, excluding Kosovo, there are 138 municipalities ("opštine") and 23 cities ("gradovi"), which form the basic units of local self-government. Apart from municipalities, there are 24 districts ("okruzi"), with the City of Belgrade constituting an additional district. Except for Belgrade, which has an elected local government, districts are regional centers of state authority, but have no powers of their own; they present purely administrative divisions.
Serbia has two autonomous provinces ("autonomne pokrajine"): Vojvodina in the north and claims Kosovo and Metohija in the south, while the remaining area, termed Central Serbia, never had its own regional authority. Following the Kosovo War, UN peacekeepers entered Kosovo, as per UNSC Resolution 1244. In 2008, Kosovo declared independence. The government of Serbia did not recognize the declaration, considering it illegal and illegitimate.
Demographics.
As of 2011 census, Serbia (excluding Kosovo) has a total population of 7,186,862 and the overall population density is medium as it stands at 92.8 inhabitants per square kilometer. The census was not conducted in Kosovo which held its own census that numbered their total population at 1,739,825, excluding Serb-inhabited North Kosovo, as Serbs from that area (about 50,000) boycotted the census.
Serbia is in an acute demographic crisis since the beginning of the 1990s, as death rate has continuously exceeded its birth rate. It has one of the most negative population growth rates in the world, ranking 225th out of 233 countries and territories overall. The total fertility rate of 1.44 children per mother, is one of the lowest in the world. Serbia has a comparatively old overall population (among the 10 oldest in the world), with the average age of 42.2 years. The life expectancy in Serbia at birth is 74.2 years. A fifth of all households consist of only one person and just one-fourth of 4 and more persons.
During the 1990s, Serbia used to have the largest refugee population in Europe. Refugees and internally displaced persons (IDPs) in Serbia formed between 7% and 7.5% of its population – about half a million refugees sought refuge in the country following the series of Yugoslav wars, mainly from Croatia (and to a lesser extent from Bosnia and Herzegovina) and the IDPs from Kosovo. Meanwhile, it is estimated that 300,000 people left Serbia during the 1990s, 20% of which had a higher education.
Serbs with 5,988,150 are the largest ethnic group in Serbia, representing 83% of the total population (excluding Kosovo). With a population of 253,899, Hungarians are the largest ethnic minority in Serbia, concentrated predominately in northern Vojvodina and representing 3.5% of the country's population (13% in Vojvodina). Roma population stands at 147,604 according to the 2011 census but unofficial estimates place their actual number between 400,000 and 500,000. Bosniaks with 145,278 are third largest ethnic minority mainly inhabiting Raška region in southwestern part of the country. Other minority groups include Croats, Slovaks, Albanians, Montenegrins, Vlachs, Romanians, Macedonians and Bulgarians. The Chinese, estimated at about 15,000, are the only significant immigrant minority.
The majority of the population, or 59.7%, reside in urban areas and some 16.1% in Belgrade alone. Serbia has only one city with more than a million inhabitants and four with more than 100,000 inhabitants.
Religion.
The Constitution of Serbia defines it as a secular state with guaranteed religious freedom. Orthodox Christians with 6,079,396 comprise 84.5% of country's population. The Serbian Orthodox Church is the largest and traditional church of the country, adherents of which are overwhelmingly Serbs. Other Orthodox Christian communities in Serbia include Montenegrins, Romanians, Vlachs, Macedonians and Bulgarians.
Roman Catholics number 356,957 in Serbia, or roughly 5% of the population, mostly in Vojvodina (especially its northern part) which is home to minority ethnic groups such as Hungarians, Croats, Bunjevci, as well as to some Slovaks and Czechs.
Protestantism accounts for about 1% of the country's populaton, chiefly among Slovaks in Vojvodina as well as among Reformist Hungarians.
Greek Catholic Church is adhered by around 25,000 citizens of Serbia (0.37% of the population), mostly among Rusyns in Vojvodina.
Muslims, with 222,282 or 3% of the population, form the third largest religious group. Islam has a strong historic following in the southern regions of Serbia, primarily in southern Raška. Bosniaks are the largest Islamic community in Serbia; estimates are that some third of country's Roma people are Muslim.
There are only 578 Jews by faith in Serbia. Jews from Spain settled in Serbia after the expulsion from Spain in the late 15th century. The community flourished and reached a peak of 33,000 before World War II (of which almost 90% were living in Belgrade and Vojvodina); however, the wars that later ravaged the region resulted in a great part of the Jewish Serbian population emigrating from the country. Today, the Belgrade Synagogue is the only functioning synagogue, saved by the local population during World War II from destruction at the hands of the Nazis. Other synagogues, such as Subotica Synagogue, which used to be the fourth largest synagogue building in Europe, and Novi Sad Synagogue, have been converted into museums and art spaces.
Atheists numbered 80,053 or 1.1% of population and additional 4,070 declared as agnostics.
Language.
The official language is Serbian, a member of the South Slavic group of languages, which is native to 88% of the population. Serbian is the only European language with active digraphia, using both Cyrillic and Latin alphabets. Serbian Cyrillic was devised in 1814 by Serbian linguist Vuk Karadžić, who created the alphabet on phonemic principles.
Recognized minority languages are: Hungarian, Slovak, Albanian, Romanian, Bulgarian and Rusyn, as well as other standard forms of Serbo-Croatian: Bosnian and Croatian. All these languages are in official use in municipalities or cities where the ethnic minority exceeds 15% of the total population. In Vojvodina, the provincial administration uses, besides Serbian, five other languages (Hungarian, Slovak, Croatian, Romanian and Rusyn).
Economy.
Serbia has an emerging market economy in upper-middle income range. According to the IMF, Serbian nominal GDP in 2014 is officially estimated at $42.648 billion or $5,924 per capita while purchasing power parity GDP was $90.746 billion or $12,605 per capita. The economy is dominated by services which accounts for 60.3% of GDP, followed by industry with 31.8% of GDP, and agriculture at 7.9% of GDP. The official currency of Serbia is Serbian dinar (ISO code: RSD), and the central bank is National Bank of Serbia. The Belgrade Stock Exchange is the only stock exchange in the country, with market capitalization of $8.65 billion (as of August 2014) and BELEX15 as the main index representing the 15 most liquid stocks.
The economy has been affected by the global economic crisis. After eight years of strong economic growth (average of 4.45% per year), Serbia entered the recession in 2009 with negative growth of −3% and again in 2012 with −1.5%. As the government was fighting effects of crisis the public debt has doubled in 4 years: from pre-crisis level of 29.2% to 63.8% of GDP.
Active labor force in 2014 stood at 1.703 million, of whom 59.6% are employed in services sector, 23.9% are employed in the agriculture and 16.5% are employed in industry. The average monthly net salary in June 2014 was 44,883 dinars (US$528,50). The unemployment remains an acute problem, with rate of 20.1% as of 2013.
Since 2000, Serbia has attracted over $25 billion in foreign direct investment (FDI). Blue-chip corporations making investments in Serbia include: FIAT, Siemens, Bosch, Philip Morris, Michelin, Coca-Cola, Carlsberg and others. In the energy sector, Russian energy giants, Gazprom and Lukoil have made large investments.
Serbia has an unfavorable trade balance: imports exceed exports by 28.9%. Serbia's exports, however, recorded a steady growth in last couple of years reaching $14.61 billion in 2013. The country has free trade agreements with the EFTA and CEFTA, a preferential trade regime with the European Union, a Generalized System of Preferences with the United States, and individual free trade agreements with Russia, Belarus, Kazakhstan, and Turkey.
Agriculture.
Serbia has very favourable natural conditions (land and climate) for varied agricultural production. It has 5,056,000 ha of agricultural land (0.7 ha per capita), out of which 3,294,000 ha is arable land (0.45 ha per capita). In 2013, Serbia exported agricultural and food products worth $2.8 billion, and the export-import ratio was 180%. Agricultural exports constitute one-fifth of all Serbia's sales on the world market. Serbia is one of the largest provider of frozen fruit to the EU (largest to the French market, and 2nd largest to the German market). Agricultural production is most prominent in Vojvodina on the fertile Pannonian Plain. Other agricultural regions include Mačva, Pomoravlje, Tamnava, Rasina, and Jablanica.
In the structure of the agricultural production 70% is from the crop field production, and 30% is from the livestock production. Serbia is world's second largest producer of plums (582,485 tons; second to China), second largest of raspberries (89,602 tons, second to Poland), it is also significant producer of maize (6.48 million tons, ranked 32nd in the world) and wheat (2.07 million tons, ranked 35th in the world). Other important agricultural products are: sunflower, sugar beet, soybean, potato, apple, pork meat, beef, poultry and dairy.
There are 56,000 ha of vineyards in Serbia, producing about 230 million litres of wine annually. Most famous viticulture regions are: Vršac area, Župa, Fruška Gora, Topola area.
Industry.
The industry is the economy sector which was hardest hit by the UN sanctions and trade embargo and NATO bombing during the 1990s and transition to market economy during the 2000s. The industrial output saw dramatic downsizing: in 2013 it is expected to be only a half of that of 1989. Main industrial sectors include: automotive, mining, non-ferrous metals, food-processing, electronics, pharmaceuticals, clothes.
Automotive industry (with FIAT as a forebearer) is dominated by cluster located in Kragujevac and its vicinity, and contributes to country's exports with about $2 billion. Serbia's mining industry is comparatively strong: country is world's 18th largest producer of coal (7th in the Europe) extracted from large deposits in Kolubara and Kostolac basins; it is also world's 23rd largest (3rd in Europe) producer of copper which is extracted by RTB Bor, a large domestic copper mining company; significant gold extraction is developed around Majdanpek.
Food industry is well known both regionally and internationally and is one of the strong points of the economy. Some of the international brand-names established production in Serbia: PepsiCo and Nestle in food-processing sector; Coca-Cola (Belgrade), Heineken (Novi Sad) and Carlsberg (Bačka Palanka) in beverage industry; Nordzucker in sugar industry. Clothing and textile industry has seen a surge in recent years with significant greenfeild investments by foreign companies: Benneton in Niš, Geox in Vranje, Calzedonia in Sombor, Falke in Leskovac and others.Serbia's electronics industry had its peak in the 1980s and the industry today is only a third of what it was back then, but has witnessed a something of revival in last decade with investments of companies such as Siemens (wind turbines) in Subotica, Panasonic (lighting devices) in Svilajnac, and Gorenje (electrical home appliances) in Valjevo. The pharmaceutical industry in Serbia comprises 20 manufacturers of generic drugs, of which Hemofarm in Vršac and Galenika in Belgrade, account for 80% of production volume. Domestic production meets over 60% of the local demand.
Energy.
Energy sector is one of the largest and most important sectors to the country's economy. Serbia is net exporter of electricity and importer of key fuels (such as oil and gas).
Serbia has abundance of one natural fuel (coal) and relatively significant but not sufficient of the others (oil and gas). Serbia's proven reserves of 5.5 billion tons of coal lignite are 5th largest in the world (second in Europe, after Germany). Coal is found in two large deposits: Kolubara (4 billion tons of reserves) and Kostolac (1.5 billion tons). Despite being small on a world scale, Serbia's oil and gas resources (77.4 million tons of oil equivalent and 48.1 billion cubic meters, respectively) have a certain regional importance since they are largest in the region of former Yugoslavia as well as the Balkans (excluding Romania). Almost 90% of the discovered oil and gas are to be found in Banat and those oil and gas fields are by size among the largest in the Pannonian basin but the average on a European scale.
The production of electricity in 2012 in Serbia was 36.06 billion kilowatt-hours (KWh), while the final electricity consumption amounted to 35.5 billion kilowatt-hours (KWh). Most of the electricity produced comes from thermal-power plants (72.7% of all electricity) and to a lesser degree from hydroelectric-power plants (27.3%). There are 6 lignite-operated thermal-power plants with an installed power of 3,936 MW; largest of which are 1,502 MW-Nikola Tesla 1 and 1,160 MW-Nikola Tesla 2, both in Obrenovac. Total installed power of 9 hydroelectric-power plants is 2,831 MW, largest of which is Đerdap 1 with capacity of 1,026 MW. In addition to this, there are mazute and gas-operated thermal-power plants with an installed power of 353 MW. The entire production of electricity is concentrated in Elektroprivreda Srbije (EPS), public electric-utility power company.
The current oil production in Serbia amounts to over 1.1 million tons of oil equivalent and satisfies some 43% of country's needs while the rest is imported. National petrol company, Naftna Industrija Srbije (NIS), was acquired in 2008 by Gazprom Neft. The company has completed $700 million modernisation of oil-refinery in Pančevo (capacity of 4.8 million tons) and is currently in the midst of converting oil refinery in Novi Sad into lubricants-only refinery. It also operates network of 334 filling stations in Serbia (74% of domestic market) and additional 36 stations in Bosnia and Herzegovina, 31 in Bulgaria, and 28 in Romania. There are 155 kilometers of crude oil pipelines connecting Pančevo and Novi Sad refineries as a part of trans-national Adria oil pipeline.
Serbia is heavily dependent on foreign sources of natural gas, with only 17% coming from domestic production (totalling 491 million cubic meters in 2012) and the rest is imported, mainly from Russia (via gas pipelines that run through Ukraine and Hungary). Srbijagas, public gas company, operates the natural gas transportation system which comprise 3,177 kilometers of trunk and regional natural gas pipelines and a 450 million cubic meter underground gas storage facility at Banatski Dvor. Major European transit gas pipeline, South Stream pipeline, will pass through Serbia in length of 422 kilometers and will have capacity of 40.5 billion cubic meters. Construction of Serbian portion of the pipeline started in 2013 and is financed with $2.6 billion by the Russian energy giant Gazprom.
Transport.
Serbia has a strategic transportation location since country's backbone, Morava valley, represents by far the easiest route of land travel from continental Europe to Asia Minor and the Near East.
Serbian road network carries the bulk of traffic in the country. Total length of roads is 40,845 km, of which 1,372 km are "class 1a-state roads" (i.e. major national roads, including some 634 km of motorways); 4,153 km are "class 1b-state roads"; 11,540 km are "class 2-state roads" (regional roads) and 23,780 km are "municipal roads" (local roads). The road network, except for the most of class 1a roads, are of comparatively lower quality to the Western European standards because of lack of financial resources for their maintenance in the last 20 years. There are currently 241 kilometers of motorways ("autoputevi") under construction which are all due to be completed by 2016: 66 km-long section of the A1 motorway (from south of Leskovac to Bujanovac), 102 km-long segment of A2 (between Obrenovac and Čačak), and 83 kilometers on the A4 (east of Niš to the Bulgarian border). Work on the construction of the A5 (110 km section between intersection with A1 northeast of Kruševac and Požega on the west) as well as the remaining part of A2 (52 km-long sections Belgrade-Obrenovac and Čačak-Požega) is set to commence in 2014 and be completed by 2016 and 2017, respectively. Coach transport is very extensive: almost every place in the country is connected by bus, from largest cities to the villages; in addition there are international routes (mainly to countries of Western Europe with large Serb diaspora). Routes, both domestic and international, are served by more than 100 bus companies, biggest of which are Lasta and Niš-Ekspres. As of 2011, there are 1,677,510 registered passenger cars or 1 passenger car per 4.3 inhabitants.
Serbia has 3,819 kilometers of rail tracks, of which 1,279 are electrified and 283 kilometers are double-track railroad. The major rail hub is Belgrade (and to a lesser degree Niš), while the most important railroads include: Belgrade-Bar (Montenegro), Belgrade-Šid-Zagreb (Croatia)/Belgrade-Niš-Sofia (Bulgaria) (part of Pan-European Corridor X),
Belgrade-Subotica-Budapest (Hungary) and Niš-Thessaloniki (Greece). Although still a major mode of freight transportation, railroads face increasing problems with the maintenance of the infrastructure and lowering speeds. All rail services are operated by public rail company, Serbian Railways.
There are only two cities in Serbia (excluding Kosovo) served by international airports with regular passenger traffic: Belgrade and Niš. Belgrade Nikola Tesla Airport served 4.638 million passengers in 2014, and is a hub of flagship carrier Air Serbia.
Serbia has a developed inland water transport since there are 1,716 kilometers of navigable inland waterways (1,043 km of navigable rivers and 673 km of navigable canals), which are almost all located in northern third of the country. The most important inland waterway is the Danube (part of Pan-European Corridor VII). Other navigable rivers include Sava, Tisza, Begej and Timiş River, all of which connect Serbia with Northern and Western Europe through the Rhine–Main–Danube Canal and North Sea route, to Eastern Europe via the Tisza, Begej and Danube Black Sea routes, and to Southern Europe via the Sava river. More than 2.1 million tons of cargo were transported on Serbian rivers and canals in 2011 while the largest river ports are: Belgrade, Novi Sad, Pančevo, Smederevo, Prahovo and Šabac.
Telecommunications.
Fixed telephone lines have 89% of households in Serbia, and with about 9.8 million users the number of cellphones surpasses the number of total population of Serbia itself by 35%. The largest cellphone provider is Telekom Srbija with 5.65 million subscribers, followed by Telenor with 3.1 million users and Vip mobile with just over 1 million. Computers have 59.9% of households and 55.8% have internet connection (43.4% have a broadband connection). Some 58% of households have cable TV, which is one of the highest rates in Europe. Digital television transition is set to be completed by the mid-2015.
Tourism.
Serbia is not a mass-tourism destination but nevertheless has diverse range of touristic products. In 2013, total of 2,192,435 tourists were recorded in accommodations, of which 921,768 were foreign, while the average length of a tourist stay was 3.6 days (2.3 days for foreign tourists). Foreign exchange earnings for the same year were estimated at $1.053 billion, while total income from tourism is estimated at $2.5 billion.
Tourism is mainly focused on the mountains and spas of the country, which are mostly visited by domestic tourists, as well as Belgrade which is preferred choice of foreign tourists. The most famous mountain resorts are Kopaonik, Stara Planina, and Zlatibor. There are also many spas in Serbia, the biggest of which is Vrnjačka Banja, Soko Banja, and Banja Koviljača. City-break and conference tourism is developed in Belgrade (which was visited by 517,401 foreign tourists in 2013, more than a half of all international visits to the country) and to a lesser degree Novi Sad. Other touristic products that Serbia offer are natural wonders like Đavolja varoš, Christian pilgrimage to the many Orthodox monasteries across the country and the river cruising along the Danube. There are several internationally popular music festivals held in Serbia, such as EXIT (with 25–30,000 foreign visitors coming from 60 different countries) and the Guča trumpet festival.
Education and science.
According to 2011 census, literacy in Serbia stands at 98% of population while computer literacy is at 49% (complete computer literacy is at 34.2%). Same census showed the following levels of education: 16.2% of inhabitants have higher education (10.6% have bachelors or master's degrees, 5.6% have an associate degree), 49% have a secondary education, 20.7% have an elementary education, and 13.7% have not completed elementary education.
Education in Serbia is regulated by the Ministry of Education and Science. Education starts in either preschools or elementary schools. Children enroll in elementary schools at the age of seven. Compulsory education consists of eight grades of elementary school. Students have the opportunity to attend gymnasiums and vocational schools for another four years, or to enroll in vocational training for 2 to 3 years. Following the completion of gymnasiums or vocational schools, students have the opportunity to attend university. Elementary and secondary education are also available in languages of recognised minorities in Serbia, where classes are held in Hungarian, Slovak, Albanian, Romanian, Rusyn, Bulgarian as well as Bosnian and Croatian languages.
There are 17 universities in Serbia (eight public universities with a total number of 85 faculties and nine private universities with 51 faculties). In 2010/2011 academic year, 181,362 students attended 17 universities (148,248 at public universities and some 33,114 at private universities) while 47,169 attended 81 "higher schools". Public universities in Serbia are: the University of Belgrade (oldest, founded in 1808, and largest university with 89,827 undergraduates and graduates), University of Novi Sad (founded in 1960 and with student body of 47,826), University of Niš (founded in 1965; 27,000 students), University of Kragujevac (founded in 1976; 14,000 students), University of Priština – Kos. Mitrovica, Public University of Novi Pazar as well as two specialist universities – University of Arts and University of Defence. Largest private universities include Megatrend University and Singidunum University, both in Belgrade, and Educons University in Novi Sad. Public universities tend to be of a better quality and therefore more renowned than private ones. Thе University of Belgrade (placed in 301–400 bracket on 2013 Shanghai Ranking of World Universities, being best-placed university in Southeast Europe after those in Athens and Thessaloniki) and University of Novi Sad are generally considered as the best institutions of higher learning in the country.
Serbia spent 0.64% of GDP on scientific research in 2012, which is one of the lowest R&D budgets in Europe. Serbia has a long history of excellence in maths and computer sciences which has created a strong pool of engineering talent, although economic sanctions during the 1990s and chronic underinvestment in research forced many scientific professionals to leave the country. Nevertheless, there are several areas in which Serbia still excels such as growing information technology sector, which includes software development as well as outsourcing. It generated $200 million in exports in 2011, both from international investors and a significant number of dynamic homegrown enterprises. In 2005 the global technology giant, Microsoft, founded the Microsoft Development Center, only its fourth such centre in the world. Among the scientific institutes operating in Serbia, the largest are the Mihajlo Pupin Institute and Vinča Nuclear Institute, both in Belgrade. The Serbian Academy of Sciences and Arts is a learned society promoting science and arts from its inception in 1841. With a strong science and technological ecosystem, Serbia has produced a number of renowned scientists that have greatly contributed to the field of science and technology.
Culture.
For centuries straddling the boundaries between East and West, Serbia had been divided among the Eastern and Western halves of the Roman Empire; then between the Kingdom of Hungary, Frankish Kingdom and Byzantium; and then between the Ottoman Empire and the Habsburg Empire, as well as Venice in the south. These overlapping influences have resulted in cultural varieties throughout Serbia; its north leans to the profile of Central Europe, while the south is characteristic of the wider Balkans and even the Mediterranean. The Byzantine influence on Serbia was profound, firstly through the introduction of Eastern Christianity (Orthodoxy) in the Early Middle Ages. The Serbian Orthodox Church has had an enduring status in Serbia, with the many Serbian monasteries constituting the most valuable cultural monuments left from Serbia in the Middle Ages.
Serbia has four cultural monuments inscribed in the list of UNESCO World Heritage: the early medieval capital Stari Ras and the 13th-century monastery Sopoćani; the 12th-century Studenica monastery; the Roman complex of Gamzigrad–Felix Romuliana; and finally the endangered Medieval Monuments in Kosovo (comprising the monasteries of Visoki Dečani, Our Lady of Ljeviš, Gračanica and Patriarchate of Peć).
There are two literary monuments on UNESCO's Memory of the World Programme: the 12th-century Miroslav Gospel, and scientist Nikola Tesla's valuable archive. The Ministry of Culture and Information is tasked with preserving the nation's cultural heritage and overseeing its development. Further activities supporting development of culture are undertaken at local government level.
Art.
Traces of Roman and early Byzantine Empire architectural heritage are found in many royal cities and palaces in Serbia, like Sirmium, Gamzigrad-Felix Romuliana and Justiniana Prima.
Serbian monasteries, with their fresco and icon paintings, are pinnacle of Serbian medieval art. At the beginning, they were under the influence of Byzantine Art which was particularly felt after the fall of Constantinople in 1204, when many Byzantine artists fled to Serbia. The most noted of these monasteries is Studenica (built around 1190). It was a model for later monasteries, like the Mileševa, Sopoćani, Žiča, Gračanica and Visoki Dečani. The most famous Serbian medieval fresco is the "Mironosnice na Grobu" (or the "White Angel") from the Mileševa monastery. In the end of 14th and the 15th centuries, autochotonous architectural style known as Morava style evolved in area around Morava Valley. A characteristic of this style was the wealthy decoration of the frontal church walls. Examples of this include Manasija, Ravanica and Kalenić monasteries. Country is dotted with many well-preserved medieval fortifications and castles such as Smederevo Fortress (largest lowland fortress in Europe), Golubac, Maglič, and Ram.
During the time of Turkish occupation, Serbian art was virtually non-existent, with the exception of several Serbian artists who lived in the lands ruled by the Habsburg Monarchy. Traditional Serbian art showed some Baroque influences at the end of the 18th century as shown in the works of Nikola Nešković, Teodor Kračun, Zaharije Orfelin and Jakov Orfelin.
Serbian painting showed the influence of Biedermeier, Neoclassicism and Romanticism during the 19th century. The most important Serbian painters of the first half of the 20th century were Paja Jovanović and Uroš Predić of Realism, Cubist Sava Šumanović, Milena Pavlović-Barili and Nadežda Petrović of Impressionism, Expressionist Milan Konjović. Noted painters of the second half of 20th century include Marko Čelebonović, Petar Lubarda, Milo Milunović, and Vladimir Veličković.
There are around 100 art museums in Serbia, of which the most prominent is the National Museum, founded in 1844; it houses one of the largest art collections in the Balkans with more than 400,000 exhibits, over 5,600 paintings and 8,400 drawings and prints, including many foreign masterpiece collections. Other art museums of note are Museum of Contemporary Art in Belgrade and Museum of Vojvodina in Novi Sad.
Literature.
The beginning of Serbian literacy dates back to the activity of the brothers Cyril and Methodius in the Balkans. Monuments of Serbian literacy from the early 11th century can be found, written in Glagolitic. Starting in the 12th century, books were written in Cyrillic. From this epoch, the oldest Serbian Cyrillic book editorial are the Miroslav Gospels. The Miroslav Gospels are considered to be the oldest book of Serbian medieval history.
Notable medieval authors include Saint Sava, Nun Jefimija, Stefan Lazarević, Constantine of Kostenets and others. Baroque trends in Serbian literature emerged in the late 17th century. Notable Baroque-influenced authors were Gavril Stefanović Venclović, Jovan Rajić, Zaharije Orfelin, Andrija Zmajević and others. Dositej Obradović was the most prominent figure of the Age of Enlightenment, while the most notable Classicist writer was Jovan Sterija Popović, although his works also contained elements of Romanticism. In the era of national revival, in the first half of the 19th century, Vuk Stefanović Karadžić collected Serbian folk literature, and reformed the Serbian language and spelling, paving the way for Serbian Romanticism. The first half of the 19th century was dominated by Romanticism, with Branko Radičević, Đura Jakšić, Jovan Jovanović Zmaj and Laza Kostić being the most notable representatives, while the second half of the century was marked by Realist writers such as Milovan Glišić, Laza Lazarević, Simo Matavulj, Stevan Sremac, Vojislav Ilić, Branislav Nušić, Radoje Domanović and Borisav Stanković. The 20th century was dominated by the prose writers Miloš Crnjanski, Isidora Sekulić, Ivo Andrić (who was awarded Nobel Prize in Literature in 1961), Branko Ćopić, Miodrag Bulatović, Meša Selimović, Borislav Pekić, Danilo Kiš, Dobrica Ćosić, Aleksandar Tišma, Dragoslav Mihailović, Milorad Pavić and ohers. There were also many valuable poetic achievements, as seen by the writings of Milan Rakić, Jovan Dučić, Vladislav Petković Dis, Rastko Petrović, Stanislav Vinaver, Dušan Matić, Desanka Maksimović, Branko Miljković, Vasko Popa, Oskar Davičo, Miodrag Pavlović, Stevan Raičković, and others. Most notable contemporary authors include David Albahari, Svetislav Basara, Goran Petrović, Vladimir Arsenijević, Zoran Živković and others.
There are 551 public libraries in Serbia (excluding Kosovo), the biggest of which are two national libraries: National Library of Serbia in Belgrade with funds of about 5 million volumes, and Matica Srpska (oldest Serbian cultural institution, founded in 1826) in Novi Sad with nearly 3.5 million volumes. In 2010, there were 10,989 books and brochures published. The book publishing market is dominated by several major publishers such as Laguna and Vulkan (both of which operate their own bookstore chains) and the industry's centerpiece event, annual Belgrade Book Fair, is the most visited cultural event in Serbia with 158,128 visitors in 2013. The highlight of the literary scene is awarding of NIN Prize, given every January since 1954 for the best newly published novel in Serbian language (during times of Yugoslavia, in Serbo-Croatian language).
Music.
Composer and musicologist Stevan Stojanović Mokranjac is considered founder of modern Serbian music.
The Serbian composers of the first generation Petar Konjović, Stevan Hristić, and Miloje Milojević maintained the national expression and modernized the romanticism into the direction of impressionism. Other famous classical Serbian composers include Isidor Bajić, Stanislav Binički and Josif Marinković. There are three opera houses in Serbia: Opera of the National Theatre and Madlenianum Opera, both in Belgrade, and Opera of the Serbian National Theatre in Novi Sad. Four symphonic orchestra operate in the country: Belgrade Philharmonic Orchestra, Niš Symphony Orchestra, Symphonic Orchestra of Radio Television of Serbia, and Novi Sad Philharmonic Orchestra. The Choir of Radio Television of Serbia is a leading vocal ensemble in the country. The BEMUS is one of the most prominent classical music festivals in the South East Europe.
Traditional Serbian music includes various kinds of bagpipes, flutes, horns, trumpets, lutes, psalteries, drums and cymbals. The "kolo" is the traditional collective folk dance, which has a number of varieties throughout the regions. The most popular are those from Užice and Morava region. Sung epic poetry has been an integral part of Serbian and Balkan music for centuries. In the highlands of Serbia these long poems are typically accompanied on a one-string fiddle called the "gusle", and concern themselves with themes from history and mythology. There are records of "gusle" being played at the court of the 13th-century King Stefan Nemanjić.
The Serbian rock which was during the 1960s, 1970s and 1980s part of former Yugoslav rock scene, used to be well developed, featuring various rock genres, and was well covered in the media, which included numerous magazines, radio and TV shows. During the 1990s and 2000s popularity of rock music declined in Serbia, and although several major mainstream acts managed to sustain their popularity, an underground and independent music scene developed. The most notable Serbian rock acts include Bajaga i Instruktori, Đorđe Balašević, Disciplina Kičme, Električni Orgazam, Galija, Idoli, Korni Grupa, Partibrejkers, Pekinška Patka, Rambo Amadeus, Riblja Čorba, Smak, Šarlo Akrobata, Van Gogh, YU Grupa, and others.
Pop music has mainstream popularity. Željko Joksimović won second place at the 2004 Eurovision Song Contest and Marija Šerifović managed to win the 2007 Eurovision Song Contest with the song "Molitva", and Serbia was the host of the 2008 edition of the contest. Most popular pop singers include likes of Zdravko Čolić, Vlado Georgiev, Nataša Bekvalac among others.
Turbo-folk music is subgenre that has developed in Serbia in the late 1980s and the beginning of the 1990s and has since enjoyed an immense popularity. It is a blend of folk music with pop and/or dance elements and can be seen as a result of the urbanization of folk music. In recent period turbo-folk featured even more pop music elements, and some of the performers were labeled as pop-folk.
Balkan Brass, or "truba" ("trumpet") is a popular genre, especially in Central and Southern Serbia where Balkan Brass originated. The music has its tradition from the First Serbian Uprising. The trumpet was used as a military instrument to wake and gather soldiers and announce battles, the trumpet took on the role of entertainment during downtime, as soldiers used it to transpose popular folk songs. When the war ended and the soldiers returned to the rural life, the music entered civilian life and eventually became a music style, accompanying births, baptisms, weddings, and funerals. There are two main varieties of this genre, one from Western Serbia and the other from Southern Serbia. The best known Serbian Brass musician is Boban Marković, also one of the biggest names in the world of modern brass band bandleaders.
Most popular music festival are Guča Trumpet Festival with over 300,000 annual visitors and EXIT in Novi Sad ("The best European festival" in 2007 by UK Festival Awards and Yourope – the European Association of the 40 largest festivals in Europe) with 200,000 visitors in 2013. Other festivals include Nišville Jazz Festival in Niš and Gitarijada rock festival in Zaječar.
Theatre and cinema.
Serbia has a well-established theatrical tradition with Joakim Vujić considered the founder of modern Serbian theater. Serbia has 38 professional theatres, the most important of which are National Theatre in Belgrade, Serbian National Theatre in Novi Sad, National Theatre in Subotica, National Theatre in Niš and Knjaževsko-srpski teatar in Kragujevac (the oldest theatre in Serbia, established in 1835). The Belgrade International Theatre Festival – BITEF, founded in 1967, is one of the oldest theater festivals in the world, and it has become one of the five biggest European festivals. Sterijino pozorje is, on the other hand, festival showcasing national drama plays. The most important Serbian playwrighters were Jovan Sterija Popović and Branislav Nušić, while today renowned names are Dušan Kovačević and Biljana Srbljanović.
The Serbian cinema is one of the most dynamic smaller European cinematographies. Serbia's film industry is heavily subsidised by the government, mainly through grants approved by the Film Centre of Serbia. In 2011, there were 17 domestic feature films produced. There are 20 operating cinemas in the country, of which 10 are multiplexes, with total attendance exceeding 2.6 million and comparatively high percentage of 32.3% of total sold tickets for domestic films. Modern PFI Studios located in Šimanovci is nowadays Serbia's only film studio complex; it consists of 9 state-of-the-art sound stages and attracts mainly international productions, primarily American and West European. The Yugoslav Film Archive used to be former Yugoslavia's and now is Serbia national film archive – with over 95 thousand film prints, it is among five largest film archives in the world.
Serbian cinema dates back to 1896 with the release of the oldest movie in the Balkans, "The Life and Deeds of the Immortal Vožd Karađorđe", a biography about Serbian revolutionary leader, Karađorđe.
The most famous Serbian filmmaker is Emir Kusturica who won two Golden Palms for Best Feature Film at the Cannes Film Festival, for "When Father Was Away on Business" in 1985 and then again for "Underground" in 1995. Other renowned directors include Goran Paskaljević, Dušan Makavejev, Goran Marković, Srđan Dragojević and Srdan Golubović among others. Steve Tesich, Serbian-American screenwriter, won the Academy Award for Best Original Screenplay in 1979 for the movie Breaking Away.
Media.
The freedom of the press and the freedom of speech are guaranteed by the constitution of Serbia. Serbia is ranked 54th out of 180 countries in the 2014 Press Freedom Index report compiled by Reporters Without Borders. Both reports noted that media outlets and journalists continue to face partisan and government pressure over editorial policies. Also, the media are now more heavily dependent on advertising contracts and government subsidies to survive financially.
According to AGB Nielsen Research in 2009, Serbs on average watch five hours of television per day, making it the highest average in Europe. There are six nationwide free-to-air television channels, with public broadcaster Radio Television of Serbia (RTS) operating two (RTS1 and RTS2) and remaining five are private broadcasters: Prva, B92, Pink and Happy TV. Viewing shares for these channels in 2012 were as follows: 23.5% for RTS1, 19.6% for Pink, 16.1% for Prva, 8.1% for B92, 3.6% for RTS2, and 2.8% for Happy TV. There are 28 regional television stations and 74 local television stations. Besides terrestrial channels there are a dozen Serbian television channels available only on cable or satellite.
There are 220 radio stations in Serbia. Out of these, nine are radio stations with national coverage, including three of public broadcaster Radio Television of Serbia (Radio Belgrade 1, Radio Belgrade 2/Radio Belgrade 3 and Radio Belgrade 202), and five private ones (Radio S, Radio B92, Radio Indeks, Radio Fokus, and Radio Hit FM). Also, there are 49 regional stations and 162 local stations.
There are 340 newspapers published in Serbia. Some 14 daily newspapers are published in the country out of which 10 are nationwide dailies. Dailies "Politika" and "Danas" are Serbia's papers of record, former being the oldest newspaper in the Balkans, founded in 1904. Highest circulation newspapers are tabloids "Večernje Novosti", "Blic", "Kurir", and "Alo!", all with more than 100,000 copies sold. There are two sport newspapers ("Sportski žurnal" and "Sport"), one business daily "Privredni pregled", two regional newspapers ("Dnevnik" published in Novi Sad and "Narodne novine" from Niš), one daily on Hungarian language ("Magyar Szo" published in Subotica), and a free newspaper of "24 sata", distributed only in Belgrade and Novi Sad.
There are 1,262 magazines published in the country. Those include weekly news magazines "NIN" and "Vreme", popular science magazine of "Politikin Zabavnik", women's "Lepota & Zdravlje", auto magazine "SAT revija", IT magazine "Svet kompjutera". In addition, there is a wide selection of Serbian editions of international magazines, such as "Cosmopolitan", "Elle", "Grazia", "Men's Health", "National Geographic", "Le Monde diplomatique", "Playboy", "Hello!" and others.
The state-owned news agency Tanjug, founded in 1943, runs a wire service in Serbian and English on politics, economics, society and culture. It broadcasts around 400 pieces of information and over 100 photographs, video and audio recordings every day. Other news agencies include Beta and Fonet.
As of 2014, the most visited websites in Serbian (mainly on the .rs domain) are the Serbian version of Google followed by online editions of printed daily Blic , news web-portal of B92 broadcaster, news portal of printed daily Kurir and classifieds KupujemProdajem.
Cuisine.
Serbian cuisine is largely heterogeneous, sharing characteristics of the Balkans (especially former Yugoslavia), the Mediterranean (Greek in particular), Turkish, and Central European (especially Austrian and Hungarian) cuisines. Food is very important in Serbian social life, particularly during religious holidays such as Christmas, Easter and feast days i.e. slava.
Staples of the Serbian diet include bread, meat, fruits, vegetables, and dairy products. Bread is the basis of all Serbian meals, and it plays an important role in Serbian cuisine and can be found in religious rituals. A traditional Serbian welcome is to offer bread and salt to guests. Meat is widely consumed, as is fish. Serbian specialties include ćevapčići (caseless sausages made of minced meat, which is always grilled and seasoned), pljeskavica, sarma, kajmak (a dairy product similar to clotted cream), gibanica (cheese and kajmak pie), proja (cornbread), and kačamak (corn-flour porridge).
Serbians claim their country as the birthplace of rakia ("rakija"), a highly alcoholic drink primarily distilled from fruit. Rakia in various forms is found throughout the Balkans, notably in Bulgaria, Croatia, Slovenia, Montenegro, Hungary and Turkey. A famous type of rakia is Slivovitz ("šljivovica"), a plum brandy, which is considered the national drink of Serbia.
Sports.
Sports play an important role in Serbian society, and the country has a strong sporting history. The most popular sports in Serbia are football, basketball, tennis, volleyball, water polo and handball.
Professional sports in Serbia are organized by sporting federations and leagues (in case of team sports). One of particularities of Serbian professional sports is existence of many multi-sports clubs (called "sports societies"), biggest and most successful of which are Red Star, Partizan, and Beograd in Belgrade, Vojvodina in Novi Sad, Radnički in Kragujevac, Spartak in Subotica.
Football is the most popular sport in Serbia, and the Football Association of Serbia with 146,845 registered players, is the largest sporting association in the country. Dragan Džajić was officially recognized as "the best Serbian player of all times" by the Football Association of Serbia, and more recently the likes of Nemanja Vidić, Dejan Stanković and Branislav Ivanović the elite clubs of Europe, developing the nation's reputation as one of the world's biggest exporters of footballers. The Serbia national football team lacks relative success although it qualified for three of the last four FIFA World Cups. The two main football clubs in Serbia are Red Star (winner of the 1991 European Cup) and Partizan (finalist of the 1966 European Cup), both from Belgrade. The rivalry between the two clubs is known as the "Eternal Derby", and is often cited as one of the most exciting sports rivalries in the world.
Serbia is one of the traditional powerhouses of world basketball, as Serbia men's national basketball team have won two World Championships (in 1998 and 2002), three European Championships (1995, 1997, and 2001, respectively) and silver medal at 1996 Olympics as well. A total of 22 Serbian players have played in the NBA in last two decades, including Predrag "Peja" Stojaković (three-time NBA All-Star) and Vlade Divac (2001 NBA All-Star and FIBA Hall of Famer). The renowned "Serbian coaching school" produced many of the most successful European basketball coaches of all times, such as Željko Obradović, who won a record 8 Euroleague titles as a coach. KK Partizan was the 1992 European champion.
Recent success of Serbian tennis players has led to an immense growth in the popularity of tennis in Serbia. Novak Đoković, eight-time Grand Slam champion, finished in 2011, 2012 and 2014 as No. 1 in the world and is also currently No. 1 in the ATP Rankings. Ana Ivanovic (champion of 2008 French Open) and Jelena Janković were both ranked No. 1 in the WTA Rankings. There were two No. 1 ranked-tennis double players as well: Nenad Zimonjić (three-time men's double and four-time mixed double Grand Slam champion) and Slobodan Živojinović. The Serbia men's tennis national team won the 2010 Davis Cup while Serbia women's tennis national team reached the final at 2012 Fed Cup.
Serbia is one of the leading volleyball countries in the world. Its men's national team won the gold medal at 2000 Olympics, and has won the European Championship twice. The women's national volleyball team won the European Championship in 2011.
The Serbia men's national water polo team is the second most successful national team after Hungary, having won two World Championships (2005 and 2009), and five European Championships in 2001, 2003, 2006, 2012, and 2014 respectively. VK Partizan has won a joint-record seven European champion titles.
Other noted Serbian athletes include: swimmers Milorad Čavić (2009 World champion on 50 meters butterfly and silver medalist on 100 meters butterfly as well as 2008 Olympic silver medalist on 100 meters butterfly in historic race with American swimmer Michael Phelps) and Nađa Higl (2009 World champion in 200 meters breaststroke – the first Serbian woman to become a world champion in swimming); track and field athletes Emir Bekrić (hurdler; bronze medalist at the 2013 World Championships) and Ivana Španović (long-jumper; bronze medalist at the 2013 World Championships); shooter Jasna Šekarić (1988 Olympic gold medalist) and taekwondoist Milica Mandić (2012 Olympic gold medalist).
Serbia has hosted several major sport competitions in the last ten years, including the 2005 Men's European Basketball Championship, 2005 Men's European Volleyball Championship, 2006 Men's European Water Polo Championship, 2009 Summer Universiade, 2012 European Men's Handball Championship, and 2013 World Women's Handball Championship. The most important annual sporting events held in the country are Belgrade Marathon and Tour de Serbie cycling race.
Public Holidays.
The public holidays in Serbia are defined by the "Law of national and other holidays in the Republic of Serbia".
1 If any of the non-religious holidays falls on a Sunday, then it extends to the next working day.
References.
Sources:
</dl>

</doc>
<doc id="29266" url="http://en.wikipedia.org/wiki?curid=29266" title="Relationship between religion and science">
Relationship between religion and science

The relationship between religion and science has been a subject of study since Classical antiquity, addressed by philosophers, theologians, scientists, and others. Perspectives from different geographical regions, cultures and historical epochs show significant diversity, with some characterizing the relationship as one of conflict, others describing it as one of harmony, and still others proposing little interaction.
Science and religion generally pursue knowledge of the universe using different methodologies. Science acknowledges reason, empiricism, and evidence, while religions include revelation, faith and sacredness. These methodologies are totally different. They are diametrically opposed. Reason, empiricism, and evidence simply do not recognize revelation, faith, and sacredness as valid sources of knowledge. Further, revelation, faith, and sacredness, which are examples of religious dogma, only accept conflicting scientific opinion when the evidence becomes overwhelmingly accepted by the general public.
Despite these differences, most scientific and technical innovations prior to the scientific revolution were achieved by societies organized by religious traditions. Many features of the scientific method were first pioneered by ancient civilizations such as the Greeks, Egyptians, Indians, and Sumerians. Later during the Middle Ages, the Catholic church was responsible for saving much of the scientific knowledge from these civilizations, thus allowing the scientific method to develop in Europe during and after the Renaissance and through the enlightenment period. Islam also made great contributions to areas such as Mathematics, and Astronomy. Many of the most noted scientists in history, such as Blaise Pascal, Copernicus, and the founder of modern genetics Gregor Mendel, were devout Christians. The Big Bang theory was first proposed by a Jesuit priest named Georges Lemaître. Hinduism has historically embraced reason and empiricism, holding that science brings legitimate, but incomplete knowledge of the world. Confucian thought has held different views of science over time. Most Buddhists today view science as complementary to their beliefs.
Events in Europe such as the Galileo affair, associated with the Scientific revolution and the Age of Enlightenment, led scholars such as John William Draper to postulate a conflict thesis, holding that religion and science conflict methodologically, factually and politically. This thesis is advanced by contemporary scientists such as Richard Dawkins, Steven Weinberg and Carl Sagan, as well as by many creationists. While the conflict thesis remains popular for the public, it has lost favor among most contemporary historians of science.
Many theologians, philosophers and scientists in history have found no conflict between their faith and science. Biologist Stephen Jay Gould, other scientists, and some contemporary theologians hold that religion and science are non-overlapping magisteria, addressing fundamentally separate forms of knowledge and aspects of life. Scientists Francisco Ayala, Kenneth R. Miller, John Polkinghorne, Denis Alexander and Francis Collins see no necessary conflict between religion and science. Some theologians or historians of science, including John Lennox, Thomas Berry, Brian Swimme and Ken Wilber propose an interconnection between them.
Public acceptance of scientific facts may be influenced by religion; many in the United States reject the idea of evolution by natural selection, especially regarding human beings. Nevertheless, the American National Academy of Sciences has written that "the evidence for evolution can be fully compatible with religious faith," a view officially endorsed by many religious denominations globally.
Perspectives.
The kinds of interactions that might arise between science and religion have been categorized, according to theologian, Anglican priest and physicist John Polkinghorne are: (1) conflict between the disciplines, (2) independence of the disciplines, (3) dialogue between the disciplines where they overlap and (4) integration of both into one field.
This typology is similar to ones used by theologians Ian Barbour and John Haught. More typologies that categorize this relationship can be found among the works of other science and religion scholars such as theologian and biochemist Arthur Peacocke.
Incompatibility.
According to Guillermo Paz-y-Miño-C and Avelina Espinosa, the historical conflicts between science and religion are intrinsic to the incompatibility between scientific rationalism/empiricism and the belief in supernatural causation; these authors have formally proposed the incompatibility hypothesis (IH) to explain the "everlasting-conflict-science-and-faith". According to Jerry Coyne, views on evolution and levels of religiosity in some countries, along with the existence of books explaining reconciliation between evolution and religion, indicate that people have trouble in believing both at the same time, thus implying incompatibility.
According to Lawrence Krauss, compatibility or incompatibility is a theological concern, not a scientific concern. In Lisa Randall's view, questions of incompatibility or otherwise are not answerable since by accepting revelations one is abandoning rules of logic which are needed to identify if there are indeed contradictions between holding certain beliefs. Daniel Dennett holds that incompatibility exists because religion is not problematic to a certain point before it collapses into a number of excuses for keeping certain beliefs, in light of evolutionary implications.
According to Neil deGrasse Tyson, the central difference between the nature of science and religion is that the claims of science rely on experimental verification, while the claims of religions rely on faith, and these are irreconcilable approaches to knowing. Because of this both are incompatible as currently practiced and the debate of compatibility or incompatibility will be eternal. Philosopher and physicist Victor J. Stenger's view is that science and religion are incompatible due to conflicts between approaches of knowing and the availability of alternative plausible natural explanations for phenomena that is usually explained in religious contexts. Neuroscientist and author Sam Harris views science and religion as being in competition, with religion now "losing the argument with modernity". However, Harris disagrees with Jerry Coyne and Daniel Dennett's narrow view of the debate and argues that it is very easy for people to reconcile science and religion because some things are above strict reason, scientific expertise or domains do not spill over to religious expertise or domains necessarily, and mentions "There simply IS no conflict between religion and science."
According to Richard Dawkins, he is hostile to fundamentalist religion because it actively debauches the scientific enterprise. According to Dawkins, religion "subverts science and saps the intellect". He believes that when science teachers attempt to expound on evolution, there is hostility aimed towards them by parents who are skeptical because they believe it conflicts with their religious beliefs, that even some textbooks have had the word 'evolution' systematically removed.
Others such as Francis Collins, Kenneth R. Miller, George Coyne and Francisco J. Ayala argue for compatibility since they do not agree that science is incompatible with religion and vice versa. They argue that science provides many opportunities to look for and find God in nature and to reflect on their beliefs. According to Kenneth Miller, he disagrees with Jerry Coyne's assessment and argues that since significant portions of scientists are religious and the proportion of Americans believing in evolution is much higher, it implies that both are indeed compatible. Karl Giberson argues that when discussing compatibility, some scientific intellectuals often ignore the viewpoints of intellectual leaders in theology and instead argue against less informed masses, thereby, defining religion by non intellectuals and slanting the debate unjustly. He argues that leaders in science sometimes trump older scientific baggage and that leaders in theology do the same, so once theological intellectuals are taken into account, people who represent extreme positions like Ken Ham and Eugene Scott will become irrelevant.
Conflict thesis.
The conflict thesis, which holds that religion and science have been in conflict continuously throughout history, was popularized in the 19th century by John William Draper's and Andrew Dickson White's accounts. It was in the 19th century that relationship between science and religion became an actual formal topic of discourse, while before this no one had pitted science against religion or vice versa, though occasional complex interactions had been expressed before the 19th century. Most contemporary historians of science now reject the conflict thesis in its original form and no longer support it. Instead, it has been superseded by subsequent historical research which has resulted in a more nuanced understanding: Historian of science, Gary Ferngren, has stated "Although popular images of controversy continue to exemplify the supposed hostility of Christianity to new scientific theories, studies have shown that Christianity has often nurtured and encouraged scientific endeavour, while at other times the two have co-existed without either tension or attempts at harmonization. If Galileo and the Scopes trial come to mind as examples of conflict, they were the exceptions rather than the rule."
Most historians today have moved away from a conflict model, which is based mainly on two historical episodes (Galileo and Darwin) for a "complexity" model, because religious figures were on both sides of each dispute and there was no overall aim by any party involved to discredit religion.
An often cited example of conflict was the Galileo affair, whereby interpretations of the Bible were used to attack ideas by Copernicus on Heliocentrism. By 1616 Galileo went to Rome to try to persuade Catholic Church authorities not to ban Copernicus' ideas. In the end, a decree of the Congregation of the Index was issued, declaring that the ideas that the Sun stood still and that the Earth moved were "false" and "altogether contrary to Holy Scripture", and suspending Copernicus's De Revolutionibus until it could be corrected. Galileo was found "vehemently suspect of heresy", namely of having held the opinions that the Sun lies motionless at the center of the universe, that the Earth is not at its centre and moves. He was required to "abjure, curse and detest" those opinions. However, before all this, Pope Urban VIII had personally asked Galileo to give arguments for and against heliocentrism in a book, and to be careful not to advocate heliocentrism as physically proven yet. Pope Urban VIII asked that his own views on the matter be included in Galileo's book. Only the latter was fulfilled by Galileo. Whether unknowingly or deliberately, Simplicio, the defender of the Aristotelian/Ptolemaic geocentric view in "Dialogue Concerning the Two Chief World Systems", was often portrayed as an unlearned fool who lacked mathematical training. Although the preface of his book claims that the character is named after a famous Aristotelian philosopher (Simplicius in Latin, Simplicio in Italian), the name "Simplicio" in Italian also has the connotation of "simpleton". Unfortunately for his relationship with the Pope, Galileo put the words of Urban VIII into the mouth of Simplicio. Most historians agree Galileo did not act out of malice and felt blindsided by the reaction to his book. However, the Pope did not take the suspected public ridicule lightly, nor the physical Copernican advocacy. Galileo had alienated one of his biggest and most powerful supporters, the Pope, and was called to Rome to defend his writings.
Independence.
In the view of physicist and Hindu monk Mauricio Garrido non-Euclidean geometry proved that Euclidean axioms, such as "there is only one straight line between two points", which were considered self-evident, absolute truths until the 19th century, are in fact interchangeable with different axioms. Therefore, claims by any ideology to exclusive truth, proved by reason or by any other method are obviously wrong.
A modern view, described by Stephen Jay Gould as "non-overlapping magisteria" (NOMA), is that science and religion deal with fundamentally separate aspects of human experience and so, when each stays within its own domain, they co-exist peacefully. While Gould spoke of independence from the perspective of science, W. T. Stace viewed independence from the perspective of the philosophy of religion. Stace felt that science and religion, when each is viewed in its own domain, are both consistent and complete.
The USA's National Academy of Science supports the view that science and religion are independent.
Science and religion are based on different aspects of human experience. In science, explanations must be based on evidence drawn from examining the natural world. Scientifically based observations or experiments that conflict with an explanation eventually must lead to modification or even abandonment of that explanation. Religious faith, in contrast, does not depend on empirical evidence, is not necessarily modified in the face of conflicting evidence, and typically involves supernatural forces or entities. Because they are not a part of nature, supernatural entities cannot be investigated by science. In this sense, science and religion are separate and address aspects of human understanding in different ways. Attempts to put science and religion against each other create controversy where none needs to exist.
According to Archbishop John Habgood, both science and religion represent distinct ways of approaching experience and these differences are sources of debate. He views science as descriptive and religion as prescriptive. He stated that if science and mathematics concentrate on what the world "ought to be", in the way that religion does, it may lead to improperly ascribing properties to the natural world as happened among the followers of Pythagoras in the sixth century B.C. In contrast, proponents of a normative moral science take issue with the idea that science has "no" way of guiding "oughts". Habgood also stated that he believed that the reverse situation, where religion attempts to be descriptive, can also lead to inappropriately assigning properties to the natural world. A notable example is the now defunct belief in the Ptolemic (heliocentric) planetary model that held sway until changes in scientific and religious thinking were brought about by Galileo and proponents of his views.
Parallels in method.
According to Ian Barbour, Thomas S. Kuhn asserted that science is made up of paradigms that arise from cultural traditions, which is similar to the secular perspective on religion.
Michael Polanyi asserted that it is merely a commitment to universality that protects against subjectivity and has nothing at all to do with personal detachment as found in many conceptions of the scientific method. Polanyi further asserted that all knowledge is personal and therefore the scientist must be performing a very personal if not necessarily subjective role when doing science. Polanyi added that the scientist often merely follows intuitions of "intellectual beauty, symmetry, and 'empirical agreement'". Polanyi held that science requires moral commitments similar to those found in religion.
Two physicists, Charles A. Coulson and Harold K. Schilling, both claimed that "the methods of science and religion have much in common." Schilling asserted that both fields—science and religion—have "a threefold structure—of experience, theoretical interpretation, and practical application." Coulson asserted that science, like religion, "advances by creative imagination" and not by "mere collecting of facts," while stating that religion should and does "involve critical reflection on experience not unlike that which goes on in science." Religious language and scientific language also show parallels (cf. rhetoric of science).
Dialogue.
The "religion and science community" consists of those scholars who involve themselves with what has been called the "religion-and-science dialogue" or the "religion-and-science field." The community belongs to neither the scientific nor the religious community, but is said to be a third overlapping community of interested and involved scientists, priests, clergymen, theologians, and engaged non-professionals. Institutions interested in the intersection between science and religion include the Center for Theology and the Natural Sciences, the Institute on Religion in an Age of Science, the Ian Ramsey Centre, and the Faraday Institute. Journals addressing the relationship between science and religion include Theology and Science and . Eugenie Scott has written that the "science and religion" movement is, overall, composed mainly of theists who have a healthy respect for science and may be beneficial to the public understanding of science. She contends that the "Christian scholarship" movement is not a problem for science, but that the "Theistic science" movement, which proposes abandoning methodological materialism, does cause problems in understanding of the nature of science.
The modern dialogue between religion and science is rooted in Ian Barbour's 1966 book "Issues in Science and Religion". Since that time it has grown into a serious academic field, with academic chairs in the subject area, and two dedicated academic journals, "" and Theology and Science. Articles are also sometimes found in mainstream science journals such as American Journal of Physics
and Science.
Philosopher Alvin Plantinga has argued that there is superficial conflict but deep concord between science and religion, and that there is deep conflict between science and naturalism. Plantinga, in his book "Where the Conflict Really Lies: Science, Religion, and Naturalism", heavily contests the linkage of naturalism with science, as conceived by Richard Dawkins, Daniel Dennett and like-minded thinkers; while Daniel Dennett thinks that Plantinga stretches science to an unacceptable extent. Philosopher Maarten Boudry, in reviewing the book, has commented that he resorts to creationism and fails to "stave off the conflict between theism and evolution." Cognitive scientist Justin L. Barrett, by contrast, reviews the same book and writes that "those most needing to hear Plantinga's message may fail to give it a fair hearing for rhetorical rather than analytical reasons."
Cooperative.
As a general view, this holds that while interactions are complex between influences of science, theology, politics, social, and economic concerns, the productive engagements between science and religion throughout history should be duly stressed as the norm.
Scientific and theological perspectives often coexist peacefully. Christians and some non-Christian religions have historically integrated well with scientific ideas, as in the ancient Egyptian technological mastery applied to monotheistic ends, the flourishing of logic and mathematics under Hinduism and Buddhism, and the scientific advances made by Muslim scholars during the Ottoman empire. Even many 19th-century Christian communities welcomed scientists who claimed that science was not at all concerned with discovering the ultimate nature of reality. According to Lawrence M. Principe, the Johns Hopkins University Drew Professor of the Humanities, from a historical perspective this points out that much of the current-day clashes occur between limited extremists—both religious and scientistic fundamentalists—over a very few topics, and that the movement of ideas back and forth between scientific and theological thought has been more usual. To Principe, this perspective would point to the fundamentally common respect for written learning in religious traditions of rabbinical literature, Christian theology, and the Islamic Golden Age, including a Transmission of the Classics from Greek to Islamic to Christian traditions which helped spark the Renaissance. Religions have also given key participation in development of modern universities and libraries; centers of learning & scholarship were coincident with religious institutions - whether pagan, Muslim, or Christian.
Bahá'í.
A fundamental principle of the Bahá'í Faith is the harmony of religion and science. Bahá'í scripture asserts that true science and true religion can never be in conflict. `Abdu'l-Bahá, the son of the founder of the religion, stated that religion without science is superstition and that science without religion is materialism. He also admonished that true religion must conform to the conclusions of science.
Buddhism.
Buddhism and science have been regarded as compatible by numerous authors. Some philosophic and psychological teachings found in Buddhism share points in common with modern Western scientific and philosophic thought. For example, Buddhism encourages the impartial investigation of nature (an activity referred to as "Dhamma-Vicaya" in the Pali Canon)—the principal object of study being oneself. Buddhism and science both show a strong emphasis on causality. However, Buddhism doesn't focus on materialism.
Tenzin Gyatso, the 14th Dalai Lama, maintains that empirical scientific evidence supersedes the traditional teachings of Buddhism when the two are in conflict. In his book "The Universe in a Single Atom" he wrote, "My confidence in venturing into science lies in my basic belief that as in science, so in Buddhism, understanding the nature of reality is pursued by means of critical investigation." and "If scientific analysis were conclusively to demonstrate certain claims in Buddhism to be false," he says, "then we must accept the findings of science and abandon those claims."
Christianity.
Most sources of knowledge available to early Christians were connected to pagan world-views. There were various opinions on how Christianity should regard pagan learning, which included its ideas about nature. For instance, among early Christian teachers, Tertullian (c. 160–220) held a generally negative opinion of Greek philosophy, while Origen (c. 185–254) regarded it much more favorably and required his students to read nearly every work available to them.
Earlier attempts at reconciliation of Christianity with Newtonian mechanics appear quite different from later attempts at reconciliation with the newer scientific ideas of evolution or relativity. Many early interpretations of evolution polarized themselves around a "struggle for existence." These ideas were significantly countered by later findings of universal patterns of biological cooperation. According to John Habgood, all man really knows here is that the universe seems to be a mix of good and evil, beauty and pain, and that suffering may somehow be part of the process of creation. Habgood holds that Christians should not be surprised that suffering may be used creatively by God, given their faith in the symbol of the Cross. 
Robert John Russell has examined consonance and dissonance between modern physics, evolutionary biology, and Christian theology.
Christian philosophers Augustine of Hippo (354-430) and Thomas Aquinas held that scriptures can have multiple interpretations on certain areas where the matters were far beyond their reach, therefore one should leave room for future findings to shed light on the meanings. The "Handmaiden" tradition, which saw secular studies of the universe as a very important and helpful part of arriving at a better understanding of scripture, was adopted throughout Christian history from early on. Also the sense that God created the world as a self operating system is what motivated many Christians throughout the Middle Ages to investigate nature.
Modern historians of science such as J.L. Heilbron, Alistair Cameron Crombie, David Lindberg, Edward Grant, Thomas Goldstein, and Ted Davis have reviewed the popular notion that medieval Christianity was a negative influence in the development of civilization and science. In their views, not only did the monks save and cultivate the remnants of ancient civilization during the barbarian invasions, but the medieval church promoted learning and science through its sponsorship of many universities which, under its leadership, grew rapidly in Europe in the 11th and 12th centuries, St. Thomas Aquinas, the Church's "model theologian", not only argued that reason is in harmony with faith, he even recognized that reason can contribute to understanding revelation, and so encouraged intellectual development. He was not unlike other medieval theologians who sought out reason in the effort to defend his faith. Some of today's scholars, such as Stanley Jaki, have claimed that Christianity with its particular worldview, was a crucial factor for the emergence of modern science.
David C. Lindberg states that the widespread popular belief that the Middle Ages was a time of ignorance and superstition due to the Christian church is a "caricature". According to Lindberg, while there are some portions of the classical tradition which suggest this view, these were exceptional cases. It was common to tolerate and encourage critical thinking about the nature of the world. The relation between Christianity and science is complex and cannot be simplified to either harmony or conflict, according to Lindberg. Lindberg reports that "the late medieval scholar rarely experienced the coercive power of the church and would have regarded himself as free (particularly in the natural sciences) to follow reason and observation wherever they led. There was no warfare between science and the church." Ted Peters in "Encyclopedia of Religion" writes that although there is some truth in the "Galileo's condemnation" story but through exaggerations, it has now become "a modern myth perpetuated by those wishing to see warfare between science and religion who were allegedly persecuted by an atavistic and dogma-bound ecclesiastical authority". In 1992, the Catholic Church's seeming vindication of Galileo attracted much comment in the media.
A degree of concord between science and religion can be seen in religious belief and empirical science. The belief that God created the world and therefore humans, can lead to the view that he arranged for humans to know the world. This is underwritten by the doctrine of imago dei. In the words of Thomas Aquinas, "Since human beings are said to be in the image of God in virtue of their having a nature that includes an intellect, such a nature is most in the image of God in virtue of being most able to imitate God".
During the Enlightenment, a period "characterized by dramatic revolutions in science" and the rise of Protestant challenges to the authority of the Catholic Church via individual liberty, the authority of Christian scriptures became strongly challenged. As science advanced, acceptance of a literal version of the Bible became "increasingly untenable" and some in that period presented ways of interpreting scripture according to its spirit on its authority and truth.
Individual scientists' beliefs.
Many well-known historical figures who influenced Western science considered themselves Christian such as Copernicus, Galileo, Kepler, Newton and Boyle.
Isaac Newton, for example, believed that gravity caused the planets to revolve about the Sun, and credited God with the design. In the concluding General Scholium to the Philosophiae Naturalis Principia Mathematica, he wrote: "This most beautiful System of the Sun, Planets and Comets, could only proceed from the counsel and dominion of an intelligent and powerful being." Other famous founders of science who adhered to Christian beliefs include Galileo, Johannes Kepler, and Blaise Pascal.
According to "100 Years of Nobel Prizes" a review of Nobel prizes award between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference.
Perspectives on evolution.
In recent history, the theory of evolution has been at the center of some controversy between Christianity and science. Christians who accept a literal interpretation of the biblical account of creation find incompatibility between Darwinian evolution and their interpretation of the Christian faith. Creation science or scientific creationism is a branch of creationism that attempts to provide scientific support for the Genesis creation narrative in the Book of Genesis and attempts to disprove generally accepted scientific facts, theories and scientific paradigms about the history of the Earth, cosmology and biological evolution. It began in the 1960s as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and falsify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. In 1925, The State of Tennessee passed the Butler Act, which prohibited the teaching of the theory of evolution in all schools in the state. Later that year, a similar law was passed in Mississippi, and likewise, Arkansas in 1927. In 1968, these "anti-monkey" laws were struck down by the Supreme Court of the United States as unconstitutional, "because they established a religious doctrine violating both the First and Fourth Amendments to the Constitution.
Most scientists have rejected creation science for several reasons, including that its claims do not refer to natural causes and cannot be tested. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms.
Theistic evolution attempts to reconcile Christian beliefs and science by accepting the scientific understanding of the age of the Earth and the process of evolution. It includes a range of beliefs, including views described as evolutionary creationism, which accepts some findings of modern science but also upholds classical religious teachings about God and creation in Christian context.
Reconciliation in Britain in the early 20th century.
In "Reconciling Science and Religion: The Debate in Early-twentieth-century Britain", historian of biology Peter J. Bowler argues that in contrast to the conflicts between science and religion in the U.S. in the 1920s (most famously the Scopes Trial), during this period Great Britain experienced a concerted effort at reconciliation, championed by intellectually conservative scientists, supported by liberal theologians but opposed by younger scientists and secularists and conservative Christians. These attempts at reconciliation fell apart in the 1930s due to increased social tensions, moves towards neo-orthodox theology and the acceptance of the modern evolutionary synthesis.
In the 20th century, several ecumenical organizations promoting a harmony between science and Christianity were founded, most notably the American Scientific Affiliation, The Biologos Foundation, Christians in Science, The Society of Ordained Scientists, and The Veritas Forum.
Roman Catholicism.
While refined and clarified over the centuries, the Roman Catholic position on the relationship between science and religion is one of harmony, and has maintained the teaching of natural law as set forth by Thomas Aquinas. For example, regarding scientific study such as that of evolution, the church's unofficial position is an example of theistic evolution, stating that faith and scientific findings regarding human evolution are not in conflict, though humans are regarded as a special creation, and that the existence of God is required to explain both monogenism and the spiritual component of human origins. Catholic schools have included all manners of scientific study in their curriculum for many centuries.
Galileo once stated "The intention of the Holy Spirit is to teach us how to go to heaven, not how the heavens go." In 1981 John Paul II, then pope of the Roman Catholic Church, spoke of the relationship this way: "The Bible itself speaks to us of the origin of the universe and its make-up, not in order to provide us with a scientific treatise, but in order to state the correct relationships of man with God and with the universe. Sacred Scripture wishes simply to declare that the world was created by God, and in order to teach this truth it expresses itself in the terms of the cosmology in use at the time of the writer".
Influence of a biblical world view on early modern science.
According to Andrew Dickson White's A History of the Warfare of Science with Theology in Christendom from the 19th century, a biblical world view affected negatively the progress of science through time. Dickinson also argues that immediately following the Reformation matters were even worse. The interpretations of Scripture by Luther and Calvin became as sacred to their followers as the Scripture itself. For instance, when Georg Calixtus ventured, in interpreting the Psalms, to question the accepted belief that "the waters above the heavens" were contained in a vast receptacle upheld by a solid vault, he was bitterly denounced as heretical. Today, much of the scholarship in which the conflict thesis was originally based is considered to be inaccurate. For instance, the claim that early Christians rejected scientific findings by the Greco-Romans is false, since the "handmaiden" view of secular studies was seen to shed light on theology. This view was widely adapted throughout the early medieval period and afterwards by theologians (such as Augustine) and ultimately resulted in fostering interest in knowledge about nature through time. Also, the claim that people of the Middle Ages widely believed that the Earth was flat was first propagated in the same period that originated the conflict thesis and is still very common in popular culture. Modern scholars regard this claim as mistaken, as the contemporary historians of science David C. Lindberg and Ronald L. Numbers write: "there was scarcely a Christian scholar of the Middle Ages who did not acknowledge [earth's] sphericity and even know its approximate circumference." From the fall of Rome to the time of Columbus, all major scholars and many vernacular writers interested in the physical shape of the earth held a spherical view with the exception of Lactantius and Cosmas.
H. Floris Cohen argued for a biblical Protestant, but not excluding Catholicism, influence on the early development of modern science. He presented Dutch historian R. Hooykaas' argument that a biblical world-view holds all the necessary antidotes for the hubris of Greek rationalism: a respect for manual labour, leading to more experimentation and empiricism, and a supreme God that left nature and open to emulation and manipulation. It supports the idea early modern science rose due to a combination of Greek and biblical thought.
Oxford historian Peter Harrison is another who has argued that a biblical worldview was significant for the development of modern science. Harrison contends that Protestant approaches to the book of scripture had significant, if largely unintended, consequences for the interpretation of the book of nature. Harrison has also suggested that literal readings of the Genesis narratives of the Creation and Fall motivated and legitimated scientific activity in seventeenth-century England. For many of its seventeenth-century practitioners, science was imagined to be a means of restoring a human dominion over nature that had been lost as a consequence of the Fall.
Historian and professor of religion Eugene M. Klaaren holds that "a belief in divine creation" was central to an emergence of science in seventeenth-century England. The philosopher Michael Foster has published analytical philosophy connecting Christian doctrines of creation with empiricism. Historian William B. Ashworth has argued against the historical notion of distinctive mind-sets and the idea of Catholic and Protestant sciences. Historians James R. Jacob and Margaret C. Jacob have argued for a linkage between seventeenth century Anglican intellectual transformations and influential English scientists (e.g., Robert Boyle and Isaac Newton). John Dillenberger and Christopher B. Kaiser have written theological surveys, which also cover additional interactions occurring in the 18th, 19th, and 20th centuries. Philosopher of Religion, Richard Jones, has written a philosophical critique of the "dependency thesis" which assumes that modern science emerged from Christian sources and doctrines. Though he acknowledges that modern science emerged in a religious framework, that Christinaity greatly elevated the importance of science by sanctioning and religiously legitimizing it in medieval period, and that Christianity created a favorable social context for it to grow; he argues that direct Christian beliefs or doctrines were not primary source of scientific pursuits by natural philosophers, nor was Christianity, in and of itself, exclusively or directly necessary in developing or practicing modern science.
Oxford University historian and theologian John Hedley Brooke wrote that "when natural philosophers referred to "laws" of nature, they were not glibly choosing that metaphor. Laws were the result of legislation by an intelligent deity. Thus the philosopher René Descartes (1596-1650) insisted that he was discovering the "laws that God has put into nature." Later Newton would declare that the regulation of the solar system presupposed the "counsel and dominion of an intelligent and powerful Being." Historian Ronald L. Numbers stated that this thesis "received a boost" from mathematician and philosopher Alfred North Whitehead's "Science and the Modern World" (1925). Numbers has also argued, "Despite the manifest shortcomings of the claim that Christianity gave birth to science—most glaringly, it ignores or minimizes the contributions of ancient Greeks and medieval Muslims—it too, refuses to succumb to the death it deserves." The sociologist Rodney Stark of Baylor University, argued in contrast that "Christian theology was essential for the rise of science."
Confucianism and traditional Chinese religion.
The historical process of Confucianism has largely been antipathic towards scientific discovery. However the religio-philosophical system itself is more neutral on the subject than such an analysis might suggest. In his writings On Heaven, Xunzi espoused a proto-scientific world view. However during the Han Synthesis the more anti-empirical Mencius was favored and combined with Daoist skepticism regarding the nature of reality. Likewise, during the Medieval period, Zhu Xi argued against technical investigation and specialization proposed by Chen Liang. After contact with the West, scholars such as Wang Fuzhi would rely on Buddhist/Daoist skepticism to denounce all science as a subjective pursuit limited by humanity's fundamental ignorance of the true nature of the world. After the May Fourth Movement, attempts to modernize Confucianism and reconcile it with scientific understanding were attempted by many scholars including Feng Youlan and Xiong Shili. Given the close relationship that Confucianism shares with Buddhism, many of the same arguments used to reconcile Buddhism with science also readily translate to Confucianism. However, modern scholars have also attempted to define the relationship between science and Confucianism on Confucianism's own terms and the results have usually led to the conclusion that Confucianism and science are fundamentally compatible.
Hinduism.
In Hinduism, the dividing line between objective sciences and spiritual knowledge ("adhyatma vidya") is a linguistic paradox. Hindu scholastic activities and ancient Indian scientific advancements were so interconnected that many Hindu scriptures are also ancient scientific manuals and vice versa. In 1835, English was made the primary language for teaching in higher education in India, exposing Hindu scholars to Western secular ideas; thus starting a renaissance regarding religious and philosophical thought. Hindu sages maintained that logical argument and rational proof using Nyaya is the way to obtain correct knowledge. From a Hindu perspective, modern science is a legitimate, but incomplete, step towards knowing and understanding reality. Hinduism views that science only offers a limited view of reality, but all it offers is right and correct. To clarify, the scientific level of understanding focuses on how things work and from where they originate, while Hinduism strives to understand the ultimate purposes for the existence of living things. To obtain and broaden the knowledge of the world for spiritual perfection, many refer to the Bhāgavata for guidance because it draws upon a scientific and theological dialogue. Hinduism offers methods to correct and transform itself in course of time. For instance, Hindu views on the development of life include a range of viewpoints in regards to evolution, creationism, and the origin of life within the traditions of Hinduism. For instance, it has been suggested that Wallace-Darwininan evolutionary thought was a part of Hindu thought centuries before modern times. The Shankara and the Sāmkhya did not have a problem with the theory of evolution, but instead, argued about the existence of God and what happened after death. These two distinct groups argued among each other's philosophies because of their sacred texts, not the idea of evolution. With the publication of Darwin's "On the Origin of Species", many Hindus were eager to connect their scriptures to Darwinism, finding similarities between Brahma's creation, Vishnu's incarnations, and evolution theories.
Samkhya, the oldest school of Hindu philosophy prescribes a particular method to analyze knowledge. According to Samkhya, all knowledge is possible through three means of valid knowledge –
Nyaya, the Hindu school of logic, accepts all these 3 means and in addition accepts one more - "Upamāna" (comparison).
The accounts of the emergence of life within the universe vary in description, but classically the deity called Brahma, from a Trimurti of three deities also including Vishnu and Shiva, is described as performing the act of 'creation', or more specifically of 'propagating life within the universe' with the other two deities being responsible for 'preservation' and 'destruction' (of the universe) respectively. In this respect some Hindu schools do not treat the scriptural creation myth literally and often the creation stories themselves do not go into specific detail, thus leaving open the possibility of incorporating at least some theories in support of evolution. Some Hindus find support for, or foreshadowing of evolutionary ideas in scriptures, namely the Vedas.
The incarnations of Vishnu (Dashavatara) is almost identical to the scientific explanation of the sequence of biological evolution of man and animals. The sequence of avatars starts from an aquatic organism (Matsya), to an amphibian (Kurma), to a land-animal (Varaha), to a humanoid (Narasimha), to a dwarf human (Vamana), to 5 forms of well developed human beings (Parashurama, Rama, Balarama/Buddha, Krishna, Kalki) who showcase an increasing form of complexity (Axe-man, King, Plougher/Sage, wise Statesman, mighty Warrior). In fact, many Hindu gods are represented with features of animals as well as those of humans, leading many Hindus to easily accept evolutionary links between animals and humans. In India, the home country of Hindus; educated Hindus widely accept the theory of biological evolution. In a survey of 909 people, 77% of respondents in India agreed with Charles Darwin's Theory of Evolution, and 85 per cent of God-believing people said they believe in evolution as well.
As per Vedas, another explanation for the creation is based on the five elements: earth, water, fire, air and aether.
The Hindu religion traces its beginnings to the sacred Vedas. Everything that is established in the Hindu faith such as the gods and goddesses, doctrines, chants, spiritual insights, etc. flow from the poetry of Vedic hymns. The Vedas offer an honor to the sun and moon, water and wind, and to the order in Nature that is universal. This naturalism is the beginning of what further becomes the connection between Hinduism and science.
Islam.
From an Islamic standpoint, science, the study of nature, is considered to be linked to the concept of "Tawhid" (the Oneness of God), as are all other branches of knowledge. In Islam, nature is not seen as a separate entity, but rather as an integral part of Islam's holistic outlook on God, humanity, and the world. The Islamic view of science and nature is continuous with that of religion and God. This link implies a sacred aspect to the pursuit of scientific knowledge by Muslims, as nature itself is viewed in the Qur'an as a compilation of signs pointing to the Divine. It was with this understanding that science was studied and understood in Islamic civilizations, specifically during the eighth to sixteenth centuries, prior to the colonization of the Muslim world. Robert Briffault, in "The Making of Humanity", asserts that the very existence of science, as it is understood in the modern sense, is rooted in the scientific thought and knowledge that emerged in Islamic civilizations during this time.
With the decline of Islamic Civilizations in the late Middle Ages and the rise of Europe, the Islamic scientific tradition shifted into a new period. Institutions that had existed for centuries in the Muslim world looked to the new scientific institutions of European powers. This changed the practice of science in the Muslim world, as Islamic scientists had to confront the western approach to scientific learning, which was based on a different philosophy of nature. From the time of this initial upheaval of the Islamic scientific tradition to the present day, Muslim scientists and scholars have developed a spectrum of viewpoints on the place of scientific learning within the context of Islam, none of which are universally accepted or practiced. However, most maintain the view that the acquisition of knowledge and scientific pursuit in general is not in disaccord with Islamic thought and religious belief.
Ahmadiyya.
The Ahmadiyya movement emphasize that there is no contradiction between Islam and science. For example, Ahmadi Muslims universally accept in principle the process of evolution, albeit divinely guided, and actively promote it. Over the course of several decades the movement has issued various publications in support of the scientific concepts behind the process of evolution, and frequently engages in promoting how religious scriptures, such as the Qur'an, supports the concept. For general purposes, the second Khalifa of the community, Mirza Basheer-ud-Din Mahmood Ahmad says:
The Holy Quran directs attention towards science, time and again, rather than evoking prejudice against it. The Quran has never advised against studying science, lest the reader should become a non-believer; because it has no such fear or concern. The Holy Quran is not worried that if people will learn the laws of nature its spell will break. The Quran has not prevented people from science, rather it states, "Say, 'Reflect on what is happening in the heavens and the earth.'" (Al Younus) 
Jainism.
Jainism does not support belief in a creator deity. According to Jain doctrine, the universe and its constituents - soul, matter, space, time, and principles of motion have always existed (a static universe similar to that of Epicureanism and steady state cosmological model). All the constituents and actions are governed by universal natural laws. It is not possible to create matter out of nothing and hence the sum total of matter in the universe remains the same (similar to law of conservation of mass). Similarly, the soul of each living being is unique and uncreated and has existed since beginningless time.[a]
The Jain theory of causation holds that a cause and its effect are always identical in nature and hence a conscious and immaterial entity like God cannot create a material entity like the universe. Furthermore, according to the Jain concept of divinity, any soul who destroys its karmas and desires, achieves liberation. A soul who destroys all its passions and desires has no desire to interfere in the working of the universe. Moral rewards and sufferings are not the work of a divine being, but a result of an innate moral order in the cosmos; a self-regulating mechanism whereby the individual reaps the fruits of his own actions through the workings of the karmas.
Through the ages, Jain philosophers have adamantly rejected and opposed the concept of creator and omnipotent God and this has resulted in Jainism being labeled as "nastika darsana" or atheist philosophy by the rival religious philosophies. The theme of non-creationism and absence of omnipotent God and divine grace runs strongly in all the philosophical dimensions of Jainism, including its cosmology, karma, moksa and its moral code of conduct. Jainism asserts a religious and virtuous life is possible without the idea of a creator god.
Perspectives from the scientific community.
History.
In the 17th century, founders of the Royal Society largely held conventional and orthodox religious views, and a number of them were prominent Churchmen. While theological issues that had the potential to be divisive were typically excluded from formal discussions of the early Society, many of its fellows nonetheless believed that their scientific activities provided support for traditional religious belief. Clerical involvement in the Royal Society remained high until the mid-nineteenth century, when science became more professionalised.
Albert Einstein supported the compatibility of some interpretations of religion with science. In "Science, Philosophy and Religion, A Symposium" published by the Conference on Science, Philosophy and Religion in Their Relation to the Democratic Way of Life, Inc., New York in 1941, Einstein stated:
 Accordingly, a religious person is devout in the sense that he has no doubt of the significance and loftiness of those superpersonal objects and goals which neither require nor are capable of rational foundation. They exist with the same necessity and matter-of-factness as he himself. In this sense religion is the age-old endeavor of mankind to become clearly and completely conscious of these values and goals and constantly to strengthen and extend their effect. If one conceives of religion and science according to these definitions then a conflict between them appears impossible. For science can only ascertain what is, but not what should be, and outside of its domain value judgments of all kinds remain necessary. Religion, on the other hand, deals only with evaluations of human thought and action: it cannot justifiably speak of facts and relationships between facts. According to this interpretation the well-known conflicts between religion and science in the past must all be ascribed to a misapprehension of the situation which has been described.
Einstein thus expresses views of ethical non-naturalism (contrasted to ethical naturalism).
Prominent modern scientists who are atheists include evolutionary biologist Richard Dawkins and Nobel Prize–winning physicist Stephen Weinberg. Prominent scientists advocating religious belief include Nobel Prize–winning physicist and United Church of Christ member Charles Townes, evangelical Christian and past head of the Human Genome Project Francis Collins, and climatologist John T. Houghton.
Studies on scientists' beliefs.
Statistical analysis of Nobel prizes awarded between 1901 and 2000 reveals that (65.4%) of Nobel Prizes Laureates, have identified Christianity in its various forms as their religious preference. Specifically on the science related prizes, Christians have won a total of 72.5% of all the Chemistry, 65.3% in Physics, 62% in Medicine, and 54% in all Economics awards. Jews have won 17.3% of the prizes in Chemistry, 26.2% in Medicine, and 25.9% in Physics. Atheists, Agnostics, and Freethinkers have won 7.1% of the prizes in Chemistry, 8.9% in Medicine, and 4.7% in Physics. According to a study that was done by University of Nebraska–Lincoln in 1998, 60% of Nobel prize laureates in physics from 1901 to 1990 had a Christian background.
Many studies have been conducted in the United States and have generally found that scientists are less likely to believe in God than are the rest of the population. Precise definitions and statistics vary, but generally about 1/3 of scientists are atheists, 1/3 agnostic, and 1/3 have some belief in God (although some might be deistic, for example). This is in contrast to the more than roughly 3/4 of the general population that believe in some God in the United States. Belief also varies slightly by field. Two surveys on physicists, geoscientists, biologists, mathematicians, and chemists have noted that, from those specializing in these fields, physicists had lowest percentage of belief in God (29%) while chemists had highest (41%). Among members of the National Academy of Sciences, only 7.0% expressed personal belief, while 72.2% expressed disbelief and another 20.8% were agnostic concerning the existence of a personal god who answers prayer.
In 1916, 1,000 leading American scientists were randomly chosen from "American Men of Science" and 41.8% believed God existed, 41.5% disbelieved, and 16.7% had doubts/did not know; however when the study was replicated 80 years later using "American Men and Women of Science" in 1996, results were very much the same with 39.3% believing God exists, 45.3% disbelieved, and 14.5% had doubts/did not know. In the same 1996 survey, scientists in the fields of biology, mathematics, and physics/astronomy, belief in a god that is "in intellectual and affective communication with humankind" was most popular among mathematicians (about 45%) and least popular among physicists (about 22%). In total, in terms of belief toward a personal god and personal immortality, about 60% of United States scientists in these fields expressed either disbelief or agnosticism and about 40% expressed belief. This compared with 58% in 1914 and 67% in 1933.
A survey conducted between 2005 and 2007 by Elaine Howard Ecklund of University at Buffalo, The State University of New York on 1,646 natural and social science professors at 21 elite US research universities found that, in terms of belief in God or a higher power, more than 60% expressed either disbelief or agnosticism and more than 30% expressed belief. More specifically, nearly 34% answered "I do not believe in God" and about 30% answered "I do not know if there is a God and there is no way to find out." In the same study, 28% said they believed in God and 8% believed in a higher power that was not God. Ecklund stated that scientists were often able to consider themselves spiritual without religion or belief in god. Ecklund and Scheitle concluded, from their study, that the individuals from non-religious backgrounds disproportionately had self-selected into scientific professions and that the assumption that becoming a scientist necessarily leads to loss of religion is untenable since the study did not strongly support the idea that scientists had dropped religious identities due to their scientific training. Instead, factors such as upbringing, age, and family size were significant influences on religious identification since those who had religious upbringing were more likely to be religious and those who had a non-religious upbringing were more likely to not be religious. The authors also found little difference in religiosity between social and natural scientists.
In terms of perceptions, most social and natural scientists from 21 American elite universities did not perceive conflict between science and religion, while 36.6% did. However, in the study, scientists who had experienced limited exposure to religion tended to perceive conflict. In the same study they found that nearly one in five atheist scientists who are parents (17%) are part of religious congregations and have attended a religious service more than once in the past year. Some of the reasons for doing so are their scientific identity (wishing to expose their children to all sources of knowledge so they can make up their own minds), spousal influence, and desire for community.
A study conducted by the Pew Research Center found that members of the American Association for the Advancement of Science (AAAS) were "much less religious than the general public," with 51% believing in some form of deity or higher power. Specifically, 33% of those polled believe in God, 18% believe in a universal spirit or higher power, and 41% did not believe in either God or a higher power. 48% say they have a religious affiliation, equal to the number who say they are not affiliated with any religious tradition. 17% were atheists, 11% were agnostics, 20% were nothing in particular, 8% were Jewish, 10% were Catholic, 16% were Protestant, 4% were Evangelical, 10% were other religion. The survey also found younger scientists to be "substantially more likely than their older counterparts to say they believe in God". Among the surveyed fields, chemists were the most likely to say they believe in God.
Religious beliefs of US professors were recently examined using a nationally representative sample of more than 1,400 professors. They found that in the social sciences: 23.4% did not believe in God, 16% did not know if God existed, 42.5% believed God existed, and 16% believed in a higher power. Out of the natural sciences: 19.5% did not believe in God, 32.9% did not know if God existed, 43.9% believed God existed, and 3.7% believed in a higher power.
Farr Curlin, a University of Chicago Instructor in Medicine and a member of the MacLean Center for Clinical Medical Ethics, noted in a study that doctors tend to be science-minded religious people. He helped author a study that "found that 76 percent of doctors believe in God and 59 percent believe in some sort of afterlife." and "90 percent of doctors in the United States attend religious services at least occasionally, compared to 81 percent of all adults." He reasoned, "The responsibility to care for those who are suffering and the rewards of helping those in need resonate throughout most religious traditions."
Physicians in the United States, by contrast, are much more religious than scientists, with 76% stating a belief in God.
Public perceptions of science.
According to a 2007 poll by the Pew Forum, "while large majorities of Americans respect science and scientists, they are not always willing to accept scientific findings that squarely contradict their religious beliefs." The Pew Forum states that specific factual disagreements are "not common today", though 40% to 50% of Americans do not accept the evolution of humans and other living things, with the "strongest opposition" coming from evangelical Christians at 65% saying life did not evolve. 51% of the population believes humans and other living things evolved: 26% through natural selection only, 21% somehow guided, 4% don't know. In the U.S., biological evolution is the only concrete example of conflict where a significant portion of the American public denies scientific consensus for religious reasons. In terms of advanced industrialized nations, the United States is the most religious.
Creationism is not an exclusively American phenomenon. A poll on adult Europeans revealed that 40% believed in naturalistic evolution, 21% in theistic evolution, 20% in special creation, and 19% are undecided; with the highest concentrations of young earth creationists in Switzerland (21%), Austria (20.4%), Germany (18.1%). Other countries such as Netherlands, Britain, and Australia have experienced growth in such views as well.
Research on perceptions of science among the American public conclude that most religious groups see no general epistemological conflict with science and they have no differences with nonreligious groups in the propensity of seeking out scientific knowledge, although there may be subtle epistemic or moral conflicts when scientists make counterclaims to religious tenets. Findings from the Pew Center note similar findings and also note that the majority of Americans (80-90%) show strong support for scientific research, agree that science makes society and individual's lives better, and 8 in 10 Americans would be happy if their children were to become scientists. Even strict creationists tend to have very favorable views on science. A study on a national sample of US college students examined whether these students viewed the science / religion relationship as reflecting primarily conflict, collaboration, or independence. The study concluded that the majority of undergraduates in both the natural and social sciences do not see conflict between science and religion. Another finding in the study was that it is more likely for students to move away from a conflict perspective to an independence or collaboration perspective than towards a conflict view.
In the US, people who had no religious affiliation were no more likely than the religious population to have New Age beliefs and practices.
A study conducted on adolescents from Christian schools in Northern Ireland, noted a positive relationship between attitudes towards Christianity and science once attitudes towards scientism and creationism were accounted for.
Cross-national studies, which have pooled data on religion and science from 1981-2001, have noted that countries with high religiosity also have stronger faith in science, while less religious countries have more skepticism of the impact of science and technology. The United States is noted there as distinctive because of greater faith in both God and scientific progress. Other research cites the National Science Foundation's finding that America has more favorable public attitudes towards science than Europe, Russia, and Japan despite differences in levels of religiosity in these cultures.

</doc>
<doc id="29268" url="http://en.wikipedia.org/wiki?curid=29268" title="Stephen Sondheim">
Stephen Sondheim

Stephen Joshua Sondheim (), born March 22, 1930, is an American composer and lyricist known for more than a half-century contributions to musical theatre. Sondheim has received an Academy Award; eight Tony Awards (more than any other composer, including a Special Tony Award for Lifetime Achievement in the Theatre); eight Grammy Awards; a Pulitzer Prize, and the Laurence Olivier Award. Described by Frank Rich of "The New York Times" as "now the greatest and perhaps best-known artist in the American musical theater," His best-known works as composer and lyricist include "A Funny Thing Happened on the Way to the Forum", "Company", "Follies", "A Little Night Music", ', "Sunday in the Park with George" and "Into the Woods". He wrote the lyrics for "West Side Story" and '.
Sondheim has also written film music, contributing "Goodbye for Now" to Warren Beatty's 1981 "Reds". He wrote five songs for 1990's "Dick Tracy", including "Sooner or Later (I Always Get My Man)" by Madonna (which won the Academy Award for Best Song).
The composer was president of the Dramatists Guild from 1973 to 1981. To celebrate his 80th birthday, the former Henry Miller's Theatre was renamed the Stephen Sondheim Theatre on September 15, 2010 and the BBC Proms held a concert in his honor. Cameron Mackintosh has called Sondheim "possibly the greatest lyricist ever".
Early years.
Sondheim was born into a Jewish family in New York City, the son of Etta Janet ("Foxy", née Fox) and Herbert Sondheim. His father manufactured dresses designed by his mother. The composer grew up on the Upper West Side of Manhattan and, after his parents divorced, on a farm near Doylestown, Pennsylvania. As the only child of well-to-do parents living in the San Remo on Central Park West, he was described in Meryle Secrest's biography ("Stephen Sondheim: A Life") as an isolated, emotionally-neglected child. When he lived in New York, Sondheim attended the Ethical Culture Fieldston School. He later attended the New York Military Academy and George School, a private Quaker preparatory school in Bucks County, Pennsylvania where he wrote his first musical ("By George") and from which he graduated in 1946. Sondheim spent several summers at Camp Androscoggin. 
He traces his interest in theatre to "Very Warm for May", a Broadway musical he saw when he was nine. "The curtain went up and revealed a piano," Sondheim recalled. "A butler took a duster and brushed it up, tinkling the keys. I thought that was thrilling."
When Sondheim was ten, his father (a distant figure) abandoned him and his mother. Although Herbert sought custody of Stephen, because he left Foxy for another woman (Alicia, with whom he had two sons) he was unsuccessful. Sondheim explained to Secrest that he was "what they call an institutionalized child, meaning one who has no contact with any kind of family. You're in, though it's luxurious, you're in an environment that supplies you with everything but human contact. No brothers and sisters, no parents, and yet plenty to eat, and friends to play with and a warm bed, you know?"
Sondheim detested his mother, who was said to be psychologically abusive and projected her anger from her failed marriage on her son: "When my father left her, she substituted me for him. And she used me the way she used him, to come on to and to berate, beat up on, you see. What she did for five years was treat me like dirt, but come on to me at the same time." She once wrote him a letter saying that the "only regret [she] ever had was giving him birth". When his mother died in the spring of 1992, Sondheim did not attend her funeral.
Career.
Mentorship by Oscar Hammerstein II.
When Sondheim was about ten years old (around the time of his parents' divorce) he became friends with James Hammerstein, son of lyricist and playwright Oscar Hammerstein II. The elder Hammerstein became Sondheim's surrogate father, influencing him profoundly and developing his love of musical theatre. Sondheim met Hal Prince, who would direct many of his shows, at the opening of "South Pacific" (Hammerstein's musical with Richard Rodgers). The comic musical he wrote at George School, "By George", was a success among his peers and buoyed the young songwriter's ego. When Sondheim asked Hammerstein to evaluate it as though he had no knowledge of its author, he said it was the worst thing he had ever seen: "But if you want to know why it's terrible, I'll tell you". They spent the rest of the day going over the musical, and Sondheim later said: "In that afternoon I learned more about songwriting and the musical theater than most people learn in a lifetime."
Hammerstein designed a course of sorts for Sondheim on constructing a musical. He had the young composer write four musicals, each with one of the following conditions:
None of the "assignment" musicals was produced professionally. "High Tor" and "Mary Poppins" have never been produced; the rights holder for the original "High Tor" refused permission, and "Mary Poppins" was unfinished.
College and early career.
Sondheim began attending Williams College, a liberal arts college in Williamstown, Massachusetts whose theatre program attracted him. His first teacher there was Robert Barrow:
 ... everybody hated him because he was very dry, and I thought he was wonderful because he was very dry. And Barrow made me realize that all my romantic views of art were nonsense. I had always thought an angel came down and sat on your shoulder and whispered in your ear 'dah-dah-dah-DUM.' Never occurred to me that art was something worked out. And suddenly it was skies opening up. As soon as you find out what a leading tone is, you think, Oh my God. What a diatonic scale is – Oh my God! The logic of it. And, of course, what that meant to me was: Well, I can do that. Because you just don't know. You think it's a talent, you think you're born with this thing. What I've found out and what I believed is that everybody is talented. It's just that some people get it developed and some don't.
The composer told Meryle Secrest, "I just wanted to study composition, theory, and harmony without the attendant musicology that comes in graduate school. But I knew I wanted to write for the theatre, so I wanted someone who did not disdain theatre music." Barrow suggested that Sondheim study with Milton Babbitt, who Sondheim described as "a frustrated show composer" with whom he formed "a perfect combination." When he met Babbitt, he was working on a musical for Mary Martin based on the myth of Helen of Troy. Sondheim and Babbitt would meet once a week in New York City for four hours (at the time, Babbitt was teaching at Princeton University). According to Sondheim, they spent the first hour dissecting Rodgers and Hart or George Gershwin or studying Babbitt's favorites (Buddy DeSylva, Lew Brown and Ray Henderson). They then proceeded to other forms of music (such as Mozart's Jupiter Symphony), critiquing them the same way. Babbitt and Sondheim, fascinated by mathematics, studied songs by a variety of composers (especially Jerome Kern). Sondheim told Secrest that Kern had the ability "to develop a single motif through tiny variations into a long and never boring line and his maximum development of the minimum of material". He said about Babbitt, "I am his maverick, his one student who went into the popular arts with all his serious artillery". At Williams, Sondheim wrote a musical adaption of "Beggar on Horseback" (a 1924 play by George S. Kaufman and Marc Connelly, with permission from Kaufman) which had three performances. A member of the Beta Theta Pi fraternity, he graduated "magna cum laude" in 1950. 
"A few painful years of struggle" followed, when Sondheim auditioned songs, lived in his father's dining room to save money and spent time in Hollywood writing for the television series "Topper". He devoured 1940s and 1950s films, and has called cinema his "basic language"; his film knowledge got him through "The $64,000 Question" contestant tryouts. Sondheim dislikes movie musicals, favoring classic dramas such as "Citizen Kane", "The Grapes of Wrath" and "A Matter of Life and Death": "Studio directors like Michael Curtiz and Raoul Walsh ... were heroes of mine. They went from movie to movie to movie, and every third movie was good and every fifth movie was great. There wasn't any cultural pressure to make art".
At age 22, Sondheim had finished the four shows requested by Hammerstein. Julius and Philip Epstein's "Front Porch in Flatbush", unproduced at the time, was being shopped around by Lemuel (Lem) Ayers. Ayers approached Frank Loesser and another composer, who turned him down. Ayers and Sondheim met as ushers at a wedding, and Ayers commissioned Sondheim for three songs for the show; Julius Epstein flew in from California and hired Sondheim, who worked with him in California for four or five months. After eight auditions for backers, half the money needed was raised. The show, retitled "Saturday Night", was intended to open during the 1954–55 Broadway season; however, Ayers died of leukemia in his early forties. The rights transferred to his widow, Shirley, and due to her inexperience the show did not continue as planned; it opened off-Broadway in 2000. Sondheim later said, "I don't have any emotional reaction to "Saturday Night" at all – except fondness. It's not bad stuff for a 23-year-old. There are some things that embarrass me so much in the lyrics – the missed accents, the obvious jokes. But I decided, leave it. It's my baby pictures. You don't touch up a baby picture – you're a baby!"
Early Broadway success.
Burt Shevelove invited Sondheim to a party; Sondheim arrived before him, and knew no one else well. He saw a familiar face: Arthur Laurents, who had seen one of the auditions of "Saturday Night", and they began talking. Laurents told him he was working on a musical version of "Romeo and Juliet" with Leonard Bernstein, but they needed a lyricist; Betty Comden and Adolph Green, who were supposed to write the lyrics, were under contract in Hollywood. He said that although he was not a big fan of Sondheim's music, he enjoyed the lyrics from "Saturday Night" and he could audition for Bernstein. Sondheim met Bernstein the following day, played for him and Bernstein said he would let him know. The composer wanted to write music and lyrics; after consulting with Hammerstein, Bernstein told Sondheim he could write music later. In 1957, "West Side Story" opened; directed by Jerome Robbins, it ran for 732 performances. Sondheim has expressed dissatisfaction with his lyrics, saying that they do not always fit the characters and are sometimes too consciously poetic. While Bernstein was working on "Candide", Sondheim reportedly wrote some of "West Side Story"‍ '​s music; Bernstein's co-lyricist credit disappeared from "West Side Story" during its tryout, possibly as a trade-off. Sondheim insisted that Bernstein told the producers to list him as the sole lyricist. He described the division of the royalties, saying that Bernstein received three percent and he received one percent. Bernstein suggested evening the percentage at two percent each, but Sondheim refused because he wanted the credit. Sondheim later said he wished "someone stuffed a handkerchief in my mouth because it would have been nice to get that extra percentage".
After "West Side Story" opened, Shevelove lamented the lack of "low-brow comedy" on Broadway and mentioned a possible musical based on Plautus' Roman comedies. When Sondheim was interested in the idea he called a friend, Larry Gelbart, to co-write the script. The show went through a number of drafts, and was interrupted briefly by Sondheim's next project.
In 1959, Sondheim was approached by Laurents and Robbins for a musical version of Gypsy Rose Lee's memoir after Irving Berlin and Cole Porter turned it down. Sondheim agreed, but Ethel Merman – cast as Mama Rose – had just finished "Happy Hunting" with an unknown composer (Harold Karr) and lyricist (Matt Dubey). Although Sondheim wanted to write the music and lyrics, Merman refused to let another first-time composer write for her and demanded that Jule Styne write the music. Sondheim, concerned that writing lyrics again would pigeonhole him as a lyricist, called his mentor for advice. Hammerstein told him he should take the job, because writing a vehicle for a star would be a good learning experience. Sondheim agreed; "Gypsy" opened on May 21, 1959, and ran for 702 performances.
Death of Oscar Hammerstein II.
In 1960, Sondheim lost his mentor and father figure, Oscar Hammerstein II. Sondheim recalled that shortly before his death, he was at Hammerstein's house and Hammerstein had given Sondheim a portrait of himself. Sondheim gave it back and asked him to inscribe it and said later of the request that it was "weird..it's like asking your father to inscribe something." Reading the inscription, "For Stevie, My Friend and Teacher," still chokes up the award-winning composer, who wistfully comments "that describes Oscar better than anything I could say."
Walking away from the house that evening, Sondheim recalled that he got the sinking feeling that this was probably going to be the final goodbye and a great sadness and gloom descended upon him. Sondheim never saw his mentor again. Three days later, Hammerstein lost his battle with stomach cancer. His protégé took the podium at the funeral and delivered a heartfelt eulogy.
As both composer and lyricist.
The first musical for which Sondheim wrote both the music and lyrics was "A Funny Thing Happened on the Way to the Forum". It opened in 1962 and ran 964 performances. The book, based on the farces of Plautus, was written by Burt Shevelove and Larry Gelbart. Sondheim's score was not especially well received at the time. Even though the show won several Tony Awards, including best musical, Sondheim did not receive a nomination.
Broadway failures and other projects.
At this point, Sondheim had participated in three straight hits. His next show ended the streak. "Anyone Can Whistle" (1964) was a 9-performance flop, although it introduced Angela Lansbury to musical theatre and has developed a cult following.
"Do I Hear a Waltz?", based on the 1952 Laurents play "The Time of the Cuckoo", was originally intended to be another Rodgers & Hammerstein musical with Mary Martin as the lead, but was in need of a new lyricist. Laurents and Rodgers' daughter, Mary Rodgers, both asked Sondheim to fill in and Sondheim agreed. Even though Richard Rodgers and Sondheim agreed that the original play did not lend itself to musicalization, the team went ahead and began writing the musical. The musical was plagued with problems, partly due to Richard Rodgers' alcoholism as a way to cope with his self-perceived diminishing ability to write and the loss of his partner, Oscar Hammerstein II. After this show, Sondheim decided that he would henceforth work only on projects where he could write both the music and lyrics himself. Sondheim has said that this is the one project he has regretted. He asked author and playwright James Goldman to join him as bookwriter for a new musical. Inspired by a "New York Times" article about a gathering of former showgirls from the Ziegfeld Follies, they decided upon a story about ex-showgirls. The show was titled "The Girl Upstairs" (which would later become "Follies").
In 1966, Sondheim semi-anonymously provided the lyric for "The Boy From...," a parody of "The Girl from Ipanema" that was a highlight of the off-Broadway revue "The Mad Show." (The official songwriting credit went to the linguistically minded pseudonym "Esteban Ria Nido," which translates from the Spanish to "Stephen River Nest". In the show's playbill, the lyrics are credited to "Nom De Plume".) In that same year, James Goldman and Sondheim hit a creative wall working on "The Girls Upstairs". Goldman asked Sondheim about writing a TV musical. The result was "Evening Primrose", starring Anthony Perkins and Charmian Carr. It was written for the television anthology series ABC Stage 67 and premiered on November 16, 1966. Both Sondheim and director Paul Bogart admitted that the musical was only written because Goldman needed rent money. Sondheim asked producer Hubbell Robinson to produce it, but the network was not a fan of the title or Sondheim's alternative title, "A Little Night Music".
After completing "Evening Primrose", Jerome Robbins had tried to convince Sondheim to adapt Bertolt Brecht's "The Measures Taken", but Sondheim admitted that he did not like the play and did not like a lot of Brecht's work. Robbins wanted to adapt another Brecht play "The Exception and the Rule" and called John Guare to adapt the book. Bernstein had not written for the stage in a while, and his contract conducting the New York Philharmonic was ending. Sondheim was invited to Robbins' house, who unbeknownst to Sondheim, was trying to be convinced to write the lyrics to a musical adaption of "The Exception and the Rule". Guare was asked to convince Sondheim to do the lyrics. According to Robbins, if Sondheim didn't do it, Bernstein wouldn't do it. After Guare told him about the show, Sondheim agreed to do it. Guare asked, "Why haven't you all worked together since "West Side Story"?" to which Sondheim replied, "You'll see". Guare recalled a moment when Robbins had put him in a house Robbins had rented for Gold and Fizdale, and he put Guare in a locked room, saying he could not come out until he was finished. Any finished papers were slid under the door. Guare said working with Sondheim was like being with an old college roommate, they just talked and talked. Guare heavily depended on Sondheim to help him "decode and decipher their crazy way of working." Guare said that Bernstein only worked after midnight and Robbins only worked in the bright and early morning. Guare also commented that Bernstein's score, which was supposed to be light, was heavily influenced by Bernstein's feeling he needed to make a major musical statement. Stuart Ostrow, who had ties with Sondheim with "The Girls Upstairs" (later titled "Follies"), agreed to produce the musical, now entitled "A Pray By Blecht" (later titled "The Race to Urga"). An opening date was set and they were in the middle of auditions when Robbins asked to be excused for a moment. He did not come back and Guare asked where he went and the doorman said he got in a limousine and was headed to Kennedy Airport. This caused Bernstein to burst into tears and say "It's over". Sondheim said of the project, "I was ashamed of the whole project. It was arch and didactic in the worst way." He wrote one and half songs, and threw them both away (the only time he has ever done that). Eighteen years later, Bernstein and Robbins asked Sondheim to retry adapting the show, but Sondheim refused.
He has resided in a Turtle Bay brownstone in Manhattan since his fortunes swelled from writing "Gypsy" in 1959. While at his brownstone in 1969, Sondheim was playing music and he received a knock on the door. It was his neighbor, Katharine Hepburn, and she was in "bare feet – this angry, red-faced lady" and she told him "'You have been keeping me awake all night!". Hepburn had been practicing for her musical debut in "Coco" and was being distracted. Sondheim asked why she didn't ask him to play for her, which she stated she had lost his phone number. With a wry smile, Sondheim reflected back saying, "My guess is that she wanted to stand there in her bare feet, suffering for her art".
Collaborations with Hal Prince (1970–1981).
After the completion of "Do I Hear a Waltz", Sondheim devoted himself to both composing and writing lyrics for a series of varied and adventurous musicals. Sondheim collaborated with producer/director Harold Prince on six musicals between 1970 and 1981, beginning with the innovative "concept musical" "Company" in 1970. "Company" (1970) centered on a set of characters and themes rather than a straightforward plot. With a book by George Furth, the show opened on April 26, 1970, at the Alvin Theatre, where it ran for 705 performances after seven previews. It would go on to win the Tony Award for Best Musical, Best Music, and Best Lyrics, among others.
"Follies" (1971), with a book by James Goldman, opened on April 4, 1971, at the Winter Garden Theatre and closed after 522 performances and 12 previews. The story concerns a reunion in a crumbling Broadway theatre, scheduled for demolition, of the past performers of the "Weismann's Follies," a musical revue (based on the Ziegfeld Follies), that played in that theatre between the World Wars. It focuses on two couples, Buddy and Sally Durant Plummer and Benjamin and Phyllis Rogers Stone, who are attending the reunion.
After "Follies" was "A Little Night Music" (1973), a more traditionally plotted show based on the film "Smiles of a Summer Night" by Ingmar Bergman, was one of his greatest successes. "Time" magazine called it "Sondheim's most brilliant accomplishment to date." Notably, the score was mostly composed in waltz time (either ¾ time, or multiples thereof.) Further success was accorded to "A Little Night Music" when "Send in the Clowns" became a hit single for Judy Collins. Although it was Sondheim's only Top 40 hit, his songs are frequently performed and recorded by cabaret artists and theatre singers in their solo careers. "A Little Night Music" opened on Broadway at the Shubert Theatre on February 25, 1973, and closed on August 3, 1974, after 601 performances and 12 previews. It moved to the Majestic Theatre on September 17, 1973, where it completed its run.
"By Bernstein" premiered at the off-Broadway Westside Theatre on November 23, 1975, and closed on December 7, 1975. It ran for 40 previews and 17 performances. The lyrics and music were by Leonard Bernstein, with additional lyrics from other lyricists, including Sondheim. It was conceived and written by Betty Comden and Adolph Green and Norman L. Berman. The production was directed by Michael Bawtree with a cast of Jack Bittner, Margery Cohen, Jim Corti, Ed Dixon, Patricia Elliott, Kurt Peterson, and Janie Sell. The two known songs that had Sondheim contributions are "In There" from the adaption of "The Exception and the Rule" (which would later be named "The Race to Urga") and a cut song from "West Side Story" "Kids Ain't (Like Everybody Else)".
"Pacific Overtures" (1976) was the most non-traditional of the Sondheim—Prince collaborations, an intellectual exploration of the westernization of Japan.
"" (1979), Sondheim's most operatic score and libretto (which, along with "Pacific Overtures" and "A Little Night Music", has been seen in opera houses), once again explores an unlikely topic, this time murderous revenge and cannibalism. The book, by Hugh Wheeler, is based on Christopher Bond's 1973 stage version of the Victorian original.
"Merrily We Roll Along" (1981), with a book by George Furth, is one of Sondheim's more "traditional" scores and was thought to hold potential to generate some hit songs (Frank Sinatra and Carly Simon each recorded a different song from the show). Sondheim's music director, Paul Gemignani, said, "Part of Steve's ability is this extraordinary versatility." "Merrily", however, was a 16-performance flop. In spite of this, its score has endured due to subsequent recordings and productions. According to Martin Gottfried, "Sondheim had set out to write traditional songs ... But [despite] that there is nothing ordinary about the music." Sondheim and Furth have extensively revised the show since its initial opening. Sondheim later stated, "Did I feel betrayed? I'm not sure I would put it like that. What did surprise me was the feeling around the Broadway community – if you can call it that, though I guess I will for lack of a better word – that they wanted Hal and me to fail."
Collaborations with James Lapine (1984–1994).
The failure of "Merrily" greatly affected Sondheim; he was ready to quit theatre and do movies or create video games or write mysteries. He was later quoted as saying, "I wanted to find something to satisfy myself that does not involve Broadway and dealing with all those people who hate me and hate Hal." The collaboration between Sondheim and Prince would largely end after "Merrily" – until the 2003 production of "Bounce", another failure.
However, instead of quitting the theatre following the failure of "Merrily", Sondheim decided "that there are better places to start a show," and found a new collaborator in the "artsy" James Lapine. Sondheim saw a show in 1981 that gave him hope again. The show was "Twelve Dreams", and it was an Off-Broadway play at the Public Theatre. Sondheim recalled after "Merrily", "I was discouraged, and I don't know what would have happened if I hadn't discovered "Twelve Dreams" at the Public Theatre". Lapine has a taste "for the avant-garde and for visually oriented theatre in particular." Their first collaboration was "Sunday in the Park with George" (1984), in which Sondheim's music evoked the pointillist painting technique of its subject, Georges Seurat. In 1985, he and Lapine won the Pulitzer Prize for Drama for "Sunday in the Park with George". The show had its first revival on Broadway in 2008.
The Sondheim–Lapine collaboration also produced a musical reimagining classic fairy-tales, "Into the Woods" (1987). Sondheim has been attributed as the first composer to bring rap into Broadway with the Witch in the opening number of "Into the Woods". Sondheim actually attributes the first rap in theatre to Meredith Willson, with "Rock Island" from "The Music Man".
Their last work together was the rhapsodic "Passion" (1994), which was adapted from the Italian film "Passione D'Amore" by Ettore Scola. After a run of 280 performances, "Passion" became the shortest-running show to win the Tony Award for Best Musical.
Later work.
"Assassins" premiered in 1990 with music and lyrics by Sondheim and a book by Weidman. The show opened off-Broadway at the Playwrights Horizons on December 18, 1990, and closed on February 16, 1991, after 73 performances. The idea came from when Sondheim was a panelist at producer Stuart Ostrow's Musical Theater Lab, and he read a script by playwright Charles Gilbert. Sondheim asked Gilbert for permission to use his idea. Gilbert consented and offered to write the book; but Sondheim declined, having already had collaborator John Weidman in mind.
"Saturday Night" was shelved until a 1997 production at London's Bridewell Theatre. In 1998 "Saturday Night" received a professional recording, followed by a revised version with two new songs and an Off-Broadway run at Second Stage Theatre in 2000 and a full British premiere with the new songs in 2009 at London's Jermyn Street Theatre.
In the late nineties, Sondheim and Weidman reunited with Hal Prince for the musical comedy "Wise Guys", a project that took a long time to complete that follows brothers Addison and Wilson Mizner. Though a Broadway production starring Nathan Lane and Victor Garber and directed by Sam Mendes was announced for Spring 2000, the New York debut of the musical was delayed. Rechristened "Bounce" in 2003, the show was mounted at the Goodman Theatre in Chicago, and at the Kennedy Center in Washington, D.C.. "Bounce" received disappointing reviews and never reached Broadway. A revised version of "Bounce" premiered off-Broadway at The Public Theater under the new name "Road Show" from October 28, 2008 through December 28, 2008, under the direction of John Doyle.
Regarding his interest in writing new work, Sondheim was quoted in a 2006 "Time Out: London" interview as saying, "No ... It's age. It's a diminution of energy and the worry that there are no new ideas. It's also an increasing lack of confidence. I'm not the only one. I've checked with other people. People expect more of you and you're aware of it and you shouldn't be." In December 2007, however, Sondheim said that, along with continued work on "Bounce", he was "nibbling at a couple of things with John Weidman and James Lapine."
Lapine created a "multimedia revue," formerly titled "Sondheim: a Musical Revue", which had been scheduled to premiere in April 2009 at the Alliance Theatre, Atlanta, Georgia. However, that production was canceled, due to "difficulties encountered by the commercial producers attached to the project ... in raising the necessary funds". A revised version, "Sondheim on Sondheim", was produced by the Roundabout Theatre Company and premiered on Broadway at Studio 54 in a limited engagement from March 19, 2010 in previews, opening April 22 through June 13. The cast featured Barbara Cook, Vanessa L. Williams, Tom Wopat, Norm Lewis and Leslie Kritzer.
Sondheim collaborated with Wynton Marsalis in a musical "event," which was performed at New York City Center in an Encores! Special Event, titled "A Bed and a Chair: A New York Love Affair," which took place November 13–17, 2013. The staged concert was directed by John Doyle, with choreography by Parker Esse, and consisted of "more than two-dozen Sondheim compositions, each piece newly re-imagined by Marsalis." The cast featured Bernadette Peters, Jeremy Jordan, Norm Lewis, and Cyrille Aimee with four dancers, and the Jazz at Lincoln Center Orchestra, conducted by David Loud. Steven Suskin described the concert in "Playbill" as "neither a new musical, a revival, nor a standard songbook revue; it is, rather, a staged-and-sung chamber jazz rendition of a string of songs." He further added that "Half of the songs come from "Company" and "Follies"; most of the other Sondheim musicals are represented, including the lesser-known "Passion" and "Road Show"".
Sondheim wrote some new songs for the film adaptation of "Into the Woods", including one entitled "Rainbows," which Sondheim also included in his second book.
Upcoming projects.
It was announced in February 2012 that Sondheim will be collaborating on a new musical with David Ives. Sondheim has said that he has "about 20–30 minutes of the musical completed." The show is tentatively called "All Together Now" and is assumed to follow the format of "Merrily We Roll Along". Sondheim said of the project, "two people and what goes into their relationship ... We'll write for a couple of months, then have a workshop. It seemed experimental and fresh 20 years ago. I have a feeling it may not be experimental and fresh any more." On October 11, 2014, it was confirmed the Sondheim and Ives' new musical will be based on two of Luis Buñuel's films, "The Exterminating Angel" and "The Discreet Charm of the Bourgeoisie", and will premiere at The Public Theater.
Conversation with Frank Rich.
On April 28, 2002, during the Sondheim Celebration at the Kennedy Center, Sondheim and Frank Rich of the "New York Times" held a "conversation". In March 2008, Sondheim and Rich appeared in four interviews/conversations in California and Portland, Oregon titled "A Little Night Conversation with Stephen Sondheim". In September 2008, they appeared at Oberlin College in Oberlin, Ohio. The "Cleveland Jewish News" reported on the Oberlin event, writing: "Sondheim said: 'Movies are photographs; the stage is larger than life.' What musicals does Sondheim admire the most? "Porgy and Bess" tops a list which includes "Carousel", "She Loves Me", and "The Wiz", which he saw six times. Sondheim took a dim view of today's musicals. What works now, he said, are musicals that are easy to take; audiences don't want to be challenged." Sondheim and Rich had more conversations on January 18, 2009 at Avery Fisher Hall, on February 2, 2009 at the Landmark Theatre, Richmond, Virginia, on February 21, 2009 at the Kimmel Center, Philadelphia, Pennsylvania, and on April 20, 2009 at the University of Akron College of Fine and Applied Arts, EJ Thomas Hall, Akron, Ohio. The conversations were reprised at Tufts and Brown Universities in February 2010 and the University of Tulsa in April 2010. They spoke again at Lafayette College on March 8, 2011.
Sondheim had an additional "conversation with" Sean Patrick Flahaven (associate editor of "The Sondheim Review") at the Kravis Center in West Palm Beach, Florida, on February 4, 2009, during which he spoke of many of his songs and shows. "On the perennial struggles of Broadway: 'I don't see any solution for Broadway's problems except subsidized theatre, as in most civilized countries of the world.'"
On February 1, 2011, Sondheim sat down with The Salt Lake Tribune's former theatre critic Nancy Melich in front of an audience of 1200 at the Kingsbury Hall. Melich said of the night He was visibly taken by the university choir, who sang two songs during the evening, 'Children Will Listen' and 'Sunday', and then returned to reprise 'Sunday'. During that final moment, Sondheim and I were standing, facing the choir of students from the University of Utah's opera program, our backs to the audience, and I could see tears welling in his eyes as the voices rang out. Then, all of a sudden, he raised his arms and began conducting, urging the student singers to go full out, which they did, the crescendo building, their eyes locked with his, until the final 'on an ordinary Sunday' was sung. It was thrilling, and a perfect conclusion to a remarkable evening—nothing ordinary about it.
On March 13, 2008, "A Salon With Stephen Sondheim," was hosted by the Academy for New Musical Theatre in Hollywood. Tickets sold out in three minutes.
Work away from Broadway.
Sondheim's career has been varied, encompassing much beyond the composition of musicals.
An avid fan of games, in 1968 and 1969 Sondheim published a series of cryptic crossword puzzles in "New York" magazine. In 1987, "Time" referred to his love of puzzlemaking as "legendary in theater circles," adding that the central character in Anthony Shaffer's hit play "Sleuth" was inspired by Sondheim. (There was a rumor that "Sleuth" was given the working title "Who's Afraid of Stephen Sondheim?", but in a "New York Times" interview on March 10, 1996, Shaffer denied ever using the title.) Sondheim's love of puzzles and mysteries can also be seen in the intricate "whodunit" he co-wrote with longtime friend Anthony Perkins, "The Last of Sheila". This 1973 film, directed by Herbert Ross, starred Dyan Cannon, Raquel Welch, James Mason, James Coburn and Richard Benjamin.
He tried his hand at playwriting one more time – in 1996 he collaborated with "Company" librettist George Furth on a play called "Getting Away with Murder". It was not a success, and the Broadway production closed after 29 previews and 17 performances.
His compositional efforts have included a number of film scores, notably a set of songs written for Warren Beatty's 1990 film version of "Dick Tracy"; one song, "Sooner or Later (I Always Get My Man)" (as performed by Madonna), won Sondheim an Academy Award.
Unfinished or canceled works.
Sondheim was asked to translate "Mahagonny-Songspiel", although he did not state the time. He said, "But, I'm not a Brecht/Weill fan and that's really all there is to it. I'm an apostate: I like Weill's music when he came to America better than I do his stuff before ... I love The Threepenny Opera but, outside of The Threepenny Opera, the music of his I like is the stuff he wrote in America – when he was not writing with Brecht, when he was writing for Broadway." He was also asked to musicalize Nathanael West's "A Cool Million" with James Lapine around 1982, but he refused.
Sondheim worked with William Goldman on "Singing Out Loud", a movie musical, in 1992. Sondheim stated that Goldman wrote one or two drafts of the script and Sondheim wrote six and a half songs, only to have director Rob Reiner lose interest in the project. The songs "Dawn" and "Sand" from the project were recorded for the albums "Sondheim at the Movies" and "Unsung Sondheim". Sondheim and Leonard Bernstein wrote "The Race to Urga", scheduled to play at the Lincoln Center in 1969, but when Jerome Robbins left the project, it went unproduced.
Sondheim, in 1991, was working with Terrence McNally on a musical entitled "All Together Now". McNally said, "Steve was interested in telling the story of a relationship from the present back to the moment when the couple first met. We worked together a while, but we were both involved with so many other projects that this one fell through". The script, with concept notes by McNally and Sondheim, is archived in the Harry Ransom Humanities Research Center at the University of Texas at Austin. The story follows Arden Scott, a 30-something female sculptor, and Daniel Nevin, a slightly younger, sexually charismatic restaurateur.
Books.
Sondheim wrote a book of annotations of his lyrics titled "Finishing the Hat" (2010), a collection of his lyrics "from productions dating 1954–1981. In addition to published and unpublished lyrics from "West Side Story", "Follies" and "Company", the tome finds Sondheim discussing his relationship with Oscar Hammerstein II and his collaborations with composers, actors and directors throughout his lengthy career." This book, part one of a two-part series, is named after a song he wrote for "Sunday in the Park With George". Sondheim said "It's going to be long. I'm not, by nature, a prose writer, but I'm literate, and I have a couple of people who are vetting it for me, whom I trust, who are excellent prose writers." "Finishing the Hat" was published in October 2010. The review of the book in "The New York Times" stated that "The lyrics under consideration here, written during a 27-year period, aren't presented as fixed and sacred paradigms, carefully removed from tissue paper for our reverent inspection. They're living, evolving, flawed organisms, still being shaped and poked and talked to by the man who created them." The book was number 11 on "The New York Times" Hardcover Nonfiction list for November 5, 2010.
The follow-up book, "Look, I Made a Hat: Collected Lyrics (1981–2011) with Attendant Comments, Amplifications, Dogmas, Harangues, Digressions, Anecdotes and Miscellany" was released on November 22, 2011. The book begins with "Sunday in the Park With George", where "Finishing the Hat" stopped, and includes sections on his work in movies and television.
Mentorship of others.
Sondheim was mentored at a young age by Oscar Hammerstein II, and he too has returned the favor to young theatre lovers, exclaiming he loves "passing on what Oscar passed on to me". In 1979, when Adam Guettel (son of Mary Rodgers and grandson of Richard Rodgers, both composers) was 14, he showed Sondheim his work, much like Sondheim did with Hammerstein. Guettel said he left "crestfallen," and Sondheim wrote him a letter apologizing that he didn't mean to be "not very encouraging," but more that Sondheim was trying to be "constructive".
The second was to a fledgling composer named Jonathan Larson. Larson, who had musicalized "Nineteen Eighty-Four" and called his work "Superbia", had a workshop set up for the musical, which Sondheim attended. In the musical "Tick, Tick... Boom!", the actual phone message left by Sondheim is played, in which he apologizes for leaving after the show but wants to meet with him sometime, and that he was impressed with his work. After Larson's death, Sondheim said Larson was one of the few composers "attempting to blend contemporary pop music with theater music, which doesn't work very well; he was on his way to finding a real synthesis. A good deal of pop music has interesting lyrics, but they are not theater lyrics." Sondheim explained that a musical theatre composer "must have a sense of what is theatrical, of how you use music to tell a story, as opposed to writing a song. Jonathan understood that instinctively."
Major works.
Unless otherwise noted, music and lyrics by Stephen Sondheim.
Revues and anthologies.
"Side By Side By Sondheim" (1976), "Marry Me A Little" (1980), "You're Gonna Love Tomorrow" (1983) "Putting It Together" (1993), and "Sondheim on Sondheim" (2010) are anthologies or revues of Sondheim's work as composer and lyricist, featuring both songs performed and cut from productions. "Jerome Robbins' Broadway" features "You Gotta Have a Gimmick" from "Gypsy", "Suite of Dances" from "West Side Story", and "Comedy Tonight" from "A Funny Thing Happened on the Way to the Forum". A new revue, "Secret Sondheim ... a celebration of his lesser known work", conceived and directed by Tim McArthur, plays the Jermyn Street Theatre, London, in July 2010.
Sondheim's work is also featured in "The Madwoman of Central Park West", including the songs "Pretty Women" and "Everybody Ought to Have a Maid".
Minor works.
Film and TV.
In 1976 Sondheim appeared, together with theatre critic Frank Rich, John Weidman (book for "Pacific Overtures") and members of the original cast of "Pacific Overtures" in a television program titled "Anatomy of a Song." Sondheim plays piano as cast sings the song "Someone in a Tree". Sondheim discusses his working methods, the genesis of the show, and names "Someone in a Tree" his favorite song to date.
Honors and awards.
Several benefits and concerts were performed to celebrate Sondheim's 80th birthday in 2010. Among them were the New York Philharmonic's "Sondheim: The Birthday Concert", which was held March 15 and 16, 2010 at Lincoln Center's Avery Fisher Hall and hosted by David Hyde Pierce. The concert included Sondheim music and songs performed, in some cases, by the original performers. Lonny Price directed, with Paul Gemignani conducting. The performers included: Laura Benanti, Matt Cavenaugh, Michael Cerveris, Victoria Clark, Jenn Colella, Jason Danieley, Alexander Gemignani, Joanna Gleason, Nathan Gunn, George Hearn, Patti LuPone, Marin Mazzie, Audra McDonald, John McMartin, Donna Murphy, Karen Olivo, Laura Osnes, Mandy Patinkin, Bernadette Peters, Bobby Steggert, Elaine Stritch, Jim Walton, Chip Zien, the 2009 Broadway revival cast of West Side Story and a ballet performed by Blaine Hoven and Maria Riccetto set to Stephen Sondheim's score of Warren Beatty's "Reds". Jonathan Tunick also made a special appearance to pay tribute to his longtime collaborator. The concert was telecast on the PBS "Great Performances" show during November 2010, and the DVD of the performance was released on November 16, 2010.
The Roundabout Theatre Company benefit "Sondheim 80" was held on March 22, 2010. The evening included a performance of "Sondheim on Sondheim", plus dinner and a show at the New York Sheraton. There was "a very personal star-studded musical tribute" with new songs by contemporary musical theatre writers. The composers, who sang their own songs, included Tom Kitt and Brian Yorkey, Michael John LaChiusa, Andrew Lippa, Robert Lopez and Kristen Anderson-Lopez, Lin-Manuel Miranda (accompanied by Rita Moreno), Duncan Sheik, and Jeanine Tesori and David Lindsay-Abaire. Bernadette Peters performed a song (unnamed) that was dropped from a Sondheim show.
The New York City Center birthday celebration and benefit concert on April 26, 2010 featured (in order of appearance): Michael Cerveris, Alexander Gemignani, Donna Murphy, Debra Monk, Joanna Gleason, Maria Friedman, Mark Jacoby, Len Cariou, B.D. Wong, Claybourne Elder, Alexander Hanson, Catherine Zeta-Jones, Raul Esparza, Sutton Foster, Nathan Lane, Michele Pawk, the original cast of "Into the Woods"; Kim Crosby, Chip Zien, Danielle Ferland, & Ben Wright, Angela Lansbury, and Jim Walton. This concert was directed by John Doyle and co-hosted by Mia Farrow. During the concert, greetings were read. These greetings were written by: Sheila Hancock, Julia McKenzie, Milton Babbitt, Judi Dench, and Glynis Johns. After Catherine Zeta-Jones performed "Send in the Clowns," a recorded greeting from Julie Andrews was played. During her greeting, she sang a little of "Not a Day Goes By." Patti LuPone, Barbara Cook, Bernadette Peters, Tom Aldredge and Victor Garber were originally scheduled to perform, but withdrew from the concert. One of the beneficiaries of the concert was Young Playwrights Inc.
On July 31, 2010, a BBC Proms concert was held to celebrate Sondheim's 80th Birthday at the Royal Albert Hall in London. It featured songs from many of his musicals, including a performance of "Send in the Clowns" from "A Little Night Music" by Judi Dench (reprising her role as Desirée from the 1995 production of that musical), and performances from many other stars of opera, Broadway, stage and screen, including Bryn Terfel and Maria Friedman.
On November 19, 2010, The New York Pops performed at Carnegie Hall to celebrate Sondheim's 80th birthday, led by Steven Reineke. Kate Baldwin, Aaron Lazar, Christiane Noll, Paul Betz, Renee Rakelle, Marilyn Maye (singing "I'm Still Here"), and Alexander Gemignani were all on hand to sing songs including "I Remember," "Another Hundred People," "Children Will Listen," and "Getting Married Today." Sondheim made an on-stage appearance during the concert's encore of his song "Old Friends."
Sondheim has received the following honors:
He has won these awards:
Stephen Sondheim is also a member of the American Theatre Hall of Fame
Legacy.
This organization, founded by Sondheim in 1981, is intended to introduce young people to writing for the theatre. He is the Executive Vice President.
The Stephen Sondheim Center for the Performing Arts opened December 7–9, 2007, and is located at the Fairfield Arts & Convention Center in Fairfield, Iowa. The Center opened with performances from seven Broadway performers, including Len Cariou, Liz Callaway, and Richard Kind, all of whom had taken part in the musicals of Sondheim. The center is the first one in the world named after him, with a Broadway theatre the second.
In 1993 the Stephen Sondheim Society was set up to promote and provide information about the works of Stephen Sondheim. "Sondheim - the Magazine" is the Society magazine devoted to Sondheim's work, and is sent to every member of the Society. The Society aims to create a greater interest and appreciation of them by means of circulating information and providing a focal point where those interested can share such interests. It issues news, provides education, maintains a database of information, organizes productions, meetings, outings, and other events, assists with publicity and promotion, publishes articles, and performs other tasks. It runs a website at .
An annual event, the competition gives 12 young musical theatre students from top UK drama schools and universities the opportunity to compete for a prize of £1,000. Per Sondheim's request, a prize is also offered for a new song by a young composer, judged by George Stiles and Anthony Drewe. Each contestant performs one Sondheim song and one new song.
Most of the episode titles from the television series "Desperate Housewives" reference his work in some way, through the use of either song titles or lyrics. The final episode in the series is titled "Finishing the Hat" (airing May 13, 2012).
In 1990, Sondheim took the Cameron Mackintosh chair in musical theatre at Oxford, and in this capacity ran workshops with promising writers of musicals, such as George Stiles, Anthony Drewe, Andrew Peggie, Paul James, Stephen Keeling and others. These writers jointly set up the Mercury Workshop in 1992, which eventually merged with the New Musicals Alliance to become MMD, a UK-based organisation developing new musical theatre, of which Sondheim continues to be patron.
The Signature Theatre, Arlington, Virginia, established a new award, "The Sondheim Award," "as a tribute to America's most influential contemporary musical theatre composer." The first award was presented at a gala fund-raiser on April 27, 2009, with help from performers Bernadette Peters, Michael Cerveris, Will Gartshore and Eleasha Gamble. Sondheim himself was the first recipient of the award, which also includes a $5000 honorarium for the recipients' choice of a nonprofit organization. The 2010 honoree was Angela Lansbury, with Peters and Catherine Zeta-Jones as honorary hosts for the Gala Benefit held on April 12, 2010. The 2011 honoree was Bernadette Peters. Other awardees have been Patti LuPone (2012), Hal Prince (2013) and Jonathan Tunick (2014).
A Broadway theatre on West 43rd Street in New York City, Henry Miller's Theatre, was renamed the Stephen Sondheim Theatre on September 15, 2010, in honor of his 80th birthday. In attendance were Nathan Lane, Patti LuPone, and John Weidman. Sondheim said of the naming, "I'm deeply embarrassed. Thrilled, but deeply embarrassed. I've always hated my last name. It just doesn't sing. I mean, it's not Belasco. And it's not Rodgers and it's not Simon. And it's not Wilson. It just doesn't sing. It sings better than Schoenfeld and Jacobs. But it just doesn't sing." Lane said of the day, "We love our corporate sponsors and we love their money, but there's something sacred about naming a theatre, and there's something about this that is right and just."
According to "the Daily Telegraph", Sondheim is "almost certainly" the only living composer to have a quarterly journal published in his name. "The Sondheim Review", founded in 1994, exists to chronicle and promote the works of Stephen Sondheim.
Musical style.
Sondheim says that when he asked Babbitt if he could study atonality, Babbitt replied "You haven't exhausted tonal resources for yourself yet, so I'm not going to teach you
atonal." Sondheim agreed, and despite frequent dissonance and a highly chromatic style, his music remains resolutely tonal.
Sondheim's work is notable for his use of complex polyphony in the vocal parts, such as the chorus of five minor characters who function as a sort of Greek chorus in 1973's "A Little Night Music". He also displays a penchant for angular harmonies and intricate melodies. His musical influences are varied; Sondheim has claimed that he "loves Bach" but his favorite period is Brahms to Stravinsky.
Personal life.
Sondheim has been described as being extremely introverted, a largely solitary figure. In an interview with Frank Rich, Sondheim said that "the outsider feeling – somebody who people want to both kiss and kill – occurred quite early in my life." He is openly gay and in a relationship with Jeff Romley. Sondheim lived with Peter Jones, a dramatist, for several years until 1999.

</doc>
<doc id="29269" url="http://en.wikipedia.org/wiki?curid=29269" title="Self-determination">
Self-determination

The right of nations to self-determination (from German: "Selbstbestimmungsrecht der Völker") is a cardinal principle in modern international law (commonly regarded as a "jus cogens" rule), binding, as such, on the United Nations as authoritative interpretation of the Charter’s norms. It states that nations based on respect for the principle of equal rights and fair equality of opportunity have the right to freely choose their sovereignty and international political status with no external compulsion or interference which can be traced back to the Atlantic Charter, signed on 14 August 1941, by Franklin D. Roosevelt, President of the United States of America, and Winston Churchill, Prime Minister of the United Kingdom who pledged The Eight Principal points of the Charter. The principle does not state how the decision is to be made, or what the outcome should be, whether it be independence, federation, protection, some form of autonomy or full assimilation. Neither does it state what the delimitation between nations should be—or what constitutes a nation. In fact, there are conflicting definitions and legal criteria for determining which groups may legitimately claim the right to self-determination.
On 14 December 1960, the United Nations General Assembly adopted United Nations General Assembly Resolution 1514 (XV) under titled Declaration on the Granting of Independence to Colonial Countries and Peoples provided for the granting of independence to colonial countries and peoples in providing an inevitable legal linkage between self-determination and its goal of decolonisation, and a postulated new international law-based right of freedom also in economic self-determination. In Article 5 states: Immediate steps shall be taken in Trust and Non-Self-Governing Territories, or all other territories which have not yet attained independence, to transfer all powers to the peoples of those territories, without any conditions or reservations, in accordance with their freely expressed will and desire, without any distinction as to race, creed or colour, in order to enable them to enjoy complete independence and freedom, moreover on 15 December 1960 the United Nations General Assembly adopted United Nations General Assembly Resolution 1541 (XV) under titled Principles which should guide members in determining whether or nor an obligation exists to transmit the information called for under Article 73e of the United Nations Charter in Article 3 provided that [  i ]  nadequacy of political, economic, social or educational preparedness should never serve as a pretext for delaying independence. To monitor the implementation of Resolution 1514 in 1961 the General Assembly created the Special Committee referred to popularly as the Special Committee on Decolonization to ensure decolonization complete compliance with the principle of self-determination in General Assembly Resolution 1541 (XV), 12 Principle of the Annex defining free association with an independent State, integration into an independent State, or independence as the three legitimate options of full self-government compliance with the principle of self-determination.
"National aspirations must be respected; people may now be dominated and governed only by their own consent. "Self determination" is not a mere phrase; it is an imperative principle of action. . . . "—Woodrow Wilson with his famous "self-determination" speech on 11 February 1918 after he announced his Fourteen Points on 8 January 1918.
By extension the term self-determination has come to mean the free choice of one's own acts without external compulsion.
History.
Pre-20th century.
Origins.
The employment of imperialism, through the expansion of empires, and the concept of political sovereignty, as developed after the Treaty of Westphalia, also explain the emergence of self-determination during the modern era. During, and after, the Industrial Revolution many groups of people recognized their shared history, geography, language, and customs. Nationalism emerged as a uniting ideology not only between competing powers, but also for groups that felt subordinated or disenfranchised inside larger states, in this situation self-determination can be seen as a reaction to imperialism. Such groups often pursued independence and sovereignty over territory, but sometimes a different sense of autonomy has been pursued or achieved.
Empires.
The world possessed several traditional, continental empires such as the Ottoman, Russian, Austrian/Habsburg, and the Qing Empire. Political scientists often define competition in Europe during the Modern Era as a balance of power struggle, which also induced various European states to pursue colonial empires, beginning with the Spanish and Portuguese, and later including the British, French, Dutch, and German.
During the early 19th century, competition in Europe produced multiple wars, most notably the Napoleonic Wars. After this conflict, the British Empire became dominant and entered its "imperial century", while nationalism became a powerful political ideology in Europe.
Later, after the Franco-Prussian War in 1870, "New Imperialism" was unleashed with France and later Germany establishing colonies in Asia, the Pacific, and Africa. Japan also emerged as a new power. Multiple theaters of competition developed across the world:
The Ottoman Empire, Austrian Empire, Russian Empire, Qing Empire and the new Empire of Japan maintained themselves, often expanding or contracting at the expense of another empire. All ignored notions of self-determination for those governed.
Rebellions and emergence of nationalism.
The revolt of New World British colonists in North America, during the mid-1770s, has been seen as the first assertion of the right of national and democratic self-determination, because of the explicit invocation of natural law, the natural rights of man, as well as the consent of, and sovereignty by, the people governed; these ideas were inspired particularly by John Locke's enlightened writings of the previous century. Thomas Jefferson further promoted the notion that the will of the people was supreme, especially through authorship of the United States Declaration of Independence which inspired Europeans throughout the 19th century. The French Revolution was motivated similarly and legitimatized the ideas of self-determination on that Old World continent.
Within the New World during the early 19th century, most of the nations of Spanish America achieved independence from Spain. The United States supported that status, as policy in the hemisphere relative to European colonialism, with the Monroe Doctrine. The American public, organized associated groups, and Congressional resolutions, often supported such movements, particularly the Greek War of Independence (1821–29) and the demands of Hungarian revolutionaries in 1848. Such support, however, never became official government policy, due to balancing of other national interests. After the American Civil War and with increasing capability, the United States government did not accept self-determination as a basis during its Purchase of Alaska and attempted purchase of the West Indian islands of Saint Thomas and Saint John in the 1860s, or its growing influence in the Hawaiian Islands, that led to annexation in 1898. With its victory in the Spanish–American War in 1899 and its growing stature in the world, the United States supported annexation of the former Spanish colonies of Guam, Puerto Rico and the Philippines, without the consent of their peoples, and it retained "quasi-suzerainty" over Cuba, as well.
Nationalist sentiments emerged inside the traditional empires including: Pan-Slavism in Russia; Ottomanism, Kemalist ideology and Arab nationalism in the Ottoman Empire; State Shintoism and Japanese identity in Japan; and Han identity in juxtaposition to the Manchurian ruling class in China. Meanwhile in Europe itself there was a rise of nationalism, with nations such as Greece, Hungary, Poland and Bulgaria seeking or winning their independence.
Karl Marx supported such nationalism, believing it might be a "prior condition" to social reform and international alliances. In 1914 Vladimir Lenin wrote: "[It] would be wrong to interpret the right to self-determination as meaning anything but the right to existence as a separate state. "
World Wars I and II.
Europe, Asia and Africa.
Woodrow Wilson revived America's commitment to self-determination, at least for European states, during World War I. When the Bolsheviks came to power in Russia in November 1917, they called for Russia's immediate withdrawal as a member of the Allies of World War I. They also supported the right of all nations, including colonies, to self-determination. " The 1918 Constitution of the Soviet Union acknowledged the right of secession for its constituent republics.
This presented a challenge to Wilson's more limited demands. In January 1918 Wilson issued his Fourteen Points of January 1918 which, among other things, called for adjustment of colonial claims, as long as the interests of colonial powers had equal weight with the claims of subject peoples. The Treaty of Brest-Litovsk in March 1918 led to Russia's exit from the war and the independence of Armenia, Finland, Estonia, Latvia, Ukraine, Lithuania, Georgia and Poland. The end of the war led to the dissolution of the defeated Austro-Hungarian Empire and the creation by the Allies of Czechoslovakia and the union of the State of Slovenes, Croats and Serbs and the Kingdom of Serbia as new states. However, this imposition of states where some nationalities (especially Poles, Czechs, and Serbs and Romanians) were given power over nationalities who disliked and distrusted them eventually helped lead to World War II. Also Germany lost land after WWI: Northern Slesvig voted to return to Denmark after Schleswig_plebiscites,_1920. The defeated Ottoman empire was dissolved into the Republic of Turkey and several smaller nations, including Yemen, plus the new Middle East Allied "mandates" of Syria and Lebanon (future Syria, Lebanon and Hatay State), Palestine (future Transjordan and Israel), Mesopotamia (future Iraq). The League of Nations was proposed as much as a means of consolidating these new states, as a path to peace.
During the 1920s and 1930s there were some successful movements for self-determination in the beginnings of the process of decolonization. In the Statute of Westminster the United Kingdom granted independence to Canada, New Zealand, Newfoundland, the Irish Free State, the Commonwealth of Australia, and the Union of South Africa after the British parliament declared itself as incapable of passing laws over them without their consent. Egypt, Afghanistan and Iraq also achieved independence from Britain and Lebanon from France. Other efforts were unsuccessful, like the Indian independence movement. And Italy, Japan and Germany all initiated new efforts to bring certain territories under their control, leading to World War II. In particular, the National Socialist Program invoked this right of nations in its first point (out of 25), as it was publicly proclaimed on 24 February 1920 by Adolf Hitler.
In Asia, Japan became a rising power and gained more respect from Western powers after its victory in the Russo-Japanese War. Japan joined the Allied Powers in World War I and attacked German colonial possessions in the Far East, adding former German possessions to its own empire. In the 1930s, Japan gained significant influence in Inner Mongolia and Manchuria after it invaded Manchuria. It established Manchukuo, a puppet state in Manchuria and eastern Inner Mongolia. This was essentially the model Japan followed as it invaded other areas in Asia and established the Greater East Asia Co-Prosperity Sphere.
In 1912, the Republic of China officially succeeded the Qing Dynasty, while Outer Mongolia, Tibet and Tuva proclaimed their independence. Independence was not accepted by the government of China. By the Treaty of Kyakhta (1915) Outer Mongolia recognized China's sovereignty. However, the Soviet threat of seizing parts of Inner Mongolia induced China to recognize Outer Mongolia's independence, provided that a referendum was held. The referendum took place on October 20, 1945, with (according to official numbers) 100% of the electorate voting for independence.
Many of Eastern Asia's current disputes to sovereignty and self-determination stem from unresolved disputes from World War II. After its fall, the Empire of Japan renounced control over many of its former possessions including Korea, Sakhalin Island, and Taiwan. In none of these areas were the opinions of affected people consulted, or given significant priority. Korea was specifically granted independence but the receiver of various other areas was not stated in the Treaty of San Francisco, giving Taiwan "de facto" independence although its political status continues to be ambiguous...
The Cold War world.
The UN Charter.
In 1941 Allies of World War II signed the Atlantic Charter and accepted the principle of self-determination. In January 1942 twenty-six states signed the Declaration by United Nations, which accepted those principles. The ratification of the United Nations Charter in 1945 at the end of World War II placed the right of self-determination into the framework of international law and diplomacy.
However, the charter and other resolutions did not insist on full independence as the best way of obtaining self-government, nor did they include an enforcement mechanism. Moreover, new states were recognized by the legal doctrine of uti possidetis juris, meaning that old administrative boundaries would become international boundaries upon independence if they had little relevance to linguistic, ethnic, and cultural boundaries. Nevertheless, justified by the language of self-determination, between 1946 and 1960, the peoples of thirty-seven new nations freed themselves from colonial status in Asia, Africa, and the Middle East. The territoriality issue inevitably would lead to more conflicts and independence movements within many states and challenges to the assumption that territorial integrity is as important as self-determination.
The communist versus capitalist worlds.
Decolonization in the world was contrasted by the Soviet Union's successful post-war expansionism. Tuva and several regional states in Eastern Europe, the Baltic, and Central Asia had been fully annexed by the Soviet Union during World War II. Now, it extended its influence by establishing satellite states Eastern Germany and the countries of Eastern Europe, along with support for revolutionary movements in China and North Korea. Although satellite states were independent and possessed sovereignty, the Soviet Union violated principles of self-determination by suppressing the Hungarian revolution of 1956 and the Prague Spring Czechoslovak reforms of 1968. It invaded Afghanistan to support a communist government assailed by local tribal groups. However, Marxism-Leninism and its theory of imperialism were also strong influences in the national emancipation movements of third world nations rebelling against colonial or puppet regimes. In many Third World countries, communism became an ideology that united groups to oppose imperialism or colonization.
Soviet actions were contained by the United States which saw communism as a menace to its interests. Throughout the cold war, the United States created, supported, and sponsored regimes with various success that served their economic and political interests, among them anti-communist regimes such as that of Augusto Pinochet in Chile and Suharto in Indonesia. To achieve this, a variety of means was implemented, including the orchestration of coups, sponsoring of anti-communist countries and military interventions. Consequently, many self-determination movements, which spurned some type of anti-communist government, were accused of being Soviet-inspired or controlled.
Asia.
In Asia, the Soviet Union had already converted Mongolia into a satellite state but abandoned propping up the Second East Turkestan Republic and gave up its Manchurian claims to China. The new People's Republic of China had gained control of mainland China in the Chinese Civil War. The Korean War shifted the focus of the Cold War from Europe to Asia, where competing superpowers took advantage of decolonization to spread their influence.
In 1947, India gained independence from the British Empire. The empire was in decline but adapted to these circumstances by creating the British Commonwealth—since 1949 the Commonwealth of Nations—which is a free association of equal states. As India obtained its independence, multiple ethnic conflicts emerged in relation to the formation of a statehood during the Partition of India which resulted in Islamic Pakistan and Secular India. Before the advent of the British, no empire based in mainland India had controlled any part of what now makes up the country's Northeast, part of the reason for the ongoing insurgency in Northeast India. In 1971 Bangladesh obtained independence from Pakistan.
Burma also gained independence from the British Empire, but declined membership in the Commonwealth. Internal conflict in Burma that challenge the ruling government persist.
Indonesia gained independence from the Netherlands in 1949 after the latter failed to restore colonial control. As mentioned above, Indonesia also wanted a powerful position in the region that could be lessened by the creation of united Malaysia. The Netherlands retained Dutch New Guinea, but Indonesia threatened to invade and annex it. A vote was supposedly taken under the UN sponsored Act of Free Choice to allow West New Guineans to decide their fate, although many dispute its veracity. Later, Portugal relinquished control over East Timor in 1975, at which time Indonesia promptly invaded and annexed it.
North Borneo and Sarawak.
Another controversial episode with perhaps more relevance was the British beginning their exit from British Malaya. An experience concerned the findings of a "United Nations Assessment Team" that led the British territories of North Borneo and Sarawak in 1963 to determine whether or not the populations wished to become a part of the new Malaysia Federation. The United Nation Team's mission followed on from an earlier assessment by the British-appointed Cobbold Commission which had arrived in the territories in 1962 and held hearings to determine public opinion. It also sifted through 1600 letters and memoranda submitted by individuals, organisations and political parties. Cobbold concluded that around two thirds of the population favoured to the formation of Malaysia while the remaining third wanted either independence or continuing control by the United Kingdom. The United Nations team largely confirmed these findings, which were later accepted by the General Assembly, and both territories subsequently wish to formed the new Federation of Malaysia. The conclusions of both the Cobbold Commission and the United Nations team were arrived at without any referendums self-determination being held. Unlike in Singapore, however, no referendum was ever conducted in Sarawak and North Borneo. they sought to consolidate several of the previous ruled entities then there was Manila Accord, an agreement between the Philippines, Federation of Malaya and Indonesia on 31 July 1963 to abide by the wishes of the people of North Borneo and Sarawak within the context of United Nations General Assembly Resolution 1541 (XV), Principle 9 of the Annex taking into account referendums in North Borneo and Sarawak that would be free and without coercion. This also triggered the Indonesia – Malaysia confrontation because Indonesia opposed the violation of the agreements.
After the Cold War.
The Cold War began to wind down after Mikhail Gorbachev assumed power in March 1985. With the cooperation of the American president Ronald Reagan, Gorbachev wound down the size of the Soviet Armed Forces and reduced nuclear arms in Europe, while liberalizing the economy.
In 1989 – 90, the communist regimes of Soviet satellite states collapsed in rapid succession in Poland, Hungary, Czechoslovakia, East Germany, Bulgaria, Romania, and Mongolia. East and West Germany united, Czechoslovakia peacefully split into Czech Republic and Slovakia, while in 1990 Yugoslavia began a violent break up into its former 6 sub-unit republics. Kosovo, which was previously an autonomous unit of Serbia declared independence in 2008, but has received less international recognition.
In December 1991, Gorbachev resigned as president and the Soviet Union dissolved relatively peacefully into fifteen sovereign republics, all of which rejected communism and most of which adopted democratic reforms and free-market economies. Inside those new republics, four major areas have claimed their own independence, but not received widespread international recognition.
After decades of civil war, Indonesia finally recognized the independence of East Timor in 2002.
In 1949, the Communists won the civil war and established the People's Republic of China in Mainland China. The Kuomintang-led Republic of China government retreated to Taipei, its jurisdiction now limited to Taiwan and several outlying islands. Since then, the People's Republic of China has been involved in disputes with the ROC over issues of sovereignty and the political status of Taiwan.
As noted, self-determination movements remain strong in some areas of the world. Some areas possess "de facto" independence, such as Taiwan, North Cyprus, Kosovo, and South Ossetia, but their independence is disputed by one or more major states. Significant movements for self-determination also persist for locations that lack "de facto" independence, such as Kurdistan, Balochistan, Chechnya, and the State of Palestine
Current issues.
Since the early 1990s, the legitimatization of the principle of national self-determination has led to an increase in the number of conflicts within states, as sub-groups seek greater self-determination and full secession, and as their conflicts for leadership within groups and with other groups and with the dominant state become violent. The international reaction to these new movements has been uneven and often dictated more by politics than principle. The year 2000 United Nations Millennium Declaration failed to deal with these new demands, mentioning only "the right to self-determination of peoples which remain under colonial domination and foreign occupation. "
In an issue of "Macquarie University Law Journal" Associate Professor Aleksandar Pavkovic and Senior Lecturer Peter Radan outlined current legal and political issues in self-determination. These include:
Defining "peoples".
There is not yet a recognized legal definition of "peoples" in international law. Vita Gudeleviciute of Vytautas Magnus University Law School, reviewing international law and UN resolutions, finds in cases of non-self-governing peoples (colonized and/or indigenous) and foreign military occupation "a people" is the entire population of the occupied territorial unit, no matter their other differences. In cases where people lack representation by a state's government, the unrepresented become a separate people. Present international law does not recognize ethnic and other minorities as separate peoples, with the notable exception of cases in which such groups are systematically disenfranchised by the government of the state they live in. Other definitions offered are "peoples" being self-evident (from ethnicity, language, history, etc.), or defined by "ties of mutual affection or sentiment", i.e. "loyalty", or by mutual obligations among peoples. Or the definition may be simply that a people is a group of individuals who unanimously choose a separate state. If the "people" are unanimous in their desire for self-determination, it strengthens their claim. For example, the populations of federal units of the Yugoslav federation were considered a people in the breakup of Yugoslavia, although some of those units had very diverse populations. Libertarians who argue for self-determination distinguish between the voluntary nation (the land, the culture, the terrain, the people) and the state, the coercive apparatus, which they have a right to choose or self-determine.
Self-determination versus territorial integrity.
National self-determination appears to challenge the principle of territorial integrity (or sovereignty) of states as it is the will of the people that makes a state legitimate. This implies a people should be free to choose their own state and its territorial boundaries. However, there are far more self-identified nations than there are existing states and there is no legal process to redraw state boundaries according to the will of these peoples. According to the Helsinki Final Act of 1975, the UN, ICJ and international law experts, there is no contradiction between the principles of self-determination and territorial integrity, with the latter taking precedence.
Pavkovic and Radan describe three theories of international relations relevant to self-determination.
Allen Buchanan, author of seven books on self-determination and secession, supports territorial integrity as a moral and legal aspect of constitutional democracy. However, he also advances a "Remedial Rights Only Theory" where a group has "a general right to secede if and only if it has suffered certain injustices, for which secession is the appropriate remedy of last resort. " He also would recognize secession if the state grants, or the constitution includes, a right to secede.
Vita Gudeleviciute holds that in cases of non-self-governing peoples and foreign military occupation the principle of self-determination trumps that of territorial integrity. In cases where people lack representation by a state's government, they also may be considered a separate people, but under current law cannot claim the right to self-determination. On the other hand, she finds that secession within a single state is a domestic matter not covered by international law. Thus there are no on what groups may constitute a seceding people.
A number of states have laid claim to territories, which they allege were removed from them as a result of colonialism. This is justified by reference to Paragraph 6 of UN Resolution 1514(XV), which states that any attempt "aimed at partial or total disruption of the national unity and the territorial integrity of a country is incompatible with the purposes and principles of the Charter". This, it is claimed, applies to situations where the territorial integrity of a state had been disrupted by colonisation, so that the people of a territory subject to a historic territorial claim are prevented from exercising a right to self-determination. This interpretation is rejected by many states, who argue that Paragraph 2 of UN Resolution 1514(XV) states that "all peoples have the right to self-determination" and Paragraph 6 cannot be used to justify territorial claims. The original purpose of Paragraph 6 was "to ensure that acts of self-determination occur within the established boundaries of colonies, rather than within sub-regions". Further, the use of the word "attempt" in Paragraph 6 denotes future action and cannot be construed to justify territorial redress for past action. An attempt sponsored by Spain and Argentina to qualify the right to self-determination in cases where there was a territorial dispute was rejected by the UN General Assembly, which re-iterated the right to self-determination was a universal right.
Methods of increasing minority rights.
In order to accommodate demands for minority rights and avoid secession and the creation of a separate new state, many states decentralize or devolve greater decision-making power to new or existing subunits or autonomous areas. More limited measures might include restricting demands to the maintenance of national cultures or granting non-territorial autonomy in the form of national associations which would assume control over cultural matters. This would be available only to groups that abandoned secessionist demands and the territorial state would retain political and judicial control, but only if would remain with the territorially organized state.
Self-determination versus majority rule/equal rights.
Pavković explores how national self-determination, in the form of creation of a new state through secession, could override the principles of majority rule and of equal rights, which are primary liberal principles. This includes the question of how an unwanted state can be imposed upon a minority. He explores five contemporary theories of secession. In "anarcho-capitalist" theory only landowners have the right to secede. In communitarian theory, only those groups that desire direct or greater political participation have the right, including groups deprived of rights, per Allen Buchanan. In two nationalist theories, only national cultural groups have a right to secede. Australian professor Harry Beran's democratic theory endorses the equality of the right of secession to all types of groups. Unilateral secession against majority rule is justified if the group allows secession of any other group within its territory.
Constitutional law.
Most sovereign states do not recognize the right to self-determination through secession in their constitutions. Many expressly forbid it. However, there are several existing models of self-determination through greater autonomy and through secession.
In liberal constitutional democracies the principle of majority rule has dictated whether a minority can secede. In the United States Abraham Lincoln acknowledged that secession might be possible through amending the United States Constitution. The Supreme Court in "Texas v. White", held secession could occur "through revolution, or through consent of the States. " The British Parliament in 1933 held that Western Australia only could secede from Australia upon vote of a majority of the country as a whole; the previous two-thirds majority vote for secession via referendum in Western Australia was insufficient.
The Chinese Communist Party followed the Soviet Union in including the right of secession in its 1931 constitution in order to entice ethnic nationalities and Tibet into joining. However, the Party eliminated the right to secession in later years, and had anti-secession clause written into the Constitution before and after the founding the People's Republic of China. The 1947 Constitution of the Union of Burma contained an express state right to secede from the union under a number of procedural conditions. It was eliminated in the 1974 constitution of the Socialist Republic of the Union of Burma (officially the "Union of Myanmar"). Burma still allows "local autonomy under central leadership. "
As of 1996 the constitutions of Austria, Ethiopia, France, Singapore, Saint Kitts and Nevis Republics have express or implied rights to secession. Switzerland allows for the secession from current and the creation of new cantons. In the case of proposed Quebec separation from Canada the Supreme Court of Canada in 1998 ruled that only both a clear majority of the province and a constitutional amendment confirmed by all participants in the Canadian federation could allow secession.
The 2003 draft of the European Union Constitution allowed for the voluntary withdrawal of member states from the union, although the State wanted to leave could not be involved in the vote deciding whether or not they can leave the Union. There was much discussion about such self-determination by minorities before the final document underwent the unsuccessful ratification process in 2005.
Drawing new borders.
In determining international borders between sovereign states, self-determination has yielded to a number of other principles. Once groups exercise self-determination through secession, the issue of the proposed borders may prove more controversial than the fact of secession. The bloody Yugoslav wars in the 1990s were related mostly to borders issues because the international community applied a version of uti possidetis juris in transforming existing internal borders of the various Yugoslav republics into international borders, despite the conflicts of ethnic groups within those boundaries. In the 1990s indigenous populations of the northern two-thirds of Quebec state opposed to being incorporated into a Quebec nation and stated a determination to resist it by force.
The border between Northern Ireland and the Irish Free State was based on the borders of existing counties and did not include all of historic Ulster. A Boundary Commission was established to consider re-drawing it. Its proposals, which amounted to a small net transfer to Northern Ireland, were leaked to the press and then not acted upon. In December 1925, the governments of the Irish Free State, Northern Ireland, and the United Kingdom agreed to accept the existing border. Most Irish Nationalists and Irish Republicans claim all of Northern Ireland and are not particularly interested in new borders. 
Notable cases of self-determination.
There have been a number of notable cases of self-determination. For more information on past movements see list of historical autonomist and secessionist movements and lists of decolonized nations. Also see list of autonomous areas by country and list of territorial autonomies and list of active autonomist and secessionist movements.
Australia.
Recently (2003 onwards), self-determination has become the topic of some debate in Australia in relation to Aboriginal and Torres Strait Islander people. In the 1970s, the Indigenous community approached the Federal Government and requested the right to administer their own communities. This encompassed basic local government functions, ranging from land dealings and management of community centres to road maintenance and garbage collection, as well as setting education programmes and standards in their local schools.
Azawad.
The traditional homeland of the Tuareg peoples was divided up by the modern borders of Mali, Algeria and Niger. Numerous rebellions occurred over the decades, but in 2012 the Tuaregs succeeded in occupying their land and declaring the independence of Azawad. However, their movement was hijacked by the Islamist terrorist group, Ansar Dine.
Basque Country.
The Basque Country (Basque: "Euskal Herria", Spanish: "País Vasco", French: "Pays Basque") as a cultural region (not to be confused with the homonym Autonomous Community of the Basque country) is a European region in the western Pyrenees that spans the border between France and Spain, on the Atlantic coast. It comprises the autonomous communities of the Basque Country and Navarre in Spain and the Northern Basque Country in France.
Since the 19th century, Basque nationalism has demanded the right of some kind of self-determination. This desire for independence is particularly stressed among leftist Basque nationalists. The right of self-determination was asserted by the Basque Parliament in 1990, 2002 and 2006.
Since self-determination is not recognized in the Spanish Constitution of 1978, some Basques abstained and some voted against it in the referendum of December 6 of that year. It was approved by a clear majority at the Spanish level, and with 74. 6% of the votes in the Basque Country. However, the overall turnout in the Basque Country was 45% when the Spanish overall turnover was 67, 91%. The derived autonomous regime for the BAC was approved by Spanish Parliament and also by the Basque citizens in referendum. The autonomous statue of Navarre ("Amejoramiento del Fuero": "improvement of the charter") was approved by the Spanish Parliament and, like the statues of 13 out 17 Spanish autonomous communities, it didn´t need a referendum to enter into force.
"Euskadi Ta Askatasuna" or ETA (English: Basque Homeland and Freedom; pronounced ]), is an armed Basque nationalist, separatist and terrorist organization. Founded in 1959, it evolved from a group advocating traditional cultural ways to a paramilitary group with the goal of Basque independence. Its ideology is Marxist-Leninist.
Biafra.
From 1999 to the present day, the indigenous people of Biafra have been agitating for independence to revive their country. They have registered a human rights organization known as Bilie Human Rights Initiative both in Nigeria and in the United Nations to advocate for their right to self-determination and achieve independence by the rule of law.
Canada.
In Canada, many in the province of Quebec have wanted the province to separate from Confederation. The Parti Québécois has asserted Quebec's "right to self-determination. " There is debate on under which conditions would this right be realized. French-speaking Quebec nationalism and support for maintaining Québécois culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.
Catalonia.
After the 2012 Catalan march for independence, in which more than 1. 5 million citizens marched, the President of Catalonia, Artur Mas, called for new parliamentary elections on 25 November 2012 to elect a new parliament that would exercise the right of self-determination for Catalonia. The Parliament of Catalonia voted to hold a 'referendum or consultation' in the next four-year legislature in which the people of Catalonia would decide on becoming a new independent and sovereign state. The parliamentary decision was approved by a large majority of MPs: 84 voted for, 21 voted against, and 25 abstained. On December 2013 the President of the Generalitat Artur Mas and the governing coalition agreed to set the referendum for self-determination on 9 November 2014.
Chechnya.
Under Dzhokhar Dudayev, Chechnya declared independence as the Chechen Republic of Ichkeria, using self-determination, Russia's history of bad treatment of Chechens, and a history of independence before invasion by Russia as main motives. Russia has restored control over Chechnya, but the separatist government functions still in exile, though it has been split into two entities: the Akhmed Zakayev-run secular Chechen Republic (based in Poland, the UK and the USA), and the Islamic Caucasus Emirate.
Darfur.
The Justice and Equality Movement has called for greater autonomy and self-determination of the Darfuri peoples. However, it is unclear whether they seek full independence or just greater autonomy.
Easter Island.
The illegal occupation of Easter Island has been met with resistance by self-determination activists in recent years.
Falkland Islands.
Self-determination is referred to in the Falkland Islands Constitution and is a factor in the Falkland Islands sovereignty dispute. The population has existed for over nine generations, continuously for over 175 years. In the 2012 referendum organised by the Falkland Islands Government, 99. 8% voted to remain British. The British Government considers that since the majority of inhabitants wish to remain British, transfer of sovereignty to Argentina would be counter to their right to self-determination.
Argentina states that the principle of self-determination is not applicable since the UK illegally occupied the islands in 1833 expelling the Argentine authorities and preventing their return, and the future establishment of Argentines from the mainland.
Gibraltar.
The right to self-determination is referred to in the pre-amble of Chapter 1 of the Gibraltar constitution, and, since the United Kingdom also gave assurances that the right to self-determination of Gibraltarians would be respected in any transfer of sovereignty over the territory, is a factor in the dispute with Spain over the territory. The impact of the right to self-determination of Gibraltarians was seen in the 2002 Gibraltar sovereignty referendum, where Gibraltarian voters overwhelmingly rejected a plan to share sovereignty over Gibraltar between the UK and Spain. However, the UK government differs with the Gibraltarian government in that it considers Gibraltarian self-determination to be limited by the Treaty of Utrecht, which prevents Gibraltar achieving independence without the agreement of Spain, a position that the Gibraltarian government does not accept. 
The Spanish government denies that Gibraltarians have the right to self-determination, considering them to be "an artificial population without any genuine autonomy" and not "indigenous". However, the Partido Andalucista has agreed to recognise the right to self-determination of Gibraltarians.
Kurdistan.
Kurdistan is a historical region primarily inhabited by the Kurdish people of the middle east. The territory is currently part of 4 states Turkey, Iraq, Syria and Iran. There are Kurdish self-determination movements in each of the 4 states. Iraqi Kurdistan has to date achieved the largest degree of self-determination through the formation of the Kurdistan Regional Government, an entity recognised by the Iraqi Federal Constitution.
Although the right of the creation of a Kurdish state was recognized following World War I in the Treaty of Sèvres, the treaty was then annulled by the Treaty of Lausanne. To date two separate Kurdish republics and one Kurdish Kingdom have declared sovereignty. The Republic of Ararat (Ağrı Province, Turkey), the Republic of Mehabad (West Azerbaijan Province, Iran) and the Kingdom of Kurdistan (Sulaymaniyah Province, Iraqi Kurdistan, Iraq), each of these fledgling states was crushed by military intervention. The Patriotic Union of Kurdistan which currently holds the Iraqi presidency and the Kurdistan Democratic Party which governs the Kurdistan Regional Government both explicitly commit themselves to the development of Kurdish self-determination, but opinions vary as to the question of self-determination sought within the current borders and countries.
Kachinland.
KACHINLAND - a country bordered with Peoples Republic of China in the East, Democratic Republic of India in the West, Tibet in the North and Burma in the South. About 50% of the total area of the Kachinland is hills and mountains up to the height of 5, 881 metres above the sea level. It lies between north latitude 23° 27' and 28° 25' longitude 96° 0' and 98° 44'. The area of Kachinland is 89, 041 km2 (34, 379 sq mi). The capital of the country is Myitkyina.
Nagalim.
Naga refers to a vaguely-defined conglomeration of distinct tribes living on the border of India and Burma. Each of these tribes lived in a sovereign village before the arrival of the British, but developed a common identity as the area was Christianized. After the British let India, a section of Nagas under the leadership of Angami Zapu Phizo sought to establish a separate country for the Nagas. Phizo's group, the Naga National Council (NNC), claimed that 99. 9% of the Nagas wanted an independent Naga country according to a referendum conducted by it. It waged a secessionist insurgency against the Government of India. The NNC collapsed after Phizo got his dissenters killed or forced them to seek refuge with the Government. Phizo escaped to London, while NNC's successor secessionist groups continued to stage violent attacks against the Indian Government. The Naga People's Convention (NPC), another major Naga organization, was opposed to the secessionists. Its efforts led to the creation of a separate Nagaland state within India in 1963. The secessionst violence declined considerably ater the Shillong Accord of 1975. However, three factions of the National Socialist Council of Nagaland (NSCN) continue to seek an independent country which would include parts of India and Burma. They envisage a sovereign, predominantly Christian nation called "Nagalim".
South Africa.
Section 235 of the South African Constitution allows for the right to self-determination of a community, within the framework of "the right of the South African people as a whole to self-determination", and pursuant to national legislation. This section of the constitution was one of the negotiated settlements during the handing over of political power in 1994. Supporters of an independent Afrikaner homeland have argued that their goals are reasonable under this new legislation.
Turkish Cypriots.
Since Turkey's invasion and continued occupation of Cyprus in 1974, following ethnic clashes and turmoil on the island, an administration recognized by Turkey only was declared in 1983 – the Turkish Republic of Northern Cyprus. Turkish Cypriots and their former leader, Fazıl Küçük said that Turkish Cypriots had the right of self-determination, as well as Greek Cypriots. The Turkish Cypriots prior to the invasion constituted an 18% minority and were not concentrated in a specific region of the island. Only after the forced removal of the Greek Cypriots from the North of Cyprus did they form a local majority.
United States.
The colonization of the North American continent and its Native American population has been the source of legal battles since the early 19th century. Surviving Native Americans have been resettled onto separate tracts of land (reservations), which have retained a certain degree of autonomy within the United States. The federal government recognizes Tribal Sovereignty and has established a number of laws attempting to clarify the relationship between the federal, state, and tribal governments. The Constitution and later federal laws recognize the local sovereignty of tribal nations, but do not recognize full sovereignty equivalent to that of foreign nations, hence the term "domestic dependent nations".
Certain Chicano nationalist groups seek to "recreate" an ethnic-based state called Aztlán, the legendary homeland of the Aztecs comprising the Southwestern United States. Furthermore, black nationalists have a claim to the black belt region of the United States, from which to form an independent African-American state of New Afrika.
There are multiple active Hawaiian autonomy or independence movements each with their own distinct modality of realizing some level of political control over single or several islands. The groups range from those seeking territorial units similar to Indian reservations under the United States with the least amount of independent control to the Hawaiian sovereignty movement which would have the most amount of control, The Hawaiian Sovereignty movement aims at reviving the Hawaiian nation under the Hawaiian constitution, which supposedly has while under control of the United States.
Since 1972, the U.N. Decolonization Committee has called for Puerto Rico's decolonization and for the U.S. to recognize the island's right to self-determination and independence. In 2007 the Decolonization Subcommittee called for the United Nations General Assembly to review the political status of Puerto Rico, a power reserved by the 1953 Resolution. This follows the 1967 passage of a plebiscite act that provided for a vote on the status of Puerto Rico with three status options: continued commonwealth, statehood, and independence. In the first plebscite the commonwealth option won with 60.4% of the votes but U.S. congressional committees failed to enact legislation to address the status issue. In subsequent plebiscites in 1993 and 1998, the status quo was upheld. In a referendum that took place in November 2012, a majority Puerto Rican residents voted to change the territory's relationship with the United States, with the statehood option apparently being the preferred option, however a large number of ballots—one-third of all votes cast—were left blank on the question of preferred alternative status. When counted, the blank votes are viewed as anti-statehood votes, resulting that the statehood option would have received less than 50% of all ballots received. As of January 2014, Washington has not taken action to address the results of this plebiscite.
Many current U.S. state, regional and city secession groups use the language of self-determination. A 2008 Zogby International poll revealed that 22% of Americans believe that "any state or region has the right to peaceably secede and become an independent republic. "
It is important to note that in the case of Hawaii, the struggle for self-determination does not fall under secession, as it is less a break from federal administration, than a return to the process through which cession was claimed to have occurred: namely the ongoing occupation via a US imposed military coup; and/or removal from the UN list of Non-Self-Governing Territories. The US largely ignored its Sacred Trust to educate or properly inform the citizenry of Hawaii of its options for self-determination and sidestepped guidelines laid out in UN General Assembly resolution 742 (1953). U.N. Resolution 742 declares that one of the “factors indicative of the attainment of independence or of other separate systems of self-government, ” is “freedom of choosing on the basis of the right of self-determination of peoples between several possibilities including independence. ”
West Papua.
The self-determination of the West Papuan people has been violently suppressed by the Indonesian government and ignored by the international community.
Western Sahara.
There is an active secessionist movement based on the self-determination of the Sahrawi people against the illegal occupation and settlement of their territory by the Moroccan government.
Bibliography.
</dl>

</doc>
<doc id="29271" url="http://en.wikipedia.org/wiki?curid=29271" title="Scale">
Scale

Scale or scales may refer to:

</doc>
<doc id="29275" url="http://en.wikipedia.org/wiki?curid=29275" title="Southcentral Alaska">
Southcentral Alaska

Southcentral Alaska is the portion of the U.S. state of Alaska consisting of the shorelines and uplands of the central Gulf of Alaska. Most of the population of the state lives in this region, concentrated in and around the city of Anchorage.
The area includes Cook Inlet, the Matanuska-Susitna Valley, the Kenai Peninsula, Prince William Sound, and the Copper River Valley. Tourism, fisheries, and petroleum production are important economic activities.
Cities.
The major city is Anchorage. Other towns include Palmer, Wasilla, Kenai, Soldotna, Homer, Seward, Valdez, and Cordova.
Climate.
The climate of Southcentral Alaska is subarctic. Temperatures range from an average high of 65°F (18°C) in July to an average low of 10°F (-12°C) in December. The hours of daylight per day varies from 18 hours in June and July to 6 hours in December and January. The coastal areas consist of temperate rainforests and alder shrublands. The interior areas are covered by boreal forests.
Mountains.
The terrain of Southcentral Alaska is shaped by five mountain ranges:
Southcentral Alaska contains several dormant and active volcanoes. The Wrangell Volcanoes are older, lie in the East, and include Mount Blackburn, Mount Bona, Mount Churchill, Mount Drum, Mount Gordon, Mount Jarvis, Mount Sanford, and Mount Wrangell. The Cook Inlet volcanoes are newer, lie in the West, and include Mount Redoubt, Mount Iliamna, Hayes Volcano, Mount Augustine, Fourpeaked Mountain and Mount Spurr. Most recently, Augustine and Fourpeaked erupted in 2006, and Mount Redoubt erupted in March 2009, resulting in airplane flight cancellations.
External links.
 

</doc>
<doc id="29276" url="http://en.wikipedia.org/wiki?curid=29276" title="Spinor">
Spinor

In geometry and physics, spinors are elements of a (complex) vector space that can be associated to Euclidean space. Like geometric vectors and more general tensors, spinors transform linearly when the Euclidean space is subjected to a slight (infinitesimal) rotation. When a sequence of such small rotations is composed (integrated) to form an overall final rotation, however, the resulting spinor transformation depends on which sequence of small rotations was used, "unlike" for vectors and tensors. A spinor transforms to its negative when the space is rotated through a complete turn from 0° to 360° (see picture), and it is this property that characterizes spinors. It is also possible to associate a substantially similar notion of spinor to Minkowski space in which case the Lorentz transformations of special relativity play the role of rotations. Spinors were introduced in geometry by Élie Cartan in 1913. In the 1920s physicists discovered that spinors are essential to describe the intrinsic angular momentum, or "spin", of the electron and other subatomic particles.
Spinors are characterized by the specific way in which they behave under rotations. They change in different ways depending not just on the overall final rotation, but the details of how that rotation was achieved (by a continuous path in the rotation group). There are two topologically distinguishable classes (homotopy classes) of paths through rotations that result in the same overall rotation, as famously illustrated by the belt trick puzzle (below). These two inequivalent classes yield spinor transformations of opposite sign. The spin group is the group of all rotations keeping track of the class. It doubly-covers the rotation group, since each rotation can be obtained in two inequivalent ways as the endpoint of a path. The space of spinors by definition is equipped with a (complex) linear representation of the spin group, meaning that elements of the spin group act as linear transformations on the space of spinors, in a way that genuinely depends on the homotopy class.
Although spinors can be defined purely as elements of a representation space of the spin group (or its Lie algebra of infinitesimal rotations), they are typically defined as elements of a vector space that carries a linear representation of the Clifford algebra. The Clifford algebra is an associative algebra that can be constructed from Euclidean space and its inner product in a basis independent way. Both the spin group and its Lie algebra are embedded inside the Clifford algebra in a natural way, and in applications the Clifford algebra is often the easiest to work with. After choosing an orthonormal basis of Euclidean space, a representation of the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations. The spinors are the column vectors on which these matrices act. In three Euclidean dimensions, for instance, the Pauli spin matrices are a set of gamma matrices, and the two-component complex column vectors on which these matrices act are spinors. However, the particular matrix representation of the Clifford algebra, and hence what precisely constitutes a "column vector" (or spinor), involves the choice of basis and gamma matrices in an essential way. As a representation of the spin group, this realization of spinors as (complex) column vectors will either be irreducible if the dimension is odd, or it will decompose into a pair of so-called "half-spin" or Weyl representations if the dimension is even.
Introduction.
What characterizes spinors and distinguishes them from geometric vectors and other tensors is subtle. Consider applying a rotation to the coordinates of a system. No object in the system itself has moved, only the coordinates have, so there will always be a compensating change in those coordinate values when applied to any object of the system. Geometrical vectors, for example, have components that will undergo "the same" rotation as the coordinates. More broadly, any tensor associated with the system (for instance, the stress of some medium) also has coordinate descriptions that adjust to compensate for changes to the coordinate system itself. Spinors do not appear at this level of the description of a physical system, when one is concerned only with the properties of a single isolated rotation of the coordinates. Rather, spinors appear when we imagine that instead of a single rotation, the coordinate system is gradually (continuously) rotated between some initial and final configuration. For any of the familiar and intuitive ("tensorial") quantities associated with the system, the transformation law does not depend on the precise details of how the coordinates arrived at their final configuration. Spinors, on the other hand, are constructed in such a way that makes them "sensitive" to how the gradual rotation of the coordinates arrived there: they exhibit path-dependence. It turns out that, for any final configuration of the coordinates, there are actually two ("topologically") inequivalent "gradual" (continuous) rotations of the coordinate system that result in this same configuration. This ambiguity is called the homotopy class of the gradual rotation. The belt trick puzzle (shown) famously demonstrates two different rotations, one through an angle of 2π and the other through an angle of 4π, having the same final configurations but different classes. Spinors actually exhibit a sign-reversal that genuinely depends on this homotopy class. This distinguishes them from vectors and other tensors, none of which can feel the class.
Spinors can be exhibited as concrete objects using a choice of Cartesian coordinates. In three Euclidean dimensions, for instance, spinors can be constructed by making a choice of Pauli spin matrices corresponding to (angular momenta about) the three coordinate axes. These are 2×2 matrices with complex entries, and the two-component complex column vectors on which these matrices act by matrix multiplication are the spinors. In this case, the spin group is isomorphic to the group of 2×2 unitary matrices with determinant one, which naturally sits inside the matrix algebra. This group acts by conjugation on the real vector space spanned by the Pauli matrices themselves, realizing it as a group of rotations among them, but it also acts on the column vectors (that is, the spinors).
More generally, a Clifford algebra can be constructed from any vector space V equipped with a (nondegenerate) quadratic form, such as Euclidean space with its standard dot product or Minkowski space with its standard Lorentz metric. Given a suitably normalized basis of V, the Clifford algebra is generated by gamma matrices, matrices that satisfy a set of canonical anti-commutation relations, and the space of spinors is the space of column vectors with formula_1 components on which those matrices act. Although the Clifford algebra can be defined abstractly in a coordinate-independent way, its particular realization as a specific algebra of matrices depends on which orthogonal axes the gamma matrices represent. So what precisely constitutes a "column vector" (or spinor) also depends on such arbitrary choices. The orthogonal Lie algebra (i.e., the infinitesimal "rotations") and the spin group associated to the quadratic form are both (canonically) contained in the Clifford algebra, so every Clifford algebra representation also defines a representation of the Lie algebra and the spin group. Depending on the dimension and metric signature, this realization of spinors as column vectors may be irreducible or it may decompose into a pair of so-called "half-spin" or Weyl representations.
Overview.
There are essentially two frameworks for viewing the notion of a spinor.
One is representation theoretic. In this point of view, one knows beforehand that there are some representations of the Lie algebra of the orthogonal group that cannot be formed by the usual tensor constructions. These missing representations are then labeled the spin representations, and their constituents "spinors". In this view, a spinor must belong to a representation of the double cover of the rotation group SO("n", R), or more generally of double cover of the generalized special orthogonal group SO+("p", "q", R) on spaces with metric signature ("p", "q"). These double covers are Lie groups, called the spin groups Spin("n") or Spin("p", "q"). All the properties of spinors, and their applications and derived objects, are manifested first in the spin group. Representations of the double covers of these groups yield projective representations of the groups themselves, which do not meet the full definition of a representation.
The other point of view is geometrical. One can explicitly construct the spinors, and then examine how they behave under the action of the relevant Lie groups. This latter approach has the advantage of providing a concrete and elementary description of what a spinor is. However, such a description becomes unwieldy when complicated properties of spinors, such as Fierz identities, are needed.
Clifford algebras.
The language of Clifford algebras (sometimes called geometric algebras) provides a complete picture of the spin representations of all the spin groups, and the various relationships between those representations, via the classification of Clifford algebras. It largely removes the need for "ad hoc" constructions.
In detail, let "V" be a finite-dimensional complex vector space with nondegenerate bilinear form "g". The Clifford algebra Cℓ("V", "g") is the algebra generated by "V" along with the anticommutation relation "xy" + "yx" = 2"g"("x", "y"). It is an abstract version of the algebra generated by the gamma or Pauli matrices. If "V" = C"n", with the standard form "g"("x", "y") = "x"t"y" = "x"1"y"1 + ... + "x"n"y""n" we denote the Clifford algebra by Cℓ"n"(C). Since by the choice of an orthonormal basis every complex vectorspace with non-degenerate form is isomorphic to this standard example, this notation is abused more generally if dimC("V") = "n". If "n" = 2"k" is even, Cℓ"n"(C) is isomorphic as an algebra (in a non-unique way) to the algebra Mat(2"k", C) of 2"k" × 2"k" complex matrices (by the Artin-Wedderburn theorem and the easy to prove fact that the Clifford algebra is central simple). If "n" = 2"k" + 1 is odd, Cℓ2"k"+1(C) is isomorphic to the algebra Mat(2"k", C) ⊕ Mat(2"k", C) of two copies of the 2"k" × 2"k" complex matrices. Therefore, in either case Cℓ("V", "g") has a unique (up to isomorphism) irreducible representation (also called simple Clifford module), commonly denoted by Δ, of dimension 2["n"/2]. Since the Lie algebra so("V", "g") is embedded as a Lie subalgebra in Cℓ("V", "g") equipped with the Clifford algebra commutator as Lie bracket, the space Δ is also a Lie algebra representation of so("V", "g") called a spin representation. If "n" is odd, this Lie algebra representation is irreducible. If "n" is even, it splits further into two irreducible representations Δ = Δ+ ⊕ Δ− called the Weyl or "half-spin representations".
Irreducible representations over the reals in the case when "V" is a real vector space are much more intricate, and the reader is referred to the Clifford algebra article for more details.
Spin groups.
Spinors form a vector space, usually over the complex numbers, equipped with a linear group representation of the spin group that does not factor through a representation of the group of rotations (see diagram). The spin group is the group of rotations keeping track of the homotopy class. Spinors are needed to encode basic information about the topology of the group of rotations because that group is not simply connected, but the simply connected spin group is its double cover. So for every rotation there are two elements of the spin group that represent it. Geometric vectors and other tensors cannot feel the difference between these two elements, but they produce "opposite" signs when they affect any spinor under the representation. Thinking of the elements of the spin group as homotopy classes of one-parameter families of rotations, each rotation is represented by two distinct homotopy classes of paths to the identity. If a one-parameter family of rotations is visualized as a ribbon in space, with the arc length parameter of that ribbon being the parameter (its tangent, normal, binormal frame actually gives the rotation), then these two distinct homotopy classes are visualized in the two states of the belt trick puzzle (above). The space of spinors is an auxiliary vector space that can be constructed explicitly in coordinates, but ultimately only exists up to isomorphism in that there is no "natural" construction of them that does not rely on arbitrary choices such as coordinate systems. A notion of spinors can be associated, as such an auxiliary mathematical object, with any vector space equipped with a quadratic form such as Euclidean space with its standard dot product, or Minkowski space with its Lorentz metric. In the latter case, the "rotations" include the Lorentz boosts, but otherwise the theory is substantially similar.
Terminology in physics.
The most typical type of spinor, the Dirac spinor, is an element of the fundamental representation of Cℓ"p"+"q"(C), the complexification of the Clifford algebra Cℓ"p", "q"(R), into which the spin group Spin("p", "q") may be embedded. On a 2"k"- or 2"k"+1-dimensional space a Dirac spinor may be represented as a vector of 2"k" complex numbers. (See Special unitary group.) In even dimensions, this representation is reducible when taken as a representation of Spin("p", "q") and may be decomposed into two: the left-handed and right-handed Weyl spinor representations. In addition, sometimes the non-complexified version of Cℓ"p","q"(R) has a smaller real representation, the Majorana spinor representation. If this happens in an even dimension, the Majorana spinor representation will sometimes decompose into two Majorana–Weyl spinor representations.
Of all these, only the Dirac representation exists in all dimensions. Dirac and Weyl spinors are complex representations while Majorana spinors are real representations.
The Dirac, Lorentz, Weyl, and Majorana spinors are interrelated, and their relation can be elucidated on the basis of real geometric algebra.
Spinors in representation theory.
One major mathematical application of the construction of spinors is to make possible the explicit construction of linear representations of the Lie algebras of the special orthogonal groups, and consequently spinor representations of the groups themselves. At a more profound level, spinors have been found to be at the heart of approaches to the Atiyah–Singer index theorem, and to provide constructions in particular for discrete series representations of semisimple groups.
The spin representations of the special orthogonal Lie algebras are distinguished from the tensor representations given by Weyl's construction by the weights. Whereas the weights of the tensor representations are integer linear combinations of the roots of the Lie algebra, those of the spin representations are half-integer linear combinations thereof. Explicit details can be found in the spin representation article.
Attempts at intuitive understanding.
The spinor can be described, in simple terms, as “vectors of a space the transformations of which are related in a particular way to rotations in physical space”. Stated differently:
Several ways of illustrating everyday analogies have been formulated in terms of the plate trick, tangloids and other examples of orientation entanglement.
Nonetheless, the concept is generally considered notoriously difficult to understand, as illustrated by Michael Atiyah's statement that is recounted by Dirac's biographer Graham Farmelo:
History.
The most general mathematical form of spinors was discovered by Élie Cartan in 1913. The word "spinor" was coined by Paul Ehrenfest in his work on quantum physics.
Spinors were first applied to mathematical physics by Wolfgang Pauli in 1927, when he introduced his spin matrices. The following year, Paul Dirac discovered the fully relativistic theory of electron spin by showing the connection between spinors and the Lorentz group. By the 1930s, Dirac, Piet Hein and others at the Niels Bohr Institute (then known as the Institute for Theoretical Physics of the University of Copenhagen) created toys such as Tangloids to teach and model the calculus of spinors.
Spinor spaces were represented as left ideals of a matrix algebra in 1930, by G. Juvet and by Fritz Sauter. More specifically, instead of representing spinors as complex-valued 2D column vectors as Pauli had done, they represented them as complex-valued 2 × 2 matrices in which only the elements of the left column are non-zero. In this manner the spinor space became a minimal left ideal in Mat(2, C).
In 1947 Marcel Riesz constructed spinor spaces as elements of a minimal left ideal of Clifford algebras. In 1966/1967, David Hestenes replaced spinor spaces by the even subalgebra Cℓ01,3(R) of the spacetime algebra Cℓ1,3(R). As of the 1980s, the theoretical physics group at Birkbeck College around David Bohm and Basil Hiley has been developing algebraic approaches to quantum theory that build on Sauter and Riesz' identification of spinors with minimal left ideals.
Examples.
Some simple examples of spinors in low dimensions arise from considering the even-graded subalgebras of the Clifford algebra Cℓ"p", "q"(R). This is an algebra built up from an orthonormal basis of "n" = "p" + "q" mutually orthogonal vectors under addition and multiplication, "p" of which have norm +1 and "q" of which have norm −1, with the product rule for the basis vectors
Two dimensions.
The Clifford algebra Cℓ2,0(R) is built up from a basis of one unit scalar, 1, two orthogonal unit vectors, "σ"1 and "σ"2, and one unit pseudoscalar "i" = "σ"1"σ"2. From the definitions above, it is evident that ("σ"1")2 = ("σ"2")2 = 1, and ("σ"1"σ"2)("σ"1"σ"2) = −"σ"1"σ"1"σ"2"σ"2 = −1.
The even subalgebra Cℓ02,0(R), spanned by "even-graded" basis elements of Cℓ2,0(R), determines the space of spinors via its representations. It is made up of real linear combinations of 1 and "σ"1"σ"2. As a real algebra, Cℓ02,0(R) is isomorphic to field of complex numbers C. As a result, it admits a conjugation operation (analogous to complex conjugation), sometimes called the "reverse" of a Clifford element, defined by
which, by the Clifford relations, can be written
The action of an even Clifford element "γ" ∈ Cℓ02,0(R) on vectors, regarded as 1-graded elements of Cℓ2,0(R), is determined by mapping a general vector "u" = "a"1"σ"1 + "a"2"σ"2 to the vector
where "γ"∗ is the conjugate of "γ", and the product is Clifford multiplication. In this situation, a spinor is an ordinary complex number. The action of "γ" on a spinor "φ" is given by ordinary complex multiplication:
An important feature of this definition is the distinction between ordinary vectors and spinors, manifested in how the even-graded elements act on each of them in different ways. In general, a quick check of the Clifford relations reveals that even-graded elements conjugate-commute with ordinary vectors:
On the other hand, comparing with the action on spinors "γ"("φ") = "γφ", "γ" on ordinary vectors acts as the "square" of its action on spinors.
Consider, for example, the implication this has for plane rotations. Rotating a vector through an angle of "θ" corresponds to "γ"2 = exp("θ σ"1"σ"2), so that the corresponding action on spinors is via "γ" = ± exp("θ σ"1"σ"2/2). In general, because of logarithmic branching, it is impossible to choose a sign in a consistent way. Thus the representation of plane rotations on spinors is two-valued.
In applications of spinors in two dimensions, it is common to exploit the fact that the algebra of even-graded elements (that is just the ring of complex numbers) is identical to the space of spinors. So, by abuse of language, the two are often conflated. One may then talk about "the action of a spinor on a vector." In a general setting, such statements are meaningless. But in dimensions 2 and 3 (as applied, for example, to computer graphics) they make sense.
Three dimensions.
The Clifford algebra Cℓ3,0(R) is built up from a basis of one unit scalar, 1, three orthogonal unit vectors, "σ"1, "σ"2 and "σ"3, the three unit bivectors "σ"1"σ"2, "σ"2"σ"3, "σ"3"σ"1 and the pseudoscalar "i" = "σ"1"σ"2"σ"3. It is straightforward to show that ("σ"1)2 = ("σ"2)2 = ("σ"3)2 = 1, and ("σ"1"σ"2)2 = ("σ"2"σ"3)2 = ("σ"3"σ"1)2 = ("σ"1"σ"2"σ"3)2 = −1.
The sub-algebra of even-graded elements is made up of scalar dilations,
and vector rotations
where
corresponds to a vector rotation through an angle "θ" about an axis defined by a unit vector "v" = "a"1"σ"1 + "a"2"σ"2 + "a"3"σ"3.
As a special case, it is easy to see that, if "v" = "σ"3, this reproduces the "σ"1"σ"2 rotation considered in the previous section; and that such rotation leaves the coefficients of vectors in the "σ"3 direction invariant, since
The bivectors "σ"2"σ"3, "σ"3"σ"1 and "σ"1"σ"2 are in fact Hamilton's quaternions i, j and k, discovered in 1843:
With the identification of the even-graded elements with the algebra H of quaternions, as in the case of two dimensions the only representation of the algebra of even-graded elements is on itself. Thus the (real) spinors in three-dimensions are quaternions, and the action of an even-graded element on a spinor is given by ordinary quaternionic multiplication.
Note that the expression (1) for a vector rotation through an angle "θ", the angle appearing in "γ" was halved. Thus the spinor rotation "γ"("ψ") = "γψ" (ordinary quaternionic multiplication) will rotate the spinor "ψ" through an angle one-half the measure of the angle of the corresponding vector rotation. Once again, the problem of lifting a vector rotation to a spinor rotation is two-valued: the expression (1) with (180° + "θ"/2) in place of "θ"/2 will produce the same vector rotation, but the negative of the spinor rotation.
The spinor/quaternion representation of rotations in 3D is becoming increasingly prevalent in computer geometry and other applications, because of the notable brevity of the corresponding spin matrix, and the simplicity with which they can be multiplied together to calculate the combined effect of successive rotations about different axes.
Explicit constructions.
A space of spinors can be constructed explicitly with concrete and abstract constructions. The
equivalence of these constructions are a consequence of the uniqueness of the spinor representation of the complex Clifford algebra. For a complete example in dimension 3, see spinors in three dimensions.
Component spinors.
Given a vector space "V" and a quadratic form "g" an explicit matrix representation of the Clifford algebra Cℓ("V", "g") can be defined as follows. Choose an orthonormal basis "e"1 … "e"n for "V" i.e. "g"("e""μ""e""ν") = "η""μν" where "η""μμ" = ±1 and "η""μν" = 0 for "μ" ≠ "ν". Let "k" = ⌊ "n"/2 ⌋. Fix a set of 2"k" × 2"k" matrices "γ"1 … "γ""n" such that "γ""μ""γ""ν" + "γ""ν""γ""μ" = 2"η""μν"1 (i.e. fix a convention for the gamma matrices). Then the assignment "e""μ" → "γ""μ" extends uniquely to an algebra homomorphism Cℓ("V", "g") → Mat(2"k", C) by sending the monomial "e""μ"1 … "e""μ""k" in the Clifford algebra to the product "γ""μ"1 … "γ""μ""k" of matrices and extending linearly. The space Δ = C2"k" on which the gamma matrices act is a now a space of spinors. One needs to construct such matrices explicitly, however. In dimension 3, defining the gamma matrices to be the Pauli sigma matrices gives rise to the familiar two component spinors used in non relativistic quantum mechanics. Likewise using the 4 × 4 Dirac gamma matrices gives rise to the 4 component Dirac spinors used in 3+1 dimensional relativistic quantum field theory. In general, in order to define gamma matrices of the required kind, one can use the Weyl–Brauer matrices.
In this construction the representation of the Clifford algebra Cℓ("V", "g"), the Lie algebra so("V", "g"), and the Spin group Spin("V", "g"), all depend on the choice of the orthonormal basis and the choice of the gamma matrices. This can cause confusion over conventions, but invariants like traces are independent of choices. In particular, all physically observable quantities must be independent of such choices. In this construction a spinor can be represented as a vector of 2"k" complex numbers and is denoted with spinor indices (usually "α", "β", "γ"). In the physics literature, abstract spinor indices are often used to denote spinors even when an abstract spinor construction is used.
Abstract spinors.
There are at least two different, but essentially equivalent, ways to define spinors abstractly. One approach seeks to identify the minimal ideals for the left action of Cℓ("V", "g") on itself. These are subspaces of the Clifford algebra of the form Cℓ("V", "g")"ω", admitting the evident action of Cℓ("V", "g") by left-multiplication: "c" : "xω" → "cxω". There are two variations on this theme: one can either find a primitive element "ω" that is a nilpotent element of the Clifford algebra, or one that is an idempotent. The construction via nilpotent elements is more fundamental in the sense that an idempotent may then be produced from it. In this way, the spinor representations are identified with certain subspaces of the Clifford algebra itself. The second approach is to construct a vector space using a distinguished subspace of "V", and then specify the action of the Clifford algebra "externally" to that vector space.
In either approach, the fundamental notion is that of an isotropic subspace "W". Each construction depends on an initial freedom in choosing this subspace. In physical terms, this corresponds to the fact that there is no measurement protocol that can specify a basis of the spin space, even if a preferred basis of "V" is given.
As above, we let ("V", "g") be an "n"-dimensional complex vector space equipped with a nondegenerate bilinear form. If "V" is a real vector space, then we replace "V" by its complexification "V" ⊗R C and let "g" denote the induced bilinear form on "V" ⊗R C. Let "W" be a maximal isotropic subspace, i.e. a maximal subspace of "V" such that "g"|"W" = 0. If "n" =  2"k" is even, then let "W"∗ be an isotropic subspace complementary to "W". If "n" =  2"k" + 1 is odd, let "W"∗ be a maximal isotropic subspace with "W" ∩ "W"∗ = 0, and let "U" be the orthogonal complement of "W" ⊕ "W"∗. In both the even- and odd-dimensional cases "W" and "W"∗ have dimension "k". In the odd-dimensional case, "U" is one-dimensional, spanned by a unit vector "u".
Minimal ideals.
Since "W"′ is isotropic, multiplication of elements of "W"′ inside Cℓ("V", "g") is skew. Hence vectors in "W"′ anti-commute, and Cℓ("W"′, "g"|"W"′) = Cℓ("W"′, 0) is just the exterior algebra Λ∗"W"′. Consequently, the "k"-fold product of "W"′ with itself, "W"′"k", is one-dimensional. Let "ω" be a generator of "W"′"k". In terms of a basis "w"′1..., "w"′k of in "W"′, one possibility is to set
Note that "ω"2 = 0 (i.e., "ω" is nilpotent of order 2), and moreover, "w"′"ω" = 0 for all "w"′ ∈ "W"′. The following facts can be proven easily:
In detail, suppose for instance that "n" is even. Suppose that "I" is a non-zero left ideal contained in Cℓ("V", "g")"ω". We shall show that "I" must be equal to Cℓ("V", "g")"ω" by proving that it contains a nonzero scalar multiple of "ω".
Fix a basis "w"i of "W" and a complementary basis "w"i′ of "W"′ so that
Note that any element of "I" must have the form "αω", by virtue of our assumption that "I" ⊂ Cℓ("V", "g") "ω". Let "αω" ∈ "I" be any such element. Using the chosen basis, we may write
where the "a"i1…ip are scalars, and the "B"j are auxiliary elements of the Clifford algebra. Observe now that the product
Pick any nonzero monomial "a" in the expansion of "α" with maximal homogeneous degree in the elements "w"i:
then
is a nonzero scalar multiple of "ω", as required.
Note that for "n" even, this computation also shows that
as a vector space. In the last equality we again used that "W" is isotropic. In physics terms, this shows that Δ is built up like a Fock space by creating spinors using anti-commuting creation operators in "W" acting on a vacuum "ω".
Exterior algebra construction.
The computations with the minimal ideal construction suggest that a spinor representation can
also be defined directly using the exterior algebra Λ∗ "W" = ⊕"j" Λ"j" "W" of the isotropic subspace "W".
Let Δ = Λ∗ "W" denote the exterior algebra of "W" considered as vector space only. This will be the spin representation, and its elements will be referred to as spinors.
The action of the Clifford algebra on Δ is defined first by giving the action of an element of "V" on Δ, and then showing that this action respects the Clifford relation and so extends to a homomorphism of the full Clifford algebra into the endomorphism ring End(Δ) by the universal property of Clifford algebras. The details differ slightly according to whether the dimension of "V" is even or odd.
When dim("V") is even, "V" = "W" ⊕ "W"′ where "W"′ is the chosen isotropic complement. Hence any "v" ∈ "V" decomposes uniquely as "v" = "w" + "w"′ with "w" ∈ "W" and "w"′ ∈ "W"′. The action of "v" on a spinor is given by
where "i"("w"′) is interior product with "w"′ using the non degenerate quadratic form to identify "V" with "V"∗, and ε(w) denotes the exterior product. It may be verified that
and so "c" respects the Clifford relations and extends to a homomorphism from the Clifford algebra to End(Δ).
The spin representation Δ further decomposes into a pair of irreducible complex representations of the Spin group (the half-spin representations, or Weyl spinors) via
When dim("V") is odd, "V" = "W" ⊕ "U" ⊕ "W"′, where "U" is spanned by a unit vector "u" orthogonal to "W". The Clifford action "c" is defined as before on "W" ⊕ "W"′, while the Clifford action of (multiples of) "u" is defined by
As before, one verifies that "c" respects the Clifford relations, and so induces a homomorphism.
Hermitian vector spaces and spinors.
If the vector space "V" has extra structure that provides a decomposition of its complexification into two maximal isotropic subspaces, then the definition of spinors (by either method) becomes natural.
The main example is the case that the real vector space "V" is a hermitian vector space ("V", "h"), i.e., "V" is equipped with a complex structure "J" that is an orthogonal transformation with respect to the inner product "g" on "V". Then "V" ⊗R C splits in the ±"i" eigenspaces of "J". These eigenspaces are isotropic for the complexification of "g" and can be identified with the complex vector space ("V", "J") and its complex conjugate ("V", −"J"). Therefore for a hermitian vector space ("V", "h") the vector space Λ"V" (as well as its complex conjugate Λ"V") is a spinor space for the underlying real euclidean vector space.
With the Clifford action as above but with contraction using the hermitian form, this construction gives a spinor space at every point of an almost Hermitian manifold and is the reason why every almost complex manifold (in particular every symplectic manifold) has a Spinc structure. Likewise, every complex vector bundle on a manifold carries a Spinc structure.
Clebsch–Gordan decomposition.
A number of Clebsch–Gordan decompositions are possible on the tensor product of one spin representation with another. These decompositions express the tensor product in terms of the alternating representations of the orthogonal group.
For the real or complex case, the alternating representations are
In addition, for the real orthogonal groups, there are three characters (one-dimensional representations)
The Clebsch–Gordan decomposition allows one to define, among other things:
Even dimensions.
If "n" = 2"k" is even, then the tensor product of Δ with the contragredient representation decomposes as
which can be seen explicitly by considering (in the Explicit construction) the action of the Clifford algebra on decomposable elements "αω" ⊗ "βω"′. The rightmost formulation follows from the transformation properties of the Hodge star operator. Note that on restriction to the even Clifford algebra, the paired summands Γ"p" ⊕ "σ"Γ"p" are isomorphic, but under the full Clifford algebra they are not.
There is a natural identification of Δ with its contragredient representation via the conjugation in the Clifford algebra:
So Δ ⊗ Δ also decomposes in the above manner. Furthermore, under the even Clifford algebra, the half-spin representations decompose
For the complex representations of the real Clifford algebras, the associated reality structure on the complex Clifford algebra descends to the space of spinors (via the explicit construction in terms of minimal ideals, for instance). In this way, we obtain the complex conjugate Δ of the representation Δ, and the following isomorphism is seen to hold:
In particular, note that the representation Δ of the orthochronous spin group is a unitary representation. In general, there are Clebsch–Gordan decompositions
In metric signature ("p", "q"), the following isomorphisms hold for the conjugate half-spin representations
Using these isomorphisms, one can deduce analogous decompositions for the tensor products of the half-spin representations Δ± ⊗ Δ±.
Odd dimensions.
If "n" = 2"k" + 1 is odd, then
In the real case, once again the isomorphism holds
Hence there is a Clebsch–Gordan decomposition (again using the Hodge star to dualize) given by
Consequences.
There are many far-reaching consequences of the Clebsch–Gordan decompositions of the spinor spaces. The most fundamental of these pertain to Dirac's theory of the electron, among whose basic requirements are

</doc>
<doc id="29278" url="http://en.wikipedia.org/wiki?curid=29278" title="Safety engineering">
Safety engineering

Safety engineering is an engineering discipline which assures that engineered systems provide acceptable levels of safety. It is strongly related to systems engineering, industrial engineering and the subset system safety engineering. Safety engineering assures that a life-critical system behaves as needed, even when components fail.
Analysis techniques.
Analysis techniques can be split into two categories: qualitative and quantitative methods. Both approaches share the goal of finding causal dependencies between a hazard on system level and failures of individual components. Qualitative approaches focus on the question "What must go wrong, such that a system hazard may occur?", while quantitative methods aim at providing estimations about probabilities, rates and/or severity of consequences.
Traditionally, safety analysis techniques rely solely on skill and expertise of the safety engineer. In the last decade model-based approaches have become prominent. In contrast to traditional methods, model-based techniques try to derive relationships between causes and consequences from some sort of model of the system.
Traditional methods for safety analysis.
The two most common fault modeling techniques are called failure mode and effects analysis and fault tree analysis. These techniques are just ways of finding problems and of making plans to cope with failures, as in probabilistic risk assessment. One of the earliest complete studies using this technique on a commercial nuclear plant was the WASH-1400 study, also known as the Reactor Safety Study or the Rasmussen Report.
Failure modes and effects analysis.
Failure Mode and Effects Analysis (FMEA) is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level. For functional FMEA, failure modes are identified for each function in a system or equipment item, usually with the help of a functional block diagram. For piece-part FMEA, failure modes are identified for each piece-part component (such as a valve, connector, resistor, or diode). The effects of the failure mode are described, and assigned a probability based on the failure rate and failure mode ratio of the function or component. This quantiazation is difficult for software ---a bug exists or not, and the failure models used for hardware components do not apply. Temperature and age and manufacturing variability affect a resistor; they do not affect software.
Failure modes with identical effects can be combined and summarized in a Failure Mode Effects Summary. When combined with criticality analysis, FMEA is known as Failure Mode, Effects, and Criticality Analysis or FMECA, pronounced "fuh-MEE-kuh".
Fault tree analysis.
Fault tree analysis (FTA) is a top-down, deductive analytical method. In FTA, initiating primary events such as component failures, human errors, and external events are traced through Boolean logic gates to an undesired top event such as an aircraft crash or nuclear reactor core melt. The intent is to identify ways to make top events less probable, and verify that safety goals have been achieved.
Fault trees are a logical inverse of success trees, and may be obtained by applying de Morgan's theorem to success trees (which are directly related to reliability block diagrams).
FTA may be qualitative or quantitative. When failure and event probabilities are unknown, qualitative fault trees may be analyzed for minimal cut sets. For example, if any minimal cut set contains a single base event, then the top event may be caused by a single failure. Quantitative FTA is used to compute top event probability, and usually requires computer software such as CAFTA from the Electric Power Research Institute or SAPHIRE from the Idaho National Laboratory.
Some industries use both fault trees and event trees. An event tree starts from an undesired initiator (loss of critical supply, component failure etc.) and follows possible further system events through to a series of final consequences. As each new event is considered, a new node on the tree is added with a split of probabilities of taking either branch. The probabilities of a range of "top events" arising from the initial event can then be seen.
Safety certification.
Usually a failure in safety-certified systems is acceptable if, on average, less than one life per 109 hours of continuous operation is lost to failure. Most Western nuclear reactors, medical equipment, and commercial aircraft are certified to this level. The cost versus loss of lives has been considered appropriate at this level (by FAA for aircraft systems under Federal Aviation Regulations).
Preventing failure.
Once a failure mode is identified, it can usually be mitigated by adding extra or redundant equipment to the system. For example, nuclear reactors contain dangerous radiation, and nuclear reactions can cause so much heat that no substance might contain them. Therefore reactors have emergency core cooling systems to keep the temperature down, shielding to contain the radiation, and engineered barriers (usually several, nested, surmounted by a containment building) to prevent accidental leakage. Safety-critical systems are commonly required to permit no single event or component failure to result in a catastrophic failure mode.
Most biological organisms have a certain amount of redundancy: multiple organs, multiple limbs, etc.
For any given failure, a fail-over or redundancy can almost always be designed and incorporated into a system.
Safety and reliability.
Safety is not reliability. If a medical device fails, it should fail safely; other alternatives will be available to the surgeon. If an aircraft fly-by-wire control system fails, there is no backup. Electrical power grids are designed for both safety and reliability; telephone systems are designed for reliability, which becomes a safety issue when emergency (e.g. US "911") calls are placed.
Probabilistic risk assessment has created a close relationship between safety and reliability. Component reliability, generally defined in terms of component failure rate, and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system Mean Time Between Failure (MTBF), system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.
Safety generally cannot be achieved through component reliability alone. Catastrophic failure probabilities of 10−9 per hour correspond to the failure rates of very simple components such as resistors or capacitors. A complex system containing hundreds or thousands of components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10−4 or 10−5 per hour. If a system failure is catastrophic, usually the only practical way to achieve 10−9 per hour failure rate is through redundancy. Two redundant systems with independent failure modes, each having an MTBF of 100,000 hours, could achieve a failure rate on the order of 10−10 per hour because of the multiplication rule for independent events.
When adding equipment is impractical (usually because of expense), then the least expensive form of design is often "inherently fail-safe". That is, change the system design so its failure modes are not catastrophic. Inherent fail-safes are common in medical equipment, traffic and railway signals, communications equipment, and safety equipment.
The typical approach is to arrange the system so that ordinary single failures cause the mechanism to shut down in a safe way (for nuclear power plants, this is termed a passively safe design, although more than ordinary failures are covered). Alternately, if the system contains a hazard source such as a battery or rotor, then it may be possible to remove the hazard from the system so that its failure modes cannot be catastrophic. The U.S. Department of Defense Standard Practice for System Safety (MIL–STD–882) places the highest priority on elimination of hazards through design selection.
One of the most common fail-safe systems is the overflow tube in baths and kitchen sinks. If the valve sticks open, rather than causing an overflow and damage, the tank spills into an overflow. Another common example is that in an elevator the cable supporting the car keeps spring-loaded brakes open. If the cable breaks, the brakes grab rails, and the elevator cabin does not fall.
Some systems can never be made fail safe, as continuous availability is needed. For example, loss of engine thrust in flight is dangerous. Redundancy, fault tolerance, or recovery procedures are used for these situations (e.g. multiple independent controlled and fuel fed engines). This also makes the system less sensitive for the reliability prediction errors or quality induced uncertainty for the separate items. On the other hand, failure detection & correction and avoidance of common cause failures becomes here increasingly important to ensure system level reliability.

</doc>
<doc id="29279" url="http://en.wikipedia.org/wiki?curid=29279" title="SIGGRAPH">
SIGGRAPH

SIGGRAPH (short for Special Interest Group on GRAPHics and Interactive Techniques) is the name of the annual conference on computer graphics (CG) convened by the ACM SIGGRAPH organization. The first SIGGRAPH conference was in 1974. The conference is attended by tens of thousands of computer professionals. Past SIGGRAPH conferences have been held in Los Angeles, Dallas, New Orleans, Boston, Vancouver, and elsewhere across the United States.
Overview.
Some highlights of the conference are its Animation Theater and Electronic Theater presentations, where recently created CG films are played. There is a large exhibition floor, where several hundred companies set up elaborate booths and compete for attention and recruits. Most of the companies are in the engineering, graphics, motion picture, or video game industries. There are also many booths for schools which specialize in computer graphics or interactivity.
Dozens of research papers are presented each year, and SIGGRAPH is widely considered the most prestigious forum for the publication of computer graphics research. The recent paper acceptance rate for SIGGRAPH has been less than 26%. The submitted papers are peer-reviewed in a single-blind process. There has been some criticism about the preference of SIGGRAPH paper reviewers for novel results rather than useful incremental progress. The papers accepted for presentation at SIGGRAPH are currently printed in a special issue of the "ACM Transactions on Graphics" journal. Prior to 2002, SIGGRAPH papers were printed in the "SIGGRAPH Conference Proceedings" series of publications.
In addition to the papers, there are numerous panels of industry experts set up to discuss a wide variety of topics, from computer graphics to machine interactivity to education. SIGGRAPH also offers many full- and half-day courses in state-of-the-art computer graphics topics, as well as shorter "sketch" presentations where artists and researchers discuss their latest work.
In 1984, under LucasFilm Computer Group, John Lasseter's first computer animated short, "The Adventures of André & Wally B.", premiered at SIGGRAPH. Pixar's first computer animated short, "Luxo, Jr." debuted in 1986. Pixar has debuted numerous shorts at the conference since.
SIGGRAPH has several awards programs to recognize outstanding contributions to computer graphics. The most prestigious is the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics. It has been awarded every two years since 1983 to recognize an individual's lifetime achievement in computer graphics.
Conference areas.
The following conference areas are the areas scheduled for SIGGRAPH 2012, as some conference areas vary annually.
SIGGRAPH events around the world.
SIGGRAPH Asia.
Since 2008, a second yearly SIGGRAPH conference has been held in Asia. The first SIGGRAPH Asia conference was held in Singapore from the tenth to the thirteenth of December 2008 at the Suntec Singapore International Convention and Exhibition Centre; the second one in Yokohama, Japan in the period from 16 December to 19 December 2009 at Pacifico Yokohama; and the third in Seoul, Korea in the time period from 15 December to 18 December 2010 at Coex Convention & Exhibition Center Seoul.
SyysGraph Finland.
ACM SIGGRAPH Helsinki runs an evening-long graphics conference called , which is held autumn every year. The seminar strives to bring the latest updates of the 3D graphics field, demos, animations and interactive technologies. The presentations are held in English.

</doc>
<doc id="29285" url="http://en.wikipedia.org/wiki?curid=29285" title="Semtex">
Semtex

Semtex is a general-purpose plastic explosive containing RDX and PETN. It is used in commercial blasting, demolition, and in certain military applications. Semtex became notoriously popular with terrorists because it was, until recently, extremely difficult to detect, as in the case of Pan Am Flight 103.
For its original military use it was manufactured under the name B 1. It has been manufactured in Czechoslovakia under its current name since 1964, labeled as "SEMTEX 1A", since 1967 as "SEMTEX H" and since 1987 as "SEMTEX 10".
Composition.
The composition of the two most common variants differ according to their use. The 1A (or 10) variant is used for blasting, and is based mostly on crystalline PETN. The version 1AP and 2P are formed as hexagonal booster charges; a special assembly of PETN and wax inside the charge assures high reliability for detonating cord or detonator. The H (or SE) variant is intended for explosion hardening.
History.
Semtex was invented in the late 1950s by Stanislav Brebera, a chemist at VCHZ Synthesia, Czechoslovakia. The explosive is named after Semtín, a suburb of Pardubice in Czechoslovakia where the mixture was first manufactured starting in 1964. The plant was later renamed to become Explosia a.s., a subsidiary of Synthesia.
Semtex was very similar to other plastic explosives, especially C-4, in being easily malleable; but it is usable over a greater temperature range than other plastic explosives, since it stays plastic between −40 and +60 °C; it is also waterproof. There are also visual differences: while C-4 is off-white in colour, Semtex is red or brick-orange.
The new explosive was widely exported, notably to the government of North Vietnam, which received 14 tons during the Vietnam War. However, the main consumer was Libya; about 700 tons of Semtex were exported to Libya between 1975 and 1981 by Omnipol. It has also been used by Islamic militants in the Middle East and by the Provisional Irish Republican Army (IRA) and the Irish National Liberation Army in Northern Ireland.
Exports fell after the name became closely associated with terrorist attacks. Export of Semtex was progressively tightened and since 2002 all of Explosia's sales have been controlled by a government ministry. s of 2001[ [update]], approximately only 10 tons of Semtex were produced annually, almost all for domestic use.
Also in response to international agreements, Semtex has a detection taggant added to produce a distinctive vapor signature to aid detection. First, ethylene glycol dinitrate was used, later switched to 2,3-dinitro-2,3-dimethylbutane (3,4-dinitrohexane, DMDNB) or "p"-mononitrotoluene, which is used currently. According to the manufacturer, the taggant agent was voluntarily being added by 1991, years before the protocol signed became compulsory. Batches of Semtex made before 1990, however, are untagged, though it is not known whether there are still major stocks of such old batches of Semtex. According to the manufacturer, even this untagged Semtex can now be detected. The shelf life of Semtex was reduced from 10 years before the 1990s to five years now. Explosia states that there is no compulsory tagging allowing reliable post-detonation detection of a certain plastic explosive (such as incorporating a unique metallic code into the mass of the explosive), so Semtex is not tagged in this way.
On May 25, 1997, Bohumil Šole, a scientist often said to have been involved with inventing Semtex, strapped the explosive to his body and committed suicide in the Priessnitz spa of Jeseník. Šole, 63, was being treated there for depression. Twenty other people were hurt in the explosion, while six were seriously injured. According to the manufacturer, Explosia, he was not a member of the team that developed the explosive.

</doc>
<doc id="29286" url="http://en.wikipedia.org/wiki?curid=29286" title="Schedl">
Schedl

Schedl is a German surname. Notable people with the surname include: 

</doc>
<doc id="29287" url="http://en.wikipedia.org/wiki?curid=29287" title="Lehi (group)">
Lehi (group)

Lehi (]; Hebrew: לח"י – לוחמי חרות ישראל‎ "Lohamei Herut Israel – Lehi", "Fighters for the Freedom of Israel – Lehi"), commonly referred to in English as the Stern Gang, was a militant Zionist group founded by Avraham ("Yair") Stern in the British Mandate of Palestine. Its avowed aim was to evict the British authorities from Palestine by resort to force, allowing unrestricted immigration of Jews and the formation of a Jewish state, a 'new totalitarian Hebrew republic'. It was initially called the National Military Organization in Israel, upon being founded in August 1940, but was renamed Lehi one month later.
Lehi split from the Irgun militant group in 1940 in order to continue fighting the British during World War II. Lehi initially sought an alliance with Fascist Italy and Nazi Germany, offering to fight alongside them against the British in return for the transfer of all Jews from Nazi-occupied Europe to Palestine. Believing that Nazi Germany was a lesser enemy of the Jews than Britain, Lehi twice attempted to form an alliance with the Nazis. During World War II it declared that it would establish a Jewish state based upon "nationalist and totalitarian principles". After Stern's death in 1942, the new leadership of Lehi began to move it towards support for Joseph Stalin's Soviet Union. In 1944 Lehi officially declared its support for National Bolshevism. It said that its National Bolshevism involved an amalgamation of left-wing and right-wing political elements – Stern said Lehi incorporated elements of both the left and the right – however this change was unpopular and Lehi began to lose support as a result.
Lehi and the Irgun were jointly responsible for the massacre in Deir Yassin. Lehi assassinated Lord Moyne, British Minister Resident in the Middle East, and made many other attacks on the British in Palestine. On 29 May 1948, the government of Israel, having inducted its activist members into the Tzahal, formally disbanded Lehi, though some of its members carried out one more terrorist act, the assassination of Folke Bernadotte some months later, an act condemned by Bernadotte's replacement as mediator, Ralph Bunche. Israel granted a general amnesty to Lehi members on 14 February 1949. In 1980, Israel instituted a military decoration in "award for activity in the struggle for the establishment of Israel," the Lehi ribbon. Former Lehi leader Yitzhak Shamir became Prime Minister of Israel in 1983.
Founding of Lehi.
Lehi was created in August 1940 by Avraham Stern. Stern had been a member of the Irgun ("Irgun Tsvai Leumi" – "National Military Organization") high command. Zeev Jabotinsky, then the Irgun's supreme commander, had decided that diplomacy and working with Britain would best serve the Zionist cause. World War II was in progress, and Britain was fighting Nazi Germany. The Irgun suspended its underground military activities against the British for the duration of the war.
Stern argued that the time for Zionist diplomacy was over and that it was time for armed struggle against the British. Like other Zionists, he objected to the White Paper of 1939, which restricted both Jewish immigration and Jewish land purchases in Palestine. For Stern, 'no difference existed between Hitler and Chamberlain, between Dachau or Buchenwald and sealing the gates of Eretz Israel.'
Stern wanted to open Palestine to all Jewish refugees from Europe, and considered this as by far the most important issue of the day. Britain would not allow this. Therefore, he concluded, the "Yishuv" (Jews of Palestine) should fight the British rather than support them in the war. When the Irgun made a truce with the British, Stern left the Irgun to form his own group, which he called "Irgun Tsvai Leumi B'Yisrael" ("National Military Organization in Israel"), later "Lohamei Herut Israel" ("Fighters for the Freedom of Israel").
Stern and his followers believed that dying for the 'foreign occupier' who was obstructing the creation of the Jewish State was useless. They differentiated between 'enemies of the Jewish people' (the British) and 'Jew haters' (the Nazis), believing that the former needed to be defeated and the latter manipulated.
In September 1940, the organization was officially named "Lehi".
In 1940, the idea of the Final Solution was still "unthinkable", and Stern believed that Hitler wanted to make Germany "judenrein" through emigration, as opposed to extermination. In December 1940, Lehi even contacted Germany with a proposal to aid German conquest in the Middle East in return for recognition of a Jewish state open to unlimited immigration.
Goals and methods.
Lehi had three main goals:
Lehi believed in its early years that its goals would be achieved by finding a strong international ally that would expel the British from Palestine, in return for Jewish military help; this would require the creation of a broad and organised military force "demonstrating its desire for freedom through military operations."
Lehi also referred to themselves as 'terrorists' and may have been one of the last organizations to do so.
An article titled "Terror" in the Lehi underground newspaper "He Khazit" ("The Front" ) argued as follows:
Neither Jewish ethics nor Jewish tradition can disqualify terrorism as a means of combat. We are very far from having any moral qualms as far as our national war goes. We have before us the command of the Torah, whose morality surpasses that of any other body of laws in the world: "Ye shall blot them out to the last man."
But first and foremost, terrorism is for us a part of the political battle being conducted under the present circumstances, and it has a great part to play: speaking in a clear voice to the whole world, as well as to our wretched brethren outside this land, it proclaims our war against the occupier.
We are particularly far from this sort of hesitation in regard to an enemy whose moral perversion is admitted by all.
The article described the goals of terror:
Yitzhak Shamir, one of the three leaders of Lehi after Avraham Stern's assassination, argued for the legitimacy of Lehi's actions:
There are those who say that to kill [T.G.] Martin [a CID sergeant who had recognised Shamir in a lineup] is terrorism, but to attack an army camp is guerrilla warfare and to bomb civilians is professional warfare. But I think it is the same from the moral point of view. Is it better to drop an atomic bomb on a city than to kill a handful of persons? I don’t think so. But nobody says that President Truman was a terrorist. All the men we went for individually – Wilkin, Martin, MacMichael and others – were personally interested in succeeding in the fight against us.
So it was more efficient and more moral to go for selected targets. In any case, it was the only way we could operate, because we were so small. For us it was not a question of the professional honor of a soldier, it was the question of an idea, an aim that had to be achieved. We were aiming at a political goal. There are many examples of what we did to be found in the Bible – Gideon and Samson, for instance. This had an influence on our thinking. And we also learned from the history of other peoples who fought for their freedom – the Russian and Irish revolutionaries, Giuseppe Garibaldi and Josip Broz Tito.
"18 Principles of Rebirth".
Avraham Stern laid out the ideology of Lehi in the essay "18 Principles of Rebirth":
Relationship with fascism and socialism.
Unlike the left-wing Haganah and right-wing Irgun, Lehi members were not a homogeneous collective with a single political, religious, or economic ideology. They were a combination of militants united by the goal of liberating the land of Israel from British rule. Most Lehi leaders defined their organization as an anti-imperialism movement and stated that their opposition to British colonial rule in Palestine was not based on a particular policy but rather on the presence of a foreign power over the homeland of the Jewish people. Avraham Stern defined the British Mandate as "foreign rule" regardless of British policies and took a radical position against such imperialism even if it were to be benevolent.
In the early years of the state of Israel Lehi veterans could be found supporting nearly all political parties and some Lehi leaders founded a left-wing political party called the Fighters' List with Natan Yellin-Mor as its head. The party took part in the elections in January 1949 and won a single parliamentary seat. A number of Lehi veterans established the Semitic Action movement in 1956 which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.
Some writers have stated that Lehi's true goals were the creation of a totalitarian state. Perlinger and Weinberg write that the organisation's ideology placed "its world view in the quasi-fascist radical Right, which is characterised by xenophobia, a national egotism that completely subordinates the individual to the needs of the nation, anti-liberalism, total denial of democracy and a highly centralised government." Perliger and Weinberg state that most Lehi members were admirers of the Italian Fascist movement.
Others counter these claims. They note that when Lehi founder Avraham Stern went to study in fascist Italy, he refused to join the for foreign students, even though members got large reductions in tuition.
Evolution and tactics of the organization.
Many Lehi combatants received professional training. Some attended the state military academy in Civitavecchia, in Fascist Italy. Others received military training from instructors of the Polish Armed Forces in 1938–1939. This training was conducted in Trochenbrod (Zofiówka) in Wołyń Voivodeship, Podębin near Łódź, and the forests around Andrychów. They were taught how to use explosives. One of them reported later:"Poles treated terrorism as a science. We have mastered mathematical principles of demolishing constructions made of concrete, iron, wood, bricks and dirt."
The group was initially unsuccessful. Early attempts to raise funds through criminal activities, including a bank robbery in Tel Aviv in 1940 and another robbery on 9 January 1942 in which Jewish passers-by were killed, brought about the temporary collapse of the group. An attempt to assassinate the head of the British secret police in Lod in which three police personnel were killed, two Jewish and one British, elicited a severe response from the British and Jewish establishments who collaborated against Lehi.
Stern's group was seen as a terrorist organisation by the British authorities, who instructed the Defence Security Office (the colonial branch of MI5) to track down its leaders. In 1942, Stern, after he was arrested, was shot dead in disputed circumstances by Inspector Geoffrey J. Morton of the CID. The arrest of several other members led momentarily to the group's eclipse, until it was revived after the September 1942 escape of two of its leaders, Yitzhak Shamir and Eliyahu Giladi, aided by two other escapees Natan Yellin-Mor (Friedman) and Israel Eldad (Sheib). (Giladi was later killed by Lehi under circumstances that remain mysterious.) Shamir's codename was "Michael", a reference to one of Shamir's heroes, Michael Collins. Lehi was guided by spiritual and philosophical leaders such as Uri Zvi Greenberg and Israel Eldad. After the killing of Giladi, the organization was led by a triumvirate of Eldad, Shamir, and Yellin-Mor.
Lehi adopted a non-socialist platform of Anti-Imperialist ideology. It viewed the continued British rule of Palestine as a violation of the Mandate's provision generally, and its restrictions on Jewish immigration to be an intolerable breach of international law. However they also targeted Jews whom they regarded as traitors, and during the 1948 Arab-Israeli War they joined in operations with the Haganah and Irgun against Arab targets, for example Deir Yassin.
According to a compilation by Nachman Ben-Yehuda, Lehi was responsible for 42 assassinations, more than twice as many as the Irgun and Haganah combined during the same period. Of those Lehi assassinations that Ben-Yehuda classified as political, more than half the victims were Jews.
Lehi also rejected the authority of the Jewish Agency for Israel and related organizations, operating entirely on its own throughout nearly all of its existence.
Lehi prisoners captured by the British generally refused to present a defence when brought to trial. They would only read out statements in which they declared that the court, representing an occupying force, had no jurisdiction over them and therefore was illegal. For the same reason, Lehi prisoners refused to plead for amnesty, even when it was clear that this would have spared them the death penalty. In one case Moshe Barazani, a Lehi member, and Meir Feinstein, an Irgun member, committed suicide in prison with a grenade smuggled inside an orange so the British could not hang them.
Wartime contacts with Italy and Germany.
In mid-1940, Stern became convinced that the Italians were interested in the establishment of a fascist Jewish state in Palestine. He conducted negotiations, he thought, with the Italians via an intermediary Moshe Rotstein, and drew up a document that became known as the "Jerusalem Agreement". In exchange for Italy's recognition of, and aid in obtaining, Jewish sovereignty over Palestine, Stern promised that Zionism would come under the aegis of Italian fascism, with Haifa as its base, and the Old City of Jerusalem under Vatican control, except for the Jewish quarter. In Heller's words, Stern's proposal would "turn the 'Kingdom of Israel' into a satellite of the Axis powers."
However, the "intermediary" Rotstein was in fact an agent of the Irgun, conducting a sting operation under the direction of the Irgun intelligence leader in Haifa, Israel Pritzker, in cooperation with the British. Secret British documents about the affair were uncovered by historian Eldad Harouvi (now director of the Palmach Archives) and confirmed by former Irgun intelligence officer Yitzhak Berman. When Rotstein's role later became clear, Lehi sentenced him to death and assigned Yaacov Eliav to kill him, but the assassination never took place. However, Pritzker was killed by Lehi in 1943.
Late in 1940, Lehi, having identified a common interest between the intentions of the new German order and Jewish national aspirations, proposed forming an alliance in World War II with Nazi Germany. It offered assistance in transferring the Jews of Europe to Palestine, in return for Germany's help in expelling Britain from Mandatory Palestine. Late in 1940, Lehi representative Naftali Lubenchik went to Beirut to meet German official Werner Otto von Hentig (who also was involved with the Haavara or Transfer Agreement, which had been transferring German Jews and their funds to Palestine since 1933). Lubenchik told von Hentig that Lehi had not yet revealed its full power and that they were capable of organizing a whole range of anti-British operations.
On the assumption that the destruction of Britain was the Germans' top objective, the organization offered cooperation in the following terms. Lehi would support sabotage and espionage operations in the Middle East and in eastern Europe anywhere where they had cells. Germany would recognize an independent Jewish state in Palestine/Eretz Israel, and all Jews leaving their homes in Europe, by their own will or because of government injunctions, could enter Palestine with no restriction of numbers.
Stern also proposed recruiting some 40,000 Jews from occupied Europe to invade Palestine with German support to oust the British. On 11 January 1941, Vice Admiral Ralf von der Marwitz, the German Naval attaché in Turkey, filed a report (the "Ankara document") conveying an offer by Lehi to "actively take part in the war on Germany's side" in return for German support for "the establishment of the historic Jewish state on a national and totalitarian basis, bound by a treaty with the German Reich."
According to Yellin-Mor,
According to Joseph Heller,
Von der Marwitz delivered the offer, classified as secret, to the German Ambassador in Turkey and on 21 January 1941 it was sent to Berlin. There was never any response.
A second attempt to contact the Nazis was made at the end of 1941, but it was even less successful. The emissary Yellin-Mor was arrested in Syria before he could carry out his mission.
This proposed alliance with Nazi Germany cost Lehi and Stern much support. The Stern Gang also had links with, and support from, the Vichy France Sûreté's Lebanese offices.
Later history.
As a group that never had over a few hundred members, Lehi relied on audacious but small-scale operations to bring their message home. They adopted the tactics of groups such as the Socialist Revolutionaries and the Combat Organization of the Polish Socialist Party in Czarist Russia, and the Irish Republican Army. To this end, Lehi conducted small-scale operations such as individual assassinations of British officials (notable targets included Lord Moyne, CID detectives, and Jewish "collaborators"), and random shootings against soldiers and police officers. Another strategy, adopted in 1946, was to send bombs in the mail to British politicians. Other actions included sabotaging infrastructure targets: bridges, railroads, telephone and telegraph lines, and oil refineries, as well as the use of vehicle bombs against British military, police, and administrative targets. Lehi financed its operations from private donations, extortion, and bank robbery. Its campaign of violence lasted from 1944 to 1948. Initially conducted together with the Irgun, it included a six-month suspension to avoid being targeted by the Haganah during the Hunting Season, and later operated jointly with the Haganah and Irgun under the Jewish Resistance Movement. After the Jewish Resistance Movement was dissolved, it operated independently as part of the general Jewish insurgency in Palestine.
Assassination of Lord Moyne.
On 6 November 1944, Lehi assassinated Lord Moyne, the British Minister Resident in the Middle East, in Cairo. Moyne was the highest ranking British official in the region. Yitzhak Shamir claimed later that Moyne was assassinated because of his support for a Middle Eastern Arab Federation and anti-Semitic lectures in which Arabs were held to be racially superior to Jews. The assassination rocked the British government, and outraged Winston Churchill, the British Prime Minister. The two assassins, Eliahu Bet-Zouri and Eliahu Hakim were captured and used their trial as a platform to make public their political propaganda. They were executed. In 1975 their bodies were returned to Israel and given a state funeral. In 1982, postage stamps were issued for 20 Olei Hagardom, including Bet-Zouri and Hakim, in a souvenir sheet called "Martyrs of the struggle for Israel's independence."
Tel Aviv car park raid.
On 25 April 1946, a Lehi unit attacked a car park in Tel Aviv occupied by the British 6th Airborne Division. Under a barrage of heavy covering fire, Lehi fighters broke into the car park, shot soldiers they encountered at close range, stole rifles from arms racks, laid mines to cover the retreat, and withdrew. Seven soldiers were killed in the attack, which caused widespread outrage among the British security forces in Palestine. It resulted in retaliatory anti-Jewish violence by British troops and a punitive curfew imposed on Tel Aviv by the British Army.
British police station in Haifa.
On 12 January 1947, Lehi members drove a truckload of explosives into a British police station in Haifa killing four and injuring 140, in what has been called 'the world's first true truck bomb'.
Operations in Europe.
Following the bombing of the British embassy in Rome, October 1946, a series of operations against targets in the United Kingdom were launched. On 7 March 1947, Lehi's only successful operation in Britain was carried out when a Lehi bomb severely damaged the British Colonial Club, a London recreational facility for soldiers and students from Britain's colonies in Africa and the West Indies. On 15 April 1947 a bomb consisting of twenty-four sticks of explosives was planted in the Colonial Office, Whitehall. It failed to explode due to a fault in the timer. Five weeks later, on 22 May, five alleged Lehi members were arrested in Paris with bomb making material including explosives of the same type as found in London. On 2 June, two Lehi members, Betty Knouth and Yaakov Levstien, were arrested crossing from Belgium to France. Envelopes addressed to British officials, with detonators, batteries and a time fuse were found in one of Knouth's suitcases. Knouth was sentenced to a year in prison, Levstien to eight months. The British Security Services identified Knouth as the person who planted the bomb in the Colonial Office. Shortly after their arrest, 21 letter bombs were intercepted addressed to senior British figures. The letters had been posted in Italy. The intended recipients included Bevin, Attlee, Churchill and Eden. Knouth aka Gilberte/Elizabeth Lazarus. Levstein was travelling as Jacob Elias; his fingerprints connected him to the deaths of several Palestine Policemen as well as an attempt on the life of the British High Commissioner. In 1973, Margaret Truman wrote that letter bombs were also posted to her father, U.S. President Harry S. Truman, in 1947. Former Lehi leader Yellin-Mor admitted that letter bombs had been sent to British targets but denied that any had been sent to Truman.
Death threat against Hugh Trevor-Roper.
Shortly after the 1947 publication of "The Last Days of Hitler", Lehi issued a death threat against the author, Hugh Trevor-Roper, for his portrayal of Hitler, feeling that Trevor-Roper had attempted to exonerate the German populace from responsibility.
Cairo-Haifa train bombings.
During the lead-up to the 1948 Arab–Israeli War, Lehi mined the Cairo–Haifa train several times. On 29 February 1948, Lehi mined the train north of Rehovot, killing 28 British soldiers and wounding 35. On 31 March, Lehi mined the train near Binyamina, killing 40 civilians and wounding 60.
Deir Yassin massacre.
One of the most widely known acts of Lehi was the attack on the Palestinian-Arab village of Deir Yassin.
In the months before the British evacuation from Palestine, the Arab League-sponsored Arab Liberation Army (ALA) occupied several strategic points along the road between Jerusalem and Tel Aviv, cutting off supplies to the Jewish part of Jerusalem. One of these points was Deir Yassin. By March 1948, the road was cut off and Jewish Jerusalem was under siege. The Haganah launched Operation "Nachshon" to break the siege.
On 6 April, the Haganah attacked al-Qastal, a village two kilometers north of Deir Yassin, also overlooking the Jerusalem-Tel Aviv road.
Then on 9 April 1948, about 120 Lehi and Irgun fighters, acting in cooperation with the Haganah, attacked and captured Deir Yassin. The attack was at night, the fighting was confused, and many civilian inhabitants of the village were killed. This action had great consequences for the war, and remains a cause celebre for Palestinians ever since.
Exactly what happened has never been established clearly. The Arab League reported a great massacre: 254 killed, with rape and lurid mutilations. Israeli investigations claimed the actual number of dead was between 100 and 120, and there were no mass rapes, but most of the dead were civilians, and admitted some were killed deliberately. Lehi and Irgun both denied an organized massacre. Accounts by Lehi veterans such as Ezra Yakhin note that many of the attackers were killed or wounded, assert that Arabs fired from every building and that Iraqi and Syrian soldiers were among the dead, and even that some Arab fighters dressed as women.
However, Jewish authorities, including Haganah, the Chief Rabbinate, the Jewish Agency, and David Ben-Gurion, also condemned the attack, lending credence to the charge of massacre. The Jewish Agency even sent a letter of condemnation, apology, and condolence to King Abdullah I of Jordan.
Both the Arab reports and Jewish responses had hidden motives: the Arab leaders wanted to encourage Palestinian Arabs to fight rather than surrender, to discredit the Zionists with international opinion, and to increase popular support in their countries for an invasion of Palestine. The Jewish leaders wanted to discredit Irgun and Lehi.
Ironically, the Arab reports backfired in one respect: frightened Palestinian Arabs did not surrender, but did not fight either – they fled, allowing Israel to gain much territory with little fighting and also without absorbing many Arabs.
Lehi similarly interpreted events at Deir Yassin as turning the tide of war in favor of the Jews. Lehi leader Israel Eldad later wrote in his memoirs from the underground period that "without Deir Yassin the State of Israel could never have been established."
The Deir Yassin story did not much sway international opinion. It did increase not only support but pressure on Arab governments to intervene, notably Abdullah of Jordan, who was now compelled to join the invasion of Palestine after Israel's declaration of independence on 14 May.
Dissolution.
The conflict between Lehi and mainstream Jewish and subsequently Israeli organizations came to an end when Lehi was formally dissolved and integrated into the Israeli Defense Forces on 31 May 1948, its leaders getting amnesty from prosecution or reprisals as part of the integration.
Assassination of Count Folke Bernadotte.
Although Lehi had stopped operating nationally after May 1948, the group continued to function in Jerusalem. On 17 September 1948, Lehi assassinated UN mediator Count Folke Bernadotte. The assassination was directed by Yehoshua Zettler and carried out by a four-man team led by Meshulam Makover. The fatal shots were fired by Yehoshua Cohen. The Security Council described the assassination as a "cowardly act which appears to have been committed by a criminal group of terrorists".
Three days after the assassination, the Israeli government passed the Ordinance to Prevent Terrorism and declared Lehi to be a terrorist organization. Many Lehi members were arrested, including leaders Nathan Yellin-Mor and Matitiahu Schmulevitz who were arrested on 29 September. Eldad and Shamir managed to escape arrest. Yellin-Mor and Schmulevitz were charged with leadership of a terrorist organization and on 10 February 1949 were sentenced to 8 years and 5 years imprisonment, respectively. However the State (Temporary) Council soon announced a general amnesty for Lehi members and they were released.
Lehi in politics.
Some of the Lehi leadership founded a left-wing political party called the Fighters' List with the jailed Yellin-Mor as its head. The party took part in the elections in January 1949 and won one seat. Thanks to a general amnesty for Lehi members granted on 14 February 1949, Yellin-Mor was released from prison to take up his place in the Knesset. However, the party disbanded after failing to win a seat in the 1951 elections.
In 1956, some Lehi veterans established the Semitic Action movement, which sought the creation of a regional federation encompassing Israel and its Arab neighbors on the basis of an anti-colonialist alliance with other indigenous inhabitants of the Middle East.
Not all Lehi alumni gave up political violence after independence: former members were involved in the activities of the Kingdom of Israel militant group, the 1957 assassination of Rudolf Kastner, and likely the 1952 attempted assassination of David-Zvi Pinkas.
Service ribbon.
In 1980, Israel instituted the Lehi ribbon, red, black, grey, pale blue and white, which is awarded to former members of the Lehi underground who wished to carry it, "for military service towards the establishment of the State of Israel".
"Unknown Soldiers" anthem.
The words and music of a song "Unknown Soldiers" (also translated "Anonymous Soldiers") were written by Avraham Stern in 1932 during the early days of the Irgun. It became the Irgun's anthem until the split with Lehi in 1940, after which it became the Lehi anthem.
Prominent members of Lehi.
A number of Lehi's members went on to play important roles in Israel's public life.

</doc>
<doc id="29288" url="http://en.wikipedia.org/wiki?curid=29288" title="Server-side scripting">
Server-side scripting

Server-side scripting is a technique used in web development which involves employing scripts on a web server which produce a response customized for each user's (client's) request to the website. The alternative is for the web server itself to deliver a static web page. Scripts can be written in any of a number of server-side scripting languages that are available (see below). Server-side scripting is distinguished from client-side scripting where embedded scripts, such as JavaScript, are run client-side in a web browser, but both techniques are often used together.
Server-side scripting is often used to provide a customized interface for the user. These scripts may assemble client characteristics for use in customizing the response based on those characteristics, the user's requirements, access rights, etc. Server-side scripting also enables the website owner to hide the source code that generates the interface, whereas with client-side scripting, the user has access to all the code received by the client. A down-side to the use of server-side scripting is that the client needs to make further requests over the network to the server in order to show new information to the user via the web browser. These requests can slow down the experience for the user, place more load on the server, and prevent use of the application when the user is disconnected from the server.
When the server serves data in a commonly used manner, for example according to the HTTP or FTP protocols, users may have their choice of a number of client programs (most modern web browsers can request and receive data using both of those protocols). In the case of more specialized applications, programmers may write their own server, client, and communications protocol, that can only be used with one another.
Programs that run on a user's local computer without ever sending or receiving data over a network are not considered clients, and so the operations of such programs would not be considered client-side operations.
History.
Netscape introduced an implementation of JavaScript for server-side scripting with Netscape Enterprise Server, first released in December, 1994 (soon after releasing JavaScript for browsers).
Server-side scripting was later used in early 1995 by Fred DuFresne while developing the first web site for Boston, MA television station WCVB. The technology is described in . The patent was issued in 1998 and is now owned by Open Invention Network (OIN). In 2010 OIN named Fred DuFresne a for his work on server-side scripting.
Today, a variety of services use server-side scripting to deliver results back to a client as a paid or free service. An example would be WolframAlpha, which is a computational knowledge engine that computes results outside the clients environment and returns the computed result back. A more commonly used service is Google's proprietary search engine, which searches millions of cached results related to the user specified keyword and returns an ordered list of links back to the client. Apple's Siri application also employs server-side scripting outside of a web application. The application takes an input, computes a result, and returns the result back to the client.
Explanation.
In the earlier days of the web, server-side scripting was almost exclusively performed by using a combination of C programs, Perl scripts, and shell scripts using the Common Gateway Interface (CGI). Those scripts were executed by the operating system, and the results were served back by the web server. Many modern web servers can directly execute on-line scripting languages such as ASP and PHP either by the web server itself or via extension modules (e.g. mod_perl or mod_php) to the web server. For example, WebDNA includes its own embedded database system. Either form of scripting (i.e., CGI or direct execution) can be used to build up complex multi-page sites, but direct execution usually results in less overhead because of the lower number of calls to external interpreters.
Dynamic websites sometimes use custom web application servers, such as the Python "Base HTTP Server" library, although some may not consider this to be server-side scripting. When designing using dynamic web-based scripting techniques, like classic ASP or PHP, developers must have a keen understanding of the logical, temporal, and physical separation between the client and the server. For a user's action to trigger the execution of server-side code, for example, a developer working with classic ASP must explicitly cause the user's browser to make a request back to the web server. Creating such interactions can easily consume much development time and lead to unreadable code.
Server-side scripts are completely processed by the servers instead of clients. When clients request a page containing server-side scripts, the applicable server processes the scripts and returns an HTML page to the client. For example, an ASP page is not processed by the browser; instead it is interpreted by the server which can process ASP scripts and return an HTML page to the client.
Languages.
There are a number of server-side scripting languages available, including:

</doc>
<doc id="29290" url="http://en.wikipedia.org/wiki?curid=29290" title="Samuel Huntington">
Samuel Huntington

Samuel Huntington may refer to: 

</doc>
<doc id="29292" url="http://en.wikipedia.org/wiki?curid=29292" title="Script">
Script

Script or scripting may refer to:

</doc>
<doc id="29293" url="http://en.wikipedia.org/wiki?curid=29293" title="Optical spectrometer">
Optical spectrometer

An optical spectrometer (spectrophotometer, spectrograph or spectroscope) is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The variable measured is most often the light's intensity but could also, for instance, be the polarization state. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as wavenumber or electron volts, which has a reciprocal relationship to wavelength. A spectrometer is used in spectroscopy for producing spectral lines and measuring their wavelengths and intensities. Spectrometer is a term that is applied to instruments that operate over a very wide range of wavelengths, from gamma rays and X-rays into the far infrared. If the instrument is designed to measure the spectrum in absolute units rather than relative units, then it is typically called a spectrophotometer. The majority of spectrophotometers are used in spectral regions near the visible spectrum.
In general, any particular instrument will operate over a small portion of this total range because of the different techniques used to measure different portions of the spectrum. Below optical frequencies (that is, at microwave and radio frequencies), the spectrum analyzer is a closely related electronic device.
Spectrometers are used in many fields. For example, they are used in astronomy to analyze the radiation from astronomical objects and deduce chemical composition. The spectrometer uses a prism or a grating to spread the light from a distant object into a spectrum. This allows astronomers to detect many of the chemical elements by their characteristic spectral fingerprints. If the object is glowing by itself, it will show spectral lines caused by the glowing gas itself. These lines are named for the elements which cause them, such as the hydrogen alpha, beta, and gamma lines. Chemical compounds may also be identified by absorption. Typically these are dark bands in specific locations in the spectrum caused by energy being absorbed as light from other objects passes through a gas cloud. Much of our knowledge of the chemical makeup of the universe comes from spectra.
Spectroscopes.
Spectroscopes are often used in astronomy and some branches of chemistry. Early spectroscopes were simply prisms with graduations marking wavelengths of light. Modern spectroscopes generally use a diffraction grating, a movable slit, and some kind of photodetector, all automated and controlled by a computer.
Joseph von Fraunhofer developed the first modern spectroscope by combining a prism, diffraction slit and telescope in a manner that increased the spectral resolution and was reproducible in other laboratories. Fraunhofer also went on to invent the first diffraction spectroscope. Gustav Robert Kirchhoff and Robert Bunsen discovered the application of spectroscopes to chemical analysis and used this approach to discover caesium and rubidium. Kirchhoff and Bunsen's analysis also enabled a chemical explanation of stellar spectra, including Fraunhofer lines.
When a material is heated to incandescence it emits light that is characteristic of the atomic makeup of the material.
Particular light frequencies give rise to sharply defined bands on the scale which can be thought of as fingerprints. For example, the element sodium has a very characteristic double yellow band known as the Sodium D-lines at 588.9950 and 589.5924 nanometers, the color of which will be familiar to anyone who has seen a low pressure sodium vapor lamp.
In the original spectroscope design in the early 19th century, light entered a slit and a collimating lens transformed the light into a thin beam of parallel rays. The light then passed through a prism (in hand-held spectroscopes, usually an Amici prism) that refracted the beam into a spectrum because different wavelengths were refracted different amounts due to dispersion. This image was then viewed through a tube with a scale that was transposed upon the spectral image, enabling its direct measurement.
With the development of photographic film, the more accurate spectrograph was created. It was based on the same principle as the spectroscope, but it had a camera in place of the viewing tube. In recent years, the electronic circuits built around the photomultiplier tube have replaced the camera, allowing real-time spectrographic analysis with far greater accuracy. Arrays of photosensors are also used in place of film in spectrographic systems. Such spectral analysis, or spectroscopy, has become an important scientific tool for analyzing the composition of unknown material and for studying astronomical phenomena and testing astronomical theories.
In modern spectrographs in the UV, visible, and near-IR spectral ranges, the spectrum is generally given in the form of photon number per unit wavelength (nm or μm), wavenumber (μm−1, cm−1), frequency (THz), or energy (eV), with the units indicated by the abscissa. In the mid- to far-IR, spectra are typically expressed in units of Watts per unit wavelength (μm) or wavenumber (cm−1). In many cases, the spectrum is displayed with the units left implied (such as "digital counts" per spectral channel).
Spectrographs.
A spectrograph is an instrument that separates an incoming wave into a frequency spectrum. There are several kinds of machines referred to as "spectrographs", depending on the precise nature of the waves. The first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.
A spectrograph is sometimes called polychromator, as an analogy to monochromator.
External links.
 at DMOZ

</doc>
<doc id="29294" url="http://en.wikipedia.org/wiki?curid=29294" title="IBM System/360">
IBM System/360

The IBM System/360 (S/360) was a mainframe computer system family announced by IBM on April 7, 1964, and delivered between 1965 and 1978. It was the first family of computers designed to cover the complete range of applications, from small to large, both commercial and scientific. The design made a clear distinction between architecture and implementation, allowing IBM to release a suite of compatible designs at different prices. All but the incompatible model 44 and the most expensive systems used microcode to implement the instruction set, which featured 8-bit byte addressing and binary, decimal and (hexadecimal) floating-point calculations.
The slowest System/360 model announced in 1964, the Model 30, could perform up to 34,500 instructions per second, with memory from 8 to 64 KB. High performance models came later. The 1967 System 360 Model 91 could do up to 16.6 million instructions per second. The larger 360 models could have up to 8 MB of internal main memory, though main memory that big was unusual—a more typical large installation might have as little as 256 KB of main storage, but 512 KB, 768 KB or 1024 KB was more common. Up to 8 megabytes of slower (8 microsecond) Large Capacity Storage (LCS) was also available.
System/360 was extremely successful in the market, allowing customers to purchase a smaller system with the knowledge they would always be able to migrate upward if their needs grew, without reprogramming of application software or replacing peripheral devices. Many consider the design one of the most successful computers in history, influencing computer design for years to come.
The chief architect of System/360 was Gene Amdahl, and the project was managed by Fred Brooks, responsible to Chairman Thomas J. Watson Jr. The commercial release was piloted by another of Watson's lieutenants, John R. Opel, who managed the launch of IBM’s System 360 mainframe family in 1964.
Application level compatibility (with some restrictions) for System/360 software is maintained to present day with the System z servers.
System/360 history.
A family of computers.
Contrasting with at-the-time normal industry practice, IBM created an entire series of computers (or CPUs) from small to large, low to high performance, all using the same instruction set (with two exceptions for specific markets). This feat allowed customers to use a cheaper model and then upgrade to larger systems as their needs increased without the time and expense of rewriting software. IBM was the first manufacturer to exploit microcode technology to implement a compatible range of computers of widely differing performance, although the largest, fastest, models had hard-wired logic instead.
This flexibility greatly lowered barriers to entry. With other vendors (with the notable exception of ICT), customers had to choose between machines they could outgrow and machines that were potentially overpowered (and thus too expensive). This meant that many companies simply did not buy computers.
Models.
IBM initially announced a series of six computers and forty common peripherals. IBM eventually delivered fourteen models, including rare one-off models for NASA. The least expensive model was the Model 20 with as little as 4 KB of core memory, eight 16-bit registers instead of the sixteen 32-bit registers of real 360s, and an instruction set that was a subset of that used by the rest of the range.
The initial announcement in 1964 included Models 30, 40, 50, 60, 62, and 70. The first three were low- to middle-range systems aimed at the IBM 1400 series market. All three first shipped in mid-1965. The last three, intended to replace the 7000 series machines, never shipped and were replaced by the 65 and 75, which were first delivered in November 1965, and January 1966, respectively.
Later additions to the low-end included models 20 (1966, mentioned above), 22 (1971), and 25 (1968). The Model 22 was a recycled Model 30 with minor limitations: a smaller maximum memory configuration, and slower I/O channels, which limited it to slower and lower-capacity disk and tape devices than on the 30.
The Model 44 (1966) was a specialized model, designed for scientific computing and for real-time computing and process control, featuring some additional instructions, and with all storage-to-storage instructions and five other complex instructions eliminated.
A succession of high-end machines included the Model 67 (1966, mentioned below, briefly anticipated as the 64 and 66), 85 (1969), 91 (1967, anticipated as the 92), 95 (1968), and 195 (1971). The 85 design was intermediate between the System/360 line and the follow-on System/370 and was the basis for the 370/165. There was a System/370 version of the 195, but it did not include Dynamic Address Translation.
The implementations differed substantially, using different native data path widths, presence or absence of microcode, yet were extremely compatible. Except where specifically documented, the models were architecturally compatible. The 91, for example, was designed for scientific computing and provided out-of-order instruction execution (and could yield "imprecise interrupts" if a program trap occurred while several instructions were being read), but lacked the decimal instruction set used in commercial applications. New features could be added without violating architectural definitions: the 65 had a dual-processor version (M65MP) with extensions for inter-CPU signalling; the 85 introduced cache memory. Models 44, 75, 91, 95, and 195 were implemented with hardwired logic, rather than microcoded as all other models.
The Model 67, announced in August 1965, was the first production IBM system to offer dynamic address translation hardware to support time-sharing. "DAT" is now more commonly referred to as an MMU. An experimental one-off unit was built based on a model 40. Before the 67, IBM had announced models 64 and 66, DAT versions of the 60 and 62, but they were almost immediately replaced by the 67 at the same time that the 60 and 62 were replaced by the 65. DAT hardware would reappear in the S/370 series in 1972, though it was initially absent from the series. Like its close relative, the 65, the 67 also offered dual CPUs.
IBM stopped marketing all System/360 models by the end of 1977.
Backward compatibility.
IBM's existing customers had a large investment in software that executed on second generation machines. Many models offered the option of emulation of the customer's previous computer (e.g. the IBM 1400 series on a Model 30 or the IBM 7094 on a Model 65) using a combination of special hardware, special microcode and an emulation program that used the emulation instructions to simulate the target system, so that old programs could run on the new machine. However, customers had to halt the computer and load the emulation program. The Model 85 and later System/370 retained the emulation options, but allowed them to execute under operating system control alongside native programs.
Successors and variants.
System/360 (excepting the Model 20) was replaced by the compatible System/370 range in 1970 and Model 20 users were targeted to move to the IBM System/3. (The idea of a major breakthrough with FS technology was dropped in the mid-1970s for cost-effectiveness and continuity reasons.) Later compatible IBM systems include the 3090, the ES/9000 family, 9672 (System/390 family), the zSeries, System z9, System z10 and IBM zEnterprise System.
Computers that were mostly identical or compatible in terms of the machine code or architecture of the System/360 included Amdahl's 470 family (and its successors), Hitachi mainframes, the UNIVAC 9000 series, Fujitsu as the Facom, the RCA Spectra 70 series, and the English Electric System 4. The System 4 machines were built under license to RCA. RCA sold the Spectra series to what was then UNIVAC, where they became the UNIVAC Series 70. UNIVAC also developed the UNIVAC Series 90 as successors to the 9000 series and Series 70. The Soviet Union produced a System/360 clone named the ES EVM.
The IBM 5100 portable computer, introduced in 1975, offered an option to execute the System/360's APL.SV programming language through a hardware emulator. IBM used this approach to avoid the costs and delay of creating a 5100-specific version of APL.
Special radiation-hardened and otherwise somewhat modified System/360s, in the form of the System/4 Pi avionics computer, are used in several fighter and bomber jet aircraft. In the complete 32-bit AP-101 version, 4 Pi machines were used as the replicated computing nodes of the fault-tolerant Space Shuttle computer system (in five nodes). The U.S. Federal Aviation Administration operated the IBM 9020, a special cluster of modified System/360s for air traffic control, from 1970 until the 1990s. (Some 9020 software is apparently still used via emulation on newer hardware.)
Technical description.
Influential features.
The System/360 introduced a number of industry standards to the marketplace, such as:
Architectural overview.
The System/360 series had a computer system architecture specification. This specification makes no assumptions on the implementation itself, but rather describes the interfaces and expected behavior of an implementation. The architecture describes mandatory interfaces that must be available on all implementations, and optional interfaces. Some aspects of this architecture are:
Some of the optional features are:
All models of System/360, except for the Model 20 and Model 44, implemented that specification.
Binary arithmetic and logical operations are performed as register-to-register and as memory-to-register/register-to-memory as a standard feature. If the Commercial Instruction Set option was installed, packed decimal arithmetic could be performed as memory-to-memory with some memory-to-register operations. The Scientific Instruction Set feature, if installed, provided access to four floating point registers that could be programmed for either 32-bit or 64-bit floating point operations. The Models 85 and 195 could also operate on 128-bit extended-precision floating point numbers stored in pairs of floating point registers, and software provided emulation in other models. The System/360 used an 8-bit byte, 32-bit word, 64-bit double-word, and 4-bit nibble. Machine instructions had operators with operands, which could contain register numbers or memory addresses. This complex combination of instruction options resulted in a variety of instruction lengths and formats.
Memory addressing was accomplished using a base-plus-displacement scheme, with registers 1 through F (15). A displacement was encoded in 12 bits, thus allowing a 4096-byte displacement (0-4095), as the offset from the address put in a base register.
Register 0 could not be used as a base register nor as an index register (nor as a branch address register), as "0" was reserved to indicate an address in the first 4 KB of memory, that is, if register 0 was specified as described, the value 0x00000000 was implicitly input to the effective address calculation in place of whatever value might be contained within register 0 (or if specified as a branch address register, then no branch was taken, and the contents of register 0 was ignored, but the linkage address was loaded).
This specific behavior permitted initial execution of an interrupt routines, since base registers would not necessarily be set to 0 during the first few instruction cycles of an interrupt routine. It isn't needed for IPL ("Initial Program Load" or boot), as one can always clear a register without the need to save it.
With the exception of the Model 67, all addresses were real memory addresses. Virtual memory was not available in most IBM mainframes until the System/370 series. The Model 67 introduced a virtual memory architecture, which MTS, CP-67, and TSS/360 used—but not IBM's mainline System/360 operating systems.
The System/360 machine-code instructions are 2 bytes long (no memory operands), 4 bytes long (one operand), or 6 bytes long (two operands). Instructions are always situated on 2-byte boundaries.
Operations like MVC (Move-Characters) (Hex: D2) can only move at most 256 bytes of information. Moving more than 256 bytes of data required multiple MVC operations. (The System/370 series introduced a family of more powerful instructions such as the MVCL "Move-Characters-Long" instruction, which supports moving up to 16 MB as a single block.)
An operand is two bytes long, typically representing an address as a 4-bit nibble denoting a base register and a 12-bit displacement relative to the contents of that register, in the range 000–FFF (shown here as hexadecimal numbers). The address corresponding to that operand is the contents of the specified general-purpose register plus the displacement. For example, an MVC instruction that moves 256 bytes (with length code 255 in hexadecimal as FF) from base register 7, plus displacement 000, to base register 8, plus displacement 001, would be coded as the 6-byte instruction "D2FF 8001 7000" (operator/length/address1/address2).
The System/360 was designed to separate the "system state" from the "problem state". This provided a basic level of security and recoverability from programming errors. Problem (user) programs could not modify data or program storage associated with the system state. Addressing, data, or operation exception errors made the machine enter the system state through a controlled routine so the operating system could try to correct or terminate the program in error. Similarly, it could recover certain processor hardware errors through the "machine check" routines.
Channels.
Peripherals interfaced to the system via "channels". A channel was a specialized processor with the instruction set optimized for transferring data between a peripheral and main memory. In modern terms, this could be compared to direct memory access (DMA).
Byte-multiplexor and selector channels.
There were initially two types of channels; byte-multiplexer channels (known at the time simply as "multiplexor channels"), for connecting "slow speed" devices such as card readers and punches, line printers, and communications controllers, and selector channels for connecting high speed devices, such as disk drives, tape drives, data cells and drums. Every System/360 (except for the Model 20, which was not a standard 360) had a byte-multiplexer channel and 1 or more selector channels. The smaller models (up to the model 50) had integrated channels, while for the larger models (model 65 and above) the channels were large separate units in separate cabinets, such as the IBM 2860 and 2870. (The 60, 62, and 70 had allowed only for 2860 selector channels, on the assumption that they would all have smaller 360s attached, which would do the slow-speed work.)
The byte-multiplexer channel was able to handle I/O to/from several devices simultaneously at the device's highest rated speeds, hence the name, as it multiplexed I/O from those devices onto a single data path to main memory. Devices connected to a byte-multiplexer channel were configured to operate in 1-byte, 2-byte, 4-byte, or "burst" mode. The larger "blocks" of data were used to handle progressively faster devices. For example, a 2501 card reader operating at 600 cards per minute would be in 1-byte mode, while a 1403-N1 printer would be in burst mode. Also, the byte-multiplexer channels on larger models had an optional selector subchannel section that would accommodate tape drives. The byte-multiplexor's channel address was typically "0" and the selector subchannel addresses were from "C0" to "FF." Thus, tape drives on System/360 were commonly addressed at 0C0-0C7. Other common byte-multiplexer addresses were: 00A: 2501 Card Reader, 00C/00D: 2540 Reader/Punch, 00E/00F: 1403-N1 Printers, 010-013: 3211 Printers, 020-0BF: 2701/2703 Telecommunications Units. These addresses are still commonly used in z/VM virtual machines.
System/360 models 40 and 50 had an integrated 1052-7 console that was usually addressed as 01F, however, this was not connected to the byte-multiplexer channel, but rather, had a direct internal connection to the mainframe. The model 30 attached a different model of 1052 through a 1051 control unit. The models 60 through 75 also used the 1052-7.
Selector channels enabled I/O to high speed devices. These storage devices were attached to a control unit and then to the channel. The control unit let clusters of devices be attached to the channels. On higher speed models, multiple selector channels, which could operate simultaneously or in parallel, improved overall performance.
Control units were connected to the channels with gray "bus and tag" cable pairs. The bus cables carried the address and data information and the tag cables identified what data was on the bus. The general configuration of a channel was to connect the devices in a chain, like this: Mainframe—Control Unit X—Control Unit Y—Control Unit Z. Each control unit was assigned a "capture range" of addresses that it serviced. For example, control unit X might capture addresses 40-4F, control unit Y: C0-DF, and control unit Z: 80-9F. Capture ranges had to be a multiple of 8, 16, 32, 64, or 128 devices and be aligned on appropriate boundaries. Each control unit in turn had one or more devices attached to it. For example, you could have control unit Y with 6 disks, that would be addressed as C0-C5.
The cable ordering of the control units on the channel was also significant. Each control unit was "strapped" as High or Low priority. When a device selection was sent out on a mainframe's channel, the selection was sent from X->Y->Z->Y->X. If the control unit was "high" then the selection was checked in the outbound direction, if "low" then the inbound direction. Thus, control unit X was either 1st or 5th, Y was either 2nd or 4th, and Z was 3rd in line. It was also possible to have multiple channels attached to a control unit from the same or multiple mainframes, thus providing a rich high-performance, multiple-access, and backup capability.
Typically the total cable length of a channel was limited to 200 feet, less being preferred. Each control unit accounted for about 10 "feet" of the 200-foot limit.
Block multiplexer channel.
IBM introduced a new type of I/O channel on the Model 85 and Model 195: the 2880 block multiplexer channel. The channel allowed a device to suspend a channel program, pending the completion of an I/O operation and thus to free the channel for use by another device. The initial use for this was the 2305 fixed-head disk, which had 8 "exposures" (alias addresses) and rotational position sensing (RPS).
These channels could support either standard 1.5 MB/second connections or, with the 2-byte interface feature, 3 MB/second; the later used one tag cable and two bus cables.
Basic hardware components.
Being somewhat uncertain of the reliability and availability of the then new monolithic integrated circuits, IBM chose instead to design custom hybrid integrated circuits using discrete flip chip mounted glass encapsulated transistors and diodes with silk screened resistors on a ceramic substrate. This substrate was then either encapsulated in plastic or covered with a metal lid to create a "Solid Logic Technology" (SLT) module.
A number of these were then mounted onto a small multi-layer printed circuit "SLT card". Each card had one or two sockets on one edge that plugged onto pins on one of the computer's "SLT boards". This was the reverse of how most other company's cards were mounted, where the cards had pins which plugged into sockets on the computer's boards.
Up to twenty SLT boards could be assembled side-by-side (vertically and horizontally) to form a "logic gate". Several gates mounted together constituted a box-shaped "logic frame". The outer gates were generally hinged along one vertical edge so they could be swung open to provide access to the fixed inner gates. The larger machines could have more than one frame bolted together to produce the final unit, such as a multi-frame Central Processing Unit (CPU).
Operating system software.
The smaller System/360 models used the Basic Operating System/360 (BOS/360), Tape Operating System (TOS/360), or Disk Operating System/360 (DOS/360, which evolved into DOS/VS, DOS/VSE, VSE/AF, VSE/SP, VSE/ESA, and then z/VSE).
The larger models used Operating System/360 (OS/360): Primary Control Program (PCP), Multiprogramming with a Fixed number of Tasks (MFT), which evolved into OS/VS1, and Multiprogramming with a Variable number of Tasks (MVT), which evolved into MVS. MVT took a long time to develop into a usable system, and the less ambitious MFT was widely used. PCP was used on intermediate machines; the final releases of OS/360 included only MFT and MVT.
When it announced the Model 67 in August 1965, IBM also announced TSS/360 (Time-Sharing System) for delivery at the same time as the 67. TSS/360, a response to Multics, was an ambitious project that included many advanced features. It never worked properly, was delayed, canceled, reinstated, and finally canceled again in 1971. It was replaced by CP-67, MTS (Michigan Terminal System), TSO (Time Sharing Option for OS/360), or one of several other time-sharing systems.
CP-67, the original virtual machine system, was also known as CP/CMS. CP/67 was developed outside the IBM mainstream at IBM's Cambridge Scientific Center, in cooperation with MIT researchers. CP/CMS eventually won wide acceptance, and led to the development of VM/370 (aka VM/CMS) and today's z/VM.
The Model 20 offered a simplified and rarely used tape-based system called TPS (Tape Processing System), and DPS (Disk Processing System) that provided support for the 2311 disk drive. TPS could run on a machine with 8 KB of memory; DPS required 12 KB, which was pretty hefty for a Model 20. Many customers ran quite happily with 4 KB and CPS (Card Processing System). With TPS and DPS, the card reader was used to read the Job Control Language cards that defined the stack of jobs to run and to read in transaction data such as customer payments. The operating system was held on tape or disk, and results could also be stored on the tapes or hard drives. Stacked job processing became an exciting possibility for the small but adventurous computer user.
A little known and little used suite of 80 column punched-card utility programs known as Basic Programming Support (BPS) (jocularly: Barely Programming Support), a precursor of TOS, was available for smaller systems.
Component names.
IBM created a new naming system for the new components created for System/360, although well-known old names, like IBM 1403 and IBM 1052, were retained. In this new naming system, components were given four-digit numbers starting with 2. The second digit described the type of component, as follows:
Peripherals.
IBM developed a new family of peripheral equipment for System/360, carrying over a few from its older 1400 series. Interfaces were standardized, allowing greater flexibility to mix and match processors, controllers and peripherals than in the earlier product lines.
In addition, System/360 computers could use certain peripherals that were originally developed for earlier computers. These earlier peripherals used a different numbering system, such as the IBM 1403 chain printer. The 1403, an extremely reliable device that had already earned a reputation as a workhorse, was sold as the 1403-N1 when adapted for the System/360.
Also available were optical character recognition (OCR) readers 1287 and 1288.
Most small systems were sold with an IBM 1052-7 as the console typewriter. This was tightly integrated into the CPU — the keyboard would physically lock under program control. Certain high-end machines could optionally be purchased with a 2250 graphical display, costing upwards of US $100,000. The 360/85 used a 5450 display console that was not compatible with anything else in the line; the later 3066 console for the 370/165 and 370/168 used the same basic display design as the 360/85.
Direct access storage devices (DASD).
The first disk drives for System/360 were IBM 2302s:60–65 and IBM 2311s.:54–58
The 156 KB/second 2302 was based on the earlier 1302 and was available as a model 3 with two 112.79 MB modules:60 or as a model 4 with four such modules.:60
The 2311, with a removable 1316 disk pack, was based on the IBM 1311 and had a theoretical capacity of 7.2 MB, although actual capacity varied with record design.:31 (When used with a 360/20, the 1316 pack was formatted into fixed-length 270 byte sectors, giving a maximum capacity of 5.4MB.)
In 1966, the first 2314s shipped. This device had up to eight usable disk drives with an integral control unit; there were nine drives, but one was reserved as a spare. Each drive used a removable 2316 disk pack with a capacity of nearly 28 MB. The disk packs for the 2311 and 2314 were "physically" large by today's standards — e.g., the 1316 disk pack was about 14 in in diameter and had six platters stacked on a central spindle. The top and bottom outside platters did not store data. Data were recorded on the inner sides of the top and bottom platters and both sides of the inner platters, providing 10 recording surfaces. The 10 read/write heads moved together across the surfaces of the platters, which were formatted with 203 concentric tracks. To reduce the amount of head movement (seeking), data was written in a virtual cylinder from inside top platter down to inside bottom platter. These disks were not usually formatted with fixed-sized sectors as are today's hard drives (though this "was" done with CP/CMS). Rather, most System/360 I/O software could customize the length of the data record (variable-length records), as was the case with magnetic tapes.
Some of the most powerful early System/360s used high-speed head-per-track drum storage devices. The 3,500 RPM 2301, which replaced the 7320, was part of the original System/360 announcement, with a capacity of 4 MB. The 303.8 KB/second IBM 2303:74–76 was announced on January 31, 1966, with a capacity of 3.913 MB. These were the only drums announced for System/360 and System/370, and their niche was later filled by fixed-head disks.
The 6,000 RPM 2305 appeared in 1970, with capacities of 5 MB (2305-1) or 11 MB (2305-2) per module. Although these devices did not have large capacity, their speed and transfer rates made them attractive for high-performance needs. A typical use was overlay linkage (e.g. for OS and application subroutines) for program sections written to alternate in the same memory regions. Fixed head disks and drums were particularly effective as paging devices on the early virtual memory systems. The 2305, although often called a "drum" was actually a head-per-track disk device, with 12 recording surfaces and a data transfer rate up to 3 MB per second.
Rarely seen was the IBM 2321 Data Cell, a mechanically complex device that contained multiple magnetic strips to hold data; strips could be randomly accessed, placed upon a cylinder-shaped drum for read/write operations; then returned to an internal storage cartridge. The IBM Data Cell [noodle picker] was among several IBM trademarked "speedy" mass online direct-access storage peripherals (reincarnated in recent years as "virtual tape" and automated tape librarian peripherals). The 2321 file had a capacity of 400 MB, at the time when the 2311 disk drive only had 7.2 MB. The IBM Data Cell was proposed to fill cost/capacity/speed gap between magnetic tapes—which had high capacity with relatively low cost per stored byte—and disks, which had higher expense per byte. Some installations also found the electromechanical operation less dependable and opted for less mechanical forms of direct-access storage.
The Model 44 was unique in offering an integrated single-disk drive as a standard feature. This drive used the 2315 "ramkit" cartridge and provided 1,171,200 bytes of storage.:11
Tape drives.
The 2400 tape drives consisted of a combined drive and control unit, plus individual 1/2" tape drives attached. With System/360, IBM switched from IBM 7 track to 9 track tape format. 2400 drives could be purchased that read and wrote 7 track tapes for compatibility with the older IBM 729 tape drives. In 1967, a slower and cheaper pair of tape drives with integrated control unit was introduced: the 2415. In 1968, the IBM 2420 tape system was released, offering much higher data rates, self-threading tape operation and 1600bpi packing density. It remained in the product line until 1979.
Remaining machines.
Few of these machines remain. Despite being sold or leased in very large numbers for a mainframe system of its era, only a few System/360 computers still exist, and none of them still run. Most machines were scrapped when they could no longer profitably be leased, certainly for the value of the gold and other precious metal content of their circuits but possibly also to keep these machines from competing with IBM's newer computers, such as the System/370. As with all classic mainframe systems, complete System/360 computers were prohibitively large to put in storage, and too expensive to maintain.
The Smithsonian Institution owns a System/360 Model 65, though it is no longer on public display. The Computer History Museum in Mountain View, CA has a non-working System/360 Model 30 on display, as do the Museum of Transport and Technology (Motat) in Auckland, New Zealand and the Vienna University of Technology in Austria. The University of Western Australia Computer Club has a complete in storage. The IBM museum in Sindelfingen has two System/360s – a Model 20 and a Model 91 floating point machine. The control panel of the most complex System/360 model type built, the FAA IBM 9020, comprising up to 12 System/360 Model 65s and Model 50s in its maximum configuration is on display in the Computer Science department of Stanford University as . It was manufactured in 1971 and decommissioned in 1993. The IBM Endicott History and Heritage Center in Endicott, NY has a non-working System/360 Model 30 and an associated 2401 magnetic tape drive on display.
IBM 360 in popular culture.
 (Movie; 1970): "IBM 360" in the background 15min 39sec into the movie.
 (TV Series: 2007-2016): The "IBM 360" was featured as a plot device where a company leased the system to the advertising agency and was a prominent background in the seventh season. 
References.
</dl>
External links.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="29298" url="http://en.wikipedia.org/wiki?curid=29298" title="Spouse">
Spouse

A spouse is a life partner in a marriage, civil union, domestic partnership or common-law marriage (common-law marriage does not exist under English law). The term is gender neutral, whereas a male spouse is a husband and a female spouse is a wife. Although a spouse is a form of significant other, the latter term also includes non-marital partners who play a social role similar to that of a spouse, but do not have rights and duties reserved by law to a spouse.
Legal status.
The legal status of a spouse, and the specific rights and obligations associated with that status, vary significantly between different jurisdictions of the world. These regulations are usually described in family law statutes. However, in many parts of the world, where civil marriage is not that prevalent, there is instead customary marriage, which is usually regulated informally by the community. In many parts of the world, spousal rights and obligations are related to the payment of bride price, dowry or dower. Historically, many societies have given sets of rights and obligations to male marital partners that have been very different from the sets of rights and obligations given to female marital partners. In particular, the control of marital property, inheritance rights, and the right to dictate the activities of children of the marriage, have typically been given to male marital partners. However, this practice was curtailed to a great deal in many countries in the twentieth century, and more modern statutes tend to define the rights and duties of a spouse without reference to gender. Among the last European countries to establish full gender equality in marriage were Switzerland, Greece, Spain, and France in the 1980s. In various marriage laws around the world, however, the husband continues to have authority; for instance the Civil Code of Iran states at Article 1105: "In relations between husband and wife; the position of the head of the family is the exclusive right of the husband".
Depending on jurisdiction, the refusal or inability of a spouse to perform the marital obligations may constitute a ground for divorce, legal separation or annulment. The latter two options are more prevalent in countries where the dominant religion is Roman Catholicism, some of which introduced divorce only recently (ie. Italy in 1970, Portugal in 1975, Brazil in 1977, Spain in 1981, Argentina in 1987, Paraguay in 1991, Colombia in 1991, Ireland in 1996, Chile in 2004 and Malta in 2011). In recent years, many Western countries have adopted no fault divorce. In some parts of the world, the formal dissolution of a marriage is complicated by the payments and goods which have been exchanged between families (this is common where marriages are arranged). This often makes it difficult to leave a marriage, especially for the woman: in some parts of Africa, once the bride price has been paid, the wife is seen as belonging to the husband and his family; and if she wants to leave, the husband may demand back the bride price that he had paid to the girl's family. The girl's family often cannot or does not want to pay it back.
Regardless of legislation, personal relations between spouses may also be influenced by local culture and religion, which may promote male authority over the wife: for instance the word (ba`al), Hebrew for "husband", used throughout the Bible, is synonymous with "owner" and "master".
Minimum age.
There is often a minimum legal marriageable age. The United Nations Population Fund stated the following:
Procreation.
Although in Western countries spouses sometimes choose not to have children, such a choice is not accepted in some parts of the world. In some cultures and religions, the quality of a spouse imposes an obligation to have children. In northern Ghana, for example, the payment of bride price signifies a woman's requirement to bear children, and women using birth control are at risks of threats and coercion.
Choosing a spouse.
There are many ways in which a spouse is chosen, which vary across the world, and include love marriage, arranged marriage, and forced marriage. The latter is in some jurisdictions a void marriage or a voidable marriage. Forcing someone to marry is also a criminal offense in some countries.
Violence between spouses.
Violence between spouses occurs when one spouse inflicts violence on the other (sometimes the violence can be mutual). The prevalence of this type of violence is difficult to estimate accurately, but in some parts of the world it is very common.

</doc>
<doc id="29299" url="http://en.wikipedia.org/wiki?curid=29299" title="Sexuality (disambiguation)">
Sexuality (disambiguation)

Human sexuality is the capacity to have erotic experiences and responses.
Sexuality may also refer to:

</doc>
<doc id="29301" url="http://en.wikipedia.org/wiki?curid=29301" title="Semiotics">
Semiotics

Semiotics (also called semiotic studies; not to be confused with the Saussurean tradition called semiology) is the study of meaning-making, the philosophical theory of signs and symbols. This includes the study of signs and sign processes (semiosis), indication, designation, likeness, analogy, metaphor, symbolism, signification, and communication. Semiotics is closely related to the field of linguistics, which, for its part, studies the structure and meaning of language more specifically. The Semiotic Tradition explores the study of signs and symbols as a significant part of communications. As different from linguistics, however, semiotics also studies non-linguistic sign systems. Semiotics often is divided into three branches:
Semiotics frequently is seen as having important anthropological dimensions; for example, Umberto Eco proposes that every cultural phenomenon may be studied as communication. Some semioticians focus on the logical dimensions of the science, however. They examine areas belonging also to the life sciences – such as how organisms make predictions about, and adapt to, their semiotic niche in the world (see semiosis). In general, semiotic theories take "signs" or sign systems as their object of study: the communication of information in living organisms is covered in biosemiotics (including zoosemiotics).
Syntactics is the branch of semiotics that deals with the formal properties of signs and symbols. More precisely, syntactics deals with the "rules that govern how words are combined to form phrases and sentences".
Charles Morris adds that semantics deals with the relation of signs to their designata and the objects that they may or do denote; and, pragmatics deals with the biotic aspects of semiosis, that is, with all the psychological, biological, and sociological phenomena that occur in the functioning of signs.
Terminology.
The term derives from the Greek σημειωτικός "sēmeiōtikos", "observant of signs", (from σημεῖον "sēmeion", "a sign, a mark",) and it was first used in English by Henry Stubbes (spelt "semeiotics") in a very precise sense to denote the branch of medical science relating to the interpretation of signs. John Locke used the term sem(e)iotike in Book 4, Chapter 21 of "An Essay Concerning Human Understanding" (1690). Here he explains how science may be divided into three parts:
All that can fall within the compass of human understanding, being either, first, the nature of things, as they are in themselves, their relations, and their manner of operation: or, secondly, that which man himself ought to do, as a rational and voluntary agent, for the attainment of any end, especially happiness: or, thirdly, the ways and means whereby the knowledge of both the one and the other of these is attained and communicated; I think science may be divided properly into these three sorts.—Locke, 1823/1963, p. 174
Locke then elaborates on the nature of this third category, naming it Σημειωτική ("Semeiotike") and explaining it as "the doctrine of signs" in the following terms:
Nor is there any thing to be relied upon in Physick, but an exact knowledge of medicinal physiology (founded on observation, not principles), semiotics, method of curing, and tried (not excogitated, not commanding) medicines.—Locke, 1823/1963, 4.21.4, p. 175
In the nineteenth century, Charles Sanders Peirce defined what he termed "semiotic" (which he sometimes spelled as "semeiotic") as the "quasi-necessary, or formal doctrine of signs", which abstracts "what must be the characters of all signs used by... an intelligence capable of learning by experience", and which is philosophical logic pursued in terms of signs and sign processes. The Peirce scholar and editor Max H. Fisch claimed in 1978 that "semeiotic" was Peirce's own preferred rendering of Locke's σημιωτική.
Charles Morris followed Peirce in using the term "semiotic" and in extending the discipline beyond human communication to animal learning and use of signals.
Ferdinand de Saussure, however, founded his semiotics, which he called semiology, in the social sciences:
It is... possible to conceive of a science which studies the role of signs as part of social life. It would form part of social psychology, and hence of general psychology. We shall call it semiology (from the Greek "semeîon", 'sign'). It would investigate the nature of signs and the laws governing them. Since it does not yet exist, one cannot say for certain that it will exist. But it has a right to exist, a place ready for it in advance. Linguistics is only one branch of this general science. The laws which semiology will discover will be laws applicable in linguistics, and linguistics will thus be assigned to a clearly defined place in the field of human knowledge.—Cited in Chandler's "Semiotics for Beginners", Introduction.
While the Saussurean semiotic is dyadic (sign/syntax, signal/semantics), the Peircean semiotic is triadic (sign, object, interpretant), being conceived of as philosophical logic studied in terms of signs that are not always linguistic or artificial. The Peircean semiotic addresses not only the external communication mechanism, as per Saussure, but the internal representation machine, investigating not just sign processes, or modes of inference, but the whole inquiry process in general. Peircean semiotics further subdivides each of the three triadic elements into three sub-types. For example, signs can be icons, indices and symbols.
Yuri Lotman introduced Eastern Europe to semiotics and adopted Locke’s coinage as the name to subtitle ("Σημειωτική") his founding at the University of Tartu in Estonia in 1964 of the first semiotics journal, "Sign Systems Studies".
Thomas Sebeok assimilated "semiology" to "semiotics" as a part to a whole, and was involved in choosing the name "Semiotica" for the first international journal devoted to the study of signs.
History.
The importance of signs and signification has been recognized throughout much of the history of philosophy, and in psychology as well. Plato and Aristotle both explored the relationship between signs and the world, and Augustine considered the nature of the sign within a conventional system. These theories have had a lasting effect in Western philosophy, especially through scholastic philosophy. (More recently, Umberto Eco, in his "Semiotics and the Philosophy of Language", has argued that semiotic theories are implicit in the work of most, perhaps all, major thinkers.)
The general study of signs that began in Latin with Augustine culminated in Latin with the 1632 "Tractatus de Signis" of John Poinsot, and then began anew in late modernity with the attempt in 1867 by Charles Sanders Peirce to draw up a “new list of categories”. Peirce aimed to base his new list directly upon experience precisely as constituted by action of signs, in contrast with the list of Aristotle’s categories which aimed to articulate within experience the dimension of being that is independent of experience and knowable as such, through human understanding.
The estimative powers of animals interpret the environment as sensed to form a “meaningful world” of objects, but the objects of this world (or "Umwelt", in Jakob von Uexküll’s term,) consist exclusively of objects related to the animal as desirable (+), undesirable (–), or “safe to ignore” (0).
In contrast to this, human understanding adds to the animal Umwelt a relation of self-identity within objects which transforms objects experienced into "things" as well as +, –, 0 objects. Thus the generically animal objective world as Umwelt, becomes a species-specifically human objective world or Lebenswelt, wherein linguistic communication, rooted in the biologically underdetermined Innenwelt of human animals, makes possible the further dimension of cultural organization within the otherwise merely social organization of animals whose powers of observation may deal only with directly sensible instances of objectivity. This further point, that human culture depends upon language understood first of all not as communication, but as the biologically underdetermined aspect or feature of the human animal’s Innenwelt, was originally clearly identified by Thomas A. Sebeok. Sebeok also played the central role in bringing Peirce’s work to the center of the semiotic stage in the twentieth century, first with his expansion of the human use of signs (“anthroposemiosis”) to include also the generically animal sign-usage ("zoösemiosis"), then with his further expansion of semiosis (based initially on the work of Martin Krampen, but taking advantage of Peirce’s point that an interpretant, as the third item within a sign relation, “need not be mental”) to include the vegetative world (“phytosemiosis”).
Peirce’s distinction of an interpretant from an interpreter, with the further qualification that the former need not be “of a mental mode of being”—not his demonstration that sign relations are perforce irreducibly triadic, as is commonly assumed in his following so far as the followers continue the modern tradition of ignoring the Latin Age of philosophy’s history—was his most revolutionary move and most seminal contribution to the doctrine of signs. Peirce’s "interpretant" notion opened the way to understanding an action of signs beyond the realm of animal life (study of "phytosemiosis" + "zoösemiosis" + "anthroposemiosis" = "biosemiotics"), which was his first advance beyond Latin Age semiotics.
Other early theorists in the field of semiotics include Charles W. Morris. Max Black argued that the work of Bertrand Russell was seminal in the field.
Formulations.
Semioticians classify signs or sign systems in relation to the way they are transmitted (see modality). This process of carrying meaning depends on the use of codes that may be the individual sounds or letters that humans use to form words, the body movements they make to show attitude or emotion, or even something as general as the clothes they wear. To coin a word to refer to a "thing" (see lexical words), the community must agree on a simple meaning (a denotative meaning) within their language, but that word can transmit that meaning only within the language's grammatical structures and codes (see syntax and semantics). Codes also represent the values of the culture, and are able to add new shades of connotation to every aspect of life.
To explain the relationship between semiotics and communication studies, communication is defined as the process of transferring data and-or meaning from a source to a receiver. Hence, communication theorists construct models based on codes, media, and contexts to explain the biology, psychology, and mechanics involved. Both disciplines also recognize that the technical process cannot be separated from the fact that the receiver must decode the data, i.e., be able to distinguish the data as salient, and make meaning out of it. This implies that there is a necessary overlap between semiotics and communication. Indeed, many of the concepts are shared, although in each field the emphasis is different. In "Messages and Meanings: An Introduction to Semiotics", Marcel Danesi (1994) suggested that semioticians' priorities were to study signification first, and communication second. A more extreme view is offered by Jean-Jacques Nattiez (1987; trans. 1990: 16), who, as a musicologist, considered the theoretical study of communication irrelevant to his application of semiotics.
Semiotics differs from linguistics in that it generalizes the definition of a sign to encompass signs in any medium or sensory modality. Thus it broadens the range of sign systems and sign relations, and extends the definition of language in what amounts to its widest analogical or metaphorical sense.
Peirce's definition of the term "semiotic" as the study of necessary features of signs also has the effect of distinguishing the discipline from linguistics as the study of contingent features that the world's languages happen to have acquired in the course of their evolutions.
From a subjective standpoint, perhaps more difficult is the distinction between semiotics and the philosophy of language. In a sense, the difference lies between separate traditions rather than subjects. Different authors have called themselves "philosopher of language" or "semiotician". This difference does "not" match the separation between analytic and continental philosophy.
On a closer look, there may be found some differences regarding subjects. Philosophy of language pays more attention to natural languages or to languages in general, while semiotics is deeply concerned with non-linguistic signification. Philosophy of language also bears connections to linguistics, while semiotics might appear closer to some of the humanities (including literary theory) and to cultural anthropology.
Semiosis or "semeiosis" is the process that forms meaning from any organism's apprehension of the world through signs. Scholars who have talked about semiosis in their sub-theories of semiotics include C. S. Peirce, John Deely, and Umberto Eco. Cognitive semiotics is combining methods and theories developed in the disciplines of cognitive methods and theories developed in semiotics and the humanities, with providing new information into human signification and its manifestation in cultural practices. The research on cognitive semiotics brings together semiotics from linguistics, cognitive science, and related disciplines on a common meta-theoretical platform of concepts, methods, and shared data.
Cognitive semiotics may also be seen as the study of meaning-making by employing and integrating methods and theories developed in the cognitive sciences. This involves conceptual and textual analysis as well as experimental investigations. Cognitive semiotics initially was developed at the Center for Semiotics at Aarhus University (Denmark), with an important connection with the Center of Functionally Integrated Neuroscience (CFIN) at Aarhus Hospital. Amongst the prominent cognitive semioticians are Per Aage Brandt, Svend Østergaard, Peer Bundgård, Frederik Stjernfelt, Mikkel Wallentin, Kristian Tylén, Riccardo Fusaroli, and Jordan Zlatev. Zlatev later in co-operation with Göran Sonesson established CCS (Center for Cognitive Semiotics) at Lund University, Sweden.
Current applications.
Applications of semiotics include:
In some countries, its role is limited to literary criticism and an appreciation of audio and visual media, but this narrow focus may inhibit a more general study of the social and political forces shaping how different media are used and their dynamic status within modern culture. Issues of technological determinism in the choice of media and the design of communication strategies assume new importance in this age of mass media.
Publication of research is both in dedicated journals such as "Sign Systems Studies", established by Yuri Lotman and published by Tartu University Press; "Semiotica", founded by Thomas A. Sebeok and published by Mouton de Gruyter; "Zeitschrift für Semiotik"; "European Journal of Semiotics"; "Versus" (founded and directed by Umberto Eco), et al.; "The American Journal of Semiotics"; and as articles accepted in periodicals of other disciplines, especially journals oriented toward philosophy and cultural criticism.
The major semiotic book series "Semiotics, Communication, Cognition", published by De Gruyter Mouton (series editors Paul Cobley and Kalevi Kull) replaces the former "Approaches to Semiotics" (more than 120 volumes) and "Approaches to Applied Semiotics" (series editor Thomas A. Sebeok). Since 1980 the Semiotic Society of America has produced an annual conference series: "".
Branches.
Semiotics has sprouted a number of subfields, including, but not limited to, the following:
Pictorial semiotics.
Pictorial semiotics is intimately connected to art history and theory. It goes beyond them both in at least one fundamental way, however. While art history has limited its visual analysis to a small number of pictures that qualify as "works of art", pictorial semiotics focuses on the properties of pictures general sense. It has also focused on how the artistic conventions of images can be interpreted through pictorial codes. Pictorial codes are the way in which viewers of pictorial representations seem to automatically decipher the artistic conventions of images by being unconsciously familiar with them.
According to Göran Sonesson, a Swedish semiotician, pictures can be analyzed by three models: the narrative model, which concentrates on the relationship between pictures and time in a chronological manner as in a comic strip; the rhetoric model, which compares pictures with different devices as in a metaphor; and the laokoon (or laocoon) model which considers the limits and constraints of pictorial expressions by comparing textual mediums that utilize time with visual mediums that utilize space.
The break from traditional art history and theory—as well as from other major streams of semiotic analysis—leaves open a wide variety of possibilities for pictorial semiotics. Some influences have been drawn from phenomenological analysis, cognitive psychology, structuralist and cognitivist linguistics, and visual anthropology and sociology.
Semiotics of food.
Food has been one traditional topic of choice in relating semiotic theory because it is extremely accessible and easily relatable to the average individual’s life.
Food is said to be semiotic because it transforms meaning with preparation. Food that is eaten by a wild animal raw from a carcass is obviously different in meaning when compared to a food that is prepared by humans in a kitchen to represent a cultural dish.
Food also may be said to be symbolic of certain social codes. “If food is treated as a code, the messages it encodes will be found in the pattern of social relations being expressed. The message is about different degrees of hierarchy, inclusion and exclusion, boundaries and transactions across boundaries”.
Food is a semiotic regardless of how it is prepared. Whether food is prepared with precision in a fine dining restaurant, picked from a dumpster, plucked, devoured, or even consumed by a wild animal, meaning always may be extracted from the way a certain food has been prepared and the context in which it is served.
Semiotics and globalization.
Studies have shown that semiotics may make or break a brand. Culture codes strongly influence whether a population likes or dislikes a brand’s marketing, especially internationally. If the company is unaware of a culture’s codes, it runs the risk of failing in its marketing. Globalization has caused the development of a global consumer culture where products have similar associations, whether positive or negative, across numerous markets.
Mistranslations may lead to instances of Engrish or Chinglish, terms for unintentionally humorous cross-cultural slogans intended to be understood in English. This may be caused by a sign that, in Peirce's terms, mistakenly indexes or symbolizes something in one culture, that it does not in another. In other words, it creates a connotation that is culturally-bound, and that violates some culture code. Theorists who have studied humor such as Schopenhauer suggest that contradiction or incongruity creates absurdity and therefore, humor. Violating a culture code creates this construct of ridiculousness for the culture that owns the code. Intentional humor also may fail cross-culturally because jokes are not on code for the receiving culture.
A good example of branding according to cultural code is Disney’s international theme park business. For example, Disney fits well with Japan's cultural code because the Japanese value “cuteness”, politeness, and gift giving as part of their culture code; Tokyo Disneyland sells the most souvenirs of any Disney theme park. In contrast, Disneyland Paris failed when it launched as Euro Disney because the company did not research the codes underlying European culture. Its storybook retelling of European folktales was taken as elitist and insulting, and the strict appearance standards that it had for employees resulted in discrimination lawsuits in France. Disney souvenirs were perceived as cheap trinkets. The park was a financial failure because its code violated the expectations of European culture in ways that were offensive.
On the other hand, some researchers have suggested that it is possible to successfully pass a sign perceived as a cultural icon, such as the Coca-Cola or McDonald's logos, from one culture to another. This may be accomplished if the sign is migrated from a more economically-developed to a less developed culture. The intentional association of a product with another culture is called Foreign Consumer Culture Positioning (FCCP). Products also may be marketed using global trends or culture codes, for example, saving time in a busy world; but even these may be fine-tuned for specific cultures.
Research also found that, as airline industry brandings grow and become more international, their logos become more symbolic and less iconic. The iconicity and symbolism of a sign depends on the cultural convention and, are on that ground in relation with each other. If the cultural convention has greater influence on the sign, the signs get more symbolic value.
Main institutions.
A world organisation of semioticians—the International Association for Semiotic Studies, with its journal "Semiotica"—was established in 1969. The larger research centers together with extensive teaching program include the semiotics departments at the University of Tartu, Aarhus University, and Bologna University.
References.
</dl>

</doc>
<doc id="29305" url="http://en.wikipedia.org/wiki?curid=29305" title="Sojourner Truth">
Sojourner Truth

Sojourner Truth (;  1797 – November 26, 1883) was an African-American abolitionist and women's rights activist. Truth was born into slavery in Swartekill, Ulster County, New York, but escaped with her infant daughter to freedom in 1826. After going to court to recover her son, she became the first black woman to win such a case against a white man. Sojourner Truth was named Isabella ("Bell") Baumfree when she was born. She gave herself the name Sojourner Truth in 1843. Her best-known speech was delivered extemporaneously, in 1851, at the Ohio Women's Rights Convention in Akron, Ohio. The speech became widely known as "Ain't I a Woman?" (a title taken from a version of the speech rewritten by a white writer using a stereotypical Southern dialect.) During the Civil War, Truth helped recruit black troops for the Union Army; after the war, she tried unsuccessfully to secure land grants from the federal government for former slaves.
Early years.
Truth was one of the ten or twelve children born to James and Elizabeth Baumfree (or Bomefree). Colonel Hardenbergh bought James and Elizabeth Baumfree from slave traders and kept their family at his estate in a big hilly area called by the Dutch name Swartekill (just north of present-day Rifton), in the town of Esopus, New York, 95 mi north of New York City. Charles Hardenbergh inherited his father's estate and slaves.
When Charles Hardenbergh died in 1806, nine-year-old Truth (known as Belle), was sold at an auction with a flock of sheep for $100 to John Neely, near Kingston, New York. Until that time, Truth spoke only Dutch. She later described Neely as cruel and harsh, relating how he beat her daily and once even with a bundle of rods. Neely sold her in 1808, for $105, to Martinus Schryver of Port Ewen, a tavern keeper, who owned her for eighteen months. Schryver sold her in 1810 to John Dumont of West Park, New York. Although this fourth owner was kindly disposed toward her, considerable tension existed between Truth and Dumont's second wife, Elizabeth Waring Dumont, who harassed her and made her life more difficult. (John Dumont's first wife, Sarah "Sally" Waring Dumont (Elizabeth's sister), died around 1805, five years before he bought Truth.)
Around 1815, Truth met and fell in love with a slave named Robert from a neighboring farm. Robert's owner (Charles Catton, Jr., a landscape painter) forbade their relationship; he did not want his slave to have children with a slave he did not own, because he would not own the children. One day Robert sneaked over to see Truth. When Catton and his son found him, they savagely beat Robert until Dumont finally intervened, and Truth never saw Robert again. He later died as a result of the injuries, and the experience haunted Truth throughout her life. Truth eventually married an older slave named Thomas. She bore five children: James, her firstborn, who died in childhood; Diana (1815), fathered by either Robert or John Dumont; and Peter (1821), Elizabeth (1825), and Sophia (ca. 1826), all born after she and Thomas united.
Freedom.
The state of New York began, in 1799, to legislate the abolition of slavery, although the process of emancipating New York slaves was not complete until July 4, 1827. Dumont had promised to grant Truth her freedom a year before the state emancipation, "if she would do well and be faithful." However, he changed his mind, claiming a hand injury had made her less productive. She was infuriated but continued working, spinning 100 pounds of wool, to satisfy her sense of obligation to him.
Late in 1826, Truth escaped to freedom with her infant daughter, Sophia. She had to leave her other children behind because they were not legally freed in the emancipation order until they had served as bound servants into their twenties. She later said:
She found her way to the home of Isaac and Maria Van Wagenen, who took her and her baby in. Isaac offered to buy her services for the remainder of the year (until the state's emancipation took effect), which Dumont accepted for $20. She lived there until the New York State Emancipation Act was approved a year later.
Truth learned that her son Peter, then five years old, had been sold illegally by Dumont to an owner in Alabama. With the help of the Van Wagenens, she took the issue to court and, after months of legal proceedings, got back her son, who had been abused by his new owner. Truth became one of the first black women to go to court against a white man and win the case.
Truth had a life-changing religious experience during her stay with the Van Wagenens, and became a devout Christian. In 1829 she moved with her son Peter to New York City, where she worked as a housekeeper for Elijah Pierson, a Christian Evangelist. In 1832, she met Robert Matthews, also known as Prophet Matthias, and went to work for him as a housekeeper at the Matthias Kingdom communal colony. In a bizarre twist of fate, Elijah Pierson died, and Robert Matthews and Truth were accused of stealing from and poisoning him. Both were acquitted and Robert Matthews moved west.
In 1839, Truth's son Peter took a job on a whaling ship called the "Zone of Nantucket". From 1840 to 1841, she received three letters from him, though in his third letter he told her he had sent five. Peter said he also never received any of her letters. When the ship returned to port in 1842, Peter was not on board and Truth never heard from him again.
"The Spirit Calls Me".
On June 1, 1843, Truth changed her name to "Sojourner Truth" and told her friends: "The Spirit calls me, and I must go." She became a Methodist, and left to make her way traveling and preaching about the abolition of slavery. In 1844, she joined the Northampton Association of Education and Industry in Northampton, Massachusetts. Founded by abolitionists, the organization supported women's rights and religious tolerance as well as pacifism. There were, in its four-and-a-half year history a total of 240 members though no more than 120 at any one time. They lived on 470 acres, raising livestock, running a sawmill, a gristmill, and a silk factory. While there, Truth met William Lloyd Garrison, Frederick Douglass, and David Ruggles. In 1846, the group disbanded, unable to support itself. In 1845, she joined the household of George Benson, the brother-in-law of William Lloyd Garrison. In 1849, she visited John Dumont before he moved west.
Truth started dictating her memoirs to her friend Olive Gilbert, and in 1850 William Lloyd Garrison privately published her book, "The Narrative of Sojourner Truth: A Northern Slave". That same year, she purchased a home in what would become the village of Florence in Northampton for $300, and spoke at the first National Women's Rights Convention in Worcester, Massachusetts. In 1854, with proceeds from sales of the Narrative and cartes-de-visite entitled "I sell the shadow to support the substance," she paid off the mortgage held by her friend from the Community, Samuel L. Hill. In 1856 she bought a neighboring lot and then, on September 3, 1857, sold the entire property to Daniel Ives and moved to Battle Creek, Michigan.
"Ain't I a Woman?".
In 1851, Truth joined George Thompson, an abolitionist and speaker, on a lecture tour through central and western New York State. In May, she attended the Ohio Women's Rights Convention in Akron, Ohio where she delivered her famous extemporaneous speech on women's rights, later known as "Ain't I a Woman". The convention was organized by Hannah Tracy and Frances Dana Barker Gage, who both were present when Truth spoke. Different versions of Truth's words have been recorded, with the first one published a month later by Marius Robinson, a newspaper owner and editor who was in the audience. Robinson's recounting of the speech included no instance of the question "Ain't I a Woman?" Twelve years later in May 1863, Gage published another, very different, version. In it, Truth's speech pattern had characteristics of Southern slaves, and the speech included sentences and phrases that Robinson didn't report. Gage's version of the speech became the historic standard, and is known as "Ain't I a Woman?" because that question was repeated four times. Truth's own speech pattern was not Southern in nature, as she was born and raised in New York, and spoke only Dutch until she was nine years old.
In contrast to Robinson's report, Gage's 1863 version included Truth saying her 13 children were sold away from her into slavery. Truth is widely believed to have had five children, with one sold away, and was never known to boast more children. Gage's 1863 recollection of the convention conflicts with her own report directly after the convention: Gage wrote in 1851 that Akron in general and the press in particular were largely friendly to the woman's rights convention, but in 1863 she wrote that the convention leaders were fearful of the "mobbish" opponents. Other eyewitness reports of Truth's speech told a calm story, one where all faces were "beaming with joyous gladness" at the session where Truth spoke; that not "one discordant note" interrupted the harmony of the proceedings. In contemporary reports, Truth was warmly received by the convention-goers, the majority of whom were long-standing abolitionists, friendly to progressive ideas of race and civil rights. In Gage's 1863 version, Truth was met with hisses, with voices calling to prevent her from speaking.
Over the next 10 years, Truth spoke before dozens, perhaps hundreds, of audiences. From 1851 to 1853, Truth worked with Marius Robinson, the editor of the Ohio "Anti-Slavery Bugle", and traveled around that state speaking. In 1853, she spoke at a suffragist "mob convention" at the Broadway Tabernacle in New York City; that year she also met Harriet Beecher Stowe. In 1856, she traveled to Battle Creek, Michigan, to speak to a group called the Friends of Human Progress. In 1858, someone interrupted a speech and accused her of being a man; Truth opened her blouse and revealed her breasts."
Other notable speeches.
Mob Convention—September 7, 1853: At the convention, young men greeted her with "a perfect storm,” hissing and groaning. In response, Truth said, "You may hiss as much as you please, but women will get their rights anyway. You can't stop us, neither". Sojourner, like other public speakers, often adapted her speeches to how the audience was responding to her. In her speech, Sojourner speaks out for women's rights. She incorporates religious references in her speech, particularly the story of Esther. She then goes on to say that, just as women in scripture, women today are fighting for their rights. Moreover, Sojourner scolds the crowd for all their hissing and rude behavior, reminding them that God says to "Honor thy father and thy mother."
American Equal Rights Association—May 9–10, 1867: Her speech was addressed to the American Equal Rights Association, and divided into three sessions. Sojourner was received with loud cheers instead of hisses, now that she had a better-formed reputation established. The Call had advertised her name as one of the main convention speakers.
For the first part of her speech, she spoke mainly about the rights of black women. Sojourner argued that because the push for equal rights had led to black men winning new rights, now was the best time to give black women the rights they deserve too. Throughout her speech she kept stressing that "we should keep things going while things are stirring" and fears that once the fight for colored rights settles down, it would take a long time to warm people back up to the idea of colored women's having equal rights.
In the second sessions of Sojourner's speech, she utilized a story from the Bible to help strengthen her argument for equal rights for women. She ended her argument by accusing men of being self-centered, saying, "man is so selfish that he has got women's rights and his own too, and yet he won't give women their rights. He keeps them all to himself."
For the final session of Sojourner's speech, the center of her attention was mainly on women's right to vote. Sojourner told her audience that she owned her own house, as did other women, and must therefore pay taxes. Nevertheless, they were still unable to vote because they were women. Black women who were slaves were made to do hard manual work, such as building roads. Sojourner argues that if these women were able to perform such tasks, then they should be allowed to vote because surely voting is easier than building roads.
Eighth Anniversary of Negro Freedom—New Year's Day, 1871: On this occasion the Boston papers related that “...seldom is there an occasion of more attraction or greater general interest. Every available space of sitting and standing room was crowded". She starts off her speech by giving a little background about her own life. Sojourner recounts how her mother told her to pray to God that she may have good masters and mistresses. She goes on to retell how her masters were not good to her, about how she was whipped for not understanding English, and how she would question God why he had not made her masters be good to her. Sojourner admits to the audience that she had once hated white people, but she says once she met her final master, Jesus, she was filled with love for everyone. Once slaves were emancipated, she tells the crowd she knew her prayers had been answered.
That last part of Sojourner's speech brings in her main focus. Some freed slaves were living on government aid at that time, paid for by taxpayers. Sojourner announces that this is not any better for those colored people than it is for the members of her audience. She then proposes that black people are given their own land. Because a portion of the South's population contained rebels that were unhappy with the abolishment of slavery, that region of the United States was not well suited for colored people. She goes on to suggest that colored people be given land out west to build homes and prosper on.
On a mission.
1843 was a turning point for Truth. She changed her religion and adopted her chosen name. She became a Millerite Adventist in 1843, attending several Adventist campmeetings and set out preaching. However, she left the Millerites for a time after Jesus did not appear in 1844. Later in 1846 she reassociated with former members of the Millerite Movement who had joined the Seventh-day Adventist Church.
Truth sold her home in Northampton in 1857 and bought a house in Harmonia, Michigan, just west of Battle Creek. According to the 1860 census, her household in Harmonia included her daughter, Elizabeth Banks (age 35), and her grandsons James Caldwell (misspelled as "Colvin"; age 16) and Sammy Banks (age 8).
During the Civil War, Truth helped recruit black troops for the Union Army. Her grandson, James Caldwell, enlisted in the 54th Massachusetts Regiment. In 1864, Truth was employed by the National Freedman's Relief Association in Washington, D.C., where she worked diligently to improve conditions for African-Americans. In October of that year, she met President Abraham Lincoln. In 1865, while working at the Freedman's Hospital in Washington, Truth rode in the streetcars to help force their desegregation.
Truth is credited with writing a song, "", for the 1st Michigan Colored Regiment; it was said to be composed during the war and sung by her in Detroit and Washington, D.C. It is sung to the tune of "John Brown's Body" or "The Battle Hymn of the Republic". Although Truth claimed to have written the words, it has been disputed (see "Marching Song of the First Arkansas").
In 1867, Truth moved from Harmonia to Battle Creek. In 1868, she traveled to western New York and visited with Amy Post, and continued traveling all over the East Coast. At a speaking engagement in Florence, Massachusetts, after she had just returned from a very tiring trip, when Truth was called upon to speak she stood up and said,
In 1870, Truth tried to secure land grants from the federal government to former slaves, a project she pursued for seven years without success. While in Washington, D.C., she had a meeting with President Ulysses S. Grant in the White House. In 1872, she returned to Battle Creek and tried to vote in the presidential election, but was turned away at the polling place.
Truth spoke about abolition, women's rights, prison reform, and preached to the Michigan Legislature against capital punishment. Not everyone welcomed her preaching and lectures, but she had many friends and staunch support among many influential people at the time, including Amy Post, Parker Pillsbury, Frances Gage, Wendell Phillips, William Lloyd Garrison, Laura Smith Haviland, Lucretia Mott, Ellen G. White, and Susan B. Anthony."
Death and legacy.
Several days before Sojourner Truth died, a reporter came from the "Grand Rapids Eagle" to interview her. "Her face was drawn and emaciated and she was apparently suffering great pain. Her eyes were very bright and mind alert although it was difficult for her to talk." Truth died on November 26, 1883, at her home in Battle Creek, Michigan. More than 3,000 people crowded into the Battle Creek Tabernacle to pay their last respects to the black heroine. Uriah Smith presided at the services. Ellen Bradbury Paulson, who attended the funeral, said of Sojourner Truth: "She was a good SDA." She was buried at Oak Hill Cemetery in Battle Creek, beside other family members and many Seventh-day Adventist pioneers.
The calendar of saints of the Episcopal Church remembers Sojourner Truth annually, together with Elizabeth Cady Stanton, Amelia Bloomer and Harriet Ross Tubman on July 20. The calendar of saints of the Lutheran Church remembers Sojourner Truth together with Harriet Tubman on March 10.
Cultural references and commemorations.
Other honors and commemorations include (by year):
As of March 2015, K-12 schools in several states, including , , , and , are named after her, as is Sojourner–Douglass College in Baltimore (in part).

</doc>
<doc id="29306" url="http://en.wikipedia.org/wiki?curid=29306" title="STOVL">
STOVL

"See also: V/STOL"
A short take-off and vertical landing aircraft (STOVL aircraft) is a fixed-wing aircraft that is able to take off from a short runway (or take off vertically if it does not have a heavy payload) and land vertically (i.e. with no runway). The formal NATO definition (since 1991) is:
On aircraft carriers, non-catapult-assisted, fixed-wing short takeoffs are accomplished with the use of thrust vectoring, which may also be used in conjunction with a runway "ski-jump". Use of STOVL tends to allow aircraft to carry a larger payload as compared to during VTOL use, while still only requiring a short runway. The most famous examples are the Hawker Siddeley Harrier and the Sea Harrier. Although technically VTOL aircraft, they are operationally STOVL aircraft due to the extra weight carried at take-off for fuel and armaments. The same is true of the F-35B Lightning II, which demonstrated VTOL capability in test flights but is operationally STOVL.
History.
In 1951, the Lockheed XFV and the Convair XFY Pogo tailsitters were both designed around the Allison YT40 turboprop engine driving contra-rotating propellers.
The British Hawker P.1127 took off vertically in 1960, and demonstrated conventional take-off in 1961. It was developed into the Hawker Siddeley Harrier which flew in 1967.
In 1962, Lockheed built the XV-4 Hummingbird for the U.S. Army. It sought to "augment" available thrust by injecting the engine exhaust into an ejector pump in the fuselage. First flying vertically in 1963, it suffered a fatal crash in 1964. It was converted into the XV-4B Hummingbird for the U.S. Air Force as a testbed for separate, vertically mounted lift engines, similar to those used in the Yak-38 Forger. That plane flew and later crashed in 1969. The Ryan XV-5 Vertifan, which was also built for the U.S. Army at the same time as the Hummingbird, experimented with gas-driven lift fans. That plane used fans in the nose and each wing, covered by doors which resembled half garbage can lids when raised. However, it crashed twice, and proved to generate a disappointing amount of lift, and was difficult to transition to horizontal flight.
Of dozens of VTOL and V/STOL designs tried from the 1950s to 1980s, only the subsonic Hawker Siddeley Harrier and Yak-38 Forger reached operational status, with the Forger being withdrawn after the fall of the Soviet Union.
Boeing had studied another odd-looking supersonic fighter in the 1960s which never made it beyond photos in Aviation Week. Rockwell International built, and then abandoned, the Rockwell XFV-12 supersonic fighter which had an unusual wing which opened up like window blinds to create an ejector pump for vertical flight. It never generated enough lift to get off the ground despite developing 20,000 lbf of thrust. The French had a nominally Mach 2 Dassault Mirage IIIV fitted with no less than 8 lift engines that flew (and crashed), but did not have enough space for fuel or payload for combat missions. The German EWR VJ 101 used swiveling engines mounted on the wingtips with fuselage mounted lift engines, and the VJ 101C X1 reached supersonic flight (Mach 1.08) on 29 July 1964. The supersonic Hawker Siddeley P.1154, which competed with the Mirage IIIV for use in NATO, was cancelled even as the aircraft were being built.
NASA uses the abbreviation SSTOVL for Supersonic Short Take-Off / Vertical Landing, and as of 2012, the X-35B/F-35B are the only aircraft to conform with this combination within one flight.
The experimental Mach 1.7 Yakovlev Yak-141 did not find an operational customer, but its rotating rear nozzle technology found good use with the F-35B.
The F-35 Lightning II is expected to enter service by 2016.
Larger STOVL designs were considered, the Armstrong Whitworth AW.681 cargo aircraft was under development when cancelled in 1965. The Dornier Do 31 got as far as three experimental aircraft before cancellation in 1970.
Although mostly a VTOL design, the V-22 Osprey has increased payload when taking off from a short runway.

</doc>
<doc id="29307" url="http://en.wikipedia.org/wiki?curid=29307" title="Russian aircraft carrier Admiral Kuznetsov">
Russian aircraft carrier Admiral Kuznetsov

Admiral Flota Sovetskovo Soyuza Kuznetsov (Russian: Адмира́л фло́та Сове́тского Сою́за Кузнецо́в "Admiral of the Fleet of the Soviet Union Kuznetsov") was built by the Black Sea Shipyard, the sole manufacturer of Soviet aircraft carriers, in Mykolaiv within the Ukrainian Soviet Socialist Republic. The initial name of the ship was Riga; she was launched as Leonid Brezhnev, embarked on sea trials as Tbilisi, and finally named Kuznetsov. She is an aircraft cruiser (heavy aircraft-carrying missile cruiser (TAVKR) in Russian classification) serving as the flagship of the Russian Navy.
She was originally commissioned in the Soviet Navy, and was intended to be the lead ship of her class, but the only other ship of her class, "Varyag", was never completed or commissioned by the Soviet, Russian or Ukrainian navy. Later, this second hull was sold to the People's Republic of China by Ukraine, completed in Dalian and launched as "Liaoning". "Kuznetsov" was named after the Admiral of the Fleet of the Soviet Union Nikolay Gerasimovich Kuznetsov.
Role.
While designated an aircraft carrier by the West, the design of the "Admiral Kuznetsov"-class implies a mission different from that of either the United States Navy’s carriers or those of the Royal Navy. The term used by her builders to describe the Russian ships is "tyazholyy avianesushchiy raketnyy kreyser" (TAVKR or TARKR) – "heavy aircraft-carrying missile cruiser" – intended to support and defend strategic missile-carrying submarines, surface ships, and naval missile-carrying aircraft of the Russian Navy.
"Admiral Kuznetsov"'s main fixed-wing aircraft is the multi-role Sukhoi Su-33. It can perform air superiority, fleet defence, and air support missions and can also be used for direct fire support of amphibious assault, reconnaissance and placement of naval mines.
The carrier also carries the Kamov Ka-27 and Kamov Ka-27S helicopters for anti-submarine warfare, search and rescue, and small transport.
For take-off of fixed wing aircraft, "Admiral Kuznetsov" uses a ski-jump at the end of her deck. On take-off aircraft accelerate toward and up the ski-jump using their afterburners. This results in the aircraft leaving the deck at a higher angle and elevation than on an aircraft carrier with a flat deck and catapults. The ski-jump take-off is less demanding on the pilot, since the acceleration is lower, but results in a clearance speed of only 120–140 km/h (75–85 mph) requiring an aircraft design which will not stall at those speeds.
The "cruiser" role is facilitated by "Kuznetsov's" complement of 12 long-range surface-to-surface anti-ship Granit (SS-N-19) (NATO name Shipwreck) cruise missiles. This armament justifies the ship's Russian type designator "heavy aircraft-carrying missile cruiser".
History.
"Admiral Flota Sovetskovo Soyuza Kuznetsov", constructed at Nikolayev South Shipyard in Mykolaiv, Ukrainian SSR, was launched in 1985, and became fully operational in 1995. An official ceremony marking the start of construction took place on 1 September 1982; in fact she was laid down in 1983. The vessel was first named "Riga", then the name was changed to "Leonid Brezhnev", this was followed by "Tbilisi". Finally, on 4 October 1990, she was renamed "Admiral Flota Sovetskovo Soyuza Kuznetsov", referred to in short as "Admiral Kuznetsov". The ship was 71% complete by mid-1989. In November 1989 she undertook her first aircraft operation trials. In December 1991, she sailed from the Black Sea to join the Northern Fleet. Only from 1993 on did she receive aircraft.
1995–96 Mediterranean Deployment.
From 23 December 1995 through 22 March 1996 "Kuznetsov" made its first 90-day Mediterranean deployment with 13 Su-33, 2 Su-25 UTG, and 11 helicopters aboard. The deployment was to allow the carrier, which was accompanied by a frigate, destroyer and oiler, to adapt to the Mediterranean climate and conditions and to allow continuous flight operations until 21:00 each day, as the Barents Sea only receives about one hour of sunlight during this time of year. This cruise marked the 300th anniversary of the Russian Navy celebrated in 1996. During that period the carrier lay at anchor at sea off the port of Tartus, Syria. While in the Mediterranean her aircraft, mainly Su-33 fighters, made flights close to the Israeli shore line and were intercepted by Israeli F-16s. The carrier, during the deployment, encountered a severe water shortage due to the break down of evaporators.
1997–98.
At the end of 1997 she remained immobilized in a Northern Fleet shipyard, awaiting funding for major repairs, which were halted when they were only 20% complete. The overhaul was completed in July 1998, and the ship returned to active service in the Northern fleet on 3 November 1998.
2000.
"Kuznetsov" apparently remained in port for about two years before preparing for another Mediterranean deployment scheduled for the winter of 2000–2001. This deployment was cancelled due to the loss of the nuclear-powered submarine "Kursk". The "Kuznetsov" participated in operations related to the rescue and salvage of the "Kursk" submarine in late 2000. Plans for further operations were postponed or cancelled.
2003–04.
In late 2003 and early 2004, "Kuznetsov" went to sea for inspection and trials. In late October 2004, she participated in a fleet exercise of the Russian Navy in the Atlantic Ocean, and again in September 2005. During the 2005 exercise, one of her Su-33 fighters was involved in an accident, and fell from the carrier into the Atlantic Ocean.
2006.
On 27 September 2006 it was announced that "Admiral Kuznetsov" will return to service in the Northern Fleet by the end of the year. The ship will undergo another modernization refit, in an attempt to correct some of her many technical issues. Admiral Vladimir Masorin, Commander-in-Chief of the Russian Navy, also stated that several Su-33 fighters assigned to the aircraft carrier would return to the ship after undergoing maintenance and refits of their own.
2007–08 Mediterranean deployment.
 
From 5 December 2007 through 3 February 2008 "Kuznetsov" made its second Mediterranean deployment. 
On 11 December 2007, "Admiral Kuznetsov" passed by Norwegian oil platforms in the North Sea, 60 nmi outside Bergen, Norway. Su-33 fighters and Kamov helicopters were launched from the carrier while she was in the area of the rigs. The incident caused the Norwegian helicopter service to stop its flights out to the rigs, due to a risk of collision with Russian aircraft operating from the carrier. The Russian carrier was in international waters during the maneuver.
"Admiral Kuznetsov" then proceeded to the Mediterranean Sea, where she participated in an exercise together with 11 other Russian Navy surface ships and 47 aircraft. She performed three tactical training missions, using live and simulated missile launches with both air and surface missiles.
The aircraft carrier arrived back in Severomorsk on 3 February 2008 along with the Udaloy II class destroyer "Admiral Chabanenko" and Udaloy I class ASW Destroyer "Admiral Levchenko".
After a maintenance period she was back at sea on 11 October 2008 where drills were held in the Barents Sea. Russian President and Supreme Commander-in-Chief Dmitry Medvedev visited the ship on 12 October 2008 during the Stability-2008 strategic exercises.
2008–09 Mediterranean deployment.
From 5 December 2008 through 2 March 2009 "Kuznetsov" made its third Mediterranean deployment.
On 5 December 2008 the aircraft carrier and several other vessels left Severomorsk heading for the Atlantic on a tour which was announced would be lasting several months and which would include combat training including joint drills with Russia's Black Sea Fleet and visits to several ports in the Mediterranean. On this tour while the "Admiral Kuznetsov" anchored off Turkey on 7 January 2009 a small fire broke out on the ship. One crew member was killed by carbon monoxide poisoning. The fire was caused by a short-circuit.
On 16 February 2009, "Admiral Kuznetsov", along with other Russian naval vessels was involved in a large oil spill while she refuelled off the south coast of Ireland.
On 2 March 2009 "Admiral Kuznetsov" returned to her main base in Severomorsk after a three-month voyage in the Northern Atlantic and the Mediterranean waters.
2010.
In September 2010 "Admiral Kuznetsov" left a dry dock after scheduled repairs and is getting ready for a training mission in the Barents Sea at the end of that month.
2011–12 Mediterranean deployment.
The Russian Main Navy Staff announced that "Kuznetsov" will begin a deployment to the Atlantic and Mediterranean in the early days of December. The carrier will be escorted by the large ASW ship "Admiral Chabanenko". In late November 2011, Pravda and Reuters announced that a squadron led by the "Admiral Kuznetsov" will deploy to its naval facility in Tartus as a show of support for the al-Assad regime. However, in contradiction a Russian naval spokesman stated to the Izvestia daily that "The call of the Russian ships in Tartus should not be seen as a gesture towards what is going on in Syria," and "This was planned already in 2010 when there were no such events there. There has been active preparation and there is no need to cancel this," noting that "Admiral Kuznetsov" would also be making port calls in Beirut, Genoa and Cyprus.
On 29 November 2011, Army General Nikolay Makarov, Chief of the Russian General Staff, said that sending ships of the Russian Navy to the Mediterranean Sea is linked to exercises and not to the situation in Syria. "In the event of necessity, namely to carry out repairs, to take water and food on board and to allow rest for the crews, Russian ships may visit Tartus but in this case this has not been included in the plan of the trip," the Interfax source said. He also noted that the size of "Admiral Kuznetsov" does not allow it to moor in Tartus because the port does not have suitable infrastructure, i.e. large enough mooring.
On 6 December 2011, "Admiral Kuznetsov" and escorting ships departed its Northern Fleet homebase for a several month Mediterranean deployment. During the deployment "Kuznetsov" will also exercise with ships from the Russian Baltic and Black Sea Fleets.
On 12 December 2011 "Admiral Kuznetsov" with its carrier group, has been spotted northeast of the Orkneys off the coast of northern Scotland from where it has been shadowed by HMS "York" for a week. This was the first time the carrier had deployed near the UK; due to severe weather, the group took shelter in international waters in the Moray Firth, some 30 miles from the UK coast. The Kuznetsov then sailed around the top of Scotland and into the Atlantic past western Ireland, where it conducted flying operations with her Sukhoi Su-33 Flanker jets and Kamov Ka-27 helicopters in international airspace.
On 8 January 2012, "Admiral Kuznetsov" anchored near shore outside Tartus while other ships in its escort entered the port to use the leased Russian naval support facility. After replenishing supplies, all of the ships will continue their Mediterranean deployment on 9 January.
On 17 February 2012, "Admiral Kuznetsov" returned to its homebase of Severomorsk.
2013-14 Deployment.
On 1 June 2013, it was announced that the ship would return to the Mediterranean by the end of the year.
On 17 December, "Kuznetsov" departed its homebase for the Mediterranean Sea and will not return earlier than May 2014.
On 1 January 2014, "Kuznetsov" celebrated New Year while at anchor in international waters of the Moray Firth off northeast Scotland. The anchorage allowed replenishment of ship's supplies and respite for the crew from stormy weather off the southwest coast of Norway. She then proceeded to the Mediterranean Sea, docking in Cyprus on 28 February.
In May 2014, the ship and its task group: the Kirov Class nuclear powered cruiser "Petr Velikiy"; three tankers; "Sergey Osipov", "Kama" and "Dubna"; one Ocean-going Tug "Altay" and the Tank Landing Ship "Minsk" (a Ropucha-class landing ship part of the Baltic Fleet?) sailed home and approached the UK.
Although financial and technical problems have resulted in limited operations for the ship, it is expected that "Admiral Kuznetsov" will remain in active duty until at least 2030.
MiG-29K for the "Admiral Kuznetsov".
According to the newspaper "Bulletin Reports," the Russian Navy expects to buy the Mikoyan MiG-29K for "Admiral Kuznetsov" by 2011, according to an informed source in the Russian Ministry of Defence, noting that the contract may be concluded in the next two years. Information was confirmed by the general designer of one of the defence enterprises, which produces sub-assemblies for these aircraft, while the MiG corporation refrained from comment.
Currently, the Navy has a fleet of 19 carrier-based Su-33 fighters, a resource which will expire by 2015. The production of new Su-33 is possible, but not cost-effective for such small volumes. At the same time, the MiG-29K in this respect are more convenient, because the Indian Navy already operates 24 aircraft, and has ordered a total of 45. As noted by Konstantin Makienko, it lessens the series article cost and allows Russia to save on development. India has paid 730 million dollars for the development and delivery of 16 fighters, while the 24 planes for Russia's fleet would cost about $1 billion.
Mid-life refit.
In April 2010 it was announced that by the end of 2012 the ship will enter Severodvinsk Sevmash shipyard for a major refit and modernisation. The report states that the refit will include upgrades to the obsolete electronics and sensor equipment, installation of the new anti-aircraft system and increase of the air wing by the removal of the P-700 Granit anti-ship missiles. Upgrades might also include exchanging the troublesome steam powerplant to gas-turbine or even nuclear propulsion and installation of catapults to the angled deck. As of December 2014 no such major refit has occurred.

</doc>
<doc id="29311" url="http://en.wikipedia.org/wiki?curid=29311" title="Subaru Forester">
Subaru Forester

The Subaru Forester is a compact crossover manufactured since 1997 by Fuji Heavy Industries and sold under the Subaru brand. Available in Japan from 1997, the Forester shares its platform with the Impreza. It was introduced in 1997 as a compact crossover wagon. It has been crowned Motor Trend's 2009 and 2014 SUV of the Year and The Car Connection's Best Car To Buy 2014.
First generation (SF, 1997–2002).
The Forester was introduced at the Tokyo Motor Show November 1995 as the Streega concept, and made available for sale February 1997 in Japan, and to the US market in 1998. The Forester was one of the first emerging crossover SUVs. It was built in the style of a car, but had a taller stance, higher h-point seating, and an all-wheel drive drive train. Subaru advertising employed the slogan "SUV tough, Car Easy". It used the Impreza platform but with the larger 2.5-liter DOHC EJ25D four-cylinder boxer engine from the Outback, making 165 hp at 5600 rpm and 162 lbft of torque at 4000 rpm.
In Japan, the Forester replaced the Subaru Impreza Gravel Express, known in the USA as the Subaru Outback Sport. However, the Outback Sport remained in production for the U.S. market. The Forester appeared after the introduction of the Nissan Rasheen in Japan with a similar appearance, and the Forester's Japanese competitors include the Mitsubishi RVR, and the Suzuki Grand Vitara. Due to the Forester's low center of gravity, it meets the United States federal safety standards for passenger vehicles, and does not require a risk of rollover warning label on the driver's visor. Size and price-wise, it fits between the shared Impreza platform, and the larger Legacy.
The automatic transmissions used on AWD equipped vehicles will normally send 60% of the engine's torque to the front wheels and 40% to the rear wheels, using a computer-controlled, continuously variable, multi-plate transfer clutch. When the transmission detects a speed difference between the front and rear axle sets, the transmission progressively sends power to the rear wheels. Under slip conditions it can achieve an equal split in front and rear axle speeds.
When accelerating or driving uphill, the vehicle's weight shifts rearward, reducing front wheel traction, causing the transmission to automatically send torque to the rear wheels to compensate. When braking or driving downhill, the vehicle's weight shifts towards the front, reducing rear wheel traction. The transmission again compensates by sending torque to the front wheels for better steering control and braking performance. If the automatic is placed in reverse or first gear, the transmission divides the torque 50-50 to both front and rear wheels. The manual transmission cars are set up with a near 50/50 torque split as a base setting, and it varied from there. Essentially, the manual cars are set up with more bias towards the rear than the automatic cars.
The trim levels were the basic model "L" and the fully equipped "S" for the USA versions.
Forester L came with a high level of standard equipment, including ABS, air conditioning, power windows, power locks, cruise control, digital temperature gauge, multi-reflector halogen headlights, fog lights, roof rack, rear window defogger, trailer harness connector, reclining front bucket seats with adjustable lumbar support, tilt steering, tinted glass, AM/FM/cassette stereo with its antenna laminated in the left-rear quarter window. Notably new in 2001 were the three-point seatbelts for all five seating positions, including force limiters in front and height-adjustable shoulder belt anchors for front and rear outboard positions, plus rear seat headrests for all three seating positions.
Forester S adds a viscous limited-slip differential, rear disc brakes, 16 × 6.5-inch alloy wheels with 215/60R16 tires (the L uses 15 × 6-inch steel wheels), upgraded moquette upholstery, heated front seats with net storage pockets in back, dual vanity mirrors, heated sideview mirrors, heated windshield wipers, and keyless entry. New equipment for 2001 included Titanium pearl paint for the bumpers and cladding; six-disc in-dash CD sound system; leather-wrapped steering wheel, shift knob and handbrake handle; variable intermittent wipers with de-icers and driver’s side fin; and the five-spoke alloy wheels. Some models were equipped with the $1000 optional premium package on the Forester S, including monotone paint (Sedona Red Pearl), power moonroof, front side-impact airbags, and gold accent wheels. Other options were the $800 automatic transmission, $39 chrome tailpipe cover and $183 auto-dimming rear-view mirror with compass, bringing the sticker price to $25,412 including $495 delivery (USA dollars quoted).
Australia.
The Forester had four main models available in Australia:
There was a change in body styling for all 2001–2002 models, and the 2001/2002 GT spec also had a change in engine management and power output was increased from 125 to.
US.
The U.S. Market was offered the car starting in 1998 with either the 2.5 DOHC (1998 only) or 2.5 SOHC naturally aspirated engine (no turbocharged engines). In 2001 Subaru updated the exterior with a modest 'face-lift' to the front, rear and sides, and the interior's dashboard.
1998 - 2000 versions sold in the United States:
The 2001-2002 versions carried over adding the S Premium model, albeit with the aforementioned mild-redesign:
Second generation (SG, 2003–2008).
The second generation was introduced as a 2003 model at the 2002 Chicago Auto Show, based on the new Impreza platform, featuring several fine-tune improvements over the past model. The 2003 Forester features weight-saving refinements such as an aluminum hood, perforated rails, and a hydro-formed front sub-frame. The most noticeable change was the offering of 2.5 L versions (normally aspirated and turbocharged) and in the U.S. the introduction of the turbo charged 2.5L model.
In the U.S., the naturally aspirated (non-turbo) X (previously L) and XS (previously S) were released in 2003. In 2004, the turbocharged XT version was released. However, the same model had been available since the late 1990s elsewhere in the world. The X and XS models feature a 2.5 L SOHC engine, while the XT model features a 2.5 L turbocharged DOHC engine. Both engines have timing belt driven camshafts. The XT model uses the same Mitsubishi TD04 turbocharger used in the Subaru Impreza WRX. The engine in the 2004 to 2013 Forester XT is the EJ255. The '04 and '05 version was essentially the same engine used in the Impreza WRX STi, with a few differences, including cylinder heads. Those seeking additional power for their Forester XT can replace the turbocharger and intercooler with used STI components which are readily available. All Forester 2.5 L engines are of the interference engine type.
In 2004, Subaru launched an STI variant of the Forester, the Forester STI, for the Japanese Market. It shared the same engine as the 2005 Subaru Impreza WRX STI, but thanks to different tuning generated 320 bhp. Starting with the 2004 XT, the turbocharged version had active valve control system AVCS cylinder heads. The i-AVLS active valve lift system became standard on the naturally aspirated version of the Forester in 2006. This increased horsepower and torque figures to 173 HP and 166 ft-lbs. The 2006 XT received a higher compression ratio to 8.0:1 from 8.2:1. This increased the XT's power to 230 HP and 235 ft-lbs.
For the 2006 model year, Subaru gave the SG a facelift, using redesigned headlights, tail-lights, bonnet, grille, front bumper and side-moldings.
Safety.
MY03-04 Models has a 4-Star ANCAP safety rating
MY05 Forester Model had a mid-life update, the update increased its ANCAP safety rating to 5 Stars.
In 2006, the turbocharged engine (powering the Forester XT) was awarded International Engine of the Year. This engine is also used in the Subaru Impreza WRX, as well as the re-badged Saab 9-2XAero.
Maintenance.
All of the 2.5-liter 4-cylinder engines have a timing belt made of rubber and cord. This belt must be replaced at 105,000 mi. These engines are interference engines, meaning that if the timing belt breaks or stretches, the pistons will hit the valves, resulting in an engine teardown, and a likely rebuild. Also, if this belt is replaced around 105,000 miles, it is a good idea to change the water pump, thermostat, belt tensioner and all the idler pulleys for this belt. The water pump and thermostat are behind this belt.
In Australia for the Series II (MY06) cars, Subaru changed the recommended service interval for the timing belt replacement from 100,000 kilometers to 125,000 kilometers.
The 2.5-liter 4-cylinder engine in the first-generation Foresters featured head gaskets which were prone to premature failure. For 2003 and later, this issue was addressed with a revised, higher performing design, but is still an issue.
US.
The U.S. Market was offered the car with either the 2.5 SOHC naturally aspirated engine, or the 2.5 DOHC turbocharged version
added in 2004.
2004 versions sold in the United States:
2005 versions:
In 2006, the Forester line included these models (XS model was deleted):
In 2007, Subaru added the 'Sports' model, which changed some interior and exterior features and added the VDT/VDC transmission to the XT Sports turbo Automatic model:
In 2008, models carried over adding the VDT/VDC Automatic transmission to the XT Limited Turbo model as well:
Australia.
The Forester had three main models available in Australia until July 2005:
The Forester at the time had three main models available in Australia from August 2005 Series II:
The Luxury Pack edition was an option on all models - allowing for leather seats and a sunroof. These options were also included with the Columbia edition. The Weekender edition included fog lamps, roof racks and alloy wheels.
Standard with the Manufacture Year 2006 (MY06) Forester came with larger side mirrors with indicator lights, curtain airbags giving a 5 star safety rating, remodelled centre console and exterior with a new look nose, lights and bumpers and the rear lost the large Subaru badge under the rear window.
India.
The Forester was sold in India as a Chevrolet alongside other unique Chevrolet models sold there. However, since General Motors no longer holds an ownership stake in Subaru's parent company, Fuji Heavy Industries, sales in India of the Chevrolet-badged Forester have ended.
Third generation (SH, 2008–2013).
Subaru unveiled the model year 2008 Forester in Japan on December 25, 2007. The North American version debuted at the 2008 North American International Auto Show in Detroit.
The third generation Forester was styled by Subaru Chief Designer Mamoru Ishii. The dimensions derive from engineers using the basic body structure of the Japanese-spec Impreza wagon with the rear platform of the U.S.-spec Impreza sedan. The Forester's wheelbase has increased 3.5 in, with overall increases of 3.0 in in length, 1.8 in in width and 4.3 in in height.
Notably, the third generation Forester forgoes the frameless side windows used on Subarus since the early 1970s. The third generation embraces the car based SUV, and leaves behind the older wagon design; it's larger in nearly every dimension and features a sloping roof line with more cargo space.
The independent double wishbone rear suspension has been redesigned for better handling and a smoother ride over the previous generation. "Sportshift" has been included with the four-speed computer-controlled automatic transmission. The in-dash, touch-screen satellite navigation system is Bluetooth compatible, and has the premium stereo integrated. Subaru also offers the six-speaker premium stereo, with surround sound enhancement, separate from the navigation system.
The new model adds 3.5 in to the Forester's wheelbase, thereby improving interior space and cargo room (31 cuft expandable to 69 cuft). Ground clearance for the Forester is now 8.9 in.
Europe.
The Forester is available in Europe with the popular 2.0 liter EJ20 gasoline engine with Active Valve Control System (AVCS) matched to either 5-speed manual or 4-speed automatic gearbox, and the all-new diesel-powered horizontally opposed (boxer) engine, called the Subaru EE and six-speed manual gearbox, which was introduced at the 2008 Paris Motor Show in October. The diesel engine produces a power output of 147 PS (145 hp).
In the UK, the gasoline-powered Forester is offered in the popular X and XS models, while trim level for the diesel models are X, XC, and XS NavPlus.
In Russia, Belarus and Ukraine 2.5 and 2.5 Turbo engines are also available.
Australia.
There are seven specifications with various trim and performance levels:
Summary of standard trim and equipment over different Australian models.
US.
The Forester trim levels are the 2.5X, the 2.5X Premium, the 2.5X Limited and the 2.5XT and 2.5XT Limited both with turbo. The interior colors are either black or light gray with three upholstery selections, including leather. Nine exterior colors are offered, with four colors offered with a pearlescent appearance.
Starting July 2008, Subaru no longer offered a special-edition L.L. Bean trim level on the Forester.
The USA 2.5X model has been certified PZEV emissions (Rated 175 hp instead 170 hp), and a badge has been attached to the rear of the vehicle on the bottom left-hand side of the tailgate. All other USA models have been certified LEV2. The PZEV Forester is available for sale in all fifty states, unlike other manufacturers who only sell PZEV certified vehicles in states that have adopted California emission standards. The engine without the turbo runs on unleaded gasoline rated at 87 octane, and the turbo engine (EJ255) requires premium fuel rated minimum 91 octane.
Safety equipment includes front airbags with side curtain airbags and front passenger side airbags (for a total of six airbags) and brake assist that detects panic-braking situations and applies maximum braking force more quickly.
The manual transmission is equipped with Incline Start Assist which, on an incline, holds the brake for a second after releasing it to allow for time to depress the accelerator.
Some of the standard equipment found on the 2.5X include Subaru's VDC (Vehicle Dynamics Control), 16 inch steel wheels, and an auxiliary audio jack for MP3 players. Optional equipment includes 17 inch alloy wheels, panoramic moonroof, heated front seats and heated side-view mirrors. The L.L. Bean edition adds automatic climate control, leather upholstery, an upgraded stereo with six speakers and a six disc in-dash CD changer over the four-speaker stereo with single disc CD player, and an in-dash navigation system, as well as L.L. Bean signature floor mats and rear cargo tray.
The 2.5 XT comes with the premium stereo standard, as well as 17 inch alloy wheels, and the panoramic moonroof. The 2.5 XT Limited adds leather upholstery with heated front seats, in-dash navigation, a rear spoiler, and automatic climate control. For 2009, XT models come only with a 4-speed Automatic with Sport Shift.
Forester XTI concept.
The Forester XTI concept vehicle uses the 2.5 liter intercooled turbo engine from Subaru WRX STI, 6-speed manual transmission, 18 x 8-inch S204 forged alloy wheels with Yokohama Advan Neova 255/40R18 performance tires, adjustable coil-over suspension, Brembo brakes with 4-piston front calipers, 2-piston rear calipers, Super Sport ABS and Electronic Brake-force Distribution (EBD), leather and Alcantara sport seats, a special instrument cluster, front dash and center console and leather-wrapped steering wheel. Engine is rated 315 hp and 300 lbft torque.
The vehicle was unveiled in 2008 SEMA show.
Mountain Rescue Vehicle.
Subaru produced a specialized vehicle for the National Ski Patrol based on 2.5XT turbo. It includes diamond plate floor, rear steel walls, a 9,500-pound winch and a roof-mounted toboggan. The vehicle was unveiled in 2008 SEMA show.
Facelift.
In 2010 for the 2011 model year, the Subaru Forester received a new grille insert. With an optional roof-rack the Forester is also raised 2.5 inches from its base model-height. The naturally aspirated Foresters are equipped with an all new third generation motor with DOHC 2.5l FB25 and 2.0l flat four FB20.
Fourth generation (SJ, 2014–present).
Changes to the fourth-generation include:
The vehicle was unveiled in 2012 Guangzhou Motor Show, followed by the 2013 New York International Auto Show.
Japan models went on sale in November 2012. Early model includes 2.0i, 2.0i-L, 2.0i-L EyeSight, 2.0i-S EyeSight, 2.0XT (280PS), 2.0XT EyeSight (280PS). 2.0i engine models include 6-speed manual (2.0i, 2.0i-L) or Lineartronic CVT transmission; 2.0XT (280PS) engine models include Lineartronic CVT transmission.
US models went on sale in March 2013 as 2014 model year vehicles. Early models include 2.5i in base, Premium, Limited and top-line Touring versions; 2.0XT (253PS) in Premium and Touring versions. Base and Premium model 2014 Foresters can be equipped with the manual 6-speed transmission or the Lineartronic CVT. All other models are equipped with the Lineartronic CVT. An option on Limited/Touring 2.5i and Premium/Touring 2.0XT is new X-Mode control and Hill Descent Control(HDC) features. These are not available on other models.
X Mode.
The 2014 Forester has a new feature called X Mode that allows owners to go through more extreme conditions both on the road and off. The concept is that any driver, regardless of skill level, can drive safely on wet roads or muddy areas. It works by distributing torque evenly to all four wheels and, should one or more wheels begins to slip, X Mode kicks in, and the vehicle moves forward without losing traction (in mud or snow). This means no free-spinning tires. After it is engaged by a simple push button, X Mode stays engaged up until the vehicle's speed is about 25 mph then disengages itself.
EyeSight Driver Assist System.
The 2014 top-of-the-line Touring model Forester offers Subaru's EyeSight driver assist technology that uses stereo cameras mounted on either side of the rearview mirror. Eyesight offers several driver assist technologies/features which include:
The system can be manually turned on or off. Being an optical, instead of radar, based system, it has limitations in limited visibility situations; driving into the sun, fog, or where the windshield is not cleared (snow, mud, etc.) may cause the system to disengage.
Engines.
The 2014 and 2015 models had a major revamp of interior comfort: passenger seat is higher, sound system more respectable, bench seats in rear higher and the console is re-positioned for the person riding in the center.
Marketing.
As part of the US market launch of 2014 Subaru Forester, the 'Dog Tested. Dog Approved.' campaign returned, featuring an all-new cast of canines making their television debut during Animal Planet's Puppy Bowl IX, which included Grant Weber, a Subaru canine Sales Associate, who sells Subaru vehicles to dogs. New commercials for the campaign included 'Lint Roller', 'Tailgate', 'Let's Talk Financing', 'On the Lot'. A new Facebook application was launched, which included Dog Matchmaker, Ask a Dog - Live, Four-Paw Drivers Club, Put Your Dog in a Subaru. Subaru also partnered with MapMyFitness to continue the MapMyDOGWALK mobile application. During Puppy Bowl IX, Dog Tested fans can interact with the campaign content on Facebook or Animal Planet's Puppy Bowl Co-Viewing Application.

</doc>
<doc id="29314" url="http://en.wikipedia.org/wiki?curid=29314" title="Symphonic rock">
Symphonic rock

Symphonic rock is a subgenre of progressive rock. Since early in progressive rock's history, the term has been used to distinguish more classically influenced progressive rock from the more psychedelic and experimental forms of progressive rock.
Symphonic rock can be described as the combining of progressive rock with classical music traditions. Some artists perform rock arrangements of themes from classical music or compose original pieces in classical composition structures. Additionally, they may play with the accompaniment of a symphony orchestra or use a synthesizer or mellotron to emulate orchestral instruments.
As the term is used in music criticism (and this article), orchestral renditions of hit rock and pop songs do not necessarily qualify as symphonic rock, though various outlets sometimes market them using that term. Using an orchestra does not make a piece symphonic rock; it must meet the criteria for being progressive rock in addition to the qualities listed for being symphonic.
Attributes.
Classical devices often employed in symphonic rock include the following
Artists.
As early as 1966, with The Mothers of Invention's "Freak Out!" and The Beach Boys' "Pet Sounds", concept albums started appearing that tied all the songs on an album into a thematic whole. The Beatles followed with their album "Sgt. Pepper's Lonely Hearts Club Band" (1967) incorporating significant orchestral passages and studio editing. In the same year, The Moody Blues' "Days of Future Passed" merged rock playing with orchestral accompaniment. Procol Harum's 1968 album "Shine On Brightly" contained the epic length song "In Held Twas In I". These works and experiments led to what would become progressive rock, specifically "symphonic prog" in the United Kingdom.
The Moody Blues.
The Moody Blues are an English rock band. Among their innovations was a fusion with classical music, most notably in their 1967 album "Days of Future Passed". For many, the mellotron epitomised The Moody Blues and their symphonic sound.
The Nice.
Initially formed as a backup band for the jazz singer P. P. Arnold, The Nice went on to produce their first album "The Thoughts of Emerlist Davjack" (1967). It was the first attempt at art rock heavily influenced by psychedelic rock. The album "Ars Longa Vita Brevis" (1968) contained rock versions of classical compositions such as the "Karelia Suite" by Jean Sibelius and "The Brandenberg Concerto" by J.S. Bach. It was their third album "Nice" (1969) where they charted a course for "symphonic prog" with its mix of psychedelic rock, jazz, blues and classical elements. They ended with "Five Bridges" (1970), a work commissioned by the Newcastle Arts Festival. The "Five Bridges Suite" contains five movements written by the band members instead of using existing classical works. The big criticism of the group was that they did not have a good lead singer. Keyboardist Keith Emerson went on to form Emerson, Lake & Palmer.
King Crimson.
King Crimson released their landmark album "In the Court of the Crimson King" in 1969, bringing acclaim to progressive rock. Guitarist Robert Fripp enriched his melodic palette by reading Vincent Persichetti's "Twentieth Century Harmony: Creative Aspects and Practice" and used techniques of atonal harmony, such as whole tone scales. The 1974 song "Red" (1974) used an octatonic scale.
Renaissance.
Renaissance were formed after the breakup of the Yardbirds. Their first album, "Renaissance" (1969), came out two months after "In the Court of the Crimson King". The album saw them mixing classical, eastern, jazz and folk elements with rock. They became a recognised symphonic prog band when they reformed in 1971 with Annie Haslam as the vocalist for the album "Prologue" (1972). They peaked with their albums "Ashes Are Burning" (1973), "Turn of the Cards" (1974) and "Scheherazade and Other Stories" (1975).
Emerson, Lake & Palmer.
Emerson, Lake & Palmer kicked off '70s Symphonic Prog with their first album "Emerson, Lake & Palmer" (1970). ELP performed classical compositions such as Mussorgsky's "Pictures at an Exhibition" on the album "Pictures At An Exhibition" (1971) and Aaron Copland's "Fanfare for the Common Man" on "Works, Volume 1" (1977) with electric instrumentation.
Emerson, Lake & Palmer also composed suites that are considered classics of the genre. The 1971 song "Tarkus" uses quartal harmony. Other songs include "The Endless Enigma" on the album "Trilogy" (1972) and "Karn Evil 9" on the album "Brain Salad Surgery" (1973).
Genesis.
Genesis emerged in the '70s as a symphonic prog band with their second album "Trespass" (1970). The high point of their symphonic prog output was "Foxtrot" (1972), which contains their only long suite, "Supper's Ready", written as a variation of sonata form, and "Selling England by the Pound" (1973). After Peter Gabriel left the group following the tour for the concept album "The Lamb Lies Down On Broadway" (1974), they continued in the Symphonic Prog vein until 1976's "Wind & Wuthering" with Phil Collins as the lead singer. After Steve Hackett left the group in 1977, they became a crossover, arena rock band. Hackett, prior to leaving Genesis, released his own symphonic prog album "Voyage of the Acolyte" (1975).
Gentle Giant.
Gentle Giant were formed after the break-up of the psychedelic pop band, Simon Dupree and the Big Sound. The band's first album was released in 1970 and its music is a mixture of different genres such as blues, classical music, soul, folk, jazz fusion and psychedelic rock. After this album, the band departed from this style and turned to a more experimental sound, as well as including more symphonic elements. The band's music combined multi-part vocal harmonies, complex lyrics, organization with concept album form, frequent changes in tempo, frequent use of syncopation and non-standard time signatures, use of complex melodies frequently contrasting harmonies with dissonance, extensive use of instrumental and vocal counterpoint, use of musical structures typically associated with classical music and use of classical and medieval instrumentation not generally associated with rock music.
Pink Floyd.
Pink Floyd are usually not included in the symphonic rock genre because they are considered by critics not to be "orchestral" enough in sound. However, their 1970 album "Atom Heart Mother" contains the "Atom Heart Mother" suite with extensive orchestra use. By "Meddle" (1971), specifically the suite "Echoes", they moved in the direction of symphonic prog rather than psychedelic rock. With albums such as "Wish You Were Here" (1975), an ode to former band member Syd Barrett and screed against the music business, and "Animals" (1977), a concept album based on Animal Farm by George Orwell, they were firmly in the realm of symphonic prog. "The Trial" on the album "The Wall" is set up in the manner of a Broadway musical number, complete with string and brass ensembles, and is only topped by David Gilmour's guitar riff.
Yes.
Yes, starting with their third album "The Yes Album" (1971), produced a highly successful blend of classical, psychedelic and progressive ensemble rock. Their approach was similar to classical music; each instrument played its own melodic line to generate a grand musical theme. The vocals in some songs (e.g. "Yours Is No Disgrace") were treated as just another instrument in the composition. They were also writing multi-part suites such as "Starship Trooper" on "The Yes Album" (1971), "Heart of the Sunrise" on "Fragile" (1971), "Close To the Edge" on "Close to the Edge" (1972) and "Awaken" on "Going For The One" (1977). What the mainstream rock press perceived as the excess of Tales From Topographic Oceans (1973) marked the start of the backlash against Prog music.
Focus.
Dutch band Focus, containing Thijs van Leer and Jan Akkerman, began to assert symphonic prog with their second album "Focus II" ("Moving Waves") (1971) despite their hit song "Hocus Pocus" (which is noted for lead singer Thijs van Leer's yodeling). This album brought them worldwide acclaim. After their next two albums "Focus III" (1972) and "Hamburger Concerto" (1974), they fell away from symphonic prog.
Rick Wakeman.
Rick Wakeman was an on again-off again keyboard virtuoso for Yes who also produced several solo symphonic prog concept albums. "The Six Wives of Henry VIII" (1973) consisted exclusively of Rick and all his synthesisers. "Journey To the Centre of the Earth" (1974) and "The Myths and Legends of King Arthur and the Knights of the Round Table" (1975) made use of symphony orchestras and choirs along with his band. The following year "No Earthly Connection" (1976) had him pare down the production to just using his band, the "English Rock Ensemble".
Camel.
Camel, consisting of Andrew Latimer and former Them keyboardist Peter Bardens, included elements of hard rock, jazz and blues in their brand of symphonic prog starting with "Camel" (1973). They moved firmly into symphonic prog with their next album "Mirage" (1974) and their 1975 concept album "The Snow Goose". Unfortunately, they did not have a recognisable lead singer, unlike Yes, ELP or Genesis ("The Snow Goose" was entirely instrumental) which limited their success). After "Moonmadness" (1976), they moved into more mainstream territory, but returned to symphonic prog with the concept albums "Dust and Dreams" (1991) and "Harbour of Tears" (1996).
Electric Light Orchestra.
Others such as Electric Light Orchestra played rock music with orchestral arrangements and/or orchestra backing. They also released their own concept album with 1974's "Eldorado". But starting with 1975's "Face the Music" they favoured more of a pop rock style.
Development in Other Countries.
Europe.
Italy.
Italy's symphonic rock boomed in 1972, after the successes of New Trolls' "Concerto Grosso, No. 2" and Van der Graaf Generator's "Pawn Hearts". The most popular bands, such as Banco del Mutuo Soccorso, Premiata Forneria Marconi and Le Orme, played symphonic prog heavily influenced by classical music, against the backdrop of the Italian canzone tradition. Bands like New Trolls, Osanna, Metamorfosi, Alphataurus, Semiramis, and Biglietto per l'Inferno had a harder edge, but still with traits of the symphonic tradition. (see Italian progressive rock)
France.
Bands in France arose in the mid- to late '70s influenced by both King Crimson and Genesis. Ange, influenced also by French folk music, released "Le Cimetière des Arlequins" (1973) and "Au-delà du Délire" (1974). Atoll, incorporating the influences of the Mahavishnu Orchestra, released "L'Araignée-Mal" (1975). Pulsar, heavily influenced by Pink Floyd, released "The Strands of the Future" (1976) and "Halloween" (1977). Mona Lisa, noted for being very theatrical, released "Avant Qu'il Ne Soit Trop Tard" (1978).
Germany.
Amidst the Krautrock bands, there were several symphonic prog bands that gained recognition. Triumvirat, best described as an ELP clone, released "Illusions on a Double Dimple" (1973), "Spartacus" (1975) and "Old Loves Die Hard" (1976). Grobschnitt were known not only for their symphonic passages, suites and synthesiers, but also for the absurd humour and strange noises in their work, as heard on "Ballerman" (1974) and "Rockpommel's Land" (1977). Both use English vocals. Anyone's Daughter started up just as Prog was ebbing in America and Britain. They had a string of symphonic prog albums with "Adonis" (1979), "Anyone's Daughter" (1980), "Piktors Verwandlungen" (1981) and "In Blau" (1982). With the latter two albums, the band sung the lyrics in their native German. Novalis, noted for a heavy organ sound with comparisons to King Crimson and Pink Floyd, released "Novalis" (1975) and "Summerabend" (1976). Eloy, a progressive rock band formed by guitarist Frank Bornemann in 1969, had a distinctly symphonic sound in the second part of the '70s when Jurgen Rosenthal and Detlev Schmidtchen were part of the group (which also included Klaus-Peter Matziol). The album "Dawn" includes "Symphonic Orchestra arranged and conducted by Wolfgang Maus." These four artists (ELOY from 1976 to 1979) also produced the albums "Ocean", "Silent Cries And Mighty Echoes", and "Eloy Live".
Yugoslavia.
During the 1970s, a large number of Yugoslav progressive rock bands experimented with symphonic sound. In 1973, progressive rock band Korni Grupa, under the name Kornelyans, released the symphonic rock album "Not an Ordinary Life". The symphonic rock band Opus released only one album, "Opus 1" (1975), featuring Dušan Prelević on vocals, before disbanding. In 1978, keyboardist Laza Ristovski and drummer Ipe Ivandić, both members of the hard rock band Bijelo Dugme, released the symphonic rock-oriented album "Stižemo". In 1979, Bijelo Dugme released symphonic-influenced album "Bitanga i princeza", and during the same year, their frontman Željko Bebek released his first solo album, symphonic-oriented "Skoro da smo isti".
Outside Europe.
North America.
Once these groups became popular in the United States and Canada, bands such as Kansas, Rush, and Todd Rundgren's Utopia appeared in the mid '70s. These bands tended to have more of a hard rock edge than the European bands mentioned above. French-Canadian band Harmonium had a short career in the mid '70s during which their most notable release was "Si on avait besoin d'une cinquième saison" (If We Needed A Fifth Season) (1975).
For a long time, the US Rock and Roll Hall of Fame declined to induct any "prog" bands. Pink Floyd were finally inducted in 1996 and were the only "prog" band of any kind in the Hall. It was then announced in December 2009 that Genesis will be inducted into the hall. Speculation is that this induction will open the floodgates for the other overlooked "prog" bands. In 2013, Rush was inducted into the Hall of Fame, while later on it was announced that Yes were now nominated for the Hall.
Japan.
In 1967, instrumental rock band, Takeshi Terauchi & Bunnys covered many European classical songs in rock format on their album "Let's Go Unmei". Takeshi Terauchi continued to release instrumental rock-style covers of classical music with his band, The Blue Jeans, for instance on their album "Let's Go Ereki Kokyokyoku". Bands in Japan arose in the early 1980s, many of them influenced by Pink Floyd, King Crimson, Yes, and Genesis. Representative bands include Ali Project, Symphonia, Pagent, Midas, Mugen, Gerard, Outer Limits, X Japan, Raphael, Mr. Sirius, Shingetsu, Versailles, and Asturias.
End of the "classic" period.
By the late 1970s, the "classic" period of symphonic rock was coming to an end. The backlash against any "prog" by the mainstream rock press was in full force. Disco and punk rock emerged as the primary forms of music.
1980s neo-progressive era.
In the 1980s, British neo-progressive rock bands such as Marillion, IQ, Pallas, Solstice, Twelfth Night and Pendragon continued the traditions of 1970s' symphonic prog, but a little less complex. The main influences on the neo-prog genre are Genesis, Yes, Camel, and Pink Floyd.
International.
While the 1980s saw neo-prog keeping a glimmer alive in the UK and US, symphonic prog was reaching other countries around the world. Asia Minor from Turkey released "Between Flesh and Devine" (1981), Bacamarte from Brazil released "Depois Do Fim" (1983) and Hungarian band Solaris released "Marsbéli Krónikák" (Martian Chronicles) (1984). Only Solaris survived into the '90s releasing "Nostradamus Book of Prophecies" in 1999. The early 1990s saw groups like After Crying from Hungary release "Overground Music" (1990), Quaterna Requiem from Brazil release "Velha Gravura" (1990), Änglagård from Sweden release "Hybris" (1992) and Isildurs Bane, also from Sweden, release "The Voyage - A Trip To Elsewhere" (1992). The last two paved the way for a symphonic prog movement in Sweden.
1990's Resurgence.
Interest in symphonic prog flowered again, starting with Echolyn's eponymous first album in 1991. This marked a resurgence to a degree in the 1990s and 2000s with bands such as The Flower Kings, Spock's Beard, Transatlantic, Karmakanic, Nexus, Pär Lindh Project, Arena, Glass Hammer, and many others.
Muse.
The English alternative rock and new prog band Muse featured , commonly known as simply "Exogenesis", on their 2009 fifth studio album "The Resistance". Written by lead vocalist, guitarist and pianist Matthew Bellamy over the course of a number of years, the song is presented as a symphony in three movements entitled "Overture", "Cross-Pollination" and "Redemption" respectively, each occupying a separate track at the end of the album. The song has been acclaimed by critics, including "NME", who declared "Exogenesis" as one of the highlights of "The Resistance", describing it as "more bombastic than anything Muse have ever previously done."
MCT: The Symphonic Rock Band.
Formed in 2011, the Wisconsin-based, 40-person band performs music written and arranged by Curtis Aderholdt. Featuring lead-vocalists Tyler Kundinger and Loren De Lonay, the band is said to be "wow-ing audiences of their own." The symphonic rock band is currently recording new material for a potential 2015 release.

</doc>
<doc id="29316" url="http://en.wikipedia.org/wiki?curid=29316" title="Sandinista National Liberation Front">
Sandinista National Liberation Front

The Sandinista National Liberation Front (Spanish: "Frente Sandinista de Liberación Nacional", FSLN) is today a democratic socialist political party in Nicaragua. Its members are called Sandinistas ] in both English and Spanish. The party is named after Augusto César Sandino who led the Nicaraguan resistance against the United States occupation of Nicaragua in the 1930s.
The FSLN overthrew Anastasio Somoza Debayle in 1979, ending the Somoza dynasty, and established a revolutionary government in its place. Following their seizure of power, the Sandinistas ruled Nicaragua from 1979 to 1990, first as part of a Junta of National Reconstruction. Following the resignation of centrist members from this Junta, the FSLN took exclusive power in March 1981. They instituted a policy of mass literacy, devoted significant resources to health care, and promoted gender equality. A militia, known as the Contras was formed in 1981 to overthrow the Sandinista government and was funded and trained by the US Central Intelligence Agency. In 1984 elections were held and described as free and fair by international observers but were boycotted by some opposition parties. The FSLN won the majority of the votes, and those who did oppose the Sandinistas won approximately a third of the seats. The Contras continued their rebellion, until 1989. After revising the constitution in 1987 and after years of resisting the United States-supported Contras the FSLN lost the election in 1990 to Violeta Barrios de Chamorro, but they retained a plurality of seats in the legislature.
The FSLN remains one of Nicaragua's two leading parties. The FSLN often polls in opposition to the Constitutionalist Liberal Party, or PLC. In the 2006 Nicaraguan general election, former FSLN President Daniel Ortega was re-elected President of Nicaragua with 38.7% of the vote compared to 29% for his leading rival, bringing in the country's second Sandinista government after 16 years of the opposition winning elections. Ortega and the FSLN were re-elected again in the presidential election of November 2011.
History.
Origin of the term "Sandinista".
The Sandinistas took their name from Augusto César Sandino (1895–1934), the charismatic leader of Nicaragua's nationalist rebellion against the US occupation of the country during the early 20th century (ca. 1922–1934). Sandino was assassinated in 1934 by the Nicaraguan National Guard ("Guardia Nacional"), the US-equipped police force of Anastasio Somoza, whose family ruled the country from 1936 until they were overthrown by the Sandinistas in 1979.
Founding (1961–70).
The FSLN originated in the milieu of various oppositional organisations, youth and student groups in the late 1950s and early 1960s. The University of Léon, and the National Autonomous University of Nicaragua (UNAN) in Managua were two of the principal centers of activity. Inspired by the Revolution and the FLN in Algeria, the FSLN itself was founded in 1961 by Carlos Fonseca, Silvio Mayorga, Tomás Borge and others as "The National Liberation Front" (FLN). Only Tomás Borge lived long enough to see the Sandinista victory in 1979.
The term "Sandinista", was added two years later, establishing continuity with Sandino's movement, and using his legacy in order to develop the newer movement's ideology and strategy. By the early 1970s, the FSLN was launching limited military initiatives.
Rise (1970–76).
On December 23, 1972, a powerful earthquake leveled the capital city, Managua. The earthquake killed 10,000 of the city's 400,000 residents and left another 50,000 homeless. About 80% of Managua's commercial buildings were destroyed. President Anastasio Somoza Debayle's National Guard embezzled much of the international aid that flowed into the country to assist in reconstruction, and several parts of downtown Managua were never rebuilt. The president gave reconstruction contracts preferentially to family and friends, thereby profiting from the quake and increasing his control of the city's economy. By some estimates, his personal wealth soared to US$400 million in 1974.
In December 1974, a guerrilla group affiliated with FSLN directed by Eduardo Contreras and Germán Pomares seized government hostages at a party in the house of the Minister of Agriculture in the Managua suburb Los Robles, among them several leading Nicaraguan officials and Somoza relatives. The siege was carefully timed to take place after the departure of the US ambassador from the gathering. At 10:50 pm, a group of 15 young guerrillas and their commanders, Pomares and Contreras, entered the house. They killed the Minister, who tried to shoot them, during the takeover. The guerrillas received US$2 million ransom, and had their official communiqué read over the radio and printed in the newspaper "La Prensa".
Over the next year, the guerrillas also succeeded in getting 14 Sandinista prisoners released from jail, and with them, were flown to Cuba. One of the released prisoners was Daniel Ortega, who would later become the president of Nicaragua. The group also lobbied for an increase in wages for National Guard soldiers to 500 córdobas ($71 at the time). The Somoza government responded with further censorship, intimidation, torture, and murder.
In 1975, Somoza imposed a state of siege, censoring the press, and threatening all opponents with internment and torture. Somoza's National Guard also increased its violence against individuals and communities suspected of collaborating with the Sandinistas. Many of the FSLN guerrillas were killed, including its leader and founder Carlos Fonseca in 1976. Fonseca had returned to Nicaragua in 1975 from his exile in Cuba to try to reunite fractures that existed in the FSLN. He and his group were betrayed by a peasant who informed the National Guard that they were in the area. The guerrilla group was ambushed, and Fonseca was wounded in the process. The next morning Fonseca was executed by the National Guard.
Split (1977–78).
Following the FSLN's defeat at the battle of Pancasán in 1967, the organization adopted the "Prolonged Popular War" ("Guerra Popular Prolongada", GPP) theory as its strategic doctrine. The GPP was based on the "accumulation of forces in silence": while the urban organization recruited on the university campuses and collected funds through bank holdups, the main cadres were to go permanently to the north central mountain zone. There they would build a grassroots peasant support base in preparation for renewed rural guerrilla warfare.
As a consequence of the repressive campaign of the National Guard, in 1975 a group within the FSLN's urban mobilization arm began to question the viability of the GPP. In the view of the young orthodox Marxist intellectuals, such as Jaime Wheelock, economic development had turned Nicaragua into a nation of factory workers and wage-earning farm laborers. Wheelock's faction was known as the "Proletarian Tendency".
Shortly after, a third faction arose within the FSLN. The "Insurrectional Tendency", also known as the "Third Way" or "Terceristas", led by Daniel Ortega, his brother Humberto Ortega, and Mexican-born Victor Tirado Lopez, was more pragmatic and called for tactical, temporary alliances with non-communists, including the right-wing opposition, in a popular front against the Somoza regime. By attacking the Guard directly, the Terceristas would demonstrate the weakness of the regime and encourage others to take up arms.
In October 1977, a group of prominent Nicaraguan professionals, business leaders, and clergymen allied with the Terceristas to form "El Grupo de los Doce", (The Group of Twelve) in Costa Rica. The group's main idea was to organize a provisional government from Costa Rica. The new strategy of the Terceristas also included unarmed strikes and rioting by labor and student groups coordinated by the FSLN's "United People's Movement" (Movimiento Pueblo Unido – MPU).
Insurrection (1978).
On January 10, 1978, Pedro Joaquín Chamorro, the popular editor of the opposition newspaper "La Prensa" and leader of the "Democratic Union of Liberation" (Unión Democrática de Liberación – UDEL), was assassinated. Although his assassins were not identified at the time, evidence implicated President Somoza's son and other members of the National Guard. Spontaneous riots followed in several cities, while the business community organized a general strike demanding Somoza's resignation.
The Terceristas joined the turmoil in early February with attacks in several Nicaraguan cities. The National Guard responded by further increasing repression and using force to contain and intimidate all government opposition. The nationwide strike that paralyzed the country for ten days weakened the private enterprises and most of them decided to suspend their participation in less than two weeks. Meanwhile, Somoza asserted his intention to stay in power until the end of his presidential term in 1981. The United States government showed its displeasure with Somoza by suspending all military assistance to the regime, but continued to approve economic assistance to the country for humanitarian reasons.
In August, the Terceristas staged a spectacular hostage-taking. Twenty-three Tercerista commandos led by Edén Pastora seized the entire Nicaraguan congress and took nearly 1,000 hostages, including Somoza's nephew José Somoza Abrego and cousin Luis Pallais Debayle. Somoza gave in to their demands and paid a $500,000 ransom, released 59 political prisoners (including GPP chief Tomás Borge), broadcast a communiqué with FSLN's call for general insurrection and gave the guerrillas safe passage to Panama.
A few days later six Nicaraguan cities rose in revolt. Armed youths took over the highland city of Matagalpa. Tercerista cadres attacked Guard posts in Managua, Masaya, León, Chinandega and Estelí. Large numbers of semi-armed civilians joined the revolt and put the Guard garrisons of the latter four cities under siege. The September Insurrection of 1978 was subdued at the cost of several thousand, mostly civilian, casualties. Members of all three factions fought in these uprisings, which began to blur the divisions and prepare the way for unified action.
Reunification (1979).
In early 1979, President Jimmy Carter and the United States no longer supported the Somoza regime, but did not want a left-wing government to take power in Nicaragua. The moderate "Broad Opposition Front" ("Frente Amplio Opositor" - FAO) which opposed Somoza was made up of a conglomeration of dissidents within the government as well as the "Democratic Union of Liberation" (UDEL) and the "Twelve", representatives of the Terceristas (whose founding members included Casimiro A. Sotelo, later to become Ambassador to the U.S. AND Canada representing the FSLN). The FAO and Carter came up with a plan that would remove Somoza from office but left no part in government power for the FSLN. The FAO's efforts lost political legitimacy, as Nicaraguans protested that they did not want "Somocismo sin Somoza" (Somocism without Somoza).
The "Twelve" abandoned the coalition in protest and formed the "National Patriotic Front" ("Frente Patriotico Nacional" – FPN) together with the "United People's Movement" (MPU). This strengthened the revolutionary organizations as tens of thousands of youths joined the FSLN and the fight against Somoza. A direct consequence of the spread of the armed struggle in Nicaragua was the official reunification of the FSLN that took place on 7 March 1979. Nine men, three from each tendency, formed the National Directorate which would lead the reunited FSLN. They were: Daniel Ortega, Humberto Ortega and Víctor Tirado (Terceristas); Tomás Borge, Bayardo Arce, and Henry Ruiz (GPP faction); and Jaime Wheelock, Luis Carrión and Carlos Núñez.
Nicaraguan Revolution.
The FSLN evolved from one of many opposition groups to a leadership role in the overthrow of the Somoza regime. By mid-April 1979, five guerrilla fronts opened under the joint command of the FSLN, including an internal front in the capital city Managua. Young guerrilla cadres and the National Guardsmen were clashing almost daily in cities throughout the country. The strategic goal of the Final Offensive was the division of the enemy's forces. Urban insurrection was the crucial element because the FSLN could never hope to achieve simple superiority in men and firepower over the National Guard.
On June 4, a general strike was called by the FSLN to last until Somoza fell and an uprising was launched in Managua. On June 16, the formation of a provisional Nicaraguan government in exile, consisting of a five-member Junta of National Reconstruction, was announced and organized in Costa Rica. The members of the new junta were Daniel Ortega (FSLN), Moisés Hassan (FPN), Sergio Ramírez (the "Twelve"), Alfonso Robelo (MDN) and Violeta Barrios de Chamorro, the widow of "La Prensa"‍ '​s director Pedro Joaquín Chamorro. By the end of that month, with the exception of the capital, most of Nicaragua was under FSLN control, including León and Matagalpa, the two largest cities in Nicaragua after Managua.
On July 9, the provisional government in exile released a government program, in which it pledged to organize an effective democratic regime, promote political pluralism and universal suffrage, and ban ideological discrimination, except for those promoting the "return of Somoza's rule". On July 17, Somoza resigned, handed over power to Francisco Urcuyo, and fled to Miami. While initially seeking to remain in power to serve out Somoza's presidential term, Urcuyo seceded his position to the junta and fled to Guatemala two days later.
On July 19, the FSLN army entered Managua, culminating the first goal of the Nicaraguan revolution. The war left approximately 30,000-50,000 dead and 150,000 Nicaraguans in exile. The five-member junta entered the Nicaraguan capital the next day and assumed power, reiterating its pledge to work for political pluralism, a mixed economic system, and a nonaligned foreign policy.
Sandinista rule (1979–90).
The Sandinistas inherited a country in ruins with a debt of 1.6 billion dollars (US), an estimated 30,000 to 50,000 war dead, 600,000 homeless, and a devastated economic infrastructure. To begin the task of establishing a new government, they created a Council (or "junta") of National Reconstruction, made up of five appointed members. Three of the appointed members belonged to FSLN, which included—Sandinista militants Daniel Ortega, Moises Hassan, and novelist Sergio Ramírez (a member of Los Doce "the Twelve"). Two opposition members, businessman Alfonso Robelo, and Violeta Barrios de Chamorro (the widow of Pedro Joaquín Chamorro), were also appointed. Only three votes were needed to pass law.
The FSLN also established a Council of State, subordinate to the junta, which was composed of representative bodies. However, the Council of State only gave political parties twelve of forty-seven seats; the rest of the seats were given to Sandinista mass-organizations. Of the twelve seats reserved for political parties, only three were not allied to the FSLN. Due to the rules governing the Council of State, in 1980 both non-FSLN junta members resigned. Nevertheless, as of the 1982 State of Emergency, opposition parties were no longer given representation in the council. The preponderance of power also remained with the Sandinistas through their mass organizations, including the Sandinista Workers' Federation ("Central Sandinista de Trabajadores"), the Luisa Amanda Espinoza Nicaraguan Women's Association ("Asociación de Mujeres Nicaragüenses Luisa Amanda Espinoza"), the National Union of Farmers and Ranchers ("Unión Nacional de Agricultores y Ganaderos"), and most importantly the Sandinista Defense Committees (CDS). The Sandinista-controlled mass organizations were extremely influential over civil society and saw their power and popularity peak in the mid-1980s.
Upon assuming power, the FSLN's official political platform included the following: nationalization of property owned by the Somozas and their supporters; land reform; improved rural and urban working conditions; free unionization for all workers, both urban and rural; price fixing for commodities of basic necessity; improved public services, housing conditions, education; abolition of torture, political assassination and the death penalty; protection of democratic liberties; equality for women; non-aligned foreign policy; formation of a "popular army" under the leadership of the FSLN and Humberto Ortega.
The FSLN's literacy campaign, which saw teachers flood the countryside, is often noted as their greatest success. Within six months, half a million people had been taught rudimentary reading, bringing the national illiteracy rate down from over 50% to just under 12%. Over 100,000 Nicaraguans participated as literacy teachers. One of the stated aims of the literacy campaign was to create a literate electorate which would be able to make informed choices at the promised elections. The successes of the literacy campaign was recognized by UNESCO with the award of a Nadezhda Krupskaya International Prize.
The FSLN also created neighborhood groups similar to the Cuban Committees for the Defense of the Revolution, called Sandinista Defense Committees ("Comités de Defensa Sandinista" or CDS). Especially in the early days following the overthrow of Somoza, the CDS's served as "de facto" units of local governance. Their obligations included political education, the organization of Sandinista rallies, the distribution of food rations, organization of neighborhood/regional cleanup and recreational activities, and policing to control looting, and the apprehension of counter-revolutionaries. The CDS's organized civilian defense efforts against Contra activities and a network of intelligence systems in order to apprehend their supporters. These activities led critics of the Sandinistas to argue that the CDS was a system of local spy networks for the government used to stifle political dissent, and it is true that the CDS did hold limited powers—such as the ability to suspend privileges such as driver licenses and passports—if locals refused to cooperate with the new government. After the initiation of heavier U.S. military involvement in the Nicaraguan conflict the CDS was empowered to enforce wartime bans on political assembly and association with other political parties (i.e., parties associated with the "Contras").
By 1980, conflicts began to emerge between the Sandinista and non-Sandinista members of the governing junta. Violeta Chamorro and Alfonso Robelo resigned from the governing junta in 1980, and rumours began that members of the Ortega junta would consolidate power amongst themselves. These allegations spread, and rumors intensified that it was Ortega's goal to turn Nicaragua into a state modeled after Cuban Socialism. In 1979 and 1980, former Somoza supporters and ex-members of Somoza's National Guard formed irregular military forces, while the original core of the FSLN began to splinter. Armed opposition to the Sandinista Government eventually divided into two main groups: The Fuerza Democrática Nicaragüense (FDN), a U.S. supported army formed in 1981 by the CIA, U.S. State Department, and former members of the widely condemned Somoza-era Nicaraguan National Guard; and the Alianza Revolucionaria Democratica (ARDE) Democratic Revolutionary Alliance, a group that had existed since before the FSLN and was led by Sandinista founder and former FSLN supreme commander, Edén Pastora, a.k.a. "Commander Zero". and Milpistas, former anti-Somoza rural militias, which eventually formed the largest pool of recruits for the Contras. Although independent and often at conflict with each other, these guerrilla bands—along with a few others—all became generally known as "Contras" (short for "contrarrevolucionarios", en. "counter-revolutionaries").
The opposition militias were initially organized and largely remained segregated according to regional affiliation and political backgrounds. They conducted attacks on economic, military, and civilian targets. During the Contra war, the Sandinistas arrested suspected members of the Contra militias and censored publications they accused of collaborating with the enemy (i.e. the U.S., the FDN, and ARDE, among others).
State of Emergency (1982–88).
In March 1982 the Sandinistas declared an official State of Emergency. They argued that this was a response to attacks by counter-revolutionary forces. The State of Emergency lasted six years, until January 1988, when it was lifted.
Under the new "Law for the Maintenance of Order and Public Security" the "Tribunales Populares Anti-Somozistas" allowed for the indefinite holding of suspected counter-revolutionaries without trial. The State of Emergency, however, most notably affected rights and guarantees contained in the "Statute on Rights and Guarantees of Nicaraguans". Many civil liberties were curtailed or canceled such as the freedom to organize demonstrations, the inviolability of the home, freedom of the press, freedom of speech, and the freedom to strike.
All independent news program broadcasts were suspended. In total, twenty-four programs were cancelled. In addition, Sandinista censor Nelba Cecilia Blandón issued a decree ordering all radio stations to take broadcasts from government radio station La Voz de La Defensa de La Patria every six hours.
The rights affected also included certain procedural guarantees in the case of detention including habeas corpus. The State of Emergency was not lifted during the 1984 elections. There were many instances where rallies of opposition parties were physically broken up by Sandinsta youth or pro-Sandinista mobs. Opponents to the State of Emergency argued its intent was to crush resistance to the FSLN. James Wheelock justified the actions of the Directorate by saying "... We are annulling the license of the false prophets and the oligarchs to attack the revolution."
On October 5, 1985 the Sandinistas broadened the 1982 State of Emergency and suspended many more civil rights. A new regulation also forced any organization outside of the government to first submit any statement it wanted to make public to the censorship bureau for prior censorship.
After seizing power in Nicaragua, the Sandinista regime instituted dictatorial rule as early as December 1979, and formally announced a State of Emergency in 1982. Under the new "Law for the Maintenance of Order and Public Security" the "Tribunales Populares Anti-Somozistas" allowed for the indefinite holding of suspected counter-revolutionaries without trial. The State of Emergency, however, most notably affected rights and guarantees contained in the "Statute on Rights and Guarantees of Nicaraguans. Many civil liberties were curtailed or canceled such as the freedom to organize demonstrations, the inviolability of the home, freedom of the press, freedom of speech and, the freedom to strike. [2] All independent news program broadcasts were suspended. In total, twenty-four programs were cancelled. In addition, Sandinista censor Nelba Cecilia Blandón issued a decree ordering all radio stations to hook up every six hours to government radio station, La Voz de La Defensa de La Patria.[3] The rights affected also included certain procedural guarantees in the case of detention including habeas corpus.[4] The State of Emergency was not lifted during the 1984 elections. There were many instances where rallies of opposition parties were physically broken up by Sandinsta youth or pro-Sandinista mobs. Opponents to the State of Emergency argued its intent was to crush resistance to the FSLN. James Wheelock justified the actions of the Directorate by saying "... We are annulling the license of the false prophets and the oligarchs to attack the revolution.” [5]
Jamie Glazov describes human rights under this government as follows: "All Nicaraguans had to take part in the Marxist experiment. Thus, in perfect Khmer Rouge style, the Sandinistas inflicted a ruthless forcible relocation of tens of thousands of Indians from their land. Like Stalin, they used state-created famine as a weapon against these "enemies of the people." The Sandinista army committed myriad atrocities against the Indian population, killing and imprisoning approximately 15,000 innocent people. The crimes included not only mass murders of innocent natives themselves, but a calculated liquidation of their entire leadership – as the Soviet army had perpetrated against the Poles in Katyn in 1943. According to the Nicaraguan Commission of Jurists, the Sandinistas carried out over 8,000 political executions within three years of the revolution. The number of "anti-revolutionary" Nicaraguans who had "disappeared" in Sanadinista hands or had died "trying to escape" were numbered in the thousands. By 1983, the number of political prisoners in the Sandinistas' ruthless tyranny were estimated at 20,000. Torture was institutionalized. Numerous human rights organizations, including Amnesty International and the United Nations Human Rights Commission, have documented the atrocious record of Sandinista human rights abuses, which stood as the worst in Latin America. Political prisoners in Sandinista prisons, such as in Las Tejas, were consistently beaten, deprived of sleep and tortured with electric shocks. They were routinely denied food and water and kept in dark cubicles that had a surface of less than one square meter, known as chiquitas (little ones). These cubicles were too small to sit up in, were completely dark and had no sanitation and almost no ventilation.”[6]
The Sandinistas sent Soviet helicopter gunships and elite army units to attack the Indians; carried out mass arrests, jailings and torture; burned down 65 Indian communities; inflicted ethnic cleansing on 70,000 Indians; and tried to starve the Indians by cutting off food supplies. The Sandinistas boasted that they were “ready to eliminate the last Miskito Indian to take Sandinism to the Atlantic Coast.” [7]
For decades, Nicaragua had experienced some of the fastest economic growth in the hemisphere. Within a few years of Sandinista rule, wages had been fixed below poverty level and there was mass unemployment. There were shortages of nearly all basic goods, with inflation at 30,000%. Government studies found that three-quarters of schoolchildren suffered from malnutrition, while living standards were lower than Haiti. The World Bank found that Nicaragua was on the economic level of Somalia.
An inability of the Sandanistas to defeat the Contras caused Daniel Ortega to call the first free and open elections in Nicaragua since the Communists seized power for 1990. Although he was heavily favored to win, in a shocking turn of events he was defeated in an election for the Presidency of Nicaragua by Violetta Chamorro in 1990. Ortega stepped down with the defeat, but left much of his power structure behind that would make it difficult for the new government.
Some emergency measures were taken before 1982. In December 1979 special courts called "Tribunales Especiales" were established to speed up the processing of 7,000-8,000 National Guard prisoners. These courts operated through relaxed rules of evidence and due process and were often staffed by law students and inexperienced lawyers. However, the decisions of the "Tribunales Especiales" were subject to appeal in regular courts. Many of the National Guard prisoners were released immediately due to lack of evidence. Others were pardoned or released by decree. By 1986 only 2,157 remained in custody and only 39 were still being held in 1989 when they were released under the Esquipulas II agreement.
Sandinistas vs. Contras.
Upon assuming office in 1981, U.S. President Ronald Reagan condemned the FSLN for joining with Cuba in supporting "Marxist" revolutionary movements in other Latin American countries such as El Salvador. His administration authorized the CIA to begin financing, arming and training rebels, most of whom were the remnants of Somoza's National Guard, as anti-Sandinista guerrillas that were branded "counter-revolutionary" by leftists ("contrarrevolucionarios" in Spanish). This was shortened to "Contras", a label the force chose to embrace. Edén Pastora and many of the indigenous guerrilla forces, who were not associated with the "Somozistas", also resisted the Sandinistas.
The Contras operated out of camps in the neighboring countries of Honduras to the north and Costa Rica (see Edén Pastora cited below) to the south. As was typical in guerrilla warfare, they were engaged in a campaign of economic sabotage in an attempt to combat the Sandinista government and disrupted shipping by planting underwater mines in Nicaragua's Corinto harbour, an action condemned by the World Court as illegal. The U.S. also sought to place economic pressure on the Sandinistas, and, as with Cuba, the Reagan administration imposed a full trade embargo.
The contras also carried out a systematic campaign to disrupt the social reform programs of the government. This campaign included attacks on schools, health centers and the majority of the rural population that was sympathetic to the Sandinistas. Widespread murder, rape, and torture were also used as tools to destabilize the government and to "terrorize" the population into collaborating with the Contras. Throughout this campaign, the contras received military and financial support from the CIA and the Reagan Administration. This campaign has been condemned internationally for its many human rights violations. Contra supporters have often tried to downplay these violations, or countered that the Sandinista government carried out much more. In particular, the Reagan administration engaged in a campaign to alter public opinion on the contras that has been termed "white propaganda". In 1984, the International Court of Justice judged that the United States Government had been in violation of International law when it supported the contras.
After the U.S. Congress prohibited federal funding of the Contras through the Boland Amendment in 1983, the Reagan administration continued to back the Contras by raising money from foreign allies and covertly selling arms to Iran (then engaged in a vicious war with Iraq), and channelling the proceeds to the Contras (see the Iran–Contra affair). When this scheme was revealed, Reagan admitted that he knew about Iranian "arms for hostages" dealings but professed ignorance about the proceeds funding the Contras; for this, National Security Council aide Lt. Col. Oliver North took much of the blame.
Senator John Kerry's 1988 U.S. Senate Committee on Foreign Relations report on links between the Contras and drug imports to the US concluded that "senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems." According to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the US-backed president of Panama. The Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, "San Jose Mercury News" reporter Gary Webb published a series titled "Dark Alliance", linking the origins of crack cocaine in California to the CIA-Contra alliance. Freedom of Information Act inquiries by the National Security Archive and other investigators unearthed a number of documents showing that White House officials, including Oliver North, knew about and supported using money raised via drug trafficking to fund the Contras. Sen. John Kerry's report in 1988 led to the same conclusions. However, the Justice Department denied the allegations.
The Contra war unfolded differently in the northern and southern zones of Nicaragua. Contras based in Costa Rica operated on Nicaragua's Atlantic coast, which is sparsely populated by indigenous groups including the Miskito, Sumo, Rama, Garifuna, and Mestizo. Unlike Spanish-speaking western Nicaragua, the Atlantic Coast is predominantly English-speaking and was largely ignored by the Somoza regime. The "costeños" did not participate in the uprising against Somoza and viewed Sandinismo with suspicion from the outset.
Elections.
1984 election.
While the Sandinistas encouraged grassroots pluralism, they were perhaps less enthusiastic about national elections. They argued that popular support was expressed in the insurrection and that further appeals to popular support would be a waste of scarce resources. International pressure and domestic opposition eventually pressed the government toward a national election. Tomás Borge warned that the elections were a concession, an act of generosity and of political necessity. On the other hand, the Sandinistas had little to fear from the election given the advantages of incumbency and the restrictions on the opposition, and they hoped to discredit the armed efforts to overthrow them.
A broad range of political parties, ranging in political orientation from far-left to far-right, competed for power. Following promulgation of a new populist constitution, Nicaragua held national elections in 1984. Independent electoral observers from around the world—including groups from the UN as well as observers from Western Europe—found that the elections had been fair. Several groups, however, disputed this, including: UNO, a broad coalition of anti-Sandinista activists; COSEP, an organization of business leaders; the Contra group "FDN", organized by former Somozan-era National Guardsmen; landowners; businessmen; peasant highlanders; and what some claimed as their patron, the U.S. government.
Although initially willing to stand in the 1984 elections, the UNO, headed by Arturo Cruz (a former Sandinista) declined participation in the elections based on their own objections to the restrictions placed on the electoral process by the State of Emergency and the official advisement of President Ronald Reagan's State Department, who feared that their participation would legitimize the election process. Among other parties that abstained was COSEP, who had warned the FSLN that they would decline participation unless freedom of the press was reinstituted. Coordinadora Democrática (CD) also refused to file candidates and urged Nicaraguans not to take part in the election, the Independent Liberal Party (PLI), headed by Virgilio Godoy Reyes announced its refusal to participate in October. Consequently, when the elections went ahead the U.S. raised objections based upon political restrictions instituted by the State of Emergency (e.g., censorship of the press, cancellation of habeas corpus, and the curtailing of free assembly).
Daniel Ortega and Sergio Ramírez were elected president and vice-president, and the FSLN won an overwhelming 61 out of 96 seats in the new National Assembly, having taken 67% of the vote on a turnout of 75%. Despite international validation of the elections by multiple political and independent observers (virtually all from among U.S. allies) the United States refused to recognize the elections, with President Ronald Reagan denouncing the elections as a sham. According to a detailed study, since the 1984 election was for posts subordinate to the Sandinista Directorate, the elections were no more subject to approval by vote than the Central Committee of the Communist Party is in countries of the East Bloc. Daniel Ortega began his six-year presidential term on January 10, 1985. After the United States Congress turned down continued funding of the Contras in April 1985, the Reagan administration ordered a total embargo on United States trade with Nicaragua the following month, accusing the Sandinista government of threatening United States security in the region.
1990 election.
The elections of 1990, which had been mandated by the constitution passed in 1987, saw the Bush administration funnel $49.75 million of 'non-lethal' aid to the Contras, as well as $9m to the opposition UNO—equivalent to $2 billion worth of intervention by a foreign power in a US election at the time, and proportionately five times the amount George Bush had spent on his own election campaign. When Violetta Chamorro visited the White House in November 1989, the US pledged to maintain the embargo against Nicaragua unless Violeta Chamorro won.
In August 1989, the month that campaigning began, the Contras redeployed 8,000 troops into Nicaragua, after a funding boost from Washington, continued their guerrilla war. No fewer than 50 FSLN candidates were assassinated. The Contras also distributed thousands of UNO leaflets.
Years of conflict had left 50,000 casualties and $12b of damages in a society of 3.5m people and an annual GNP of $2b. After the war, a survey was taken of voters: 75.6% agreed that if the Sandinistas had won, the war would never have ended. 91.8% of those who voted for the UNO agreed with this. (William I Robinson, op cit) The Library of Congress Country Studies on Nicaragua states:
Opposition (1990–2006).
In 1987, due to a stalemate with the Contras, the Esquipulas II treaty was brokered by Costa Rican President Óscar Arias Sánchez. The treaty's provisions included a call for a cease-fire, freedom of expression, and national elections. After the February 26, 1990 elections, the Sandinistas lost and peacefully passed power to the National Opposition Union (UNO), an alliance of 14 opposition parties ranging from the conservative business organization COSEP to Nicaraguan communists. UNO's candidate, Violeta Barrios de Chamorro, replaced Daniel Ortega as president of Nicaragua.
Reasons for the Sandinista loss in 1990 are disputed. Defenders of the defeated government assert that Nicaraguans voted for the opposition due to the continuing U.S. economic embargo and potential Contra threat. Others have alleged that the United States threatened to continue to support the Contras and continue the civil war if the regime was not voted out of power. Opponents claim that Contra warfare had largely died down, and that the Sandinistas had grown increasingly unpopular, particularly due to conscription and crackdowns on political freedoms. An important reason, regardless of perspective, was that after a decade of the U.S. backed war and embargo, Nicaragua's economy and infrastructure were badly damaged and the United States obviously supported only parties in opposition to the Sandinista. The U.S. also helped keep the rightist factions united so there would not be two strong rightist candidates.
After their loss, most of the Sandinista leaders held most of the private property and businesses that had been confiscated and nationalized by the FSLN government. This process became known as the "piñata" and was tolerated by the new Chamorro government. Ortega also claimed to "rule from below" through groups he controls such as labor unions and student groups. Prominent Sandinistas also created a number of nongovernmental organizations to promote their ideas and social goals.
Daniel Ortega remained the head of the FSLN, but his brother Humberto resigned from the party and remained at the head of the Sandinista Army, becoming a close confidante and supporter of Chamorro. The party also experienced a number of internal divisions, with prominent Sandinistas such as Ernesto Cardenal and Sergio Ramírez resigning to protest what they described as heavy-handed domination of the party by Daniel Ortega. Ramírez also founded a separate political party, the Sandinista Renovation Movement (MRS); his faction came to be known as the "renovistas", who favor a more social democratic approach than the "ortodoxos", or hardliners. In the 1996 Nicaraguan election, Ortega and Ramírez both campaigned unsuccessfully as presidential candidates on behalf of their respective parties, with Ortega receiving 43% of the vote while Arnoldo Alemán of the Constitutional Liberal Party received 51%. The Sandinistas won second place in the congressional elections, with 36 of 93 seats.
Daniel Ortega was re-elected as leader of the FSLN in 1998. Municipal elections in November 2000 saw a strong Sandinista vote, especially in urban areas, and former Tourism Minister Herty Lewites was elected mayor of Managua. This significant result led to expectations of a close race in the presidential elections scheduled for November 2001. Daniel Ortega and Enrique Bolaños of the Constitutional Liberal Party (PLC) ran neck-and-neck in the polls for much of the campaign, but in the end the PLC won a clear victory. The results of these elections were that the FSLN won 42.6% of the vote for parliament (versus 52.6% for the PLC), giving them 41 out of the 92 seats in the National Assembly (versus 48 for the PLC). In the presidential race, Ortega lost to Bolaños 46.3% to 53.6%.
Daniel Ortega was once again re-elected as leader of the FSLN in March 2002 and re-elected as president of Nicaragua in November 2006.
Back in government.
In 2006, Daniel Ortega was elected president with 38% of the vote (see Nicaraguan general election, 2006). This occurred despite the fact that the breakaway Sandinista Renovation Movement continued to oppose the FSLN, running former Mayor of Managua Herty Lewites as its candidate for president. However, Lewites died just several month before the elections.
The FSLN also won 38 seats in the congressional elections, becoming the party with the largest representation in parliament. The split in the Constitutionalist Liberal Party helped to allow the FSLN to become the largest party in Congress, however it should be noted that the Sandinista vote had a minuscule split between the FSLN and MRS, and that the liberal party combined is larger than the Frente Faction. In 2010, several liberal congressmen raised accusations about the FSLN presumably attempting to buy votes in order to pass constitutional reforms that would allow Ortega to run for office for the 6th time since 1984. In 2011, Ortega was re-elected as President.
"Zero Hunger Project".
The "Zero Hunger Program", which aims to reduce poverty in the rural areas over a five-year period, was inaugurated by President Daniel Ortega and other members of his administration in the northern department of Jinotega. The program was designed to achieve the first objective of the United Nations' Millennium Development Goals, "to eradicate extreme poverty and reduce hunger to zero."
"Zero Hunger" with its budget of US$150 million plans to deliver a US$2,000 bond or voucher to 75,000 rural families between 2007 and 2012. The voucher will consist of the delivery of a pregnant cow and a pregnant sow, five chickens and a rooster, seeds, fruit-bearing plants and plants for reforestation. The project's short-term objective is to have each rural family capable of producing enough milk, meat, eggs, fruits, vegetables and cereals to cover its basic needs while its medium range objective is to establish local markets and export certain products.
The families that benefit from the project will be required to pay back 20 percent of the amount that they receive in order to create a rural fund that will guarantee the continuity of the program. NGOs and representatives from each community will be in charge of managing the project.
Ideology.
Through the media and the works of FSLN leaders such as Carlos Fonseca, the life and times of Augusto César Sandino became the unique symbol of this revolutionary force in Nicaragua. The ideology of Sandinismo gained momentum in 1974, when a Sandinista initiated hostage situation resulted in the Somoza government adhering to FSLN demands and publicly printing and airing work on Sandino in well known newspapers and media outlets.
During the long struggle against Somoza, the FSLN leaders' internal disagreements over strategy and tactics were reflected in three main factions:
Nevertheless, while ideologies varied between FSLN leaders, all leaders essentially agreed that Sandino provided a path for the Nicaragua masses to take charge, and the FSLN would act as the legitimate vanguard. The extreme end of the ideology links Sandino to Roman Catholicism and portrays him as descending from the mountains in Nicaragua knowing he would be betrayed and killed. Generally however, most Sandinistas associated Sandino on a more practical level, as a heroic and honest person who tried to combat the evil forces of imperialist national and international governments that existed in Nicaragua's history.
Principles of government.
For purposes of making sense of how to govern, the FSLN drew four fundamental principles from the work of Carlos Fonseca and his understanding of the lessons of Sandino. According to Bruce E. Wright, "the Governing Junta of National Reconstruction agreed, under Sandinista leadership, that these principles had guided it in putting into practice a form of government that was characterized by those principles." It is generally accepted that these following principles have evolved the "ideology of Sandinismo." Three of these (excluding popular participation, which was presumably contained in Article 2 of the Constitution of Nicaragua) were to ultimately be guaranteed by Article 5 of the Constitution of Nicaragua. They are as follows:
It is accepted by many scholars that the period of the FSLN guiding the Nicaraguan revolution through the control of the state was a living experiment in an attempt to construct a truly democratic and revolutionary socialism. Bruce E. Wright claims that "this was a crucial contribution from Fonseca's work that set the template for FSLN governance during the revolutionary years and beyond."
Policies and programs.
Foreign policy.
Cuban assistance.
Beginning in 1967, the Cuban General Intelligence Directorate, or DGI, had begun to establish ties with various Nicaraguan revolutionary organizations. By 1970 the DGI had managed to train hundreds of Sandinista guerrilla leaders and had vast influence over the organization. After the successful ousting of Somoza, DGI involvement in the new Sandinista government expanded rapidly. An early indication of the central role that the DGI would play in the Cuban-Nicaraguan relationship is a meeting in Havana on July 27, 1979, at which diplomatic ties between the two countries were re-established after more than 25 years. Julián López Díaz, a prominent DGI agent, was named Ambassador to Nicaragua. Cuban military and DGI advisors, initially brought in during the Sandinista insurgency, would swell to over 2,500 and operated at all levels of the new Nicaraguan government.
The Cubans would like to have helped more in the development of Nicaragua towards socialism. Following the US invasion of Grenada, countries previously looking for support from Cuba saw that the United States was likely to take violent action to discourage this.
Cuban assistance after the revolution.
The early years of the Nicaraguan revolution had strong ties to Cuba. The Sandinista leaders acknowledged that the FSLN owed a great debt to the socialist island. Once the Sandinistas assumed power, Cuba gave Nicaragua military advice, as well as aid in education, health care, vocational training and industry building for the impoverished Nicaraguan economy. In return, Nicaragua provided Cuba with grains and other foodstuffs to help Cuba overcome the effects of the US embargo.
Relationship with eastern bloc intelligence agencies.
Pre-Revolution.
According to Cambridge University historian Christopher Andrew, who undertook the task of processing the Mitrokhin Archive, Carlos Fonseca Amador, one of the original three founding members of the FSLN had been recruited by the KGB in 1959 while on a trip to Moscow. This was one part of Aleksandr Shelepin's 'grand strategy' of using national liberation movements as a spearhead of the Soviet Union's foreign policy in the Third World, and in 1960 the KGB organized funding and training for twelve individuals that Fonseca handpicked. These individuals were to be the core of the new Sandinista organization. In the following several years, the FSLN tried with little success to organize guerrilla warfare against the government of Luis Somoza Debayle. After several failed attempts to attack government strongholds and little initial support from the local population, the National Guard nearly annihilated the Sandinistas in a series of attacks in 1963. Disappointed with the performance of Shelepin's new Latin American "revolutionary vanguard", the KGB reconstituted its core of the Sandinista leadership into the ISKRA group and used them for other activities in Latin America.
According to Andrew, Mitrokhin says during the following three years the KGB handpicked several dozen Sandinistas for intelligence and sabotage operations in the United States. Andrew and Mitrokhin say that in 1966, this KGB-controlled Sandinista sabotage and intelligence group was sent to northern Mexico near the US border to conduct surveillance for possible sabotage.
In July 1961 during the Berlin Crisis of 1961 KGB chief Alexander Shelepin sent a memorandum to Soviet premier Nikita Khrushchev containing an array of proposals to create a situation in various areas of the world which would favor dispersion of attention and forces by the US and their satellites, and would tie them down during the settlement of the question of a German peace treaty and West Berlin. It was planned, inter alia, to organize an armed mutiny in Nicaragua in coordination with Cuba and with the "Revolutionary Front Sandino". Shelepin proposed to make appropriations from KGB funds in addition to the previous assistance $10,000 for purchase of arms.
Khrushchev sent the memo with his approval to his deputy Frol Kozlov and on August 1 it was, with minor revisions, passed as a CPSU Central Committee directive. The KGB and the Soviet Ministry of Defense were instructed to work out more; specific measures and present them for consideration by the Central Committee.
Cooperation with foreign intelligence agencies during the 1980s.
Other researchers have documented the contribution made from other Warsaw Pact intelligence agencies to the fledgling Sandinista government including the East German Stasi, by using recently declassified documents from Berlin as well as from former Stasi spymaster Markus Wolf who described the Stasi's assistance in the creation of a secret police force modeled on East Germany's
Educational assistance.
Cuba was instrumental in the Nicaraguan Literacy Campaign. Nicaragua was a country with a very high rate of illiteracy, but the campaign succeeded in lowering the rate from 50% to 12%. The revolution in Cuban education since the ousting of the US-backed Batista regime not only served as a model for Nicaragua but also provided technical assistance and advice. The Literacy Campaign was one of the success stories of the Sandinistas' reign and Cuba played an important part in this, providing teachers on a yearly basis after the revolution. Prevost states that "Teachers were not the only ones studying in Cuba, about 2,000 primary and secondary students were studying on the Isle of Youth and the cost was covered by the host country (Cuba)".
1980 literacy campaign.
The 1980 Literacy Campaign is considered to have been a major contribution to Nicaraguan society during the Sandinista rule. The goals of the literacy campaign were socio-political, strategic as well as educational. It was the most prominent campaign with regards to the new education system. Illiteracy in Nicaragua was significantly reduced from 50.3% to 12.9%. One of the government's major concerns was the previous education system under the Somoza regime which did not see education as a major factor on the development of the country. As mentioned in the Historical Program of the FSLN of 1969, education was seen as a right and the pressure to stay committed to the promises made in the program was even stronger. 1980 was declared the "Year of Literacy" and the major goals of the campaign that started only 8 months after the FSLN took over. This included the eradication of illiteracy, the integration of different classes, races, gender and age. Political awareness and the strengthening of political and economic participation of the Nicaraguan people was also a central goal of the Literacy Campaign. The campaign was a key component of the FSLN's cultural transformation agenda.
The basic reader which was disseminated and used by teacher was called "Dawn of the People" based on the themes of Sandino, Carlos Fonseca, and the Sandinista struggle against imperialism and defending the revolution. Political education was aimed at creating a new social values based on the principles of Sandinista socialism, such as social solidarity, worker's democracy, egalitarianism, and anti-imperialism.
Health care.
Health care was another area where the Sandinistas made incredible gains and are widely recognized for this accomplishment, e.g. by Oxfam. In this area Cuba also played a role by again offering expertise and know-how to Nicaragua. Over 1,500 Cuban doctors worked in Nicaragua and provided more than five million consultations. Cuban personnel were essential in the elimination of polio, the decrease in whooping cough, rubella, measles and the lowering of the infant mortality rate. Gary Prevost states that Cuban personnel made it possible for Nicaragua to have a truly national health care system reaching a majority of its citizens.
Vocational assistance.
Cuba has participated in the training of Nicaraguan workers in the use of new machinery imported to Nicaragua. The Nicaraguan revolution put the country's government on the United States' black book; therefore the Sandinistas would not receive any aid from the United States. The United States embargo against Nicaragua, imposed by the Reagan administration in May 1985, made it impossible for Nicaragua to receive spare parts for US-made machines, so this led Nicaragua to look to other countries for help. Cuba was the best choice because of the shared language and proximity and also because it had imported similar machinery over the years. Nicaraguans went to Cuba for short periods of three to six months and this training involved close to 3,000 workers. Many countries, including Canada and the UK, sent farm equipment to Nicaragua.
Industry and infrastructure.
Cuba helped Nicaragua in huge projects such as building roads, power plants and sugar mills. Cuba also attempted to help Nicaragua build the first overland route linking Nicaragua's Atlantic and Pacific coasts. The road was meant to traverse 260 miles of jungle, but completion of the road and usage was hindered by the Contra war, and it was never completed.
Another significant feat was the building of the Tipitapa-Malacatoya sugar mill. It was completed and inaugurated during a visit by Fidel Castro in January 1985. The plant used the newest technology available and was built by workers trained in Cuba. Also during this visit Castro announced that all debts incurred on this project were absolved. Cuba also provided numerous technicians to aid in the sugar harvest and assist in the rejuvenation of several old sugar mills. Cubans also assisted in building schools and similar projects.
Ministry of Culture.
After the Nicaraguan revolution, the Sandinista government established a Ministry of Culture in 1980. The ministry was spearheaded by Ernesto Cardenal, a famous poet and priest. The ministry was established in order to socialize the modes of cultural production. This extended to various art forms, including dance, music, art, theatre and poetry. The project was created to democratize culture on a national level. The aim of the ministry was to "democratize art" by making it accessible to all social classes as well as protecting the right of the oppressed to produce, distribute and receive art. In particular, the ministry was devoted to the development of working class and "campesino", or peasant culture. Therefore, the ministry sponsored cultural workshops throughout the country until October 1988 when the Ministry of Culture was integrated into the Ministry of Education because of financial troubles.
The objective of the workshops was to recognize and celebrate neglected forms of artistic expression. The ministry created a program of cultural workshops known as, "Casas de Cultura and Centros Populares de Cultura". The workshops were set up in poor neighbourhoods and rural areas and advocated universal access and consumption of art in Nicaragua. The ministry assisted in the creation of theatre groups, folklore and artisanal production, song groups, new journals of creation and cultural criticism, and training programs for cultural workers. Moreover, the ministry created a Sandinista daily newspaper named "Barricada" and its weekly cultural addition named "Ventana" along with the "Television Sandino, Radio Sandino" and the Nicaraguan film production unit called the INCINE. There were existing papers which splintered after the revolution and produced other independent, pro-Sandinista newspapers, such as "El Nuevo Diario" and its literary addition "Nuevo Amanecer Cultural". Furthermore, Editorial Nueva Nicaragua a state publishing house for literature was created. The ministry collected and published political poetry of the revolutionary period, known as testimonial narrative, a form of literary genre that recorded the experiences of individuals in the course of the revolution.
The ministry also developed a new anthology of Ruben Dario, a Nicaraguan poet and writer, established a Ruben Dario prize for Latin American writers, the Leonel Rugama prize for young Nicaraguan writers, as well as public poetry readings and contests, cultural festivals and concerts. The Sandinista regime tried to keep the revolutionary spirit alive by empowering its citizens artistically. At the time of its inception, the Ministry of Culture needed according to Cardenal, “to bring a culture to the people who were marginalized from it. We want a culture that is not the culture of an elite, of a group that is considered ‘cultivated,’ but rather of an entire people.” Nevertheless, the success of the Ministry of Culture had mixed results and by 1985 criticism arose over artistic freedom in the poetry workshops. The poetry workshops became a matter for criticism and debate. Critics argued that the ministry imposed too many principles and guidelines for young writers in the workshop, such as, asking them to avoid metaphors in their poetry and advising them to write about events in their everyday life. Critical voices came from established poets and writers represented by the "Asociacion Sandinista de Trabajadores de la Cultura" (ASTC) and from the "Ventana" both of which were headed by Rosario Murillo. They argued that young writers should be exposed to different poetic styles of writing and resources developed in Nicaragua and elsewhere. Furthermore, they argued that the ministry exhibited a tendency that favored and fostered political and testimonial literature in post-revolutionary Nicaragua.
Economy.
The new government, formed in 1979 and dominated by the Sandinistas, resulted in a socialist model of economic development. The new leadership was conscious of the social inequities produced during the previous thirty years of unrestricted economic growth and was determined to make the country's workers and peasants, the "economically underprivileged", the prime beneficiaries of the new society. Consequently, in 1980 and 1981, unbridled incentives to private investment gave way to institutions designed to redistribute wealth and income. Private property would continue to be allowed, but all land belonging to the Somozas was confiscated.
However, the ideology of the Sandinistas put the future of the private sector and of private ownership of the means of production in doubt. Even though under the new government both public and private ownership were accepted, government spokespersons occasionally referred to a reconstruction phase in the country's development, in which property owners and the professional class would be tapped for their managerial and technical expertise. After reconstruction and recovery, the private sector would give way to expanded public ownership in most areas of the economy. Despite such ideas, which represented the point of view of a faction of the government, the Sandinista government remained officially committed to a mixed economy.
Economic growth was uneven in the 1980s. Restructuring of the economy and the rebuilding immediately following the end of the civil war caused the GDP to jump about 5 percent in 1980 and 1981. Each year from 1984 to 1990, however, showed a drop in the GDP. Reasons for the contraction included the reluctance of foreign banks to offer new loans, the diversion of funds to fight the new insurrection against the government, and, after 1985, the total embargo on trade with the United States, formerly Nicaragua's largest trading partner. After 1985 the government chose to fill the gap between decreasing revenues and mushrooming military expenditures by printing large amounts of paper money. Inflation skyrocketed, peaking in 1988 at more than 14,000 percent annually.
Measures taken by the government to lower inflation were largely wiped out by natural disaster. In early 1988, the administration of Daniel José Ortega Saavedra (Sandinista junta coordinator 1979–85, president 1985–90) established an austerity program to lower inflation. Price controls were tightened, and a new currency was introduced. As a result, by August 1988, inflation had dropped to an annual rate of 240 percent. The following month, however, Hurricane Joan cut a devastating path directly across the center of the country. Damage was extensive, and the government's program of massive spending to repair the infrastructure destroyed its anti-inflation measures.
In its eleven years in power, the Sandinista government never overcame most of the economic inequalities that it inherited from the Somoza era. Years of war, policy missteps, natural disasters, and the effects of the United States trade embargo all hindered economic development. The early economic gains of the Sandinistas were wiped out by seven years of sometimes precipitous economic decline, and in 1990, by most standards, Nicaragua and most Nicaraguans were considerably poorer than they were in the 1970s.
Women in revolutionary Nicaragua.
The women of Nicaragua prior to, during and after the revolution played a prominent role within the nation's society as they have commonly been recognized, throughout history and across all Latin American states, as its backbone. Nicaraguan women were therefore directly affected by all of the positive and negative events that took place during this revolutionary period. The victory of the Sandinista National Liberation Front (FSLN) in 1979 brought about major changes and gains for women, mainly in legislation, broad educational opportunities, training programs for working women, childcare programs to help women enter the work force and greatly increased participation and even leadership positions in a whole range of political activities. This, in turn, reduced the great burdens that the women of Nicaragua were faced with prior to the revolution. During the Sandinista government, women were more active politically. The great majority of members of the neighborhood committees (Comités de Defensa Sandinista) were women. By 1987, 31% of the executive positions in the Sandinista government, 27% of the leadership positions of the FSLN, and 25% of the FSLN's active membership were women.
Supporters of the Sandinistas see their era as characterized by the creation and implementation of successful social programs which were free and made widely available to the entire nation. Some of the more successful programs for women that were implemented by the Sandinistas were in the areas of Education Nicaraguan Literacy Campaign, Health, and Housing. Providing subsidies for basic foodstuffs and the introduction of mass employment were also memorable contributions of the FSLN. The Sandinistas were particularly advantageous for the women of Nicaraguan as they promoted progressive views on gender as early as 1969 claiming that the revolution would "abolish the detestable discrimination that women have suffered with regard to men and establish economic, political and cultural equality between men and women." This was evident as the FSLN began integrating women into their ranks by 1967, unlike other left-wing guerilla groups in the region. This goal was not fully reached because the roots of gender inequality were not explicitly challenged. Women's participation within the public sphere was also substantial, as many took part in the armed struggle as part of the FSLN or as part of counter-revolutionary forces.
Nicaraguan women also organized independently in support of the revolution and their cause. Some of those organizations were the Socialist Party (1963), Federación Democrática (which support the FSLN in rural areas), and Luisa Amanda Espinoza Association of Nicaraguan Women ("Asociación de Mujeres Nicaragüenses Luisa Amanda Espinosa", AMNLAE). However, since Daniel Ortega, was defeated in the 1990 election by the United Nicaraguan Opposition (UNO) coalition headed by Violeta Chamorro, the situation for women in Nicaragua was seriously altered. In terms of women and the labor market, by the end of 1991 AMNLAE reported that almost 16,000 working women—9,000 agricultural laborers, 3,000 industrial workers, and 3,800 civil servants, including 2,000 in health, 800 in education, and 1,000 in administration—had lost their jobs. The change in government also resulted in the drastic reduction or suspension of all Nicaraguan social programs, which brought back the burdens characteristic of pre-revolutionary Nicaragua. The women were forced to maintain and supplement community social services on their own without economic aid or technical and human resource.
Relationship with the Catholic Church.
The Roman Catholic Church's relationship with the Sandinistas was extremely complex. Initially, the Church was committed to supporting the Somoza regime. The Somoza dynasty was willing to secure the Church a prominent place in society as long as it did not attempt to subvert the authority of the regime. Under the constitution of 1950 the Roman Catholic Church was recognized as the official religion and church-run schools flourished. It was not until the late 1970s that the Church began to speak out against the corruption and human rights abuses that characterized the Somoza regime.
The Catholic hierarchy initially disapproved of the Sandinistas' revolutionary struggle against the Somoza dynasty. In fact, the revolutionaries were perceived as proponents of "godless communism" that posed a threat to the traditionally privileged place that the Church occupied within Nicaraguan society. Nevertheless, the increasing corruption and repression characterizing the Somoza rule and the likelihood that the Sandinistas would emerge victorious ultimately influenced Archbishop Miguel Obando y Bravo to declare formal support for the Sandinistas' armed struggle. Throughout the revolutionary struggle, the Sandinistas enjoyed the grassroots support of clergy who were influenced by the reforming zeal of Vatican II and dedicated to a "preferential option for the poor" (for comparison, see liberation theology). Numerous Christian base communities (CEBs) were created in which lower level clergy and laity took part in consciousness raising initiatives to educate the peasants about the institutionalized violence they were suffering from. Some priests took a more active role in supporting the revolutionary struggle. For example, Father Gaspar García Laviana took up arms and became a member of FSLN.
Soon after the Sandinistas assumed power, the hierarchy began to oppose the Sandinistas government. The Archbishop was a vocal source of domestic opposition. The hierarchy was alleged to be motivated by fear of the emergence of the 'popular church' which challenged their centralized authority. The hierarchy also opposed social reforms implemented by the Sandinistas to aid the poor, allegedly because they saw it as a threat to their traditionally privileged position within society. In response to this perceived opposition, the Sandinistas shut down the church-run Radio Católica radio station on multiple occasions.
The Sandinistas' relationship with the Roman Catholic Church deteriorated as the Contra War dragged on. The hierarchy refused to speak out against the counterrevolutionary activities of the contras and failed to denounce American military aid. State media accused the Catholic Church of being reactionary and supporting the Contras. According to former President Ortega, "The conflict with the church was strong, and it costs us, but I don't think it was our fault. ... There were so many people being wounded every day, so many people dying, and it was hard for us to understand the position of the church hierarchy in refusing to condemn the contras." The hierarchy-state tensions were brought to the fore with Pope John Paul II 1983 visit to Nicaragua. Hostility to the Catholic Church became so great that at one point, FSLN militants shouted down Pope John Paul II as he tried to say Mass. Therefore, while the activities of the 'popular church' contributed to the success of the Sandinista revolution, the hierarchy's opposition was a major factor in the downfall of the revolutionary government.
Human rights violations by the Sandinistas.
"Time" magazine in 1983 published reports of human rights violations in an article which stated that "According to Nicaragua's Permanent Commission on Human Rights, the regime detains several hundred people a month; about half of them are eventually released, but the rest simply disappear." "Time" also interviewed a former deputy chief of Nicaraguan military counterintelligence, who stated that he had fled Nicaragua after being ordered to eliminate 800 Miskito prisoners and make it look like they had died in combat. Another article described Sandinista neighbourhood "Defense Committees", modeled on similar Cuban Committees for the Defense of the Revolution, which according to critics were used to unleash mobs on anyone who was labeled a counterrevolutionary. Nicaragua's only opposition newspaper, La Prensa, was subject to strict censorship. The newspaper's editors were forbidden to print anything negative about the Sandinistas either at home or abroad.
Nicaragua's Permanent Commission on Human Rights reported 2,000 murders in the first six months and 3,000 disappearances in the first few years. It has since documented 14,000 cases of torture, rape, kidnapping, mutilation and murder.
The Inter-American Commission on Human Rights (IACHR) in a 1981 report found evidence for mass executions in the period following the revolution. It stated "In the Commission's view, while the government of Nicaragua clearly intended to respect the lives of all those defeated in the civil war, during the weeks immediately subsequent to the Revolutionary triumph, when the government was not in effective control, illegal executions took place which violated the right to life, and these acts have not been investigated and the persons responsible have not been punished." The IACHR also stated that: "The Commission is of the view that the new regime did not have, and does not now have, a policy of violating the right to life of political enemies, including among the latter the former guardsmen of the Government of General Somoza, whom a large sector of the population of Nicaragua held responsible for serious human rights violations during the former regime; proof of the foregoing is the abolition of the death penalty and the high number of former guardsmen who were prisoners and brought to trial for crimes that constituted violations of human rights."
A 1983 report from the same source documented allegations of human rights violations against the Miskito Indians, which were alleged to have taken place after opposition forces (the Contras) infiltrated a Miskito village in order to launch attacks against government soldiers, and as part of a subsequent forced relocation program. Allegations included arbitrary imprisonment without trial, "disappearances" of such prisoners, forced relocation, and destruction of property. In late 1981, the CIA conspiracy "Operation Red Christmas" was exposed to separate the Atlantic region from the rest of Nicaragua. Red Christmas aimed to seize territory on Nicaragua's mainland and overthrow the Nicaraguan government. The Nicaraguan government responded to the provocations by transferring some 8500 Miskitos 50 miles south to a settlement called Tasba Pri. The U.S. government accused Nicaragua of "genocide". The U.S. government produced a photo alleged to showed Miskito bodies being burned by Sandinista troops; however, the photo was actually of people killed by Somoza's National Guard in 1978.
The 1991 annual report by the same organization, "In September 1990, the Commission was informed of the discovery of common graves in Nicaragua, especially in areas where fighting had occurred. The information was provided by the Nicaraguan Pro Human Rights Association, which had received its first complaint in June 1990. By December 1991, that Association had received reports of 60 common graves and had investigated 15 of them. While most of the graves seem to be the result of summary executions by members of the Sandinista People's Army or the State Security, some contain the bodies of individuals executed by the Nicaraguan Resistance."
The 1992 annual report by the same organization contains details of mass graves and investigations which suggest that mass executions had been carried out. One such grave contained 75 corpses of peasants who were believed to have been executed in 1984 by government security forces pretending to be members of the contras. Another grave was also found in the town of Quininowas which contained six corpses, believed to be an entire family killed by government forces when the town was invaded. A further 72 graves were reported as being found, containing bodies of people, the majority of whom were believed to have been executed by agents of the state and some also by the contras.
Politicization of human rights.
The issue of human rights also became highly politicised at this time as human rights is claimed to be a key component of propaganda created by the Reagan administration to help legitimise its policies in the region. The Inter-Church Committee on Human Rights in Latin America (ICCHRLA) in its "Newsletter" stated in 1985 that: "The hostility with which the Nicaraguan government is viewed by the Reagan administration is an unfortunate development. Even more unfortunate is the expression of that hostility in the destabilization campaign developed by the US administration. ... An important aspect of this campaign is misinformation and frequent allegations of serious human rights violations by the Nicaraguan authorities." Among the accusations in the Heritage Foundation report and the Demokratizatsiya article are references to alleged policies of religious persecution, particularly anti-semitism. The ICCHRLA in its newsletter stated that: "From time to time the current U.S. administration, and private organizations sympathetic to it, have made serious and extensive allegations of religious persecution in Nicaragua. Colleague churches in the United States undertook onsite investigation of these charges in 1984. In their report, the delegation organized by the Division of Overseas Ministries of the National Council of Churches of Christ in the United States concluded that there is 'no basis for the charge of systematic religious persecution'. The delegation 'considers this issue to be a device being used to justify aggressive opposition to the present Nicaraguan government.'" On the other hand, some elements of the Catholic Church in Nicaragua, among them Archbishop Miguel Obando y Bravo, strongly criticized the Sandinistas. The Archbishop stated "The government wants a church that is aligned with the Marxist–Leninist regime." The Inter-American Commission on Human Rights states that: "Although it is true that much of the friction between the Government and the churches arises from positions that are directly or indirectly linked to the political situation of the country, it is also true that statements by high government officials, official press statements, and the actions of groups under the control of the Government have gone beyond the limits within which political discussions should take place and have become obstacles to certain specifically religious activities."
Human Rights Watch also stated in its 1989 report on Nicaragua that: "Under the Reagan administration, U.S. policy toward Nicaragua's Sandinista government was marked by constant hostility. This hostility yielded, among other things, an inordinate amount of publicity about human rights issues. Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the "contras"."
In 1987, a report was published by the UK based NGO Catholic Institute for International Relations (CIIR, now known as "Progressio"), a human rights organization which identifies itself with Liberation theology. The report, "Right to Survive: Human Rights in Nicaragua", discussed the politicisation of the human rights issue: "The Reagan administration, with scant regard for the truth, has made a concerted effort to paint as evil a picture as possible of Nicaragua, describing it as a 'totalitarian dungeon'. Supporters of the Sandinistas ... have argued that Nicaragua has a good record of human rights compared with other Central American countries and have compared Nicaragua with other countries at war." The CIIR report refers to estimates made by the NGO Americas Watch which count the number of non-battle related deaths and disappearances for which the government was responsible up to the year 1986 as "close to 300".
According to the CIIR report, Amnesty International and Americas Watch stated that there is no evidence that the use of torture was sanctioned by the Nicaraguan authorities, although prisoners reported the use of conditions of detention and interrogation techniques that could be described as psychological torture. The Red Cross made repeated requests to be given access to prisoners held in state security detention centers, but were refused. The CIIR was critical of the Permanent Commission on Human Rights (PCHR or CPDH in Spanish), claiming that the organisation had a tendency to immediately publish accusations against the government without first establishing a factual basis for the allegations. The CIIR report also questioned the independence of the Permanent Commission on Human Rights, referring to an article in the "Washington Post" which claims that the National Endowment for Democracy, an organization funded by the US government, allocated a concession of US$50,000 for assistance in the translation and distribution outside Nicaragua of its monthly report, and that these funds were administered by the Committee for Democracy in Central America (Prodemca), a US-based organization which later published full-page adverisments in the "Washington Post "and "New York Times" supporting military aid to the Contras. The Permanent Commission denies that it received any money which it claims was instead used by others for translating and distributing their monthly reports in other nations.
The Nicaraguan-based magazine "Revista Envio", which describes its stance as one of "critical support for the Sandinistas", refers to the report: "The CPDH: Can It Be Trusted?" written by Scottish lawyer Paul Laverty. In the report, Laverty observes that: "The entire board of directors [of the Permanent Commission], are members of or closely identify with the 'Nicaraguan Democratic Coordinating Committee' (Coordinadora), an alliance of the more rightwing parties and COSEP, the business organization." He goes on to express concern about CPDH's alleged tendency to provide relatively few names and other details in connection with alleged violations. "According to the 11 monthly bulletins of 1987 (July being the only month without an issue), the CPDH claims to have received information on 1,236 abuses of all types. However, of those cases, only 144 names are provided. The majority of those 144 cases give dates and places of alleged incidents, but not all. This means that only in 11.65% of its cases is there the minimal detail provided to identify the person, place, date, incident and perpetrator of the abuse."
On the other hand, the Inter-American Commission on Human Rights states: "During its on-site observation in 1978 under the Government of General Somoza, the Permanent Commission on Human Rights in Nicaragua, (CPDH) gave the Commission notable assistance, which certainly helped it to prepare its report promptly and correctly." and in 1980 "It cannot be denied that the CPDH continues to play an important role in the protection of human rights, and that a good number of people who consider that their human rights have been ignored by the Government are constantly coming to it." The IACHR also continued to meet with representatives of the Permanent Commission and report their assessments in later years.
The Heritage Foundation stated that: "While elements of the Somoza National Guard tortured political opponents, they did not employ psychological torture." The International Commission of Jurists stated that under the Somoza regime cruel physical torture was regularly used in the interrogation of political prisoners.
Throughout the 1980s the Sandinista government was regarded as "Partly Free" by Freedom House.
US government allegations of support for foreign rebels.
The United States State Department accused the Sandinistas of many cases of illegal foreign intervention.
The first allegation supporting the FMLN rebels in El Salvador with safehaven; training; command-and-control headquarters and advice; and weapons, ammunition, and other vital supplies. As evidence was cited captured documents, testimonials of former rebels and Sandinistas, aerial photographs, tracing captured weapons back to Nicaragua, and captured vehicles from Nicaragua smuggling weapons. El Salvador was in the midst of a Civil War in the period in question and the US was heavily supporting El Salvador against the FMLN guerrillas.
There were also accusations of subversive activities in Honduras, Costa Rica, and Colombia and in the case of Honduras and Costa Rica outright military operations by Nicaraguan troops.
Symbols.
The flag of the FSLN consists of an upper half in red, a lower half in black, and the letters F S L N in white. It is a modified version of the flag Sandino used in the 1930s, during the war against the U.S. occupation of Nicaragua which consisted of two vertical stripes, equally in size, one red and the other black with a skull (like the traditional Jolly Roger flag). These colors came from the Mexican anarchist movements that Sandino got involved with during his stay in Mexico in the early 1920s.
In recent times, there has been a dispute between the FSLN and the dissident Sandinista Renovation Movement (MRS) about the use of the red and black flag in public activities. Although the MRS has its own flag (orange with a silhouette of Sandino's hat in black), they also use the red-and-black flag in honor of Sandino's legacy. They state that the red-and-black flag is a symbol of Sandinismo as a whole, not only of the FSLN party.
Popular culture.
Since the conflict with Nicaragua in the 1980s, variations of the term "Sandinista" are now sometimes used in the United States to refer to fanatical supporters of a certain cause. In the Spanish language, the suffix "-ista" is used to indicate a predilection towards the root. (It is the equivalent of "-ist" in English)
As a reaction to an anti-Sandinista statement by British Prime Minister Margaret Thatcher and her proposal to ban the use of the word itself, punk rock group The Clash used the title "Sandinista!" for their 1980 triple album. The album contains the song "Washington Bullets" which references the Sandinistas and other events and groups involved in Latin American history, starting from 1959.
English anarcho-punk band Chumbawamba recorded the song "An Interlude: Beginning To Take It Back" on their album "Pictures of Starving Children Sell Records". The song chronicles the history of the Sandinistas, as well as their conflict with the Contras, and reflects an optimistic hope for the future of Nicaragua.
Chilean new wave group Los Prisioneros mention the Sandinistas in their song ¿Quién mató a Marilyn?, in a passage asking who killed Marilyn Monroe.
In 2007, the popular Puerto Rican Reggaeton–rap band Calle 13 mentioned the Sandinista movement in their song "Llegale a mi guarida". The lyrics claimed: "Respeto a Nicaragua y a la lucha sandinista" ("I respect Nicaragua and the Sandinista struggle").
In an episode of the 1980s American sitcom "The Golden Girls", Blanche, Dorothy, and Rose return home to find Sophia bound, gagged, and tied to a chair. When Dorothy removed the gag and asked who had done this to her, Sophia replied "the Sandinistas!" (It was really a released prisoner named Merrill, who was searching for Blanche.)
The 1983 film "Last Plane Out" about journalist Jack Cox's experiences in Nicaragua portrayed the Sandanistas as crazed communist psychopaths while making Anastasio Somoza Debayle look like a sympathetic hero.
The 2010 video game "" includes a group of FSLN Revolutionaries forced into Costa Rica as an important group of supporting characters, including Amanda. The Anti-Somoza revolution itself figures prominently into the plot of the game as well, being described within the game's narrative as being started by the KGB agent Vladimir Zadornov in order to make Nicaragua a communist state so the Soviet Union could force the United States out of Central America entirely.
Presidents of the Executive.
The party has given the following Presidents of the Republic, namely:
Presidential Candidates.
Won, getting the 67.20% of the valid votes cast 735.067 votes equivalent to well above the second party of the Democratic Conservative Party (PCD ) who just won 154.127 corresponding to a 14.00% of the valid votes.
Lost, as 579.886 A total valid votes equivalent to 40.82%, well below that obtained by the main opposition Mrs. Violeta Barrios de Chamorro candidate of the National Opposition Union (UNO) who won 777.552 to obtain valid votes equivalent to 54.74%.
Lost, as 669,443 A total valid votes equivalent to 37.75%, well below that obtained by his main opponent on Arnoldo Aleman Lacayo candidate of the Liberal Alliance (AL) who won 904.908 to obtain valid votes equivalent to 51.03%.
Lost, as 915,417 A total valid votes equivalent to 42.30%, well below that obtained by the main opposition Enrique Bolaños Geyer candidate Liberal Constitutionalist Party (PLC) who won by getting 1,216,863 valid votes equivalent to 56.30%.
Won, getting the 37.99% of the valid votes cast, 930.802 votes equivalent to relatively higher than the two main opposition parties. They were the party of the Second Nicaraguan Liberal Alliance (ALN) with the degree candidate Eduardo Montealegre Rivas who won 693.391 votes recorded corresponding to a 28.30% and third place went to the Constitutionalist Liberal Party with Dr. José Rizo Castellón who earned a total 664.225 of valid votes corresponding to 27.11%.
Won in National Elections held on November 6, 2011 was the amount of 1,569,287 for 62.46% of the total valid votes, what became the Commander Daniel Ortega Saavedra in the presidential candidate who won more votes in a Presidential election in the history of Nicaragua, in addition to that obtained a lead of more than 30% of valid votes doubling the number of votes obtained by radial businessman Fabio Gadea Mantilla on behalf of the Independent Liberal Party (PLI) who obtained the amount of 778.889 votes recorded for 31.00%. The big loser of these elections was the former President Arnoldo Aleman Lacayo candidate Liberal Constitutionalist Party (PLC) who was located in a distant third with a 5.91% equivalent to 148.507 votes. These results demonstrate that the presidential Ortega was the right which caused the change in the correlation of political forces in the country.
References.
Bibliography.
</dl>

</doc>
<doc id="29318" url="http://en.wikipedia.org/wiki?curid=29318" title="Streptococcus">
Streptococcus

Streptococcus is a genus of coccus (spherical) Gram-positive bacteria belonging to the phylum Firmicutes and the Lactobacillales (lactic acid bacteria) order. Cellular division occurs along a single axis in these bacteria, and thus they grow in chains or pairs, hence the name—from Greek στρεπτός "streptos", meaning easily bent or twisted, like a chain (twisted chain).
Contrast this with "Staphylococci", which divide along multiple axes and generate grape-like clusters of cells. Most "Streptococci" are oxidase- and catalase-negative, and many are facultative anaerobes.
In 1984, many organisms formerly considered "Streptococcus" were separated out into the genera "Enterococcus" and "Lactococcus". Currently, over 50 species are recognised in this genus.
Pathogenesis and classification.
In addition to streptococcal pharyngitis (strep throat), certain "Streptococcus" species are responsible for many cases of pink eye, meningitis, bacterial pneumonia, endocarditis, erysipelas, and necrotizing fasciitis (the 'flesh-eating' bacterial infections). However, many streptococcal species are not pathogenic, and form part of the commensal human microbiota of the mouth, skin, intestine, and upper respiratory tract. Furthermore, streptococci are a necessary ingredient in producing Emmentaler ("Swiss") cheese.
Species of "Streptococcus" are classified based on their hemolytic properties. Alpha-hemolytic species cause oxidization of iron in hemoglobin molecules within red blood cells, giving it a greenish color on blood agar. Beta-hemolytic species cause complete rupture of red blood cells. On blood agar, this appears as wide areas clear of blood cells surrounding bacterial colonies. Gamma-hemolytic species cause no hemolysis.
Beta-hemolytic streptococci are further classified by Lancefield grouping, a serotype classification (that is, describing specific carbohydrates present on the bacterial cell wall). The 20 described serotypes are named Lancefield groups A to V (excluding I and J).
In the medical setting, the most important groups are the alpha-hemolytic streptococci "S. pneumoniae" and "Streptococcus" "viridans "group, and the beta-hemolytic streptococci of Lancefield groups A and B (also known as “group A strep” and “group B strep”).
Alpha-hemolytic.
When alpha hemolysis (α-hemolysis) is present, the agar under the colony is dark and greenish. Streptococcus pneumoniae and a group of oral streptococci (Streptococcus viridans or viridans streptococci) display alpha hemolysis. This is sometimes called green hemolysis because of the color change in the agar. Other synonymous terms are incomplete hemolysis and partial hemolysis. Alpha hemolysis is caused by hydrogen peroxide produced by the bacterium, oxidizing hemoglobin to green methemoglobin.
The viridans group: alpha-hemolytic.
Table: Medically relevant streptococci
Beta-hemolytic.
Beta hemolysis (β-hemolysis), sometimes called complete hemolysis, is a complete lysis of red cells in the media around and under the colonies: the area appears lightened (yellow) and transparent. Streptolysin, an exotoxin, is the enzyme produced by the bacteria which causes the complete lysis of red blood cells. There are two types of streptolysin: Streptolysin O (SLO) and streptolysin S (SLS). Streptolysin O is an oxygen-sensitive cytotoxin, secreted by most Group A streptococcus (GAS), and interacts with cholesterol in the membrane of eukaryotic cells (mainly red and white blood cells, macrophages, and platelets), and usually results in β-hemolysis under the surface of blood agar. Streptolysin S is an oxygen-stable cytotoxin also produced by most GAS strains which results in clearing on the surface of blood agar. SLS affects immune cells, including polymorphonuclear leukocytes and lymphocytes, and is thought to prevent the host immune system from clearing infection. Streptococcus pyogenes, or Group A beta-hemolytic Strep (GAS), displays beta hemolysis.
Some weakly beta-hemolytic species cause intense beta hemolysis when grown together with a strain of Staphylococcus. This is called the CAMP test.[1] Streptococcus agalactiae displays this property. Clostridium perfringens can be identified presumptively with this test. Listeria monocytogenes is also positive on sheep's blood agar.
Group A.
"S. pyogenes", also known as group A "Streptococcus" (GAS), is the causative agent in a wide range of group A streptococcal infections. These infections may be noninvasive or invasive. The noninvasive infections tend to be more common and less severe. The most common of these infections include streptococcal pharyngitis (strep throat) and impetigo. Scarlet fever is also a noninvasive infection, but has not been as common in recent years. 
The invasive infections caused by group A β-hemolytic streptococci tend to be more severe and less common. This occurs when the bacterium is able to infect areas where it is not usually found, such as the blood and the organs. The diseases that may be caused include streptococcal toxic shock syndrome, necrotizing fasciitis, pneumonia, and bacteremia.
Additional complications may be caused by GAS, namely acute rheumatic fever and acute glomerulonephritis. Rheumatic fever, a disease that affects the joints, kidneys, and heart valves, is a consequence of untreated strep A infection caused not by the bacterium itself. Rheumatic fever is caused by the antibodies created by the immune system to fight off the infection cross-reacting with other proteins in the body. This "cross-reaction" causes the body to essentially attack itself and leads to the damage above. Globally, GAS has been estimated to cause more than 500,000 deaths every year, making it one of the world's leading pathogens. Group A "Streptococcus" infection is generally diagnosed with a rapid strep test or by culture.
Group B.
"S. agalactiae", or group B "Streptococcus", GBS, causes pneumonia and meningitis in neonates and the elderly, with occasional systemic bacteremia. They can also colonize the intestines and the female reproductive tract, increasing the risk for premature rupture of membranes during pregnancy, and transmission of the organism to the infant. The American Congress of Obstetricians and Gynecologists (formerly the American College of Obstetricians and Gynecologists), American Academy of Pediatrics, and the Centers for Disease Control recommend all pregnant women between 35 and 37 weeks gestation to be tested for GBS. Women who test positive should be given prophylactic antibiotics during labor, which will usually prevent transmission to the infant.
The United Kingdom has chosen to adopt a risk factor-based protocol, rather than the culture-based protocol followed in the US. Current guidelines state that if one or more of the following risk factors are present, then women should be treated with intrapartum antibiotics:
This protocol results in treatment of 15–20% of pregnant women and prevention of 65–70% of cases of early onset GBS sepsis.
Group C.
This group includes "S. equi", which causes strangles in horses, and "S. zooepidemicus"—"S. equi" is a clonal descendent or biovar of the ancestral "S. zooepidemicus"—which causes infections in several species of mammals, including cattle and horses. "S. dysgalactiae" is also a member of group C, β-haemolytic streptococci that can cause pharyngitis and other pyogenic infections similar to group A streptococci.
Group D (enterococci).
Many former group D streptococci have been reclassified and placed in the genus "Enterococcus" (including "E. faecalis", "E. faecium", "E. durans", and "E. avium"). For example, "Streptococcus faecalis" is now "Enterococcus faecalis". "E. faecalis" is sometimes alpha hemolytic and "E. faecium" is sometimes beta hemolytic.
The remaining nonenterococcal group D strains include "Streptococcus bovis" and "Streptococcus equinus".
Nonhemolytic streptococci rarely cause illness. However, weakly hemolytic group D beta-hemolytic streptococci and "Listeria monocytogenes" (which is actually a Gram-positive bacillus) should not be confused with nonhemolytic streptococci.
Group F streptococci.
Group F streptococci were first described in 1934 by Long and Bliss amongst the "minute haemolytic streptococci". They are also known as "Streptococcus anginosus" (according to the Lancefield classification system) or as members of the "S. milleri" group (according to the European system).
Group G streptococci.
These streptococci are usually, but not exclusively, beta-hemolytic. "S. canis" is an example of a GGS which is typically found on animals, but can cause infection in humans.
Group H streptococci.
Group H streptococci cause infections in medium-sized canines. Group H streptococci rarely cause illness unless a human has direct contact with the mouth of a canine. One of the most common ways this can be spread is human-to-canine, mouth-to-mouth contact. However, the canine may lick the human's hand and infection can be spread, as well.
Molecular taxonomy and phylogenetics.
Streptococci have been divided into six groups on the basis of their 16S rDNA sequences: "S. anginosus, S.bovis, S. mitis, S. mutans, S. pyogenes" and "S. salivarius". The 16S groups have been confirmed by whole genome sequencing (see figure). The important pathogens "S. pneumoniae" and "S. pyogenes" belong to the "S. mitis" and "S. pyogenes" groups, respectively, while the causative agent of dental caries, "Streptococcus mutans", is basal to the "Streptococcus" group.
Genomics.
The genomes of hundreds of species have been sequenced. Most "Streptococcus" genomes are 1.8 to 2.3 Mb in size and encode 1,700 to 2,300 proteins. Some important genomes are listed in the table. The four species shown in the table ("S. pyogenes, S. agalactiae, S. pneumoniae", and "S. mutans") have an average pairwise protein sequence identity of about 70%.

</doc>
<doc id="29322" url="http://en.wikipedia.org/wiki?curid=29322" title="SignWriting">
SignWriting

SignWriting is a system of writing sign languages. It is highly featural and visually iconic, both in the shapes of the characters, which are abstract pictures of the hands, face, and body, and in their spatial arrangement on the page, which does not follow a sequential order like the letters that make up written English words. It was developed in 1974 by Valerie Sutton, a dancer who had two years earlier developed DanceWriting.
History.
As Sutton was teaching DanceWriting to the Royal Danish Ballet, Lars von der Lieth, who was doing research on sign language at the University of Copenhagen, thought it would be useful to use a similar notation for the recording of sign languages. Sutton based SignWriting on DanceWriting, and finally expanded the system to the complete repertoire of MovementWriting. However, only SignWriting and DanceWriting have been widely used.
Although not the first writing system for sign languages (see Stokoe notation), SignWriting is the first to adequately represent facial expressions and shifts in posture, and to accommodate representation of series of signs longer than compound words and short phrases. It is the only system in regular use, used for example to publish college newsletters in American Sign Language, and has been used for captioning of YouTube videos. Sutton notes that SignWriting has been used or investigated in over 40 countries on every inhabited continent. However, it is not clear how widespread its use is in each country.
In Brazil, during the FENEIS (National Association of the Deaf) annual meeting in 2001, the association voted to accept SignWriting as the preferred method of transcribing Lingua Brasileira de Sinais (Libras) into a written form. The strong recommendation to the Brazilian government from that association was that SignWriting be taught in all Deaf schools. Currently SignWriting is taught on an academic level at the University Federal de Santa Catarina (UFSC) as part of its Brazilian Sign Language curriculum. SignWriting is also being used in the recently published Brazilian Sign Language Dictionary containing more than 3,600 signs used by the Deaf of São Paulo, published by the University of São Paulo under the direction of Prof. Fernando Capovilla (EJ669813 – Brazilian Sign Language Lexicography and Technology: Dictionary, Digital Encyclopedia, Chereme-based Sign Retrieval, and Quadriplegic Deaf Communication Systems. Abstracted from Educational Resources Information Center).
Some initial studies found that Deaf communities prefer video or writing systems for the dominant language, however this claim has been disputed by the work of Steve and Dianne Parkhurst in Spain where they found initial resistance, later renewed interest, and finally pride. "If Deaf people learn to read and write in their own signing system, that increases their self-esteem", says Dianne Parkhurst.
Probably the most exhaustive study of the application of SignWriting to a specific sign language is Maria Galea's work on using it to write Maltese Sign Language.
Symbols.
In SignWriting, a combination of iconic symbols for handshapes, orientation, body locations, facial expressions, contacts, and movement are used to represent words in sign language. Since SignWriting, as a featural script, represents the actual physical formation of signs rather than their meaning, no phonemic or semantic analysis of a language is required to write it. A person who has learned the system can "feel out" an unfamiliar sign in the same way an English speaking person can "sound out" an unfamiliar word written in the Latin alphabet, without even needing to know what the sign means.
The number of symbols is extensive and often provides multiple ways to write a single sign. Just as it took many centuries for English spelling to become standardized, spelling in SignWriting is not yet standardized for any sign language.
Words may be written from the point of view of the signer or the viewer. However, almost all publications use the point of view of the signer, and assume the right hand is dominant. Sutton originally designed the script to be written horizontally (left-to-right), like English, and from the point of view of the observer, but later changed it to vertical (top-to-bottom) and from the point of view of the signer, to conform to the wishes of Deaf writers.
Orientation.
The orientation of the palm is indicated by filling in the glyph for the hand shape. A hollow outline (white) glyph indicates that one is facing the palm of the hand, a filled (black) glyph indicates that one is facing the back of the hand, and split shading indicates that one is seeing the hand from the side. Although in reality the wrist may turn to intermediate positions, only the four orientations of palm, back, and either side are represented in SignWriting, as they are enough to represent sign languages.
If an unbroken glyph is used, then the hand is placed in the vertical (wall or face) plane in front of the signer, as occurs when finger spelling. A band erased across the glyph through the knuckles shows that the hand lies in the horizontal plane, parallel to the floor. (If one of the basic hand-shape glyphs is used, such as the simple square or circle, this band breaks it in two; however, if there are lines for fingers extended from the base, then they become detached from the base, but the base itself remains intact.)
The diagram to the left shows a BA-hand (flat hand) in six orientations. For the three vertical orientations on the left side, the hand is held in front of the signer, fingers pointing upward. All three glyphs can be rotated, like the hands of a clock, to show the fingers pointing at an angle, to the side, or downward. For the three horizontal orientations on the right side of the diagram, the hand is held outward, with the fingers pointing away from the signer, and presumably toward the viewer. They can also be rotated to show the fingers pointing to the side or toward the signer. Although an indefinite number of orientations can be represented this way, in practice only eight are used for each plane—that is, only multiples of 45° are found.
Hand shapes.
There are over a hundred glyphs for hand shapes, but all the ones used in ASL are based on five basic elements:
A line halfway across the square or pentagon shows the thumb across the palm. These are the E, B, and (with spread fingers) 4 hands of fingerspelling.
These basic shapes are modified with lines jutting from their faces and corners to represent fingers that are not positioned as described above. Straight lines represent straight fingers (these may be at an angle to indicate that they are not in line with the palm; if they point toward or away from the signer, they have a diamond shape at the tip); curved lines for curved (cupped) fingers; hooked lines for hooked fingers; right-angle lines, for fingers bent at only one joint; and crossed lines, for crossed fingers, as shown in the chart at right. The pentagon and C are only modified to show that the fingers are spread rather than in contact; the angle is only modified to show whether the thumb touches the finger tips or juts out to the side. Although there are some generalizations which can be made for the dozens of other glyphs, which are based on the circle and square, the details are somewhat idiosyncratic and each needs to be memorized.
For the top sign, the arrows show that the two '1' hands move in vertical circles, and that although they move at the same time (tie bar), the left hand (hollow arrowhead) starts away from the body (thin line) going up while the right hand (solid arrowhead) starts near the body (thick line) going down.
With the bottom sign, the right 'X' palm-down hand moves down-side-down relative to the stationary palm-up 'B' hand. This is overly exact: The ASL sign will work with any downward zigzag motion, and the direction and starting point of the circles is irrelevant.
Finger movement.
There are only a few symbols for finger movement. They may be doubled to show that the movement is repeated.
A solid bullet represents flexing the middle joint of a finger or fingers, and a hollow bullet represents straightening a flexed finger. That is, a 'D' hand with a solid bullet means that it becomes an 'X' hand, while an 'X' hand with a hollow bullet means that it becomes a 'D' hand. If the fingers are already flexed, then a solid bullet shows that they squeeze. For example, a square (closed fist, 'S' hand) with double solid bullets is the sign for 'milk' (iconically squeezing an udder).
A downward-pointing chevron represents flexing at the knuckles, while an upward-pointing chevron (^) shows that the knuckles straighten. That is, a 'U' hand with a down chevron becomes an 'N' hand, while and 'N' hand with an up chevron becomes a 'U' hand.
A zigzag like two chevrons (^^) joined together means that the fingers flex repeatedly and in sync. A double-line zigzag means that the fingers wriggle or flutter out of sync.
Hand movement.
Hundreds of arrows of various sorts are used to indicate movement of the hands through space. Movement notation gets quite complex, and because it is more exact than it needs to be for any one sign language, different people may choose to write the same sign in different ways.
For movement with the left hand, the Δ-shaped arrowhead is hollow (white); for movement with the right hand, it is solid (black). When both hands move as one, an open (Λ-shaped) arrowhead is used.
As with orientation, movement arrows distinguish two planes: Movement in the vertical plane (up & down) is represented by arrows with double stems, as at the bottom of the diagram at left, while single-stemmed arrows represent movement parallel to the floor (to & fro). In addition, movement in a diagonal plane uses modified double-stemmed arrows: A cross bar on the stem indicates that the motion is away as well up or down, and a solid dot indicates approaching motion. To & fro movement that also goes over or under something uses modified single-stemmed arrows, with the part of the arrow representing near motion thicker than the rest. These are iconic, but conventionalized, and so need to be learned individually.
Straight movements are in one of eight directions for either plane, as in the eight principal directions of a compass. A long straight arrow indicates movement from the elbow, a short arrow with a cross bar behind it indicates motion from the wrist, and a simple short arrow indicates a small movement. (Doubled, in opposite directions, these can show nodding from the wrist.) A secondary curved arrow crossing the main arrow shows that the arm twists while it moves. (Doubled, in opposite directions, these can show shaking of the hand.) Arrows can turn, curve, zigzag, and loop-the-loop.
Shoulder, head, and eye movement.
Arrows on the face at the eyes show the direction of gaze.
Contact.
Six contact glyphs show hand contact with the location of the sign. That is, a handshape glyph located at the side of the face, together with a contact glyph, indicates that the hand touches the side of the face. The choice of the contact glyph indicates the manner of the contact:
Location.
If the signing hand is located at the other hand, the symbol for it is one of the hand shapes above. In practice, only a subset of the more simple hand shapes occurs.
Additional symbols are used to represent sign locations at the face or body parts other than the hands. A circle shows the head.
Expression.
There are symbols to represent facial movements that are used in various sign languages, including eyes, eyebrows, nose movements, cheeks, mouth movements, and breathing changes. The direction of head movement and eyegaze can also be shown.
Body movement.
Shoulders are shown with a horizontal line. Small arrows can be added to show shoulder and torso movement. Arms and even legs can be added if necessary.
Prosody.
There are also symbols that indicate speed of movement, whether movement is simultaneous or alternating, and punctuation.
Punctuation.
Various punctuation symbols exist that correspond to commas, periods, question and exclamation marks, and other punctuation symbols of other scripts. These are written between signs, and lines do not break between a sign and its following punctuation symbol.
Arrangement of symbols.
One of the unusual characteristics of SignWriting is its use of two-dimensional layout within an invisible 'sign box'. The relative positions of the symbols within the box iconically represent the locations of the hands and other parts of the body involved in the sign being represented. As such, there is no obvious linear relationship between the symbols within each sign box, unlike the sequence of characters within each word in most scripts for spoken languages. This is also unlike other sign language scripts which arrange symbols linearly as in spoken languages. However, since in sign languages many phonetic parameters are articulated simultaneously, these other scripts require arbitrary conventions for specifying the order of different parameters of handshape, location, motion, etc. Although SignWriting does have conventions for how symbols are to be arranged relative to each other within a sign, the two-dimensional layout results in less arbitrariness and more iconicity than other sign language scripts.
Outside of each sign, however, the script is linear, reflecting the temporal order of signs. Signs are most commonly now written in vertical columns (although formerly they were written horizontally). Sign boxes are arranged from top to bottom within the column, interspersed with punctuation symbols, and the columns progress left to right across the page. Within a column, signs may be written down the center or shifted left or right in 'lanes' to indicate side-to-side shifts of the body.
Sequencing of signs in dictionaries.
Sutton orders signs in ten groups based on which fingers are extended on the dominant hand. These are equivalent to the numerals one through ten in ASL. Each group is then subdivided according to the actual hand shape, and then subdivided again according to the plane the hand is in (vertical, then horizontal), then again according to the basic orientation of the hand (palm, side, back). An ordering system has been proposed using this beginning and examples from both American Sign Language and Brazilian Sign Language (LIBRAS). The current system of ordering for SignWriting is called the Sign Symbol Sequence which is parsed by the creator of each sign as recorded into the on-line dictionary. This system allows for internal ordering by features including handshape, orientation, speed, location, and other clustered features not found in spoken dictionaries.
Advantages and disadvantages.
Some of the advantages of SignWriting, compared to other writing systems for sign languages, are:
However, it has a few disadvantages as well.
See also the discussion below about Unicode.
SignWriting in Unicode.
SignWriting is the first writing system for sign languages to have been proposed for inclusion in Unicode. A proposal to encode the symbol set in Unicode is currently under consideration. This proposal is based on SignWriting's standardized symbol set and defined character encoding model.
The proposal, however, only covers the symbol set. It does not address layout, the positioning of the symbols in two dimensions. The lack of an obvious linear relationship between the symbols comprising a sign means that it is not clear how to encode the layout of symbols in Unicode. Current custom software does this by recording Cartesian (X-Y) coordinates for each symbol. As yet there is no agreement in the SignWriting community as to whether this approach should be followed in a Unicode encoding, since it allows many arbitrary ways to represent a sign, differing only in small differences of the coordinates assigned to each symbol. Alternative approaches have not been worked out in enough detail to determine if they would work as the basis for an encoding.
Accessibility.
Sutton has released the International SignWriting Alphabet 2010 under the SIL Open Font License. Steve Slevinski has released the SignWriting MediaWiki Plugin for viewing SignWriting within MediaWiki. The extension is released under the GNU General Public License.

</doc>
<doc id="29323" url="http://en.wikipedia.org/wiki?curid=29323" title="Suez Canal">
Suez Canal

The Suez Canal (Arabic: قناة السويس‎ "Qanāt al-Suwais") is an artificial sea-level waterway in Egypt, connecting the Mediterranean Sea and the Red Sea. Opened November 17, 1869 after 10 years of construction, it allows ships to travel between Europe and South Asia without navigating around Africa thereby reducing the sea voyage distance between Europe and India by about 7000 km. The northern terminus is Port Said; the southern terminus is Port Tewfik at the city of Suez. Ismailia is on its west bank, 3 km from the half-way point. In 2012, 17,225 vessels traversed the canal (47 per day).
When built, the canal was 164 km long and 8 m deep. After several enlargements, it is 193.30 km long, 24 m deep and 205 m wide. It consists of the northern access channel of 22 km, the canal itself of 162.25 km and the southern access channel of 9 km.
The canal is single lane with passing places in the "Ballah By-Pass" and the Great Bitter Lake. It contains no locks; seawater flows freely through it. In general, the canal north of the Bitter Lakes flows north in winter and south in summer. The current south of the lakes changes with the tide at Suez.
The canal is owned and maintained by the Suez Canal Authority (SCA) of Egypt. Under the Convention of Constantinople, it may be used "in time of war as in time of peace, by every vessel of commerce or of war, without distinction of flag."
In August 2014, construction was launched to construct a second canal, the New Suez Canal, for half of the route of the canal, costing $8.4 billion, to increase the canal's capacity. Funding was arranged by issuing interest-bearing investment certificates exclusively to Egyptian entities and individuals and the target amount was collected over only six working days. The expansion is expected to double the capacity of the Suez Canal from 49 to 97 ships a day. Construction of the project is expected to take a year.
History.
Nile–Red Sea Canal(s).
Ancient west–east canals were built to facilitate travel from the Nile river to the Red Sea. One smaller canal is believed to have been constructed under the auspices of Senusret II or Ramesses II. Another canal, probably incorporating a portion of the first, was constructed under the reign of Necho II, but the only fully functional canal was engineered and completed by Darius I.
2nd millennium BC.
The legendary Sesostris (likely either Pharaoh Senusret II or Senusret III of the Twelfth dynasty of Egypt) is suggested to have perhaps started work on an ancient canal joining the Nile with the Red Sea (1897 BC–1839 BC). (It is said that in ancient times the Red Sea reached northward to the Bitter Lakes and Lake Timsah.)
In his "Meteorology", Aristotle wrote:
One of their kings tried to make a canal to it (for it would have been of no little advantage to them for the whole region to have become navigable; Sesostris is said to have been the first of the ancient kings to try), but he found that the sea was higher than the land. So he first, and Darius afterwards, stopped making the canal, lest the sea should mix with the river water and spoil it.
Strabo wrote that Sesostris started to build a canal, and Pliny the Elder wrote:
165. Next comes the Tyro tribe and, on the Red Sea, the harbour of the Daneoi, from which Sesostris, king of Egypt, intended to carry a ship-canal to where the Nile flows into what is known as the Delta; this is a distance of over 60 miles. Later the Persian king Darius had the same idea, and yet again Ptolemy II, who made a trench 100 feet wide, 30 feet deep and about 35 miles long, as far as the Bitter Lakes.
In the second half of the 19th century, French cartographers discovered the remnants of an ancient north–south canal past the east side of Lake Timsah and ending near the north end of the Great Bitter Lake. This proved to be the celebrated canal made by the Persian king Darius I, as his stele commemorating its construction was found at the site. (This ancient, second canal may have followed a course along the shoreline of the Red Sea when it once extended north to Lake Timsah.) In the 20th century the northward extension of this ancient canal was discovered, extending from Lake Timsah to the Ballah Lakes. This was dated to the Middle Kingdom of Egypt by extrapolating the dates of ancient sites along its course.
The reliefs of the Punt expedition under Hatshepsut 1470 BC depict seagoing vessels carrying the expeditionary force returning from Punt. This has given rise to the suggestion that a navigable link existed between the Red Sea and the Nile. Evidence seems to indicate its existence by the 13th century BC during the time of Ramesses II.
Canals dug by Necho, Darius I and Ptolemy.
Remnants of an ancient west–east canal through the ancient Egyptian cities of Bubastis, Pi-Ramesses, and Pithom were discovered by Napoleon Bonaparte and his engineers and cartographers in 1799.
According to the "Histories" of the Greek historian Herodotus, about 600 BC, Necho II undertook to dig a west–east canal through the Wadi Tumilat between Bubastis and Heroopolis, and perhaps continued it to the Heroopolite Gulf and the Red Sea. Regardless, Necho is reported as having never completed his project.
Herodotus was told that 120,000 men perished in this undertaking, but this figure is doubtlessly exaggerated. According to Pliny the Elder, Necho's extension to the canal was about 57 English miles, equal to the total distance between Bubastis and the Great Bitter Lake, allowing for winding through valleys that it had to pass through. The length that Herodotus tells us, of over 1000 stadia (i.e., over 114 miles), must be understood to include the entire distance between the Nile and the Red Sea at that time.
With Necho's death, work was discontinued. Herodotus tells us that the reason the project was abandoned was because of a warning received from an oracle that others would benefit by its successful completion. In fact, Necho's war with Nebuchadnezzar II most probably prevented the canal's continuation.
Necho's project was completed by Darius I of Persia, who ruled over Ancient Egypt after it had been conquered by his predecessor Cambyses II. We are told that by Darius's time a natural waterway passage which had existed between the Heroopolite Gulf and the Red Sea in the vicinity of the Egyptian town of Shaluf (alt. "Chalouf" or "Shaloof"), located just south of the Great Bitter Lake, had become so blocked with silt that Darius needed to clear it out so as to allow navigation once again. According to Herodotus, Darius's canal was wide enough that two triremes could pass each other with oars extended, and required four days to traverse. Darius commemorated his achievement with a number of granite stelae that he set up on the Nile bank, including one near Kabret, and a further one a few miles north of Suez. The Darius Inscriptions read:
Saith King Darius: I am a Persian. Setting out from Persia, I conquered Egypt. I ordered this canal dug from the river called the Nile that flows in Egypt, to the sea that begins in Persia. When the canal had been dug as I ordered, ships went from Egypt through this canal to Persia, even as I intended.—Darius Inscription
The canal left the Nile at Bubastis. An inscription on a pillar at Pithom records that in 270 or 269 BC it was again reopened, by Ptolemy II Philadelphus. In Arsinoe, Ptolemy constructed a navigable lock, with sluices, at the Heroopolite Gulf of the Red Sea, which allowed the passage of vessels but prevented salt water from the Red Sea from mingling with the fresh water in the canal.
Receding Red Sea and the dwindling Nile.
The Red Sea is believed by some historians to have gradually receded over the centuries, its coastline slowly moving southward away from Lake Timsah and the Great Bitter Lake. Coupled with persistent accumulations of Nile silt, maintenance and repair of Ptolemy's canal became increasingly cumbersome over each passing century.
Two hundred years after the construction of Ptolemy's canal, Cleopatra seems to have had no west–east waterway passage, because the Pelusiac branch of the Nile, which had fed Ptolemy's west–east canal, had by that time dwindled, being choked with silt.
Old Cairo to the Red Sea.
By the 8th century, a navigable canal existed between Old Cairo and the Red Sea, but accounts vary as to who ordered its construction—either Trajan or 'Amr ibn al-'As, or Omar the Great. This canal reportedly linked to the River Nile at Old Cairo and ended near modern Suez. A geography treatise by Dicuil reports a conversation with an English monk, Fidelis, who had sailed on the canal from the Nile to the Red Sea during a pilgrimage to the Holy Land in the first half of the 8th century
The Abbasid Caliph al-Mansur is said to have ordered this canal closed in 767 to prevent supplies from reaching Arabian detractors.
Repair by Tāriqu l-Ḥākim.
Al-Hakim bi-Amr Allah is claimed to have repaired the Cairo to Red Sea passageway, but only briefly, circa 1000 AD, as it soon "became choked with sand." However, we are told that parts of this canal still continued to fill in during the Nile's annual inundations.
Conception by Venice.
The successful 1488 navigation of southern Africa by Bartolomeu Dias opened a direct maritime trading route to India and the spice islands, and forever changed the balance of Mediterranean trade. One of the most prominent losers in the new order, as former middlemen, was the former spice trading center of Venice.
Napoleon discovers an ancient canal.
Napoleon Bonaparte's interest in finding the remnants of an ancient waterway passage culminated in a cadre of archaeologists, scientists, cartographers and engineers scouring the area beginning in the latter months of 1798. Their findings, recorded in the "Description de l'Égypte", include detailed maps that depict the discovery of an ancient canal extending northward from the Red Sea and then westward toward the Nile.
Napoleon had contemplated the construction of a north–south canal to join the Mediterranean and Red Sea, but this was abandoned after the preliminary survey erroneously concluded that the Red Sea was 10 m higher than the Mediterranean, requiring locks that were too expensive and very long to construct. The error came from fragmented readings mostly done during wartime, which resulted in imprecise calculations.
Though by this time unnavigable, the ancient route from Bubastis to the Red Sea still channeled water in spots as late as 1861 and as far east as Kassassin.
Mediterranean–Red Sea Canal.
Interim period.
Although the alleged difference in sea levels could be problematic for construction, the idea of finding a shorter route to the east remained alive. In 1830, F. R. Chesney submitted a report to the British government that stated that there was no difference in altitude and that the Suez Canal was feasible, but his report received no further attention. Lieutenant Waghorn established his 'Overland Route', which transported post and passengers to India via Egypt. Linant de Bellefonds, a French explorer of Egypt, became chief engineer of Egypt's Public Works. In addition to his normal duties, he surveyed the Isthmus of Suez and made plans for the Suez Canal. French Saint-Simonianists showed an interest in the canal and in 1833, Barthélemy Prosper Enfantin tried to draw Muhammad Ali's attention to the canal but was unsuccessful. Alois Negrelli, the Austrian railroad pioneer, became interested in the idea in 1836. In 1846, Prosper Enfantin's Société d'Études du Canal de Suez invited a number of experts, among them Robert Stephenson, Negrelli and Paul-Adrien Bourdaloue to study the feasibility of the Suez Canal (with the assistance of Linant de Bellefonds). Bourdaloue's survey of the isthmus was the first generally accepted evidence that there was no practical difference in altitude between the two seas. Britain, however, feared that a canal open to everyone might interfere with its India trade and therefore preferred a connection by train from Alexandria via Cairo to Suez, which was eventually built by Stephenson.
Construction by Suez Canal Company.
In 1854 and 1856 Ferdinand de Lesseps obtained a concession from Sa'id Pasha, the Khedive of Egypt and Sudan, to create a company to construct a canal open to ships of all nations. The company was to operate the canal for 99 years from its opening. De Lesseps had used his friendly relationship with Sa'id, which he had developed while he was a French diplomat in the 1830s. As stipulated in the concessions, de Lesseps convened the International Commission for the piercing of the isthmus of Suez ("Commission Internationale pour le percement de l'isthme des Suez") consisting of 13 experts from seven countries, among them John Robinson McClean, later President of the Institution of Civil Engineers in London, and again Negrelli, to examine the plans developed by Linant de Bellefonds, and to advise on the feasibility of and the best route for the canal. After surveys and analyses in Egypt and discussions in Paris on various aspects of the canal, where many of Negrelli's ideas prevailed, the commission produced a unanimous report in December 1856 containing a detailed description of the canal complete with plans and profiles. The Suez Canal Company ("Compagnie universelle du canal maritime de Suez") came into being on 15 December 1858 and work started on the shore of the future Port Said on 25 April 1859.
The excavation took some 10 years using forced labor (corvée) of Egyptian workers. Some sources estimate that over 30,000 people were working on the canal at any given period, that more than 1.5 million people from various countries were employed, and that thousands of laborers died.
The British government had opposed the project from the outset to its completion. As one of the diplomatic moves against the canal, it disapproved of the use of "slave labor" of forced workers. The British Empire was the major global naval force and officially condemned the forced work and sent armed Bedouins to start a revolt among workers. Involuntary labor on the project ceased, and the viceroy condemned the corvée, halting the project.
Angered by the British opportunism, de Lesseps sent a letter to the British government remarking on the British lack of remorse a few years earlier when forced workers died in similar conditions building the British railway in Egypt.
Initially international opinion was sceptical and Suez Canal Company shares did not sell well overseas. Britain, the United States, Austria, and Russia did not buy a significant number of shares. All French shares were quickly sold in France. A contemporary British sceptic claimed:
<br>
The canal opened under French control on 17 November 1869. Although numerous technical, political, and financial problems had been overcome, the final cost was more than double the original estimate. The opening was performed by Khedive Isma'il Pasha of Egypt and Sudan, and at Ismail's invitation French Empress Eugenie in the Imperial yacht "Aigle" piloted by Napoléon Coste, who was bestowed by the Khedive the Ottoman Order of the Medjidie. The first ship through the canal was the British P&O liner "Delta". Although "L'Aigle" was officially the first vessel through the canal, HMS "Newport", captained by George Nares, passed through it first. On the night before the canal was due to open, Captain Nares navigated his vessel, in total darkness and without lights, through the mass of waiting ships until it was in front of "L'Aigle". When dawn broke the French were horrified to find that the Royal Navy was first in line and that it would be impossible to pass them. Nares received both an official reprimand and an unofficial vote of thanks from the Admiralty for his actions in promoting British interests and for demonstrating such superb seamanship.
After the opening the Suez Canal Company was in financial difficulties. The remaining works were completed only in 1871, and traffic was below expectations in the first two years. De Lesseps therefore tried to increase revenues by interpreting the kind of net ton referred to in the second concession ("tonneau de capacité") as meaning a ship's cargo capacity and not only the theoretical net tonnage of the "Moorsom System" introduced in Britain by the Merchant Shipping Act in 1854. The ensuing commercial and diplomatic activities resulted in the International Commission of Constantinople establishing a specific kind of net tonnage and settling the question of tariffs in its protocol of 18 December 1873. This was the origin of the Suez Canal Net Tonnage and the Suez Canal Special Tonnage Certificate still used.
The canal had an immediate and dramatic effect on world trade. Combined with the American transcontinental railroad completed six months earlier, it allowed the world to be circled in record time. It played an important role in increasing European colonisation of Africa. The construction of the canal was one of the reasons for the Panic of 1873, because goods from the Far East were carried in sailing vessels around the Cape of Good Hope and were stored in British warehouses. As sailing vessels were not adaptable for use through the canal, because the prevailing winds of the Mediterranean blow from west to east, British entrepôt trade suffered. External debts forced Said Pasha's successor, Isma'il Pasha, to sell his country's share in the canal for £4,000,000 (about £<br>{Inflation} - Amount must not have "" prefix: 4.   million in 2014) to the United Kingdom in 1875, but French shareholders still held the majority. Prime Minister Benjamin Disraeli was accused by William Ewart Gladstone of undermining Britain's constitutional system, because he had not referred to, or obtained consent from Parliament when purchasing the shares with funding from the Rothschilds.
The Convention of Constantinople in 1888 declared the canal a neutral zone under the protection of the British, who had occupied Egypt and Sudan at the request of Khedive Tewfiq to suppress the Urabi Revolt against his rule. The revolt went on from 1879 to 1882. As a result of British involvement on the side of Khedive Tewfiq, Britain gained control of the canal in 1882. The British defended the strategically important passage against a major Ottoman attack in 1915, during the First World War. Under the Anglo-Egyptian Treaty of 1936, the UK retained control over the canal. In 1951 Egypt repudiated the treaty, and in October 1954 the UK agreed to remove its troops. Withdrawal was completed on 18 July 1956.
Suez Crisis.
Because of Egyptian overtures towards the Soviet Union, the United Kingdom and the United States withdrew their pledge to support the construction of the Aswan Dam. Egyptian President Gamal Abdel Nasser responded by nationalizing the canal in 1956 and transferring it to the Suez Canal Authority, intending to finance the dam project using revenue from the canal. This led to the Suez Crisis, known in the Arab World as the "Tripartite Aggression", in which the UK, France, and Israel invaded Egypt. According to the pre-agreed war plans under the Protocol of Sèvres, the Israelis invaded the Sinai Peninsula, forcing Egypt to engage them militarily, and allowing the Anglo-French partnership to declare the resultant fighting a threat to the canal and enter the war on Israel's side.
To save the British from what he thought was a disastrous action and to stop the war from a possible escalation, Canadian Secretary of State for External Affairs Lester B. Pearson proposed the creation of the first United Nations peacekeeping force to ensure access to the canal for all and an Israeli withdrawal from the Sinai Peninsula. On 4 November 1956, a majority at the United Nations voted for Pearson's peacekeeping resolution, which mandated the UN peacekeepers to stay in Sinai unless both Egypt and Israel agreed to their withdrawal. The United States backed this proposal by putting pressure on the British government through the selling of sterling, which would cause it to depreciate. Britain then agreed to withdraw its troops. Pearson was later awarded the Nobel Peace Prize. As a result of damage and ships sunk under orders from Nasser the canal was closed until April 1957, when it was cleared with UN assistance. A UN force (UNEF) was established to maintain the free navigability of the canal, and peace in the Sinai Peninsula.
According to the historian Abd aI-Azim Ramadan, Nasser's decision to nationalize the Suez Canal was his alone, made without political or military consultation. The events leading up to the nationalization of the Suez Canal Company, as other events during Nasser’s rule, showed Nasser’s inclination to solitary decision making. He considers Nasser to be far from a rational, responsible leader.
Arab–Israeli wars of 1967 and 1973.
In May 1967, president Gamal Abdel Nasser ordered the UN peacekeeping forces out of Sinai, including the Suez Canal area. Israel objected to the closing of the Straits of Tiran to Israeli shipping. The canal had been closed to Israeli shipping since 1949, except for a short period in 1951–1952.
After the 1967 Six Day War the canal was closed by an Egyptian blockade until 5 June 1975. As a result, 15 cargo ships known as the "Yellow Fleet" were trapped in the canal for over eight years.
In 1973, during the Yom Kippur War, the canal was the scene of a major crossing by the Egyptian army into Israeli-occupied Sinai and a counter-crossing by the Israeli army to Egypt. Much wreckage from this conflict remains visible along the canal's edges.
After the Yom Kippur War the United States initiated Operation Nimbus Moon. The amphibious assault ship USS "Iwo Jima" was sent to the Canal, carrying 12 RH-53D minesweeping helicopters of HM-12. These partly cleared the canal between May and December 1974. She was relieved by the LST USS "Barnstable County" (LST1197). The British Royal Navy initiated Operation Rheostat and Task Group 65.2 provided the minehunters HMS "Maxton", HMS "Bossington" and HMS "Wilton", and HMS "Abdiel", a practice minelayer/MCMV support ship that spent two periods of 6 months in 1974 and in 1975 based at Ismailia. When the Canal Clearance Operations were completed, the canal and its lakes were considered 99% clear of mines. The canal was then reopened by Egyptian President Anwar Sadat aboard an Egyptian destroyer, which led the first convoy northbound to Port Said in 1975.
The UNEF mandate expired in 1979. Despite the efforts of the United States, Israel, Egypt, and others to obtain an extension of the UN role in observing the peace between Israel and Egypt, as called for under the Egypt–Israel Peace Treaty of 1979, the mandate could not be extended because of the veto by the Soviet Union in the UN Security Council, at the request of Syria. Accordingly, negotiations for a new observer force in the Sinai produced the Multinational Force and Observers (MFO), stationed in Sinai in 1981 in coordination with a phased Israeli withdrawal. It is there under agreements between the United States, Israel, Egypt, and other nations.
Canal layout and operation.
Capacity.
The canal allows passage of ships up to 20 m draft or 240,000 deadweight tons and up to a height of 68 m above water level and a maximum beam of 77.5 m under certain conditions. The canal can handle more traffic and larger ships than the Panama Canal. Some supertankers are too large to traverse the canal. Others can offload part of their cargo onto a canal-owned boat to reduce their draft, transit, and reload at the other end of the canal.
Navigation.
The canal has no locks because of the flat terrain, and the minor sea level difference between each end is inconsequential for shipping. As the canal has no sea surge gates, the ports at the ends would be subject to the sudden impact of tsunamis from the Mediterranean Sea and Red Sea, according to a 2012 article in the "Journal of Coastal Research".
There is one shipping lane with passing areas in Ballah-Bypass near El Qantara and in the Great Bitter Lake.
On a typical day, three convoys transit the canal, two southbound and one northbound. The passage takes between 11 and 16 hours at a speed of around 8 kn. The low speed helps prevent erosion of the banks by ships' wakes.
By 1955 about two-thirds of Europe's oil passed through the canal. Around 8% of world sea trade is carried via the canal. In 2008 21,415 vessels passed through the canal and the receipts totaled $5.381 billion, with the average cost per ship of $251,000.
New Rules of Navigation came into force on 1 January 2008, passed by the board of directors of the Suez Canal Authority (SCA) to organise vessels' transit. The most important amendments include allowing vessels with 62 ft draught to pass, increasing the allowed breadth from 32 m to 40 m (following improvement operations), and imposing a fine on vessels using divers from outside the SCA inside the canal boundaries without permission. The amendments allow vessels loaded with dangerous cargo (such as radioactive or flammable materials) to pass if they conform with the latest amendments provided by international conventions.
The SCA has the right to determine the number of tugs required to assist warships traversing the canal, to achieve the highest degree of safety during transit.
Operation.
The canal is too narrow for free two-way traffic, so ships pass in convoys and they use bypasses. The by-passes are 78 km out of 193 km (40%). From north to south, they are: Port Said by-pass (entrances) 36.5 km, Ballah by-pass & anchorage, 9 km, Timsah by-pass 5 km, and the Deversoir by-pass (northern end of the Great Bitter Lake) 27.5 km. The by-passes were completed in 1980.
Typically it takes a ship 12 to 16 hours to transit the canal. The canal's 24-hour capacity is about 76 standard ships.
In August 2014, Egypt chose a consortium that includes the Egyptian army and global engineering firm Dar al-Handasah to develop an international industrial and logistics hub in the Suez Canal area, and began the construction of a new Canal-Section combined with expansion and deep digging of the rest of the canal. This will allow navigation in both directions simultaneously. It is anticipated that these projects will be finished in August 2015.
Convoy sailing.
Since the canal does not cater for unregulated two-way traffic, all ships transit in convoys on regular times, scheduled on a 24-hour basis. Each day a single northbound convoy starts at 06.00 from Suez, getting an unhindered passage. At by-passes, the convoy uses the eastern route. Interwoven in this convoy's passage are two southbound convoys. The first starts at 0.00 from Port Said, and anchors in the Great Bitter Lake to let the northbound pass; the second starts at 07.00 and anchors in the western Ballah by-pass to let the northbound convoy pass. Due to the Ballah canal dimensions, this convoy is restricted to smaller and often unloaded ships.
Canal crossings.
From north to south, the crossings are:
A railway on the west bank runs parallel to the canal for its entire length.
Alternative routes.
Cape Agulhas.
The main alternative is around Cape Agulhas, the southernmost point of Africa, commonly referred as the Cape of Good Hope route. This was the only sea route before the canal was constructed, and when the canal was closed. It is still the only route for ships that are too large for the canal. In the early 21st century the long route has enjoyed increased popularity because of increasing piracy in Somalia. Between 2008 and 2010, it is estimated that the canal lost 10% of traffic due to the threat of piracy, and another 10% due to the financial crisis. An oil tanker going from Saudi Arabia to the United States has 2700 mi longer to go when taking the route south of Africa rather than the canal.
Before the canal's opening in 1869 goods were sometimes offloaded from ships and carried overland between the Mediterranean and the Red Sea.
Northern Sea Route.
In recent years, the shrinking Arctic sea ice has made the Northern Sea Route feasible for commercial cargo ships between Europe and East Asia during a six-to-eight-week window in the summer months, shortening the voyage by thousands of miles compared to that through the Suez Canal. According to polar climate researchers, as the extent of the Arctic summer ice pack recedes the route will become passable without the help of icebreakers for a greater period each summer.
The Bremen-based Beluga Group claimed in 2009 to be the first Western company to attempt using the Northern Sea Route without assistance from icebreakers, cutting 4000 nautical miles off the journey between Ulsan, Korea and Rotterdam, the Netherlands.
Negev desert railroad.
Israel has declared that it will construct a railroad through the Negev desert to compete with the canal, with construction partly financed by China.
Environmental impact.
The opening of the canal created the first salt-water passage between the Mediterranean and the Red Sea. Although the Red Sea is about 1.2 m higher than the eastern Mediterranean, the current between the Mediterranean and the middle of the canal at the Bitter Lakes flows north in winter and south in summer. The current south of the Bitter Lakes is tidal, varying with the tide at Suez. The Bitter Lakes, which were hypersaline natural lakes, blocked the migration of Red Sea species into the Mediterranean for many decades, but as the salinity of the lakes gradually equalised with that of the Red Sea the barrier to migration was removed, and plants and animals from the Red Sea have begun to colonise the eastern Mediterranean. The Red Sea is generally saltier and more nutrient-poor than the Atlantic, so the Red Sea species have advantages over Atlantic species in the less salty and nutrient-rich eastern Mediterranean. Accordingly, most Red Sea species invade the Mediterranean biota, and only few do the opposite. This migratory phenomenon is called Lessepsian migration (after Ferdinand de Lesseps) or "Erythrean invasion". Also impacting the eastern Mediterranean, starting in 1968, was the operation of Aswan High Dam across the Nile. While providing for increased human development, the project reduced the inflow of freshwater and ended all natural nutrient-rich silt entering the eastern Mediterranean at the Nile Delta. This provided less natural dilution of Mediterranean salinity and ended the higher levels of natural turbidity, additionally making conditions more like those in the Red Sea.
Invasive species originated from the Red Sea and introduced into the Mediterranean by the canal have become a major component of the Mediterranean ecosystem and have serious impacts on the ecology, endangering many local and endemic species. About 300 species from the Red Sea have been identified in the Mediterranean, and there are probably others yet unidentified. The Egyptian government's intent to enlarge the canal has raised concerns from marine biologists, fearing that this will worsen the invasion of Red Sea species.
Construction of the canal was preceded by cutting a small fresh-water canal called Sweet Water Canal from the Nile delta along Wadi Tumilat to the future canal, with a southern branch to Suez and a northern branch to Port Said. Completed in 1863, these brought fresh water to a previously arid area, initially for canal construction, and subsequently facilitating growth of agriculture and settlements along the canal.
Timeline.
Leadership.
Presidents of the Suez Canal Company (1858–1956):
Chairmen of the Suez Canal Authority (1956–present):

</doc>
<doc id="29324" url="http://en.wikipedia.org/wiki?curid=29324" title="Signal processing">
Signal processing

Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as "signals". It uses mathematical, statistical, computational, heuristic, and linguistic representations, formalisms, and techniques for representation, modelling, analysis, synthesis, discovery, recovery, sensing, acquisition, extraction, learning, security, or forensics.
History.
According to Alan V. Oppenheim and Ronald W. Schafer, the principles of signal processing can be found in the classical numerical analysis techniques of the 17th century. Oppenheim and Schafer further state that the "digitalization" or digital refinement of these techniques can be found in the digital control systems of the 1940s and 1950s.
Application fields of signal processing.
In communication systems, signal processing may occur at:
Categories of signal processing.
Analog signal processing.
Analog signal processing is for signals that have not been digitized, as in legacy radio, telephone, radar, and television systems. This involves linear electronic circuits as well as non-linear ones. The former are, for instance, passive filters, active filters, additive mixers, integrators and delay lines. Non-linear circuits include compandors, multiplicators (frequency mixers and voltage-controlled amplifiers), voltage-controlled filters, voltage-controlled oscillators and phase-locked loops.
Discrete-time signal processing.
Discrete-time signal processing is for sampled signals, defined only at discrete points in time, and as such are quantized in time, but not in magnitude.
"Analog discrete-time signal processing" is a technology based on electronic devices such as sample and hold circuits, analog time-division multiplexers, analog delay lines and analog feedback shift registers. This technology was a predecessor of digital signal processing (see below), and is still used in advanced processing of gigahertz signals.
The concept of discrete-time signal processing also refers to a theoretical discipline that establishes a mathematical basis for digital signal processing, without taking quantization error into consideration.
Digital signal processing.
Digital signal processing is the processing of digitized discrete-time sampled signals. Processing is done by general-purpose computers or by digital circuits such as ASICs, field-programmable gate arrays or specialized digital signal processors (DSP chips). Typical arithmetical operations include fixed-point and floating-point, real-valued and complex-valued, multiplication and addition. Other typical operations supported by the hardware are circular buffers and look-up tables. Examples of algorithms are the Fast Fourier transform (FFT), finite impulse response (FIR) filter, Infinite impulse response (IIR) filter, and adaptive filters such as the Wiener and Kalman filters.
Nonlinear signal processing.
Nonlinear signal processing involves the analysis and processing of signals produced from nonlinear systems and can be in the time, frequency, or spatio-temporal domains. Nonlinear systems can produce highly complex behaviors including bifurcations, chaos, harmonics, and subharmonics which cannot be produced or analyzed using linear methods.

</doc>
<doc id="29328" url="http://en.wikipedia.org/wiki?curid=29328" title="Six-Day War">
Six-Day War

The Six-Day War (Hebrew: מלחמת ששת הימים, "Milhemet Sheshet Ha Yamim"; Arabic: النكسة, "an-Naksah", "The Setback" or حرب ۱۹٦۷, "Ḥarb 1967", "War of 1967"), also known as the June War, 1967 Arab–Israeli War, or Third Arab–Israeli War, was fought between June 5 and 10, 1967 by Israel and the neighboring states of Egypt (known at the time as the United Arab Republic), Jordan, and Syria.
Relations between Israel and its neighbours had never fully normalized following the 1948 Arab-Israeli War, and in the period leading up to June 1967 tensions became dangerously heightened. As a result, following the mobilisation of Egyptian forces along the Israeli border in the Sinai Peninsula, Israel launched a series of preemptive airstrikes against Egyptian airfields on June 5. The Egyptians, whose defensive infrastructure was in a poor state, were caught by surprise and virtually the entire Egyptian air force was destroyed with few Israeli losses, giving the Israelis air superiority. Simultaneously, the Israelis launched a ground offensive into the Gaza strip and through the northern and central routes of the Sinai, which again caught the Egyptians by surprise. After some initial resistance, the Egyptian leader, Gamal Abdel Nasser, ordered the evacuation of the Sinai. On June 6 and 7, Israeli forces rushed westward in pursuit of the Egyptians whose retreat was disorganized and chaotic. The Israelis inflicted heavy losses on the retreating Egyptian forces. By June 7 the Israelis had reached the Suez canal and had taken Sharm el Sheikh in the south of the peninsula. Conquest of the Sinai was completed on June 8 when Israeli forces reached the peninsula's western coast.
On June 5, Nasser had induced Syria and Jordan to begin attacks on Israel by using the initially confused situation to claim that Egypt had defeated the Israeli air strike. In the afternoon of June 5, Israel retaliated against Jordan by launching an offensive to encircle East Jerusalem. Initially, Israeli forces held back from moving into the Old City for a number of reasons including potentially negative international reaction. However on June 7 the Israeli Minister of Defense, Moshe Dayan, gave the order to attack. After heavy fighting, the Israelis completed the conquest of the city later that day. Also on June 7, Israeli forces seized the West Bank cities of Nablus and Bethlehem from the Jordanians. When King Hussein ordered the Jordanian forces to retreat across the River Jordan, the Israeli forces occupied the rest of the West Bank unopposed. Israel's retaliation against Syria on June 5 took the form of an air strike in the evening which destroyed two-thirds of the Syrian air force, giving the Israelis air superiority over the Syrians. On June 9, Dayan ordered a ground invasion of the Golan Heights. Despite an extensive fortifications system and heavy fighting, the Israelis broke through the Syrian first line of defense. By June 10, Israeli forces had taken the Golan plateau and the Syrians had retreated eastward behind the ceasefire "purple line".
On June 11, a ceasefire was signed. Arab casualties were far heavier than that of Israel: less than a thousand Israelis had been killed compared to over 20,000 from the Arab forces. Israel's military success was attributable to the element of surprise, an innovative and well executed battle plan and the poor quality and leadership of the Arab forces. Israel seized control of the Gaza Strip and the Sinai Peninsula (from Egypt), the West Bank and East Jerusalem (from Jordan) and the Golan Heights (from Syria). The area under Israeli control tripled, significantly contributing to the country's defensibility, as would be shown in the subsequent Yom Kippur War. Although Israeli morale and international prestige was greatly increased by the outcome of the war, the resulting displacement of civilian populations would have long-term consequences. 300,000 Palestinians fled the West Bank and about 100,000 Syrians left the Golan to become refugees. Across the Arab world, Jewish minority communities were expelled. Israel made peace with Egypt following the Camp David Accords of 1978 and completeď a staged withdrawal from the Sinai in 1982. However, the position of the other occupied territories became a long-standing and bitter cause of conflict between Israel and the Palestinians, and the Arab world in general.
Background.
After the 1956 Suez Crisis, Egypt agreed to the stationing of a United Nations Emergency Force (UNEF) in the Sinai to ensure all parties would comply with the 1949 Armistice Agreements. In the following years there were numerous minor border clashes between Israel and its Arab neighbors, particularly Syria. In early November, 1966, Syria signed a mutual defense agreement with Egypt. Soon thereafter, in response to PLO guerilla activity, including a mine attack that left three dead, the Israeli Defence Force (IDF) attacked the village of as-Samu in the Jordanian-occupied West Bank. Jordanian units that engaged the Israelis were quickly beaten back. King Hussein of Jordan criticized Egyptian President Gamal Abdel Nasser for failing to come to Jordan's aid, and "hiding behind UNEF skirts".
In May 1967, Nasser received false reports from the Soviet Union that Israel was massing on the Syrian border. Nasser began massing his troops in two defensive lines in the Sinai Peninsula on Israel's border (May 16), expelled the UNEF force from Gaza and Sinai (May 19) and took up UNEF positions at Sharm el-Sheikh, overlooking the Straits of Tiran. Israel reiterated declarations made in 1957 that any closure of the Straits would be considered an act of war, or justification for war, and Nasser declared the Straits closed to Israeli shipping on May 22–23. On May 30, Jordan and Egypt signed a defense pact. The following day, at Jordan's invitation, the Iraqi army began deploying troops and armored units in Jordan. They were later reinforced by an Egyptian contingent. On June 1, Israel formed a National Unity Government by widening its cabinet, and on June 4 the decision was made to go to war. The next morning, Israel launched Operation Focus, a large-scale surprise air strike that was the opening of the Six-Day War.
Military preparation.
Before the war, Israeli pilots and ground crews had trained extensively in rapid refitting of aircraft returning from sorties, enabling a single aircraft to sortie up to four times a day (as opposed to the norm in Arab air forces of one or two sorties per day). This enabled the Israeli Air Force (IAF) to send several attack waves against Egyptian airfields on the first day of the war, overwhelming the Egyptian Air Force, and allowed it to knock out other Arab air forces on the same day. This has contributed to the Arab belief that the IAF was helped by foreign air forces (see Controversies relating to the Six-Day War). Pilots were extensively schooled about their targets, and were forced to memorize every single detail, and rehearsed the operation multiple times on dummy runways in total secrecy.
The Egyptians had constructed fortified defenses in the Sinai. These designs were based on the assumption that an attack would come along the few roads leading through the desert, rather than through the difficult desert terrain. The Israelis chose not to risk attacking the Egyptian defenses head-on, and instead surprised them from an unexpected direction.
James Reston, writing in "The New York Times" on May 23, 1967, noted, "In discipline, training, morale, equipment and general competence his [Nasser's] army and the other Arab forces, without the direct assistance of the Soviet Union, are no match for the Israelis... Even with 50,000 troops and the best of his generals and air force in Yemen, he has not been able to work his way in that small and primitive country, and even his effort to help the Congo rebels was a flop."
On May 26, 1967, The C.I.A estimated:"The Israelis ... If they attack now they ... would still be able to drive the Egyptians away from the entrance to the Strait of Tiran, but it would certainly cost them heavy losses of men and materiel."
On the eve of the war, Israel believed it could win a war in 3–4 days. The United States estimated Israel would need 7–10 days to win, with British estimates supporting the U.S. view.
Armies and weapons.
Armies.
The Israeli army had a total strength, including reservists, of 264,000, though this number could not be sustained, as the reservists were vital to civilian life.
Against Jordan's forces on the West Bank, Israel deployed about 40,000 troops and 200 tanks (8 brigades). Israeli Central Command forces consisted of five brigades. The first two were permanently stationed near Jerusalem and were called the Jerusalem Brigade and the mechanized Harel Brigade. Mordechai Gur's 55th paratrooper brigade was summoned from the Sinai front. The 10th Armored Brigade was stationed north of the West Bank. The Israeli Northern Command provided a division (3 brigades) led by Major-General Elad Peled, which was stationed to the north of the West Bank, in the Jezreel Valley.
On the eve of the war, Egypt massed approximately 100,000 of its 160,000 troops in the Sinai, including all of its seven divisions (four infantry, two armoured and one mechanized), four independent infantry brigades and four independent armoured brigades. No fewer than a third of them were veterans of Egypt's continuing intervention into the North Yemen Civil War and another third were reservists. These forces had 950 tanks, 1,100 APCs, and more than 1,000 artillery pieces.
Syria's army had a total strength of 75,000 and amassed them along the Syrian border.
Jordan's army had 55,000 troops and 300 tanks along the Jordanian border, 250 of which were U.S. M48 Pattons, sizable amounts of M113 APCs, a new battalion of mechanized infantry, and a paratrooper battalion trained in the new U.S.-built school. They also had 12 battalions of artillery and six batteries of 81 mm and 120 mm mortars.
The Jordanian Armed Forces included 11 brigades totalling some 55,000 troops, equipped with some 300 modern Western tanks. Of these, nine brigades (45,000 troops, 270 tanks, 200 artillery pieces) were deployed in the West Bank, including the elite armoured 40th, and two in the Jordan Valley. The Jordanian Army, then known as the Arab Legion, was a long-term-service, professional army, relatively well-equipped and well-trained. Furthermore, Israeli post-war briefings said that the Jordanian staff acted professionally as well, but was always left "half a step" behind by the Israeli moves. The small Royal Jordanian Air Force consisted of only 24 British-made Hawker Hunter fighters, six transports, and two helicopters. According to the Israelis, the Hawker Hunter was essentially on par with the French-built Dassault Mirage III – the IAF's best plane.
100 Iraqi tanks and an infantry division were readied near the Jordanian border. Two squadrons of fighter-aircraft, Hawker Hunters and MiG 21, were rebased adjacent to the Jordanian border.
The Arab air forces were aided by volunteer pilots from the Pakistan Air Force acting in independent capacity, and by some aircraft from Libya, Algeria, Morocco, Kuwait, and Saudi Arabia to make up for the massive losses suffered on the first day of the war. PAF pilots shot down several Israeli planes.
Weapons.
With the exception of Jordan, the Arabs relied principally on Soviet weaponry. Jordan's army was equipped with American weaponry, and its air force was composed of British aircraft.
Egypt had by far the largest and the most modern of all the Arab air forces, consisting of about 420 combat aircraft, all of them Soviet-built and with a heavy quota of top-of-the line MiG-21s. Of particular concern to the Israelis were the 30 Tu-16 "Badger" medium bombers, capable of inflicting heavy damage on Israeli military and civilian centers.
Israeli weapons were mainly of Western origin. Its air force was composed principally of French aircraft while its armoured units were mostly of British and American design and manufacture. Some infantry weapons, including the ubiquitous Uzi, were of Israeli origin.
The fighting fronts.
Preemptive air attack.
Israel's first and most critical move was a surprise attack on the Egyptian Air Force. Initially, both Egypt and Israel announced that they had been attacked by the other country.
On June 5 at 7:45 Israeli time, as civil defense sirens sounded all over Israel, the IAF launched Operation Focus ("Moked"). All but 12 of its nearly 200 operational jets launched a mass attack against Egypt's airfields. The Egyptian defensive infrastructure was extremely poor, and no airfields were yet equipped with hardened aircraft shelters capable of protecting Egypt's warplanes. Most of the Israeli warplanes headed out over the Mediterranean Sea, flying low to avoid radar detection, before turning toward Egypt. Others flew over the Red Sea.
Meanwhile, the Egyptians hindered their own defense by effectively shutting down their entire air defense system: they were worried that rebel Egyptian forces would shoot down the plane carrying Field Marshal Abdel Hakim Amer and Lt-Gen. Sidqi Mahmoud, who were en route from al Maza to Bir Tamada in the Sinai to meet the commanders of the troops stationed there. In any event, it did not make a great deal of difference as the Israeli pilots came in below Egyptian radar cover and well below the lowest point at which its SA-2 surface-to-air missile batteries could bring down an aircraft.
Although the powerful Jordanian radar facility at Ajloun detected waves of aircraft approaching Egypt and reported the code word for "war" up the Egyptian command chain, Egyptian command and communications problems prevented the warning from reaching the targeted airfields. The Israelis employed a mixed attack strategy: bombing and strafing runs against planes parked on the ground, themselves, and bombing the runways with special tarmac-shredding penetration bombs developed jointly with France to disable them and leave surviving aircraft unable to take off. The runway at the Arish airfield was spared, as the Israelis expected to turn it into a military airport for their transports after the war. The surviving aircraft were later taken out by several more attack waves. The operation was more successful than expected, catching the Egyptians by surprise and destroying virtually all of the Egyptian Air Force on the ground, with few Israeli losses. Only four unarmed Egyptian training flights were in the air when the strike began. A total of 338 Egyptian aircraft were destroyed and 100 pilots were killed, although the number of aircraft actually lost by the Egyptians is disputed.
Among the Egyptian planes lost were all 30 Tu-16 bombers, 27 out of 40 Il-28 bombers, 12 Su-7 fighter-bombers, over 90 MiG-21s, 20 MiG-19s, 25 MiG-17 fighters, and around 32 assorted transport planes and helicopters. In addition, Egyptian radars and SAM missiles were also attacked and destroyed. The Israelis lost 19 planes, including two destroyed in air-to-air combat and 13 downed by anti-aircraft artillery. One Israeli plane, which was damaged and unable to break radio silence, was shot down by Israeli Hawk missiles after it strayed over the Negev Nuclear Research Center. Another was destroyed by an exploding Egyptian bomber.
The attack guaranteed Israeli air superiority for the rest of the war.
Attacks on other Arab air forces took place later in the day as hostilities broke out on other fronts.
The numbers of Arab aircraft claimed destroyed by Israel were at first regarded as "greatly exaggerated" by the Western press. However, the fact that the Egyptian Air Force, along with other Arab air forces attacked by Israel made practically no appearance for the remaining days of the conflict proved that the numbers were most likely authentic. Throughout the war, Israeli aircraft continued strafing Arab airfield runways to prevent their return to usability. Meanwhile, Egyptian state-run radio had reported an Egyptian victory, falsely claiming that 70 Israeli planes had been downed on the first day of fighting.
Gaza Strip and Sinai Peninsula.
The Egyptian forces consisted of seven divisions: four armoured, two infantry, and one mechanized infantry. Overall, Egypt had around 100,000 troops and 900–950 tanks in the Sinai, backed by 1,100 APCs and 1,000 artillery pieces. This arrangement was thought to be based on the Soviet doctrine, where mobile armour units at strategic depth provide a dynamic defense while infantry units engage in defensive battles.
Israeli forces concentrated on the border with Egypt included six armoured brigades, one infantry brigade, one mechanized infantry brigade, three paratrooper brigades, giving a total of around 70,000 men and 700 tanks, who were organized in three armoured divisions. They had massed on the border the night before the war, camouflaging themselves and observing radio silence before being ordered to advance.
The Israeli plan was to surprise the Egyptian forces in both timing (the attack exactly coinciding with the IAF strike on Egyptian airfields), location (attacking via northern and central Sinai routes, as opposed to the Egyptian expectations of a repeat of the 1956 war, when the IDF attacked via the central and southern routes) and method (using a combined-force flanking approach, rather than direct tank assaults).
The northern (El Arish) Israeli division.
The northernmost Israeli division, consisting of three brigades and commanded by Major General Israel Tal, one of Israel's most prominent armour commanders, crossed the border at two points, opposite Nahal Oz and south of Khan Yunis. They advanced swiftly, holding fire to prolong the element of surprise. Tal's forces assaulted the "Rafah Gap", a seven-mile stretch containing the shortest of three main routes through the Sinai towards El-Qantarah el-Sharqiyya and the Suez Canal. The Egyptians had four divisions in the area, backed by minefields, pillboxes, underground bunkers, hidden gun emplacements and trenches. The terrain on either side of the route was impassable. The Israeli plan was to hit the Egyptians at selected key points with concentrated armour.
Tal's advance was led by the 7th Armored Brigade under Colonel Shmuel Gonen. The Israeli plan called for the 7th Brigade to outflank Khan Yunis from the north and the 60th Armored Brigade under Colonel Menachem Aviram would advance from the south. The two brigades would link up and surround Khan Yunis, while the paratroopers would take Rafah. Gonen entrusted the breakthrough to a single battalion of his brigade.
Initially, the advance was met with light resistance, as Egyptian intelligence had concluded that it was a diversion for the main attack. However, as Gonen's lead battalion advanced, it suddenly came under intense fire and took heavy losses. A second battalion was brought up, but was also pinned down. Meanwhile, the 60th Brigade became bogged down in the sand, while the paratroopers had trouble navigating through the dunes. The Israelis continued to press their attack, and despite heavy losses, cleared the Egyptian positions and reached the Khan Yunis railway junction in little over four hours.
Gonen's brigade then advanced nine miles to Rafah in twin columns. Rafah itself was circumvented, and the Israelis attacked Sheikh Zuweid, eight miles to the southwest, which was defended by two brigades. Though inferior in numbers and equipment, the Egyptians were deeply entrenched and camouflaged. The Israelis were pinned down by fierce Egyptian resistance, and called in air and artillery support to enable their lead elements to advance. Many Egyptians abandoned after their commander and several of his staff were killed.
The Israelis broke through with tank-led assaults. However, Aviram's forces misjudged the Egyptians' flank, and were pinned between strongholds before they were extracted after several hours. By nightfall, the Israelis had finished mopping up resistance. Israeli forces had taken significant losses, with Colonel Gonen later telling reporters that "we left many of our dead soldiers in Rafah, and many burnt-out tanks." The Egyptians suffered some 2,000 casualties and lost 40 tanks.
Advance on Arish.
With the road open, Israeli forces continued advancing towards Arish. Already by late afternoon, elements of the 79th Armored Battalion had charged through the seven-mile long Jiradi defile, a narrow pass defended by well-emplaced troops of the Egyptian 112th Infantry Brigade. In fierce fighting, which saw the pass change hands several times, the Israelis charged through the position. The Egyptians suffered heavy casualties and tank losses, while Israeli losses stood at 66 dead, 93 wounded and 28 tanks. Emerging at the western end, Israeli forces advanced to the outskirts of Arish. As it reached the outskirts of Arish, Tal's division also consolidated its hold on Rafah and Khan Yunis.
The following day, the Israeli forces on the outskirts of Arish were reinforced by the 7th Brigade, which fought its way through the Jiradi pass. After receiving supplies via an airdrop, the Israelis entered the city and captured the airport at 7:50 am. The Israelis entered the city at 8:00 am. Company commander Yossi Peled recounted that "Al-Arish was totally quiet, desolate. Suddenly, the city turned into a madhouse. Shots came at us from every alley, every corner, every window and house." An IDF record stated that "clearing the city was hard fighting. The Egyptians fired from the rooftops, from balconies and windows. They dropped grenades into our half-tracks and blocked the streets with trucks. Our men threw the grenades back and crushed the trucks with their tanks." Gonen sent additional units to Arish, and the city was eventually taken.
Yoffe's attack allowed Tal to complete the capture of the Jiradi defile, Khan Yunis . All of them were taken after fierce fighting. Gonen subsequently dispatched a force of tanks, infantry and engineers under Colonel Yisrael Granit to continue down the Mediterranean coast towards the Suez Canal, while a second force led by Gonen himself turned south and captured Bir Lahfan and Jabal Libni.
The mid front (Abu-Ageila) Israeli division.
Further south, the Israeli 38th Armored Division under Major-General Ariel Sharon assaulted Um-Katef, a heavily fortified area defended by the Egyptian 2nd Infantry Division under Major-General Sa'adi Nagib, and consisting of some 16,000 troops. The Egyptians also had a battalion of tank destroyers and a tank regiment, formed of Soviet World War II armour, which included 90 T-34-85 tanks, 22 SU-100 tank destroyers, and about 16,000 men. The Israelis had about 14,000 men and 150 post-World War II tanks including the AMX-13, Centurions, and M50 Super Shermans (modified M-4 Sherman tanks).
Two armoured brigades in the meantime, under Avraham Yoffe, slipped across the border through sandy wastes that Egypt had left undefended because they were considered impassable. Simultaneously, Sharon's tanks from the west were to engage Egyptian forces on Um-Katef ridge and block any reinforcements. Israeli infantry would clear the three trenches, while heliborne paratroopers would land behind Egyptian lines and silence their artillery. An armoured thrust would be made at al-Qusmaya to unnerve and isolate its garrison.
As Sharon's division advanced into the Sinai, Egyptian forces staged successful delaying actions at Tarat Umm, Umm Tarfa, and Hill 181. An Israeli jet was downed by anti-aircraft fire, and Sharon's forces came under heavy shelling as they advanced from the north and west. The Israeli advance, which had to cope with extensive minefields, took a large number of casualties. A column of Israeli tanks managed to penetrate the northern flank of Abu Ageila, and by dusk, all units were in position. The Israelis then brought up 90 105mm and 155mm artillery guns for a preparatory barrage, while civilian buses brought reserve infantrymen under Colonel Yekutiel Adam and helicopters arrived to ferry the paratroopers. These movements were unobserved by the Egyptians, who were preoccupied with Israeli probes against their perimeter.
As night fell, the Israeli assault troops lit flashlights, each battalion a different color, to prevent friendly fire incidents. At 10:00 pm, Israeli artillery began a barrage on Um-Katef, firing some 6,000 shells in less than twenty minutes, the most concentrated artillery barrage in Israel's history. Israeli tanks assaulted the northernmost Egyptian defenses and were largely successful, though an entire armoured brigade was stalled by mines, and had only one mine-clearance tank. Israeli infantrymen assaulted the triple line of trenches in the east. To the west, paratroopers commanded by Colonel Danny Matt landed behind Egyptian lines, though half the helicopters got lost and never found the battlefield, while others were unable to land due to mortar fire.
Paratroopers dropped behind Egyptian lines, neutralized their artillery, destroying much of the ammunition dumps, and separated gun crews from their batteries, sowing enough confusion to significantly reduce Egyptian artillery fire. Egyptian reinforcements from Jabal Libni advanced towards Um-Katef to counterattack, but failed to reach their objective, being subjected to heavy air attacks and encountering Israeli lodgements on the roads. Egyptian commanders then called in artillery attacks on their own positions. The Israelis accomplished and sometimes exceeded their overall plan, and had largely succeeded by the following day. The Egyptians took heavy casualties, while the Israelis lost 40 dead and 140 wounded.
Yoffe's attack allowed Sharon to complete the capture of the Um-Katef, after fierce fighting. The main thrust at Um-Katef was stalled due to mines and craters. After IDF engineers had cleared a path by 4:00 pm, Israeli and Egyptian tanks engaged in fierce combat, often at ranges as close as ten yards. The battle ended in an Israeli victory, with 40 Egyptian and 19 Israeli tanks destroyed. Meanwhile, Israeli infantry finished clearing out the Egyptian trenches, with Israeli casualties standing at 14 dead and 41 wounded and Egyptian casualties at 300 dead and 100 taken prisoner.
Other Israeli forces.
Meanwhile, two Israeli reserve brigades under Brigadier-General Avraham Yoffe, each equipped with 100 tanks, penetrated the Sinai south of Tal's division and north of Sharon's, capturing the road junctions of Abu Ageila, Bir Lahfan, and Arish, taking all of them before midnight. Two Egyptian armoured brigades counterattacked, and a fierce battle took place until the following morning. The Egyptians were beaten back by fierce resistance coupled with airstrikes, sustaining heavy tank losses. They fled west towards Jabal Libni.
Further south, the 8th Armored Brigade under Colonel Albert Mandler, initially positioned as a ruse to draw off invasion forces from the real invasion routes, attacked the fortified bunkers at Kuntilla, a strategically valuable position whose capture would enable Mandler to block reinforcements from reaching Um-Katef and to join Sharon's upcoming attack on Nakhl. The defending Egyptian battalion, outnumbered and outgunned, fiercely resisted the attack, hitting a number of Israeli tanks. However, most of the defenders were killed, and only three Egyptian tanks, one of them damaged, survived. By nightfall, Mendler's forces had taken Kuntilla.
With the exceptions of Rafah and Khan Yunis, Israeli forces had initially avoided entering the Gaza Strip. Israeli Defense Minister Moshe Dayan had expressly forbidden entry into the area. After Palestinian positions in Gaza opened fire on the Negev settlements of Nirim and Kissufim, IDF Chief of Staff Yitzhak Rabin overrode Dayan's instructions and ordered the 11th Mechanized Brigade under Colonel Yehuda Reshef to enter the Strip. The force was immediately met with heavy artillery fire and fierce resistance from Palestinian forces and remnants of the Egyptian forces from Rafah.
By sunset, the Israelis had taken the strategically vital Ali Muntar ridge, overlooking Gaza City, but were beaten back from the city itself. Some 70 Israelis were killed, along with Israeli journalist Ben Oyserman and American journalist Paul Schutzer. Twelve members of UNEF were also killed. On the war's second day, the Israelis were bolstered by the 35th Paratroopers Brigade under Colonel Rafael Eitan, and took Gaza City along with the entire Strip. The fighting was fierce, and accounted for nearly half of all Israeli casualties on the southern front. However, Gaza rapidly fell to the Israelis.
The Egyptian army.
During the ground fighting, remnants of the Egyptian Air Force attacked Israeli ground forces, but took losses from the Israeli Air Force and from Israeli anti-aircraft units. Throughout the last four days, Egyptian aircraft flew 150 sorties against Israeli units in the Sinai.
Many of the Egyptian units remained intact and could have tried to prevent the Israelis from reaching the Suez Canal or engaged in combat in the attempt to reach the canal. However, when the Egyptian Field Marshal Abdel Hakim Amer heard about the fall of Abu-Ageila, he panicked and ordered all units in the Sinai to retreat. This order effectively meant the defeat of Egypt.
President Nasser, having learned of the results of the air strike, decided together with Field Marshal Amer to pull out the troops from Sinai within 24 hours. No detailed instructions were given concerning the manner and sequence of withdrawal.
The next fighting days.
As Egyptian columns retreated, Israeli aircraft and artillery attacked them. Israeli jets used napalm bombs during their sorties. The attacks destroyed hundreds of vehicles and caused heavy casualties. At Jabal Libni, retreating Egyptian soldiers were fired upon by their own artillery. At Bir Gafgafa, the Egyptians fiercely resisted advancing Israeli forces, knocking out three tanks and eight half-tracks, and killing 20 soldiers. Due to the Egyptians' retreat, the Israeli High Command decided not to pursue the Egyptian units but rather to bypass and destroy them in the mountainous passes of West Sinai.
Therefore, in the following two days (June 6 and 7), all three Israeli divisions (Sharon and Tal were reinforced by an armoured brigade each) rushed westwards and reached the passes. Sharon's division first went southward then westward, via An-Nakhl, to Mitla Pass with air support. It was joined there by parts of Yoffe's division, while its other units blocked the Gidi Pass. These passes became killing grounds for the Egyptians, who ran right into waiting Israeli positions and suffered heavy losses. According to Egyptian diplomat Mahmoud Riad, 10,000 men were killed in one day alone, and many others died from hunger and thirst. Tal's units stopped at various points to the length of the Suez Canal.
Israel's blocking action was partially successful. Only the Gidi pass was captured before the Egyptians approached it, but at other places, Egyptian units managed to pass through and cross the canal to safety. Due to the haste of the Egyptian retreat, soldiers often abandoned weapons, military equipment, and hundreds of vehicles. Many Egyptian soldiers were cut off from their units had to walk about 200 kilometers through by foot before reaching the Suez Canal with limited supplies of food and water and were exposed to intense heat.
Thousands of soldiers died as a result. Many Egyptian soldiers chose instead to surrender to the Israelis. However, the Israelis eventually exceeded their capabilities to provide for prisoners. As a result, they began directing soldiers towards the Suez Canal and only taking prisoner high-ranking officers, who were expected to be exchanged for captured Israeli pilots.
During the offensive, the Israeli Navy landed six combat divers from the Shayetet 13 naval commando unit to infiltrate Alexandria harbor. The divers sank an Egyptian minesweeper before being taken prisoner. Shayetet 13 commandos also infiltrated into Port Said harbor, but found no ships there. A planned commando raid against the Syrian Navy never materialized. Both Egyptian and Israeli warships made movements at sea to intimidate the other side throughout the war, but did not engage each other. However, Israeli warships and aircraft did hunt for Egyptian submarines throughout the war.
On June 7, Israel began the conquest of Sharm el-Sheikh. The Israeli Navy started the operation with a probe of Egyptian naval defenses. An aerial reconnaissance flight found that the area was less defended than originally thought. At about 4:30 am, three Israeli missile boats opened fire on Egyptian shore batteries, while paratroopers and commandos boarded helicopters and Nord Noratlas transport planes for an assault on Al-Tur, as Chief of Staff Rabin was convinced it was too risky to land them directly in Sharm el-Sheikh.
However, the city had been largely abandoned the day before, and reports from air and naval forces finally convinced Rabin to divert the aircraft to Sharm el-Sheikh. There, the Israelis engaged in a pitched battle with the Egyptians and took the city, killing 20 Egyptian soldiers and taking 8 prisoner. At 12:15 pm, Defense Minister Dayan announced that the Straits of Tiran constituted an international waterway open to all ships without restriction.
On June 8, Israel completed the capture of the Sinai by sending infantry units to Ras Sudar on the western coast of the peninsula.
Several tactical elements made the swift Israeli advance possible: first, the surprise attack that quickly gave the Israeli Air Force complete air superiority over the Egyptian Air Force; second, the determined implementation of an innovative battle plan; third, the lack of coordination among Egyptian troops. These factors would prove to be decisive elements on Israel's other fronts as well.
West Bank.
Jordan was reluctant to enter the war. Nasser used the obscurity of the first hours of the conflict to convince King Hussein that he was victorious; he claimed as evidence a radar sighting of a squadron of Israeli aircraft returning from bombing raids in Egypt, which he said was an Egyptian aircraft en route to attack Israel. One of the Jordanian brigades stationed in the West Bank was sent to the Hebron area in order to link with the Egyptians. Hussein decided to attack.
The IDF's strategic plan was to remain on the defensive along the Jordanian front, to enable focus in the expected campaign against Egypt.
Intermittent machine-gun exchanges began taking place in Jerusalem at 9:30 am, and the fighting gradually escalated as the Jordanians introduced mortar and recoilless rifle fire. Under the orders from General Narkis, the Israelis responded only with small-arms fire, firing in a flat trajectory to avoid hitting civilians, holy sites or the Old City. At 10:00 am on June 5, the Jordanian Army began shelling Israel. Two batteries of 155mm Long Tom cannons opened fire on the suburbs of Tel Aviv and Ramat David Airbase. The commanders of these batteries were instructed to lay a two-hour barrage against military and civilian settlements in central Israel. Some shells hit the outskirts of Tel Aviv.
Israel assumed that the attacks were a symbolic gesture of solidarity with Egypt, and sent a message to King Hussein promising not to initiate any action against Jordan if it stayed out of the war. King Hussein replied that it was too late, "the die was cast". At 11:15 am, Jordanian howitzers began a 6,000-shell barrage at Israeli Jerusalem. The Jordanians initially targeted kibbutz Ramat Rachel in the south and Mount Scopus in the north, then ranged into the city center and outlying neighborhoods. Military installations, the Prime Minister's Residence, and the Knesset compound were also targeted. Israeli civilian casualties totalled 20 dead and about 1,000 wounded. Some 900 buildings were damaged, including Hadassah Ein Kerem Hospital.
At 11:50 am, sixteen Jordanian Hawker Hunters attacked Netanya, Kfar Sirkin and Kfar Saba, killing one civilian, wounding seven and destroying a transport plane. Three Iraqi Hawker Hunters strafed civilian settlements in the Jezreel Valley, and an Iraqi Tu-16 attacked Afula, and was shot down near the Megiddo airfield. The attack caused minimal material damage, hitting only a senior citizens' home and several chicken coops, but sixteen Israeli soldiers were killed, most of them when the Tupolev crashed.
Israeli cabinet meets.
When the Israeli cabinet convened to decide what to do, Yigal Allon and Menahem Begin argued that this was an opportunity to take the Old City of Jerusalem, but Eshkol decided to defer any decision until Moshe Dayan and Yitzhak Rabin could be consulted. Uzi Narkiss made a number of proposals for military action, including the capture of Latrun, but the cabinet turned him down.
Dayan rejected multiple requests from Narkiss for permission to mount an infantry assault towards Mount Scopus. However, Dayan sanctioned a number of more limited retaliatory actions.
Initial response.
Shortly before 12:30 pm, the Israeli Air Force attacked Jordan's two airbases. The Hawker Hunters were refueling at the time of the attack. The Israeli aircraft came within two waves, the first of which cratered the runways and knocked out the control towers, and the second wave destroyed all 21 of Jordan's Hawker Hunter fighters, along with six transport aircraft and two helicopters. One Israeli jet was shot down by ground fire.
Israeli aircraft also attacked H-3, an Iraqi Air Force base in western Iraq. During the attack, 12 MiG-21s, 2 MiG-17s, 5 Hunter F6s, and 3 Il-28 bombers were destroyed or shot down. A Pakistani pilot stationed at the base shot down an Israeli fighter and bomber during the raid. The Jordanian radar facility at Ajloun was destroyed in an Israeli airstrike. Israeli Fouga Magister jets attacked the Jordanian 40th Brigade with rockets as it moved south from the Damiya Bridge. Dozens of tanks were knocked out, and a convoy of 26 trucks carrying ammunition was destroyed. In Jerusalem, Israel responded to Jordanian shelling with a missile strike that devastated Jordanian positions. The Israelis used the L missile, a surface-to-surface missile developed jointly with France in secret.
Jordanian battalion at Government House.
A Jordanian battalion advanced up Government House ridge and dug in at the perimeter of Government House, the headquarters of the United Nations observers, and opened fire on Ramat Rachel, the Allenby Barracks and the Jewish section of Abu Tor with mortars and recoilless rifles. UN observers fiercely protested the incursion into the neutral zone, and several manhandled a Jordanian machine gun out of Government House after the crew had set it up in a second-floor window. After the Jordanians occupied Jabel Mukaber, an advance patrol was sent out and approached Ramat Rachel, where they came under fire from four civilians, including the wife of the director, who were armed with old Czech-made weapons.
The immediate Israeli response was an offensive to retake Government House and its ridge. The Jerusalem Brigade's Reserve Battalion 161, under Lieutenant-Colonel Asher Dreizin, was given the task. Dreizin had two infantry companies and eight tanks under his command, several of which broke down or became stuck in the mud at Ramat Rachel, leaving three for the assault. The Jordanians mounted fierce resistance, knocking out two tanks.
The Israelis broke through the compound's western gate and began clearing the building with grenades, before General Odd Bull, commander of the UN observers, compelled the Israelis to hold their fire, telling them that the Jordanians had already fled. The Israelis proceeded to take the Antenna Hill, directly behind Government House, and clear out a series of bunkers to the west and south. The fighting, often conducted hand-to-hand, continued for nearly four hours before the surviving Jordanians fell back to trenches held by the Hittin Brigade, which were steadily overwhelmed. By 6:30 pm, the Jordanians had retreated to Bethlehem, having suffered about 100 casualties. All but ten of Dreizin's soldiers were casualties, and Dreizin himself was wounded three times.
Israeli invasion.
During the late afternoon of June 5, the Israelis launched an offensive to encircle Jerusalem, which lasted into the following day. During the night, they were supported by intense tank, artillery and mortar fire to soften up Jordanian positions. Searchlights placed atop the Labor Federation building, then the tallest in Israeli Jerusalem, exposed and blinded the Jordanians. The Jerusalem Brigade moved south of Jerusalem, while the mechanized Harel Brigade and paratroopers under Mordechai Gur encircled it from the north.
A combined force of tanks and paratroopers crossed no-man's land near the Mandelbaum Gate. One of Gur's paratroop battalions approached the fortified Police Academy. The Israelis used bangalore torpedoes to blast their way through barbed wire leading up to the position while exposed and under heavy fire. With the aid of two tanks borrowed from the Jerusalem Brigade, they captured the Police Academy. After receiving reinforcements, they moved up to attack Ammunition Hill.
The Jordanian defenders, who were heavily dug-in, fiercely resisted the attack. All of the Israeli officers except for two company commanders were killed, and the fighting was mostly led by individual soldiers. The fighting was conducted at close quarters in trenches and bunkers, and was often hand-to-hand. The Israelis captured the position after four hours of heavy fighting. During the battle, 36 Israeli and 71 Jordanian soldiers were killed.
The battalion subsequently drove east, and linked up with the Israeli enclave on Mount Scopus and its Hebrew University campus. Gur's other battalions captured the other Jordanian positions around the American Colony, despite being short on men and equipment and having come under a Jordanian mortar bombardment while waiting for the signal to advance.
At the same time, the mechanized Harel Brigade attacked the fortress at Latrun, which the Jordanians had abandoned due to heavy Israeli tank fire. The brigade attacked Har Adar, but seven tanks were knocked out by mines, forcing the infantry to mount an assault without armoured cover. The Israeli soldiers advanced under heavy fire, jumping between stones to avoid mines. The fighting was conducted at close-quarters, often with knives and bayonets.
The Jordanians fell back after a battle that left two Israeli and eight Jordanian soldiers dead, and Israeli forces advanced through Beit Horon towards Ramallah, taking four fortified villages along the way. By the evening, the brigade arrived in Ramallah. Meanwhile, the 163rd Infantry Battalion secured Abu Tor following a fierce battle, severing the Old City from Bethlehem and Hebron.
Meanwhile, 600 Egyptian commandos stationed in the West Bank moved to attack Israeli airfields. Led by Jordanian intelligence scouts, they crossed the border and began infiltrating through Israeli settlements towards Ramla and Hatzor. They were soon detected and sought shelter in nearby fields, which the Israelis set on fire. Some 450 commandos were killed, and the remainder escaped to Jordan.
From the American Colony, the paratroopers moved towards the Old City. Their plan was to approach it via the lightly defended Salah al-Din Street. However, they made a wrong turn onto the heavily defended Nablus Road. The Israelis ran into fierce resistance. Their tanks fired at point-blank range down the street, while the paratroopers mounted repeated charges. Despite repelling repeated Israeli charges, the Jordanians gradually gave way to Israeli firepower and momentum. The Israelis suffered some 30 casualties – half the original force – while the Jordanians lost 45 dead and 142 wounded.
Meanwhile, the Israeli 71st Battalion breached barbed wire and minefields and emerged near Wadi Joz, near the base of Mount Scopus, from where the Old City could be cut off from Jericho and East Jerusalem from Ramallah. Israeli artillery targeted the one remaining route from Jerusalem to the West Bank, and shellfire deterred the Jordanians from counterattacking from their positions at Augusta-Victoria. An Israeli detachment then captured the Rockefeller Museum after a brief skirmish.
Afterwards, the Israelis broke through to the Jerusalem-Ramallah road. At Tel al-Ful, the Israelis fought a running battle with up to thirty Jordanian tanks. The Jordanians stalled the advance and destroyed a number of half-tracks, but the Israelis launched air attacks and exploited the vulnerability of the external fuel tanks mounted on the Jordanian tanks. The Jordanians lost half their tanks, and retreated towards Jericho. Joining up with the 4th Brigade, the Israelis then descended through Shuafat and the site of what is now French Hill, through Jordanian defenses at Mivtar, emerging at Ammunition Hill.
With Jordanian defenses in Jerusalem crumbling, elements of the Jordanian 60th Brigade and an infantry battalion were sent from Jericho to reinforce Jerusalem. Its original orders were to repel the Israelis from the Latrun corridor, but due to the worsening situation in Jerusalem, the brigade was ordered to proceed to Jerusalem's Arab suburbs and attack Mount Scopus. Parallel to the brigade were infantrymen from the Imam Ali Brigade, who were approaching Issawiya. The brigades were spotted by Israeli aircraft and decimated by rocket and cannon fire. Other Jordanian attempts to reinforce Jerusalem were beaten back, either by armoured ambushes or airstrikes.
Fearing damage to holy sites and the prospect of having to fight in built-up areas, Dayan ordered his troops not to enter the Old City. He also feared that Israel would be subjected to a fierce international backlash and the outrage of Christians worldwide if it forced its way into the Old City. Privately, he told David Ben-Gurion that he was also concerned over the prospect of Israel capturing Jerusalem's holy sites, only to be forced to give them up under the threat of international sanctions.
The Old City (June 7).
On June 7, heavy fighting ensued. Dayan had ordered his troops not to enter the Old City; however, upon hearing that the UN was about to declare a ceasefire, he changed his mind, and without cabinet clearance, decided to capture it. Two paratroop battalions attacked Augusta-Victoria Hill, high ground overlooking the Old City from the east. One battalion attacked from Mount Scopus, and another attacked from the valley between it and the Old City. Another paratroop battalion, personally led by Gur, broke into the Old City, and was joined by the other two battalions after their missions were complete. The paratroopers met little resistance. The fighting was conducted solely by the paratroopers; the Israelis did not use armour during the battle out of fear of severe damage to the Old City.
In the north, one battalion from Peled's division was sent to check Jordanian defenses in the Jordan Valley. A brigade belonging to Peled's division captured the western part of the West Bank. One brigade attacked Jordanian artillery positions around Jenin, which were shelling Ramat David Airbase. The Jordanian 12th Armored Battalion, which outnumbered the Israelis, held off repeated attempts to capture Jenin. However, Israeli air attacks took their toll, and the Jordanian M48 Pattons, with their external fuel tanks, proved vulnerable at short distances, even to the Israeli-modified Shermans. Twelve Jordanian tanks were destroyed, and only six remained operational.
Just after dusk, Israeli reinforcements arrived. The Jordanians continued to fiercely resist, and the Israelis were unable to advance without artillery and air support. One Israeli jet attacked the Jordanian commander's tank, wounding him and killing his radio operator and intelligence officer. The surviving Jordanian forces then withdrew to Jenin, where they were reinforced by the 25th Infantry Brigade. The Jordanians were effectively surrounded in Jenin.
Jordanian infantry and their three remaining tanks managed to hold off the Israelis until 4:00 am, when three battalions arrived to reinforce them in the afternoon. The Jordanian tanks charged, and knocked out multiple Israeli vehicles, and the tide began to shift. After sunrise, Israeli jets and artillery conducted a two-hour bombardment against the Jordanians. The Jordanians lost 10 dead and 250 wounded, and had only seven tanks left, including two without gas, and sixteen APCs. The Israelis then fought their way into Jenin, and captured the city after fierce fighting.
After the Old City fell, the Jerusalem Brigade reinforced the paratroopers, and continued to the south, capturing Judea and Gush Etzion. Hebron was taken without any resistance. Fearful that Israeli soldiers would exact retribution for the 1929 massacre of the city's Jewish community, Hebron's residents flew white sheets from their windows and rooftops, and voluntarily gave up their weapons. The Harel Brigade proceeded eastward, descending to the Jordan River.
On June 7, Israeli forces seized Bethlehem, taking the city after a brief battle that left some 40 Jordanian soldiers dead, with the remainder fleeing. On the same day, one of Peled's brigades seized Nablus; then it joined one of Central Command's armoured brigades to fight the Jordanian forces; as the Jordanians held the advantage of superior equipment and were equal in numbers to the Israelis.
Again, the air superiority of the IAF proved paramount as it immobilized the Jordanians, leading to their defeat. One of Peled's brigades joined with its Central Command counterparts coming from Ramallah, and the remaining two blocked the Jordan river crossings together with the Central Command's 10th. Engineering Corps sappers blew up the Abdullah and Hussein bridges with captured Jordanian mortar shells, while elements of the Harel Brigade crossed the river and occupied positions along the east bank to cover them, but quickly pulled back due to American pressure. The Jordanians, anticipating an Israeli offensive deep into Jordan, assembled the remnants of their army and Iraqi units in Jordan to protect the western approaches to Amman and the southern slopes of the Golan Heights.
No specific decision had been made to capture any other territories controlled by Jordan. After the Old City was captured, Dayan told his troops to dig in to hold it. When an armoured brigade commander entered the West Bank on his own initiative, and stated that he could see Jericho, Dayan ordered him back. It was only after intelligence reports indicated that Hussein had withdrawn his forces across the Jordan River that Dayan ordered his troops to capture the West Bank. According to Narkis:
First, the Israeli government had no intention of capturing the West Bank. On the contrary, it was opposed to it. Second, there was not any provocation on the part of the IDF. Third, the rein was only loosened when a real threat to Jerusalem's security emerged. This is truly how things happened on June 5, although it is difficult to believe. The end result was something that no one had planned.
Golan Heights.
In May–June 1967 The Israeli government did everything in its power to confine the confrontation to the Egyptian front. Eshkol and his colleagues took into account the possibility of some fighting on the Syrian front.
Syria's attack.
False Egyptian reports of a crushing victory against the Israeli army and forecasts that Egyptian forces would soon be attacking Tel Aviv influenced Syria's willingness to enter the war. Syrian artillery began shelling northern Israel, and twelve Syrian jets attacked Israeli settlements in the Galilee. Israeli fighter jets intercepted the Syrian aircraft, shooting down three and driving off the rest. In addition, two Lebanese Hawker Hunter jets, two of the twelve Lebanon had, crossed into Israeli airspace and began strafing Israeli positions in the Galilee. They were intercepted by Israeli fighter jets, and one was shot down.
A minor Syrian force tried to capture the water plant at Tel Dan (the subject of a fierce escalation two years earlier), Dan, and She'ar Yashuv. These attacks were repulsed with the loss of twenty soldiers and seven tanks. An Israeli officer was also killed. But a broader Syrian offensive quickly failed. Units of Syrian reserves were broken up by Israeli air attacks, and several Syrian tanks were reported to have sunk in the Jordan River.
Other problems included tanks too wide for bridges, lack of radio communications between tanks and infantry, and units ignoring orders to advance. A post-war Syrian army report concluded "Our forces did not go on the offensive either because they did not arrive or were not wholly prepared or because they could not find shelter from the enemy's planes. The reserves could not withstand the air attacks; they dispersed after their morale plummeted." The Syrians abandoned hopes of a ground attack and began a massive bombardment of Israeli communities in the Hula Valley instead.
The Israeli Air Force attacks the Syrian airfields.
On the evening of June 5, the Israeli Air Force attacked Syrian airfields. The Syrian Air Force lost some 32 MiG 21s, and 23 MiG-15 and MiG-17 fighters, and two Ilyushin Il-28 bombers, two-thirds of its fighting strength. The Syrian aircraft that survived the attack retreated to distant bases without playing any further role in the ensuing warfare. Following the attack, Syria understood that the news it had heard from Egypt of the near-total destruction of the Israeli military could not have been true.
The Israelis debate whether the Golan Heights should be attacked.
On June 7 and 8, the Israeli leadership debated about whether the Golan Heights should be attacked as well; the attack on Syria was initially planned for June 8, but was postponed for 24 hours. At 3 am on June 9, Syria announced its acceptance of the cease-fire. Despite this, four hours later at 7 am, Israeli Defense Minister Moshe Dayan "gave the order to go into action against Syria".[i] On the 8 June, Moshe Dayan was against the attack since he was afraid of a possible Soviet intervention and the government accepted his opinion. On 9 June morning, he changed his mind and ordered the army to attack, without consultation or receiving government authorization. Syria had supported the pre-war raids that had helped raise tensions and had routinely shelled Israel from the Heights, so some Israeli leaders wanted to see Syria punished.
Military advice was that the attack would be extremely costly, since assailing the heights would be an uphill battle against a strongly fortified enemy. The western side of the Golan Heights consists of a rock escarpment that rises 500 meters (1,700 ft) from the Sea of Galilee and the Jordan River, and then flattens to a more gently sloping plateau. Dayan believed such an operation would yield losses of 30,000 and opposed it bitterly. Prime Minister Eshkol, on the other hand, was more open to the possibility of an operation in the Golan Heights, as was the head of the Northern Command, David Elazar, whose unbridled enthusiasm for and confidence in the operation may have eroded Dayan's reluctance. Eventually, as the situation on the Southern and Central fronts cleared up, intelligence estimated that the likelihood of Soviet intervention had reduced, reconnaissance showed some Syrian defenses in the Golan region collapsing, and an intercepted cable showed Nasser urging the President of Syria to immediately accept a cease-fire, Dayan became more enthusiastic about the idea, and he authorized the operation.
The Syrian army consisted of about 75,000 men grouped in nine brigades, supported by an adequate amount of artillery and armour. Israeli forces used in combat consisted of two brigades (the 8th Armored Brigade and the Golani Brigade) in the northern part of the front at Givat HaEm, and another two (infantry and one of Peled's brigades summoned from Jenin) in the center. The Golan Heights' unique terrain (mountainous slopes crossed by parallel streams every several kilometers running east to west), and the general lack of roads in the area channeled both forces along east-west axes of movement and restricted the ability of units to support those on either flank. Thus the Syrians could move north-south on the plateau itself, and the Israelis could move north-south at the base of the Golan escarpment. An advantage Israel possessed was the excellent intelligence collected by Mossad operative Eli Cohen (who was captured and executed in Syria in 1965) regarding the Syrian battle positions. Syria had built extensive defensive fortifications in depths up to 15 kilometers, comparable to the Maginot Line.
As opposed to all the other campaigns, IAF was only partially effective in the Golan because the fixed fortifications were so effective. However, the Syrian forces proved unable to put up an effective defense largely because the officers were poor military leaders and treated their soldiers poorly; often officers would retreat to escape danger, leaving their men confused and ineffective. The Israelis also had the upper hand during close combat that took place in the numerous Syrian bunkers along the Golan Heights, as they were armed with the Uzi, a submachine gun designed for close combat, while Syrian soldiers were armed with the heavier AK-47 assault rifle, designed for combat in more open areas.
The Israeli attack: the first day.
On the morning of June 9, Israeli jets began carrying out dozens of sorties against Syrian positions from Mount Hermon to Tawfiq, using rockets salvaged from captured Egyptian stocks. The airstrikes knocked out artillery batteries and storehouses and forced transport columns off the roads. The Syrians suffered heavy casualties and a drop in morale, with a number of senior officers and troops deserting. The attacks also provided time as Israeli forces cleared paths through Syrian minefields. However, the airstrikes did not seriously damage the Syrians' bunkers and trench systems, and the bulk of Syrian forces on the Golan remained in their positions.
About two hours after the airstrikes began, the 8th Armored Brigade, led by Colonel Albert Mandler, advanced into the Golan Heights from Givat HaEm. Its advance was spearheaded by Engineering Corps sappers and eight bulldozers, which cleared away barbed wire and mines. As they advanced, the force came under fire, and five bulldozers were immediately hit. The Israeli tanks, with their maneuverability sharply reduced by the terrain, advanced slowly under fire toward the fortified village of Sir al-Dib, with their ultimate objective being the fortress at Qala. Israeli casualties steadily mounted. Part of the attacking force lost its way and emerged opposite of Za'ura, a redoubt manned by Syrian reservists. With the situation critical, Colonel Mandler ordered simultaneous assaults on Za'ura and Qala. Heavy and confused fighting followed, with Israeli and Syrian tanks struggling around obstacles and firing at extremely short ranges. Mandler recalled that "the Syrians fought well and bloodied us. We beat them only by crushing them under our treads and by blasting them with our cannons at very short range, from 100 to 500 meters." The first three Israeli tanks to enter Qala were stopped by a Syrian bazooka team, and a relief column of seven Syrian tanks arrived to repel the attackers. The Israelis took heavy fire from the houses, but could not turn back, as other forces were advancing behind them, and they were on a narrow path with mines on either side. The Israelis continued pressing forward, and called for air support. A pair of Israeli jets destroyed two of the Syrian tanks, and the remainder withdrew. The surviving defenders of Qala retreated after their commander was killed. Meanwhile, Za'ura fell in an Israeli assault, and the Israelis also captured the 'Ein Fit fortress.
In the central sector, the Israeli 181st Battalion captured the strongholds of Dardara and Tel Hillal after fierce fighting. Desperate fighting also broke out along the operation's northern axis, where Golani Brigade attacked thirteen Syrian positions, including the formidable Tel Fakhr position. Navigational errors placed the Israelis directly under the Syrians' guns. In the fighting that followed, both sides took heavy casualties, but the Israelis lost all nineteen of their tanks and half-tracks.
The Israeli battalion commander then ordered his twenty-five remaining men to dismount, divide into two groups, and charge the northern and southern flanks of Tel Fakhr. The first Israelis to reach the perimeter of the southern approach laid bodily down on the barbed wire, allowing their comrades to vault over them. From there, they assaulted the fortified Syrian positions. The fighting was waged at extremely close quarters, often hand-to-hand.
On the northern flank, the Israelis broke through within minutes and cleared out the trenches and bunkers. During the seven-hour battle, the Israelis lost 31 dead and 82 wounded, while the Syrians lost 62 dead and 20 captured. Among the dead was the Israeli battalion commander. The Golani Brigade's 51st Battalion took Tel 'Azzaziat, and Darbashiya also fell to Israeli forces.
By the evening of June 9, the four Israeli brigades had all broken through to the plateau, where they could be reinforced and replaced. Thousands of reinforcements began reaching the front, those tanks and half-tracks that had survived the previous day's fighting were refueled and replenished with ammunition, and the wounded were evacuated. By dawn, the Israelis had eight brigades in the sector.
Syria's first line of defense had been shattered, but the defenses beyond that remained largely intact. Mount Hermon and the Banias in the north, and the entire sector between Tawfiq and Customs House Road in the south remained in Syrian hands. In a meeting early on the night of June 9, Syrian leaders decided to reinforce those positions as quickly as possible, and to maintain a steady barrage on Israeli civilian settlements.
The Israeli attack: the next day.
Throughout the night, the Israelis continued their advance. Though it was slowed by fierce resistance, an anticipated Syrian counterattack never materialized. At the fortified village of Jalabina, a garrison of Syrian reservists, leveling their anti-aircraft guns, held off the Israeli 65th Paratroop Battalion for four hours before a small detachment managed to penetrate the village and knock out the heavy guns.
Meanwhile, the 8th Brigade's tanks moved south from Qala, advancing six miles to Wasit under heavy artillery and tank bombardment. At the Banaias in the north, Syrian mortar batteries opened fire on advancing Israeli forces only after Golani Brigade sappers cleared a path through a minefield, killing sixteen Israeli soldiers and wounding four.
On the next day, June 10, the central and northern groups joined in a pincer movement on the plateau, but that fell mainly on empty territory as the Syrian forces retreated. At 8:30 am, the Syrians began blowing up their own bunkers, burning documents and retreating. Several units joined by Elad Peled's troops climbed to the Golan from the south, only to find the positions mostly empty. When the 8th Brigade reached Mansura, five miles from Wasit, the Israelis met no opposition and found abandoned equipment, including tanks, in perfect working condition. In the fortified Banaias village, Golani Brigade troops found only several Syrian soldiers chained to their positions.
During the day, the Israeli units stopped after obtaining manoeuvre room between their positions and a line of volcanic hills to the west. In some locations, Israeli troops advanced after an agreed-upon cease-fire to occupy strategically strong positions. To the east, the ground terrain is an open gently sloping plain. This position later became the cease-fire line known as the "Purple Line".
"Time" magazine reported: "In an effort to pressure the United Nations into enforcing a ceasefire, Damascus Radio undercut its own army by broadcasting the fall of the city of Quneitra three hours before it actually capitulated. That premature report of the surrender of their headquarters destroyed the morale of the Syrian troops left in the Golan area."
Conclusion of conflict and post-war situation.
The United States imposed en embargo on new arms agreements to all Middle East countries, including Israel. The embargo remained in force until the end of the year, despite urgent Israeli requests to lift it.
Israel.
By June 10, Israel had completed its final offensive in the Golan Heights, and a ceasefire was signed the day after. Israel had seized the Gaza Strip, the Sinai Peninsula, the West Bank of the Jordan River (including East Jerusalem), and the Golan Heights. Overall, Israel's territory grew by a factor of three, including about one million Arabs placed under Israel's direct control in the newly captured territories. Israel's strategic depth grew to at least 300 kilometers in the south, 60 kilometers in the east, and 20 kilometers of extremely rugged terrain in the north, a security asset that would prove useful in the Yom Kippur War six years later.
The political importance of the 1967 War was immense; Israel demonstrated that it was able and willing to initiate strategic strikes that could change the regional balance. Egypt and Syria learned tactical lessons and would launch an attack in 1973 in an attempt to reclaim their lost territory.
Speaking three weeks after the war ended, as he accepted an honorary degree from Hebrew University, Yitzhak Rabin gave his reasoning behind the success of Israel:
In recognition of contributions, Rabin was given the honour of naming the war for the Israelis. From the suggestions proposed, including the "War of Daring", "War of Salvation", and "War of the Sons of Light", he "chose the least ostentatious, the Six-Day War, evoking the days of creation".
Dayan's final report on the war to the Israeli general staff listed several shortcomings in Israel's actions, including misinterpretation of Nasser's intentions, overdependence on the United States, and reluctance to act when Egypt closed the Straits. He also credited several factors for Israel's success: Egypt did not appreciate the advantage of striking first and their adversaries did not accurately gauge Israel's strength and its willingness to use it.
Egypt.
According to Heikal, Nasser had admitted his responsibility for the military defeat in June 1967. According to historian Abd al-Azim Ramadan, Nasser's mistaken decisions to expel the international peacekeeping force from the Sinai Peninsula and close the Straits of Tiran in 1967 led to a state of war with Israel, despite Egypt's lack of military preparedness.
After the 1973 Yom Kippur War, Egypt reviewed the causes of its loss of the 1967 war. Issues that were identified included "the individualistic bureaucratic leadership"; "promotions on the basis of loyalty, not expertise, and the army's fear of telling Nasser the truth"; lack of intelligence; and better Israeli weapons, command, organization, and will to fight.
The aftermath of the war.
Following the war, Israel experienced a wave of national euphoria, and the press praised the military's performance for weeks afterward. New "victory coins" were minted to celebrate. In addition, the world's interest in Israel grew, and the country's economy, which had been in crisis before the war, flourished due to an influx of tourists and donations, as well as the extraction of oil from the Sinai's wells.
The war also had a great effect on Jewish diaspora, which was swept up in overwhelming support for Israel. According to Michael Oren, the war enabled American Jews to "walk with their backs straight and flex their political muscle as never before. American Jewish organizations which had previously kept Israel at arms length suddenly proclaimed their Zionism." Record numbers of Jewish immigrants arrived from Western countries after the war, although many of them would later return to their countries of origin. Most notably, the war stirred Zionist passions among Jews in the Soviet Union, who had by that time been forcibly assimilated. Many Soviet Jews subsequently applied for exit visas and began protesting for their right to immigrate to Israel. Following diplomatic pressure from the West, the Soviet government began granting exit visas to Jews in growing numbers. From 1970 to 1988, some 291,000 Soviet Jews were granted exit visas, of whom 165,000 immigrated to Israel and 126,000 immigrated to the United States.
Following the war, Israel made an offer for peace that included the return of most of the recently captured territories. According to Chaim Herzog:
The Israeli decision was to be conveyed to the Arab nations by the United States. The U.S. was informed of the decision, but not that it was to transmit it. There is no evidence of receipt from Egypt or Syria, and some historians claim that they may never have received the offer.
In September, the Khartoum Arab Summit resolved that there would be "no peace, no recognition and no negotiation with Israel". However, as Avraham Sela notes, the Khartoum conference effectively marked a shift in the perception of the conflict by the Arab states away from one centered on the question of Israel's legitimacy toward one focusing on territories and boundaries and this was underpinned on November 22 when Egypt and Jordan accepted United Nations Security Council Resolution 242. Nasser forestalled any movement toward direct negotiations with Israel. In dozens of speeches and statements, Nasser posited the equation that any direct peace talks with Israel were tantamount to surrender.
The June 19 Israeli cabinet decision did not include the Gaza Strip, and left open the possibility of Israel permanently acquiring parts of the West Bank. On June 25–27, Israel incorporated East Jerusalem together with areas of the West Bank to the north and south into Jerusalem's new municipal boundaries.
Yet another aspect of the war touches on the population of the captured territories: of about one million Palestinians in the West Bank, 300,000 (according to the United States Department of State) fled to Jordan, where they contributed to the growing unrest. The other 600,000 remained. In the Golan Heights, an estimated 80,000 Syrians fled. Only the inhabitants of East Jerusalem and the Golan Heights became entitled to receive full Israeli citizenship, as Israel applied its law, administration and jurisdiction to these territories in 1967 and 1981 respectively, and the vast majority in both territories declined to do so. See also Israeli-Palestinian conflict and Golan Heights.
Jordan and Egypt eventually withdrew their claims to sovereignty over the West Bank and Gaza respectively. (The Sinai was returned on the basis of the Camp David Accords of 1978). After the Israeli conquest of these newly acquired 'territories', a large settlement effort was launched to secure Israel's permanent foothold. There are now hundreds of thousands of Israeli settlers in the West Bank, though the Israeli settlements in Gaza were evacuated and destroyed in August 2005 as a part of Israel's unilateral disengagement plan.
The 1967 War laid the foundation for future discord in the region.
On November 22, 1967, the United Nations Security Council adopted Resolution 242, the "land for peace" formula, which called for Israeli withdrawal "from territories occupied" in 1967 and "the termination of all claims or states of belligerency". Resolution 242 recognized the right of "every state in the area to live in peace within secure and recognized boundaries free from threats or acts of force." Israel returned the Sinai to Egypt in 1978, after the Camp David Accords, and disengaged from the Gaza Strip in the summer of 2005, though its army frequently re-enters Gaza for military operations and still retains control of the seaports, airports and most of the border crossings.
The aftermath of the war is also of religious significance. Under Jordanian rule, Jews were effectively barred from visiting the Western Wall (even though Article VIII of the 1949 Armistice Agreement demanded Israeli Jewish access to the Western Wall). Jewish holy sites were not maintained, and their cemeteries had been desecrated. After the annexation to Israel, each religious group was granted administration over its holy sites. For the first time since 1948, Jews could visit the Old City of Jerusalem and pray at the Western Wall, the holiest site in modern Judaism. Despite the Temple Mount's importance in Jewish tradition, the al-Aqsa Mosque has been under sole administration of a Muslim Waqf, and Jews are barred from praying on the Temple Mount, although they are allowed to visit it. In Hebron, Jews gained access to the Cave of the Patriarchs (the second most holy site in Judaism) for the first time since the 14th century (previously Jews were only allowed to pray at the entrance). Other Jewish holy sites, as Rachel's Tomb in Bethlehem and Joseph's Tomb in Nablus, also became accessible.
After following other Arab nations in declaring war, Mauritania remained in a declared state of war with Israel until about 1999. The Soviet bloc (except Romania) broke off relations with Israel.
Casualties.
Between 776 and 983 Israelis were killed and 4,517 were wounded. 15 Israeli soldiers were captured. Arab casualties were far greater. Between 9,800 and 15,000 Egyptian soldiers were listed as killed or missing in action. An additional 4,338 Egyptian soldiers were captured. Jordanian losses are estimated to be 6,000 killed or missing and 533 captured, though Gawrych cites a number of some 700 killed in action with another 2,500 wounded. The Syrians were estimated to have sustained between 1,000 and 2,500 killed in action. Between 367 and 591 Syrians were captured.
Controversies.
Preemptive strike v. unjustified attack.
At the commencement of hostilities, both Egypt and Israel announced that they had been attacked by the other country. The Israeli government later abandoned its initial position, acknowledging Israel had struck first, claiming that it was a preemptive strike in the face of a planned invasion by Egypt. On the other hand, the Arab view was that it was unjustified to attack Egypt.
Allegations of atrocities against Egyptian soldiers.
It has been alleged that Nasser did not want Egypt to learn of the true extent of his defeat and so ordered to hide from the public the killing of Egyptian army stragglers making their way back to the Suez canal zone. There have also been allegations from both Israeli and Egyptian sources that Israeli troops killed unarmed Egyptian prisoners.
Allegations of military support from the US, UK and Soviet Union.
There have been a number of allegations of direct military support of Israel during the war by the US and the UK, including the supply of equipment (despite an embargo) and the participation of US forces in the conflict. Many of these allegations and conspiracy theories have been disputed and it has been claimed that some were given currency in the Arab world to explain the Arab defeat.
It has also been claimed that the Soviet Union, in support of its Arab allies, used its naval strength in the Mediterranean to act as a major restraint on the US Navy.
America features prominently in Arab conspiracy theories purporting to explain the June 1967 defeat. Mohamed Hassanein Heikal, a confidant of Nasser, claims that President Lyndon B. Johnson was obsessed with Nasser and that Johnson conspired with Israel to bring him down. The reported Israeli troop movements seemed all the more threatening because they were perceived in the context of a US conspiracy against Egypt. Salah Bassiouny of the Foreign ministry, claims that Foreign Ministry saw the reported Israeli troop movements as credible because Israel had reached the level at which it could find strategic alliance with the United States. During the war, Cairo announced that American and British planes were participating in the Israeli attack. Nasser broke off diplomatic relations following this allegation. Nasser's image of the United States was such that he might well have believed the worst. However Anwar Sadat implied that Nasser used this deliberate conspiracy in order to accuse the United States as a political cover-up for domestic consumption. Lutfi Abd al-Qadir, the director of Radio Cairo during the late 1960s, who accompanied Nasser to his visits in Moscow, had his conspiracy theory that both the Soviets and the Western powers wanted to topple Nasser or to reduce his influence.
The USS "Liberty" incident.
On June 8, 1967, USS "Liberty", a United States Navy electronic intelligence vessel sailing 13 nmi off Arish (just outside Egypt's territorial waters), was attacked by Israeli jets and torpedo boats, nearly sinking the ship, killing 34 sailors and wounding 171. Israel said the attack was a case of mistaken identity, and that the ship had been misidentified as the Egyptian vessel "El Quseir". Israel apologized for the mistake, and paid compensation to the victims or their families, and to the United States for damage to the ship. After an investigation, the U.S. accepted the explanation that the incident was friendly fire and the issue was closed by the exchange of diplomatic notes in 1987. Others however, including the then United States Secretary of State Dean Rusk, Chief of Naval Operations at the time, Admiral Thomas Moorer, some survivors of the attack and intelligence officials familiar with transcripts of intercepted signals on the day, have rejected these conclusions as unsatisfactory and maintain that the attack was made in the knowledge that the ship was American.
Displaced populations.
Arab.
As a result of the war, a wave of Palestinians was displaced. An estimated 300,000 Palestinians left the West Bank and Gaza, most of whom settled in Jordan.
In his book "Righteous Victims", Israeli "New Historian" Benny Morris writes:
In three villages southwest of Jerusalem and at Qalqilya, houses were destroyed "not in battle, but as punishment ... and in order to chase away the inhabitants ... contrary to government ... policy," Dayan wrote in his memoirs. In Qalqilya, about a third of the homes were razed and about 12,000 inhabitants were evicted, though many then camped out in the environs. The evictees in both areas were allowed to stay and later were given cement and tools by the Israeli authorities to rebuild at least some of their dwellings.
But many thousands of other Palestinians now took to the roads. Perhaps as many as seventy thousand, mostly from the Jericho area, fled during the fighting; tens of thousands more left over the following months. Altogether, about one-quarter of the population of the West Bank, about 200–250,000 people, went into exile. ... They simply walked to the Jordan River crossings and made their way on foot to the East Bank. It is unclear how many were intimidated or forced out by the Israeli troops and how many left voluntarily, in panic and fear. There is some evidence of IDF soldiers going around with loudspeakers ordering West Bankers to leave their homes and cross the Jordan. Some left because they had relatives or sources of livelihood on the East Bank and feared being permanently cut off.
Thousands of Arabs were taken by bus from East Jerusalem to the Allenby bridge, though there is no evidence of coercion. The free Israeli-organized transportation, which began on June 11, 1967, went on for about a month. At the bridge they had to sign a document stating that they were leaving of their own free will. Perhaps as many as seventy thousand people emigrated from the Gaza Strip to Egypt and elsewhere in the Arab world.
On July 2, the Israeli government announced that it would allow the return of those 1967 refugees who desired to do so, but no later than August 10, later extended to September 13. The Jordanian authorities probably pressured many of the refugees, who constituted an enormous burden, to sign up to return. In practice only 14,000 of the 120,000 who applied were actually allowed by Israel back into the West Bank by the beginning of September. After that, only a trickle of "special cases" were allowed back, perhaps 3,000 in all. (328–29)
In addition, between 80,000 and 110,000 Syrians fled the Golan Heights, of which about 20,000 were from the city of Quneitra. According to more recent research by the Israeli daily "Haaretz", 130,000 Syrian inhabitants fled or were expelled from the territory, most of them by the Israeli army.
Jews in Arab countries.
The minority Jews living across the Arab world immediately faced persecution and expulsion, following the Israeli victory. According to historian and ambassador Michael B. Oren:
Mobs attacked Jewish neighborhoods in Egypt, Yemen, Lebanon, Tunisia, and Morocco, burning synagogues and assaulting residents. A pogrom in Tripoli, Libya, left 18 Jews dead and 25 injured; the survivors were herded into detention centers. Of Egypt's 4,000 Jews, 800 were arrested, including the chief rabbis of both Cairo and Alexandria, and their property sequestered by the government. The ancient communities of Damascus and Baghdad were placed under house arrest, their leaders imprisoned and fined. A total of 7,000 Jews were expelled, many with merely a satchel.
Jews in Communist countries.
Following the war, a series of antisemitic purges began in Communist countries. Some 11,200 Jews from Poland immigrated to Israel during the 1968 Polish political crisis and the following year.
Notes.
1. ^ Photograph: 
3.^ Both Egypt and Israel announced that they had been attacked by the other country. 
4. ^ Lenczowski 1990, p. 105–115, Citing Moshe Dayan, "Story of My Life", and Nadav Safran, "From War to War: The Arab–Israeli Confrontation, 1948–1967", p. 375 Israel clearly did not want the US government to know too much about its dispositions for attacking Syria, initially planned for June 8, but postponed for 24 hours. It should be pointed out that the attack on the Liberty occurred on June 8, whereas on June 9 at 3 am, Syria announced its acceptance of the cease-fire. Despite this, at 7 am, that is, four hours later, Israel's minister of defense, Moshe Dayan, "gave the order to go into action against Syria."
References.
</dl>

</doc>
<doc id="29329" url="http://en.wikipedia.org/wiki?curid=29329" title="Spectrum">
Spectrum

A spectrum (plural "spectra" or "spectrums") is a condition that is not limited to a specific set of values but can vary infinitely within a continuum. The word was first used scientifically within the field of optics to describe the rainbow of colors in visible light when separated using a prism. As scientific understanding of light advanced, it came to apply to the entire electromagnetic spectrum.
Spectrum has since been applied by analogy to topics outside of optics. Thus, one might talk about the "spectrum of political opinion", or the "spectrum of activity" of a drug, or the "autism spectrum". In these uses, values within a spectrum may not be associated with precisely quantifiable numbers or definitions. Such uses imply a broad range of conditions or behaviors grouped together and studied under a single title for ease of discussion.
In most modern usages of "spectrum" there is a unifying theme between extremes at either end. Some older usages of the word did not have a unifying theme, but they led to modern ones through a sequence of events set out below. Modern usages in mathematics did evolve from a unifying theme, but this may be difficult to recognize.
Etymology.
In Latin "spectrum" means "image" or "apparition", including the meaning "spectre". Spectral evidence is testimony about what was done by spectres of persons not present physically, or hearsay evidence about what ghosts or apparitions of Satan said. It was used to convict a number of persons of witchcraft at Salem, Massachusetts in the late 17th century. The word "spectrum" [Spektrum] was strictly used to designate a ghostly optical afterimage by Goethe in his "Theory of Colors" and Schopenhauer in "On Vision and Colors".
The prefix "spectro-" is used to form words relating to spectra. For example, a spectrometer is a device used to record spectra and spectroscopy is the use of a spectrometer for chemical analysis.
Physical science.
In the 17th century the word "spectrum" was introduced into optics by Isaac Newton, referring to the range of colors observed when white light was dispersed through a prism. Soon the term referred to a plot of light intensity or power as a function of frequency or wavelength, also known as a spectral density.
The term "spectrum" was expanded to apply to other waves, such as sound waves that could also be measured as a function of frequency, frequency spectrum and power spectrum of a signal. The term now applies to any signal that can be measured or decomposed along a continuous variable such as energy in electron spectroscopy or mass to charge ratio in mass spectrometry. Spectrum is also used to refer to a graphical representation of the signal as a function of the dependent variable.
Electromagnetic spectrum.
Electromagnetic spectrum refers to the full range of all frequencies of electromagnetic radiation and also to the characteristic distribution of electromagnetic radiation emitted or absorbed by that particular object. Devices used to measure an electromagnetic spectrum are called spectrograph or spectrometer. The visible spectrum is the part of the electromagnetic spectrum that can be seen by the human eye. The wavelength of visible light ranges from 390 to 700 nm. The absorption spectrum of a chemical element or chemical compound is the spectrum of frequencies or wavelengths of incident radiation. The emission spectrum refers to the spectrum of radiation emitted due to an atom or molecule making a transition from a higher to a lower energy state.
The radio spectrum is the part of the electromagnetic spectrum corresponding to frequencies lower below 300 GHz, which corresponds to wavelengths longer than about 1 mm. The microwave spectrum corresponds to frequencies between 300 MHz (0.3 GHz) and 300 GHz and wavelengths between one meter and one millimeter.
In astronomy, stellar classification is the classification of stars based on their characteristic electromagnetic spectra.
Mass spectrum.
A mass spectrum is a plot of ion abundance as a function of mass-to-charge ratio that is obtained by a mass spectrometer instrument. The mass spectrum can be used to determine the quantity and mass mass of atoms and molecules. Tandem mass spectrometry specra are used to determile molecular structure.
Energy spectrum.
In physics, the energy spectrum of a particle is the number of particles or intensity of a particle beam as a function of particle energy. Examples of techniques that produce an energy spectrum are alpha-particle spectroscopy, electron energy loss spectroscopy, and mass-analyzed ion-kinetic-energy spectrometry.
Spectrogram.
In acoustics, a spectrogram is a visual representation of the frequency spectrum of sound as a function of time or another variable.
Biological science.
Antibiotic spectrum of activity is a component of antibiotic classification. A broad-spectrum antibiotic is active against a wide range of bacteria, whereas a narrow-spectrum antibiotic is effective against specific families of bacteria. An example of a commonly used broad-spectrum antibiotic is ampicillin. An example of a narrow spectrum antibiotic is Dicloxacillin, which acts on beta-lactamase-producing Gram-positive bacteria such as "Staphylococcus aureus".
In psychiatry, the spectrum approach uses the term spectrum to describe a range of linked conditions, sometimes also extending to include singular symptoms and traits. For example, the autism spectrum describes a range of conditions classified as neurodevelopmental disorders.
Mathematics.
In mathematics, the spectrum of a matrix is the multiset of the eigenvalue of the matrix.
In functional analysis, the concept of the spectrum of a bounded operator is a generalization of the eigenvalue concept for matrices.
In algebraic topology, a spectrum is an object representing a generalized cohomology theory.
Social science.
In social science, economic spectrum is used to indicate the range of social class. In political science, the term political spectrum refers to a system of classifying political positions in one or more dimensions, for example in a range including right wing and left wing.

</doc>
<doc id="29330" url="http://en.wikipedia.org/wiki?curid=29330" title="Social dynamics">
Social dynamics

Social dynamics can refer to the behavior of groups that results from the interactions of individual group members as well to the study of the relationship between individual interactions and group level behaviors. The field of social dynamics brings together ideas from Economics, Sociology, Social Psychology, and other disciplines, and is a sub-field of complex adaptive systems or complexity science. The fundamental assumption of the field is that individuals are influenced by one another's behavior. The field is closely related to system dynamics. Like system dynamics, social dynamics is concerned with changes over time and emphasizes the role of feedbacks. However, in social dynamics individual choices and interactions are typically viewed as the source of aggregate level behavior, while system dynamics posits that the structure of feedbacks and accumulations are responsible for system level dynamics. Research in the field typically takes a behavioral approach, assuming that individuals are boundedly rational and act on local information. Mathematical and computational modeling are important tools for studying social dynamics. Because social dynamics focuses on individual level behavior, and recognizes the importance of heterogeneity across individuals, strict analytic results are often impossible. Instead, approximation techniques, such as mean field approximations from statistical physics, or computer simulations are used to understand the behaviors of the system. In contrast to more traditional approaches in economics, scholars of social dynamics are often interested in non-equilibrium, or dynamic, behavior. That is, behavior that changes over time.
References.
Weidlich, W. (1997) "Sociodynamics applied to the evolution of urban and regional structures". "Discrete Dynamics in Nature and Society", Vol. 1, pp. 85–98.
Available on line: http://www.hindawi.com/GetArticle.aspx?doi=10.1155/S1026022697000101

</doc>
<doc id="29333" url="http://en.wikipedia.org/wiki?curid=29333" title="Social evolution">
Social evolution

Social evolution is a subdiscipline of evolutionary biology that is concerned with social behaviors that have fitness consequences for individuals other than the actor. Social behaviors can be categorized according to the fitness consequences they entail for the actor and recipient.
This classification was proposed by W. D. Hamilton. He proposes that natural selection favors mutually beneficial or selfish behaviors. Hamilton's insight was to show how kin selection could explain altruism and spite.
Social evolution is also often regarded (especially, in the field of social anthropology) as evolution of social systems and structures.
In 2010, Harvard biologist E. O. Wilson, a founder of modern sociobiology, proposed a new theory of social evolution. He argued that the traditional approach of focusing on eusociality had limitations, which he illustrated primarily with examples from the insect world.

</doc>
<doc id="29336" url="http://en.wikipedia.org/wiki?curid=29336" title="Systemic functional grammar">
Systemic functional grammar

Systemic functional grammar (SFG) is a form of grammatical description originated by Michael Halliday. It is part of a social semiotic approach to language called "systemic functional linguistics". In these two terms, "systemic" refers to the view of language as "a network of systems, or interrelated sets of options for making meaning"; "functional" refers to Halliday's view that language is as it is because of what it has evolved to do (see Metafunction). Thus, what he refers to as the "multidimensional architecture of language" "reflects the multidimensional nature of human experience and interpersonal relations."
Influences.
Halliday describes his grammar as built on the work of Saussure, Louis Hjelmslev, Malinowski, J.R. Firth, and the Prague school linguists. In addition, he drew on the work of the American anthropological linguists Boas, Sapir and Whorf. His "main inspiration" was Firth, to whom he owes, among other things, the notion of language as system. Among American linguists, Benjamin Lee Whorf had "the most profound effect on my own thinking". Whorf "showed how it is that human beings do not all mean alike, and how their unconscious ways of meaning are among the most significant manifestations of their culture" 
From his studies in China, he lists Luo Changpei and Wang Li as two scholars from whom he gained "new and exciting insights into language". He credits Luo for giving him a diachronic perspective and insights into a non-Indo-European language family. From Wang Li he learnt "many things, including research methods in dialectology, the semantic basis of grammar, and the history of linguistics in China".
Basic tenets.
Some interrelated key terms underpin Halliday's approach to grammar, which forms part of his account of how language works. These concepts are: system, (meta)function, and rank. Another key term is lexicogrammar. In this view, grammar and lexis are two ends of the same continuum. 
Analysis of the grammar is taken from a trinocular perspective, meaning from three different levels. So to look at lexicogrammar, we can analyze it from two more levels, 'above'(semantic) and 'below' (phonology). This grammar gives emphasis to the view from above. 
For Halliday, grammar is described as systems not as rules, on the basis that every grammatical structure involves a choice from a describable set of options. Language is thus a "meaning potential". Grammarians in SF tradition use system networks to map the available options in a language. In relation to English, for instance, Halliday has described systems such as "mood", "agency", "theme", etc. Halliday describes grammatical systems as closed, i.e. as having a finite set of options. By contrast, lexical sets are open systems, since new words come into a language all the time.
These grammatical systems play a role in the construal of meanings of different kinds. This is the basis of Halliday's claim that language is "metafunctionally" organised. He argues that the raison d'être of language is meaning in social life, and for this reason all languages have three kinds of semantic components. All languages have resources for construing experience (the "ideational" component), resources for enacting humans' diverse and complex social relations (the "interpersonal" component), and resources for enabling these two kinds of meanings to come together in coherent text (the "textual" function). Each of the grammatical systems proposed by Halliday are related to these metafunctions. For instance, the grammatical system of 'mood' is considered to be centrally related to the expression of interpersonal meanings, 'process type' to the expression of experiential meanings, and 'theme' to the expression of textual meanings.
Traditionally the "choices" are viewed in terms of either the content or the structure of the language used. In SFG, language is analysed in three ways (strata): semantics, phonology, and lexicogrammar. SFG presents a view of language in terms of both structure (grammar) and words (lexis). The term "lexicogrammar" describes this combined approach.
Metafunctions.
From early on in his account of language, Halliday has argued that it is inherently functional. His early papers on the grammar of English make reference to the "functional components" of language, as "generalized uses of language, which, since they seem to determine the nature of the language system, require to be incorporated into our account of that system." Halliday argues that this functional organization of language "determines the form taken by grammatical structure".
Halliday refers to his functions of language as metafunctions. He proposes three general functions: the "ideational", the "interpersonal" and the "textual".
Ideational metafunction.
The ideational metafunction is the function for construing human experience. It is the means by which we make sense of "reality". Halliday divides the ideational into the logical and the experiential metafunctions. The logical metafunction refers to the grammatical resources for building up grammatical units into complexes, for instance, for combining two or more clauses into a clause complex. The experiential function refers to the grammatical resources involved in construing the flux of experience through the unit of the clause. 
The ideational metafunction reflects the contextual value of "field", that is, the nature of the social process in which the language is implicated. An analysis of a text from the perspective of the ideational function involves inquiring into the choices in the grammatical system of "transitivity": that is, process types, participant types, circumstance types, combined with an analysis of the resources through which clauses are combined. Halliday's "An Introduction to Functional Grammar" (in the third edition, with revisions by Christian Matthiessen) sets out the description of these grammatical systems.
Interpersonal metafunction.
The interpersonal metafunction relates to a text's aspects of "tenor" or interactivity. Like field, tenor comprises three component areas: the speaker/writer persona, social distance, and relative social status. Social distance and relative social status are applicable only to spoken texts, although a case has been made that these two factors can also apply to written text.
The speaker/writer persona concerns the stance, personalisation and standing of the speaker or writer. This involves looking at whether the writer or speaker has a neutral attitude, which can be seen through the use of positive or negative language. Social distance means how close the speakers are, e.g. how the use of nicknames shows the degree to which they are intimate. Relative social status asks whether they are equal in terms of power and knowledge on a subject, for example, the relationship between a mother and child would be considered unequal. Focuses here are on speech acts (e.g. whether one person tends to ask questions and the other speaker tends to answer), who chooses the topic, turn management, and how capable both speakers are of evaluating the subject.
Textual metafunction.
The textual metafunction relates to "mode"; the internal organisation and communicative nature of a text. This comprises textual interactivity, spontaneity and communicative distance. 
Textual interactivity is examined with reference to disfluencies such as hesitators, pauses and repetitions.
Spontaneity is determined through a focus on lexical density, grammatical complexity, coordination (how clauses are linked together) and the use of nominal groups. The study of communicative distance involves looking at a text’s cohesion—that is, how it hangs together, as well as any abstract language it uses.
Cohesion is analysed in the context of both lexical and grammatical as well as intonational aspects with reference to lexical chains and, in the speech register, tonality, tonicity, and tone. The lexical aspect focuses on sense relations and lexical repetitions, while the grammatical aspect looks at repetition of meaning shown through reference, substitution and ellipsis, as well as the role of linking adverbials.
Systemic functional grammar deals with all of these areas of meaning equally within the grammatical system itself.
Children’s grammar.
Michael Halliday (1973) outlined seven functions of language with regard to the grammar used by children:
Relation to other branches of grammar.
Halliday's theory sets out to explain how spoken and written texts construe meanings and how the resources of language are organised in open systems and functionally bound to meanings. It is a theory of language in use, creating systematic relations between choices and forms within the less abstract strata of grammar and phonology, on the one hand, and more abstract strata such as context of situation and context of culture on the other. It is a radically different theory of language from others which explore less abstract strata as autonomous systems, the most notable being Noam Chomsky's. Since the principal aim of systemic functional grammar is to represent the grammatical system as a resource for making meaning, it addresses different concerns. For example, it does not try to address Chomsky's thesis that there is a "finite rule system which generates all and only the grammatical sentences in a language". Halliday's theory encourages a more open approach to the definition of language as a resource; rather than focus on grammaticality as such, a systemic functional grammatical treatment focuses instead on the relative frequencies of choices made in uses of language and assumes that these relative frequencies reflect the probability that particular paths through the available resources will be chosen rather than others. Thus, SFG does not describe language as a finite rule system, but rather as a system, realised by instantiations, that is continuously expanded by the very instantiations that realise it and that is continuously reproduced and recreated with use. 
Another way to understand the difference in concerns between systemic functional grammar and most variants of generative grammar is through Chomsky's claim that "linguistics is a sub-branch of psychology". Halliday investigates linguistics more as a sub-branch of "sociology". SFG therefore pays much more attention to pragmatics and discourse semantics than is traditionally the case in formalism.
The orientation of systemic functional grammar has served to encourage several further grammatical accounts that deal with some perceived weaknesses of the theory and similarly orient to issues not seen to be addressed in more structural accounts. Examples include the model of Richard Hudson called "word grammar".
See also.
Other significant systemic functional grammarians:
Linguists also involved with the early development of the approach:

</doc>
<doc id="29340" url="http://en.wikipedia.org/wiki?curid=29340" title="Starfleet">
Starfleet

Starfleet is a fictional organization in the television and film franchise "Star Trek". Within this fictional universe it is a deep-space exploratory, peacekeeping, and military service maintained by the United Federation of Planets ("the Federation"). It is the principal means by which the Federation conducts its exploration, defense, diplomacy, and research. While the majority of its members are human and it is headquartered on Earth, Starfleet is composed of hundreds of species (which includes the majority of the franchise's protagonists).
History.
During production of early episodes of the , several details of the makeup of the "Star Trek" universe had yet to be worked out, including the operating authority for the USS "Enterprise". The terms "Star Service" (""), "Spacefleet Command" ("The Squire of Gothos"), "United Earth Space Probe Agency" ("Charlie X" and "Tomorrow Is Yesterday"), and "Space Central" ("") were all used to refer to the "Enterprise"'s operating authority before the term "Starfleet" became widespread from the episode "" onwards.
References to UESPA were inserted into episodes from later series. For example, the "Friendship One" probe (launched, on the fictional timeline, in 2067) is marked with the letters UESPA-1 in the ' episode "". Other background props included additional UESPA references, such as Captain Jean-Luc Picard's family album in "Star Trek Generations". During the production of ', some larger Starfleet insignia designs included the name "United Earth Space Probe Agency".
Many "" episodes refer to Starfleet having already been in full operation in 2119, when it funded research begun by Cochrane and Henry Archer leading to the first successful flight of Warp 3 vessels in the 2140s. This research is said to have evolved into the NX Program leading to Starfleet launching its first Warp 5-capable starship, "Enterprise" (NX-01), in 2151, followed by "Columbia" (NX-02) in 2155, as well as other vessels. However, the Starfleet that is in existence before the Federation is a different organization (the 'Earth Starfleet') than the Federation Starfleet; the Earth Starfleet is a purely scientific and exploratory organization, without any of the military functions that the Federation Starfleet provides - all military operations aboard Earth Starfleet vessels are handled by the MACO (Military Assault Command Operations) organization.
Starfleet has a Prime Directive of non-interference with developing worlds or their internal politics. This is said not to be a Human construct, but stemmed from policies originally implemented by the Vulcans, who used the attainment of warp speed as a barometer for making first contact with a civilization. The Prime Directive and Starfleet's first contact policies are at the center of several episodes in each "Star Trek" series and the film ".
Starfleet Headquarters is shown to be located on Earth, northeast of the Golden Gate Bridge in the Fort Baker area. Starfleet Academy is located in the same general area. Additionally, various episodes show Starfleet operating a series of starbases throughout Federation territory, either built as planet-side facilities or as orbital docking stations (notably the titular installation in the " series).
Mission.
Starfleet has been shown to handle diplomatic, scientific, and defense missions, although their main mandate seems to be peaceful exploration in the search of sentient life, as declared in the mission statements for different incarnations of the USS Enterprise. The flagship of Starfleet is often considered to be the starship USS "Enterprise".
In the early years of Starfleet, as seen in ', Starfleet's mission is purely exploration and is not military in any sense except for weapons designed for defensive capabilities until the retrofitting of the "Enterprise" (NX-01) and the incorporation of MACOs (Military Assault Command Operations) after the Xindi attack on Earth. It is assumed this trend continues as Starfleet adopts a more traditional military role and assumes its regular place as the exploratory and defensive arm of the United Federation of Planets. (During ', David Marcus derisively refers to Starfleet as "the military.")
Components.
Starfleet has many components, including the following:
Starfleet Academy.
As early as the original ', characters refer to attending Starfleet Academy. Later series establish it as an officer training facility with a four-year educational program. The main campus is located near Starfleet Headquarters in what is now Fort Baker, California.
Starfleet Command.
Starfleet Command is the headquarters/command center of Starfleet. The term "Starfleet Command" is first used in episode "Court Martial". Its headquarters are depicted as being in Fort Baker, across the Golden Gate from San Francisco, in ' and '. Overlooking the Command from the other side of the Golden Gate is the permanent site of the Council of the United Federation of Planets in what is now the Presidio of San Francisco. Throughout the "Star Trek" franchise, the main characters' isolation from Starfleet Command compels them to make and act upon decisions without Starfleet Command's orders or information, particularly in "" when the main protagonists have no means of contacting Earth for several years.
Starfleet Shipyards.
StarTrek.com notes that many of Starfleet's ships are built on Mare Island near San Francisco. It states:
The "Enterprise-D" and "USS Voyager" are depicted to have been constructed at a shipyard in Mars orbit.
In the 2009 film, Jim Kirk arrives at a shipyard near his home in Iowa and boards a shuttle to enlist in Starfleet; as the shuttle leaves, we see that the ship under construction there is the "Enterprise".
Starfleet Engineering Corps.
The Starfleet Engineering Corps (also called the Starfleet Corps of Engineers) is mentioned in several episodes in conjunction with projects such as hollowing out the underground laboratory complex inside the Regula I asteroid in ', the design of the "Yellowstone"-class Runabout in the alternate timeline in the ' episode "", and devising a defense against the Breen energy-dampening weapon in the ' episode "When It Rains…" As a result of these successes, Starfleet engineers have gained a reputation as the undisputed masters of technological adaptation and modification. As one minion of the Dominion in the ' episode, "" notes, Starfleet engineers are reputed to be able to "Turn rocks into replicators."
Additionally, Pocket Books has published a series of eBooks and novels in the "Starfleet Corps of Engineers" series.
Starfleet Intelligence.
Starfleet Intelligence is an intelligence agency of the United Federation of Planets. It is entrusted with foreign and domestic espionage, counter-espionage, and state security.
Starfleet Judge Advocate General.
The Starfleet Judge Advocate General (or "JAG") is the branch charged with overseeing legal matters within Starfleet. Several episodes revolve around or involve JAG officers and procedures:
Dialog in "Court Martial" reveals that a court-martial may be convened in the absence of any JAG officers by three presiding command-level officers. Additionally, dialog in "The Measure of a Man" indicates that the loss of a starship automatically leads to a JAG court-martial. Courts-martial were held following the loss of the USS "Pegasus" and USS "Stargazer". In the Voyager episode "" Tuvok states that the Captain has the authority to conduct a court-martial on the ship, given the circumstances of the ship being isolated from the Federation.
Starfleet Medical.
Starfleet Medical is the medical branch of Starfleet.
Gates McFadden, who played Dr. Beverly Crusher, left "" during its second season. The character is described during this season, and after her return, as having been assigned to Starfleet Medical.
Starfleet Operations.
Numerous starship dedication plaques identify other personnel associated with Starfleet Operations. Rear Admiral James T. Kirk served 18 months as Starfleet's Chief of Operations.
Starfleet Security.
Starfleet Security is an agency of Starfleet referred to in several episodes of ' and '. Security itself is also a branch of Starfleet first introduced in the . Main characters in subsequent series have been security officers.
Starfleet Tactical.
Starfleet Tactical is a rarely mentioned department of Starfleet responsible for planning defensive strategies as well as weapons research and development.
Different species in Starfleet.
Although Humans are the most often seen crew members onscreen, Starfleet is shown to be composed of individuals from over 150 races, with Vulcans perhaps being the most common aliens seen.
Already in ' the USS "Enterprise" and other ships have a mixed-species crew, although this does not appear to be an absolute rule; the USS "Intrepid" is shown with an all-Vulcan crew in the episode "". In later series this is confirmed with another all-Vulcan crew, from the USS "T'Kumbra", featured in the ' episode "Take Me Out to the Holosuite".
In keeping with this idea, for its first two seasons " was the only show to have an entirely Human crew owing to its being set before the formation of the Federation, though the vessel did carry Phlox, a Denobulan serving in a medical exchange program, and T'Pol, then serving as an observer from the Vulcan High Command.
" saw the introduction of Starfleet's first Klingon officer, and other races such as Bolians, Betazoids, and Trill were seen and given more central roles in later series; some of these, notably Klingons, had been shown as enemies in earlier episodes.
Various episodes show that Earth citizenship was not a necessary pre-condition to joining Starfleet. T'Pol of Vulcan is shown as the first non-human Starfleet officer, having received a commission as a Commander following the Xindi mission and her resignation from the Vulcan High Command. Even after the formation of the Federation, citizenship is still not a requirement, as several officers are from planets that are not part of the Federation. For example, Star Trek: TNG's Ensign Ro Laren, a Bajoran aboard the USS "Enterprise"-D that had graduated from Starfleet Academy. Fellow Bajoran Kira Nerys was field commissioned as a Starfleet commander so that she could aid the Cardassian resistance during the Dominion War. Also, Quinn and Icheb from "" both spoke of joining Starfleet.
An example of the process imagined by the writers is given when the character Nog, the first Ferengi to do so, attempts to apply to the Academy. Here he is told that since he is from a non-member world (Ferenginar), he requires a letter of recommendation from a command-level officer before his application can be considered, with the implication that this is the standard procedure for all non-Federation applicants to Starfleet.
In the Star Trek Expanded Universe, an example of what typically becomes of a new Federation member world's military is depicted when the Bajoran Militia is integrated into Starfleet upon Bajor's entry into the Federation.

</doc>
<doc id="29341" url="http://en.wikipedia.org/wiki?curid=29341" title="Superheterodyne receiver">
Superheterodyne receiver

In electronics, a superheterodyne receiver (often shortened to superhet) uses frequency mixing to convert a received signal to a fixed intermediate frequency (IF) which can be more conveniently processed than the original radio carrier frequency. It was invented by US engineer Edwin Armstrong in 1918 during World War I. Virtually all modern radio receivers use the superheterodyne principle. At the cost of an extra frequency converter stage, the superheterodyne receiver provides superior selectivity and sensitivity compared with simpler designs.
History.
Background.
"Superheterodyne" is a contraction of "supersonic heterodyne", where "supersonic" indicates frequencies above the range of human hearing. The word "heterodyne" is derived from the Greek roots "hetero-" "different", and "-dyne" "power". In radio applications the term derives from the "heterodyne detector" pioneered by Canadian inventor Reginald Fessenden in 1905, describing his proposed method of producing an audible signal from the Morse code transmissions of the new continuous wave transmitters. With the older spark gap transmitters then in use, the Morse code signal consisted of short bursts of a heavily modulated carrier wave, which could be clearly heard as a series of short chirps or buzzes in the receiver's headphones. However, the signal from a continuous wave transmitter did not have any such inherent modulation and Morse Code from one of those would only be heard as a series of clicks or thumps. Fessenden's idea was to run two Alexanderson alternators, one producing a carrier frequency 3 kHz higher than the other. In the receiver's detector the two carriers would beat together to produce a 3 kHz tone thus in the headphones the Morse signals would then be heard as a series of 3 kHz beeps. For this he coined the term "heterodyne" meaning "generated by a difference" (in frequency).
Invention.
The superheterodyne principle was devised in 1918 by U.S. Army Major Edwin Armstrong in France during World War I. He invented this receiver as a means of overcoming the deficiencies of early vacuum tube triodes used as high-frequency amplifiers in radio direction finding equipment. Unlike simple radio communication, which only needs to make transmitted signals audible, direction-finders measure the received signal strength, which necessitates linear amplification of the actual carrier wave.
In a triode radio-frequency (RF) amplifier, if both the plate (anode) and grid are connected to resonant circuits tuned to the same frequency, stray capacitive coupling between the grid and the plate will cause the amplifier to go into oscillation if the stage gain is much more than unity. In early designs, dozens (in some cases over 100) low-gain triode stages had to be connected in cascade to make workable equipment, which drew enormous amounts of power in operation and required a team of maintenance engineers. The strategic value was so high, however, that the British Admiralty felt the high cost was justified.
Armstrong realized that if radio direction-finding (RDF) receivers could be operated at a higher frequency, this would allow better detection of enemy shipping. However, at that time, no practical "short wave" (defined then as any frequency above 500 kHz) amplifier existed, due to the limitations of existing triodes.
It had been noticed some time before that if a regenerative receiver was allowed to go into oscillation, other receivers nearby would suddenly start picking up stations on frequencies different from those that the stations were actually transmitted on. Armstrong (and others) eventually deduced that this was caused by a "supersonic heterodyne" between the station's carrier frequency and the oscillator frequency. Thus if a station was transmitting on 300 kHz and the oscillating receiver was set to 400 kHz, the station would be heard not only at the original 300 kHz, but also at 100 kHz and 700 kHz.
Armstrong realized that this was a potential solution to the "short wave" amplification problem, since the beat frequency still retained its original modulation, but on a lower carrier frequency. To monitor a frequency of 1500 kHz for example, he could set up an oscillator at, for example, 1560 kHz, which would produce a heterodyne difference frequency of 60 kHz, a frequency that could then be more conveniently amplified by the triodes of the day. He termed this the "Intermediate Frequency" often abbreviated to "IF".
In December 1919, Major E. H. Armstrong gave publicity to an indirect method of obtaining short-wave amplification, called the super-heterodyne. The idea is to reduce the incoming frequency, which may be, say 1,500,000 cycles (200 meters), to some suitable super-audible frequency that can be amplified efficiently, then passing this current through a radio frequency amplifier and finally rectifying and carrying on to one or two stages of audio frequency amplification.
Development.
Armstrong was able to put his ideas into practice, and the technique was soon adopted by the military. However, it was less popular when commercial radio broadcasting began in the 1920s, mostly due to the need for an extra tube (for the oscillator), the generally higher cost of the receiver, and the level of technical skill required to operate it. For early domestic radios, tuned radio frequency receivers ("TRF"), also called the Neutrodyne, were more popular because they were cheaper, easier for a non-technical owner to use, and less costly to operate. Armstrong eventually sold his superheterodyne patent to Westinghouse, who then sold it to RCA, the latter monopolizing the market for superheterodyne receivers until 1930.
Early superheterodyne receivers used IFs as low as 20 kHz, often based on the self-resonance of iron-cored transformers. This made them extremely susceptible to image frequency interference, but at the time, the main objective was sensitivity rather than selectivity. Using this technique, a small number of triodes could be made to do the work that formerly required dozens of triodes.
In the 1920s, commercial IF filters looked very similar to 1920s audio interstage coupling transformers, had very similar construction and were wired up in an almost identical manner, and so they were referred to as "IF Transformers". By the mid-1930s however, superheterodynes were using much higher intermediate frequencies, (typically around 440–470 kHz), with tuned coils similar in construction to the aerial and oscillator coils. However, the name "IF Transformer" was retained and is still used today. Modern receivers typically use a mixture of ceramic resonator or SAW (surface-acoustic wave) resonators as well as traditional tuned-inductor IF transformers.
By the 1930s, improvements in vacuum tube technology rapidly eroded the TRF receiver's cost advantages, and the explosion in the number of broadcasting stations created a demand for cheaper, higher-performance receivers.
The development of the tetrode vacuum tube containing a screen grid led to a multi-element tube in which the mixer and oscillator functions could be combined, first used in the so-called autodyne mixer. This was rapidly followed by the introduction of tubes specifically designed for superheterodyne operation, most notably the pentagrid converter. By reducing the tube count, this further reduced the advantage of preceding receiver designs.
By the mid-1930s, commercial production of TRF receivers was largely replaced by superheterodyne receivers. The superheterodyne principle was eventually taken up for virtually all commercial radio and TV designs.
Design and principle of operation.
The principle of operation of the superheterodyne receiver depends on the use of heterodyning or frequency mixing. The signal from the antenna is filtered sufficiently at least to reject the "image frequency" (see below) and possibly amplified. A local oscillator in the receiver produces a sine wave, which mixes with that signal, shifting it to a specific intermediate frequency (IF), usually a lower frequency. The IF signal is itself filtered and amplified and possibly processed in additional ways. The demodulator uses the IF signal rather than the original radio frequency to recreate a copy of the original information (such as audio).
The diagram at right shows the minimum requirements for a single-conversion superheterodyne receiver design. The following essential elements are common to all superheterodyne circuits: a receiving antenna; a tuned stage, which may optionally contain amplification (RF amplifier); a variable frequency local oscillator; a frequency mixer; a band pass filter and intermediate frequency (IF) amplifier; and a demodulator plus additional circuitry to amplify or process the original audio signal (or other transmitted information).
Circuit description.
To receive a radio signal, a suitable antenna is required. This is often built into a receiver, especially in the case of AM broadcast band radios. The output of the antenna may be very small, often only a few microvolts. The signal from the antenna is tuned and may be amplified in a so-called radio frequency (RF) amplifier, although this stage is often omitted. One or more tuned circuits at this stage block frequencies that are far removed from the intended reception frequency. In order to tune the receiver to a particular station, the frequency of the local oscillator is controlled by the tuning knob (for instance). Tuning of the local oscillator and the RF stage may use a variable capacitor, or varicap diode. The tuning of one (or more) tuned circuits in the RF stage must track the tuning of the local oscillator.
Notice that the accompanying diagram shows a fixed-frequency local oscillator, as the symbol is for a fixed-frequency crystal frequency-determining device. A tuneable receiver would show a variable-frequency oscillator with operational connection to the tuned circuits of the antenna and radio-frequency amplifier stages.
Local oscillator and mixer.
The signal is then fed into a circuit where it is mixed with a sine wave from a variable frequency oscillator known as the local oscillator (LO). The mixer uses a non-linear component to produce both sum and difference beat frequencies signals, each one containing the modulation contained in the desired signal. The output of the mixer may include the original RF signal at "f"RF, the local oscillator signal at "f"LO, and the two new heterodyne frequencies "f"RF + "f"LO and "f"RF − "f"LO. The mixer may inadvertently produce additional frequencies such as third- and higher-order intermodulation products. Ideally, the IF bandpass filter removes all but the desired IF signal at "f"IF. The IF signal contains the original modulation (transmitted information) that the received radio signal had at "f"RF.
Historically, vacuum tubes were expensive, so broadcast AM receivers would save costs by employing a single tube as both a mixer and also as the local oscillator. The pentagrid converter tube would oscillate and also provide signal amplification as well as frequency shifting.
The frequency of the local oscillator "f"LO is set so the desired reception radio frequency "f"RF mixes to "f"IF. There are two choices for the local oscillator frequency because the dominant mixer products are at "f"RF ± "f"LO. If the local oscillator frequency is less than the desired reception frequency, it is called low-side injection ("f"IF = "f"RF − "f"LO); if the local oscillator is higher, then it is called high-side injection ("f"IF = "f"LO − "f"RF).
The mixer will process not only the desired input signal at fRF, but also all signals present at its inputs. There will be many mixer products (heterodynes). Most other signals produced by the mixer (such as due to stations at nearby frequencies) can be filtered out in the IF amplifier; that gives the superheterodyne receiver its superior performance. However, if "f"LO is set to "f"RF + "f"IF, then an incoming radio signal at "f"LO + "f"IF will "also" produce a heterodyne at "f"IF; this is called the "image frequency" and must be rejected by the tuned circuits in the RF stage. The image frequency is 2 "f"IF higher (or lower) than "f"RF, so employing a higher IF frequency "f"IF increases the receiver's "image rejection" without requiring additional selectivity in the RF stage.
To suppress the unwanted image, the tuning of the RF stage and the LO may need to "track" each other. In some cases, a narrow-band receiver can have a fixed tuned RF amplifier. In that case, only the local oscillator frequency is changed. In most cases, a receiver's input band is wider than its IF center frequency. For example, a typical AM broadcast band receiver covers 510 kHz to 1655 kHz (a roughly 1160 kHz input band) with a 455 kHz IF frequency; an FM broadcast band receiver covers 88 MHz to 108 MHz band with a 10.7 MHz IF frequency. In that situation, the RF amplifier must be tuned so the IF amplifier does not see two stations at the same time. If the AM broadcast band receiver LO were set at 1200 kHz, it would see stations at both 745 kHz (1200−455 kHz) and 1655 kHz. Consequently, the RF stage must be designed so that any stations that are twice the IF frequency away are significantly attenuated.. The tracking can be done with a multi-section variable capacitor or some varactors driven by a common control voltage. An RF amplifier may have tuned circuits at both its input and its output, so three or more tuned circuits may be tracked. In practice, the RF and LO frequencies need to track closely but not perfectly.
Intermediate frequency amplifier.
The stages of an intermediate frequency amplifier ("IF amplifier" or "IF strip") are tuned to a fixed frequency that does not change as the receiving frequency changes. The fixed frequency simplifies optimization of the IF amplifier. The IF amplifier is selective around its center frequency "f"IF. The fixed center frequency allows the stages of the IF amplifier to be carefully tuned for best performance (this tuning is called "aligning" the IF amplifier). If the center frequency changed with the receiving frequency, then the IF stages would have had to track their tuning. That is not the case with the superheterodyne.
Typically, the IF center frequency "f"IF is chosen to be less than the desired reception frequency "f"RF. The choice has some performance advantages. First, it is easier and less expensive to get high selectivity at a lower frequency. For the same bandwidth, a tuned circuit at a lower frequency needs a lower Q. Stated another way, for the same filter technology, a higher center frequency will take more IF filter stages to achieve the same selectivity bandwidth. Second, it is easier and less expensive to get high gain at a lower frequency. When used at high frequencies, many amplifiers show a constant gain–bandwidth product (dominant pole) characteristic. If an amplifier has a gain–bandwidth product of 100 MHz, then it would have a voltage gain of 100 at 1 MHz but only 10 at 10 MHz. If the IF amplifier needed a voltage gain of 10,000, then it would need only two stages with an IF at 1 MHz but four stages at 10 MHz.
Usually the intermediate frequency is lower than the reception frequency "f"RF, but in some modern receivers (e.g. scanners and spectrum analyzers) a higher IF frequency is used to minimize problems with image rejection or gain the benefits of fixed-tuned stages. The Rohde & Schwarz EK-070 VLF/HF receiver covers 10 kHz to 30 MHz. It has a band switched RF filter and mixes the input to a first IF of 81.4 MHz. The first LO frequency is 81.4 to 111.4 MHz, so the primary images are far away. The first IF stage uses a crystal filter with a 12 kHz bandwidth. There is a second frequency conversion (making a triple-conversion receiver) that mixes the 81.4 MHz first IF with 80 MHz to create a 1.4 MHz second IF. Image rejection for the second IF is not a major problem because the first IF provides adequate image rejection and the second mixer is fixed tuned.
In order to avoid interference to receivers, licensing authorities will avoid assigning common IF frequencies to transmitting stations. Standard intermediate frequencies used are 455 kHz for medium-wave AM radio, 10.7 MHz for broadcast FM receivers, 38.9 MHz (Europe) or 45 MHz (US) for television, and 70 MHz for satellite and terrestrial microwave equipment. To avoid tooling costs associated with these components, most manufacturers then tended to design their receivers around a fixed range of frequencies offered, which resulted in a worldwide "de facto" standardization of intermediate frequencies.
In early superhets, the IF stage was often a regenerative stage providing the sensitivity and selectivity with fewer components. Such superhets were called super-gainers or regenerodynes.
Bandpass filter.
The IF stage includes a filter and/or multiple tuned circuits in order to achieve the desired selectivity. This filtering must therefore have a band pass equal to or less than the frequency spacing between adjacent broadcast channels. Ideally a filter would have a high attenuation to adjacent channels, but maintain a flat response across the desired signal spectrum in order to retain the quality of the received signal. This may be obtained using one or more dual tuned IF transformers, a quartz crystal filter, or a multipole ceramic crystal filter.
Demodulation.
The received signal is now processed by the demodulator stage where the audio signal (or other baseband signal) is recovered and then further amplified. AM demodulation requires the simple rectification of the RF signal (so-called envelope detection), and a simple RC low pass filter to remove remnants of the intermediate frequency. FM signals may be detected using a discriminator, ratio detector, or phase-locked loop. Continuous wave (Morse code) and single sideband signals require a product detector using a so-called beat frequency oscillator, and there are other techniques used for different types of modulation. The resulting audio signal (for instance) is then amplified and drives a loudspeaker.
When so-called high-side injection has been used, where the local oscillator is at a "higher" frequency than the received signal (as is common), then the frequency spectrum of the original signal will be reversed. This must be taken into account by the demodulator (and in the IF filtering) in the case of certain types of modulation such as single sideband.
Advanced designs.
To overcome obstacles such as image response, in some cases multiple stages with two or more IFs of different values are used. For example, for a receiver that can tune from 500 kHz to 30 MHz, three frequency converters might be used, and the radio would be referred to as a "triple conversion superheterodyne";
The reason that this is done is the difficulty in obtaining sufficient selectivity in the front-end tuning with higher shortwave frequencies.
With a 455 kHz IF it is easy to get adequate front end selectivity with broadcast band (under 1600 kHz) signals. For example, if the station being received is on 600 kHz, the local oscillator will be set to 600 + 455 = 1055 kHz. But a station on 1510 kHz could also potentially produce an IF of 455 kHz and so cause image interference. However because 600 kHz and 1510 kHz are so far apart, it is easy to design the front end tuning to reject the 1510 kHz frequency.
However at 30 MHz, things are different. The oscillator would be set to 30.455 MHz to produce a 455 kHz IF, but a station on 30.910 would also produce a 455 kHz beat, so both stations would be heard at the same time. But it is virtually impossible to design an RF tuned circuit that can adequately discriminate between 30 MHz and 30.91 MHz, so one approach is to "bulk downconvert" whole sections of the shortwave bands to a lower frequency, where adequate front-end tuning is easier to arrange.
For example the ranges 29 MHz to 30 MHz; 28 MHz to 29 MHz etc. might be converted down to 2 MHz to 3 MHz, there they can be tuned more conveniently. This is often done by first converting each "block" up to a higher frequency (typically 40 MHz) and then using a second mixed to convert it down to the 2 MHz to 3 MHz range. The 2 MHz to 3 MHz "IF" is basically another self-contained superheterodyne receiver, most likely with a standard IF of 455 kHz.
Other uses.
In the case of modern television receivers, no other technique was able to produce the precise bandpass characteristic needed for vestigial sideband reception, similar to that used in the NTSC system first approved by the U.S. in 1941. By the 1980s these had been replaced with precision electromechanical surface acoustic wave (SAW) filters. Fabricated by precision laser milling techniques, SAW filters are cheaper to produce, can be made to extremely close tolerances, and are very stable in operation.
Modern designs.
Microprocessor technology allows replacing the superheterodyne receiver design by a software defined radio architecture, where the IF processing after the initial IF filter is implemented in software. This technique is already in use in certain designs, such as very low-cost FM radios incorporated into mobile phones, since the system already has the necessary microprocessor.
Radio transmitters may also use a mixer stage to produce an output frequency, working more or less as the reverse of a superheterodyne receiver.
Advantages and drawbacks of the superheterodyne design.
Superheterodyne receivers have essentially replaced all previous receiver designs. The development of modern semiconductor electronics negated the advantages of designs (such as the regenerative receiver) that used fewer vacuum tubes. The superheterodyne receiver offers superior sensitivity, frequency stability and selectivity. Compared with the tuned radio frequency receiver (TRF) design, superhets offer better stability because a tuneable oscillator is more easily realized than a tuneable amplifier. Operating at a lower frequency, IF filters can give narrower passbands at the same Q factor than an equivalent RF filter. A fixed IF also allows the use of a crystal filter or similar technologies that cannot be tuned. Regenerative and super-regenerative receivers offered a high sensitivity, but often suffer from stability problems making them difficult to operate.
Although the advantages of the superhet design are overwhelming, we note a few drawbacks that need to be tackled in practice.
Image frequency ("f"img).
One major disadvantage to the superheterodyne receiver is the problem of image frequency. In heterodyne receivers, an image frequency is an undesired input frequency equal to the station frequency plus twice the intermediate frequency. The image frequency results in two stations being received at the same time, thus producing interference. Image frequencies can be eliminated by sufficient attenuation on the incoming signal by the RF amplifier filter of the superheterodyne receiver.
For example, an AM broadcast station at 580 kHz is tuned on a receiver with a 455 kHz IF. The local oscillator is tuned to 580 + 455 = 1035 kHz. But a signal at 580 + 455 + 455 = 1490 kHz is also 455 kHz away from the local oscillator; so both the desired signal and the image, when mixed with the local oscillator, will also appear at the intermediate frequency. This image frequency is within the AM broadcast band. Practical receivers have a tuning stage before the converter, to greatly reduce the amplitude of image frequency signals; additionally, broadcasting stations in the same area have their frequencies assigned to avoid such images.
The unwanted frequency is called the "image" of the wanted frequency, because it is the "mirror image" of the desired frequency reflected formula_2. A receiver with inadequate filtering at its input will pick up signals at two different frequencies simultaneously: the desired frequency and the image frequency. Any noise or random radio station at the image frequency can interfere with reception of the desired signal.
Early Autodyne receivers typically used IFs of only 150 kHz or so, as it was difficult to maintain reliable oscillation if higher frequencies were used. As a consequence, most Autodyne receivers needed quite elaborate antenna tuning networks, often involving double-tuned coils, to avoid image interference. Later superhets used tubes especially designed for oscillator/mixer use, which were able to work reliably with much higher IFs, reducing the problem of image interference and so allowing simpler and cheaper aerial tuning circuitry.
Sensitivity to the image frequency can be minimised only by (1) a filter that precedes the mixer or (2) a more complex mixer circuit that suppresses the image. In most receivers this is accomplished by a bandpass filter in the RF front end. In many tunable receivers, the bandpass filter is tuned in tandem with the local oscillator.
Image rejection is an important factor in choosing the intermediate frequency of a receiver. The farther apart the bandpass frequency and the image frequency are, the more the bandpass filter will attenuate any interfering image signal. Since the frequency separation between the bandpass and the image frequency is formula_3, a higher intermediate frequency improves image rejection. It may be possible to use a high enough first IF that a fixed-tuned RF stage can reject any image signals.
The ability of a receiver to reject interfering signals at the image frequency is measured by the image rejection ratio. This is the ratio (in decibels) of the output of the receiver from a signal at the received frequency, to its output for an equal-strength signal at the image frequency.
Local oscillator radiation.
It is difficult to keep stray radiation from the local oscillator below the level that a nearby receiver can detect. The receiver's local oscillator can act like a low-power CW transmitter. Consequently, there can be mutual interference in the operation of two or more superheterodyne receivers in close proximity.
In intelligence operations, local oscillator radiation gives a means to detect a covert receiver and its operating frequency. The method was used by MI-5 during Operation RAFTER. This same technique is also used in radar detector detectors used by traffic police in jurisdictions where radar detectors are illegal.
A method of significantly reducing the local oscillator radiation from the receiver's antenna is to use an RF amplifier between the receiver's antenna and its mixer stage.
Local oscillator sideband noise.
Local oscillators typically generate a single frequency signal that has negligible amplitude modulation but some random phase modulation. Either of these impurities spreads some of the signal's energy into sideband frequencies. That causes a corresponding widening of the receiver's frequency response, which would defeat the aim to make a very narrow bandwidth receiver such as to receive low-rate digital signals. Care needs to be taken to minimize oscillator phase noise, usually by ensuring that the oscillator never enters a non-linear mode.

</doc>
<doc id="29344" url="http://en.wikipedia.org/wiki?curid=29344" title="Seventh Day Baptists">
Seventh Day Baptists

Seventh Day Baptists are Christian Baptists who observe Sabbath on the seventh-day of the week, Saturday, in accordance with the Biblical Sabbath of the ten commandments (Exodus 20:8, deuteronomy 5:12). The Seventh Day Baptist World Federation today represents over 50,000 Baptists in 22 countries.
History.
Seventh Day Baptists trace the beginning of their movement to coalescing factors during the decade of the 1650s in England. These factors included the continuing Baptist movement in England, English language publications about the Sabbath in the early 1600s, and a relative freedom of religion from state interference in Oliver Cromwell's commonwealth. Once the factors had coalesced, individuals associated with the movement chose to accept punishment meted out by the State rather than renounce their Sabbath conviction.
The first recorded "Seventh Day Baptist" meeting was held at The Mill Yard Church in London in 1651 under the leadership of Dr. Peter Chamberlen. However many Seventh Day Baptists believe that it had originated in 1617 with John Trask and his wife. They believe that the records for this were lost in a fire.
The first "Seventh Day Baptist" church in America was at Newport, Rhode Island in December 1671. Samuel and Tacy Hubbard, two members of the First Baptist Church of Newport, pastored by John Clarke (1609–76), withdrew from that church and joined with Stephen Mumford, a "Seventh Day Baptist" from England, and 4 others, covenanting to meet together for worship, calling themselves Sabbatarian Baptists. Mumford, for his part, arrived in Rhode Island in 1665, and was mentioned as an advocate for seventh-day Sabbath in many records of that time. Other churches rose in Pennsylvania and New Jersey, and soon spread north into Connecticut and New York, and south into Virginia and the Carolinas. Seventh-day Sabbatarianism also emerged among the Germans at Ephrata, Pennsylvania, founded in 1735. Ephrata was incorporated as the German Religious Society of Seventh Day Baptists in 1814, and the site where their community was founded came to be known at the Ephrata Cloister. The Seventh Day Baptist General Conference was organized in 1801.
The World Field.
In 1995, the "Seventh Day Baptists" had 253 churches and over 20 000 members in India, 78 churches with 4885 members in the United States, 2 churches with 55 members in England, and 1 church of 40 members in Canada. Conferences and associations exist in many other countries including Australia, Brazil, India, Jamaica, the Netherlands, New Zealand and amongst the Seventh Day Christians of Poland. Some conferences have sent missionaries to other nations including Malawi, Fiji and Argentina. The Seventh Day Baptist World Federation was founded in 1964–65, and it now represents over 50 000 Baptists in 17 member organizations in 22 countries.
Baptist Beliefs.
Other than the belief that Christian Sabbath is Saturday rather than Sunday, Seventh Day Baptists are very similar to other Baptists. However, due to the Baptist tradition of freedom of conscience, even within Baptists, there are a lot of variations in doctrines. The same principle applies to Seventh Day Baptists. The Seventh Day Baptists do not hold a binding creed, and the belief system is relatively more flexible than mainstream Christianity, and the teachings Seventh Day Baptists hold may also vary from member to member.
Some of the basic beliefs are baptism of believers by immersion; 
the practice of a non-liturgical form of worship, and the belief of religious freedom and the separation of church and state.
Statement of Belief.
Each church and association of Seventh Day Baptist churches may have a statement of belief. A representative statement, from the conference of the USA and Canada, is as follows:
Introduction.
Seventh Day Baptists consider liberty of thought under the guidance of the Holy Spirit to be essential to Christian belief and practice. Therefore we encourage the unhindered study and open discussion of Scripture. We uphold the individual's freedom of conscience in seeking to determine and obey the will of God.
The following statement is not intended to be exhaustive, but is an expression of our common belief, which is derived from our understanding of Scripture.
I. God.
We believe in one God, infinite and perfect, the Creator and Sustainer of the universe who exists eternally in three persons—Father, Son, and Holy Spirit—and desires to share His love in a personal relationship with everyone.
The Father.
We believe in God the Father, who is sovereign over all, and is loving and just as He forgives the repentant and condemns the unrepentant.
The Son.
We believe in God the Son, who became incarnate in Jesus Christ, our Lord and Savior. He gave Himself on the cross as the complete and final sacrifice for sin. As our Risen Lord, He is the mediator between God the Father and mankind.
The Holy Spirit.
We believe in God the Holy Spirit, the Comforter, who gives spiritual birth to believers lives within them, and empowers them for witnessing and service. We believe the Holy Spirit inspired the Scriptures, convicts of sin and instructs in righteousness.
II. The Bible.
We believe that the Bible is the inspired Word of God and is our final authority in matters of faith and practice. We believe that Jesus Christ, in His life and teachings as recorded in the Bible, is the supreme interpreter of God's will for mankind.
III. Mankind.
We believe that mankind was created in the image of God and is therefore the noblest work of creation. We believe that human beings have moral responsibility and are created to enjoy both divine and human fellowship as children of God.
IV. Sin and Salvation.
We believe that sin is disobedience to God and failure to live according to His will. Because of sin all people have separated themselves from God. We believe that because we are sinners, we are in need of a Savior.
We believe that salvation from sin and death is the gift of God by redeeming love accomplished by Christ's death and resurrection, and is received only by repentance and faith in Him. We believe that all who repent of their sin and receive Christ as Savior will not be punished at the final judgment but enjoy eternal life.
V. Eternal Life.
We believe that Jesus rose from the dead and lives eternally with the Father, and that He will come again with power and great glory. We believe that eternal life begins in knowing God through a commitment to Jesus Christ. We believe that because He died and lives again, resurrection with spiritual and imperishable bodies is the gift of God to believers.
VI. The Church.
We believe that the church of God is all believers gathered by the Holy Spirit and joined into one body, of which Christ is the Head. We believe that the local church is a community of believers organized in covenant relationship for worship, fellowship and service, practicing and proclaiming common convictions, while growing in grace and in the knowledge of our Lord and Savior Jesus Christ.
We believe in the priesthood of all believers and practice the autonomy of the local congregation, as we seek to work in association with others for more effective witness.
VII. Baptism.
We believe that baptism of believers in obedience to Christ's command is a witness to the acceptance of Jesus Christ as Savior and Lord. We believe in baptism by immersion as a symbol of death to sin, a pledge to a new life in Him.
VIII. The Lord's Supper.
We believe that the Lord's Supper commemorates the suffering and death of our Redeemer until He comes, and is a symbol of union in Christ and a pledge of renewed allegiance to our risen Lord.
IX. Sabbath.
We believe that the Sabbath of the Bible, the seventh day of the week, is sacred time, a gift of God to all people, instituted at creation, affirmed in the Ten Commandments and reaffirmed in the teaching and example of Jesus and the apostles.
We believe that the gift of Sabbath rest is an experience of God's eternal presence with His people.
We believe that in obedience to God and in loving response to His grace in Christ, the Sabbath should be faithfully observed as a day of rest, worship, and celebration.
X. Evangelism.
We believe that Jesus Christ commissions us to proclaim the Gospel, to make disciples, to baptize and to teach observance of all that He has commanded. We are called to be witnesses for Christ throughout the world and in all human relationships.
Organizational Structure and Offices.
Offices of the General Conference for the USA and Canada are maintained in Janesville, Wisconsin. The Missionary Society offices are in Westerly, Rhode Island, and the Board of Christian Education has offices in Alfred Station, New York. The Seventh Day Baptist General Conference (USA and Canada) is a member of the Baptist World Alliance. The current General Secretary of the Seventh Day Baptist World Federation (founded in 1965) is Pastor Jan Lek, of the SDB Church of Amsterdam (the Netherlands).

</doc>
<doc id="29345" url="http://en.wikipedia.org/wiki?curid=29345" title="Shem">
Shem

Shem (; Hebrew: שֵם,  "Shem",  "Šēm"; Greek: Σημ "Sēm"; Ge'ez: ሴም, "Sēm"; "renown; prosperity; name"; Arabic: سام "Sām") was one of the sons of Noah in the Hebrew Bible as well as in Islamic literature. According to some Rabbinic traditions, Shem was born without a foreskin (aposthia); which may indicate a basis for circumcision that predates the covenant of Abraham. There is however, no explicit indication of this in the Genesis text. Genesis 10:21 refers to relative ages of Shem and his brother Japheth, but with sufficient ambiguity to have yielded different English translations. The verse is translated in the KJV as "Unto Shem also, the father of all the 
children of Eber, the brother of Japheth the elder, even to him were children born.". However, the New American Standard Bible gives, "Also to Shem, the father of all the children of Eber, and the older brother of Japheth, children were born."
Genesis 11:10 records that Shem was 
still 100 years old at the birth of Arphaxad, two years after the flood; and that he lived for another 500 years after this, making his age at death 600 years.
The children of Shem were Elam, Asshur, Arphaxad, Lud and Aram, in addition to daughters. Abraham, the patriarch of the Hebrews and Arabs, was one of the descendants of Arphaxad.
Islamic literature describes Shem as one of the believing sons of Noah. Some sources even identify Shem as a 
prophet in his own right and that he was the next prophet after his father. In one Muslim legend, Shem was one of the people that God made Jesus resurrect as a sign to the Children of Israel.
The 1st-century historian Flavius Josephus, among many others, recounted the tradition that these five sons were the progenitors of the nations of Elam, Assyria, Chaldea, Lydia, and Levantine, respectively.
The associated term "Semitic" is still a commonly used term for the Semitic languages, as a subset of the Afro-Asiatic languages, denoting the common linguistic heritage of Arabic, Aramaic, Akkadian, Ethiopic, Hebrew and Canaanite-Phoenician languages.
According to some Jewish traditions (e.g., B. Talmud Nedarim 32b; Genesis Rabbah 46:7; Genesis Rabbah 56:10; Leviticus Rabbah 25:6; Numbers Rabbah 4:8.), Shem is believed to have been Melchizedek, King of Salem whom Abraham is recorded to have met after the battle of the four kings.
Shem is mentioned in Genesis 5:32, 6:10; 7:13; 9:18,23,26-27; 10; 11:10; also in 1 Chronicles 1:4.
Proposed lineages from Shem.
Descendants in Genesis 10 and 11.
<poem>
According to the Bible, Genesis 10:22-31
22 The children of Shem: Elam, and Asshur, and Arphaxad and Lud and Aram.
23 And the children of Aram; Uz and Hul, and Gether and Mash.
24 And Arphaxad begat Salah and Salah begat Eber.
25 And unto Eber were born two sons: the name of one "[was]" Peleg; for in his days was the earth divided; <br>and his brother's name "[was]" Joktan.
26 And Joktan begat Almodad, and Sheleph, and Hazarmaveth, and Jerah.
27 And Hadoram, and Uzal and Diklah,
28 And Obal, and Abimael and Sheba,
29 And Ophir, and Havilah, and Jobab: all these were the sons of Joktan.
30 And their dwelling was from Mesha, as thou goest unto Sephar a mount of the east 
31 These "[are]" the sons of Shem, after their families, after their tongues, in their lands, after their nations.
</poem>
Excerpts from Genesis 11:10-26—
<poem>
Shem [was] an hundred years old, and begat Arphaxad two years after the flood ...
Arphaxad lived five and thirty years, and begat Salah ...
Salah lived thirty years, and begat Eber ...
Eber lived four and thirty years, and begat Peleg ...
Peleg lived thirty years, and begat Reu ...
Reu lived two and thirty years, and begat Serug ...
Serug lived thirty years, and begat Nahor ...
Nahor lived nine and twenty years, and begat Terah ...
Terah lived seventy years, and begat Abram, Nahor, and Haran ... and Haran begat Lot
</poem>
Genealogies according to "Book of Jasher".
A rabbinic document that surfaced in the 17th century, claiming to be the lost "Book of Jasher" provides some names not found in any other source. Some have reconstructed more complete genealogies based on this information as follows:
Shem. Also Sem Literal meanings are named or renown (father of the Semitic races - Shemites). The sons of Shem were: 

</doc>
<doc id="29346" url="http://en.wikipedia.org/wiki?curid=29346" title="Sambuca">
Sambuca

Sambuca (]) is an Italian anise-flavoured, usually colourless, liqueur. Its most common variety is often referred to as "white sambuca" to differentiate it from other varieties that are deep blue in colour ("black sambuca") or bright red ("red sambuca"). Like other anise-flavoured liqueurs, the ouzo effect is sometimes observed when combined with water.
Ingredients.
Sambuca is flavoured with essential oils obtained from anise, star anise, liquorice and other spices. It also contains elderflowers. The oils are added to pure alcohol, a concentrated solution of sugar, and other flavouring. It is commonly bottled at 42% alcohol by volume.
History.
The etymology is disputed: the Molinari company states that the name "Sambuca" comes from an Arabic word: "Zammut". This was the name of an anise-flavoured drink that arrived to the port of Civitavecchia by ships coming from the East. The "Oxford English Dictionary" states, however, that the term comes from the Latin word "sambucus", meaning "elderberry".
The Greek word "Sambuca" was first used as the name of another elderberry liquor that was created in Civitavecchia about 130 years ago.
The first commercial version of such a drink started at the end of 1800 in Civitavecchia, where Luigi Manzi sold "Sambuca Manzi". In 1945, soon after the end of Second World War, commendatore Angelo Molinari started producing "Sambuca Extra Molinari", which helped popularise Sambuca throughout Italy.
Serving.
Sambuca may be served neat. It may also be served on the rocks or with water, resulting in the ouzo effect from the anethole in the anise.
Sambuca is considered to go particularly well with coffee. Like other anise liqueurs, it may be drunk after coffee as a ammazzacaffè or added directly to coffee in place of sugar.
The most iconic serving of sambuca is a shot with three coffee beans, called "con la mosca", which means "with the fly". The three coffee beans are said to represent health, happiness and prosperity, or the Holy Trinity. The shot may be ignited to toast the coffee beans with the flame extinguished immediately before drinking.

</doc>
<doc id="29349" url="http://en.wikipedia.org/wiki?curid=29349" title="Sweeney Todd">
Sweeney Todd

Sweeney Todd is a fictional character who first appeared as the main protagonist of the Victorian penny dreadful "The String of Pearls" (1846–47).
The tale became a staple of Victorian melodrama and London urban legend, and has been retold many times since, most notably in the Tony award-winning by Stephen Sondheim and Hugh Wheeler.
Claims that Sweeney Todd was a historical person are strongly disputed by scholars, although possible legendary prototypes exist.
Plot synopsis.
In the original version of the tale, Todd is a barber who dispatches his victims by pulling a lever as they sit in his barber chair. His victims fall backward down a revolving trapdoor into the basement of his shop, generally causing them to break their necks or skulls. In case they are alive, Todd goes to the basement and "polishes them off" (slitting their throats with his straight razor). In some adaptations, the murdering process is reversed, with Todd slitting his customers' throats before dispatching them into the basement through the revolving trapdoor. After Todd has robbed his dead victims of their goods, Mrs. Lovett, his partner in crime (in some later versions, his friend and/or lover), assists him in disposing of the bodies by baking their flesh into meat pies and selling them to the unsuspecting customers of her pie shop. Todd's barber shop is situated at 186 Fleet Street, London, next to St. Dunstan's church, and is connected to Mrs. Lovett's pie shop in nearby Bell Yard by means of an underground passage. In most versions of the story, he and Mrs. Lovett hire an unwitting orphan boy, Tobias Ragg, to serve the pies to customers.
Literary history.
Sweeney Todd first appeared in a story titled "The String of Pearls: A Romance". This penny dreadful was published in 18 weekly parts, in Edward Lloyd's "The People's Periodical and Family Library", issues 7–24, 21 November 1846 to 20 March 1847. It was probably written by James Malcolm Rymer, though Thomas Peckett Prest has also been credited with it; possibly each worked on the serial from part to part. Other attributions include Edward P. Hingston, George Macfarren, and Albert Richard Smith. In February/March 1847, before the serial was even completed, George Dibden Pitt adapted "The String of Pearls" as a melodrama for the Britannia Theatre in Hoxton. It was in this alternative version of the tale, rather than the original, that Todd acquired his catchphrase: "I'll polish him off".
Lloyd published another, lengthier, penny part serial from 1847–48, with 92 episodes. It was then published in book form in 1850 as "The String of Pearls", subtitled "The Barber of Fleet Street. A Domestic Romance". This expanded version of the story was 732 pages long. A plagiarised version of this book appeared in America c. 1852–53 as "Sweeney Todd: or the Ruffian Barber. A Tale of Terror of the Seas and the Mysteries of the City" by "Captain Merry" (a pseudonym for American author Harry Hazel, 1814–89).
In 1865 the French novelist Paul H.C. Féval (1816-1887) famous as a writer of horror and crime novels and short stories, referred to what he called "L'Affaire de la Rue des Marmousets", in the introductory chapter to his book "La Vampire". A version of this story is related by the author Jacques Yonnet in his book Rue des maléfices (1954). This version is set in late medieval (1387) Paris, at the corner of the Rue des Marmousets and the Rue des Deux-Hermites. The familiar plot of the barber and the pastrycook who sell pies made with human flesh is followed, the "dénouement" following one of the victims' dogs alerting neighbors and the gendarmes. The two confess, and are summarily burned alive; the houses where the crimes took place are then razed. Whether this version of the story is based on "The String of Pearls" or its dramatisation, or a much older tale alluded to by Féval, is unclear, in any case, it may well be the source for some recent versions that move the tale from London to Paris.
In 1875, Frederick Hazleton's c. 1865 dramatic adaptation "Sweeney Todd, the Barber of Fleet Street: or the String of Pearls" (see below) was published as Vol 102 of "Lacy's Acting Edition of Plays".
A scholarly, annotated edition of the original 1846–47 serial was published in volume form in 2007 by the Oxford University Press under the title of "Sweeney Todd: The Demon Barber of Fleet Street", edited by Robert Mack.
Alleged historical basis.
The original story of Sweeney Todd was quite possibly based on an older urban legend, originally based on dubious pie-fillings. In Charles Dickens' "Pickwick Papers" (1836–37), the servant Sam Weller says that a pieman used cats "for beefsteak, veal and kidney, 'cording to the demand", and recommends that people should buy pies only "when you know the lady as made it, and is quite sure it ain't kitten." Dickens then developed this in "Martin Chuzzlewit" (1843–44), published two years before the appearance of Sweeney Todd in "The String of Pearls" (1846–47), with a character called Tom Pinch who is grateful that his own "evil genius did not lead him into the dens of any of those preparers of cannibalic pastry, who are represented in many country legends as doing a lively retail business in the metropolis".
Claims that Sweeney Todd was a real person were first made in the introduction to the 1850 (expanded) edition of "The String of Pearls" and have persisted to the present day. In two books, Peter Haining argued that Sweeney Todd was an historical figure who committed his crimes around 1800. Nevertheless, other researchers who have tried to verify his citations find nothing in these sources to back Haining's claims. A check of the website Old Bailey for "Associated Records 1674–1834", for an alleged trial in December 1801 and hanging of Sweeney Todd for January 1802, shows no reference; the only murder trial for this period is that of a Governor/Lt Col. Joseph Wall, who was hanged 28 January 1802 for killing a Benjamin Armstrong on 10 July 1782 on the isle of Gorée, West Africa, and the discharge of a Humphrey White in January 1802.
In literature.
A late (1890s) reference to the urban legend of the murdering barber can be found in the poem by the Australian bush poet Banjo Paterson—"The Man from Ironbark".
In his 2012 novel" Dodger", Terry Pratchett portrays Sweeney Todd as a tragic figure, having lost his mind after being exposed to the horrors of the Napoleonic Wars as a barber surgeon.
In performing arts.
In stage productions.
In January 2014[ [update]], a world premiere adaptation for the stage exploring the Sweeney Todd tale based on the original Penny Dreadful story “The String Of Pearls” will be performed at the Lyceum Theatre, Crewe, England.
In rhyming slang.
In rhyming slang, Sweeney Todd is the Flying Squad (a branch of the UK's Metropolitan Police), which inspired the television series "The Sweeney".

</doc>
<doc id="29352" url="http://en.wikipedia.org/wiki?curid=29352" title="Selection sort">
Selection sort

In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O("n"2) time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.
Example.
Here is an example of this sort algorithm sorting five elements:
Selection sort can also be used on list structures that make add and remove efficient, such as a linked list. In this case it is more common to "remove" the minimum element from the remainder of the list, and then "insert" it at the end of the values sorted so far. For example:
Analysis.
Selection sort is not difficult to analyze compared to other sorting algorithms since none of the loops depend on the data in the array. Selecting the lowest element requires scanning all "n" elements (this takes "n" − 1 comparisons) and then swapping it into the first position. Finding the next lowest element requires scanning the remaining "n" − 1 elements and so on, for ("n" − 1) + ("n" − 2) + ... + 2 + 1 = "n"("n" − 1) / 2 ∈ Θ("n"2) comparisons (see arithmetic progression). Each of these scans requires one swap for "n" − 1 elements (the final element is already in place).
Comparison to other sorting algorithms.
Among simple average-case Θ("n"2) algorithms, selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the "k"th iteration, the first "k" elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the "k" + 1st element, while selection sort must scan all remaining elements to find the "k" + 1st element.
Simple calculation shows that insertion sort will therefore usually perform about half as many comparisons as selection sort, although it can perform just as many or far fewer depending on the order the array was in prior to sorting. It can be seen as an advantage for some real-time applications that selection sort will perform identically regardless of the order of the array, while insertion sort's running time can vary considerably. However, this is more often an advantage for insertion sort in that it runs much more efficiently if the array is already sorted or "close to sorted."
While selection sort is preferable to insertion sort in terms of number of writes (Θ("n") swaps versus Ο("n"2) swaps), it almost always far exceeds (and never beats) the number of writes that cycle sort makes, as cycle sort is theoretically optimal in the number of writes. This can be important if writes are significantly more expensive than reads, such as with EEPROM or Flash memory, where every write lessens the lifespan of the memory.
Finally, selection sort is greatly outperformed on larger arrays by Θ("n" log "n") divide-and-conquer algorithms such as mergesort. However, insertion sort or selection sort are both typically faster for small arrays (i.e. fewer than 10–20 elements). A useful optimization in practice for the recursive algorithms is to switch to insertion sort or selection sort for "small enough" sublists.
Variants.
Heapsort greatly improves the basic algorithm by using an implicit heap data structure to speed up finding and removing the lowest datum. If implemented correctly, the heap will allow finding the next lowest element in Θ(log "n") time instead of Θ("n") for the inner loop in normal selection sort, reducing the total running time to Θ("n" log "n").
A bidirectional variant of selection sort, called cocktail sort, is an algorithm which finds both the minimum and maximum values in the list in every pass. This reduces the number of scans of the list by a factor of 2, eliminating some loop overhead but not actually decreasing the number of comparisons or swaps. Note, however, that cocktail sort more often refers to a bidirectional variant of bubble sort.
Selection sort can be implemented as a stable sort. If, rather than swapping in step 2, the minimum value is inserted into the first position (that is, all intervening items moved down), the algorithm is stable. However, this modification either requires a data structure that supports efficient insertions or deletions, such as a linked list, or it leads to performing Θ("n"2) writes.
In the bingo sort variant, items are ordered by repeatedly looking through the remaining items to find the greatest value and moving all items with that value to their final location. Like counting sort, this is an efficient variant if there are many duplicate values. Indeed, selection sort does one pass through the remaining items for each item moved. Bingo sort does one pass for each value (not item): after an initial pass to find the biggest value, the next passes can move every item with that value to its final location while finding the next value as in the following pseudocode (arrays are zero-based and the for-loop includes both the top and bottom limits, as in Pascal):
Thus, if on average there are more than two items with the same value, bingo sort can be expected to be faster because it executes the inner loop fewer times than selection sort.
References.
</dl>

</doc>
<doc id="29353" url="http://en.wikipedia.org/wiki?curid=29353" title="Syracuse University">
Syracuse University

Syracuse University, commonly referred to as Syracuse, 'Cuse, or SU, is a private research university located in Syracuse, New York. The institution's roots can be traced to the Genesee Wesleyan Seminary (later becoming Genesee College), founded by the Methodist Episcopal Church in Lima, New York, in 1831. Following several years of debate over relocating the college to Syracuse, the university was established in 1870, independent of the college. Since 1920, the university has identified itself as nonsectarian, although it maintains a relationship with The United Methodist Church.
The campus is located in the University Hill neighborhood of Syracuse, east and southeast of downtown, on one of the larger hills. Its large campus features an eclectic mix of buildings, ranging from nineteenth-century Romanesque Revival structures to contemporary buildings. SU is organized into 13 schools and colleges, with nationally recognized programs in information studies and library science, architecture, communications, business administration, sport management, public administration, engineering and the College of Arts and Sciences.
Syracuse University athletic teams, known as the Orange, participate in 20 intercollegiate sports. SU is a member of the Atlantic Coast Conference for all NCAA Division I athletics, except for women's ice hockey, and the rowing team. SU is also a member of the Eastern College Athletic Conference.
History.
Founding.
The Genesee Wesleyan Seminary was founded in 1831 by the Genesee Annual Conference of the Methodist Episcopal Church in Lima, New York, south of Rochester. In 1850, it was resolved to enlarge the institution from a seminary into a college, or to connect a college with the seminary, becoming Genesee College. However, the location was soon thought by many to be insufficiently central. Its difficulties were compounded by the next set of technological changes: the railroad that displaced the Erie Canal as the region's economic engine bypassed Lima completely. The trustees of the struggling college then decided to seek a locale whose economic and transportation advantages could provide a better base of support.
The college began looking for a new home at the same time that Syracuse, ninety miles to the east, was engaged in a search to bring a university to the city, having failed to convince Ezra Cornell and Andrew Dickson White to locate Cornell University there rather than in Ithaca. Syracuse resident White pressed that the new university should locate on the hill in Syracuse (the current location of Syracuse University) due to the city's attractive transportation hub, which would ease the recruitment of faculty, students, and other persons of note. However, as a young carpenter working in Syracuse, Cornell had been twice robbed of his wages, and thereafter considered Syracuse a Sodom and Gomorrah insisting that the university be located in Ithaca on his large farm on East Hill, overlooking the town and Cayuga Lake.
Meanwhile, there were several years of dispute between the Methodist ministers, Lima, and contending cities across the state, over proposals to move Genesee College to Syracuse. At the time, the ministers wanted a share of the funds from the Morrill Land Grant Act for Genesee College. Eventually, they agreed to a "quid pro quo" donation of $25,000 from Senator Cornell in exchange for their (Methodist) support for his bill. Cornell insisted the bargain be written into the bill and Cornell became New York State's Land Grant University in 1865. In 1869, Genesee College obtained New York State approval to move to Syracuse, but Lima got a court injunction to block the move, and Genesee stayed in Lima until it was dissolved in 1875. At its founding on March 24, 1870, the state of New York granted Syracuse University its charter independent of Genesee College. The City of Syracuse offered $100,000 to establish the school. Bishop Jesse Truesdell Peck donated $25,000 to the proposed school and was elected the first president of the Board of Trustees.
Rev. Daniel Steele, a former Genesee College president, served as the first administrative leader of Syracuse until its Chancellor was appointed. The university opened in September 1871 in rented space downtown. George F. Comstock, a member of the new University's Board of Trustees, had offered the school 50 acre of farmland on a hillside to the southeast of the city center. Comstock intended Syracuse University and the hill to develop as an integrated whole; a contemporary account described the latter as "a beautiful town ... springing up on the hillside and a community of refined and cultivated membership ... established near the spot which will soon be the center of a great and beneficent educational institution."
The university was founded as coeducational, and President Peck stated at the opening ceremonies, "The conditions of admission shall be equal to all persons... there shall be no invidious discrimination here against woman... brains and heart shall have a fair chance... " Syracuse implemented this policy with a high proportion of women students. In the College of Liberal Arts, the ratio between male and female students during the 19th century was approximately even. The College of Fine Arts was predominantly female, and a low ratio of women enrolled in the College of Medicine and the College of Law. Men and women were taught together in the same courses, and many extra-curricular activities were coeducational as well. Syracuse also developed "women-only" organizations and clubs.
Expansion.
Coeducation at Syracuse traced its roots to the early days of Genesee College where educators and students like Frances Willard and Belva Lockwood were heavily influenced by the Women's movement in nearby Seneca Falls, NY. However, the progressive "co-ed" policies practiced at Genesee would soon find controversy at the new university in Syracuse. Colleges and universities admitted few women students in the 1870s. Administrators and faculty argued women had inferior minds and could not master mathematics and the classics. Dr. Erastus Otis Haven, Syracuse University chancellor and former president of the University of Michigan and Northwestern University, maintained that women should receive the advantages of higher education. He enrolled his daughter, Frances, at Syracuse where she joined the other newly admitted female students in founding the Gamma Phi Beta sorority. The inclusion of women in the early days of the university led to the proliferation of various women's clubs and societies. In fact, it was a Syracuse professor who coined the term "sorority" specifically for Gamma Phi Beta.
In the late 1880s the University engaged in a rapid building spree. Holden Observatory (1887) was followed by two Romanesque Revival buildings – von Ranke Library (1889), now Tolley Administration Building, and Crouse College (1889). Together with the Hall of Languages, these first buildings formed the basis for the "Old Row," a grouping which, along with its companion Lawn, established one of Syracuse's most enduring images. The emphatically linear organization of these buildings along the brow of the hill follows a tradition of American campus planning which dates to the construction of the "Yale Row" in the 1790s. At Syracuse, the Old Row continued to provide the framework for growth well into the twentieth century.
From its founding until through early 1920s, the University grew rapidly. It offered programs in the physical sciences and modern languages, and in 1873, Syracuse added one of the first architecture programs in the U.S. In 1874, Syracuse created the nation's first bachelor of fine arts degree, and in 1876, the school offered its first post-graduate courses in the College of Arts and Sciences. SU created its first doctoral program in 1911. SU's school of journalism, now the S.I. Newhouse School of Public Communications, was established at Syracuse in 1934.
The growth of Syracuse University from a small liberal arts college into a major comprehensive university were due to the efforts of two men, Chancellor James Day and John Archbold. James Roscoe Day was serving the Calvary Church in New York City where he befriended Archbold. Together, the two dynamic figures would oversee the first of two great periods of campus renewal in Syracuse's history.
John Dustin Archbold was a capitalist, philanthropist, and President of the Board of Trustees at Syracuse University. He was known as John D. Rockefeller's right-hand man and successor at the Standard Oil Company. He was a close friend of Syracuse University Chancellor James R. Day, and gave almost $6 million to the University over his lifetime. Said a journalist in 1917:
Mr. Archbold's ... is the president of the board of trustees of Syracuse University, an institution which has prospered so remarkably since his connection with it that its student roll has increased from hundreds to over 4,000, including 1,500 young women, placing it in the ranks of the foremost institutions of learning in the United States.
In addition to keeping the university financially solvent during its early years, he also contributed funds for eight buildings, including the full cost of Archbold Stadium (opened 1907, demolished 1978), Sims Hall (men's dormitory, 1907), the Archbold Gymnasium (1909, nearly destroyed by fire in 1947, but still in use), and the oval athletic field.
Modern.
After World War II, Syracuse University began to transform into a major research institution. Enrollment increased in the four years after the war due to the G.I. Bill, which paid tuition, room, board, and a small allowance for veterans returning from World War II. In 1946, SU admitted 9,464 freshmen, nearly four times greater than the previous incoming class. Branch campuses were established in Endicott, New York and Utica, New York.
The velocity with which the university sped through its change into a major research institution was astounding. By the end of the 1950s, Syracuse ranked twelfth nationally in terms of the amount of its sponsored research, and it had over four hundred professors and graduate students engaging in that investigation.
From the early 1950s through the 1960s, Syracuse University added programs and staff that continued the transformation of the school into a research university. In 1954, Arthur Phillips was recruited from MIT and started the first pathogen-free animal research laboratory. The lab focused on studying medical problems using animal models. The School of Social Work, which eventually merged into the College of Human Ecology, was founded in 1956. Syracuse's College of Engineering also founded the nation's second oldest computer engineering and bioengineering programs. In 1962, Samuel Irving Newhouse, Sr. donated $15 million to begin construction of a school of communications, eventually known as the SI Newhouse School of Public Communications. In 1966, Syracuse University was admitted to the Association of American Universities, an organization of leading research universities devoted to maintaining a strong system of academic research and education.
Pan Am Flight 103.
On December 21, 1988, 35 Syracuse University students were killed in the terrorist bombing of Pan Am Flight 103 over Lockerbie, Scotland. The students were returning from a study-abroad program in Europe.
That evening, Syracuse University went on with a basketball game just hours after the attack, for which it was severely criticized. The conduct of university officials in making the decision was also brought to the attention of the NCAA. The day after the bombing, the university's chancellor, Melvin A. Eggers, said on nationwide television that he should have cancelled the event. After the attacks on September 11, 2001, the NCAA left it up to the conferences to decide what to do about their sports events; many cancelled them. (the bombing of Flight 103 was the deadliest terrorist attack against the United States prior to the attacks on September 11, 2001.)
In April 1990, Syracuse University dedicated a memorial wall to the students killed on Flight 103, constructed at the entrance to the main campus in front of the Hall of Languages. Every year the university holds "Remembrance Week" during the fall semester to commemorate the students. On December 21 a service in the university's chapel at 2:03 p.m. (19:03 UTC) marks the exact minute on that date in 1988 when the plane exploded. The University also maintains a link to the tragedy with the "Remembrance Scholars" program, when 35 senior students receive scholarships during their final year at the University. With the "Lockerbie Scholars" program, two graduating students from Lockerbie Academy study at Syracuse for one year.
Campus.
The university is set on a campus that features an eclectic mix of buildings, ranging from nineteenth-century Romanesque Revival structures to contemporary buildings designed by renowned architects such as I.M. Pei. The center of campus, with its grass quadrangle, landscaped walkways, and outdoor sculptures, offers students the amenities of a traditional college experience. The university overlooks downtown Syracuse, a medium-sized city (140,600 residents in 2008). The school also owns a Sheraton Hotel, the Drumlins Country Club — a nearby, 36-hole golf course, the Fisher Center and Joseph I. Lubin House in New York City, the Paul Greenberg House in Washington, D.C., and the Minnowbrook Conference Center, a 30-acre (121,000 m²) retreat in the Adirondack Mountains of Upstate New York.
Main campus.
Also called "North Campus," the Main Campus contains nearly all academic buildings and residence halls. Its centerpiece is The Kenneth A. Shaw Quadrangle, more affectionately known as "The Quad", which is surrounded by academic and administrative buildings. The North Campus represents a large portion of the University Hill neighborhood. Buses run to South Campus, as well as downtown Syracuse and other locations in the city. About 70 percent of students live in University housing. First- and second-year students are required to live on campus. All 22 residence halls are coeducational and each contain a lounge, laundry facility, and various social/study spaces. Residence halls are secured with a card access system. Residence halls are located on both Main Campus and South Campus, the latter of which is a five-minute ride via bus. Learning communities and interest housing options are also available. Food facilities include six residential dining centers, two food courts, and several cafes.
The Comstock Tract Buildings, a historic district of older buildings on the campus, was listed on the National Register of Historic Places in 1980. Three buildings on campus—the Crouse Memorial College and the Hall of Languages, and the Pi Chapter House of Psi Upsilon Fraternity—are individually listed on the National Register.
A few blocks walk from Main Campus on East Genesee St, the Syracuse Stage building includes two proscenium theatres. The Storch is used primarily by the Drama Department and the Archbold is used primarily by Syracuse Stage, a professional regional theatre.
South campus.
After World War II, a large, undeveloped hill owned by the university was used to house returning veterans in military-style campus housing. During the 1970s, this housing was replaced by permanent two-level townhouses for two or three students each, or for graduate family housing. There are also three small residence halls which feature open doubles.
South Campus is also home to the Institute for Sensory Research, Tennity Ice Pavilion, Goldstein Student Center, Skytop Office Building and 621 Skytop Road (for administration), and the InnComplete Pub, a graduate student bar. Just north is the headquarters of SU Athletics located in the Manley Athletics Complex. Approximately 2,500 students live on the South Campus, which is connected to the main campus by frequent bus service.
Downtown.
In December 2004, the university announced that it had purchased or leased twelve buildings in downtown Syracuse. Five Design Programs; Communication, Advertising, Environmental and Interior Design, Industrial and Interactive Design, and Fashion; reside permanently in the newly renovated facilities, fittingly called The Warehouse, which was renovated by Gluckman Mayner Architects. Both programs were chosen to be located in the downtown area because of their history of working on projects directly with the community. The Warehouse also houses a contemporary art space that commissions, exhibits, and promotes the work of local and international artists in a variety of media. Hundreds of students and faculty have also been affected by the temporary move of the School of Architecture downtown for the $12 million renovation of its campus facility, Slocum Hall.
Since 2009, the Syracuse Center of Excellence in Environmental and Energy Systems, led by Syracuse University in partnership with Clarkson University and the College Environmental Science and Forestry, creates innovations in environmental and energy technologies that improve human health and productivity, security, and sustainability in urban and built environments. The Paul Robeson Performing Arts Company and the Community Folk Art Center will also be located downtown. On March 31, 2006, the university and the city announced an initiative to connect the main campus of the university with the arts and culture areas of downtown Syracuse and The Warehouse. Using natural gas, the Green Data Center generates its own electricity on site, providing cooling for servers and for a neighboring building.
The Connective Corridor project, supported by of public and private funds, will be a strip of cultural development that will connect the main campus of the university to downtown Syracuse, NY. In 2008, an engineering firm is studying traffic patterns and lighting to commence the project. A design competition was held to determine the best design for the project.
Metropolitan satellite locations.
SU has established an admissions presence in Los Angeles, California that will enhance the University's visibility on the West Coast and will join the University's West Coast offices of alumni relations, institutional advancement, and the LA semester program in the same location.
Syracuse University has also established an admissions presence in New York City and Atlanta, Georgia.
Art on campus and permanent collections.
Syracuse is home to the SU Art Galleries, whose mission is to enhance the cultural environment of its community and surrounding area. The main gallery space is located in the Shaffer Art Building on the main campus.
The Warehouse Gallery is a new contemporary art space exhibiting that is operated under the umbrella of the SU Art Galleries. Housed in a former furniture warehouse off campus, the Warehouse Gallery features works from international artists in a variety of media, its mission is to engage the community in a dialogue regarding the role the arts can play in illuminating the critical issues of our times.
Also on campus is the Louise and Bernard Palitz Gallery. Located on the second floor of the Lubin House, the Palitz gallery has a rotation of exhibitions, including two annual public shows, local and regional artists, featured items from the university's art collection, and professional artists.
There are many other venues for student work at Syracuse University. In the Shaffer Art Building is the Lowe Art Gallery, which features student work. Gallery spaces are also available for reservation on the fourth floor of the Bird Library.
Within the Schine Student Center is home to three gallery spaces. The Robert B Menschel Photography Gallery features work from professional photographers as well as students and local artists. On the third floor is the Panasci Lounge Art Hanging space for two dimensional spaces. This space can be reserved by students. The White Cube Gallery, also on the third floor is a student gallery that showcases work for the student body outside of the school of art and design.
Students can also research primary sources through the Special Collections Research Center (SCRC) which is composed of rare books, manuscripts, works of architecture and design, and popular culture (cartoons, science fiction, and pulp literature), photography, the history of recorded sound, and more.
SU has a permanent art collection of over 45,000 objects from artists including Picasso, Rembrandt, Hopper, Tiffany and Wyeth. More than 100 important paintings, sculptures, and murals are displayed in public places around campus. Notable sculptures on campus include Sol LeWitt's "Six Curved Walls", Anna Hyatt Huntington's "Diana", Jean-Antoine Houdon's "George Washington", Antoine Bourdelle's "Herakles", James Earle Fraser's "Lincoln", Malvina Hoffman's "The Struggle of Elemental Man," and Ivan Meštrović's "Moses", "Job" and "Supplicant Persephone".
Organization.
Syracuse is governed by a 70-member Board of Trustees, with 64 trustees elected by the Board to four-year terms, and six elected by the alumni to four-year terms. Of the 64 Board elected Trustees, three must represent specified conferences of the United Methodist Church. In addition, the chancellor and the President of the Syracuse Alumni Association serve as ex officio voting Trustees. Two students and one faculty member serve as non-voting representatives to the Board of Trustees. The Board of Trustees selects, and sets the salary of, the chancellor. The Syracuse University Bylaws also establish a University Senate with "general supervision over all educational matters concerning the University as a whole..." The Senate consists of administrators, faculty, students and staff.
Academics.
For the Class of 2012, there were approximately 25,000 applicants for 3,350 seats in the Freshman class. The libraries have collectively over 3.16 million volumes. In fall 2006, the university had over 12,000 full-time undergraduate students and over 1,000 part-time undergraduate students, as well as almost 4,000 full-time graduate and law students and 2,000 part-time graduate and law students. In 2005–06, the university granted over 2,600 bachelor's degrees; nearly 2,000 master's degrees; over 300 law degrees; and more than 160 doctoral degrees. "U.S. News & World Report" ranked SU 53rd among national universities in the United States for 2009 and 58th for 2013. Syracuse participates in the National Association of Independent Colleges and Universities and University and College Accountability Network (U-CAN).
Degrees.
SU offers undergraduate degrees in over 200 majors in the 9 undergraduate schools and colleges. Bachelor's degrees are offered through the Syracuse University School of Architecture, the College of Arts and Sciences, the School of Education, the David B. Falk College of Sport and Human Dynamics, the College of Engineering and Computer Science, the School of Information Studies, Martin J. Whitman School of Management, S.I. Newhouse School of Public Communications, and the College of Visual and Performing Arts. Also offered are Master's and doctoral degrees from the Graduate School and from specialized programs in the Maxwell School of Citizenship and Public Affairs, College of Law, among others. Additionally, SU offers 24 Certificates of Advanced Study Programs for specialized programs for education, counseling, and other academic areas.
The university has offered multiple international study programs since 1911. SU Abroad, formerly known as the Division of International Programs Abroad (DIPA), currently offers joint programs with universities in over 40 countries. The university operates eight international centers, called SU Abroad Centers, that offer structured programs in a variety of academic disciplines. The centers are located Beijing, Istanbul, Florence, Italy, Hong Kong, London, UK, Madrid, Spain, Strasbourg, France, and Santiago, Chile.
National recognition and ranking.
In its 2015 ranking of U.S. colleges, "U.S. News & World Report" ranked Syracuse 58th among undergraduate national universities.
In 2015, Syracuse University was ranked #24 in New York State by average professor salaries.
Many of SU's programs have been nationally recognized for excellence. A 2008 survey in the Academic Ranking of World Universities places Syracuse University in the top 100 world universities in social sciences. In the 2015 'Design Intelligence' national rankings, the Environmental and Interior Design program is ranked 9th. The School of Architecture's Bachelor of Architecture program was ranked second nationally in 2010 by the journal "DesignIntelligence" in its annual edition of "America's Best Architecture & Design Schools."
The S.I. Newhouse School of Public Communications is one of the top ranked in the country and has produced alumni in many fields of broadcasting. The School of Information Studies offers information management and technology courses at the undergraduate and graduate levels at Syracuse University. Within the School of Information Studies, "U.S. News & World Report" has ranked the graduate program as the third best Library and Information Studies graduate school in the United States. It also has the top-ranked graduate Information Systems program, the second ranked graduate program in Digital Librarianship, and the fourth ranked graduate program in School Library Media. The College of Business Administration was renamed the Martin J. Whitman School of Management in 2003, in honor of SU alumnus and benefactor Martin J. Whitman. The school is home to about 2,000 undergraduate and graduate students.
The undergraduate program was ranked No. 43 among business schools nationwide by "U.S. News & World Report" in 2014 while the graduate school was ranked No. 79. The entrepreneurship program was ranked No. 9 by the "U.S. News & World Report" in 2014, and No. 13 by both "Entrepreneur Magazine" and "The Princeton Review" in 2007. The supply chain management program was ranked No. 10 in the nation by "Supply Chain Management Review". Also, the Joseph I. Lubin School of Accounting was named No. 10 in the nation by "The Chronicle of Higher Education".
The College of Law is ranked #86 nationally, and is ranked in the top 10 by "U.S. News & World Report" for its trial and appellate advocacy program. It is an emerging leader in the relatively novel field of National Security Law. In 2007 the Law School started the Cold Case Justice Initiative, investigating cold cases from the civil rights era in the South. Its professors and students have identified 196 cases, of which more than 100 are in Georgia, and will give information to the US Department of Justice in order to have cases prosecuted. The FBI has identified 122 cold cases that it is trying to resolve.
The Maxwell School of Citizenship and Public Affairs combines social sciences with public administration and international relations. It is ranked as the top graduate school for public affairs in the US.
The graduate program of the College of Visual and Performing Art is considered one of the top 50 programs in the US. VPA ranked No. 14 in multimedia/visual communications, a specialty that includes disciplines found in the college's Department of Transmedia, which offers M.F.A. programs in art photography, art video, computer art and film. VPA also ranked No. 16 in ceramics, No. 19 in printmaking and No. 20 in sculpture, which are M.F.A. programs based in the Department of Art. Project Advance (or SUPA) is a nationally recognized concurrent enrollment program honored by the American Association for Higher Education, the Carnegie Foundation for the Advancement of Teaching, the National Commission on Excellence in Education, and the National Institute of Education.
Syracuse was ranked 5th in "The Princeton Review"'s 2014 and 1st in the 2015 list of top party schools.
Faculty.
Syracuse University has 1013 full-time instructional faculty, 96 part-time faculty, and 454 adjunct faculty. Approximately 86% of the full-time faculty have earned Ph.D.s or professional degrees. The current faculty includes scholars such as United States National Academy of Sciences member Jozef J. Zwislocki, Professor of Psychology, who developed mathematical models on the mechanics of the inner and middle ear, MacArthur Fellow Don Mitchell, Professor of Geography, who has developed studies in cultural geography, Bruce Kingma, Associate Provost and Kauffman Professor of Entrepreneurship, a pioneer in the field of information economics and online learning, Catherine Bertini, Professor of Practice in Public Administration, who has worked on the role of women in food distribution, Frederick C. Beiser, Professor of Philosophy, one of leading scholars of German idealism, Mary Karr, the Jesse Truesdell Peck Professor of Literature, who has received a Guggenheim Fellowship in poetry, John Caputo, the Thomas J. Watson Professor of Humanities, who founded weak theology, and Gustav Niebuhr Associate Professor of Religion and Media who is the former New York Times National Religion Correspondent.
Syracuse University Press.
Founded on August 2, 1943 by Chancellor William Pearson Tolley and benefactor Thomas Watson, Sr. The areas of focus for the Press include Middle East Studies, Native American Studies, Peace and Conflict Resolution, Irish Studies and Jewish Studies, among others. The Press has an international reputation in Irish Studies and Middle East Studies. It is a member of the Association of American University Presses.
University lectures.
Every year as a tradition, the University would invite notable and influential speakers from around the world. These speakers have moved the world in various ways in the areas of sustainability, advertising, redevelopment, human rights, journalism, and the environment. The lecturers have been held to the highest standard in academic and public service excellence. The University Lectures are supported by the University Trustees, alumni, and friends.
Previous University lecturers included Ishmael Beah, author of "", 45th Vice President of the United States, Al Gore, Economist and Nobel Prize winner, Muhammad Yunus, author and columnist, William Safire, environmental justice advocate Majora Carter, and environmental law attorney, Robert Kennedy Jr.
Libraries.
Syracuse University's main library is the Ernest S. Bird Library, which opened in 1972. Its seven levels contain 2.3 million books, 11,500 periodicals, 45000 ft of manuscripts and rare books, 3.6 million microforms, and a café. There are also several departmental libraries on campus. Many of the landmarks in the history of recorded communication between people are in the university's Special Collections Research Center, from cuneiform tablets and papyri to several codices dating from the 11th century, to the invention of printing. The collection also includes works by Galileo, Luther, John Calvin, Voltaire, Sir Isaac Newton, Descartes, Sir Francis Bacon, Samuel Johnson, Thomas Hobbes, Goethe, and others. In addition, the collection includes the personal library of Leopold Von Ranke.
Making sensational headlines at the time, the university outbid the Prussian government for all 19 tons of Von Ranke's prized personal library. Other collections of note include Rudyard Kipling first editions and an original second leaf of the Gutenberg Bible.
Bird Library is also home to the largest collection of national archives of Kenya and Tanzania. In July 2008, Syracuse University became the owner of the second largest collection of 78 rpm records in the United States after the Library of Congress after a donation of more than 200,000 records. The donation is valued at $1 million and more than doubles the University's collection of 78 rpm records to about 400,000. It also has a special The Harriet Tubman Research Collection, and Environmental Justice and Gender collection housed in the Martin Luther King Jr. Memorial Library. The MLK library holds over 15,000 acquisitions in African, African-American, Afro-Lationo, and Caribbean studies.
The university is also home to the Belfer Audio Laboratory and Archive, whose holdings total approximately 540,000 recordings in all formats, primarily cylinders, discs, and magnetic tapes. Some of the voices to be found include Thomas Edison, Amelia Earhart, Albert Einstein, and Oscar Wilde.
Research.
According to the Carnegie Classification of Institutions of Higher Education, Syracuse University is a research university with a high level of research activity. Through the university's Office of Research, which promotes research, technology transfer, and scholarship, and its Office of Sponsored Programs, which assists faculty in seeking and obtaining external research support, SU supports research in the fields of management and business, sciences, engineering, education, information studies, energy, environment, communications, computer science, public and international affairs, and other specialized areas. Syracuse became a member of the Association of American Universities (AAU) in 1966, an organization of leading research universities devoted to maintaining a strong system of research and education. In 2011, however, the university's board of trustees voted to pull out of the research consortium.
SU has established 29 research centers and institutes that focuses research, often across disciplines, in a variety of areas. The Burton Blatt Institute advances research in economic and social issues for individuals with disabilities, and it has international projects in the field. The Martin J Whitman School of Management supports the largest number of research centers, including The Ballentine Investment Institute, the George E. Bennett Center for Accounting and Tax Research, the Robert H. Brethren Operations Management Institute, Michael J. Falcone Center for Entrepreneurship, The H. H. Franklin Center for Supply Chain Management, Olivia and Walter Kiebach Center for International Business Studies, and the Earl V. Snyder Innovation Management Program.
Other research programs include The Syracuse Biomaterials Institute, the Alan K. Campbell Public Affairs Institute through the Maxwell School, and the Center for the Study of Popular Television through the Newhouse School of Public Communications.
Syracuse University also has collaborations with CERN and Fermi National Accelerator Laboratory, among other institutes.
Student life.
Syracuse University has a diverse student population, representing all 50 US states and over 115 countries. Approximately 10 percent of students are from outside of the US, and are supported by an international services department within the University's Division of Student Affairs. Approximately 37% of students in the fall 2010 undergraduate full-time class are from New York State. Approximately 56% of that class are women.
Media.
CitrusTV (formerly UUTV, HillTV and Synapse) is the university's entirely student-run television studio, and one of the largest student-run TV studios in the country with over 300 active members. CitrusTV produces news, sports, and entertainment content that appears on the university's campus cable channel, the Orange Television Network, and online on the CitrusTV.net website. Some content also appears in Central New York on the cable channel Time Warner Cable Sports. The station used to be a part of University Union, the largest student organization on campus, until it split to become its own recognized student organization in 2004. The station was briefly known as HillTV until the middle of the fall 2005 semester, when the university temporarily shut the station down for a controversial entertainment episode and demanded its reform. The station is located in the Robert B. Menschel Media Center, at the Waverly Avenue side of Watson Hall.
The Orange Television Network is the university-controlled campus cable TV station. It is available on channel 2 in all campus buildings, and on channel 2.1 in high definition. It operates out of the newly renovated Dick Clark Studies in Newhouse 2.
The school's independent student newspaper is "The Daily Orange", founded in 1903 and independent since 1971. The D.O. Alumni Association recently celebrated the paper's 100th anniversary.
WAER (88.3 FM) is located on the campus of SU, and is an auxiliary service of the school. The station features a jazz music and National Public Radio format, with a news and music staff providing 24-hour programming. It is best known for its sports staff, which has produced the likes of Bob Costas, Marv Albert, Mike Tirico, Sean McDonough, Ian Eagle, Brian Higgins, Dave O'Brien, Dave Pasch, and Dick Stockton. Lou Reed also hosted a free-format show on WAER during his time at Syracuse University; this free-format radio tradition at Syracuse is carried on by WERW.
WJPZ is a radio station owned and operated entirely by SU students. It broadcasts at 89.1 FM at an effective radiated power of 100 watts and can be heard throughout Syracuse, the rest of Onondaga County, and beyond to the north and east. WJPZ programs a tight CHR (Contemporary Hit Radio, or Top 40) format. Although operated by students, it is an independent organization which is incorporated and licensed by the FCC as WJPZ Radio, Inc. It is neither owned nor controlled by Syracuse University, but it does lease studio and transmitter facilities on Syracuse University property.
WERW is the entirely student-run, free format carrier current radio station on the SU campus. It is the only campus radio station currently being broadcast on iTunes college radio. The station can also be heard at 1570 AM. The studios are located in the Schine Student Center. Originally operating at 750AM, WERW was available only in the university's dorms and some other campus buildings. The station's current low power broadcast tower was erected atop the Booth Hall dormitory in 1995 to allow it to broadcast at 1570AM. With this new tower, WERW could be heard all across the university campus and in adjacent areas of the city of Syracuse.
There are also multiple student-run magazines and other print publications, including: The Onondagan Yearbook, Student Voice, Jerk Magazine, What the Health, 360, Baked Magazine, The Out Crowd, and Equal Time.
Student government.
Founded in 1957, the Student Association (SA) represents the undergraduate students of both SU and ESF. SA, elects a President and Vice President (on a unified ticket) each academic year. They also each year elect a Comptroller, who, with the assembly, oversees the allocation and designation of the Student Activity Fee that was first collected in the 1968–69 school year. The goals of SA are to participate through a unified student voice in the formulation of Syracuse University rules and regulations. The SA-SGA Alumni Organization maintains the history and an organizational timeline on its website. 
The graduate students at Syracuse University are represented by the Graduate Student Organization (GSO) while the law students at Syracuse University are represented by the Law Student Senate. Each of the three organizations elects students to serve in the Syracuse University Senate, which also includes faculty, staff, and administrators.
Fraternities and sororities.
The Syracuse University fraternity and sorority system offers organizations that are members of the Panhellenic Council (NPC), the Interfraternity Council (IFC), the National Association of Latino Fraternal Organizations, the National Multicultural Greek Council, and the National Pan-Hellenic Council (NPHC). In addition to SU students, ESF students are permitted to join the university's fraternity and sorority system.
The oldest fraternity at SU is Delta Kappa Epsilon, which established a chapter in 1871 soon after the founding of the university, followed by Psi Upsilon in 1875 and Phi Kappa Psi in 1884. Sororities were also a part of the early history of SU. Alpha Phi was founded at SU in 1872, followed by Gamma Phi Beta in 1874 and Alpha Gamma Delta in 1904. Alpha Phi Alpha established a chapter at SU in 1910, and was reorganized in 1949 and 1973. Alpha Phi Delta, the only historically Italian-American heritage fraternity, was founded at SU in 1914. The first NPHC fraternity, Omega Psi Phi, was established at SU in 1922, and the first NPHC sorority, Delta Sigma Theta in 1973. University policy prohibits fraternities and sororities from discriminating "on the basis of race, creed, color, gender, national origin, religion, marital status, age, disability, sexual orientation, or status as a disabled veteran or a veteran of the Vietnam era."
Syracuse University Ambulance.
Syracuse University Ambulance, commonly referred to as SUA, is a SU Health Services-based student organization that responds to over 1,500 medical emergencies each year. Providing basic life support (BLS), rapid cardiac defibrillation, emergent and non-emergent transportation, and special event standby services, SUA operates two full-time transporting ambulances, a supervisor's fly car, and a MCI trailer for mass-casualty incidents. Additionally, SUA operates four transport vans for non-emergency transports. Advanced life support (ALS) mutual aid is provided by the City of Syracuse's private EMS provider, Rural/Metro Medical Services. SUA was formed in 1973 by a group of students out of a need for emergency medical services on campus. Starting with only a few members and meager equipment, the Syracuse University Medical Crisis Unit was formed. The organization has evolved greatly over time but, with 70+ volunteer students, remains a student-run organization to this day. SUA provides emergency and non-emergency services 24-hours a day, 7-days a week during the academic school year and is funded by a portion of the student health fee.
Religious life.
Hendricks Chapel is an interfaith chapel located on the Quad, and services as the spiritual center of Syracuse University. The Chapel, headed by Dean Tiffany Steinwert, is home to ten chaplaincies, including Baptist, Buddhist, Evangelical Christian, Historically Black Churches, Islamic, Jewish, Lutheran, Pagan, Methodist, and Roman Catholic. In addition, there are a number of student religious groups, including groups associated with the chaplaincies as well as Adventist, Christian Science, Hindu, Mormon, Orthodox Christian, Pentecostal and more.
Additional buildings located on campus support specific religious groups, including the Alibrandi Catholic Center and the Winnick Hillel Center for Jewish Life. Off campus, the Chabad House and Islamic Center of CNY also support student religious life.
Athletics.
Syracuse University's sports teams have "the Orange" nickname since 2004, although the former names of Orangemen and Orangewomen are still used. The school's mascot is Otto the Orange. SU fields intercollegiate teams in eight men's sports and 12 women's sports.
Most of Syracuse University's intercollegiate teams participate in NCAA Division I in the Atlantic Coast Conference. The Syracuse Orange women's ice hockey team participates in College Hockey America. Crew participates in the Eastern Association of Rowing Colleges. The men's and women's basketball teams, the football team, and both the men's and women's lacrosse teams play in the Carrier Dome. Other sports are located at the nearby Manley Field House, except ice hockey which takes place in the Tennity Ice Skating Pavilion.
SU has reached 28 team national championships, including 14 men's lacrosse, six men's crew, two cross country running, and one each in boxing, football, and women's lacrosse. Under long-time head coach Jim Boeheim, men's basketball team won seven Big East regular season championships, five Big East Tournament championships, and 25 NCAA Tournament appearances, including the 2003 NCAA championship. The men's basketball team holds the largest on campus attendance record of 35,446 attendees. The record was set in the Carrier Dome playing Duke on Saturday February 1, 2014.
In 1959, Syracuse earned its first National Championship following an undefeated football season and a Cotton Bowl victory over Texas. The team featured sophomore running back Ernie Davis who, in 1961, became the first African-American to win the Heisman Trophy. Davis was slated to play for the Cleveland Browns in the same backfield as Jim Brown, but died of leukemia before being able to play professionally.
Syracuse played its first intercollegiate lacrosse game in 1916, and captured its first USILA championship in 1920. It won USILA championships in 1922, 1924, and 1925. In the modern NCAA era, Syracuse is the first school to capture 11 National Championships, the most of any team in college lacrosse history. Most recently, Syracuse reached the men's Division I championship game in 2013 after winning two championships in 2008 & 2009 seasons and reaching the quarterfinals in 2011. However, the Orange lost 16-10 to Duke University after leading 5-0 early in the game. The women's lacrosse team reached the NCAA Division I National Championship game for the first time in school history in 2012, which they lost to Northwestern.
Toward the end of the 1970s, Syracuse University was under pressure to improve its football facilities in order to remain a NCAA Division I football school. Its small concrete stadium, Archbold Stadium, was seventy years old and not up to the standards of other schools. The stadium could not be expanded; it had been reduced from 40,000 seats to 26,000 due to the fire codes. Syracuse University decided to build a new stadium. In 1978, Archbold Stadium was demolished to make way for the Carrier Dome, which was to have a domed Teflon-coated, fiberglass inflatable roof. It would also serve as the home for the men's basketball team, as a replacement for Manley Field House. The Carrier Dome was constructed between April 1979 and September 1980. The total construction cost was $26.85 million, including a $2.75 million naming gift from the Carrier Corporation.
Alumni.
Among the individuals who have attended or graduated from Syracuse University include writers Stephen Crane, Joyce Carol Oates, John D. MacDonald, Shirley Jackson, and Alice Sebold; William Safire, Pulitzer Prize winning commentator; Cambridge University historian Sir Moses I. Finley; Sir John Stanley, British Member of Parliament; Arthur Rock, legendary venture capitalist and cofounder of Intel; Vishal Sikka, CEO and MD of Infosys; Donna Shalala, former United States Secretary of Health and Human Services; Joe Biden, Vice President of the United States; Robert Jarvik, inventor of the first artificial heart implanted into human beings; Eileen Collins, first female commander of a Space Shuttle; Prince Sultan bin Salman, first Arab, first Muslim and the youngest person to travel to space; Robert Menschel, legendary partner/director at Goldman Sachs; Samuel Irving Newhouse, Jr., owner of Conde Nast publications; Lowell Paxson, founder of Home Shopping Network; musician Lou Reed; David P. Weber, lawyer and certified fraud examiner, who reported misconduct in the Bernard L. Madoff and R. Allen Stanford frauds; and Prince Al-Waleed bin Talal, a prominent investor and member of the Saudi royal family.
Alumni in journalism and broadcasting include Ted Koppel, Megyn Kelly, Michael Barkann, Bob Costas, Marv Albert, Len Berman, Dave Pasch, Sean McDonough, Ian Eagle, Dave O'Brien, Dick Stockton, Arun Shourie, Mike Tirico, Brian Higgins, Larry Hryb (an employee at Microsoft, former radio broadcaster for Clear Channel Communications), Steve Kroft of "60 Minutes, and Adam Schein of Mad Dog Sports Radio."
Notable SU alumni in the performing arts include Dick Clark, Taye Diggs, Peter Falk, Frank Langella, Aaron Sorkin, Rob Edwards, Jerry Stiller, Peter Weller, Lexington Steele and Vanessa L. Williams.
Prominent athletes include Jim Brown, actor and NFL Hall of Famer with the Cleveland Browns, arguably the greatest running back of all time;Ernie Davis, the first African-American Heisman Trophy winner immortalized in the motion picture "The Express"; Donovan McNabb, former NFL quarterback; former Indianapolis Colts wide receiver Marvin Harrison; Dwight Freeney, defensive end for the San Diego Chargers; Carmelo Anthony, forward for the New York Knicks; 7-time NBA All Star, pro basketball Hall of Famer and former Mayor of Detroit Dave Bing; Tim Green, former Atlanta Falcons player, author, lawyer, and National Public Radio commentator; Darryl Johnston, three-time Super Bowl winner with the Dallas Cowboys in the 1990s; Mikey Powell, who formerly played lacrosse for the Boston Cannons; Floyd Little, who played for the Denver Broncos; Kyle Johnson, who played the majority of his NFL career with the Denver Broncos; John Mackey a member of the NFL Hall of Fame played for the legendary Baltimore Colts (1963–71); and Tom Coughlin, New York Giants head coach.
Affiliations.
Affiliated institutions.
State University of New York College of Environmental Science and Forestry.
The College of Environmental Science and Forestry (ESF) has a long affiliation with Syracuse University, shares many campus resources, and operates its main academic campus immediately adjacent to Syracuse University. ESF was founded in 1911 as the New York State College of Forestry at Syracuse University, under the leadership of Syracuse University Trustee Louis Marshall, with the active support of Syracuse University Chancellor Day. Its founding followed the Governor's veto of annual appropriations to a separate New York State College of Forestry at Cornell.
ESF is an autonomous institution, administratively separate from SU, while resources, facilities, and some infrastructure are shared. The two schools share a common Schedule of Classes; students at both institutions may take courses at the other, and degrees from ESF bear the Syracuse University seal along with the State University of New York. A number of concurrent degree programs and certificates are offered between the schools, as well. The college receives an annual appropriation as part of the SUNY budget and the state builds and maintains all of the college's educational facilities. The state has similar relationships with five statutory colleges that are at Alfred University and Cornell University.
ESF faculty, students, and students' families join those from SU to take part in a joint convocation ceremony at the beginning of the academic year in August, and joint commencement exercises in May. ESF and SU students share access to libraries, recreational facilities, student clubs, and other activities at both institutions, except for the schools' intercollegiate sports teams, affiliated with the NCAA and NAIA, respectively. First-year ESF students live in the newly constructed Centennial Hall located on ESF's main campus.
State University of New York Upstate Medical University.
The medical school was formerly a college within SU, known as the Syracuse University Medical School. In 1950, SU sold the medical school to the State University of New York system. Beginning in the fall of 2009, a Master of Public Health degree program is now being offered by the two institutions. The program is the first of its kind in Central New York, and the first jointly offered by the two universities.
Formerly affiliated institutions.
State University of New York at Binghamton.
Binghamton University was established in 1946 as Triple Cities College, to serve the needs of local veterans of the Binghamton, New York area, who were returning from World War II. Established in Endicott, New York, the college was a branch of Syracuse University. Triple Cities College offered local students the first two years of their education, while the following two were spent at Syracuse University. In 1946, students could earn their degrees entirely at the Binghamton campus. In 1950, it was absorbed by the State University of New York and renamed Harpur College.
Utica College.
Utica College, an independent private university located in Utica, New York, was founded by Syracuse University in 1946. Utica College became independent from SU in 1995, but still offers its students the option to receive a specialized bachelor's degree from Syracuse University through a mutual relationship between the two schools.

</doc>
<doc id="29354" url="http://en.wikipedia.org/wiki?curid=29354" title="Snake oil">
Snake oil

Snake oil is an expression that originally referred to fraudulent health products or unproven medicine but has come to refer to any product with questionable or unverifiable quality or benefit. By extension, a snake oil salesman is someone who knowingly sells fraudulent goods or who is themselves a fraud, quack, charlatan, or the like.
The use of snake oil is far older than the 19th century, and it was never confined to the Americas. In Europe, viper oil had been commonly recommended for many afflictions, including the ones for which rattlesnake oil was subsequently favored (e.g., rheumatism and skin diseases).
There are two main hypotheses for the origin of the term. The more common theory is that the name originated in the Western regions of the United States and is derived from a topical preparation made from the Chinese water snake ("Enhydris chinensis") used by Chinese laborers to treat joint pain. The preparation was promoted in North America by travelling salesmen who often used accomplices in the audience to proclaim the benefits of the preparation.
One source, William S. Haubrich in his book "Medical Meanings" (1997, American College of Physicians) mentions the hypothesis that the name came from the eastern United States. The indigenous people of the New York and Pennsylvania region would rub cuts and scrapes with the petroleum collected from oil seeps that occurred naturally in the area. European settlers observed this habit, and began bottling and selling the substance as a cure-all. The preparation was sold as "Seneca oil" in mid-nineteenth century, after the local tribes. Through mispronunciation this became "Sen-ake-a oil" and eventually "snake oil". As Haubrich comments, "This story is almost too good to be true – which means it probably isn’t." It appears to be a case of folk etymology, as no historical evidence appears to exist for this transformation.
History.
Chinese laborers on railroad gangs involved in building the First Transcontinental Railroad first gave snake oil to Europeans with joint pain. When rubbed on the skin at the painful site, snake oil was claimed to bring relief. This claim was ridiculed by rival medicine salesmen, and in time, "snake oil" became a generic name for many compounds marketed as panaceas or miraculous remedies whose ingredients were usually secret, unidentified, or mischaracterized and mostly inert or ineffective.
Patent medicines originated in England, where a patent was granted to Richard Stoughton's Elixir in 1712. Since there was no federal regulation in the United States concerning safety and effectiveness of drugs until the 1906 Food and Drugs Act and various medicine salesmen or manufacturers seldom had enough skills in analytical chemistry to analyze the contents of snake oil, it became the archetype of hoax.
The snake oil peddler became a stock character in Western movies: a traveling "doctor" with dubious credentials, selling fake medicines with boisterous marketing hype, often supported by pseudo-scientific evidence. To increase sales, an accomplice in the crowd (a shill) would often attest to the value of the product in an effort to provoke buying enthusiasm. The "doctor" would leave town before his customers realized they had been cheated. This practice is also called grifting and its practitioners are called grifters.
From cure-all to quackery.
The composition of snake oil medicines varies markedly among products.
Stanley's snake oil — produced by Clark Stanley, the "Rattlesnake King" — was tested by the United States government in 1917. It was found to contain:
This is similar in composition to modern-day capsaicin-based liniments or chest rubs. None of the oil content was found to have been extracted from any actual snakes.
The government sued the manufacturer for misbranding and misrepresenting its product, winning the judgment of $20 against Clark Stanley. Soon after the decision, "snake oil" became synonymous with false cures and "snake-oil salesmen" became a tag for charlatans.

</doc>
<doc id="29355" url="http://en.wikipedia.org/wiki?curid=29355" title="Stanley Elkin">
Stanley Elkin

Stanley Lawrence Elkin (May 11, 1930 – May 31, 1995) was an American novelist, short story writer, and essayist. His extravagant, satirical fiction revolves around American consumerism, popular culture, and male-female relationships.
Biography.
Elkin was born to a Jewish family in Brooklyn, New York, and grew up in Chicago from age three onwards. He did both his undergraduate and graduate work at the University of Illinois at Urbana-Champaign, receiving a bachelor's degree in English in 1952 and a Ph.D. in 1961 for his dissertation on William Faulkner. (During this period he was drafted and served in the U.S. Army from 1955-57.) In 1953 Elkin married Joan Marion Jacobson. He was a member of the English faculty at Washington University in St. Louis from 1960 until his death, and battled multiple sclerosis for most of his adult life. In 1968, he signed the “Writers and Editors War Tax Protest” pledge, vowing to refuse tax payments in protest against the Vietnam War.
During his career, Elkin published ten novels, two volumes of novellas, two books of short stories, a collection of essays, and one (unproduced) screenplay. Elkin's work revolves about American pop culture, which it portrays in innumerable darkly comic variations. Characters and especially prose style take full precedence over plot. His language is extravagant and exuberant, baroque and flowery, taking fantastic flight from his characters' endless patter. "He was like a jazz artist who would go off on riffs," said critic William Gass. In a review of "George Mills", Ralph B. Sipper wrote, "Elkin's trademark is to tightrope his way from comedy to tragedy with hardly a slip." About the influence of ethnicity on his work Elkin said he admired most "the writers who are stylists, Jewish or not. Bellow is a stylist, and he is Jewish. William Gass is a stylist, and he is not Jewish. What I go for in my work is language."
Although living in the Midwest, Elkin spent his childhood and teenage summers in a bungalow colony called , on the Ramapo River in northern New Jersey not far from Mahwah, the home of Joyce Kilmer. This was a refuge for a close-knit group of several score families, mostly Jewish, from the summer heat of New York City and urban New Jersey. Elkin’s writings placed in New Jersey were 
Elkin won the National Book Critics Circle Award on two occasions: for "George Mills" in 1982 and for "Mrs. Ted Bliss", his last novel, in 1995. "The MacGuffin" was a finalist for the 1991 National Book Award for Fiction. However, although he enjoyed high critical praise, his books have never enjoyed popular success. The 1976 Jack Lemmon
film "Alex and the Gypsy" was based on Elkin's novella "The Bailbondsman".
Elkin died May 31, 1995 of a heart attack. His manuscripts and correspondence are archived in Olin Library at Washington University in St. Louis. Elkin's literary legacy is represented by the literary agency headed by Georges Borchardt.
He has a star on the St. Louis Walk of Fame.

</doc>
<doc id="29356" url="http://en.wikipedia.org/wiki?curid=29356" title="International Society of Cryptozoology">
International Society of Cryptozoology

The International Society of Cryptozoology (ISC) was a former professional organization founded in 1982 in Washington, D.C. It ceased to exist in 1998.
It was founded to serve as a scholarly center for documenting and evaluating evidence of unverified animals; that is, animal species or forms which have been reported in some manner but which have not been scientifically proven to exist. The study of such animals is known as cryptozoology, and "Cryptozoology" was also the title of its journal. The President was Bernard Heuvelmans, and the Vice-President Roy Mackal. The Secretary was J. Richard Greenwell (died 2005), of the University of Arizona. Loren Coleman, John Willison Green, and several other prominent cryptozoologists were either Life Members, Honorary Members, or Board Members.
The official emblem of the society was the Okapi, which was chosen because, although it was well known to the inhabitants of its region, it was unknown to the European scientific community until the English explorer Harry Johnston sent to London an Okapi skin which received international attention in 1901.
The journal "Cryptozoology" was published from 1982 to 1996. The Society also published a newsletter "ISC News".
The ISC ended its activities in 1998 due to financial problems, though a website continued until 2005.
According to the journal "Cryptozoology", the ISC served "as a focal point for the investigation, analysis, publication, and discussion of all matters related to animals of unexpected form or size, or unexpected occurrence in time or space."

</doc>
