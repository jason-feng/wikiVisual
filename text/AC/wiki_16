<doc id="28123" url="http://en.wikipedia.org/wiki?curid=28123" title="Sniper">
Sniper

A sniper is a marksman or qualified specialist who operates alone, in a pair, or with a sniper team to maintain close visual contact with the enemy and engage targets from concealed positions or distances exceeding the detection capabilities of enemy personnel. These sniper teams operate independently, with little combat asset support from their parent units. 
Snipers typically have highly selective or specialized training and use crew-served high-precision/special application rifles and optics, and often have sophisticated communication assets to feed valuable combat information back to their units.
In addition to marksmanship and long range shooting, military snipers are trained in camouflage, field craft, infiltration, special reconnaissance and observation, surveillance and target acquisition.
Etymology.
The verb "to snipe" originated in the 1770s among soldiers in British India where a hunter skilled enough to kill the elusive snipe was dubbed a "sniper". The term "sniper" was first attested in 1824 in the sense of the word "sharpshooter".
Another term, "sharp shooter" was in use in British newspapers as early as 1801. In the "Edinburgh Advertiser", 23 June 1801, can be found the following quote in a piece about the North British Militia; "This Regiment has several Field Pieces, and two companies of Sharp Shooters, which are very necessary in the modern Stile of War". The term appears even earlier, around 1781, in Continental Europe.
Modern warfare.
Different countries use different military doctrines regarding snipers in military units, settings, and tactics.
Generally, a sniper's primary function in modern warfare is to provide detailed reconnaissance from a concealed position and, if necessary, to reduce the enemy's fighting ability by neutralizing high value targets (especially officers and other key personnel) and in the process pinning down and demoralizing the enemy. Typical sniper missions include managing intelligence information they gather during reconnaissance and surveillance, target acquisition for air-strikes and artillery, assist employed combat force with fire support and counter-sniper tactics, killing enemy commanders, selecting targets of opportunity, and even destruction of military equipment, which tend to require use of anti-materiel rifles in the larger calibers such as the .50 BMG, like the Barrett M82, McMillan Tac-50, and Denel NTW-20.
Soviet Russian and derived military doctrines include squad-level snipers. Snipers have increasingly been demonstrated as being useful by US and UK forces in the recent Iraq campaign in a fire support role to cover the movement of infantry, especially in urban areas.
Military snipers from the US, UK, and other countries that adopt their military doctrine are typically deployed in two-man sniper teams consisting of a shooter and spotter. A common practice is for a shooter and a spotter to take turns in order to avoid eye fatigue. In most recent combat operations occurring in large densely populated towns such as Fallujah, Iraq, two teams would be deployed together to increase their security and effectiveness in an urban environment. A sniper team would be armed with their long range weapon, and a shorter ranged weapon to engage and protect the team should enemies come in close contact. German doctrine of largely independent snipers and emphasis on concealment developed during the Second World War have been most influential on modern sniper tactics, currently used throughout Western militaries (examples are specialized camouflage clothing, concealment in terrain and emphasis on coup d'œil).
History.
Before the development of rifling, firearms were smoothbore and inaccurate over long distance. Barrel rifling was invented at the end of the fifteenth century, but was only employed in large cannons. Over time, rifling, along with other gunnery advances, has increased the performance of modern firearms.
Early history.
Early forms of sniping, or marksmanship were used during the American Revolutionary War. For instance, in 1777 at the battle of Saratoga the Colonists hid in the trees and used early model rifles to shoot British officers. Most notably, Timothy Murphy shot and killed General Simon Fraser of Balnain on 7 October 1777 at a distance of about 400 yards. During the Battle of Brandywine, Capt. Patrick Ferguson had a tall, distinguished American officer in his rifle's iron sights. Ferguson did not take the shot, as the officer had his back to Ferguson; only later did Ferguson learn that George Washington had been on the battlefield that day.
A special unit of marksmen was established during the Napoleonic Wars in the British Army. While most troops at that time used inaccurate smoothbore muskets, the British "Green Jackets" (named for their distinctive green uniforms) used the famous Baker rifle. Through the combination of a leather wad and tight grooves on the inside of the barrel (rifling), this weapon was far more accurate, though slower to load. These Riflemen were the elite of the British Army, and served at the forefront of any engagement, most often in skirmish formation, scouting out and delaying the enemy. Another term, "sharp shooter" was in use in British newspapers as early as 1801. In the "Edinburgh Advertiser", 23 June 1801, can be found the following quote in a piece about the North British Militia; "This Regiment has several Field Pieces, and two companies of Sharp Shooters, which are very necessary in the modern Stile of War". The term appears even earlier, around 1781, in Continental Europe, translated from the German Scharfschütze.
First sniper rifle.
The Whitworth rifle was arguably the first long-range sniper rifle in the world. Designed by Sir Joseph Whitworth, a prominent British engineer, it used twisted hexagonal barrels instead of traditional round rifled barrels, which meant that the projectile did not have to bite into grooves as was done with conventional rifling. His rifle was far more accurate than the Pattern 1853 Enfield, which had shown some weaknesses during the recent Crimean War. At trials in 1857 which tested the accuracy and range of both weapons, Whitworth's design outperformed the Enfield at a rate of about three to one. The Whitworth rifle was capable of hitting the target at a range of 2,000 yards, whereas the Enfield could only manage it at 1,400 yards.
During the Crimean War, the first optical sights were designed to fit onto rifles. Much of this pioneering work was the brainchild of Colonel D. Davidson, using optical sights produced by Chance Brothers of Birmingham. This allowed a marksman to observe and target objects more accurately at a greater distance than ever before. The telescopic sight, or scope, was originally fixed and could not be adjusted, which therefore limited its range.
Despite its success at the trials, the rifle was not adopted by the British Army. However, the Whitworth Rifle Company was able to sell the weapon to the French army, and also to the Confederacy during the American Civil War. 
Both the Union and Confederate armies employed sharpshooters. The most notable incident was during the Battle of Spotsylvania Court House, where on 9 May 1864, Union General John Sedgwick was killed at a range of about 1000 yd after saying the enemy "couldn't hit an elephant at this distance."
Second Boer War.
During the Boer War the latest breech-loading rifled guns with magazines and smokeless powder were used by both sides. The British were equipped with the Lee-Metford rifle, while the Boers had received the latest Mauser rifles from Germany. In the open terrain of South Africa the marksmen were a crucial component to the outcome of the battle.
The first British sniper unit began life as the Lovat Scouts, a Scottish Highland regiment formed in 1899, that earned high praise during the Second Boer War (1899–1902). The unit was formed by Lord Lovat and reported to an American, Major Frederick Russell Burnham, the British Army Chief of Scouts under Lord Roberts. Burnham fittingly described these scouts as "half wolf and half jackrabbit.". Just like their Boer scout opponents, these scouts were well practised in the arts of marksmanship, field craft, map reading, observation, and military tactics. They were skilled woodsmen and practitioners of discretion: "He who shoots and runs away, lives to shoot another day." They were also the first known military unit to wear a ghillie suit. 
Hesketh Hesketh-Prichard said of them that "keener men never lived", and that "Burnham was the greatest scout of our time." Burnham distinguished himself in wars in South Africa, Rhodesia, and in Arizona fighting the Apaches, and his definitive work, "Scouting on Two Continents," provides a dramatic and enlightening picture of what a sniper was at the time and how he operated.
After the war, this regiment went on to formally become the first official sniper unit, then better known as "sharpshooters".
World War I.
During World War I, snipers appeared as deadly sharpshooters in the trenches. At the start of the war, only Imperial Germany had troops that were issued scoped sniper rifles. Although sharpshooters existed on all sides, the Germans specially equipped some of their soldiers with scoped rifles that could pick off enemy soldiers showing their heads out of their trench. At first the French and British believed such hits to be coincidental hits, until the German scoped rifles were discovered. During World War I, the German army received a reputation for the deadliness and efficiency of its snipers, partly because of the high-quality lenses that German industry could manufacture.
Soon the British army began to train their own snipers in specialized sniper schools. Major Hesketh Hesketh-Prichard was given formal permission to begin sniper training in 1915, and founded the First Army School of Sniping, Observation, and Scouting at Linghem in France in 1916. Starting with a first class of only six, in time he was able to lecture to large numbers of soldiers from different Allied nations, proudly proclaiming in a letter that his school was turning out snipers at three times the rate of any such other school in the world.
He also devised a metal-armoured double loophole that would protect the sniper observer from enemy fire. The front loophole was fixed, but the rear was housed in a metal shutter sliding in grooves. Only when the two loopholes were lined up—a one-to-twenty chance—could an enemy shoot between them. Another innovation was the use of a dummy head to find the location of an enemy sniper. The papier-mâché figures were painted to resemble soldiers to draw sniper fire. Some were equipped with rubber surgical tubing so the dummy could "smoke" a cigarette and thus appear realistic. Holes punched in the dummy by enemy sniper bullets then could be used for triangulation purposes to determine the position of the enemy sniper, who could then be attacked with artillery fire. He developed many of the modern techniques in sniping, including the use of spotting scopes and working in pairs, and using Kim's Game to train observational skills.
In 1920, he wrote his account of his war time activities in his book "", which is still referenced by modern authors on the subject.
The main sniper rifles used during the First World War were the German Mauser Gewehr 98; the British Pattern 1914 Enfield and Lee-Enfield SMLE Mk III, the Canadian Ross Rifle, the American M1903 Springfield, and the Russian M1891 Mosin–Nagant.
World War II.
During the interbellum, most nations dropped their specialized sniper units, notably the Germans. Effectiveness and dangers of snipers once again came to the fore during the Spanish Civil War. The only nation that had specially trained sniper units during the 1930s was the Soviet Union. Soviet snipers were trained in their skills as marksmen, in using the terrain to hide themselves from the enemy and the ability to work alongside regular forces. This made the Soviet sniper training focus more on "normal" combat situations than those of other nations.
Snipers reappeared as important factors on the battlefield from the first campaign of World War II. During Germany's 1940 campaigns, it appeared that lone, well-hidden French and British snipers could halt the German advance for a significant amount of time. For example, during the pursuit to Dunkirk, British snipers were able to significantly delay the German infantry's advance. This prompted the British once again to increase training of specialized sniper units. Apart from marksmanship, British snipers were trained to blend in with the environment, often by using special camouflage clothing for concealment. However, because the British Army offered sniper training exclusively to officers and non-commissioned officers, the resulting smaller number of trained snipers in the combat units considerably reduced their overall effectiveness.
During the Winter War, Finnish snipers took a heavy toll of the invading Soviet army. Simo Häyhä is credited with 505 confirmed kills, most with the Finnish version of the iron-sighted bolt action Mosin-Nagant.
One of the best known battles involving snipers, and the battle that made the Germans reinstate their specialized sniper training, was the Battle of Stalingrad. Their defensive position inside a city filled with rubble meant that Soviet snipers were able to inflict significant casualties on the Wehrmacht troops. Because of the nature of fighting in city rubble, snipers were very hard to spot and seriously dented the morale of the German attackers. The best known of these snipers was probably Vasily Zaytsev, featured in the novel "War of the Rats" and the subsequent film "Enemy At The Gates".
German "Scharfschützen" were prepared before the war, equipped with Karabiner 98 and later Gewehr 43 rifles, but there were often not enough of these weapons available, and as such some were armed with captured scoped Mosin-Nagant 1891/30, SVT or Czech Mauser rifles. The Wehrmacht re-established its sniper training in 1942, drastically increasing the number of snipers per unit with the creation of an additional 31 sniper training companies by 1944. German snipers were at the time the only snipers in the world issued with purpose-manufactured sniping ammunition, known as the 'effect-firing' sS round. The 'effect-firing' sS round featured an extra carefully measured propellant charge and seated a heavy 12.8 gram (198 gr) full-metal-jacketed boat-tail projectile of match-grade build quality, lacking usual features such as a seating ring to improve the already high ballistic coefficient of .584 (G1) further. For aiming optics German snipers used the Zeiss Zielvier 4x (ZF39) telescopic sight which had bullet drop compensation in 50 m increments for ranges from 100 m up to 800 m or in some variations from 100 m up to 1000 m or 1200 m. There were ZF42, Zielfernrohr 43 (ZF 4), Zeiss Zielsechs 6x and other telescopic sights by various manufacturers like the Ajack 4x, Hensoldt Dialytan 4x and Kahles Heliavier 4x with similar features employed on German sniper rifles. Several different mountings produced by various manufacturers were used for mounting aiming optics to the rifles. In February 1945 the Zielgerät 1229 active infrared aiming device was issued for night sniping with the StG 44 assault rifle.
428,335 individuals received Red Army sniper training, including Soviet and non-Soviet partisans, with 9,534 receiving the sniping 'higher qualification'. The two six-month training courses in 1942 for women alone trained nearly 55,000 snipers. On average there was at least one sniper in an infantry platoon and one in every reconnaissance platoon, including in tank and even artillery units. Some used the PTRD anti-tank rifle with an adapted scope as an early example of an anti-materiel rifle.
In the United States Armed Forces, sniper training was only very elementary and was mainly concerned with being able to hit targets over long distances. Snipers were required to be able to hit a body over 400 meters away, and a head over 200 meters away. There was almost no instruction in blending into the environment. Sniper training varied from place to place, resulting in wide variation in the qualities of snipers. The main reason the US did not extend sniper training beyond long-range shooting was the limited deployment of US soldiers until the Normandy Invasion. During the campaigns in North Africa and Italy, most fighting occurred in arid and mountainous regions where the potential for concealment was limited, in contrast to Western and Central Europe.
The U.S. Army's lack of familiarity with sniping tactics proved disastrous in Normandy and the campaign in Western Europe where they encountered well trained German snipers. In Normandy, German snipers remained hidden in the dense vegetation and were able to encircle American units, firing at them from all sides. The American and British forces were surprised by how near the German snipers could approach in safety and attack them, as well as by their ability to hit targets at up to 1,000m. A notable mistake made by inexperienced American soldiers was to lie down and wait when targeted by German snipers, allowing the snipers to pick them off one after another. German snipers often infiltrated Allied lines and sometimes when the front-lines moved, they continued to fight from their sniping positions, refusing to surrender until their rations and munitions were exhausted.
Those tactics were also consequences of changes in German enrollment. After several years of war and heavy losses on the Eastern Front, the German army was forced to rely more heavily on enrolling teenage soldiers. Due to lack of training in more complex group tactics and thanks to rifle training provided by the Hitlerjugend those soldiers would often be used as autonomous left-behind snipers. While an experienced sniper would take a few lethal shots and retreat to a safer position, those young boys, due both to disregard for their own safety and to lack of tactical experience would frequently remain in a concealed position and fight until they ran out of ammunition or were killed or wounded. While this tactic would generally end in the demise of the sniper, giving rise to the nickname "Suicide Boys" that was given to those soldiers, this irrational behavior would prove quite disruptive to the Allied forces' progression. After World War II, many elements of German sniper training and doctrine were copied by other countries.
In the Pacific War, the Empire of Japan trained snipers. In the jungles of Asia and the Pacific Islands, snipers posed a serious threat to the U.S, British, and Commonwealth troops. Japanese snipers were specially trained to use the environment to conceal themselves. Japanese snipers used foliage on their uniforms and dug well-concealed hide-outs that were often connected with small trenches. There was no need for long range accuracy because most combat in the jungle took place within a few hundred meters. Japanese snipers were known for their patience and ability to remain hidden for long periods. They almost never left their carefully camouflaged hiding spots. This meant that whenever a sniper was in the area, the location of the sniper could be determined after the sniper had fired a few shots. The Allies used their own snipers in the Pacific, notably the U.S. Marines, who used M1903 Springfield rifles.
Common sniper rifles used during the Second World War include: the Soviet M1891/30 Mosin Nagant and, to a lesser extent, the SVT-40; the German Mauser Karabiner 98k and Gewehr 43; the British Lee-Enfield No. 4 and Pattern 1914 Enfield; the Japanese Arisaka 97; the American M1903A4 Springfield and M1C Garand. The Italians trained few snipers and supplied them with a scoped Carcano Model 1891.
Longest recorded sniper kills.
The longest confirmed sniper kill in combat was achieved by Craig Harrison, a Corporal of Horse (CoH) in the Blues and Royals RHG/D of the British Army. In November 2009, Harrison struck two Taliban machine gunners consecutively south of Musa Qala in Helmand Province in Afghanistan at a range of 2475 m using a L115A3 Long Range Rifle.
The QTU Lapua external ballistics software, using continuous doppler drag coefficient (Cd) data provided by Lapua, predicts that such shots traveling 2475 m would likely have struck their targets after nearly 6.0 seconds of flight time, having lost 93% of their kinetic energy, retaining 255 m/s of their original 936 m/s velocity, and having dropped 121.39 m or 2.8° from the original bore line. Due to the extreme distances and travel time involved, even a light cross-breeze of 2.7 m/s would have diverted such shots 9.2 m off target, which would have required compensation. The calculation assumes a "flat-fire scenario" (a situation where the shooting and target positions are at equal elevation), utilizing British military custom high pressure .338 Lapua Magnum cartridges, loaded with 16.2 g (250 gr) Lapua LockBase B408 bullets, fired at 936 m/s (3,071 ft/s) muzzle velocity under the following on-site (average) atmospheric conditions: barometric pressure: 1019 hPa at sea-level equivalent or 899 hPa on-site, humidity: 25.9%, and temperature: 15 C in the region for November 2009, resulting in an air density ρ = 1.0854 kg/m3 at the 1043 m elevation of Musa Qala.
CoH Craig Harrison mentions in reports that the environmental conditions were perfect for long range shooting, no wind, mild weather, clear visibility. In a BBC interview, Harrison reported it took about nine shots for him and his spotter to initially range the target successfully.
Police.
Law enforcement snipers, commonly called police snipers, and military snipers differ in many ways, including their areas of operation and tactics. A police sharpshooter is part of a police operation and usually takes part in relatively short missions. Police forces typically deploy such sharpshooters in hostage scenarios. This differs from a military sniper, who operates as part of a larger army, engaged in warfare. Sometimes as part of a SWAT team, police snipers are deployed alongside negotiators and an assault team trained for close quarters combat. As policemen, they are trained to shoot only as a last resort, when there is a direct threat to life; the police sharpshooter has a well-known rule: "Be prepared to take a life to save a life." Police snipers typically operate at much shorter ranges than military snipers, generally under 100 m and sometimes even less than 50 m. Both types of snipers do make difficult shots under pressure, and often perform one-shot kills.
Police units that are unequipped for tactical operations may rely on a specialized SWAT team, which may have a dedicated sniper. Some police sniper operations begin with military assistance. Police snipers placed in vantage points, such as high buildings, can provide security for events. In one high-profile incident, Mike Plumb, a SWAT sniper in Columbus, Ohio, prevented a suicide by shooting a revolver out of the individual's hand, leaving him unharmed.
The need for specialized training for police sharpshooters was made apparent in 1972 during the Munich massacre when the German police could not deploy specialized personnel or equipment during the standoff at the airport in the closing phase of the crisis, and consequently all of the Israeli hostages were killed. The German police only had regular police who were selected if they engaged in hunting as a hobby. While the German army did have snipers in 1972, the use of snipers of the German army in the scenario was impossible due to the German constitution's explicit prohibition of the use of the military in domestic matters. This lack of trained snipers who could be used in civilian roles was later addressed with the founding of the specialized police counter-terrorist unit GSG 9.
Training.
Military sniper training aims to teach a high degree of proficiency in camouflage and concealment, stalking, observation and map reading as well as precision marksmanship under various operational conditions. Trainees typically shoot thousands of rounds over a number of weeks, while learning these core skills.
Snipers are trained to squeeze the trigger straight back with the ball of their finger, to avoid jerking the gun sideways. The most accurate position is prone, with a sandbag supporting the stock, and the stock's cheek-piece against the cheek. In the field, a bipod can be used instead. Sometimes a sling is wrapped around the weak arm (or both) to reduce stock movement. Some doctrines train a sniper to breathe deeply before shooting, then hold their lungs empty while they line up and take their shot. Some go further, teaching their snipers to shoot between heartbeats to minimize barrel motion.
Accuracy.
The key to sniping is accuracy, which applies to both the weapon and the shooter. The weapon should be able to consistently place shots within tight tolerances. The sniper in turn must utilize the weapon to accurately place shots under varying conditions.
A sniper must have the ability to accurately estimate the various factors that influence a bullet's trajectory and point of impact such as: range to the target, wind direction, wind velocity, altitude and elevation of the sniper and the target and ambient temperature. Mistakes in estimation compound over distance and can decrease lethality or cause a shot to miss completely.
Snipers zero their weapons at a target range or in the field. This is the process of adjusting the scope so that the bullet's points-of-impact is at the point-of-aim (centre of scope or scope's cross-hairs) for a specific distance. A rifle and scope should retain its zero as long as possible under all conditions to reduce the need to re-zero during missions.
A sandbag can serve as a useful platform for shooting a sniper rifle, although any soft surface such as a rucksack will steady a rifle and contribute to consistency. In particular, bipods help when firing from a prone position, and enable the firing position to be sustained for an extended period of time. Many police and military sniper rifles come equipped with an adjustable bipod. Makeshift bipods known as shooting sticks can be constructed from items such as tree branches or ski poles.
Range and accuracy vary depending on the cartridge and specific ammunition types that are used. Typical ranges for common battle field cartridges are as follows:
U.S. military.
Servicemen volunteer for the rigorous sniper training and are accepted on the basis of their aptitude, physical ability, marksmanship, patience and mental stability. Military snipers may be further trained as forward air controllers (FACs) to direct air strikes or forward observers (FOs) to direct artillery or mortar fire.
Russian Army.
From 2011, the Russian armed forces has run newly developed sniper courses in military district training centres. In place of the Soviet practice of mainly squad sharpshooters, which were often designated during initial training (and of whom only few become snipers "per se"), "new" Army snipers are to be trained intensively for 3 months (for conscripts) or longer (for contract soldiers). The training program includes theory and practice of countersniper engagements, artillery spotting and coordination of air support. The first instructors are the graduates of the Solnechnogorsk sniper training centre.
The method of sniper deployment, according to the Ministry of Defence, is likely to be one three-platoon company at the brigade level, with one of the platoons acting independently and the other two supporting the battalions as needed.
Targeting.
The range to the target is measured or estimated as precisely as conditions permit and correct range estimation becomes absolutely critical at long ranges, because a bullet travels with a curved trajectory and the sniper must compensate for this by aiming higher at longer distances. If the exact distance is not known the sniper may compensate incorrectly and the bullet path may be too high or low. As an example, for a typical military sniping cartridge such as 7.62x51mm NATO (.308 Winchester) M118 Special Ball round this difference (or “drop”) from 700 to(-) is 200 mm. This means that if the sniper incorrectly estimated the distance as 700 meters when the target was in fact 800 meters away, the bullet will be 200 millimeters lower than expected by the time it reaches the target.
Laser rangefinders may be used, and range estimation is often the job of both parties in a team. One useful method of range finding without a laser rangefinder is comparing the height of the target (or nearby objects) to their size on the mil dot scope, or taking a known distance and using some sort of measure (utility poles, fence posts) to determine the additional distance. The average human head is 150 mm in width, average human shoulders are 500 mm apart and the average distance from a person's pelvis to the top of their head is 1000 mm.
To determine the range to a target without a laser rangefinder, the sniper may use the mil dot reticle on a scope to accurately find the range. Mil dots are used like a slide rule to measure the height of a target, and if the height is known, the range can be as well. The height of the target (in yards) ×1000, divided by the height of the target (in mils), gives the range in yards. This is only in general, however, as both scope magnification (7×, 40×) and mil dot spacing change. The USMC standard is that 1 mil (that is, 1 milliradian) equals 3.438 MOA (minute of arc, or, equivalently, minute of angle), while the US Army standard is 3.6 MOA, chosen so as to give a diameter of 1 yard at a distance of 1000 yards (or equivalently, a diameter of 1 meter at a range of 1 kilometer.) Many commercial manufacturers use 3.5, splitting the difference, since it is easier to work with.
 "Explanation: 1 MIL = 1 milli-radian. That is, 1 MIL = 1x10^-3 radian. But, 10^-3 rad x (360 deg/ (2 x Pi) radians) = 0.0573 degrees. Now, 1 MOA = 1/60 degree = 0.01667 degrees. Hence, there are 0.0573/0.01667 = 3.43775 MOA per MIL, where MIL is defined as a milli-radian. On the other hand, defining a mil-dot by the US Army way, to equate it to 1 yd at 1000 yd, means the Army's mil-dot is approximately 3.6 MOA."
It is important to note that angular mil ("mil") is only an approximation of a milliradian and different organizations use different approximations.
At longer ranges, bullet drop plays a significant role in targeting. The effect can be estimated from a chart which may be memorized or taped to the rifle, although some scopes come with Bullet Drop Compensator (BDC) systems that only require the range be dialed in. These are tuned to both a specific class of rifle and specific ammunition. Every bullet type and load will have different ballistics. .308 Federal 175 grain (11.3 g) BTHP match shoots at 2600 ft/s. Zeroed at 100 yd, a 16.2 MOA adjustment would have to be made to hit a target at 600 yd. If the same bullet was shot with 168 grain (10.9 g), a 17.1 MOA adjustment would be necessary.
Shooting uphill or downhill is confusing for many because gravity does not act perpendicular to the direction the bullet is traveling. Thus, gravity must be divided into its component vectors. Only the fraction of gravity equal to the cosine of the angle of fire with respect to the horizon affects the rate of fall of the bullet, with the remained adding or subtracting negligible velocity to the bullet along its trajectory. To find the correct zero, the sniper multiplies the actual distance to the range by this fraction and aims as if the target were that distance away. For example, a sniper who observes a target 500 meters away at a 45-degree angle downhill would multiply the range by the cosine of 45 degrees, which is 0.707. The resulting distance will be 353 meters. This number is equal to the horizontal distance to the target. All other values, such as windage, time-to-target, impact velocity, and energy will be calculated based on the actual range of 500 meters. Recently, a small device known as a cosine indicator has been developed. This device is clamped to the tubular body of the telescopic sight, and gives an indicative readout in numerical form as the rifle is aimed up or down at the target. This is translated into a figure used to compute the horizontal range to the target.
Windage which plays a significant role, the effect increasing with wind speed or the distance of the shot. The slant of visible convections near the ground can be used to estimate crosswinds, and correct the point of aim. All adjustments for range, wind, and elevation can be performed by aiming off the target, called "holding over" or Kentucky windage. Alternatively, the scope can be adjusted so that the point of aim is changed to compensate for these factors, sometimes referred to as "dialing in". The shooter must remember to return the scope to zeroed position. Adjusting the scope allows for more accurate shots, because the cross-hairs can be aligned with the target more accurately, but the sniper must know exactly what differences the changes will have on the point-of-impact at each target range.
For moving targets, the point-of-aim is ahead of the target in the direction of movement. Known as "leading" the target, the amount of "lead" depends on the speed and angle of the target's movement as well as the distance to the target. For this technique, holding over is the preferred method. Anticipating the behavior of the target is necessary to accurately place the shot.
Hide sites and hiding techniques.
The term "hide site" refers to a covered and concealed position from which a sniper and his team can conduct surveillance and/or fire at targets. A good hide conceals and camouflages the sniper effectively, provides cover from enemy fire and allows a wide view of the surrounding area.
The main purpose of ghillie suits and hide sites is to break up the outline of a person with a rifle.
Many snipers use ghillie suits to hide and stay hidden. Ghillie suits vary according to the terrain into which the sniper wishes to blend. For example, in dry, grassy wasteland the sniper will typically wear a ghillie suit covered in dead grass.
Sniper teams.
Sniper rifles are classified as crew-served, as the term is used in the United States military. A sniper team (or sniper cell) consists of a combination of one or more "shooters" with force protection elements and support personnel: such as a "spotter" or a "flanker". Within the Table of Organization and Equipment for both the United States Army and the U.S. Marine Corps, the operator of the weapon has an assistant trained to fulfill multiple roles, in addition to being sniper qualified in the operation of the weapon. 
The shooter(s) fires the shot while the spotter(s) assists in observation of targets, atmospheric conditions and handles ancillary tasks as immediate security of their location, communication with other parties; including directing artillery fire and close air support. The flanker(s)' task is to have observed areas not immediately visible to the sniper or spotter and assist with the team's perimeter and rear security, therefore they are usually armed with an assault rifle or battle rifle. Both spotter and flanker carries additional ammunition and associated equipment.
The spotter detects, observes, and assigns targets and watches for the results of the shot. Using their spotting scope and/or rangefinder, they will also read the wind by using physical indicators and the mirage caused by the heat on the ground. Also, in conjunction with the shooter, they will accurately make calculations for distance, angle shooting (slant range), mil dot related calculations, correction for atmospheric conditions and leads for moving targets. It is not unusual for the spotter to be equipped with a notepad and a laptop computer specifically for performing these calculations.
Tactics.
Shot placement.
Shot placement varies considerably with the type of sniper being discussed. Military snipers, who generally do not engage targets at less than 300 m, usually attempt body shots, aiming at the chest. These shots depend on tissue damage, organ trauma, and blood loss to make the kill.
Police snipers who generally engage at much shorter distances may attempt more precise shot at particular parts of body or particular devices: in one event in 2007 in Marseille, a GIPN sniper took a shot from 80 m at the pistol of a police officer threatening to commit suicide, destroying the weapon and preventing the police officer from killing himself.
In a high-risk or instant-death hostage situation, police snipers may take head shots to ensure an instant kill. The snipers aim for the "apricot", or the medulla oblongata, located inside the head, a part of the brain that controls involuntary movement that lies at the base of the skull. Some ballistics and neurological researchers have argued that severing the spinal cord at an area near the second cervical vertebra is actually achieved, usually having the same effect of preventing voluntary motor activity, but the debate on the matter remains largely academic at present.
With moving targets it is necessary to lead the target to compensate for movement during the flight of the projectile.
Targets.
Snipers can target personnel or materiel, but most often they target the most important enemy personnel such as officers or specialists (e.g. communications operators) so as to cause maximum disruption to enemy operations. Other personnel they might target include those who pose an immediate threat to the sniper, like dog handlers, who are often employed in a search for snipers.
A sniper identifies officers by their appearance and behavior such as symbols of rank, talking to radio operators, sitting as a passenger in a car, having military servants, binoculars/map cases or talking and moving position more frequently. If possible, snipers shoot in descending order by rank, or if rank is unavailable, they shoot to disrupt communications.
Since most kills in modern warfare are by crew-served weapons, reconnaissance is one of the most effective uses of snipers. They use their aerobic conditioning, infiltration skills and excellent long-distance observation equipment and tactics to approach and observe the enemy. In this role, their rules of engagement let them engage only high value targets of opportunity.
Some rifles, such as the Denel NTW-20 and Vidhwansak are designed for a purely anti-materiel (AM) role, e.g. shooting turbine disks of parked aircraft, missile guidance packages, expensive optics, and the bearings, tubes or wave guides of radar sets. A sniper equipped with the correct rifle can target radar dishes, water containers, the engines of vehicles, and any number of other targets. Other rifles, such as the .50 caliber rifles produced by Barrett and McMillan are not designed exclusively as AM rifles, but are often employed in such a way, providing the range and power needed for AM applications in a lightweight package compared to most traditional AM rifles. Other calibers, such as the .408 Cheyenne Tactical and the .338 Lapua Magnum are designed to be capable of limited AM application, but are ideally suited as long range anti-personnel rounds.
Baiting.
Baiting is the utilization of dropped objects for potential targets to find and pick up. In the Iraq war, picking up weapons and munitions could be considered evidence of insurgency. Snipers would drop weapons and wait for targets to pick up the weapons so they could engage the target. According to court documents quoted by the Washington Post, the U.S. military's Asymmetric Warfare Group encouraged snipers to drop items "such as detonation cords, plastic explosives and ammunition" then kill Iraqis who handled the items.
 "Baiting is putting an object out there that we know they will use, with the intention of destroying the enemy...Basically, we would put an item out there and watch it. If someone found the item, picked it up and attempted to leave with the item, we would engage the individual as I saw this as a sign they would use the item against U.S. Forces."
 — Capt. Matthew P. Didier, the leader of an elite sniper scout platoon attached to the 1st Battalion of the 501st Infantry Regiment, in a sworn statement
Relocating.
Often in situations with multiple targets, snipers use relocation. After firing a few shots from a certain position, snipers move unseen to another location before the enemy can determine where he or she is and mount a counter-attack. Snipers will frequently use this tactic to their advantage, creating an atmosphere of chaos and confusion. In other, rarer situations, relocation is used to eliminate the factor of wind.
Sound masking.
As sniper rifles are often extremely powerful and consequently loud, it is common for snipers to use a technique known as sound masking. When employed by a highly skilled marksman, this tactic can be used as a substitute for a noise suppressor. Very loud sounds in the environment, such as artillery shells air bursting or claps of thunder, can often mask the sound of the shot. This technique is frequently used in clandestine operations, infiltration tactics, and guerrilla warfare.
Psychological warfare.
Due to the surprise nature of sniper fire, high lethality of aimed shots and frustration at the inability to locate and attack snipers, sniper tactics have a significant effect on morale. Extensive use of sniper tactics can be used to induce constant stress in opposing forces. In many ways, the psychological impact imposed by snipers is quite similar to those of landmines, booby-traps, and IEDs (constant threat, high "per event" lethality, inability to strike back).
Historically, captured snipers are often summarily executed. This happened during World War I, and World War II. As a result, if a sniper is in imminent danger of capture, he may discard any items which might indicate his status as a sniper. The risk of captured snipers being summarily executed is explicitly referred to in Chapter 6 of US Army doctrine document FM 3-060.11 entitled 'SNIPER AND COUNTERSNIPER TACTICS, TECHNIQUES, AND PROCEDURES':
Historically, units that suffered heavy and continual casualties from urban sniper fire and were frustrated by their inability to strike back effectively often have become enraged. Such units may overreact and violate the laws of land warfare concerning the treatment of captured snipers. This tendency is magnified if the unit has been under the intense stress of urban combat for an extended time. It is vital that commanders and leaders at all levels understand the law of land warfare and understand the psychological pressures of urban warfare. It requires strong leadership and great moral strength to prevent soldiers from releasing their anger and frustration on captured snipers or civilians suspected of sniping at them.<ref name="http://www.globalsecurity.org.countersniper"></ref>
The negative reputation of snipers can be traced back to the American Revolution, when American "Marksmen" would intentionally target British officers, an act considered uncivilized by the British Army at the time (this reputation would be cemented during the Battle of Saratoga, when Benedict Arnold allegedly ordered his marksmen to target British General Simon Fraser, an act that would win the battle and French support). The British side used specially selected sharpshooters as well, often German mercenaries.
To demoralize enemy troops, snipers can follow predictable patterns. During the 26th of July Movement in the Cuban Revolution, the revolutionaries led by Fidel Castro always killed the foremost man in a group of President Batista's soldiers. Realizing this, none of Batista's men would walk first, as it was suicidal. This effectively decreased the army's willingness to search for rebel bases in the mountains. An alternative approach to this psychological process is to kill the second man in the row, leading to the psychological effect of nobody wanting to follow the "leader".
Counter-sniper tactics.
The occurrence of sniper warfare has led to the evolution of many counter-sniper tactics in modern military strategies. These aim to reduce the damage caused by a sniper to an army, which can often be harmful to both combat capabilities and morale.
The risk of damage to a chain of command can be reduced by removing or concealing features which would otherwise indicate an officer's rank. Modern armies tend to avoid saluting officers in the field, and eliminate rank insignia on battle dress uniforms (BDU). Officers can seek maximum cover before revealing themselves as good candidates for elimination through actions such as reading maps or using radios.
Friendly snipers can be used to hunt the enemy sniper. Besides direct observation, defending forces can use other techniques. These include calculating the trajectory of a bullet by triangulation. Traditionally, triangulation of a sniper's position was done manually, though radar-based technology has recently become available. Once located, the defenders can attempt to approach the sniper from cover and overwhelm him. The United States military is funding a project known as RedOwl (Robot Enhanced Detection Outpost With Lasers), which uses laser and acoustic sensors to determine the exact direction from which a sniper round has been fired.
The more rounds fired by a sniper, the greater the number of chances a target has to locate him. Thus, attempts to draw fire are often made, sometimes by offering a helmet slightly out of concealment, a tactic successfully employed in the Winter War by the Finns known as "Kylmä-Kalle" (Cold Charlie). They used a shop mannequin or other doll dressed as a tempting target, such as an officer. The doll was then presented as if it were a real man sloppily covering himself. Usually, Soviet snipers were unable to resist the temptation of an apparently easy kill. Once the angle where the bullet came from was determined, a large calibre gun, such as a Lahti L-39 "Norsupyssy" ("Elephant rifle") anti-tank rifle was fired at the sniper to kill him.
Other tactics include directing artillery or mortar fire onto suspected sniper positions, the use of smoke screens, placing tripwire-operated munitions, mines, or other booby-traps near suspected sniper positions. Even dummy trip-wires can be placed to hamper sniper movement. If anti-personnel mines are unavailable, it is possible to improvise booby-traps by connecting trip-wires to hand grenades, smoke grenades or flares. Though these may not kill the sniper, they will reveal the location of the sniper(s). Booby-trap devices can be placed near likely sniper hides, or along the probable routes to and from the positions. Knowledge of sniper field-craft will assist in this task.
One very old counter-sniper tactic is to tie rags onto bushes or similar items in suspected sniper hides. These rags flutter in the breeze creating random movements in the corner of the sniper's eye, which he/she will often find distracting. The greatest virtue of this tactic is its simplicity and ease of implementation; however, it is unlikely to prevent a skilled sniper from selecting targets, and may in fact provide a sniper with additional information about the wind near the target.
The use of canine units was very successful, especially during the Vietnam War. A trained dog can easily determine direction from the sound of the bullet, and will lie down with its head pointed at the origin of the gunshot. 
Irregular and asymmetric warfare.
The use of sniping (in the sense of shooting at relatively long range from a concealed position) to murder came to public attention in a number of sensational U.S. criminal cases, including the Austin sniper incident of 1966 (Charles Whitman), the John F. Kennedy assassination (Lee Harvey Oswald), and the Beltway sniper attacks of late 2002 (Lee Boyd Malvo). However, these incidents usually do not involve the range or skill of military snipers; in all three cases the perpetrators had U.S. military training, but in other specialties. News reports will often (inaccurately) use the term sniper to describe anyone shooting with a rifle at another person.
Sniping has been used in asymmetric warfare situations, for example in the Northern Ireland Troubles, where in 1972, the bloodiest year of the conflict, the majority of the soldiers killed were shot by concealed IRA riflemen. There were some instances in the early 1990s of British soldiers and RUC personnel being shot with .50 caliber Barrett rifles by sniper teams collectively known as the South Armagh sniper.
The sniper is particularly suited to combat environments where one side is at a disadvantage. A careful sniping strategy can use a few individuals and resources to thwart the movement or other progress of a much better equipped or larger force. Sniping enables a few persons to instil terror in a much larger regular force — regardless of the size of the force the snipers are attached to. It is widely accepted that sniping, while effective in specific instances, is much more effective as a broadly deployed psychological attack or as a force-multiplier.
Snipers are less likely to be treated mercifully than non-snipers if captured by the enemy. The rationale for this is that ordinary soldiers shoot at each other at 'equal opportunity' whilst snipers take their time in tracking and killing individual targets in a methodical fashion with a relatively low risk of retaliation.
War in Iraq.
In 2003, the U.S.-led multinational coalition composed of primarily U.S. and U.K. troops occupied Iraq and attempted to establish a new government in the country. However, shortly after the initial invasion, violence against coalition forces and among various sectarian groups led to asymmetric warfare with the Iraqi insurgency and civil war between many Sunni and Shia Iraqis.
Through November 2005, when the Pentagon had last reported a sniper fatality, the Army had attributed 28 of 2,100 U.S. deaths to enemy snipers. More recently, since 2006, insurgent snipers such as "Juba" have caused problems for American troops. Claims have been made that Juba have shot up to 37 American soldiers in Iraq as of October 2006.
In 2006, training materials obtained by U.S. intelligence showed that snipers fighting in Iraq were urged to single out and attack engineers, medics, and chaplains on the theory that those casualties would demoralize entire enemy units. Among the training materials, there included an insurgent sniper training manual that was posted on the Internet. Among its tips for shooting U.S. troops, there read: "Killing doctors and chaplains is suggested as a means of psychological warfare."
Afghanistan.
Some sniper teams in Afghanistan have killed large numbers of Taliban in quite short periods of time. For example, while in Helmand Province, two British snipers (part of the Welsh Guards Battle group) shot dead a total of 75 Taliban in only 40 days during the summer of 2009. In one session of duty, lasting just two hours, they shot and killed eight Taliban. On another occasion, the same team scored a "Quigley" (i.e., killing two Taliban with a single bullet) at a range of 196 metres.
Taliban snipers have themselves caused problems for coalition forces. For example, over a four-month period in early 2011, two Taliban snipers shot dead two British soldiers and wounded six others at an outpost in Qadrat, Helmand province. In one unusual incident, an unnamed 55-year-old ex-Mujahideen fighter with a motorbike and an old British-made Enfield rifle killed two British soldiers with a single shot, hitting the first in the head and the second in the neck.
Arab Spring.
Sniper activity has been reported during the Arab Spring civil unrest in Libya in 2011, both from anti-governmental and pro-governmental supporters, and in Syria at least from pro-government forces.
Notable military snipers.
Even before firearms were available, soldiers such as archers were specially trained as elite marksmen.
Bibliography.
</dl>

</doc>
<doc id="28130" url="http://en.wikipedia.org/wiki?curid=28130" title="Sign">
Sign

A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object—for instance, thunder is a sign of storm, or medical symptoms signify a disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these.
The philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate.
Nature.
Semiotics, epistemology, logic, and philosophy of language are concerned about the nature of signs, what they are and how they signify. The nature of signs and symbols and significations, their definition, elements, and types, is mainly established by Aristotle, Augustine, and Aquinas. According to these classic sources, significance is a relationship between two sorts of things: signs and the kinds of things they signify (intend, express or mean), where one term necessarily causes something else to come to the mind. Distinguishing natural signs and conventional signs, the traditional theory of signs (Augustine) sets the following threefold partition of things:
all sorts of indications, evidences, symptoms, and physical signals, there are signs which are "always" signs (the entities of the mind as ideas and images, thoughts and feelings, constructs and intentions); and there are signs that "have" to get their signification (as linguistic entities and cultural symbols). So, while natural signs serve as the source of signification, the human mind is the agency through which signs signify naturally occurring things, such as objects, states, qualities, quantities, events, processes, or relationships. Human language and discourse, communication, philosophy, science, logic, mathematics, poetry, theology, and religion are only some of fields of human study and activity where grasping the nature of signs and symbols and patterns of signification may have a decisive value.
Types.
A sign can denote any of the following:
Christianity.
St. Augustine and Signs.
St. Augustine was the first man who synthesized the classical and Hellenistic theories of signs. For him a sign is a thing which is used to signify other things and to make them come to mind ("De Doctrina Christiana" [hereafter DDC] 1.2.2; 2.1.1). The most common signs are spoken and written words (DDC 1.2.2; 2.3.4-2.4.5). Although God cannot be fully expressible, Augustine gave emphasis to the possibility of God’s communication with humans by signs in Scripture (DDC 1.6.6). Augustine endorsed and developed the classical and Hellenistic theories of signs. Among the main stream in the theories of signs, i.e., that of Aristotle and that of Stoics, the former theory filtered into the works of Cicero (106-43 BC, "De inventione rhetorica" 1.30.47-48) and Quintilian (circa 35-100, "Institutio Oratoria" 5.9.9-10), which regarded the sign as an instrument of inference. In his commentary on Aristotle’s "De Interpretatione", Ammonius said, “according to the division of the philosopher Theophrastus, the relation of speech is twofold, first in regard to the audience, to which speech signifies something, and secondly in regard to the things about which the speaker intends to persuade the audience.” If we match DDC with this division, the first part belongs to DDC Book IV and the second part to DDC Books I-III. Augustine, although influenced by these theories, advanced his own theological theory of signs, with whose help one can infer the mind of God from the events and words of Scripture. 
Books II and III of DDC enumerate all kinds of signs and explain how to interpret them. Signs are divided into natural ("naturalia") and conventional ("data"); the latter is divided into animal ("bestiae") and human ("homines"); the latter is divided into non-words ("cetera") and words ("verba"); the latter is divided into spoken words ("voces") and written words ("litterae"); the latter is divided into unknown signs ("signa ignota") and ambiguous signs ("signa ambigua"); both the former and the latter are divided respectively into particular signs ("signa propria") and figurative signs ("signa translata"), among which the unknown figurative signs belong to the pagans. 
In addition to exegetical knowledge (Quintilian, "Institutio Oratoria" 1.4.1-3 and 1.8.1-21) which follows the order of reading ("lectio"), textual criticism ("emendatio"), explanation ("enarratio"), and judgment ("iudicium"), one needs to know the original language (Hebrew and Greek) and broad background information on Scripture (DDC 2.9.14-2.40.60).
Augustine’s understanding of signs includes several hermeneutical presuppositions as important factors. First, the interpreter should proceed with humility, because only a humble person can grasp the truth of Scripture (DDC 2.41.62). Second, the interpreter must have a spirit of active inquiry and should not hesitate to learn and use pagan education for the purpose of leading to Christian learning, because all truth is God’s truth (DDC 2.40.60-2.42.63). Third, the heart of interpreter should be founded, rooted, and built up in love which is the final goal of the entire Scriptures (DDC 2.42.63).
The sign does not function as its own goal, but its purpose lies in its role as a signification ("res significans", DDC 3.9.13). God gave signs as a means to reveal himself; Christians need to exercise hermeneutical principles in order to understand that divine revelation. Even if the Scriptural text is obscure, it has meaningful benefits. For the obscure text prevents us from falling into pride, triggers our intelligence (DDC 2.6.7), tempers our faith in the history of revelation (DDC 3.8.12), and refines our mind to be suitable to the holy mysteries (DDC 4.8.22). When interpreting signs, the literal meaning should first be sought, and then the figurative meaning (DDC 3.10.14-3.23.33). Augustine suggests the hermeneutical principle that the obscure Scriptural verse is interpreted with the help of plain and simple verses, which formed the doctrine of “scriptura scripturae interpres” (Scripture is the Interpreter of Scripture) in the Reformation Era. Moreover, he introduces the seven rules of Tyconius the Donatist to interpret the obscure meaning of the Bible, which demonstrates his understanding that all truth belongs to God (DDC 3.3.42-3.37.56). In order to apply Augustine’s hermeneutics of the sign appropriately in modern times, every division of theology must be involved and interdisciplinary approaches must be taken.

</doc>
<doc id="28131" url="http://en.wikipedia.org/wiki?curid=28131" title="Standard Alphabet by Lepsius">
Standard Alphabet by Lepsius

The Standard Alphabet by Lepsius is a Latin alphabet developed by Karl Richard Lepsius, who initially used it to transcribe Egyptian hieroglyphs and extended it to write African languages or transcribe other languages, published in 1854 and 1855, and in a revised edition (with many languages added) in 1863, it was comprehensive but it was not used much as it contains a lot of diacritic marks and therefore was difficult to read, write and typeset at that time.
Vowels.
Vowel length is indicated by a macron ("ā") or a breve ("ă") for long and short vowels, respectively. Open vowels are marked by a line under the letter ("e̱"), while a dot below the letter makes it a close vowel ("ẹ"). Central vowels are indicated by a corner (˻) below (not supported by Unicode). Rounded front vowels, especially [ø] and [y] are written with an umlaut ("ö" and "ü"), either on top or below, when the space above the letter is needed for vowel length marks (as in "ṳ̄" or "ṳ̆"). As in the , nasal vowels get a tilde ("ã"). A small circle below a letter is used to mark both the schwa ("e̥") and syllabic consonants ("r̥" or "l̥", for instance). Diphthongs do not receive any special marking, they are simply juxtaposed ("au").
Consonants.
To mark aspiration and affricates, the corresponding letters are simply written next to each other, thus "kh" in Lepsius' Standard Alphabet would be [kʰ] in IPA and "tš" would be [t͡ʃ]. For palatalization, the character ʹ is used, so "pʹ" is [pʲ] in IPA. Ejective consonants are sometimes written as double letters, although this could be mixed up with long consonants.
Tones.
Tone is marked with and acute or grave accent to the right of the corresponding vowel. Tone is not written directly, but rather needs to be established separately for each language. For example, the acute accent may indicate a high tone, a rising tone, or, in the case of Chinese, any tone called "rising" (上) for historical reasons. 
Low rising and falling tones can be distinguished from high rising and falling tones by underlining the accent mark: ⟨ma´̠, ma`̠⟩. The underline also transcribes the Chinese "yin" tones, under the mistaken impression that these tones are actually lower. Two additional tone marks, without any defined phonetic value, are used for Chinese: "level" maˏ (平) and checked maˎ (入); these may also be underlined.

</doc>
<doc id="28132" url="http://en.wikipedia.org/wiki?curid=28132" title="Sidehill gouger">
Sidehill gouger

Sidehill gougers are North American folkloric creatures adapted to living on hillsides by having legs on one side of their body shorter than the legs on the opposite side. This peculiarity allows them to walk on steep hillsides, although only in one direction; when lured or chased into the plain, they are trapped in an endless circular path. The creature is variously known as the Sidehill Ousel, Gyascutus, Sidewinder, Wampus, Gudaphro, Hunkus, Rickaboo Racker, Prock, Gwinter, or Cutter Cuss.
Sidehill gougers are herbivorous mammals who dwell in hillside burrows, and are occasionally depicted as laying eggs. There are usually 6 to 8 pups to a litter. Since the gouger is footed for hillsides, it cannot stand up on level ground. If by accident a gouger falls from a hill, it can easily be captured or starve to death. 
When a clockwise gouger meets a counter-clockwise gouger, they have to fight to the death since they can only go in one direction.
Gougers are said to have migrated to the west from New England, a feat accomplished by a pair of gougers who clung to each other in a fashion comparable to "a pair of drunks going home from town" with their longer legs on the outer sides. 
A Vermont variation is known as the Wampahoofus. It was reported that farmers crossbreed them with their cows so they could graze easily on mountain sides. There is also a similar mythical creature in France known as the dahu.
Frank C. Whitmore and Nicholas Hotton, in their joint tongue-in-cheek response to an article "Fantastic Animals" ("Smithsonian Magazine", 1972), expounded the taxonomy of sidehill gougers ("Membriinequales declivitous"), noting in particular "the sidehill dodger, which inhabits the Driftless Area of Wisconsin; the dextrosinistral limb ratio approaches unity although the metapodials on the downhill side are noticeably stouter."

</doc>
<doc id="28133" url="http://en.wikipedia.org/wiki?curid=28133" title="Strike">
Strike

Strike may refer to:

</doc>
<doc id="28134" url="http://en.wikipedia.org/wiki?curid=28134" title="Second Vatican Council">
Second Vatican Council

The Second Vatican Council (Latin: "Concilium Oecumenicum Vaticanum Secundum" or informally known as Vatican II) addressed relations between the Roman Catholic Church and the modern world. It was the twenty-first ecumenical council of the Catholic Church and the second to be held at Saint Peter's Basilica in the Vatican. The council, through the Holy See, formally opened under the pontificate of Pope John XXIII on 11 October 1962 and closed under Pope Paul VI on the Feast of the Immaculate Conception in 1965.
Several institutional changes resulted from the council, such as the renewal of consecrated life with a revised charism, and ecumenical efforts towards dialogue with other religions, the notion of the Catholic Church alone brings through ultimate salvation to mankind, and the call to holiness for everyone including the laity, which according to Pope Paul VI, is "the most characteristic and ultimate purpose of the teachings of the Council" .
The most palpable changes which followed the council include the widespread use of vernacular language in Holy Mass instead of the Latin language, the subtle disuse of ornate clerical regalia, the revision of Eucharistic prayers, the abbreviation of the liturgical calendar, along with various indirect changes such as the celebration of the Mass with the officiant facing the congregation ("versus populum"), instead of facing east toward the Lord ("ad orientem"); the displacement of the Church tabernacle from the central aisle, and for modern aesthetic changes encompassing contemporary Catholic liturgical music and artwork, many of which remain divisive and polemic among the Catholic faithful as of 2015[ [update]].
Of those who took part in the council's opening session, four have become pontiffs to date: Cardinal Giovanni Battista Montini, who on succeeding Pope John XXIII took the name of Paul VI; Bishop Albino Luciani, the future Pope John Paul I; Bishop Karol Wojtyła, who became Pope John Paul II; and Father Joseph Ratzinger, present as a theological consultant, who became Pope Benedict XVI.
Background.
During the 1950s, theological and Biblical studies of the Catholic Church had begun to sway away from the neo-scholasticism and biblical literalism that the reaction to Catholic modernism had enforced since the First Vatican Council. This shift could be seen in theologians such as Karl Rahner, SJ, Michael Herbert, and John Courtney Murray, SJ who looked to integrate modern human experience with church principles based on Jesus Christ, as well as others such as Yves Congar, Joseph Ratzinger and Henri de Lubac who looked to an accurate understanding of scripture and the early Church Fathers as a source of renewal ("ressourcement").
At the same time, the world's bishops faced tremendous challenges driven by political, social, economic, and technological change. Some of these bishops sought new ways of addressing those challenges. The First Vatican Council had been held nearly a century before but had been cut short when the Italian Army entered the city of Rome at the end of Italian unification. As a result, only deliberations on the role of the Papacy and the congruent relationship of faith and reason were completed, with examination of pastoral issues concerning the direction of the Church left unaddressed.
Pope John XXIII, however, gave notice of his intention to convene the Council on 25 January 1959, less than three months after his election in October 1958. This sudden announcement, which caught the Curia by surprise, caused little initial official comment from Church insiders. Reaction to the announcement was widespread and largely positive from both religious and secular leaders outside the Catholic Church, and the council was formally summoned by the apostolic constitution "Humanae Salutis" on 25 December 1961. In various discussions before the Council actually convened, Pope John XXIII often said that it was time to open the windows of the Church to let in some fresh air. He invited other Christians outside the Catholic Church to send observers to the Council. Acceptances came from both the Eastern Orthodox and Protestant denominations as internal observers but did not cast votes in the approbation of the conciliar documents.
Chronology.
Preparation.
Pope John XXIII's announcement on 25 January 1959 of his intention to call a general council came as a surprise even to the cardinals present. The Pontiff pre-announced the council under a full moon when the faithful with their candlelights gathered in St. Peter's square and jokingly noted about the brightness of the moon. After which, he instructed the people to go back home and "give their children a kiss of goodnight, from the Pope" himself.
He had tested the idea only ten days before with one of them, his Cardinal Secretary of State Domenico Tardini, who gave enthusiastic support to the idea. Although the Pope later said the idea came to him in a flash in his conversation with Tardini, two cardinals had earlier attempted to interest him in the idea. They were two of the most conservative, Ernesto Ruffini and Alfredo Ottaviani, who had already in 1948 proposed the idea to Pope Pius XII and who put it before John XXIII on 27 October 1958.
Actual preparations for the Council took more than two years, and included work from 10 specialised commissions, people for mass media and Christian Unity, and a Central Commission for overall coordination. These groups, composed mostly of members of the Roman Curia, produced 987 proposed constituting sessions, making it the largest gathering in any council in church history. (This compares to Vatican I, where 737 attended, mostly from Europe.) Attendance varied in later sessions from 2,100 to over 2,300. In addition, a varying number of "periti" (Latin: "experts") were available for theological consultation—a group that turned out to have a major influence as the council went forward. Seventeen Orthodox Churches and Protestant denominations sent observers. More than three dozen representatives of other Christian communities were present at the opening session, and the number grew to nearly 100 by the end of the 4th Council Sessions.
First period: 1962.
Opening.
Pope John XXIII opened the Council on 11 October 1962 in a public session and read the declaration Gaudet Mater Ecclesia before the Council Fathers.
13 October 1962 marked the initial working session of the Council. That day's agenda included the election for members of the ten conciliar commissions. Each would have sixteen elected and eight appointed members, and were expected to do most of the work of the Council. It had been expected that the members of the preparatory commissions, where the Curia was heavily represented, would be confirmed as the majorities on the conciliar commissions. Senior French Cardinal Achille Liénart addressed the Council, saying that the bishops could not intelligently vote for strangers. He asked that the vote be postponed to give all the bishops a chance to draw up their own lists. German Cardinal Josef Frings seconded that proposal, and the vote was postponed. The first meeting of the Council adjourned after only fifteen minutes.
"What is needed at the present time is a new enthusiasm, a new joy and serenity of mind in the unreserved acceptance by all of the entire Christian faith, without forfeiting that accuracy and precision in its presentation which characterized the proceedings of the Council of Trent and the First Vatican Council. What is needed, and what everyone imbued with a truly Christian, Catholic and apostolic spirit craves today, is that this doctrine shall be more widely known, more deeply understood, and more penetrating in its effects on men's moral lives. What is needed is that this certain and immutable doctrine, to which the faithful owe obedience, be studied afresh and reformulated in contemporary terms. For this deposit of faith, or truths which are contained in our time-honored teaching is one thing; the manner in which these truths are set forth (with their meaning preserved intact) is something else." .
Commissions.
The bishops met to discuss the membership of the commissions, along with other issues, both in national and regional groups, as well as in gatherings that were more informal. The "schemata" (Latin for drafts) from the preparatory sessions were thrown out, and new ones were created. When the council met on 16 October 1962, a new slate of commission members was presented and approved by the Council. One important change was a significant increase in membership from Central and Northern Europe, instead of countries such as Spain or Italy. More than 100 bishops from Africa, Asia, and Latin America were Dutch or Belgian and tended to associate with the bishops from those countries. These groups were led by Cardinals Bernardus Johannes Alfrink of the Netherlands and Leo Suenens of Belgium.
Issues.
After adjournment on 8 December, work began on preparations for the sessions scheduled for 1963. These preparations, however, were halted upon the death of Pope John XXIII on 3 June 1963, since an ecumenical council is automatically interrupted and suspended upon the death of the Pope who convened it, until the next Pope orders the council to be continued or dissolved. Pope Paul VI was elected on 21 June 1963 and immediately announced that the Council would continue.
Second period: 1963.
In the months prior to the second period, Pope Paul VI worked to correct some of the problems of organization and procedure that had been discovered during the first period. This included inviting additional lay Catholic and non-Catholic observers, reducing the number of proposed schemata to seventeen (which were made more general, in keeping with the pastoral nature of the council) and later eliminating the requirement of secrecy surrounding general sessions.
Pope Paul's opening address on 29 September 1963 stressed the pastoral nature of the council, and set out four purposes for it:
During this period, the bishops approved the constitution on the liturgy, "Sacrosanctum Concilium", and the decree on social communication, "Inter mirifica". Work went forward with the schemata on the Church, bishops and dioceses, and ecumenism. On 8 November 1963, Josef Frings criticized the Holy Office, and drew an articulate and impassioned defense by its Secretary, Alfredo Ottaviani. This exchange, known as the Ottaviani Intervention, is often considered the most dramatic of the council (Cardinal Frings' theological adviser was the young Joseph Ratzinger, who would later as a Cardinal head the same department of the Holy See, and from 2005–13 the Pope Benedict XVI). The second period ended on 4 December.
Third period: 1964.
In the time between the second and third periods, the proposed schemata were further revised on the basis of comments from the council fathers. A number of topics were reduced to statements of fundamental propositions that could gain approval during the third period, with postconciliar commissions handling implementation of these measures.
At the end of the second period, Cardinal Leo Joseph Suenens of Belgium had asked the other bishops: "Why are we even discussing the reality of the church when half of the church is not even represented here?," referring to women. In response, 15 women were appointed as auditors in September 1964. Eventually 23 women were auditors at the Second Vatican Council, including 10 women religious. There were three Americans among the auditors: Loretto Sr. Mary Luke Tobin, Basilian Sr. Claudia (Anna) Feddish, of the Byzantine rite, and Catherine McCarthy, president of the National Council of Catholic Women. The auditors had no official role in the deliberations, although they attended the meetings of subcommittees working on council documents, particularly texts that dealt with the laity. They also met together on a weekly basis to read draft documents and comment on them.
During the third period, which began on 14 September 1964, the Council Fathers worked through a large volume of proposals. Schemata on ecumenism ("Unitatis redintegratio"); the official view on Protestant and Eastern Orthodox "separated brethren", the Eastern Rite churches ("Orientalium Ecclesiarum"); and the Dogmatic Constitution of the Church ("Lumen gentium") 'were approved and promulgated by the Pope′.
Schemata on the life and ministry of priests and the missionary activity of the Church were rejected and sent back to commissions for complete rewriting. Work continued on the remaining schemata, in particular those on the Church in the modern world and religious freedom. There was controversy over revisions of the decree on religious freedom and the failure to vote on it during the third period, but Pope Paul promised that this schema would be the first to be reviewed in the next period.
Pope Paul closed the third period on 21 November by announcing a change in the Eucharistic fast and formally reaffirming Mary as "Mother of the Church".
Fourth period: 1965.
Eleven schemata remained unfinished at the end of the third period, and commissions worked to give them their final form. Schema 13, on the Church in the modern world, was revised by a commission that worked with the assistance of laymen.
Pope Paul VI opened the last period of the Council on 14 September 1965 with the establishment of the Synod of Bishops. This more permanent structure was intended to preserve close cooperation of the bishops with the Pope after the council.
The first business of the fourth period was the consideration of the decree on religious freedom, "Dignitatis humanae", one of the more controversial of the conciliar documents. The vote was 1,997 for to 224 against, a margin that widened even farther by the time the bishops finally signed the decree. The principal work of the rest of the period was work on three documents, all of which were approved by the council fathers. The lengthened and revised pastoral constitution on the Church in the modern world, "Gaudium et spes", was followed by decrees on missionary activity, "Ad gentes" and the ministry and life of priests, "Presbyterorum ordinis".
The council also gave final approval to other documents that had been considered in earlier sessions. They included the Dogmatic Constitution on Divine Revelation ("Dei verbum"), decrees on the pastoral office of bishops ("Christus Dominus"), the life of persons in religious orders (expanded and modified from earlier sessions, finally titled "Perfectae caritatis"), education for the priesthood ("Optatam totius"), Christian education ("Gravissimum educationis"), and the role of the laity ("Apostolicam actuositatem").
One of the more controversial documents was "Nostra aetate", which stated that the Jews of the time of Christ, taken indiscriminately, and all Jews today are no more responsible for the death of Christ than Christians.
"True, the Jewish authorities and those who followed their lead pressed for the death of Christ; still, what happened in His passion cannot be charged against all the Jews, without distinction, then alive, nor against the Jews of today. Although the Church is the new people of God, the Jews should not be presented as rejected or accursed by God, as if this followed from the Holy Scriptures. All should see to it, then, that in catechetical work or in the preaching of the word of God they do not teach anything that does not conform to the truth of the Gospel and the spirit of Christ. Furthermore, in her rejection of every persecution against any man, the Church, mindful of the patrimony she shares with the Jews and moved not by political reasons but by the Gospel's spiritual love, decries hatred, persecutions, displays of anti-Semitism, directed against Jews at any time and by anyone."
A major event of the final days of the council was the act of Pope Paul and Orthodox Patriarch Athenagoras of a joint expression of regret for many of the past actions that had led up to the Great Schism between the western and eastern churches.
"The old story of the Samaritan has been the model of the spirituality of the council" (Paul VI., address, 7 December): On 8 December, the Council was formally closed, with the bishops professing their obedience to the Council's decrees. To help carry forward the work of the Council, Pope Paul:
Issues.
Ecclesiology.
Perhaps the most famous and most influential product of the council is the Dogmatic Constitution on the Church, "Lumen gentium".
In its first chapter, titled "The Mystery of the Church," is the famous statement that: … the sole Church of Christ which in the Creed we profess to be one, holy, catholic and apostolic, which our Saviour, after His Resurrection, commissioned Peter to shepherd, and him and the other apostles to extend and direct with authority, which He erected for all ages as 'the pillar and mainstay of the truth.' This Church, constituted and organized as a society in the present world, subsists in the Catholic Church, which is governed by the successor of Peter and by the bishops in communion with him ("Lumen gentium", 8).
The document immediately adds, "Nevertheless, many elements of sanctification and of truth are found outside its visible confines".
According to Pope Paul VI, "the most characteristic and ultimate purpose of the teachings of the Council" is the universal call to holiness:
Liturgy.
One of the first issues considered by the council, and the matter that had the most immediate effect on the lives of individual Catholics, was the revision of the liturgy. The central idea was that there ought to be greater lay participation in the liturgy. In the mid-1960s, permissions were granted to celebrate most of the Mass in vernacular languages, including the canon from 1967 onwards. The amount of Scripture read during Mass was greatly expanded, through the introduction of multiple year lectionaries.
Neither the Second Vatican Council nor the subsequent revision of the Roman Missal abolished Latin as the liturgical language of the Roman Rite: the official text of the Roman Missal, on which translations into vernacular languages are to be based, continues to be in Latin and it can still be used in the celebration.
Scripture and divine revelation.
The council sought to revive the central role of Scripture in the theological and devotional life of the Church, building upon the work of earlier popes in crafting a modern approach to Scriptural analysis and interpretation. A new approach to interpretation was approved by the bishops. The Church was to continue to provide versions of the Bible in the "mother tongues" of the faithful, and both clergy and laity were to continue to make Bible study a central part of their lives. This affirmed the importance of Sacred Scripture as attested by Providentissimus Deus by Pope Leo XIII and the writings of the Saints, Doctors, and Popes throughout Church history but also approved historically conditioned interpretation of Scripture as presented in Pius XII's 1943 encyclical "Divino afflante Spiritu".
Bishops.
The role of the bishops was brought into renewed prominence, especially when seen collectively, as a college that has succeeded to that of the apostles in teaching and governing the Church. This college was headed by the Pope.
Objections to the council.
The questioning of the validity of the Second Vatican Council continues to be a contending point of rejection and conflict among various religious communities that are not in full communion with the Roman Catholic Church. In particular, two schools of thought may be discerned:
The most recent edition of 1983 Code of Canon Law states that Catholics may not disregard the teaching of an ecumenical council even if it does not propose such as definitive. Accordingly, it also maintains that present living Pope alone judges the criterion of membership for being "in communio" with the Church. The present canon law further articulates: "Although not an assent of faith, a religious submission of the intellect and will must be given to a Doctrine which the Supreme Pontiff or the College of Bishops declares concerning faith or morals when they exercise the authentic Magisterium, even if they do not intend to proclaim it by definitive act; therefore, the Christian faithful are to take care to avoid those things which do not agree with it."
Several notable members of clergy opposed particular documents of the Second Vatican Council:
Opposing Traditionalist Catholic groups:
Legacy.
In addition to general spiritual guidance, the Second Vatican Council produced very specific recommendations, such as in the document "Gaudiem et Spes": "Any act of war aimed indiscriminately at the destruction of entire cities of extensive areas along with their population is a crime against God and man himself. It merits unequivocal and unhesitating condemnation."
By "the spirit of Vatican II" is often meant promoting teachings and intentions attributed to the Second Vatican Council in ways not limited to literal readings of its documents, spoken of as the "letter" of the Council (cf. Saint Paul's phrase, "the letter kills, but the Spirit gives life").
The spirit of Vatican II is invoked for a great variety of ideas and attitudes. Bishop John Tong Hon of Hong Kong used it with regard merely to an openness to dialogue with others, saying: "We are guided by the spirit of Vatican II: only dialogue and negotiation can solve conflicts."
In contrast, Michael Novak described it as a spirit that: "...sometimes soared far beyond the actual, hard-won documents and decisions of Vatican II. … It was as though the world (or at least the history of the Church) were now to be divided into only two periods, pre-Vatican II and post-Vatican II. Everything 'pre' was then pretty much dismissed, so far as its "authority" mattered. For the most extreme, to be a Catholic now meant to believe more or less anything one wished to believe, or at least in the sense in which one personally interpreted it. One could be a Catholic 'in spirit'. One could take "Catholic" to mean the 'culture' in which one was born, rather than to mean a creed making objective and rigorous demands. One could imagine Rome as a distant and irrelevant anachronism, embarrassment, even adversary. Rome as 'them'."
 Such views of the Second Vatican Council were condemned by the Church's hierarchy, and the works of theologians who were active in the Council or who closely adhered to the Council's aspect of reform (such as Hans Küng) have often been criticized by the Church for espousing a belief system that the hierarchy considers radical and misguided. As Dei Verbum reads, "Therefore, following in the footsteps of the Council of Trent and of the First Vatican Council, this present council wishes to set forth authentic doctrine on divine revelation and how it is handed on…”, Vatican II did not deny previous councils' correctness.
To mark the fiftieth anniversary of the beginning of Vatican II, in October 2011, Pope Benedict XVI declared the period from October 2012 to the Solemnity of Christ the King at the end of November 2013 a "Year of Faith", as:"...a good opportunity to help people understand that the texts bequeathed by the Council Fathers, in the words of John Paul II, 'have lost nothing of their value or brilliance. They need to be read correctly, to be widely known and taken to heart as important and normative texts of the Magisterium, within the Church's Tradition... I feel more than ever in duty bound to point to the Council as the great grace bestowed on the Church in the twentieth century: there we find a sure compass by which to take our bearings in the century now beginning."
Further reading.
</dl>

</doc>
<doc id="28135" url="http://en.wikipedia.org/wiki?curid=28135" title="Slovene language">
Slovene language

Slovene or Slovenian ("slovenski jezik" or "slovenščina", not to be confused with "slovenčina", the native name of Slovak) belongs to the group of South Slavic languages. It is spoken by approximately 2.5 million speakers worldwide, the majority of whom live in Slovenia. It is the first language of about 2.1 million Slovenian people and is one of the 24 official and working languages of the European Union.
Standard Slovene.
Standard Slovene is the national standard language that was formed in the 18th century, mostly based on Upper and Lower Carniolan dialect groups, the latter being a dialect spoken by Primož Trubar. Unstandardized dialects are more preserved in regions of the Slovene Lands where compulsory schooling was in languages other than Standard Slovene, as was the case with the Carinthian Slovenes in Austria, and the Slovene minority in Italy. For example, the Resian and Torre (Ter) dialects in the Italian Province of Udine differ most from other Slovene dialects.
The distinctive characteristics of Slovene are dual grammatical number, two accentual norms (one characterized by pitch accent), and abundant inflection (a trait shared with many Slavic languages). Although Slovene is basically an SVO language, word order is very flexible, often adjusted for emphasis or stylistic reasons. Slovene has a T-V distinction: second-person plural forms are used for individuals as a sign of respect. Also, Slovene and Slovak are the two modern Slavic languages whose names for themselves literally mean "Slavic" ("slověnьskъ" in old Slavonic).
Classification.
Slovene is an Indo-European language belonging to the Western subgroup of the South Slavic branch of the Slavic languages, together with Serbo-Croatian. It is close to the Chakavian and especially Kajkavian dialects of Serbo-Croatian, but further from the Shtokavian dialect, the basis for the Bosnian, Croatian, Montenegrin, and Serbian standard languages. Furthermore, Slovene shares certain linguistic characteristics with all South Slavic languages, including those of the Eastern subgroup, such as Bulgarian. Although Slovene is almost completely intelligible with the Kajkavian dialects of Serbo-Croatian (especially the variant spoken in Hrvatsko Zagorje on the border with Slovenia), mutual intelligibility with other varieties of Serbo-Croatian is hindered by differences in vocabulary, grammar, and pronunciation. The Slovene language also has many commonalities with the West Slavic languages.
History.
Early history.
Like all Slavic languages, Slovene traces its roots to the same proto-Slavic group of languages that produced Old Church Slavonic. The earliest known examples of a distinct, written Slovene dialect are from the "Freising Manuscripts," known in Slovene as "Brižinski spomeniki". The consensus estimate of their date of origin is between 972 and 1093 (most likely before 1000). These religious writings are among the oldest surviving manuscripts in any Slavic language.
The "Freising Manuscripts" are a record of a proto-Slovene language that was spoken in a much larger territory than modern Slovene, which included most of the present-day Austrian states of Carinthia and Styria, as well as East Tyrol, the Val Pusteria in South Tyrol, and some areas of Upper and Lower Austria. By the 15th century, most of the northern areas were gradually Germanized: the northern border of the Slovene-speaking territory stabilized on the line going from north of Klagenfurt to south of Villach and east of Hermagor in Carinthia, while in Styria it was pretty much identical with the current Austrian-Slovenian border. This linguistic border remained almost unchanged until the late 19th century, when a second process of Germanization took place, mostly in Carinthia. Between the 9th and 12th century, proto-Slovene spread into northern Istria and in the areas around Trieste.
During most of the Middle Ages, Slovene was a vernacular language of the peasantry, although it was also spoken in most of the towns on Slovene territory, together with German or Italian. Although during this time, German emerged as the spoken language of the nobility, Slovene had some role in the courtly life of the Carinthian, Carniolan and Styrian nobility, as well. This is proved by the survival of certain ritual formulas in Slovene (such as the ritual installation of the Dukes of Carinthia). The words "Buge waz primi, gralva Venus!" ("God be With You, Queen Venus!"), with which Bernhard von Spanheim greeted the poet Ulrich von Liechtenstein upon his arrival to Carinthia in 1227 (or 1238), is another example of some level of Slovene knowledge among high nobility in the region.
The first printed Slovene words, "stara pravda" (meaning 'old justice'), appeared in 1515 in Vienna in a poem of the German mercenaries who suppressed the Slovene peasant revolt. Standard Slovene emerged in the second half of the 16th century, thanks to the works of Slovene Lutheran authors, who were active during the Protestant Reformation. The most prominent authors from this period are Primož Trubar, who wrote the first books in Slovene; Adam Bohorič, the author of the first Slovene grammar; and Jurij Dalmatin, who translated the entire Bible into Slovene.
From the high Middle Ages up to the dissolution of the Austro-Hungarian Empire in 1918, in the territory of present-day Slovenia, German was the language of the elite, and Slovene was the language of the common people. During this period, German had a strong influence on Slovene, and many Germanisms are preserved in contemporary colloquial Slovene. Many Slovene scientists before the 1920s also wrote in foreign languages, mostly German, which was the "lingua franca" of science throughout Central Europe at the time.
Recent history.
During the rise of Romantic Nationalism in the 19th century, the cultural movements of Illyrism and Pan-Slavism brought words from Serbo-Croatian and Czech into standard Slovene, mostly to replace words previously borrowed from German. Most of these innovations have remained, although some were dropped in later development. In the second half of the 19th century, many nationalist authors made an abundant use of Serbo-Croatian words: among them were Fran Levstik and Josip Jurčič, who wrote the first novel in Slovene in 1866. This tendency was reversed in the Fin de siècle period by the first generation of modernist Slovene authors (most notably the writer Ivan Cankar), who resorted to a more "pure" and simple language without excessive Serbo-Croatian borrowings.
During the Kingdom of Yugoslavia in the 1920s and 1930s, the influence of Serbo-Croatian increased again. This was opposed by the younger generations of Slovene authors and intellectuals; among the most fierce opponents of an excessive Serbo-Croatian influence on Slovene were the intellectuals associated with the leftist journal "Sodobnost", as well as some younger Catholic activists and authors. After 1945, numerous Serbo-Croatian words that had been used in the previous decades were dropped. The result was that a Slovene text from the 1910s is frequently closer to modern Slovene than a text from the 1920s and 1930s.
Between 1920 and 1941, the official language of the Kingdom of Yugoslavia was defined as "Serbian-Croatian-Slovene". In practice, Slovene was used in Slovenia, both in education and administration. Many state institutions used only Serbo-Croatian, and a Slovene–Serbo-Croatian bilingualism was applied in many spheres of public life in Slovenia. For examples, at the post offices, railways and in administrative offices, Serbo-Croatian was used together with Slovene. However, state employees were expected to be able to speak Slovene in Slovenia.
During the same time, western Slovenia (the Slovenian Littoral and the western districts of Inner Carniola) was under Italian administration and submitted to a violent policy of Fascist Italianization; the same policy was applied to Slovene speakers in Venetian Slovenia, Gorizia and Trieste. Between 1923 and 1943, all public use of Slovene language in these territories was strictly prohibited, and Slovene language activists were persecuted by the state.
After the Carinthian Plebiscite of 1920, a less severe policy of Germanization took place in the Slovene-speaking areas of southern Carinthia which remained under Austrian administration. After the Anschluss of 1938, the use of Slovene was strictly forbidden in Carinthia, as well. This accelerated a process of language shift in Carinthia, which continued throughout the second half of the 20th century: according to the Austro-Hungarian census of 1910, around 17% of inhabitants of Carinthia spoke Slovene in their daily communication; in 1951, this figure dropped under 10%, and by 2001 to a mere 2.8%.
During World War II, Slovenia was divided among the Axis Powers of Fascist Italy, Nazi Germany, and Hungary. Each of the occupying powers tried to either discourage or entirely suppress the Slovene language.
Following World War II, Slovenia became part of the Socialist Federal Republic of Yugoslavia. Slovene was one of the official languages of the federation. In the territory of Slovenia, it was commonly used in almost all areas of public life. One important exception was the Yugoslav army, where Serbo-Croatian was used exclusively, even in Slovenia.
National independence has revitalized the language: since 1991, when Slovenia gained independence, Slovene has been used as an official language in all areas of public life. In 2004 it became one of the official languages of the European Union upon Slovenia's admission.
Joža Mahnič, a literary historian and the then president of Slovenska matica, a prestigious publishing house, said in February 2008 that Slovene is a language rich enough to express everything, including the most sophisticated and specialised texts. In February 2010, Janez Dular, a prominent Slovenian linguist, commented that, although Slovene is not an endangered language, its scope has been shrinking, especially in science and higher education.
Geographic distribution.
The language is spoken by about 2.5 million people, mainly in Slovenia, but also by Slovene national minorities in Friuli-Venezia Giulia, Italy (around 90,000 in Venetian Slovenia, Resia Valley, Canale Valley, Province of Trieste and in those municipalities of the Province of Gorizia bordering with Slovenia), in southern Carinthia and some parts of Styria in Austria (25,000). It is also spoken in Croatia, especially in Istria, Rijeka and Zagreb (11,800-13,100), in southwestern Hungary (3-5,000), in Serbia (5,000), and by the Slovene diaspora throughout Europe and the rest of the world (around 300,000), particularly in the United States (most notably Ohio home to estimated 3,400 speakers), Canada, Argentina, Australia and South Africa.
Dialects.
Slovene is sometimes characterized as the most diverse Slavic language in terms of dialects, with different degrees of mutual intelligibility. Accounts of the number of dialects range from as few as seven dialects, often considered dialect groups or dialect bases that are further subdivided into as many as 50 dialects. Other sources characterize the number of dialects as nine or eight. Although pronunciation differs greatly from area to area, those differences do not pose major obstacles to understanding. The standard language is mainly used in public presentations or on formal occasions.
The Prekmurje dialect used to have a literary standard of its own at one point. The Resian dialects have an independent written norm that's used by their regional state institutions. Speakers of those two dialects have considerable difficulties with being understood by speakers of other varieties of Slovene, needing code-switching to the Standard Slovene. Other dialects are mutually intelligible when speakers avoid the excessive usage of regionalisms.
Regionalisms are mostly limited to culinary and agricultural expressions, although there are many exceptions. Some loanwords have become so deeply rooted into the local language, that people have considerable difficulties in finding a standard expression for the dialectical term (for instance, "kovter" meaning blanket is "prešita odeja" in Standard Slovene, but the latter term is "never" used in speech). South-western dialects incorporate a great deal of calques and loanwords from Italian, whereas eastern and north-western dialects remain replete with remnants of the German reign. Usage of those words hinders intelligibility between dialects and is greatly discouraged in formalities.
Phonology.
Slovene has a phoneme set consisting of 21 consonants and 8 vowels.
Consonants.
Slovene has 21 distinctive consonant phonemes.
All voiced obstruents are devoiced at the end of words unless immediately followed by a word beginning with a vowel or a voiced consonant. In consonant clusters, voicing distinction is neutralized and all consonants assimilate the voicing of the rightmost segment. In this context, [v], [ɣ] and [d͡z] may occur as voiced allophones of /f/, /x/ and /t͡s/, respectively (e.g. "vŕh drevésa" [ʋrɣ dreˈʋesa]).
/ʋ/ has several allophones depending on context.
The sequences /lj/, /nj/ and /rj/ occur only before a vowel. Before a consonant or word-finally, they are reduced to /l/, /n/ and /r/ respectively. This is reflected in the spelling in the case of /rj/, but not for /lj/ and /nj/.
Under certain (somewhat unpredictable) circumstances, /l/ at the end of a syllable may become [w], merging with the allophone of /ʋ/ in that position.
Vowels.
Slovene has an eight-vowel (according to Peter Jurgec nine-vowel) system, in comparison to the five-vowel system of Serbo-Croatian.
Grammar.
Nouns.
Slovene nouns retain six of the seven Slavic noun cases: nominative, accusative, genitive, dative, locative and instrumental. There is no distinct vocative; the nominative is used in that role. Nouns, adjectives and pronouns have three numbers: singular, plural and a special dual form that indicates exactly two objects.
Nouns in Slovene are either masculine, feminine or neuter gender. In addition, there is a distinction between animate and inanimate nouns, although this is only relevant for masculine nouns and only in the singular. Animate nouns have an accusative singular form that is identical to the genitive, while for inanimate nouns the accusative singular is the same as the nominative. Animacy is based mostly on semantics and is less rigid than gender. Generally speaking a noun is animate if it refers to something that is generally thought to have free will and/or the ability to move of its own accord. This includes all nouns for people and animals. All other nouns are inanimate, including plants and other non-moving life forms, and also groups of people or animals. However, there are some nouns for inanimate objects that are generally animate, which mostly include inanimate objects that are named after people or animals. This includes:
Vocabulary.
T–V distinction.
Slovene, like most other European languages, has a T–V distinction, or two forms of 'you' for formal and informal situations. Although informal address using the 2nd person singular "ti" form (known as "tikanje") is officially limited to friends and family, talk among children, and addressing animals, it is increasingly used among the middle generation to signal a relaxed attitude or lifestyle instead of its polite or formal counterpart using the 2nd person plural "vi" form (known as "vikanje").
An additional nonstandard but widespread use of a singular participle combined with a plural auxiliary verb (known as "polvikanje") signals a somewhat more friendly and less formal attitude while maintaining politeness:
The use of nonstandard forms ("polvikanje") might be frowned upon by many people and would not likely be used in a formal setting.
The use of the 3rd person plural "oni" ('they') form (known as "onikanje" in both direct address and indirect reference) as an ultra-polite form is now archaic or dialectal; it is associated with servant-master relationships in older literature, the child-parent relationship in certain conservative rural communities, and parishioner-priest relationships.
Foreign words.
Foreign words used in Slovene are of various types depending on the assimilation they have undergone. The types are:
The loanwords are mostly from German and Italian, while the more recently borrowed and less assimilated words are typically from English.
Articles.
There are no definite or indefinite articles as in English ("a", "an", "the") or German ("der", "die", "das", "ein", "eine"). A whole verb or a noun is described without articles and the grammatical gender is found from the word's termination. It is enough to say "barka" ("a" or "the barge"), "Noetova barka" ('Noah's ark'). The gender is known in this case to be feminine. In declensions, endings are normally changed; see below. If one should like to somehow distinguish between definiteness or indefiniteness of a noun, one would say "(prav/natanko/ravno) tista barka" ('that (exact) barge') for "the barge" and "neka/ena barka" ('one barge') for "a barge".
Definiteness of a noun phrase can also be discernible through the ending of the accompanying adjective. One should say "rdeči šotor" ([exactly that] red tent) or "rdeč šotor" ([a] red tent). This difference is observable only for masculine nouns in nominative or accusative case. Because of the lack of article in Slovene and audibly insignificant difference between the masculine adjective forms, most dialects do not distinguish between definite and indefinite variants of the adjective, leading to hypercorrection when speakers try to use Standard Slovenian.
Writing system.
This alphabet (Slovene: "abeceda") was derived in the mid-1840s from the system created by Croatianist Ljudevit Gaj. Intended for the Serbo-Croatian language (in all its varieties), it was patterned on the Czech alphabet of the 1830s. Before that /s/ was, for example, written as ⟨ʃ⟩, ⟨ʃʃ⟩ or ⟨ſ⟩; /tʃ/ as ⟨tʃch⟩, ⟨cz⟩, ⟨tʃcz⟩ or ⟨tcz⟩; /i/ sometimes as ⟨y⟩ as a relic from the now modern Russian yery character ⟨ы⟩, usually transliterated as "y"; /j/ as ⟨y⟩; /l/ as ⟨ll⟩; /ʋ/ as ⟨w⟩; /ʒ/ as ⟨ʃ⟩, ⟨ʃʃ⟩ or ⟨ʃz⟩.
The standard Slovene orthography, used in almost all situations, uses only the letters of the ISO basic Latin alphabet plus ⟨č⟩, ⟨š⟩, and ⟨ž⟩:
The orthography thus underdifferentiates several phonemic distinctions:
In the tonemic varieties of Slovene, the ambiguity is even worse: "e" in a final syllable can stand for any of /éː/ /èː/ /ɛ́ː/ /ɛ̀ː/ /ɛ/ /ǝ/ (although /ɛ̀ː/ is rare).
The reader is expected to gather the interpretation of the word from the context, as in these examples:
Diacritics.
To compensate for the shortcomings of the standard orthography, Slovene also uses standardized diacritics or accent marks to denote stress, vowel length and pitch accent, much like the closely related Serbo-Croatian. However, as in Serbo-Croatian, use of such accent marks is restricted to dictionaries, language textbooks and linguistic publications. In normal writing, the diacritics are almost never used, except in a few minimal pairs where real ambiguity could arise.
Two different and mutually incompatible systems of diacritics are used. The first is the simpler non-tonemic system, which can be applied to all Slovene dialects. It is more widely used and is the standard representation in dictionaries such as SSKJ. The tonemic system also includes tone as part of the representation. However, neither system reliably distinguishes schwa /ǝ/ from the front mid-vowels, nor vocalised l /w/ from regular l /l/. Some sources write these as "ǝ" and "ł" respectively, but this is not as common.
Non-tonemic diacritics.
In the non-tonemic system, the distinction between the two mid-vowels is indicated, as well as the placement of stress and length of vowels:
Tonemic diacritics.
The tonemic system uses the diacritics somewhat differently from the non-tonemic system. The high-mid vowels /eː/ and /oː/ are written "ẹ ọ" with a subscript dot, while the low-mid vowels /ɛː/ and /ɔː/ are written as plain "e o".
Pitch accent and length is indicated by four diacritical marks:
The schwa vowel /ǝ/ is written ambiguously as "e", but its accentuation will sometimes distinguish it: a long vowel mark can never appear on a schwa, while a grave accent can appear only on a schwa. Thus, only "ȅ" is truly ambiguous.
Regulation.
Standard Slovene spelling and grammar are defined by the Orthographic Committee and the Fran Ramovš Institute of the Slovenian Language, which are both part of the Slovenian Academy of Sciences and Arts ("Slovenska akademija znanosti in umetnosti", SAZU). The newest reference book of standard Slovene spelling (and to some extent also grammar) is the "Slovenski pravopis" ("SP2001"; Slovene Normative Guide). The latest printed edition was published in 2001 (reprinted in 2003 with some corrections) and contains more than 130,000 dictionary entries. In 2003, an electronic version was published.
The official dictionary of modern Slovene, which was also prepared by SAZU, is "Slovar slovenskega knjižnega jezika" ("SSKJ"; Standard Slovene Dictionary). It was published in five volumes by Državna Založba Slovenije between 1970 in 1991 and contains more than 100,000 entries and subentries with accentuation, part-of-speech labels, common collocations, and various qualifiers. In the 1990s, an electronic version of the dictionary was published and it is available online.
The SAZU considers SP2001 to be the normative source on Slovenian language. When dictionary entries in SP2001 and SSKJ differ, the SP2001 entry takes precedence.

</doc>
<doc id="28136" url="http://en.wikipedia.org/wiki?curid=28136" title="Slovak language">
Slovak language

Slovak (Slovak: "slovenský jazyk", ]; "slovenčina" ]; not to be confused with "slovenski jezik" or "slovenščina", the native names of the Slovene language) is an Indo-European language that belongs to the West Slavic languages (together with Czech, Polish, Silesian, Kashubian, and Sorbian).
Slovak is the official language of Slovakia where it is spoken by approximately 5.51 million people (2014). Slovak speakers are also found in the United States, the Czech Republic, Argentina, Serbia, Ireland, Romania, Poland, Canada, Hungary, Croatia, the United Kingdom, Australia, Austria, and Ukraine.
Orthography.
Slovak uses the Latin script with small modifications that include the four diacritics (ˇ, ´, ¨, ˆ) placed above certain letters.
The primary principle of Slovak spelling is the phonemic principle. The secondary principle is the morphological principle: forms derived from the same stem are written in the same way even if they are pronounced differently. An example of this principle is the assimilation rule (see below). The tertiary principle is the etymological principle, which can be seen in the use of "i" after certain consonants and of "y" after other consonants, although both "i" and "y" are pronounced the same way.
Finally, the rarely applied grammatical principle is present when, for example, the basic singular form and plural form of masculine adjectives are written differently with no difference in pronunciation (e.g. pekný = nice – singular versus pekní = nice – plural).
In addition, the following rules are present:
Most foreign words receive Slovak spelling immediately or after some time. For example, "weekend" is spelled "víkend", "software" – "softvér", "gay" – "gej" (both not exclusively), and "quality" is spelled "kvalita" (possibly from Italian "qualità"). Personal and geographical names from other languages using Latin alphabets keep their original spelling unless a fully Slovak form of the name exists (e.g. "Londýn" for "London").
Slovak features some heterophonic homographs (words with identical spelling but different pronunciation and meaning), the most common examples being "krásne" /ˈkraːsne/ (beautiful) versus "krásne" /ˈkraːsɲe/ (beautifully).
Syntax.
The main features of Slovak syntax are as follows:
Some examples include the following:
Word order in Slovak is relatively free, since strong inflection enables the identification of grammatical roles (subject, object, predicate, etc.) regardless of word placement. This relatively free word order allows the use of word order to convey topic and emphasis.
Some examples are as follows:
The unmarked order is subject–verb–object. Variation in word order is generally possible, but word order is not completely free.
In the above example, the noun phrase "ten veľký muž" cannot be split up, so that the following combinations are not possible:
And the following are stylistically not correct:
Morphology.
Articles.
Slovak does not have articles. The demonstrative pronoun "ten" (fem: "tá", neuter: "to") may be used in front of the noun in situations where definiteness must be made explicit.
Nouns, adjectives, pronouns.
Slovak nouns are inflected for case and number. There are six cases: nominative, genitive, dative, accusative, locative, and instrumental. The vocative is no longer morphologically marked. There are two numbers: singular and plural. Nouns have inherent gender. There are three genders: masculine, feminine, and neuter. Adjectives agree with nouns in case, number, and gender.
Numerals.
The numerals 0–10 have unique forms. 11–19 are formed by the numeral plus "násť." Compound numerals (21, 1054) are combinations of these words formed in the same order as their mathematical symbol is written (e.g. 21 = dvadsaťjeden, literally "twenty one").
The numerals are as follows:
(1) jeden (jedno (neuter), jedna (feminine)),
(2) dva (dve (neuter, feminine)),
(3) tri,
(4) štyri,
(5) päť,
(6) šesť,
(7) sedem,
(8) osem,
(9) deväť,
(10) desať, (11) jedenásť, (12) dvanásť, (13) trinásť, (14) štrnásť, (15) pätnásť, (16) šestnásť, (17) sedemnásť, (18) osemnásť, (19) devätnásť, (20) dvadsať, (21) dvadsaťjeden... (30) tridsať, (31) tridsaťjeden... (40) štyridsať... (50) päťdesiat... (60) šesťdesiat... (70) sedemdesiat... (80) osemdesiat... (90) deväťdesiat... (100) sto, (101) stojeden... (200) dvesto... (300) tristo... (900)deväťsto... (1,000) tisíc... (1,100) tisícsto... (2,000) dvetisíc... (100,000) stotisíc... (200,000) dvestotisíc... (1,000,000) milión... (1,000,000,000) miliarda...
Counted nouns have two forms. The most common form is the plural genitive (e.g. "päť domov" = five houses or "stodva žien" = one hundred two women), while the plural form of the noun when counting the amounts of 2, 3, 4, etc., is the nominative form without counting (e.g. "dva domy" = two houses or "dve ženy" = two women).
Verbs.
Verbs have three major conjugations. Three persons and two numbers (singular and plural) are distinguished. Several conjugation paradigms exist as follows:
Adverbs.
Adverbs are formed by replacing the adjectival ending with the ending -o or -e/-y. Sometimes both -o and -e are possible. Examples include the following:
The comparative/superlative of adverbs is formed by replacing the adjectival ending with a comparative/superlative ending -(ej)ší or -(ej)šie. Examples include the following:
Prepositions.
Each preposition is associated with one or more grammatical cases. The noun governed by a preposition must appear in the case required by the preposition in the given context (e.g. from friends = od priateľov). Priateľov is the genitive case of priatelia. It must appear in this case because the preposition od (=from) always calls for its objects to be in the genitive.
Po has a different meaning depending on the case of its governed noun.
Relationships to other languages.
The Slovak language is a descendant of Proto-Slavic, itself a descendant of Proto-Indo-European. It is closely related to the other West Slavic languages, primarily to Czech. It has been also influenced by German, English, Latin and Hungarian.
Czech.
Although most dialects of Czech and Slovak are mutually intelligible (see Comparison of Slovak and Czech), eastern Slovak dialects are less intelligible to speakers of Czech; they differ from Czech and from other Slovak dialects, and mutual contact between speakers of Czech and speakers of the eastern dialects is limited.
Since the dissolution of Czechoslovakia it has been allowed to use Czech in TV broadcasting and—like any other language of the world—during court proceedings (Administration Procedure Act 99/1963 Zb.). From 1999 to August 2009, the Minority Language Act 184/1999 Z.z., in its section (§) 6, contained the variously interpreted unclear provision saying that "When applying this act, it holds that the use of the Czech language fulfills the requirement of fundamental intelligibility with the state language" ; the state language is Slovak and the Minority Language Act basically refers to municipalities with more than 20% ethnic minority population (no such Czech municipalities are found in Slovakia). Since 1 September 2009 (due to an amendment to the State Language Act 270/1995 Z.z.) a language "fundamentally intelligible with the state language" (i.e. the Czech language) may be used in contact with state offices and bodies by its native speakers, and documents written in it and issued by bodies in the Czech Republic are officially accepted. Regardless of its official status, Czech is used commonly both in Slovak mass media and in daily communication by Czech natives as an equal language.
Czech and Slovak have a long history of interaction and mutual influence well before the creation of Czechoslovakia in 1918. Literary Slovak shares significant orthographic features with Czech, as well as technical and professional terminology dating from the Czechoslovak period, but phonetic, grammatical, and vocabulary differences do exist.
Other Slavic languages.
Slavic language varieties tend to be closely related, and have had a large degree of mutual influence, due to the complicated ethnopolitical history of their historic ranges. This is reflected in the many features Slovak shares with neighboring language varieties. Standard Slovak shares high degrees of mutual intelligibility with many Slavic varieties. Despite this closeness to other Slavic varieties, significant variation exists among Slovak dialects. In particular, eastern varieties differ significantly from the standard language, which is based on central and western varieties.
Eastern Slovak dialects have the greatest degree of mutual intelligibility with Rusyn of all the Slovak dialects, but both lack technical terminology and upper register expressions. Polish and Sorbian also differ quite considerably from Czech and Slovak in upper registers, but non-technical and lower register speech is readily intelligible. Some mutual intelligibility occurs with spoken Rusyn, Ukrainian, and even Russian (in this order), although their orthographies are based on the Cyrillic script.
Slovak also exhibits numerous linguistic traits that set it apart from the other West Slavic languages. The Central Slovak dialect, upon which the Standard Slovak language is based, shares various features with South Slavic languages, most notably with the Kajkavian dialect of Serbo-Croatian and with Slovene language.
English.
weekend – víkend, football – futbal, ham & eggs – hemendex, offside – ofsajd, out (football) – aut,
body check (hockey) – bodyček, couch – gauč
German.
German loanwords include "coins," Slovak "mince", German "Münze"; "to wish", Slovak "vinšovať" (colloquially, the standard term is "želať"), German "wünschen"; "funfair," Slovak "jarmok ", German "Jahrmarkt" and "color," Slovak "farba", German "Farbe".
Hungarian.
Hungarians and Slovaks have had a language interaction ever since the settlement of Hungarians in the Carpathian area. Hungarians adopted many words from various Slavic languages related to agriculture and administration, and a number of Hungarian loanwords are found in Slovak. Some examples are as follows:
Romanian.
Romanian words entered the Slovak language in the course of the so-called "Wallachian colonization" in the 14th–16th century when sheep breeding became common in Slovak mountains. Many of today's Slovak rustic-pastoral words like "bača" ("shepherd"; Rmn. "baci"), "valach" ("young shepherd"; cf. the dated exonym for Romanians, "Valach"), "magura" ("hill"; Rmn. "măgura"), "koliba"("chalet"; Rmn. "coliba"), "bryndza" (a variety of sheep cheese; Rmn. "brânză"), "striga" ("witch", "demon"; Rmn. "strigă/strigoi"), etc. were introduced into the Slovak language by Romanian shepherds during the Late Middle Ages and the Early Modern Times. The Romanian influence is most strongly felt in the dialects of the Moravian Wallachia region.
Dialects.
There are many Slovak dialects, which are divided into the following four basic groups:
The fourth group of dialects is often not considered a separate group, but a subgroup of Central and Western Slovak dialects (see e.g. Štolc, 1968), but it is currently undergoing changes due to contact with surrounding languages (Serbo-Croatian, Romanian, and Hungarian) and long-time geographical separation from Slovakia (see the studies in "Zborník Spolku vojvodinských slovakistov", e.g. Dudok, 1993).
For an external map of the three groups in Slovakia see .
The dialect groups differ mostly in phonology, vocabulary, and tonal inflection. Syntactic differences are minor. Central Slovak forms the basis of the present-day standard language. Not all dialects are fully mutually intelligible. It may be difficult for an inhabitant of the Slovak capital Bratislava (in western Slovakia) to understand a dialect from eastern Slovakia.
The dialects are fragmented geographically, separated by numerous mountain ranges. The first three groups already existed in the 10th century. All of them are spoken by the Slovaks outside Slovakia (USA, Canada, Croatian Slavonia, and elsewhere), and central and western dialects form the basis of the lowland dialects (see above).
The western dialects contain features common with the Moravian dialects in the Czech Republic, the southern central dialects contain a few features common with South Slavic languages, and the eastern dialects a few features common with Polish and the East Slavonic languages (cf. Štolc, 1994). Lowland dialects share some words and areal features with the languages surrounding them (Serbo-Croatian, Hungarian, and Romanian).

</doc>
<doc id="28142" url="http://en.wikipedia.org/wiki?curid=28142" title="Supercluster">
Supercluster

Superclusters are large groups of smaller galaxy clusters or galaxy groups and are among the largest known structures of the cosmos. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light years, while the Local Group spans over 10 million light years.
Existence.
The existence of superclusters indicates that the galaxies in our Universe are not uniformly distributed; most of them are drawn together in groups and clusters, with groups containing up to some dozens of galaxies and clusters up to several thousand galaxies. Those groups and clusters and additional isolated galaxies in turn form even larger structures called superclusters.
Their existence was first postulated by George Abell in his 1958 Abell catalogue of galaxy clusters. He called them "second-order clusters", or clusters of clusters.
Superclusters form massive structures of galaxies, called "filaments", "supercluster complexes", "walls" or "sheets", that may span between several hundred million light-years to 10 billion light-years, covering more than 5% of the observable universe. Observations of superclusters likely tell us something about the initial condition of the universe when these superclusters were created. The directions of the rotational axes of galaxies within superclusters may also give us insight and information into the early formation process of galaxies in the history of the Universe.
Interspersed among superclusters are large voids of space in which few galaxies exist. Superclusters are frequently subdivided into groups of clusters called galaxy clouds.
Diagram.
A diagram of Earth's location in the observable Universe and neighbouring superclusters of galaxies. (".")

</doc>
<doc id="28143" url="http://en.wikipedia.org/wiki?curid=28143" title="Salicylic acid">
Salicylic acid

Salicylic acid (from Latin "salix", "willow tree", from the bark of which the substance used to be obtained) is a monohydroxybenzoic acid, a type of phenolic acid and a beta hydroxy acid. It has the formula C7H6O3. This colorless crystalline organic acid is widely used in organic synthesis and functions as a plant hormone. It is derived from the metabolism of salicin. In addition to being an important active metabolite of aspirin ("acetylsalicylic acid"), which acts in part as a prodrug to salicylic acid, it is probably best known for its use as a key ingredient in topical anti-acne products. The salts and esters of salicylic acid are known as salicylates.
Chemistry.
Salicylic acid has the formula C6H4(OH)COOH, where the OH group is "ortho" to the carboxyl group. It is also known as 2-hydroxybenzoic acid. It is poorly soluble in water (2 g/L at 20 °C). Aspirin (acetylsalicylic acid or ASA) can be prepared by the esterification of the phenolic hydroxyl group of salicylic acid with the acetyl group from acetic anhydride or acetyl chloride.
Plant hormone.
Salicylic acid (SA) is a phenolic phytohormone and is found in plants with roles in plant growth and development, photosynthesis, transpiration, ion uptake and transport. SA also induces specific changes in leaf anatomy and chloroplast structure. SA is involved in endogenous signaling, mediating in plant defense against pathogens. It plays a role in the resistance to pathogens by inducing the production of pathogenesis-related proteins. It is involved in the systemic acquired resistance (SAR) in which a pathogenic attack on one part of the plant induces resistance in other parts. The signal can also move to nearby plants by salicylic acid being converted to the volatile ester, methyl salicylate.
Production.
Salicylic acid is biosynthesized from the amino acid phenylalanine. In "Arabidopsis thaliana" it can also be synthesized via a phenylalanine-independent pathway.
Sodium salicylate is commercially prepared by treating sodium phenolate (the sodium salt of phenol) with carbon dioxide at high pressure (100 atm) and high temperature (390K) -a method known as the Kolbe-Schmitt reaction. Acidification of the product with sulfuric acid gives salicylic acid:
It can also be prepared by the hydrolysis of aspirin (acetylsalicylic acid) or methyl salicylate (oil of wintergreen) with a strong acid or base.
History.
The Cherokee and other Native Americans used an infusion of the bark for fever and other medicinal purposes for centuries. The medicinal part of the plant is the inner bark and was used as a pain reliever for a variety of ailments. In 2014, archaeologists identified traces of salicylic acid on 7th century pottery fragments found in east central Colorado. The Reverend Edward Stone, a vicar from Chipping Norton, Oxfordshire, England, noted in 1763 that the bark of the willow was effective in reducing a fever.
The active extract of the bark, called "salicin", after the Latin name for the white willow ("Salix alba"), was isolated and named by the German chemist Johann Andreas Buchner in 1828. A larger amount of the substance was isolated in 1829 by Henri Leroux, a French pharmacist. Raffaele Piria, an Italian chemist, was able to convert the substance into a sugar and a second component, which on oxidation becomes salicylic acid.
Salicylic acid was also isolated from the herb meadowsweet ("Filipendula ulmaria", formerly classified as "Spiraea ulmaria") by German researchers in 1839. While their extract was somewhat effective, it also caused digestive problems such as gastric irritation, bleeding, diarrhea, and even death when consumed in high doses.
Dietary sources.
Unripe fruits and vegetables are natural sources of salicylic acid, particularly blackberries, blueberries, cantaloupes, dates, grapes, kiwi fruits, guavas, apricots, green pepper, olives, tomatoes, radish and chicory; also mushrooms. Some herbs and spices contain quite high amounts, while meat, poultry, fish, eggs and dairy products all have little to no salicylates. Of the legumes, seeds, nuts, and cereals, only almonds, water chestnuts and peanuts have significant amounts.
Medicinal and cosmetic uses.
Salicylic acid is known for its ability to ease aches and pains and reduce fevers. These medicinal properties, particularly fever relief, have been known since ancient times, and it is used as an anti-inflammatory drug.
In modern medicine, salicylic acid and its derivatives are used as constituents of some rubefacient products. For example, methyl salicylate is used as a liniment to soothe joint and muscle pain, and choline salicylate is used topically to relieve the pain of mouth ulcers.
As with other hydroxy acids, salicylic acid is a key ingredient in many skin-care products for the treatment of seborrhoeic dermatitis, acne, psoriasis, calluses, corns, keratosis pilaris, acanthosis nigricans, ichthyosis, and warts. The standard treatment for calluses is a 6% aspirin suspension in petroleum jelly, applied on the callus for one hour and then removed with washing. Salicylic acid works as a keratolytic, comedolytic, and bacteriostatic agent, causing the cells of the epidermis to shed more readily, opening clogged pores and neutralizing bacteria within, preventing pores from clogging up again by constricting pore diameter, and allowing room for new cell growth. Because of its effect on skin cells, salicylic acid is used in several shampoos to treat dandruff. Use of concentrated solutions of salicylic acid may cause hyperpigmentation on unpretreated skin for those with darker skin types (Fitzpatrick phototypes IV, V, VI), as well as with the lack of use of a broad spectrum sunblock.
Bismuth subsalicylate, a salt of bismuth and salicylic acid, is the active ingredient in stomach relief aids such as Pepto-Bismol, is the main ingredient of Kaopectate, and “displays anti-inflammatory action (due to salicylic acid) and also acts as an antacid and mild antibiotic.”
A 2004 article in "New Scientist" discussing the controversial idea of treating salicylates as micronutrients, “akin to vitamins and antioxidants,” suggested that “perhaps in future we might even call salicylate ‘vitamin S’.”
Mechanism of action.
Salicylic acid has been shown to work through several different pathways. It produces its anti-inflammatory effects via suppressing the activity of cyclooxygenase (COX), an enzyme that is responsible for the production of pro-inflammatory mediators such as the prostaglandins. It does this not by direct inhibition of COX like most other non-steroidal anti-inflammatory drugs (NSAIDs) but instead by suppression of the expression of the enzyme (via a yet-unelucidated mechanism). Salicylic acid has also been shown to activate adenosine monophosphate-activated protein kinase (AMPK), and it is thought that this action may play a role in the anticancer effects of the compound and its prodrugs aspirin and salsalate. In addition, the antidiabetic effects of salicylic acid are likely mediated by AMPK activation primarily through allosteric conformational change that increases levels of phosphorylation. Salicylic acid also uncouples oxidative phosphorylation, which leads to increased ADP:ATP and AMP:ATP ratios in the cell. As a consequence, salicylic acid may alter AMPK activity and subsequently exert its anti-diabetic properties through altered energy status of the cell. Even in AMPK knock-out mice, however, there is an anti-diabetic effect, demonstrating that there is at least one additional, yet-unidentified action of the compound.
Other uses.
Although toxic in large quantities, salicylic acid is used as a food preservative, a bactericidal, and an antiseptic. 
Sodium salicylate is a useful phosphor in the vacuum ultraviolet, with nearly flat quantum efficiency for wavelengths between 10 to 100 nm. It fluoresces in the blue at 420 nm. It is easily prepared on a clean surface by spraying a saturated solution of the salt in methanol followed by evaporation.
Safety.
As a topical agent, and as a beta-hydroxy acid (and unlike alpha-hydroxy acids), salicylic acid is capable of penetrating and breaking down fats and lipids, making it capable of causing moderate chemical burns of the skin if at very high concentrations. It is capable of damaging the lining of pores in such cases if the solvent is alcohol, acetone, or an oil. Over-the-counter limits are set at 2% for topical left on the face and 3% for those expected to be washed off, such as acne cleansers or shampoo. Caution should be exercised when handling large volumes of salicylic acid, and protective gloves are recommended for any repeat, prolonged exposure. 17% and 27% salicylic acid, which is often sold for wart removal, should not be applied to the face and should not be used for acne treatment. Even for wart removal, such a solution should be applied twice a day – more frequent use may lead to an increase in side-effects without an increase in efficacy.
When ingested, salicylic acid has a possible ototoxic effect by inhibiting prestin. It can induce transient hearing loss in zinc-deficient individuals. This finding is based on clinical studies with rats. An injection of salicylic acid induced hearing loss in zinc-deficient rats, while a simultaneous injection of zinc reversed the hearing loss. An injection of magnesium in the zinc-deficient rats did not reverse the salicylic acid-induced hearing loss.
There are no studies specifically looking at topical salicylic acid in pregnancy. Oral salicylic acid has not been associated with an increase in malformations if used during the first trimester, but use of aspirin in late pregnancy has been associated with bleeding, especially intracranial bleeding. The risks of aspirin late in pregnancy are probably not relevant for a topical exposure to salicylic acid, even late in the pregnancy, because of its low systemic levels. Topical salicylic acid is common in many over-the-counter dermatological agents, and the lack of adverse reports suggests a low teratogenic potential.
Salicylic acid overdose can lead to salicylate intoxication, which often presents clinically in a state of metabolic acidosis with compensatory respiratory alkalosis. In patients presenting with an acute overdose, a 16% morbidity rate and a 1% mortality rate are observed.
Some people are hypersensitive to salicylic acid and related compounds.
The United States Food and Drug Administration (FDA) recommends the use of sun protection when using skincare products containing salicylic acid (or any other BHA) on sun-exposed skin areas.
There are data that support an association between exposure to salicylic acid and Reye's Syndrome. The National Reye's Syndrome Foundation cautions against the usage of these substances, and other substances similar to aspirin, on children and adolescents.
Epidemiological research has shown an association between the development of Reye's Syndrome and the use of aspirin (a salicylate compound) for treating the symptoms of influenza-like illnesses, chicken pox, colds, etc.
The U.S. Surgeon General, the FDA, the Centers for Disease Control and Prevention, and the American Academy of Pediatrics recommend that aspirin and combination products containing aspirin not be given to children under 19 years of age during episodes of fever-causing illnesses, because of a concern about Reye's Syndrome.

</doc>
<doc id="28144" url="http://en.wikipedia.org/wiki?curid=28144" title="Seaborgium">
Seaborgium

Seaborgium is a synthetic element with symbol Sg and atomic number 106. Its most stable isotope 271Sg has a half-life of 1.9 minutes. A more recently discovered isotope 269Sg has a potentially slightly longer half-life (ca. 2.1 min) based on the observation of a single decay. Chemistry experiments with seaborgium have firmly placed it in group 6 as a heavier homologue to tungsten. Seaborgium is the only element named after a person (Glenn T. Seaborg) who was alive at the time the naming was publicized.
History.
Discovery.
Scientists working at the Joint Institute for Nuclear Research in Dubna, USSR reported their discovery of element 106 in June 1974. Synthesis was also reported in September 1974 at the Super HILAC accelerator at the Lawrence Berkeley Laboratory by a joint Lawrence Berkeley/Lawrence Livermore collaboration led by Albert Ghiorso and E. Kenneth Hulet. They produced the new nuclide 263Sg by bombarding a target of 249Cf with 18O ions. 
249Cf + 18O→263Sg
This nuclide decays by α emission with a half-life of 0.9 ± 0.2 seconds.
Naming.
The Berkeley/Livermore collaboration suggested the name "seaborgium" (Sg) to honor the American chemist Glenn T. Seaborg credited as a member of the American group in recognition of his participation in the discovery of several other actinides. The name selected by the team became controversial. The IUPAC adopted "unnilhexium" (symbol "Unh") as a temporary, systematic element name. In 1994 a committee of IUPAC recommended that element 106 be named "rutherfordium" and adopted a rule that no element can be named after a living person. This ruling was fiercely objected to by the American Chemical Society. Critics pointed out that a precedent had been set when einsteinium was proposed as a name during Albert Einstein's life and a survey indicated that chemists were not concerned with the fact that Seaborg was still alive. In 1997, as part of a compromise involving elements 104 to 108, the name "seaborgium" for element 106 was recognized internationally. The name rutherfordium was assigned to element 104 instead.
Chemical properties.
Extrapolated properties.
Seaborgium is projected to be the third member of the 6d series of transition metals and the heaviest member of group 6 in the Periodic Table, below chromium, molybdenum and tungsten. All the members of the group readily portray their group oxidation state of +6 and the state becomes more stable as the group is descended. Thus seaborgium is expected to form a stable +6 state. For this group, stable +5 and +4 states are well represented for the heavier members and the +3 state is known but reducing, except for chromium(III).
Much seaborgium chemical behavior is predicted by extrapolation from its lighter congeners molybdenum and tungsten. Molybdenum and tungsten readily form stable trioxides MO3, so seaborgium should form SgO3. The oxides MO3 are soluble in alkali with the formation of oxyanions, so seaborgium should form a seaborgate ion, SgO42−. In addition, WO3 reacts with acid, suggesting similar amphotericity for SgO3. Molybdenum oxide, MoO3, also reacts with moisture to form a hydroxide MoO2(OH)2, so SgO2(OH)2 is also feasible. The heavier homologues readily form the volatile, reactive hexahalides MX6 (X=Cl,F). Only tungsten forms the unstable hexabromide, WBr6. Therefore, the compounds SgF6 and SgCl6 are predicted, and "eka-tungsten character" may show itself in increased stability of the hexabromide, SgBr6. These halides are unstable to oxygen and moisture and readily form volatile oxyhalides, MOX4 and MO2X2. Therefore SgOX4 (X=F,Cl) and SgO2X2 (X=F,Cl) should be possible. In aqueous solution, a variety of anionic oxyfluoro-complexes are formed with fluoride ion, examples being MOF5− and MO3F33−. Similar seaborgium complexes are expected.
Experimental chemistry.
Gas phase.
Initial experiments aiming at probing the chemistry of seaborgium focused on the gas thermochromatography of a volatile oxychloride. Seaborgium atoms were produced in the reaction 248Cm(22Ne,4n)266Sg, thermalised, and reacted with an O2/HCl mixture. The adsorption properties of the resulting oxychloride were measured and compared with those of molybdenum and tungsten compounds. The results indicated that seaborgium formed a volatile oxychloride akin to those of the other group 6 elements:
In 2001, a team continued the study of the gas phase chemistry of seaborgium by reacting the element with O2 in a H2O environment. In a manner similar to the formation of the oxychloride, the results of the experiment indicated the formation of seaborgium oxide hydroxide, a reaction well known among the lighter group 6 homologues.
Aqueous phase.
In its aqueous chemistry, seaborgium has been shown to resemble its lighter homologues molybdenum and tungsten, forming a stable +6 oxidation state. Seaborgium was eluted from cation exchange resin using a HNO3/HF solution, most likely as neutral SgO2F2 or the anionic complex ion [SgO2F3]−. In contrast, in 0.1 M HNO3, seaborgium does not elute, unlike Mo and W, indicating that the hydrolysis of [Sg(H2O)6]6+ only proceeds as far as the cationic complex [Sg(OH)5(H2O)]+.
Nucleosynthesis.
Cold fusion experiments.
"This section deals with the synthesis of nuclei of seaborgium by so-called "cold" fusion reactions. These are processes which create compound nuclei at low excitation energy (~10-20 MeV, hence "cold"), leading to a higher probability of survival from fission. The excited nucleus then decays to the ground state via the emission of one or two neutrons only."
The first attempt to synthesise seaborgium in cold fusion reactions was performed in September 1974 by a Soviet team led by G. N. Flerov at the Joint Institute for Nuclear Research at Dubna. They reported producing a 0.48 s spontaneous fission (SF) activity which they assigned to the isotope 259Sg. Based on later evidence it was suggested that the team most likely measured the decay of 260Sg and its daughter 256Rf. The TWG concluded that, at the time, the results were insufficiently convincing.
The Dubna team revisited this problem in 1983–1984 and were able to detect a 5 ms SF activity assigned directly to 260Sg.
The team at GSI studied this reaction for the first time in 1985 using the improved method of correlation of genetic parent-daughter decays. They were able to detect 261Sg (x=1) and 260Sg and measured a partial 1n neutron evaporation excitation function.
In December 2000, the reaction was studied by a team at GANIL, France and were able to detect 10 atoms of 261Sg and 2 atoms of 260Sg to add to previous data on the reaction.
After a facility upgrade, the GSI team measured the 1n excitation function in 2003 using a metallic lead target. Of significance, in May 2003, the team successfully replaced the lead-208 target with more resistant lead(II) sulfide targets (PbS) which will allow more intense beams to be used in the future. They were able to measure the 1n,2n and 3n excitation functions and performed the first detailed alpha-gamma spectroscopy on the isotope 261Sg. They detected ~1600 atoms of the isotope and identified new alpha lines as well as measuring a more accurate half-life and new EC and SF branchings. Furthermore, they were able to detect the K X-rays from the daughter rutherfordium element for the first time. They were also able to provide improved data for 260Sg, including the tentative observation of an isomeric level. The study was continued in September 2005 and March 2006. The accumulated work on 261Sg was published in 2007.
Work in September 2005 also aimed to begin spectroscopic studies on 260Sg.
The team at the LBNL recently restudied this reaction in an effort to look at the spectroscopy of the isotope 261Sg. They were able to detect a new isomer, 261mSg, decaying by internal conversion into the ground state. In the same experiment, they were also able to confirm a K-isomer in the daughter 257Rf, namely 257m2Rf.
The team at Dubna also studied this reaction in 1974 with identical results as for their first experiments with a Pb-208 target. The SF activities were first assigned to 259Sg and later to 260Sg and/or 256Rf. Further work in 1983–1984 also detected a 5 ms SF activity assigned to the parent 260Sg.
The GSI team studied this reaction for the first time in 1985 using the method of correlation of genetic parent-daughter decays. They were able to positively identify 259Sg as a product from the 2n neutron evaporation channel.
The reaction was further used in March 2005 using PbS targets to begin a spectroscopic study of the even-even isotope 260Sg.
This reaction was studied in 1974 by the team at Dubna. It was used to assist them in their assignment of the observed SF activities in reactions using Pb-207 and Pb-208 targets. They were unable to detect any SF, indicating the formation of isotopes decaying primarily by alpha decay.
The team at Dubna also studied this reaction in their series of cold fusion reactions performed in 1974. Once again they were unable to detect any SF activities. The reaction was revisited in 2006 by the team at LBNL as part of their studies on the effect of the isospin of the projectile and hence the mass number of the compound nucleus on the yield of evaporation residues. They were able to identify 259Sg and 258Sg in their measurement of the 1n excitation function.
The team at Dubna also studied this reaction in their series of cold fusion reactions performed in 1974. Once again they were unable to detect any SF activities.
In 1994, the synthesis of seaborgium was revisited using this reaction by the GSI team, in order to study the new even-even isotope 258Sg. Ten atoms of 258Sg were detected and decayed by spontaneous fission.
Hot fusion experiments.
"This section deals with the synthesis of nuclei of seaborgium by so-called "hot" fusion reactions. These are processes which create compound nuclei at high excitation energy (~40-50 MeV, hence "hot"), leading to a reduced probability of survival from fission and quasi-fission. The excited nucleus then decays to the ground state via the emission of 3-5 neutrons."
This reaction was first studied by Japanese scientists at the Japan Atomic Energy Research Institute (JAERI) in 1998. They detected a spontaneous fission activity which they tentatively assigned to the new isotope 264Sg or 263Db, formed by EC of 263Sg.
In 2006, the teams at GSI and LBNL both studied this reaction using the method of correlation of genetic parent-daughter decays. The LBNL team measured an excitation function for the 4n,5n and 6n channels, whilst the GSI team were able to observe an additional 3n activity. Both teams were able to identify the new isotope 264Sg which decayed with a short lifetime by spontaneous fission.
In 1993, at Dubna, Yuri Lazarev and his team announced the discovery of long-lived 266Sg and 265Sg produced in the 4n and 5n channels of this nuclear reaction following the search for seaborgium isotopes suitable for a first chemical study.
It was announced that 266Sg decayed by 8.57 MeV alpha-particle emission with a projected half-life of ~20 s, lending strong support to the stabilising effect of the Z=108,N=162 closed shells.
This reaction was studied further in 1997 by a team at GSI and the yield, decay mode and half-lives for 266Sg and 265Sg have been confirmed, although there are still some discrepancies. In the recent synthesis of 270Hs (see hassium), 266Sg was found to undergo exclusively SF with a short half-life (TSF = 360 ms). It is possible that this is the ground state, (266gSg) and that the other activity, produced directly, belongs to a high spin K-isomer, 266mSg, but further results are required to confirm this.
A recent re-evaluation of the decay characteristics of 265Sg and 266Sg has suggested that all decays to date in this reaction were in fact from 265Sg, which exists in two isomeric forms. The first, 265aSg has a principal alpha-line at 8.85 MeV and a calculated half-life of 8.9 s, whilst 265bSg has a decay energy of 8.70 MeV and a half-life of 16.2 s. Both isomeric levels are populated when produced directly. Data from the decay of 269Hs indicates that 265bSg is produced during the decay of 269Hs and that 265bSg decays into the shorter-lived 261gRf isotope. This means that the observation of 266Sg as a long-lived alpha emitter is retracted and that it does indeed undergo fission in a short time.
Regardless of these assignments, the reaction has been successfully used in the recent attempts to study the chemistry of seaborgium (see below).
The synthesis of seaborgium was first realized in 1974 by the LBNL/LLNL team. In their discovery experiment, they were able to apply the new method of correlation of genetic parent-daughter decays to identify the new isotope 263Sg. In 1975, the team at Oak Ridge were able to confirm the decay data but were unable to identify coincident X-rays in order to prove that seaborgium was produced. In 1979, the team at Dubna studied the reaction by detection of SF activities. In comparison with data from Berkeley, they calculated a 70% SF branching for 263Sg. The original synthesis and discovery reaction was confirmed in 1994 by a different team at LBNL.
As decay product.
Isotopes of seaborgium have also been observed in the decay of heavier elements. Observations to date are summarised in the table below:
Isotopes.
There are 14 known isotopes of seaborgium (excluding meta-stable and K-spin isomers). The longest-lived is currently 269Sg which decays through alpha decay and spontaneous fission, with a half-life of around 2.1 minutes. The shortest-lived isotope is 258Sg which also decays through alpha decay and spontaneous fission. It has a half-life of 2.9 ms.
Nuclear isomerism.
Initial work identified an 8.63 MeV alpha-decaying activity with a half-life of ~21s and assigned to the ground state of 266Sg. Later work identified a nuclide decaying by 8.52 and 8.77 MeV alpha emission with a half-life of ~21s, which is unusual for an even-even nuclide. Recent work on the synthesis of 270Hs identified 266Sg decaying by SF with a short 360 ms half-life. The recent work on 277Cn and 269Hs has provided new information on the decay of 265Sg and 261Rf. This work suggested that the initial 8.77 MeV activity should be reassigned to 265Sg. Therefore the current information suggests that the SF activity is the ground state and the 8.52 MeV activity is a high spin K-isomer. Further work is required to confirm these assignments. A recent re-evaluation of the data has suggested that the 8.52 MeV activity should be associated with 265Sg and that 266Sg only undergoes fission.
The recent direct synthesis of 265Sg resulted in four alpha-lines at 8.94,8.84,8.76 and 8.69 MeV with a half-life of 7.4 seconds. The observation of the decay of 265Sg from the decay of 277Cn and 269Hs indicated that the 8.69 MeV line may be associated with an isomeric level with an associated half-life of ~ 20 s. It is plausible that this level is causing confusion between assignments of 266Sg and 265Sg since both can decay to fissioning rutherfordium isotopes.
A recent re-evaluation of the data has indicated that there are indeed two isomers, one with a principal decay energy of 8.85 MeV with a half-life of 8.9 s, and a second isomer which decays with energy 8.70 MeV with a half-life of 16.2 s.
The discovery synthesis of 263Sg resulted in an alpha-line at 9.06 MeV. Observation of this nuclide by decay of 271gDs, 271mDs and 267Hs has confirmed an isomer decaying by 9.25 MeV alpha emission. The 9.06 MeV decay was also confirmed. The 9.06 MeV activity has been assigned to the ground state isomer with an associated half-life of 0.3 s. The 9.25 MeV activity has been assigned to an isomeric level decaying with a half-life of 0.9 s.
Recent work on the synthesis of 271g,mDs was resulted in some confusing data regarding the decay of 267Hs. In one such decay, 267Hs decayed to 263Sg which decayed by alpha emission with a half-life of ~ 6 s. This activity has not yet been positively assigned to an isomer and further research is required.
Retracted isotopes.
In the claimed synthesis of 293Uuo in 1999 the isotope 269Sg was identified as a daughter product. It decayed by 8.74 MeV alpha emission with a half-life of 22 s. The claim was retracted in 2001. This isotope was finally created in 2010.

</doc>
<doc id="28145" url="http://en.wikipedia.org/wiki?curid=28145" title="September 15">
September 15

September 15 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28146" url="http://en.wikipedia.org/wiki?curid=28146" title="September 18">
September 18

September 18 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28147" url="http://en.wikipedia.org/wiki?curid=28147" title="September 19">
September 19

September 19 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28148" url="http://en.wikipedia.org/wiki?curid=28148" title="September 20">
September 20

September 20 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28149" url="http://en.wikipedia.org/wiki?curid=28149" title="Serpens">
Serpens

Serpens ("the Serpent", Greek Ὄφις) is a constellation of the northern hemisphere. One of the 48 constellations listed by the 2nd century astronomer Ptolemy, it remains one of the 88 modern constellations defined by the International Astronomical Union. It is unique among the modern constellations in being split into two non-contiguous parts, Serpens Caput (Serpent's Head) to the west and Serpens Cauda (Serpent's Tail) to the east. Between these two halves lies the constellation of Ophiuchus, the "Serpent-Bearer". In figurative representations, the body of the serpent is represented as passing behind Ophiuchus between μ Ser in "Serpens Caput" and ν Ser in "Serpens Cauda".
The brightest star in Serpens is Unukalhai or "Cor Serpentis" "Serpent's Heart", with an apparent magnitude of 2.63. Part of the Milky Way passes through Serpens Cauda, which is therefore rich in deep-sky objects, such as the Eagle Nebula (IC 4703) and its associated star cluster Messier 16. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. Located in Serpens Caput are Seyfert's Sextet, one of the densest galaxy clusters known, and Arp 220, the prototypical ultraluminous infrared galaxy. In addition, it also contains the Hercules–Corona Borealis Great Wall, the largest object in the universe.
History.
In Greek Mythology, Serpens represents a snake held by Asclepius, a healer. Asclepius, represented in the sky by the constellation Ophiuchus, which splits Serpens into two distinct halves, was known for killing a snake that was resurrected because a different snake had placed a certain herb on it before its "death". Serpens is depicted as either winding around Ophiuchus in the night sky or simply passing through him, although the precise reason for either of these is unknown.
In some ancient atlases, the constellations Serpens and Ophiuchus were depicted as two separate constellations, although in most they were shown as a single constellation. Back in this time, there were no official constellation boundaries, so when depicted separately, their bodies were not intertwined with each other.
In Chinese astronomy, most of the stars of Serpens represented part of a wall surrounding a marketplace, known as Tianshi, which was in Ophiuchus and part of Hercules. Serpens also contains a few Chinese constellations. Two stars in the tail represented part of Shilou, the tower with the market office. Another star in the tail represented Liesi, jewel shops. One star in the head (Mu Serpentis) marked Tianru, the crown prince's wet nurse, or sometimes rain.
There were two "serpent" constellations in Babylonian astronomy, known as Mušḫuššu and Bašmu. It appears that Mušḫuššu was depicted as a hybrid of a dragon, a lion and a bird, and loosely corresponded to Hydra.
Bašmu was a horned serpent (c.f. Ningishzida) and loosely corresponds to the Ὄφις constellation of Eudoxus of Cnidus on which the Ὄφις ("Serpens") of Ptolemy is based.
Characteristics.
Serpens is unique among the 88 modern constellations in that it is split into two disconnected regions in the sky: "Serpens Caput" (the head) and "Serpens Cauda" (the tail). The constellation is also one of the only ones that is dependent on another constellation for context; specifically, it is being held by the Serpent Bearer Ophiuchus.
Serpens Caput is bordered by Libra to the south, Virgo and Boötes to the east, Corona Borealis to the north, and Ophiuchus and Hercules to the west; Serpens Cauda is bordered by Sagittarius to the south, Scutum and Aquila to the east, and Ophiuchus to the north and west. Covering 636.9 square degrees total, it ranks 23rd of the 88 constellations in size. It appears prominently in the both the northern and southern skies during the Northern Hemisphere's summer. Its main asterism consists of 11 stars, and 108 stars in total are brighter than magnitude 6.5.
Serpens Caput's boundaries, as set by Eugène Delporte in 1930, are defined by a 15-sided polygon, while Serpens Cauda's are defined by a 25-sided polygon. In the equatorial coordinate system, the right ascension coordinates of Serpens Caput's borders lie between 15h 10.4m and 16h 22.5m, while the declination coordinates are between ° and °. Serpens Cauda's boundaries lie between right ascensions of 17h 16.9m and 18h 58.3m and declinations of ° and °. The International Astronomical Union (IAU) adopted the three-letter abbreviation "Ser" for the constellation in 1922.
Notable features.
Stars.
Head stars.
The brightest star in Serpens, Alpha Serpentis, or Unukalhai, is a red giant of spectral type K2III located approximately 22.68 pc away which marks the snake's heart. With a visual magnitude of 2.63, it can easily be seen with the naked eye even in areas with substantial light pollution. A faint companion is in orbit around the red giant star, although it is not visible to the naked eye. Located near Alpha is Lambda Serpentis, a magnitude 4.42 star rather similar to the Sun located only 12.12 pc away. Another solar analog in Serpens is the primary of Psi Serpentis, a binary star located slightly further away at approximately 14.64 pc.
Beta, Gamma, and Iota Serpentis form a distinctive triangular shape marking the head of the snake, with Kappa Serpentis being roughly midway between Gamma and Iota. The brighest of the four with an apparent magnitude of 3.67, Beta Serpentis is a white main-sequence star roughly 155.0 pc distant. It is likely that a nearby star of magnitude 9.95 is physically associated with Beta, although it is not certain. The Mira variable R Serpentis, located between Beta and Gamma, is visible to the naked eye at its maximum brightness of 5.16, but typical of Mira variables, it can fade to below magnitude 14. Gamma Serpentis itself is an F-type subgiant located only 11.25 pc distant and thus is quite bright, being of magnitude 3.84. The star is known to show solar-like oscillations
Delta Serpentis, forming part of the body of the snake between the heart and the head, is a multiple star system located 69.93 pc from Earth. Consisting of four stars, the system has a total apparent magnitude of 3.79 as viewed from Earth, although two of the stars, with a combined apparent magntitude of 3.80, provide nearly all the light that reaches Earth. The primary, a white subgiant, is a Delta Scuti variable with an average apparent magnitude of 4.23. Located very near Delta, both in the night sky and likely in actual space at an estimated distance of 70.87 pc, is the barium star 16 Serpentis. Another notable variable star visible to the naked eye is Chi Serpentis, an Alpha2 Canum Venaticorum variable located midway between Delta and Beta which varies from its median brightness of 5.33 by 0.03 magnitudes over a period of approximately 1.5 days.
The two stars in Serpens Caput that form part of the Snake's body below the heart are Epsilon and Mu Serpentis, both third-magnitude A-type main-sequence stars. Both have a peculiarity: Epsilon is an Am star, while Mu is a binary. Located slightly northwest of Mu is 36 Serpentis, another A-type main-sequence star. This star also has a peculiarity; it is a binary with the primary component being a Lambda Boötis star, meaning that it has solar-like amounts of carbon, nitrogen, and oxygen, while containing very low amounts of iron peak elements. 25 Serpentis, located a few degrees northeast of Mu Serpentis, is a spectroscopic binary consisting of a hot B-type giant and an A-type main-sequence star. The primary is a slowly pulsating B star, which causes the system to vary by 0.03 magnitudes.
Serpens Caput contains many RR Lyrae variables, although most are too faint to be seen without professional equipment. The brightest is VY Serpentis, only of 10th magnitude. Interestingly, this star's period has been increasing by approximately 1.2 seconds per century. A variable star of a different kind is Tau4 Serpentis, a cool red giant that pulsates between magnitudes 5.89 and 7.07 in 87 days. This star has been found to display an inverse P Cygni profile.
Several stars in Serpens have been found to have planets. The brightest, Omega Serpentis, located between Epsilon and Mu, is an orange giant with a planet of approximately 1.7 Jupiter-masses. NN Serpentis, an eclipsing post-common-envelope binary consisting of a white dwarf and a red dwarf, is very likely to have two planets causing variations in the period of the eclipses. Although it does not have a planet, the solar analog HD 137510 has been found to have a brown dwarf companion within the brown dwarf desert.
One of the most interesting stellar remnants in the night sky is PSR B1534+11, a system consisting of two neutron stars in orbit around each other. One of the neutron stars is a pulsar with a period of 37.9 milliseconds. Located approximately 1051 pc distant, the system demonstrates one of the best examples of general relativity. The X-ray emission from the system has been found to be present when the non-pulsar star intersects the equatorial pulsar wind of the pulsar, and the system's orbit has been found to vary slightly.
Tail stars.
The brightest star in the tail, Eta Serpentis, is similar to Alpha Serpentis' primary in that it is a red giant of spectral class K. This star, however, is known to exhibit solar-like oscillations over a period of approximately 2.16 hours. Eta Serpentis was previously classified as a carbon star, which would have made it the brightest carbon star in the sky, although this classification was found to be erroneous.
The other two stars in Serpens Cauda forming its asterism are Theta and Xi Serpentis. Xi, where the asterism crosses over to Mu Serpentis in the head, is a triple star system located approximately 105.2 pc away. Two of the stars, with a combined apparent magnitude of 3.54, form a spectroscopic binary, and thus cannot be resolved with modern equipment. The primary is a white giant with an excess of strontium. Theta, forming the tip of the tail, is also a multiple system, consisting of two A-type main-sequence stars with a combined apparent magnitude of 4.10 separated by almost half an arcminute.
Lying near the boundary with Ophiuchus are Zeta (23.55 pc distant), Nu (209.3 pc distant), and Omicron Serpentis (173.1 pc distant). All three are 4th-magnitude main-sequence stars, with Nu and Omicron being of spectral type A and Zeta being of spectral type F. Nu is a binary star with a secondary component of magnitude 9.3, while Omicron is a Delta Scuti variable with amplitude variations of 0.01 magnitudes. In 1909, the symbiotic nova RT Serpentis appeared near Omicron, although it only reached a maximum magnitude of 10.2.
The star system 59 Serpentis, also known as d Serpentis, is a triple star system consisting of a spectroscopic binary containing an A-type star and an orange giant and an orange giant secondary. The system shows irregular variations in brightness between magnitudes 5.17 and 5.2. In 1970, the nova FH Serpentis appeared just slightly north of 59 Serpentis, reaching a maximum brightness of 4.5. Also near 59 Serpentis in the Serpens Cloud are several Orion variables. MWC 297 is a Herbig Be star that in 1994 exhibited a large X-ray flare and increased in X-ray luminosity by five times before returning to the quiescent state. The star also appears to possess a circumstellar disk. Another Orion variable in the region is VV Serpentis, a Herbig Ae star that has been found to exhibit Delta Scuti pulsations. VV Serpentis has also, like MWC 297, been found to have a dusty disk surrounding it, and is also a UX Orionis star, meaning that it shows irregular variations in its brightness.
The star HR 6958, also known as MV Serpentis, is an Alpha2 Canum Venaticorum variable that is faintly visible to the naked eye. The star's metal abundance is incredibly high for most metals at the iron peak and heavier, and has also been found to contain excess silicon. Barely visible to the naked eye is HD 172365, a likely post-blue straggler in the open cluster IC 4756 that contains a large excess of lithium. HD 172189, also located in IC 4756, is an Algol variable eclipsing binary with a 5.70 day period. The primary star in the system is also a Delta Scuti variable, undergoing multiple pulsation frequencies, which, combined with the eclipses, causes the system to vary by around a tenth of a magnitude.
As the Milky Way passes through it, Serpens Cauda contains many massive OB stars. Several of these are visible to the naked eye, such as NW Serpentis, an early Be star that has been found to be somewhat variable. The variability is interesting; according to one study, it could be one of the first discovered hybrids between Beta Cephei variables and slowly pulsating B stars. Although not visible to the naked eye, HD 167971 (MY Serpentis) is a Beta Lyrae variable triple system consisting of three very hot O-type stars. A member of the cluster NGC 6604, the two eclipsing stars are both blue giants, with one being of the very early spectral type O7.5III. The remaining star is either a blue giant or supergiant of a late O or early B spectral type. Also an eclipsing binary, the HD 166734 system consists of two O-type blue supergiants in orbit around each other. Less extreme in terms of mass and temperature is HD 161701, a spectroscopic binary consisting of a B-type primary and an Ap secondary, although it is the only known spectroscopic binary to consist of a star with excess of mercury and manganese and an Ap star.
South of the Eagle Nebula on the border with Sagittarius is the eclipsing binary W Serpentis, whose primary is a white giant that is interacting with the secondary. The system has been found to contain an accretion disk, and was one of the first discovered Serpentids, which are eclipsing binaries containing exceptionally strong far-ultraviolet spectral lines. It is suspected that such Serpentids are in an earlier evolutionary phase, and will evolve first into double periodic variables and then classical Algol variables. Also near the Eagle Nebula is the eclipsing Wolf–Rayet binary CV Serpentis, consisting of a Wolf–Rayet star and a hot O-type subgiant. The system is surrounded by a ring-shaped nebula, likely formed during the Wolf–Rayet phase of the primary. The eclipses of the system very erratic, and although there are two theories as to why, neither of them is completely consistent with current understanding of stars.
Serpens Cauda contains a few X-ray binaries. One of these, GX 17+2, is a low-mass X-ray binary consisting of a neutron star and, as in all low-mass X-ray binaries, a low-mass star. The system has been classified as a Sco-like Z source, meaning that its accretion is near the Eddington limit. The system has also been found to approximately every 3 days brighten by around 3.5 K-band magnitudes, possibly due to the presence of a synchrotron jet. Another low-mass X-ray binary, Serpens X-1, undergoes occasional X-ray bursts. One in particular lasted nearly four hours, possibly explained by the burning of carbon in "a heavy element ocean".
Deep-sky objects.
Head objects.
As the Milky Way does not pass through this part of Serpens, a view to many galaxies beyond it is possible. However, a few structures of the Milky Way Galaxy are present in Serpens Caput, such as Messier 5, a globular cluster located approximately 8° southwest of α Serpentis, next to the star 5 Serpentis. Barely visible to the naked eye under good conditions, and is located approximately 25000 ly distant. Messier 5 contains a large number of known RR Lyrae variable stars, and is receding from us at over 50 km/s. Interestingly, the cluster contains two millisecond pulsars, one of which is in a binary. The cluster has been used to test for magnetic dipole moments in neutrinos, which could shed light on some hypothetical particles such as the axion. Another globular cluster is Palomar 5, located just south of Messier 5. Interestingly, many stars are leaving this globular cluster due to the Milky Way's gravity, forming a tidal tail over 30000 ly long.
Hoag's Object, located 600 million light-years from Earth, is one of the most famous of a very rare class of galaxies known as ring galaxies. The outer ring is largely composed of young blue stars while the core is made up of older yellow stars. The predominant theory regarding its formation is that the progenitor galaxy was a barred spiral galaxy whose arms had velocities too great to keep the galaxy's coherence and therefore detached.
Arp 220 is another unusual galaxy in Serpens. The prototypical ultraluminous infrared galaxy, Arp 220 is located 250 million light-years from Earth. It consists of two large spiral galaxies in the process of colliding with their nuclei orbiting at a distance of 1,200 light-years, causing extensive star formation throughout both components. It possesses a large cluster of more than a billion stars, partially covered by thick dust clouds near one of the galaxies' core.
Seyfert's Sextet is a group of six galaxies, four of which are interacting gravitationally and two of which simply appear to be a part of the group despite their greater distance. The gravitationally bound cluster lies at a distance of 190 million light-years from Earth and is approximately 100,000 light-years across, making Seyfert's Sextet one of the densest galaxy group known. Astronomers predict that the four interacting galaxies will eventually merge to form a large elliptical galaxy.
Tail objects.
Part of the Milky Way passes through the tail, and thus Serpens Cauda is rich in deep-sky objects within our own galaxy. The Eagle Nebula and its associated star cluster, Messier 16 lie 7,000 light-years from Earth in the direction of the galactic center. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. The stars being born in the Eagle Nebula, added to those with an approximate age of 5 million years have an average temperature of 45,000 kelvins and produce prodigious amounts of radiation that will eventually destroy the dust pillars. Despite its fame, the Eagle Nebula is fairly dim, with an integrated magnitude of approximately 6.0. The star-forming regions in the nebula are often evaporating gaseous globules; unlike Bok globules they only hold one protostar.
North of Messier 16, at a distance of approximately 1900 pc, is the OB association Serpens OB2, containing over 100 OB stars. Around 5 million years old, the association appears to still contain star-forming regions, and the light from its stars is illuminating the HII region S 54. Within this HII region is the open cluster NGC 6604, which is the same age as the surrounding OB association, and the cluster is now thought to simply be the densest part of it. Interestingly, the cluster appears to be producing a thermal chimney of ionized gas, caused by the interaction of the gas from the galactic disk with the galactic halo.
Another open cluster in Serpens Cauda is IC 4756, containing at least one naked-eye star (HD 172365) (another naked-eye star in the vicinity, HD 171586, is most likely likely unrelated). Located approximately 437 pc distant, the cluster is estimated to be around 800 million years old, which is quite old for an open cluster. Despite the presence of the Milky Way in Serpens Cauda, one globular cluster can be found: NGC 6535, although invisible to the naked eye, can be made out in small telescopes just north of Zeta Serpentis. Rather small and sparse for a globular cluster, this cluster contains no known RR Lyrae variables, which is unusual for a globular cluster.
MWC 922 is a star surrounded by a planetary nebula. Dubbed the Red Square Nebula due to its similarities to the Red Rectangle Nebula, the planetary nebula appears to be a nearly perfect square with a dark band around the equatorial regions. The nebula contains concentric rings, which are similar to those seen in the supernova SN 1987A. MWC 922 itself is an FS Canis Majoris variable, meaning that it is a Be star containing exceptionally bright hydrogen emission lines as well as select forbidden lines, likely due to the presence of a close binary. East of Xi Serpentis is another planetary nebula, Abell 41, containing the binary star MT Serpentis at its center. The nebula appears to have a bipolar structure, and the axis of symmetry of the nebula has been found to be within 5° of the line perpendicular to the orbital plane of the stars, strengthening the link between binary stars and bipolar planetary nebulae. On the other end of the stellar age spectrum is L483, a dark nebula which contains the protostar IRAS 18418-0440. Although classified as a class 0 protostar, it has some unusual features for such an object, such as a lack of high-velocity stellar winds, and it has been proposed that this object is in transition between class 0 and class I. A variable nebula exists around the protostar, although it is only visible in infrared light.
The Serpens cloud is a massive star-forming molecular cloud located in the southern part of Serpens Cauda. Only two million years old and 415 pc distant, the cloud is known to contain many protostars such as Serpens FIRS 1 and Serpens SVS 20. The Serpens South protocluster was uncovered by NASA's Spitzer Space Telescope in the southern portion of the cloud, and it appears that star formation is still continuing in the region.
Despite the presence of the Milky Way, several active galaxies are visible in Serpens Cauda as well, such as PDS 456, located near Xi Serpentis. The most intrinsically luminous nearby active galaxy, this AGN has been found to be extremely variable in the X-ray spectrum. This has allowed light to be shed on the nature of the supermassive black hole at the center, likely a Kerr black hole. It is possible that the quasar is undergoing a transition from an ultraluminous infrared galaxy to a classical radio-quiet quasar, but there are problems with this theory, and the object appears to be an exceptional object that does not completely lie within current classification systems. Nearby is NRAO 530, a blazar that has been known to flare in the X-rays occasionally. One of these flares was for less than 2000 seconds, making it the shortest flare ever observed in a blazar as of 2004. The blazar also appears to show periodic variability in its radio wave output over two different periods of six and ten years.
Meteor showers.
There are two daytime meteor showers that radiate from Serpens, the Omega Serpentids and the Sigma Serpentids. Both showers peak between December 18 and December 25.
External links.
Coordinates: 

</doc>
<doc id="28150" url="http://en.wikipedia.org/wiki?curid=28150" title="Sculptor Group">
Sculptor Group

The Sculptor Group is a loose group of galaxies near the south galactic pole. The group is one of the closest groups of galaxies to the Local Group; the distance to the center of the group from the Milky Way is approximately 3.9 Mpc.
The Sculptor Galaxy (NGC 253) and a few other galaxies form a gravitationally-bound core in the center of this group. A few other galaxies at the periphery may be associated with the group but may not be gravitationally bound. Because most of the galaxies in this group are actually weakly gravitationally bound, the group may also be described as a filament.
Members.
The table below lists galaxies that have been identified as associated with the Sculptor Galaxy (and hence associated with the group) by I. D. Karachentsev and collaborators.
Note that the object names used in the above table differ from the names used by Karachentsev and collaborators. NGC, IC, UGC, and PGC numbers have been used when possible to allow for easier referencing.
Foreground galaxies.
The irregular galaxy NGC 55, the spiral galaxy NGC 300, and their companion galaxies have been considered by many researchers to be part of this group. However, recent distance measurements to these and other galaxies in the same region of the sky show that NGC 55, NGC 300, and their companions may simply be foreground galaxies that are physically unassociated with the Sculptor Group.
References.
Coordinates: 

</doc>
<doc id="28151" url="http://en.wikipedia.org/wiki?curid=28151" title="State (polity)">
State (polity)

A state is an organized political community living under one system of government. Moreover, in American English, "state" and "government" are often , both referring to an organized political group that exercises authority over a particular territory. States may be sovereign. The term state is also applied to federated states that are members of a federal union, which is the sovereign state. Some states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state. The state can also be used to refer to the secular branches of government within a state, often as a manner of contrasting them with churches and civilian institutions.
Many human societies have been governed by states for millennia, but many have been stateless societies. The first states arose about 5,500 years ago in conjunction with rapid growth of cities, invention of writing, and codification of new forms of religion. Over time, a variety of different forms developed, employing a variety of justifications for their existence (such as divine right, the theory of the social contract, etc.). Today, however, the modern nation-state is the predominant form of state to which people are subject.
Definitional issues.
There is no academic consensus on the most appropriate definition of the state. The term "state" refers to a set of different, but interrelated and often overlapping, theories about a certain range of political phenomena. The act of defining the term can be seen as part of an ideological conflict, because different definitions lead to different theories of state function, and as a result validate different political strategies. And according to Jeffrey & Painter, "if we define the 'essence' of the state in one place or era, we are liable to find that in another time or space something which is also understood to be a state has different 'essential' characteristics" 
The most commonly used definition is Max Weber's, which describes the state as a compulsory political organization with a centralized government that maintains a monopoly of the legitimate use of force within a certain territory. General categories of state institutions include administrative bureaucracies, legal systems, and military or religious organizations.
According to the "Oxford English Dictionary", a state is "a an organized political community under one government; a commonwealth; a nation. b such a community forming part of a federal republic, esp the United States of America".
Confounding the definitional problem is that "state" and "government" are often used as synonyms in common conversation and even some academic discourse. According to this definitional schema, the states are nonphysical persons of international law, governments are organizations of people. The relationship between a government and its state is one of representation and authorized agency.
Types of states.
States may be classified as "sovereign" if they are not dependent on, or subject to any other power or state. Other states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state. Many states are federated states which participate in a federal union. A federated state is a territorial and constitutional community forming part of a federation. Such states differ from sovereign states, in that they have transferred a portion of their sovereign powers to a federal government.
The state and government.
A state can be distinguished from a government. The government is the particular group of people, the administrative bureaucracy that controls the state apparatus at a given time. That is, governments are the means through which state power is employed. States are served by a continuous succession of different governments. States are immaterial and nonphysical social objects, whereas governments are groups of people with certain coercive powers.
Each successive government is composed of a specialized and privileged body of individuals, who monopolize political decision-making, and are separated by status and organization from the population as a whole. Their function is to enforce existing laws, legislate new ones, and arbitrate conflicts. In some societies, this group is often a self-perpetuating or hereditary class. In other societies, such as democracies, the political roles remain, but there is frequent turnover of the people actually filling the positions.
States and nation-states.
States can also be distinguished from the concept of a "nation", which refers to a large geographical area, and the people therein who perceive themselves as having a common identity.
The state and civil society.
In the classical thought the state was identified with both political society and civil society as a form of political community, while the modern thought distinguished the nation state as a political society from civil society as a form of economic society.
Thus in the modern thought the state is contrasted with civil society.
The man versus the state.
Antonio Gramsci believed that civil society is the primary locus of political activity because it is where all forms of "identity formation, ideological struggle, the activities of intellectuals, and the construction of hegemony take place." and that civil society was the nexus connecting the economic and political sphere. Arising out of the collective actions of civil society is what Gramsci calls "political society", which Gramsci differentiates from the notion of the state as a polity. He stated that politics was not a "one-way process of political management" but, rather, that the activities of civil organizations conditioned the activities of political parties and state institutions, and were conditioned by them in turn. Louis Althusser argued that civil organizations such as church, schools, and the family are part of an "ideological state apparatus" which complements the "repressive state apparatus" (such as police and military) in reproducing social relations.
Jürgen Habermas, spoke of a public sphere that was distinct from both the economic and political sphere.
Given the role that many social groups have in the development of public policy, and the extensive connections between state bureaucracies and other institutions, it has become increasingly difficult to identify the boundaries of the state. Privatization, nationalization, and the creation of new regulatory bodies also change the boundaries of the state in relation to society. Often the nature of quasi-autonomous organizations is unclear, generating debate among political scientists on whether they are part of the state or civil society. Some political scientists thus prefer to speak of policy networks and decentralized governance in modern societies rather than of state bureaucracies and direct state control over policy.
Theories of state function.
Most political theories of the state can roughly be classified into two categories. The first are known as "liberal" or "conservative" theories, which treat capitalism as a given, and then concentrate on the function of states in capitalist society. These theories tend to see the state as a neutral entity separated from society and the economy. Marxist theories on the other hand, see politics as intimately tied in with economic relations, and emphasize the relation between economic power and political power. They see the state as a partisan instrument that primarily serves the interests of the upper class.
Anarchist.
Anarchism is a political philosophy which considers the state immoral, unnecessary, and harmful and instead promotes a stateless society, or anarchy.
Anarchists believe that the state is inherently an instrument of domination and repression, no matter who is in control of it. Anarchists note that the state possesses the monopoly on the legal use of violence. Unlike Marxists, anarchists believe that revolutionary seizure of state power should not be a political goal. They believe instead that the state apparatus should be completely dismantled, and an alternative set of social relations created, which are not based on state power at all.
Various Christian anarchists, such as Jacques Ellul, have identified the State and political power as the Beast in the Book of Revelation.
Marxist perspective.
Marx and Engels were clear in that the communist goal was a classless society in which the state would have "withered away". Their views are scattered throughout the Marx/Engels Collected Works and address past or the then extant state forms from an analytical or tactical viewpoint, not future social forms, speculation about which is generally anathema to groups considering themselves Marxist but who, not having conquered the existing state power(s) are not in the situation of supplying the institutional form of an actual society. To the extent that it makes sense, there is no single "Marxist theory of state", but rather many different "Marxist" theories that have been developed by adherents of Marxism.
Marx's early writings portrayed the state as "parasitic", built upon the superstructure of the economy, and working against the public interest. He also wrote that the state mirrors class relations in society in general, acts as a regulator and repressor of class struggle, and acts as a tool of political power and domination for the ruling class. The "Communist Manifesto" claimed that the state is nothing more than "a committee for managing the common affairs of the "bourgeoisie".
For Marxist theorists, the role of the non-socialist state is determined by its function in the global capitalist order. Ralph Miliband argued that the ruling class uses the state as its instrument to dominate society by virtue of the interpersonal ties between state officials and economic elites. For Miliband, the state is dominated by an elite that comes from the same background as the capitalist class. State officials therefore share the same interests as owners of capital and are linked to them through a wide array of social, economic, and political ties.
Gramsci's theories of state emphasized that the state is only one of the institutions in society that helps maintain the hegemony of the ruling class, and that state power is bolstered by the ideological domination of the institutions of civil society, such as churches, schools, and mass media.
Pluralism.
Pluralists view society as a collection of individuals and groups, who are competing for political power. They then view the state as a neutral body that simply enacts the will of whichever groups dominate the electoral process. Within the pluralist tradition, Robert Dahl developed the theory of the state as a neutral arena for contending interests or its agencies as simply another set of interest groups. With power competitively arranged in society, state policy is a product of recurrent bargaining. Although pluralism recognizes the existence of inequality, it asserts that all groups have an opportunity to pressure the state. The pluralist approach suggests that the modern democratic state's actions are the result of pressures applied by a variety of organized interests. Dahl called this kind of state a polyarchy.
Pluralism has been challenged on the ground that it is not supported by empirical evidence. Citing surveys showing that the large majority of people in high leadership positions are members of the wealthy upper class, critics of pluralism claim that the state serves the interests of the upper class rather than equitably serving the interests of all social groups.
Contemporary critical perspectives.
Jürgen Habermas believed that the base-superstructure framework, used by many Marxist theorists to describe the relation between the state and the economy, was overly simplistic. He felt that the modern state plays a large role in structuring the economy, by regulating economic activity and being a large-scale economic consumer/producer, and through its redistributive welfare state activities. Because of the way these activities structure the economic framework, Habermas felt that the state cannot be looked at as passively responding to economic class interests.
Michel Foucault believed that modern political theory was too state-centric, saying "Maybe, after all, the state is no more than a composite reality and a mythologized abstraction, whose importance is a lot more limited than many of us think." He thought that political theory was focusing too much on abstract institutions, and not enough on the actual practices of government. In Foucault's opinion, the state had no essence. He believed that instead of trying to understand the activities of governments by analyzing the properties of the state (a reified abstraction), political theorists should be examining changes in the practice of government to understand changes in the nature of the state.
Heavily influenced by Gramsci, Nicos Poulantzas, a Greek neo-Marxist theorist argued that capitalist states do not always act on behalf of the ruling class, and when they do, it is not necessarily the case because state officials consciously strive to do so, but because the 'structural' position of the state is configured in such a way to ensure that the long-term interests of capital are always dominant. Poulantzas' main contribution to the Marxist literature on the state was the concept of 'relative autonomy' of the state. While Poulantzas' work on 'state autonomy' has served to sharpen and specify a great deal of Marxist literature on the state, his own framework came under criticism for its 'structural functionalism.'
State autonomy (institutionalism).
State autonomy theorists believe that the state is an entity that is impervious to external social and economic influence, and has interests of its own.
"New institutionalist" writings on the state, such as the works of Theda Skocpol, suggest that state actors are to an important degree autonomous. In other words, state personnel have interests of their own, which they can and do pursue independently of (at times in conflict with) actors in society. Since the state controls the means of coercion, and given the dependence of many groups in civil society on the state for achieving any goals they may espouse, state personnel can to some extent impose their own preferences on civil society.
G. William Domhoff claims that "The idea of the American state having any significant degree of autonomy from the owners and managers of banks, corporations, and agribusinesses is a theoretical mistake based in empirical inaccuracies," and cites empirical studies showing a high degree of overlap between upper-level corporate management and high-level positions in government.
Theories of state legitimacy.
States generally rely on a claim to some form of political legitimacy in order to maintain domination over their subjects.
Divine right.
The rise of the modern day state system was closely related to changes in political thought, especially concerning the changing understanding of legitimate state power and control. Early modern defenders of absolutism, such as Thomas Hobbes and Jean Bodin undermined the doctrine of the divine right of kings by arguing that the power of kings should be justified by reference to the people. Hobbes in particular went further to argue that political power should be justified with reference to the individual, not just to the people understood collectively. Both Hobbes and Bodin thought they were defending the power of kings, not advocating for democracy, but their arguments about the nature of sovereignty were fiercely resisted by more traditional defenders of the power of kings, such as Sir Robert Filmer in England, who thought that such defenses ultimately opened the way to more democratic claims.
Rational-legal authority.
Max Weber identified three main sources of political legitimacy in his works. The first, legitimacy based on traditional grounds is derived from a belief that things should be as they have been in the past, and that those who defend these traditions have a legitimate claim to power. The second, legitimacy based on charismatic leadership is devotion to a leader or group that is viewed as exceptionally heroic or virtuous. The third is rational-legal authority, whereby legitimacy is derived from the belief that a certain group has been placed in power in a legal manner, and that their actions are justifiable according to a specific code of written laws. Weber believed that the modern state is characterized primarily by appeals to rational-legal authority.
Etymology.
The word "state" and its cognates in some other European languages ("stato" in Italian, "estado" in Spanish, "état" in French, "Staat" in German) ultimately derive from the Latin word "status", meaning "condition" or "status."
With the revival of the Roman law in 14th-century Europe, this Latin term came to refer to the legal standing of persons (such as the various "estates of the realm" - noble, common, and clerical), and in particular the special status of the king. The word also had associations with Roman ideas (dating back to Cicero) about the "status rei publicae", the "condition of public matters". In time, the word lost its reference to particular social groups and became associated with the legal order of the entire society and the apparatus of its enforcement.
In English, "state" came about as a contraction of the word "estate", which is similar to the old French "estat" and the modern French "état", both of which signify that a person has status and therefore estate. The highest estates, generally those with the most wealth and social rank, were those that held power.
The early 16th-century works of Machiavelli (especially "The Prince") played a central role in popularizing the use of the word "state" in something similar to its modern sense.
History.
The earliest forms of the state emerged whenever it became possible to centralize power in a durable way. Agriculture and writing are almost everywhere associated with this process: agriculture because it allowed for the emergence of a class of people who did not have to spend most of their time providing for their own subsistence, and writing (or the equivalent of writing, like Inca quipus) because it made possible the centralization of vital information.
The first known states were created in Ancient Egypt, Mesopotamia, India, China, the Inca civilization, and others, but it is only in relatively modern times that states have almost completely displaced alternative "stateless" forms of political organization of societies all over the planet. Roving bands of hunter-gatherers and even fairly sizable and complex tribal societies based on herding or agriculture have existed without any full-time specialized state organization, and these "stateless" forms of political organization have in fact prevailed for all of the prehistory and much of the history of the human species and civilization.
Initially states emerged over territories built by conquest in which one culture, one set of ideals and one set of laws have been imposed by force or threat over diverse nations by a civilian and military bureaucracy. Currently, that is not always the case and there are multinational states, federated states and autonomous areas within states.
Since the late 19th century, virtually the entirety of the world's inhabitable land has been parcelled up into areas with more or less definite borders claimed by various states. Earlier, quite large land areas had been either unclaimed or uninhabited, or inhabited by nomadic peoples who were not organised as states. However, even within present-day states there are vast areas of wilderness, like the Amazon Rainforest, which are uninhabited or inhabited solely or mostly by indigenous people (and some of them remain uncontacted). Also, there are states which do not hold de facto control over all of their claimed territory or where this control is challenged. Currently the international community comprises around 200 sovereign states, the vast majority of which are represented in the United Nations.
Pre-historic stateless societies.
For most of human history, people have lived in stateless societies, characterized by a lack of concentrated authority, and the absence of large inequalities in economic and political power.
The anthropologist Tim Ingold writes:
It is not enough to observe, in a now rather dated anthropological idiom, that hunter gatherers live in 'stateless societies', as though their social lives were somehow lacking or unfinished, waiting to be completed by the evolutionary development of a state apparatus. Rather, the principal of their socialty, as Pierre Clastres has put it, is fundamentally "against" the state.
The Neolithic period.
During the Neolithic period, human societies underwent major cultural and economic changes, including the development of agriculture, the formation of sedentary societies and fixed settlements, increasing population densities, and the use of pottery and more complex tools.
Sedentary agriculture led to the development of property rights, domestication of plants and animals, and larger family sizes. It also provided the basis for the centralized state form by producing a large surplus of food, which created a more complex division of labor by enabling people to specialize in tasks other than food production. Early states were characterized by highly stratified societies, with a privileged and wealthy ruling class that was subordinate to a monarch. The ruling classes began to differentiate themselves through forms of architecture and other cultural practices that were different from those of the subordinate laboring classes.
In the past, it was suggested that the centralized state was developed to administer large public works systems (such as irrigation systems) and to regulate complex economies. However, modern archaeological and anthropological evidence does not support this thesis, pointing to the existence of several non-stratified and politically decentralized complex societies.
The state in ancient Eurasia.
Mesopotamia is generally considered to be the location of the earliest civilization or complex society, meaning that it contained cities, full-time division of labor, social concentration of wealth into capital, unequal distribution of wealth, ruling classes, community ties based on residency rather than kinship, long distance trade, monumental architecture, standardized forms of art and culture, writing, and mathematics and science. It was the world's first literate civilization, and formed the first sets of written laws.
The state in classical antiquity.
Although primitive state-forms existed before the rise of the Ancient Greek empire, the Greeks were the first people known to have explicitly formulated a political philosophy of the state, and to have rationally analyzed political institutions. Prior to this, states were described and justified in terms of religious myths.
Several important political innovations of classical antiquity came from the Greek city-states and the Roman Republic. The Greek city-states before the 4th century granted citizenship rights to their free population, and in Athens these rights were combined with a directly democratic form of government that was to have a long afterlife in political thought and history.
The Feudal state.
During Medieval times in Europe, the state was organized on the principle of feudalism, and the relationship between lord and vassal became central to social organization. Feudalism led to the development of greater social hierarchies.
The formalization of the struggles over taxation between the monarch and other elements of society (especially the nobility and the cities) gave rise to what is now called the Standestaat, or the state of Estates, characterized by parliaments in which key social groups negotiated with the king about legal and economic matters. These estates of the realm sometimes evolved in the direction of fully-fledged parliaments, but sometimes lost out in their struggles with the monarch, leading to greater centralization of lawmaking and military power in his hands. Beginning in the 15th century, this centralizing process gives rise to the absolutist state.
The modern state.
Cultural and national homogenization figured prominently in the rise of the modern state system. Since the absolutist period, states have largely been organized on a national basis. The concept of a national state, however, is not synonymous with nation state. Even in the most ethnically homogeneous societies there is not always a complete correspondence between state and nation, hence the active role often taken by the state to promote nationalism through emphasis on shared symbols and national identity.
References.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="28152" url="http://en.wikipedia.org/wiki?curid=28152" title="Stevia">
Stevia

Stevia (, or ) is a sweetener and sugar substitute extracted from the leaves of the plant species "Stevia rebaudiana".
The active compounds of stevia are steviol glycosides (mainly stevioside and rebaudioside), which have up to 150 times the sweetness of sugar, are heat-stable, pH-stable, and not fermentable. These steviosides have a negligible effect on blood glucose, which makes stevia attractive to people on carbohydrate-controlled diets. Stevia's taste has a slower onset and longer duration than that of sugar, and some of its extracts may have a bitter or licorice-like aftertaste at high concentrations.
The legal status of stevia extracts as food additives and supplements varies from country to country. In Japan, for example, stevia has been widely used for decades as a sweetener. In some other countries, health concerns and political controversies have led to various restrictions, or outright prohibition. The United States, for example, banned stevia in the early 1990s and approved some specific glycoside extracts for use as food additives in 2008. The European Union approved stevia additives in 2011.
History.
Discovery.
The plant "Stevia rebaudiana" has been used for more than 1,500 years by the Guaraní peoples of South America, who called it "ka'a he'ê" ("sweet herb"). The leaves have been used traditionally for hundreds of years in both Brazil and Paraguay to sweeten local teas and medicines, and as a "sweet treat". The genus was named for Spanish botanist and physician Petrus Jacobus Stevus (Pedro Jaime Esteve 1500–1556) a professor of botany at the University of Valencia.
In 1899, Swiss botanist Moisés Santiago Bertoni, while conducting research in eastern Paraguay, first described the plant and the sweet taste in detail. Only limited research was conducted on the topic until in 1931 two French chemists isolated the glycosides that give stevia its sweet taste.
The exact structure of the aglycone (steviol) and of the glycoside were published in 1955.
Commercial use.
In the early 1970s, sweeteners such as cyclamate and saccharin were suspected of being carcinogens. Consequently, Japan began cultivating stevia as an alternative. The plant's leaves, as well as the aqueous extract of the leaves and purified steviosides, were developed as sweeteners.
The first commercial stevia sweetener in Japan was produced by the Japanese firm Morita Kagaku Kogyo Co., Ltd. in 1971. The Japanese have been using stevia in food products and soft drinks, (including Coca Cola), and for table use. Japan currently consumes more stevia than any other country, with stevia accounting for 40% of the sweetener market.
In the mid 1980s, stevia began to become popular in U.S. natural foods and health food industries, as a non-caloric natural sweetener for teas and weight-loss blends. The makers of the synthetic sweetener NutraSweet asked the FDA to require testing of the herb.
In 2007 The Coca-Cola Company announced plans to obtain approval for its stevia-derived sweetener, Rebiana, for use as a food additive within the United States by 2009, as well as plans to market Rebiana-sweetened products in 12 countries that allow stevia's use as a food additive.
In May 2008, Coca Cola and Cargill announced the availability of Truvia, a consumer brand stevia sweetener containing erythritol and Rebiana, which the FDA permitted as a food additive in December 2008. Coca-Cola announced intentions to release stevia-sweetened beverages in late December 2008.
Shortly afterward, PepsiCo and Pure Circle announced PureVia, their brand of stevia-based sweetener, but withheld release of beverages sweetened with rebaudioside A until receipt of FDA confirmation. Since the FDA permitted Truvia and PureVia, both Coca Cola and PepsiCo have introduced products that contain their new sweeteners.
As of 2006, China was the world's largest exporter of stevioside products.
Industrial extracts and derivatives.
Stevia extracts and derivatives are produced industrially by many companies, and marketed under many trade names. Some of them are
Mechanism of action.
Glycosides are molecules that contain glucose and other non-sugar substances called aglycones (molecules with other sugars are polysaccharides). The tongue's taste receptors react to the glucose in the glycosides: those with more glucose (rebaudioside) taste sweeter than those with less (stevioside). Some of the tongue's bitter receptors react to the aglycones.
In the digestive tract, rebaudiosides are metabolised into stevioside. Then stevioside is broken down into glucose and steviol. The glucose released in this process is used by bacteria in the colon and not absorbed into the bloodstream. Steviol cannot be further digested and is excreted.
Potential health effects.
A 2009 review summarized the basic research in which steviosides and related compounds are being tested for possible anti-disease actions, with no effect yet demonstrated in humans. A 2011 review found that the use of stevia sweeteners as replacements for sugar might benefit diabetic patients because it is a "non-caloric" additive.
Safety and regulations.
Steviol and rebaudioside A are not mutagenic at doses and routes of administration at which humans are exposed to them. Two 2010 review studies found no health concerns with stevia or its sweetening extracts.
The WHO's Joint Experts Committee on Food Additives has approved, based on long-term studies, an acceptable daily intake of steviol glycoside of up to 4 milligrams per kilogram of body weight. In 2010, The European Food Safety Authority established an Acceptable Daily Intake (ADI) of 4 mg/kg/day of steviol, in the form of steviol glycosides.
Availability and legal status by country.
The plant may be grown legally in most countries, although some countries restrict its use as a sweetener. The legally allowed uses and maximum dosage of the extracts and derived products vary widely from country to country.
Extraction.
Rebaudioside A has the least bitterness of all the steviol glycosides in the "Stevia rebaudiana" plant. To produce rebaudioside A commercially, stevia plants are dried and subjected to a water extraction process. This crude extract contains about 50% rebaudioside A. The various glycosides are separated and purified via crystallization techniques, typically using ethanol or methanol as solvent.
The National Research Council of Canada has patented a process for extracting sweet compounds from stevia by column extraction at temperatures from 0 to 25 °C, followed by purification by nanofiltration. A microfiltration pretreatment step is used to clarify the extract. Purification is by ultrafiltration followed by nanofiltration.
Controversy.
In 1991, after receiving an anonymous industry complaint, the United States Food and Drug Administration (FDA) labeled stevia as an "unsafe food additive" and restricted its import. The FDA's stated reason was "toxicological information on stevia is inadequate to demonstrate its safety."
Since the import ban in 1991, marketers and consumers of stevia have shared a belief that the FDA acted in response to industry pressure. Arizona congressman Jon Kyl, for example, called the FDA action against stevia "a restraint of trade to benefit the artificial sweetener industry". To protect the complainant, the FDA deleted names in the original complaint in its responses to requests filed under the Freedom of Information Act.
Stevia remained banned until after the Dietary Supplement Health and Education Act of 1994 forced the FDA in 1995 to revise its stance to permit stevia to be used as a dietary supplement, although not as a food additive – a position that stevia proponents regarded as contradictory because it simultaneously labeled stevia as safe and unsafe, depending on how it was sold.
Early studies prompted the European Commission in 1999 to ban stevia's use in food in the European Union pending further research. In 2006, research data compiled in the safety evaluation released by the World Health Organization found no adverse effects. Since 2008, the Russian Federation has allowed stevioside as a food additive "in the minimal dosage required".
In December 2008, the FDA gave a "no objection" approval for GRAS status to Truvia (developed by Cargill and The Coca-Cola Company) and PureVia (developed by PepsiCo and the Whole Earth Sweetener Company, a subsidiary of Merisant), both of which use rebaudioside A derived from the Stevia plant. However, FDA said that these products are not Stevia, but a highly purified product. 
In 2012, FDA posted a note on its website regarding crude Stevia plant: "FDA has not permitted the use of whole-leaf Stevia or crude Stevia extracts because these substances have not been approved for use as a food additive. FDA does not consider their use in food to be GRAS in light of reports in the literature that raise concerns about the use of these substances. Among these concerns are control of blood sugar and effects on the reproductive, cardiovascular, and renal systems."

</doc>
<doc id="28153" url="http://en.wikipedia.org/wiki?curid=28153" title="Search for extraterrestrial intelligence">
Search for extraterrestrial intelligence

The search for extraterrestrial intelligence (SETI) is the collective name for a number of activities undertaken to search for intelligent extraterrestrial life. SETI projects use scientific methods in this search. For example, electromagnetic radiation is monitored for signs of transmissions from civilizations on other worlds. Some of the most well-known projects are run by Harvard University, the University of California, Berkeley, and the SETI Institute. In 1995, the United States federal government ceased funding to SETI projects, forcing them to turn to private funding to continue the search, though in recent years, government funding of SETI has resumed at modest levels.
There are great challenges in searching the cosmos for signs of intelligent life, including their identification and interpretation. SETI projects use the best available scientific knowledge to conduct experiments, which has traditionally led to searches for electromagnetic radiation emitted by advanced technologies. Radio telescopes are used to investigate the cosmos using large radio antennas. A 1959 paper explored the possibility of searching the microwave spectrum. In 1960, the first modern SETI experiment was done with a 26-meter radio telescope. The first SETI conference took place in 1961. Soviet scientists took a strong interest in SETI during the 1960s and performed a number of searches with omnidirectional antennas. In 1979 the University of California, Berkeley, launched a SETI project. In 1986, UC Berkeley initiated their second SETI effort. The university began an all-sky survey using the Arecibo radio telescope in March 2014.
In the early 1980s, physicist Paul Horowitz proposed the design of a spectrum analyzer to search for SETI transmissions on 131,000 narrow band channels. In 1985 Project "META" analyzed 8.4 million channels.The follow-on to META commenced observation on October 30, 1995. In 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; Funding was restored in 1982, after Carl Sagan intervened.
Founded in 1994 in response to the US Congress cancellation of the NASA SETI program, The SETI League, Inc. is a membership-supported nonprofit organization. It pioneered the conversion of backyard satellite TV dishes 3 to(-) in diameter into research-grade radio telescopes of modest sensitivity. SETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer distributed computing project that was launched by the University of California, Berkeley, in May 1999. The SETI Institute collaborated with the Radio Astronomy Laboratory at UC Berkeley to develop a specialized radio telescope array for SETI studies, something like a mini-cyclops array.
As various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". SETI has also occasionally been the target of criticism by those who suggest that it is a form of pseudoscience. In particular, critics allege that no observed phenomena suggest the existence of extraterrestrial intelligence, and furthermore that the assertion of the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability.
Radio experiments.
Many radio frequencies penetrate Earth's atmosphere quite well, and this led to radio telescopes that investigate the cosmos using large radio antennas. Furthermore, human endeavors emit considerable electromagnetic radiation as a byproduct of communications such as television and radio. These signals would be easy to recognize as artificial due to their repetitive nature and narrow bandwidths. If this is typical, one way of discovering an extraterrestrial civilization might be to detect non-natural radio emissions from a location outside the Solar System.
Early work.
As early as 1896, Nikola Tesla suggested that an extreme version of his wireless electrical transmission system could be used to contact beings on Mars. In 1899 while investigating atmospheric electricity using a Tesla coil receiver in his Knob Hill lab, Tesla observed repetitive signals, substantially different from the signals noted from storms and Earth noise, that he interpreted as being of extraterrestrial origin. He later recalled the signals appeared in groups of one, two, three, and four clicks together. Tesla thought the signals were coming from Mars. Analysis of Tesla's research has ranged from suggestions that Tesla detected nothing, he simply was misunderstanding the new technology he was working with, to claims that Tesla may have been observing naturally occurring Jovian plasma torus signals. In the early 1900s, Guglielmo Marconi, Lord Kelvin, and David Peck Todd also stated their belief that radio could be used to contact Martians, with Marconi stating that his stations had also picked up potential Martian signals.
On August 21–23, 1924, Mars entered an opposition closer to Earth than any time in a century before or the next 80 years. In the United States, a "National Radio Silence Day" was promoted during a 36-hour period from the 21–23, with all radios quiet for five minutes on the hour, every hour. At the United States Naval Observatory, a radio receiver was lifted 3 kilometers (2 miles) above the ground in a dirigible tuned to a wavelength between 8 and 9 kilometers (~5 miles), using a "radio-camera" developed by Amherst College and Charles Francis Jenkins. The program was led by David Peck Todd with the military assistance of Admiral Edward W. Eberle (Chief of Naval Operations), with William F. Friedman (chief cryptographer of the US Army), assigned to translate any potential Martian messages.
A 1959 paper by Philip Morrison and Giuseppe Cocconi first pointed out the possibility of searching the microwave spectrum, and proposed frequencies and a set of initial targets
In 1960, Cornell University astronomer Frank Drake performed the first modern SETI experiment, named "Project Ozma", after the Queen of Oz in L. Frank Baum's fantasy books. Drake used a radio telescope 26 meters in diameter at Green Bank, West Virginia, to examine the stars Tau Ceti and Epsilon Eridani near the 1.420 gigahertz marker frequency, a region of the radio spectrum dubbed the "water hole" due to its proximity to the hydrogen and hydroxyl radical spectral lines. A 400 kilohertz band was scanned around the marker frequency, using a single-channel receiver with a bandwidth of 100 hertz. The information was stored on tape for off-line analysis. He found nothing of great interest, but has continued a pro-active involvement in the search for life beyond Earth for 50 years.
The first SETI conference took place at Green Bank, West Virginia in November 1961. The ten attendees were conference organiser Peter Pearman, Frank Drake, Philip Morrison, businessman and radio amateur Dana Atchley, chemist Melvin Calvin, astronomer Su-Shu Huang, neuroscientist John C. Lilly, inventor Barney Oliver, astronomer Carl Sagan and radio astronomer Otto Struve. From the agenda points of the conference Drake derived the Drake equation by multiplying the various factors that were discussed at the conference. The Drake equation is an estimation of how many planets in the Milky Way are inhabited by intelligent life forms.
The Soviet scientists took a strong interest in SETI during the 1960s and performed a number of searches with omnidirectional antennas in the hope of picking up powerful radio signals. Soviet astronomer Iosif Shklovskii wrote the pioneering book in the field "Universe, Life, Intelligence" (1962), which was expanded upon by American astronomer Carl Sagan as the best-selling "Intelligent Life in the Universe" (1966).
The first Kraus-style radio telescope was powered up in 1963. It was 360 ft wide, 500 ft long and 70 ft high (110 m × 150 m × 21 m). In the March 1955 issue of "Scientific American", John D. Kraus described a concept to scan the cosmos for natural radio signals using a flat-plane radio telescope equipped with a parabolic reflector. Within two years, his concept was approved for construction by Ohio State University. With US$ total in grants from the National Science Foundation, construction began on a 20 acre plot in Delaware, Ohio. This Ohio State University radio telescope was called Big Ear. Later, it began the world's first continuous SETI program, called the Ohio State University SETI program.
In 1971, NASA funded a SETI study that involved Drake, Bernard Oliver of Hewlett-Packard Corporation, and others. The resulting report proposed the construction of an Earth-based radio telescope array with 1,500 dishes known as "Project Cyclops". The price tag for the Cyclops array was US$. Cyclops was not built, but the report formed the basis of much SETI work that followed.
The OSU SETI program gained fame on August 15, 1977, when Jerry Ehman, a project volunteer, witnessed a startlingly strong signal received by the telescope. He quickly circled the indication on a printout and scribbled the exclamation "Wow!" in the margin. Dubbed the "Wow! signal", it is considered by some to be the best candidate for a radio signal from an artificial, extraterrestrial source ever discovered, but it has not been detected again in several additional searches.
In 1979 the University of California, Berkeley, launched a SETI project named "Search for Extraterrestrial Radio Emissions from Nearby Developed Intelligent Populations (SERENDIP)". In 1986, UC Berkeley initiated their second SETI effort, SERENDIP II, and has continued with four more SERENDIP efforts to the present day. The latest incarnation of the SERENDIP project is SERENDIP VI, a commensal all-sky survey using the Arecibo radio telescope began in March 2014.
Sentinel, META, and BETA.
In 1980, Carl Sagan, Bruce Murray, and Louis Friedman founded the U.S. Planetary Society, partly as a vehicle for SETI studies.
In the early 1980s, Harvard University physicist Paul Horowitz took the next step and proposed the design of a spectrum analyzer specifically intended to search for SETI transmissions. Traditional desktop spectrum analyzers were of little use for this job, as they sampled frequencies using banks of analog filters and so were restricted in the number of channels they could acquire. However, modern integrated-circuit digital signal processing (DSP) technology could be used to build autocorrelation receivers to check far more channels. This work led in 1981 to a portable spectrum analyzer named "Suitcase SETI" that had a capacity of 131,000 narrow band channels. After field tests that lasted into 1982, Suitcase SETI was put into use in 1983 with the 26 m Harvard/Smithsonian radio telescope at Oak Ridge Observatory in Harvard, Massachusetts. This project was named "Sentinel" and continued into 1985.
Even 131,000 channels were not enough to search the sky in detail at a fast rate, so Suitcase SETI was followed in 1985 by Project "META", for "Megachannel Extra-Terrestrial Assay". The META spectrum analyzer had a capacity of 8.4 million channels and a channel resolution of 0.05 hertz. An important feature of META was its use of frequency doppler shift to distinguish between signals of terrestrial and extraterrestrial origin. The project was led by Horowitz with the help of the Planetary Society, and was partly funded by movie maker Steven Spielberg. A second such effort, META II, was begun in Argentina in 1990, to search the southern sky. META II is still in operation, after an equipment upgrade in 1996.
The follow-on to META was named "BETA", for "Billion-channel Extraterrestrial Assay", and it commenced observation on October 30, 1995. The heart of BETA's processing capability consisted of 63 dedicated fast Fourier transform (FFT) engines, each capable of performing a 222-point complex FFTs in two seconds, and 21 general-purpose personal computers equipped with custom digital signal processing boards. This allowed BETA to receive 250 million simultaneous channels with a resolution of 0.5 hertz per channel. It scanned through the microwave spectrum from 1.400 to 1.720 gigahertz in eight hops, with two seconds of observation per hop. An important capability of the BETA search was rapid and automatic re-observation of candidate signals, achieved by observing the sky with two adjacent beams, one slightly to the east and the other slightly to the west. A successful candidate signal would first transit the east beam, and then the west beam and do so with a speed consistent with Earth's sidereal rotation rate. A third receiver observed the horizon to veto signals of obvious terrestrial origin. On March 23, 1999, the 26-meter radio telescope on which Sentinel, META and BETA were based was blown over by strong winds and seriously damaged. This forced the BETA project to cease operation.
MOP and Project Phoenix.
In 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; however, funding was restored in 1982, after Carl Sagan talked with Proxmire and convinced him of the program's value. In 1992, the U.S. government funded an operational SETI program, in the form of the NASA Microwave Observing Program (MOP). MOP was planned as a long-term effort to conduct a general survey of the sky and also carry out targeted searches of 800 specific nearby stars. MOP was to be performed by radio antennas associated with the NASA Deep Space Network, as well as the 140 ft radio telescope of the National Radio Astronomy Observatory at Green Bank, West Virginia and the 1000 ft radio telescope at the Arecibo Observatory in Puerto Rico. The signals were to be analyzed by spectrum analyzers, each with a capacity of 15 million channels. These spectrum analyzers could be grouped together to obtain greater capacity. Those used in the targeted search had a bandwidth of 1 hertz per channel, while those used in the sky survey had a bandwidth of 30 hertz per channel.
MOP drew the attention of the U.S. Congress, where the program was ridiculed and canceled a year after its start. SETI advocates continued without government funding, and in 1995 the nonprofit SETI Institute of Mountain View, California resurrected the MOP program under the name of Project "Phoenix", backed by private sources of funding. Project Phoenix, under the direction of Jill Tarter, is a continuation of the targeted search program from MOP and studies roughly 1,000 nearby Sun-like stars. From 1995 through March 2004, Phoenix conducted observations at the 64 m Parkes radio telescope in Australia, the 140 ft radio telescope of the National Radio Astronomy Observatory in Green Bank, West Virginia, and the 1000 ft radio telescope at the Arecibo Observatory in Puerto Rico. The project observed the equivalent of 800 stars over the available channels in the frequency range from 1200 to 3000 MHz. The search was sensitive enough to pick up transmitters with 1 GW EIRP to a distance of about 200 light years. According to Prof. Tarter, in 2012 it costs around "$2 million a year to keep SETI research going at the SETI Institute" and approximately 10 times that to support "all kinds of SETI activity around the world."
The SETI League and Project Argus.
Founded in 1994 in response to the US Congress cancellation of the NASA SETI program, The SETI League, Inc. is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world's first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.
The SETI League pioneered the conversion of backyard satellite TV dishes 3 to(-) in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute's Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10−23 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.
The name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, "Imperial Earth"; Carl Sagan, "Contact"), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.
SETI@home.
SETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer distributed computing project that was launched by the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual can become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself runs signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit is complete, the results are then automatically reported back to SETI@home servers at UC Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers give SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a was an interesting signal but was quickly shown to have a natural source.
As of 2010, after 10 years of data collection, SETI@home has listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least 3 scans (out of the goal of 9 scans), which covers about 20 percent of the full celestial sphere.
Allen Telescope Array.
The SETI Institute collaborated with the Radio Astronomy Laboratory at UC Berkeley to develop a specialized radio telescope array for SETI studies, something like a mini-cyclops array. Formerly known as the One Hectare Telescope (1HT), the concept was renamed the "Allen Telescope Array" (ATA) after the project's benefactor Paul Allen. Its sensitivity would be equivalent to a single large dish more than 100 meters in diameter if completed. Presently, the array under construction has 42 dishes at the Hat Creek Observatory in rural northern California.
The full array (ATA-350) is planned to consist of 350 or more offset-Gregorian radio dishes, each 6.1 m in diameter. These dishes are the largest producible with commercially available satellite television dish technology. The ATA was planned for a 2007 completion date, at a very modest cost of US$. The SETI Institute provided money for building the ATA while UC Berkeley designed the telescope and provided operational funding. The first portion of the array (ATA-42) became operational in October 2007 with 42 antennas. The DSP system planned for ATA-350 is extremely ambitious. Completion of the full 350 element array will depend on funding and the technical results from ATA-42.
ATA-42 (henceforth, ATA) is designed to allow multiple observers simultaneous access to the interferometer output at the same time. Typically, the ATA snapshot imager (used for astronomical surveys and SETI) is run in parallel with the beamforming system (used primarily for SETI).
ATA also supports observations in multiple synthesized pencil beams at once, through a technique known as "multibeaming." Multibeaming provides an effective filter for identifying false positives in SETI, since a very distant transmitter must appear at only one point on the sky.
SETI Institute's Center for SETI Research (CSR) uses ATA in the search for extraterrestrial intelligence, observing 12 hours a day, 7 days a week. From 2007-2015, ATA has identified hundreds of millions of technological signals. So far, all these signals have been assigned the status of noise or radio frequency interference because a) they appear to be generated by satellites or Earth-based transmitters, or b) they disappeared before the threshold time limit of ~1 hour. Researchers in CSR are presently working on ways to reduce the threshold time limit, and to expand ATA's capabilities for detection of signals that may have embedded messages.
Berkeley astronomers used the ATA to pursue several science topics, some of which might have turned up transient SETI signals, until 2011, when the collaboration between the University of California and the SETI Institute was terminated. 
The DSP system planned for the ATA is extremely ambitious. The first portion of the array became operational in October 2007 with 42 antennas. Completion of the full 350 element array will depend on funding and the technical results from the 42-element sub-array.
CNET published an article and pictures about the Allen Telescope Array (ATA) on December 12, 2008.
In April 2011, the ATA was forced to enter an 8-month "hibernation" due to funding shortfalls. Regular operation of the ATA was resumed on December 5, 2011.
In 2012, new life was breathed into the ATA thanks to a $3.6M philanthropic donation by Franklin Antonio, Co-Founder and Chief Scientist of QUALCOMM Incorporated. This gift supports upgrades of all the receivers on the ATA dishes to have dramatically (2x - 10x from 1–8 GHz) greater sensitivity than before and supporting sensitive observations over a wider frequency range from 1–18 GHz, though initially the radio frequency electronics go to only 12 GHz. As of July, 2013 the first of these receivers was installed and proven. Full installation on all 42 antennas is expected in June, 2014. ATA is especially well suited to the search for extraterrestrial intelligence SETI and to discovery of astronomical radio sources, such as heretofore unexplained non-repeating, possibly extragalactic, pulses known as fast radio bursts or FRBs.
SETI Net.
SETI Net is a private search system created by a single individual. It is closely affiliated with the SETI League and is one of the project Argus stations (DM12jw).
The SETI Net station consists of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It has a 3-meter parabolic antenna that can be directed in azimuth and elevation, an LNA that covers the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard PC as the control device and for deploying the detection algorithms.
The antenna can be pointed and locked to one sky location, enabling the system to integrate on it for long periods. Currently the Wow! signal area is being monitored when it is above the horizon, but all search data are collected and made available on the internet archive.
SETI Net started operation in the early 1980s as a way to learn about the science of the search, and has developed several software packages for the amateur SETI community. It has provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the internet, and other packages.
Realized interstellar radio message projects.
In November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.
Paper projects.
A large number of paper projects also exist. For example, the Interstellar Message Composition Project, directed by Douglas Vakoch of the SETI Institute, is charged with designing messages that could presumably be sent to extraterrestrials that convey basic scientific or mathematical principles, as well as human altruism. Vackoch's idea is to send a message of reciprocal altruism because hopefully any extraterrestrials would reciprocate with a reply back.
Vakoch has founded "Encoding Altruism", a workshop that started in 2003 in Paris that brings together anthropologists, philosophers, physicists, astronomers, theologians, musicians, and artists to address the challenge of communicating with extraterrestrials in a language and syntax that would be intelligible to an alien civilization. Vakoch's most recent research is highlighted through the Greater Good Science Center, University of California, Berkeley.
Optical experiments.
While most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes, one of the inventors of the laser, in a 1961 paper published in the journal "Nature" titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the US journal "Proceedings of the National Academy of Sciences", which was met with widespread agreement by the SETI community.
There are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, according to the uncertainty principle, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.
The other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. This means that a laser beam could be easily blocked by clouds of interstellar dust, and Earth would have to cross its direct line of fire by chance to receive it.
Optical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam's line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.
Such a system could be made to automatically steer itself through a target list, sending a pulse to each target at a constant rate. This would allow targeting of all Sun-like stars within a distance of 100 light-years. The studies have also described an automatic laser pulse detector system with a low-cost, two-meter mirror made of carbon composite materials, focusing on an array of light detectors. This automatic detector system could perform sky surveys to detect laser flashes from civilizations attempting contact.
In the 1980s, two Soviet researchers conducted a short optical SETI search, but turned up nothing. During much of the 1990s, the optical SETI cause was kept alive through searches by Stuart Kingsley, a dedicated British researcher living in the US state of Ohio.
Several optical SETI experiments are now in progress. A Harvard-Smithsonian group that includes Paul Horowitz designed a laser detector and mounted it on Harvard's 155 cm optical telescope. This telescope is currently being used for a more conventional star survey, and the optical SETI survey is "piggybacking" on that effort. Between October 1998 and November 1999, the survey inspected about 2,500 stars. Nothing that resembled an intentional laser signal was detected, but efforts continue. The Harvard-Smithsonian group is now working with Princeton University to mount a similar detector system on Princeton's 91-centimeter (36-inch) telescope. The Harvard and Princeton telescopes will be "ganged" to track the same targets at the same time, with the intent being to detect the same signal in both locations as a means of reducing errors from detector noise.
The Harvard-Smithsonian group is now building a dedicated all-sky optical survey system along the lines of that described above, featuring a 1.8-meter (72-inch) telescope. The new optical SETI survey telescope is being set up at the Oak Ridge Observatory in Harvard, Massachusetts.
The University of California, Berkeley, home of SERENDIP and SETI@home, is also conducting optical SETI searches. One is being directed by Geoffrey Marcy, an extrasolar planet hunter, and involves examination of records of spectra taken during extrasolar planet hunts for a continuous, rather than pulsed, laser signal. The other Berkeley optical SETI effort is more like that being pursued by the Harvard-Smithsonian group and is being directed by Dan Werthimer of Berkeley, who built the laser detector for the Harvard-Smithsonian group. The Berkeley survey uses a 76-centimeter (30-inch) automated telescope at Leuschner Observatory and an older laser detector built by Werthimer.
The 74m Colossus Telescope is designed to detect optical and thermal signatures of extraterrestrial
civilizations from planetary systems within 60 light years from the Sun.
Gamma-ray bursts.
Gamma-ray bursts (GRBs) are candidates for extraterrestrial communication. These high-energy bursts are observed about once per day and originate throughout the observable universe. SETI currently omits gamma ray frequencies in their monitoring and analysis because they are absorbed by the Earth's atmosphere and difficult to detect with ground-based receivers. In addition, the wide burst bandwidths pose a serious analysis challenge for modern digital signal processing systems. Still, the continued mysteries surrounding gamma-ray bursts have encouraged hypotheses invoking extraterrestrials. John A. Ball from the MIT Haystack Observatory suggests that an advanced civilization that has reached a technological singularity would be capable of transmitting a two-millisecond pulse encoding bits of information. This is "comparable to the estimated total information content of Earth's biosystem—genes and memes and including all libraries and computer media."
Search for extraterrestrial artifacts.
The possibility of using interstellar messenger probes in the search for extraterrestrial intelligence was first suggested by Ronald N. Bracewell in 1960 (see Bracewell probe), and the technical feasibility of this approach was demonstrated by the British Interplanetary Society's starship study Project Daedalus in 1978. Starting in 1979, Robert Freitas advanced arguments for the proposition that physical space-probes are a superior mode of interstellar communication to radio signals. See Voyager Golden Record.
In recognition that any sufficiently advanced interstellar probe in the vicinity of Earth could easily monitor the terrestrial Internet, Invitation to ETI was established by Prof. Allen Tough in 1996, as a Web-based SETI experiment inviting such spacefaring probes to establish contact with humanity. The project's 100 Signatories includes prominent physical, biological, and social scientists, as well as artists, educators, entertainers, philosophers and futurists. Prof. H. Paul Shuch, executive director emeritus of The SETI League, serves as the project's Principal Investigator.
In a 2004 paper, C. Rose and G. Wright showed that inscribing a message in matter and transporting it to an interstellar destination can be enormously more energy efficient than communication using electromagnetic waves if delays larger than light transit time can be tolerated. That said, for simple messages such as "hello," radio SETI could be far more efficient. If energy requirement is used as a proxy for technical difficulty, then a solarcentric Search for Extraterrestrial Artifacts (SETA) may be a useful supplement to traditional radio or optical searches.
Much like the "preferred frequency" concept in SETI radio beacon theory, the Earth-Moon or Sun-Earth libration orbits might therefore constitute the most universally convenient parking places for automated extraterrestrial spacecraft exploring arbitrary stellar systems. A viable long-term SETI program may be founded upon a search for these objects.
In 1979, Freitas and Valdes conducted a photographic search of the vicinity of the Earth-Moon triangular libration points L4 and L5, and of the solar-synchronized positions in the associated halo orbits, seeking possible orbiting extraterrestrial interstellar probes, but found nothing to a detection limit of about 14th magnitude. The authors conducted a second, more comprehensive photographic search for probes in 1982 that examined the five Earth-Moon Lagrangian positions and included the solar-synchronized positions in the stable L4/L5 libration orbits, the potentially stable nonplanar orbits near L1/L2, Earth-Moon L3, and also L2 in the Sun-Earth system. Again no extraterrestrial probes were found to limiting magnitudes of 17–19th magnitude near L3/L4/L5, 10–18th magnitude for L1/L2, and 14–16th magnitude for Sun-Earth L2.
In June 1983, Valdes and Freitas used the 26 m radiotelescope at Hat Creek Radio Observatory to search for the tritium hyperfine line at 1516 MHz from 108 assorted astronomical objects, with emphasis on 53 nearby stars including all visible stars within a 20 light-year radius. The tritium frequency was deemed highly attractive for SETI work because (1) the isotope is cosmically rare, (2) the tritium hyperfine line is centered in the SETI waterhole region of the terrestrial microwave window, and (3) in addition to beacon signals, tritium hyperfine emission may occur as a byproduct of extensive nuclear fusion energy production by extraterrestrial civilizations. The wideband- and narrowband-channel observations achieved sensitivities of 5–14 x 10−21 W/m²/channel and 0.7-2 x 10−24 W/m²/channel, respectively, but no detections were made.
Technosignatures.
Technosignatures, including all signs of technology with the exception of the interstellar radio messages that define traditional SETI, are a recent avenue in the search for extraterrestrial intelligence. Technosignatures may originate from various sources, such as Dyson spheres, city lights on extrasolar planets, or the atmospheric contamination created by an industrial civilization, and may be detectable in the future with large hypertelescopes.
Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System. An astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star. After examining some 100,000 nearby large galaxies a team of researchers has concluded that none of them contain any obvious signs of highly advanced technological civilizations.
Another form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star's light back on itself, and can be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.
Individual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Harvard-Smithsonian Center for Astrophysics has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.
A more recent approach proposed by scientists Jeff Kuhn of University of Hawaii and Svetlana Berdyugina of Kiepenheuer-Institut für Sonnenphysik, David Halliday (engineer) of Dynamic Structures and amateur astronomer Caisey Harlingten of Searchlight Observatory focuses on detecting artificial infrared radiation emission through hypertelescopes such as "Colossus", a 76 m, US$ project proposed in 2013.
Clearly, light and heat detected from planets are to be distinguished from natural sources to conclusively prove the existence of civilization on a planet.
However, as argued by the Colossus team,
a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e. only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.
Extraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft are detectable over thousands of light-years of distance through the synchrotron radiation they produce through interaction with the interstellar medium; other interstellar spacecraft designs can be detected at more modest distances. In addition, robotic probes within the Solar System are also being sought out with optical and radio searches.
Fermi paradox.
Italian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. (According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?")
The Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".
The Fermi paradox can be stated more completely as follows:
The size and age of the universe incline us to believe that many technologically advanced civilizations must exist. However, this belief seems logically inconsistent with our lack of observational evidence to support it. Either (1) the initial assumption is incorrect and technologically advanced intelligent life is much rarer than we believe, or (2) our current observations are incomplete and we simply have not detected them yet, or (3) our search methodologies are flawed and we are not searching for the correct indicators.
There are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate, or would not travel across interstellar distances.
Science writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.
Although somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).
A significant problem is the vastness of space. Despite piggybacking on the world's most sensitive radio telescope, Charles Stuart Bowyer said, the instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light years.
Post detection disclosure protocol.
The International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC) held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (Chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin." It will use, in part, the Rio Scale to evaluate the importance of releasing the information to the public.
When awarded the 2009 TED Prize SETI Institute's Jill Tarter outlined the organisation's "post detection protocol". During NASA's funding of the project, an administrator would be first informed with the intention of informing the United States executive government. The current protocol for SETI Institute is to first internally investigate the signal, seeking independent verification and confirmation. During the process, the organisation's private financiers would be secretly informed. Once a signal has been verified, a telegram would be sent via the Central Bureau for Astronomical Telegrams. Following this process, Tarter says that the organisation will hold a press conference with the aim of broadcasting to the public. SETI Institute's Seth Shostak has claimed that knowledge of the discovery would likely leak as early as the verification process.
However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".
The SETI Institute does not officially recognise the Wow! signal as of extraterrestrial origin (as it was unable to be verified). The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin though full details of the signal, such as its exact location have never been disclosed to the public. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.
Some people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world's religions.
Criticism.
As various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, has written that
Clive Trotman presents some sobering but realistic calculations emphasizing the timeframe dimension.
SETI has also occasionally been the target of criticism by those who suggest that it is a form of pseudoscience. In particular, critics allege that no observed phenomena suggest the existence of extraterrestrial intelligence, and furthermore that the assertion of the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability.
In response, SETI advocates note, among other things, that the Drake Equation was never a hypothesis, and so never intended to be testable, nor to be "solved"; it was merely a clever representation of the agenda for the world's first scientific SETI meeting in 1961, and it serves as a tool in formulating testable hypotheses. Further, they note that the existence of intelligent life on Earth is a plausible reason to expect it elsewhere, and that individual SETI projects have clearly defined "stop" conditions.
The "search" for extraterrestrial intelligence is not an assertion that extraterrestrial intelligence exists or that intelligent extraterrestrials are visiting Earth, and conflating the two can be seen as a straw man argument. There is an effort to distinguish the SETI projects from UFOlogy, the study of UFOs, which many consider to be pseudoscience. In "Skeptical Inquirer", Mark Moldwin argued that the important differences between the two projects were the acceptance of SETI by the mainstream scientific community and that "[t]he methodology of SETI leads to useful scientific results even in the absence of discovery of alien life."
Some in the UFO community, such as nuclear physicist Stanton Friedman, are highly critical of the search and say it is unscientific. Friedman has written that "if aliens are indeed visiting, then the Radio Telescope Search for ET signals would seem a useless exercise and might indicate the SS [SETI specialists] have been on the wrong track all along". He has challenged SETI scientists to debate the issues, with no takers so far. Examples of objections to SETI include questioning energy requirements as well as why advanced civilizations would use radio.
Active SETI.
Active SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be picked up by an alien intelligence. Physicist Stephen Hawking, in his book "A Brief History of Time", suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing mankind's history of treating his fellow man harshly in meetings of civilizations with a significant technology gap. He suggests, in view of this history, that we "lay low".
The concern over METI was raised by the science journal "Nature" in an editorial in October 2006, which commented on a recent meeting of the International Academy of Astronautics SETI study group. The editor said, "It is not obvious that all extraterrestrial civilizations will be benign, or that contact with even a benign one would not have serious repercussions" (Nature Vol 443 12 October 06 p 606). Astronomer and science fiction author David Brin has expressed similar concerns.
Richard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".
To lend a quantitative basis to discussions of the risks of transmitting deliberate messages from Earth, the SETI Permanent Study Group of the International Academy of Astronautics adopted in 2007 a new analytical tool, the San Marino Scale. Developed by Prof. Ivan Almar and Prof. H. Paul Shuch, the scale evaluates the significance of transmissions from Earth as a function of signal intensity and information content. Its adoption suggests that not all such transmissions are equal, and each must be evaluated separately before establishing blanket international policy regarding active SETI.
However, some scientists consider these fears about the dangers of METI as panic and irrational superstition; see, for example, Alexander L. Zaitsev's papers.
On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a "worldwide scientific, political and humanitarian discussion must occur before any message is sent". On 28 March 2015, a related essay was written by Seth Shostak and published in the New York Times.

</doc>
<doc id="28154" url="http://en.wikipedia.org/wiki?curid=28154" title="Sextans">
Sextans

Sextans is a minor equatorial constellation which was introduced in 1687 by Johannes Hevelius. Its name is Latin for the astronomical sextant, an instrument that Hevelius made frequent use of in his observations.
Notable features.
Sextans as a constellation covers a rather dim, sparse region of the sky. It has only one star above the fifth magnitude, namely α Sextantis at 4.49m. The constellation contains a few double stars, including γ, 35, and 40 Sextantis. There are a few notable variable stars, including β, 25, 23 Sextantis, and LHS 292. NGC 3115, an edge-on lenticular galaxy, is the only noteworthy deep-sky object. It also lies near the ecliptic, which causes the Moon, and some of the planets to occasionally pass through it for brief periods of time. 
The constellation is the location of the field studied by the COSMOS project, undertaken by the Hubble Space Telescope.
Sextans B is a fairly bright dwarf irregular galaxy at magnitude 6.6, 4.3 million light-years from Earth. It is part of the Local Group of galaxies.
External links.
Coordinates: 

</doc>
<doc id="28156" url="http://en.wikipedia.org/wiki?curid=28156" title="Salem al-Hazmi">
Salem al-Hazmi

Salem al-Hazmi (Arabic: سالم الحازمي‎, "Sālam al-Ḥāzmī", also transliterated as Alhazmi) (February 2, 1981 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks. 
A Saudi, Hazmi had a relatively long history with al-Qaeda before being selected for the attacks. He obtained a tourist visa through the Visa Express program and arrived in the United States in June 2001 where he would settle in New Jersey with other American 77 hijackers up until the attacks. 
On September 11, 2001, Hazmi boarded American Airlines Flight 77 and helped subdue the passengers and crew for Hani Hanjour, the pilot among the hijackers, to crash the plane into west facade of the Pentagon. His older brother, Nawaf al-Hazmi, was another hijacker aboard the same flight. At the age of 20 years and 221 days he was the youngest hijacker who participated in the attacks.
History.
Hazmi was born on February 2, 1981 to Muhammad Salim al-Hazmi, a grocer, in Mecca, Saudi Arabia. His father described Salem as a quarrelsome teenager who had problems with alcohol and petty theft. However, he stopped drinking and began to attend the mosque about three months before he left his family.
There are reports that he fought in Afghanistan with his brother, Nawaf al-Hazmi, and other reports say the two fought together in Chechnya. Salem al-Hazmi was an al-Qaeda veteran by the time he was selected for participation in the 9/11 attacks. U.S intelligence learned of Hazmi's involvement with al-Qaeda as early as 1999, but he was not placed on any watchlists. 
Known as "Bilal" during the preparations, both he and Ahmed al-Ghamdi flew to Beirut in November 2000, though on separate flights.
Along with Nawaf al-Hazmi and several other future hijackers, Salem al-Hazmi may have attended the 2000 Al Qaeda Summit in Kuala Lumpur, Malaysia. It was there that the details of the 9/11 attacks were decided upon.
In the United States.
According to the FBI and the 9/11 Commission report, Hazmi first entered the United States on June 29, 2001, although there are numerous unconfirmed reports that he was living in San Antonio, Texas with fellow hijacker Satam al-Suqami much earlier. Hazmi used the controversial Visa Express program to gain entry into the country. 
Hazmi moved to Paterson, New Jersey where he lived with Hani Hanjour. Both were among the five hijackers who applied for Virginia identity cards at the Arlington office of the Virginia Department of Motor Vehicles on August 2, 2001, although Salem already held an NJ identity card.
On August 27, brothers Nawaf and Salem purchased flight tickets through Travelocity.com using Nawaf's visa card.
With the four other Flight 77 hijackers, he worked out at a Gold's Gym in Greenbelt, Maryland from September 2 to September 6 of the same year.
Attacks.
On September 11, 2001, Hazmi boarded American Airlines Flight 77. Airport surveillance video from Washington's Dulles Airport shows two of the five hijackers, including Salem al-Hazmi, being pulled aside to undergo additional scrutiny after setting off metal detectors.
The flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI. Forensics teams confirmed that it seemed two of the hijackers were brothers, based on their DNA similarities.
Mistaken identity.
Shortly after the attacks, several sources reported that Salem al-Hazmi, 26, was alive and working at a petrochemical plant in Yanbu, Saudi Arabia. He claimed that his passport had been stolen by a pickpocket in Cairo three years before, and that the pictures and details such as date of birth released to the public by the FBI were his own. He also stated that he had never visited the United States, but volunteered to fly to the U.S. to prove his innocence. On September 19, "Al-Sharq Al-Awsat" published his photograph alongside Badr Alhazmi's, whom they claimed was the actual hijacker who had stolen his identity.
Muhammad Salim al-Hazmi, father of the two suspects, Nawaf and Salim Muhammad al-Hazmi, said that the published photos may be doctored or faked somehow. Hazmi continued, "As a father, I have a feeling that the two of them are still alive and unhurt, and will come back home in the near future when the truth is uncovered and the real culprits are found."
After some confusion and doubt Saudi Arabia admitted that in fact the names of the hijackers were correct. "The names that we got confirmed that," Interior Minister Prince Nayef said in an interview with The Associated Press. "Their families have been notified." Nayef said the Saudi leadership was shocked to learn 15 of the hijackers were from Saudi Arabia and said it was natural that the kingdom had not noticed their involvement beforehand.

</doc>
<doc id="28157" url="http://en.wikipedia.org/wiki?curid=28157" title="Satsuma Province">
Satsuma Province

Satsuma Province (薩摩国, Satsuma-no Kuni) was an old province of Japan that is now the western half of Kagoshima Prefecture on the island of Kyūshū. Its abbreviation is Sasshū (薩州).
History.
Satsuma's provincial capital was Satsumasendai. During the Sengoku Period, Satsuma was a fief of the Shimazu daimyo, who ruled much of southern Kyūshū from their castle at Kagoshima city.
In 1871, with the abolition of feudal domains and the establishment of prefectures after the Meiji Restoration, the provinces of Satsuma and Ōsumi were combined to eventually establish Kagoshima Prefecture.
Satsuma was one of the main provinces that rose in opposition to the Tokugawa Shogunate in the mid 19th century. Because of this, the oligarchy that came into power after the "Meiji Restoration" of 1868 had a strong representation from the Satsuma province, with leaders such as Ōkubo Toshimichi and Saigō Takamori taking up key government positions.
Satsuma is well known for its production of sweet potatoes, known in Japan as 薩摩芋 (satsuma-imo or "Satsuma potato"). On the other hand, Satsuma mandarins (known as "mikan" in Japan) do not specifically originate from Satsuma but were imported into the West through this province in the Meiji era.
Other websites.
 Media related to at Wikimedia Commons

</doc>
<doc id="28159" url="http://en.wikipedia.org/wiki?curid=28159" title="Scottish">
Scottish

Scottish usually refers to something of, from, or related to Scotland, including:

</doc>
<doc id="28161" url="http://en.wikipedia.org/wiki?curid=28161" title="List of brightest stars">
List of brightest stars

This is a list of the brightest individual stars determined by their "average" apparent magnitudes in the visible spectrum as seen from Earth. This is not the same as a list of the brightest stars as seen with the naked eye, as close binary or multiple star systems will appear as a single star with an apparent magnitude greater than their individual components, e.g. the binary system Rigel Kentaurus has an apparent magnitude as listed here of –0.27, but the brightest individual star is Alpha Centauri A with the apparent magnitude of –0.01. Hence, Alpha Centauri is the third brightest star in the night sky, while its brightest component Alpha Centauri A is the fourth brightest individual star. 
Stellar brightness in the main table is limited to brighter than +2.50 magnitude, as the number of observable stars increases exponentially as the magnitude increases. To the naked eye on a clear dark night, in a location far from cities and lights, the total number of stars visible is around 6,500. Telescopically, stars have been mapped, photographed and catalogued almost completely down to the 11th magnitude, and recent star surveys are continuing to catalogue even fainter stars.
List.
Below are listed the 93 brightest individual stars in order of their average apparent magnitudes.
For comparison, the non-stellar objects in our Solar System with maximum visible magnitudes below +2.50 are the Moon (−12.92), Venus (−4.89), Jupiter (−2.94), Mars (−2.91), Mercury (−2.45), and Saturn (−0.49).
An exact order of the visual brightness of stars is not perfectly defined for the following reasons:

</doc>
<doc id="28162" url="http://en.wikipedia.org/wiki?curid=28162" title="List of nearest stars and brown dwarfs">
List of nearest stars and brown dwarfs

This list contains all known stars and brown dwarfs at a distance of up to 5 parsecs (16.3 light-years) from the Solar System. In addition to the Solar System, there are another 54 stellar systems currently known lying within this distance. These systems contain a total of 56 hydrogen-fusing stars (of which 46 are red dwarfs), 14 brown dwarfs, and 4 white dwarfs. Despite the relative proximity of these objects to Earth, only nine of them have an apparent magnitude less than 6.5, which means only about 12% of these objects can be observed with the naked eye. Besides the Sun, only three are first-magnitude stars: Alpha Centauri, Sirius, and Procyon. All of these objects are located in the Local Bubble, a region within the Orion–Cygnus Arm of the Milky Way.
List.
Stars visible to the unaided eye have their magnitude shown in light blue below. The classes of the stars and brown dwarfs are shown in the color of their spectral types (these colors are derived from conventional names for the spectral types and do not represent the star's observed color). Many brown dwarfs are not listed by visual magnitude but are listed by near-IR J band magnitude. Some of the parallax and distance results are preliminary measurements.
Maps of nearby stars.
This map shows all of the star systems within 14 light-years of the Sun (shown as "Sol"), except for four brown dwarfs discovered after 2009. Double and triple stars are shown "stacked", but the true location is the star closest to the central plane. Color corresponds to the table above.
This is a 3D map of the nearest stars using the coordinates listed above. The stars in the front have a right ascension of 18h. An animated version is available . 
Future and past.
It is estimated that Scholz's star passed about 52000 AU from the Sun about 70,000 years ago.
Ross 248, currently at a distance of 10.3 light-years, has a radial velocity of −81 km/s. In about 31,000 years it may be the closest star to the Sun for several millennia, with a minimum distance of 0.927 pc in 36,000 years. Gliese 445, currently at a distance of 17.6 light-years, has a radial velocity of −119 km/s. In about 40,000 years it will be the closest star for a period of several thousand years.
Gliese 710 is currently about 63.8 ly from Earth, but its proper motion, distance, and radial velocity indicate that it will approach within a very small distance—perhaps under one light year—from the Sun within 1.4 million years, based on past and current "Hipparcos" data. At closest approach it will be a first-magnitude star about as bright as Antares. The proper motion of Gliese 710 is very small for its distance, meaning it is traveling nearly directly in our line of sight.
In a time interval of ±10 million years from the present, Gliese 710 is the star whose combination of mass and close approach distance will cause the greatest gravitational perturbation of the Solar System.
Known Hipparcos stars that have passed or will pass within 5.1 light-years of the Sun within ±2 million years:
The distance to HIP 85605 was probably underestimated by a factor of ten due to it being an optical double that is particularly problematic in the Hipparcos data reduction.

</doc>
<doc id="28163" url="http://en.wikipedia.org/wiki?curid=28163" title="Sagitta">
Sagitta

Sagitta is a constellation. Its name is Latin for "arrow", and it should not be confused with the larger constellation Sagittarius, the archer. Although Sagitta is an ancient constellation, it has no star brighter than 3rd magnitude and has the third-smallest area of all constellations (only Equuleus and Crux are smaller). It was included among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union. Located to the north of the equator, Sagitta can be seen from every location on Earth except within the Antarctic circle.
Sagitta lies within the Milky Way and is bordered by the following constellations (beginning at the north and then continuing clockwise): the little fox Vulpecula, the mythological hero Hercules, the eagle Aquila and the dolphin Delphinus.
Notable features.
Stars.
The following are some of Sagitta's brightest stars:
History.
The Greeks who may have originally identified this constellation called it "Oistos". The Romans named it Sagitta. 
Johann Bayer chose to name the stars in Sagitta in a non-brightness order, in this case giving the brightest star a designation of γ. Another example of such a deviation from the usual brightness order is the constellation Sagittarius.
Mythology.
Sagitta's shape is reminiscent of an arrow, and many cultures have interpreted it thus, among them the Persians, Hebrews, Greeks and Romans. The Arabs called it "as-Sahm", a name that was transferred "Sham" and now refers to α Sge only.
Ancient Greece.
In ancient Greece, Sagitta was regarded as the weapon that Hercules used to kill the eagle (Aquila) of Jove that perpetually gnawed Prometheus' liver. The Arrow is located beyond the north border of Aquila, the Eagle. Others believe the Arrow to be the one shot by Hercules towards the adjacent Stymphalian birds (6th labor) who had claws, beaks and wings of iron, and who lived on human flesh in the marshes of Arcadia - Aquila the Eagle and Cygnus the Swan, and the Vulture - and still lying between them, whence the title Herculea. Eratosthenes claimed it as the arrow with which Apollo exterminated the Cyclopes.
External links.
Coordinates: 

</doc>
<doc id="28164" url="http://en.wikipedia.org/wiki?curid=28164" title="Simon Ockley">
Simon Ockley

Simon Ockley (1678 – 9 August 1720) was a British Orientalist.
Ockley was born at Exeter. He was educated at Queens' College, Cambridge, and graduated B.A. in 1697, MA. in 1701, and B.D. in 1710. He became fellow of Jesus College and vicar of Swavesey, and in 1711 was chosen Adams Professor of Arabic in the university. He had a large family, and his latter days were embittered by pecuniary embarrassments, which form the subject of a chapter in Isaac D'Israeli's "Calamities of Authors".
The preface to the second volume of his "History of the Saracens" is dated from Cambridge Castle, where he lay a prisoner for debt.
Ockley maintained that a knowledge of Oriental literature was essential to the proper study of theology, and in the preface to his first book, the "Introductio ad linguas orientales" (1706), he urges the importance of the study.
He died at Swavesey.

</doc>
<doc id="28165" url="http://en.wikipedia.org/wiki?curid=28165" title="Sharable Content Object Reference Model">
Sharable Content Object Reference Model

Sharable Content Object Reference Model (SCORM) is a collection of standards and specifications for web-based electronic educational technology (also called e-learning). It defines communications between client side content and a host system (called "the run-time environment"), which is commonly supported by a learning management system. SCORM also defines how content may be packaged into a transferable ZIP file called "Package Interchange Format."
SCORM is a specification of the Advanced Distributed Learning (ADL) Initiative from the Office of the United States Secretary of Defense.
SCORM 2004 introduced a complex idea called sequencing, which is a set of rules that specifies the order in which a learner may experience content objects. In simple terms, they constrain a learner to a fixed set of paths through the training material, permit the learner to "bookmark" their progress when taking breaks, and assure the acceptability of test scores achieved by the learner. The standard uses XML, and it is based on the results of work done by AICC, IMS Global, IEEE, and Ariadne.
SCORM versions.
SCORM 1.1.
SCORM 1.1 is the first production version. It used a Course Structure Format XML file based on the AICC specifications to describe content structure, but lacked a robust packaging manifest and support for metadata. Quickly abandoned in favor of SCORM 1.2.
SCORM 1.2.
This was the first version that was widely used. It is still widely used and is supported by most Learning Management Systems.
SCORM 2004.
This is the current version. It is based on new standards for API and content object-to-runtime environment communication, with many ambiguities of previous versions resolved. Includes ability to specify adaptive sequencing of activities that use the content objects. Includes ability to share and use information about the success status for multiple learning objectives or competencies across content objects and across courses for the same learner within the same learning management system. A more robust test suite helps ensure good interoperability.
Experience API (Tin Can API).
The Experience API (also known as xAPI or Tin Can API) was finalized to version 1.0 in April 2013. The Experience API solves many of the problems inherent with older versions of SCORM. Just like SCORM, ADL is the steward of the Experience API. AICC with their CMI-5 planned to use xAPI as their transport standard, but AICC membership decided to dissolve the organization and transferred CMI-5 to ADL.
The Experience API (Tin Can API) is a web service that allows software clients to read and write experiential data in the form of “statement” objects. In their simplest form, statements are in the form of “I did this”, or more generally “actor verb object”. More complex statement forms can be used. There is also a built-in query API to help filter recorded statements, and a state API that allows for a sort of “scratch space” for consuming applications. Experience API statements are stored in a data store called a Learning Record Store, which can exist on its own or within a Learning Management System.

</doc>
<doc id="28167" url="http://en.wikipedia.org/wiki?curid=28167" title="Sejm">
Sejm

The Sejm of the Republic of Poland (; Polish: "Sejm Rzeczypospolitej Polskiej") is the lower house of the Polish parliament. It consists of 460 deputies (posłowie, literally "envoys", in Polish) elected by universal ballot and is presided over by a speaker called the "Marshal of the "Sejm" of the Republic of Poland" ("Marszałek Sejmu Rzeczypospolitej Polskiej").
In the Kingdom of Poland, "Sejm" referred to the entire three-chamber parliament of Poland, comprising the lower house (the Chamber of Envoys; Polish: "Izba Poselska"), the upper house (the Senate; Polish: "Senat") and the King. It was thus a three-estate parliament. Since the Second Polish Republic (1918–1939), "Sejm" has referred only to the lower house of the parliament; the upper house is called the "Senat Rzeczypospolitej Polskiej" ("Senate of the Republic of Poland").
History.
Kingdom of Poland.
"Sejm" stems from an Old Slavic word meaning "gathering". Its origin was the King's Councils ("wiece"), which gained power during the time of Poland's fragmentation (1146–1295). The 1182 "Sejm" in Łęczyca (known as the "First Sejm") was the most notable of these councils, in that for the first time in Poland's history it established laws constraining the power of the ruler. It forbade arbitrary sequestration of supplies in the countryside and takeover of bishopric lands after the death of a bishop. However, these early "Sejm"s were not a regular event and were formed only at the King's behest.
After the 1493 "Sejm" in Piotrków, it became a regularly convening body, to which indirect elections were held every two years. The bicameral system was also established there. The "Sejm" now comprised two chambers: the "Senat" (Senate) of 81 bishops and other dignitaries; and the Chamber of Envoys, made up of 54 envoys elected by smaller local "sejmik" (assemblies of landed nobility) in each of the Kingdom's provinces. At the time, Poland's nobility, which accounted for around 10% of the state's population (then the highest amount in Europe), was becoming particularly influential, and with the eventual development of the Golden Liberty, the "Sejm"'s powers increased dramatically.
Polish-Lithuanian Commonwealth.
Over time, the number of envoys in the lower chamber grew in number and power as they pressed the king for more privileges. The "Sejm" eventually became even more active in supporting the goals of the privileged classes when the King ordered that the landed nobility and their estates (peasants) be drafted into military service. After the Union of Lublin in 1569, the Kingdom of Poland became, through personal union with the Grand Duchy of Lithuania, the Polish-Lithuanian Commonwealth, and thus the "Sejm" was supplemented with new envoys from among the Lithuanian nobility. This "Commonwealth of Both Nations" ensured that the state of affairs surrounding the three-estates system continued, with the "Sejm", Senate and King forming the estates and supreme deliberating body of the state. In the first few decades of the 16th century, the Senate had established its precedence over the "Sejm"; however, from the mid-1500s onwards, the "Sejm" became a very powerful representative body of the "Szlachta" ("middle nobility"). Soon, the "Sejm" began to limit the king's powers severely. Its chambers reserved the final decisions in legislation, taxation, budget, and treasury matters (including military funding), foreign affairs, and the confirment of nobility. The 1573 Warsaw Confederation saw the nobles of the "Sejm" officially sanction and guarantee religious tolerance in Commonwealth territory, ensuring a refuge for those fleeing the ongoing Reformation and Counter-Reformation wars in Europe.
Until the end of the 16th century, unanimity was not required, and the majority-voting process was the most commonly used system for voting. Later, with the rise of the Polish magnates and their increasing power, the unanimity principle was re-introduced with the institution of the nobility's right of "liberum veto" (Latin: "I freely forbid"). Additionally, if the envoys were unable to reach a unanimous decision within six weeks (the time limit of a single session), deliberations were declared void and all previous acts passed by that "Sejm" were annulled. From the mid-17th century onward, any objection to a "Sejm" resolution, by either an envoy or a senator, automatically caused the rejection of other, previously approved resolutions. This was because all resolutions passed by a given session of the "Sejm" formed a whole resolution, and, as such, was published as the annual "constituent act" of the "Sejm", e.g. the ""Anno Domini" 1667" act. In the 16th century, no single person or small group dared to hold up proceedings, but, from the second half of the 17th century, the "liberum veto" was used to virtually paralyze the "Sejm", and brought the Commonwealth to the brink of collapse. The "liberum veto" was finally abolished with the adoption of Poland's 3rd May Constitution in 1791, a piece of legislation which was passed as the "Government Act", and for which the "Sejm" required four years to propagate and adopt. The constitution's acceptance, and the possible long-term consequences it may have had, is arguably the reason for which the powers of Austria-Hungary, Russia and Prussia then decided to partition the Polish-Lithuanian Commonwealth, thus putting an end to over 300 years of Polish parliamentary continuity.
It is estimated that between 1493 and 1793, a "Sejm" was held 240 times, the total debate-time sum of which was 44 years.
Partitions.
After the fall of the Duchy of Warsaw, which existed as a Napoleonic client state between 1807 and 1815, and its short-lived "Sejm" of the Duchy of Warsaw, the "Sejm" of Congress Poland was established in the "Kongresówka" (Congress Poland) of Russia; it was composed of the king (the Russian emperor), the upper house (Senate), and the lower house (Chamber of Envoys).
Overall, during the period from 1795 until the re-establishment of Poland's sovereignty in 1918, little power was actually held by any Polish legislative body and the occupying powers of Russia, Prussia (later united Germany) and Austria-Hungary propagated legislation for their own respective formerly-Polish territories at a national level.
Congress Poland.
The Chamber of Envoys, despite its name, consisted not only of 77 envoys (sent by local assemblies) from the hereditary nobility, but also of 51 deputies, elected by the non-noble population. All deputies were covered by Parliamentary immunity, with each individual serving for a term of office of six years, with half of the deputies being elected every two years. Candidates for deputy had to be able to read and write, and have a certain amount of wealth. The legal voting age was 21, except for those citizens serving in the military, the personnel of which were not allowed to vote. Parliamentary sessions were initially convened every two years, and lasted for (at least) 30 days. However, after many clashes between liberal deputies and conservative government officials, sessions were later called only four times (1818, 1820, 1826, and 1830, with the last two sessions being secret).
The "Sejm" had the right to call for votes on civil and administrative legal issues, and, with permission from the king, it could also vote on matters related to the fiscal policy and the military. It had the right to exercise control over government officials, and to file petitions. The 64-member Senate on the other hand, was composed of "voivodes" and "kasztelans" (both types of provincial governors), Russian "princes of the blood", and nine bishops. It acted as the Parliamentary Court, had the right to control "citizens' books", and had similar legislative rights as did the Chamber of Deputies.
Germany and Austria-Hungary.
In the Free City of Cracow (1815–1846), a unicameral Assembly of Representatives was established, and from 1827, a unicameral provincial "sejm" existed in the Grand Duchy of Poznań. Poles were elected to and represented the majority in both of these legislatures; however, they were largely powerless institutions and exercised only very limited power. After numerous failures in securing legislative sovereignty in the early 19th century, many Poles simply gave up trying to attain a degree of independence from their foreign master-states. In the Austrian partition, a relatively powerless "Sejm" of the Estates operated until the time of the Spring of Nations. After this, in the mid to late 19th century, only in autonomous Galicia (1861–1914) was there a unicameral and functional National "Sejm", the "Sejm" of the Land. It is recognised today as having played a major and overwhelming positive role in the development of Polish national institutions.
In the second half of the 19th century, Poles were able to become members of the parliaments of Austria, Prussia and Russia, where they formed Polish Clubs. Deputies of Polish nationality were elected to the Prussian "Landtag" from 1848, and then to the German Empire's "Reichstag" from 1871. Polish Deputies were members of the Austrian State Council (from 1867), and from 1906 were also elected to the Russian Imperial State "Duma" (lower chamber) and to the State Council (upper chamber).
Second Republic.
After the First World War and re-establishment of Polish independence, the convocation of parliament, under the democratic electoral law of 1918, became an enduring symbol of the new state's wish to demonstrate and establish continuity with the 300 year Polish parliamentary traditions established before the time of the partitions. Maciej Rataj emphatically paid tribute to this with the phrase: "There is Poland there, and so is the "Sejm".
During the interwar period of Poland's independence, the first Legislative "Sejm" of 1919, a Constituent Assembly, passed the Small Constitution of 1919, which introduced a parliamentary republic and proclaimed the principle of the "Sejm"’s sovereignty. This was then strengthened, in 1921, by the March Constitution, one of the most democratic European constitutions enacted after the end of World War I. The constitution established a political system which was based on Montesquieu's doctrine of separation of powers, and which restored the bicameral "Sejm" consisting of a lower house (to which alone the name of "Sejm"" was from then on applied) and an upper house, the Senate. In 1919, Roza Pomerantz-Meltzer, a member of the Zionist party, became the first woman ever elected to the "Sejm".
The legal content of the March Constitution allowed for "Sejm" supremacy in the system of state institutions at the expense of the executive powers, thus creating a parliamentary republic out of the Polish state. An attempt to strengthen executive powers in 1926 (through the August Amendment) proved too limited and largely failed in helping avoid legislative grid-lock which had ensued as a result of too-great parliamentary power in a state which had numerous diametrically-opposed political parties sitting in its legislature. In 1935, the parliamentary republic was weakened further when, by way of, Józef Piłsudski's May Coup, the president was forced to sign the April Constitution of 1935, an act through which the head of state assumed the dominant position in legislating for the state and the Senate increased its power at the expense of the "Sejm".
On 2 September 1939, the "Sejm" held its final pre-war session, during which it declared Poland's readiness to defend itself against invading German forces. On 2 November 1939, the President dissolved the "Sejm" and the Senate, which were then, according to plan, to resume their activity within two months after the end of the Second World War; this, however, never happened. During wartime, the National Council (1939–1945) was established to represent the legislature as part of the Polish Government in Exile. Meanwhile, in Nazi-occupied Poland, the Council of National Unity was set up; this body functioned from 1944 to 1945 as the parliament of the Polish Underground State. With the cessation of hostilities in 1945, and subsequent rise to power of the Communist-backed Provisional Government of National Unity, the Second Polish Republic legally ceased to exist.
Polish People's Republic.
The "Sejm" in the Polish People's Republic had 460 deputies throughout most of its history. At first, this number was declared to represent one deputy per 60,000 citizens (425 were elected in 1952), but, in 1960, as the population grew, the declaration was changed: The constitution then stated that the deputies were representative "of" the people and could be recalled "by" the people, but this article was never used, and, instead of the "five-point electoral law", a non-proportional, "four-point" version was used. Legislation was passed with majority voting.
The "Sejm" voted on the budget as well as on the periodic national plans that were a fixture of communist economies. The "Sejm" deliberated in sessions that were ordered to convene by the State Council.
The "Sejm" also chose a "Prezydium" ("presiding body") from among its members; the marshall of which was always a member of the United People's Party. In its preliminary session, the "Sejm" also nominated the Prime Minister, the Council of Ministers of Poland, and members of the State Council. It also chose many other government officials, including the head of the Supreme Chamber of Control and members of the State Tribunal and the Constitutional Tribunal, as well as the Ombudsman (the last three bodies of which were created in the 1980s).
The Senate of Poland was abolished by the Polish people's referendum in 1946, after which the "Sejm" became the sole legislative body in Poland. Even though the "Sejm" was largely subservient to the Communist party, it is worth noting that a single brave deputy, Romuald Bukowski (Independent) voted against the imposition of martial law in 1982.
Today.
After the end of communism in 1989, the Senate was reinstated as the upper house of a bicameral national assembly, while the "Sejm" became the lower house. The "Sejm" is now composed of 460 deputies elected by proportional representation every four years.
Between 7 and 19 deputies are elected from each constituency using the d'Hondt method (with one exception, in 2001, when the Sainte-Laguë method was used), their number being proportional to their constituency's population. Additionally, a threshold is used, so that candidates are chosen only from parties that gained at least 5% of the nationwide vote (candidates from ethnic-minority parties are exempt from this threshold).

</doc>
<doc id="28168" url="http://en.wikipedia.org/wiki?curid=28168" title="Stock exchange">
Stock exchange

A stock exchange is a form of exchange which provides services for stock brokers and traders to buy or sell stocks, bonds, and other securities. Stock exchanges also provide facilities for issue and redemption of securities and other financial instruments, and capital events including the payment of income and dividends.
Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as "continuous auction" markets, with buyers and sellers consummating transactions at a central location, such as the floor of the exchange.
To be able to trade a security on a certain stock exchange, it must be listed there. Usually, there is a central location at least for record keeping, but trade is increasingly less linked to such a physical place, as modern markets are electronic networks, which gives them advantages of increased speed and reduced cost of transactions. Trade on an exchange is by members only.
The initial public offering of stocks and bonds to investors is by definition done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).
There is usually no compulsion to issue stock via the stock exchange itself, nor must stock be subsequently traded on the exchange. Such trading is said to be "off exchange" or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global market for securities.
In recent years, various other trading venues, such as electronic communication networks, alternative trading systems and "dark pools" have taken much of the trading activity away from traditional stock exchanges.
History.
The idea of debt dates back to the ancient world, as evidenced for example by ancient Mesopotamian clay tablets recording interest-bearing loans. There is little consensus among scholars as to when corporate stock was first traded. Some see the key event as the Dutch East India Company's founding in 1602, while others point to earlier developments. Economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome.
In the Roman Republic, which existed for centuries before the Empire was founded, there were "societates publicanorum", organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had "partes" or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions "shares that had a very high price at the time." Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The "societas" declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.
Tradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.
The Dutch East India Company, formed to build up the spice trade, operated as a colonial ruler in what's now Indonesia and beyond, a purview that included conducting military operations against wishes of the exploited natives and competing colonial powers. Control of the company was held tightly by its directors, with ordinary shareholders not having much influence on management or even access to the company's accounting statements.
However, shareholders were rewarded well for their investment. The company paid an average dividend of over 16 percent per year from 1602 to 1650. Financial innovation in Amsterdam took many forms. In 1609, investors led by one Isaac Le Maire formed history's first bear syndicate, but their coordinated trading had only a modest impact in driving down share prices, which tended to be robust throughout the 17th century. By the 1620s, the company was expanding its securities issuance with the first use of corporate bonds.
Joseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book "Confusion of Confusions" explained the workings of the city's stock market. It was the earliest book about stock trading, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.
William sought to modernize England's finances to pay for its wars, and thus the kingdom's first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public.
London's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698, a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.
One of history's greatest financial bubbles occurred in the next few decades. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of "a company for carrying out an undertaking of great advantage, but nobody to know what it is."
By the end of that same year, share prices were collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States.
Role of stock exchanges.
Stock exchanges have multiple roles in the economy. This may include the following:
Raising capital for businesses.
A stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.
Common forms of capital raising.
Besides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, there are four common forms of capital raising used by companies and entrepreneurs. Most of these available options might be achieved, directly or indirectly, through a stock exchange.
Going public.
Capital intensive companies, particularly high tech companies, always need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. After the 1990s and early-2000s hi-tech listed companies' boom and bust in the world's major stock exchanges, it has been much more demanding for the high-tech entrepreneur to take his/her company public, unless either the company already has products in the market and is generating sales and earnings, or the company has completed advanced promising clinical trials, earned potentially profitable patents or conducted market research which demonstrated very positive outcomes. This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world, in the total absence of sales, earnings and any well-documented promising outcome. Anyway, every year a number of companies, including unknown highly speculative and financially unpredictable hi-tech startups, are listed for the first time in all the major stock exchanges – there are even specialized entry markets for these kind of companies or stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX, or most of the third market companies).
Limited partnerships.
A number of companies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships. In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.
Venture capital.
A third usual source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).
Corporate partners.
A fourth alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.
Mobilizing savings for investment.
When people draw their savings and invest in shares (through an IPO or the issuance of new company shares of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.
Facilitating company growth.
Companies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or a merger agreement through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.
Profit sharing.
Both casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.
Corporate governance.
By having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders, and the more stringent rules for public corporations imposed by public stock exchanges and the government. Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders and/or their families and heirs, or otherwise by a small group of investors).
Despite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies. The dot-com bubble in the late 1990s, and the subprime mortgage crisis in 2007–08, are classical examples of corporate mismanagement. Companies like Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam (2001), Webvan (2001), Adelphia (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) India were among the most widely scrutinized by the media.
To assist in corporate governance many banks and companies worldwide utilize securities identification numbers (ISIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.
However, when poor financial, ethical or managerial records are known by the stock investors, the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.
Creating investment opportunities for small investors.
As opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors because a person buys the number of shares they can afford. Therefore the Stock Exchange provides the opportunity for small investors to own shares of the same companies as large investors.
Government capital-raising for development projects.
Governments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate the need, in the short term, to directly tax citizens to finance development—though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.
Barometer of the economy.
At the stock exchange, share prices rise and fall depending, largely, on economics forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. An economic recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.
Major stock exchanges.
Major stock exchanges (top 20 by market capitalization) of issued shares of domestic companies, as of 31 January 2015 ()
Listing requirements.
Listing requirements are the set of conditions imposed by a given stock exchange upon companies that want to be listed on that exchange. Such conditions sometimes include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.
Requirements by stock exchange.
Companies must meet an exchange's requirements to have their stocks and shares listed and traded there, but requirements vary by stock exchange:
Ownership.
Stock exchanges originated as mutual organizations, owned by its member stock brokers. There has been a recent trend for stock exchanges to "demutualize", where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Españoles, and the São Paulo Stock Exchange (2007).
The Shenzhen and Shanghai stock exchanges can been characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission. 
Another example is Tashkent republican stock exchange (Uzbekistan) established in 1994, three years after collapse of Soviet Union, mainly state-owned but has a form of a public corporation (joint stock company). According to an Uzbek government decision (March 2012) 25 percent minus one share of Tashkent stock exchange was expected to be sold to Korea Exchange(KRX) in 2014.
Other types of exchanges.
In the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These "commodity exchanges" later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.
See also.
Lists:

</doc>
<doc id="28170" url="http://en.wikipedia.org/wiki?curid=28170" title="Son of God">
Son of God

Historically, many rulers have assumed titles such as son of god, son of a god or son of Heaven. The Roman Emperor Augustus referred to his relation to his deified adoptive father, Julius Caesar, as "son of a god" via the term "divi filius" which was later also used by Domitian. The motif of a person being a "son of God" is widespread in mythology as well.
The term "son of God" is sometimes used in the Old and New Testaments of the Bible to refer to those with special relationships with God. In the Old Testament, angels, just and pious men, the descendants of Seth, and the kings of Israel are all called "sons of God." In the New Testament, Adam, and, most notably, Jesus Christ are called "son of God," while followers of Jesus are called, "sons of God."
In the New Testament, "Son of God" is applied to Jesus on many occasions. Jesus is declared to be the Son of God on two separate occasions by a voice speaking from Heaven. Jesus is also explicitly and implicitly described as the Son of God by himself and by various individuals who appear in the New Testament. As applied to Jesus, the term is a reference to his role as the Messiah, the King chosen by God. The contexts and ways in which Jesus' title, Son of God, means something more than or other than Messiah remain the subject of ongoing scholarly study and discussion.
The term "Son of God" should not be confused with the term "God the Son" (Greek: Θεός ὁ υἱός), the second person of the Trinity in Christian theology. The doctrine of the Trinity identifies Jesus as God the Son, "identical in essence but distinct in person" with regard to God the Father and God the Holy Spirit (the first and third Persons of the Trinity). Nontrinitarian Christians accept the application to Jesus of the term "Son of God", which is found in the New Testament, but not the term "God the Son", which is not found there.
Rulers and Imperial titles.
Throughout history, emperors and rulers ranging from the Western Zhou dynasty (c. 1000 BC) in China to Alexander the Great (c. 360 BC) to the Emperor of Japan (c. 600 AD) have assumed titles that reflect a filial relationship with deities.
The title "Son of Heaven" i.e. 天子 (from 天 meaning sky/heaven/god and 子 meaning child) was first used in the Western Zhou dynasty (c. 1000 BC). It is mentioned in the Shijing book of songs, and reflected the Zhou belief that as Son of Heaven (and as its delegate) the Emperor of China was responsible for the well being of the whole world by the Mandate of Heaven. This title may also be translated as "son of God" given that the word "Ten" or "Tien" in Chinese may either mean sky or god. The Emperor of Japan was also called the Son of Heaven (天子 "tenshi") starting in the early 7th century.
Among the Steppe Peoples, there was also a widespread use of "Son of God/Son of Heaven" for instance, in the Third Century B.C., the ruler was called Chanyü and similar titles were used as late as the 13th Century by Genghis Khan.
Examples of kings being considered the son of god are found throughout the Ancient Near East. Egypt in particular developed a long lasting tradition. Egyptian pharaohs are known to have been referred to as the son of a particular god and their begetting in some cases is even given in sexually explicit detail. Egyptian pharaohs did not have full parity with their divine fathers but rather were subordinate.:36 Nevertheless, in the first four dynasties, the pharaoh was considered to be the embodiment of a god. Thus, Egypt was ruled by direct theocracy, wherein "God himself is recognized as the head" of the state. During the later Amarna Period, Akhenaten reduced the Pharaoh's role to one of coregent, where the Pharaoh and God ruled as father and son. Akhenaten also took on the role of the priest of god, eliminating representation on his behalf by others. Later still, the closest Egypt came to the Jewish variant of theocracy was during the reign of Herihor. He took on the role of ruler not as a god but rather as a high-priest and king.
Jewish kings are also known to have been referred to as "son of the LORD".:150 The Jewish variant of theocracy can be thought of as a representative theocracy where the king is viewed as God’s surrogate on earth. Jewish kings thus, had less of a direct connection to god than pharaohs. Unlike pharaohs, Jewish kings rarely acted as priests, nor were prayers addressed directly to them. Rather, prayers concerning the king are addressed directly to god.:36–38 The Jewish philosopher Philo is known to have likened God to a supreme king, rather than likening Jewish kings to gods.
Based on the Bible, several kings of Damascus took the title son of Hadad. From the archaeological record a stela erected by Bar-Rakib for his father Panammuwa II contains similar language. The son of Panammuwa II a king of Sam'al referred to himself as a son of Rakib.:26–27 Rakib-El is a god who appears in Phoenician and Aramaic inscriptions. Panammuwa II died unexpectedly while in Damascus. However, his son the king Bar-Rakib was not a native of Damascus but rather the ruler of Sam'al it is unknown if other rules of Sam'al used similar language.
In Greek mythology, Heracles (son of Zeus) and many other figures were considered to be sons of gods through union with mortal women. From around 360 BC onwards Alexander the Great may have implied he was a demigod by using the title "Son of Ammon–Zeus".
In 42 BC, Julius Caesar was formally deified as "the divine Julius" ("divus Iulius") after his assassination. His adopted son, Octavian (better known as Augustus, a title given to him 15 years later, in 27 BC) thus became known as "divi Iuli filius" (son of the divine Julius) or simply "divi filius" (son of the god). As a daring and unprecedented move, Augustus used this title to advance his political position in the Second Triumvirate, finally overcoming all rivals for power within the Roman state.
The word applied to Julius Caesar as deified was "divus", not the distinct word "deus". Thus Augustus called himself "Divi filius", and not "Dei filius". The line between been god and god-like was at times less than clear to the population at large, and Augustus seems to have been aware of the necessity of keeping the ambiguity. As a purely semantic mechanism, and to maintain ambiguity, the court of Augustus sustained the concept that any worship given to an emperor was paid to the "position of emperor" rather than the person of the emperor. However, the subtle semantic distinction was lost outside Rome, where Augustus began to be worshiped as a deity. The inscription DF thus came to be used for Augustus, at times unclear which meaning was intended. The assumption of the title "Divi filius" by Augustus meshed with a larger campaign by him to exercise the power of his image. Official portraits of Augustus made even towards the end of his life continued to portray him as a handsome youth, implying that miraculously, he never aged. Given that few people had ever seen the emperor, these images sent a distinct message.
Later, Tiberius (emperor from 14–37 AD) came to be accepted as the son of "divus Augustus" and Hadrian as the son of "divus Trajan". By the end of the 1st century, the emperor Domitian was being called "dominus et deus" (i.e. "master and god").
Outside the Roman Empire, the 2nd century Kushan King Kanishka I used the title "devaputra" meaning "son of God".
Judaism.
Although references to "sons of God", "son of God" and "son of the LORD" are occasionally found in Jewish literature, they never refer to physical descent from God. There are two instances where Jewish kings are figuratively referred to as a god.:150 The king is likened to the supreme king God. These terms are often used in the general sense in which the Jewish people were referred to as "children of the LORD your God".
When used by the rabbis, the term referred to Israel or to human beings in general, and not as a reference to the Jewish mashiach. In Judaism the term "mashiach" has a broader meaning and usage and can refer to a wide range of people and objects, not necessarily related to the Jewish eschaton.
Genesis.
In the introduction to the Genesis flood narrative, refers to "sons of God" who married the daughters of men and is used in a polytheistic context to refer to angels.
Exodus.
In , the Israelites as a people are called "my firstborn son" by God using the singular form.
Psalms.
In , David calls God his father. God in turn tells David that he will make David his first-born and highest king of the earth.:45:150
In , the Biblical judges are called gods and the sons of God.
Royal Psalms.
Psalm 2 is thought to be an enthronement text. The rebel nations and the uses of an iron rod are Assyrian motifs. The begetting of the king is an Egyptian one.:26 Israel’s kings are referred to as the son of the LORD. They are reborn or adopted on the day of their enthroning as the "son of the LORD".:150
Some scholars think that Psalm 110 is an alternative enthronement text. Psalm 110:1 distinguishes the king from the LORD. The LORD asks the king to sit at his right hand. Psalm 110:3 may or may not have a reference to the begetting of kings. The exact translation of 110:3 is uncertain. In the traditional Hebrew translations his youth is renewed like the morning dew. In some alternative translations the king is begotten by God like the morning dew or by the morning dew. One possible translation of 110:4 is that the king is told that he is a priest like Melchizedek. Another possibility is to translate Melchizedek not as a name but rather as a title "Righteous King". If a reference is made to Melchizedek this could be linked to pre-Israelite Canaanite belief. The invitation to sit at the right hand of the deity and the king’s enemy’s being used as footstools are both classic Egyptian motifs, as is the association of the king with the rising sun. Many scholars now think that Israelite beliefs evolved from Canaanite beliefs.:29–33:150 Jews have traditionally believed that Psalm 110 applied only to King David. Being the first Davidic king, he had certain priest-like responsibilities.
Psalm 45 is thought to be a royal wedding text. Psalm 45:7-8 may refer to the king as a god anointed by God, reflecting the king’s special relationship with God.:150
Some believe that these psalms where not meant to apply to a single king, but rather where used during the enthronement ceremony. The fact that the Royal Psalms were preserved suggests that the influence of Egyptian and other near eastern cultures on pre-exile religion needs to be taken seriously. Ancient Egyptians used similar language to describe pharaohs. Assyrian and Canaanite influences among others are also noted.:24–38
Samuel.
In God promises David regarding his offspring that "I will be to him as a father and he will be to me as a son." The promise is one of eternal kingship.:39–44
Isaiah.
In Isaiah 9:6 the next king is greeted, similarly to the passages in Psalms. Like Psalm 45:7-8 he is figuratively likened to the supreme king God.:150 Isaiah could also be interpreted as the birth of a royal child, Psalm 2 nevertheless leaves the accession scenario as an attractive possibility.:28 The king in 9:6 is thought to have been Hezekiah by Jews and various academic scholars.:28
Jeremiah.
In Jeremiah Chapter 31 God refers to himself as the father of Israel and Ephraim as his first born son. Ephraim in Jeremiah refers collectively to the northern kingdom.:43
Book of Wisdom.
The Book of Wisdom refers to a righteous man as the son of God.:157
Book of Ecclesiasticus.
In the Book of Ecclesiasticus 4:10 in the Hebrew text God calls a person who acts righteously his son. The Greek reads slightly different here he will be “like a son of the Most High".:157–158
Dead Sea Scrolls.
In some versions of Deuteronomy the Dead Sea Scrolls refer to the sons of God rather than the sons of Israel, probably in reference to angels. The Septuagint reads similarly.:147
4Q174 is a midrashic text in which God refers to the Davidic messiah as his son.
4Q246 refers to a figure who will be called the son of God and son of the Most High. It is debated if this figure represents the royal messiah, a future evil gentile king or something else.
In 11Q13 Melchizedek is referred to as god the divine judge. Melchizedek in the bible was the king of Salem. At least some in the Qumran community seemed to think that at the end of days Melchizedek would reign as their king. The passage is based on Psalm 82.
Gabriel's Revelation.
Gabriel's Revelation, also called the Vision of Gabriel or the Jeselsohn Stone, is a three-foot-tall (one metre) stone tablet with 87 lines of Hebrew text written in ink, containing a collection of short prophecies written in the first person and dated to the late 1st century BCE. One of the stories allegedly tells of a man who was killed by the Romans and resurrected in three days. 
It is a tablet described as a "Dead Sea scroll in stone".
The text seems to talk about a messianic figure from Ephraim who will break evil before righteousness by three days.:43–44 Later the text talks about a “prince of princes" a leader of Israel who is killed by the evil king and not properly buried.:44 The evil king is then miraculously defeated.:45 The text seems to refer to Jeremiah Chapter 31.:43 The choice of Ephraim as the linage of the messianic figure described in the text seems to draw on passages in Jeremiah, Zechariah and Hosea. This leader is referred to as a son of God.:43–44, 48–49
The text seems to be based on a Jewish revolt recorded by Josephus dating from 4 BCE.:45–46 Based on its dating the text seems to refer to Simon of Peraea one of the three leaders of this revolt.:47
Pseudepigrapha.
In both Joseph and Aseneth and the related text The Story of Asenath, Joseph is referred to as the son of God.:158–159 In the Prayer of Joseph both Jacob and the angel are referred to as angels and the sons of God.:157
Talmud.
This style of naming is also used for some rabbis in the Talmud.:158
Christianity.
Of all the Christological titles used in the New Testament, Son of God has had one of the most lasting impacts in Christian history and has become part of the profession of faith by many Christians. In the mainstream Trinitarian context the title implies the full divinity of Jesus as part of the Holy Trinity of Father, Son and the Spirit.
The New Testament quotes Psalm 110 extensively as applying to Jesus. A new theological understanding of Psalm 110:1 and 110:4, distinct from that of Judaism, evolved. Jesus himself quotes Psalm 110 in Luke 20:41-44, Matthew 22:41-45 and Mark 12:35-37.:211 The meanings and authenticity of these quotations are debated among modern scholars.:204 Various modern critical scholars reject that David wrote this psalm. In the Masoretic Text many Psalm including this one are explicitly attributed to David. The superscription is “of David a psalm." Some have suggested that this indicates that Psalm 110 was not written by David. The superscription as it stands is ambiguous. However, Jewish tradition ascribes Psalm 110 and indeed all Psalms to king David.:314–315 In Christianity David is consider to be a prophet. The New Testament records several psalms as having been spoken through David by the Holy Spirit. Acts 2:29-30 explicitly calls David a prophet. Jesus himself affirms the authorship of this psalm by David in Mark 12:36 and Matthew 22:43.:314–315 In the Christian reading, David the king is presented as having a lord other than the Lord God. The second lord is the Messiah, who is greater than David, because David calls him "my lord".:371–373 In Hebrew, the first "Lord" in Psalm 110 is "Yahweh" (יהוה), while the second is referred to as "adoni" (אדני), (my "adon"), a form of address that in the Old Testament is used generally for humans but also, in , for the theophanic Angel of the Lord.:319 The Greek-speaking Jewish philosopher Philo, a contemporary of Jesus, identified the Angel of the Lord with his version of the logos distinct from the later Christian logos.
It’s debated when exactly Christians came to understand Psalm 110 as introducing a distinction of persons in the Godhead and indicating that Jesus was more than a human or angelic messiah, but also a divine entity who was David’s lord.:202–205, 210–11 Hebrews 1:13 again quotes Psalm 110 to prove that the Son is superior to angels.:272:939 Psalm 110 would play a crucial role in the development of the early Christian understanding of the divinity of Jesus. The final reading of Psalm 110:1 incorporated a Preexistent Son of God greater than both David and the angels. The Apostle Creed, The Nicaea Creed and the Creed of Constantinople would all included references to Psalm 110:1.:272
 reads
I will tell of the decree of the Lord:
He said to me, "You are my son; today I have begotten you. Ask of me, and I will make the nations your heritage, and the ends of the earth your possession. You shall break them with a rod of iron, and dash them in pieces like a potter’s vessel."
Psalm 2 can be seen as referring to a particular king of Judah, but has also been understood to reference the awaited Messiah. References to Psalm 2 in the New Testament are less common then Psalm 110. The passages in Acts, Hebrews and Romans that refer to it give the appearance of being linked with Jesus’ resurrection and/or exaltation. Those in the Gospels associate it with Jesus' baptism and transfiguration. The majority of scholars believe that the earliest Christian use of this Psalm was in relation to his resurrection, suggesting that this was initially thought of as the moment when he became Son, a status that the early Christians later extended back to his earthly life, to the beginning of that earthly life and, later still, to his pre-existence, a view that Aquila Hyung Il Lee questions.:250–251
The terms "sons of God" and "son of God" appear frequently in Jewish literature, and leaders of the people, kings and princes were called "sons of God". What Jesus did with the language of divine sonship was first of all to apply it individually (to himself) and to fill it with a meaning that lifted "Son of God" beyond the level of his being merely a human being made like Adam in the image of God, his being perfectly sensitive to the Holy Spirit (), his bringing God's peace (; ) albeit in his own way (, ), or even his being God's designated Messiah.
In the New Testament, the title "Son of God" is applied to Jesus on many occasions. It is often used to refer to his divinity, from the beginning of the New Testament narrative when in the angel Gabriel announces: "the power of the Most High shall overshadow thee: wherefore also the holy thing which is begotten shall be called the Son of God."
The declaration that Jesus is the Son of God is echoed by many sources in the New Testament. On two separate occasions the declarations are by God the Father, when during the Baptism of Jesus and then during the Transfiguration as a voice from Heaven. On several occasions the disciples call Jesus the Son of God and even the Jews scornfully remind Jesus during his crucifixion of his claim to be the Son of God."
However, the concept of God as the father of Jesus, and Jesus as the exclusive divine Son of God is distinct from the concept of God as the Creator and father of all people, as indicated in the Apostle's Creed. The profession begins with expressing belief in the "Father almighty, creator of heaven and earth" and then immediately, but separately, in "Jesus Christ, his only Son, our Lord", thus expressing both senses of fatherhood within the Creed.
Synoptic Gospels.
According to the Synoptic Gospels, Jesus referred to himself obliquely as "the Son" and even more significantly spoke of God as "my Father" (; ; ). He not only spoke like "the Son" but also acted like "the Son" in knowing and revealing the truth about God, in changing the divine law, in forgiving sins, in being the one through whom others could become children of God, and in acting with total obedience as the agent for God's final kingdom. This clarifies the charge of blasphemy brought against him at the end (); he had given the impression of claiming to stand on a par with God. Jesus came across as expressing a unique filial consciousness and as laying claim to a unique filial relationship with the God whom he addressed as "Abba".
Even if historically he never called himself "the only" Son of God (cf. ; ), Jesus presented himself as Son and not just as one who was the divinely appointed Messiah (and therefore "son" of God). He made himself out to be more than only someone chosen and anointed as divine representative to fulfil an eschatological role in and for the kingdom. Implicitly, Jesus claimed an essential, "ontological" relationship of sonship towards God which provided the grounds for his functions as revealer, lawgiver, forgiver of sins, and agent of the final kingdom. Those functions (his "doing") depended on his ontological relationship as Son of God (his "being"). Jesus invited his hearers to accept God as a loving, merciful Father. He worked towards mediating to them a new relationship with God, even to the point that they too could use "Abba" when addressing God in prayer. Yet, Jesus' consistent distinction between "my" Father and "your" Father showed that he was not inviting the disciples to share with him an identical relationship of sonship. He was apparently conscious of a qualitative distinction between his sonship and their sonship which was derived from and depended on his. His way of being son was different from theirs.
Paul.
In their own way, John and Paul maintained this distinction. Paul expressed their new relationship with God as taking place through an "adoption" (; ), which makes them "children of God" () or, alternatively, "sons of God" (; (). John distinguished between the only Son of God (; ) and all those who through faith can become "children of God" (; ; and ). Paul and John likewise maintained and developed the correlative of all this, Jesus' stress on the fatherhood of God. Over 100 times John's Gospel names God as "Father". Paul's typical greeting to his correspondents runs as follows: "Grace to you and peace from "God our Father" and the/our Lord Jesus Christ" (; ; ; ; ; ; ). The greeting names Jesus as "Lord", but the context of "God our Father" implies his sonship.
Paul therefore distinguished between their graced situation as God's adopted children and that of Jesus as Son of God. In understanding the latter's "natural" divine sonship, Paul firstly spoke of God "sending his own Son in the likeness of sinful nature and to deal with sin" (). In a similar passage, Paul says that "when the fullness of time had come God sent his Son, born of a woman, born under the law" (). If one examines these three passages in some detail, it raises the question whether Paul thinks of an eternally pre-existent Son coming into the world from his Father in heaven to set humanity free from sin and death () and make it God's adopted children (). The answer will partly depend, first, on the way one interprets other Pauline passages which do not use the title "Son of God" (; ). These latter passages present a pre-existent Christ taking the initiative, through his "generosity" in "becoming poor" for us and "assuming the form of a slave". The answer will, second, depend on whether one judges and to imply that as a pre-existent being the Son was active at creation. without explicitly naming "the Son" as such, runs:
There is one God, the Father, from whom are all things and for whom we exist, and one Lord, Jesus Christ, through whom are all things and through whom we exist.
Calling God "the Father" clearly moves one toward talk of "the Son". In the case of , the whole hymn () does not give Jesus any title. However, he has just been referred to () as God's "beloved Son".
Third, it should be observed that the language of "sending" (or, for that matter, "coming" with its stress on personal purpose (; ) by itself does not necessarily imply pre-existence. Otherwise one would have to ascribe pre-existence to John the Baptist, "a man sent from God", who "came to bear witness to the light" (; cf. ). In the Old Testament, angelic and human messengers, especially prophets, were "sent" by God, but one should add at once that the prophets sent by God were never called God's sons. It makes a difference that in the cited Pauline passages it was God's Son who was sent. Here being "sent" by God means more than merely receiving a divine commission and includes coming from a heavenly pre-existence and enjoying a divine origin. Fourth, in their context, the three Son of God passages here examined (Rom. 8:3, 32; Gal. 4:4) certainly do not focus on the Son's pre-existence, but on his being sent or given up to free human beings from sin and death, to make them God's adopted children, and to let them live (and pray) with the power of the indwelling Spirit. Nevertheless, the Apostle's soteriology presupposes here a Christology that includes divine pre-existence. It is precisely because Christ is the pre-existent Son who comes from the Father that he can turn human beings into God's adopted sons and daughters.
Gospel of John.
In the Gospel of John, Jesus is the eternally pre-existent Son who was sent from heaven into the world by the Father (e.g., ; ; ). He remains conscious of the divine pre-existence he enjoyed with the Father (, ). He is one with the father (; ) and loved by the Father (; ; ; ). The Son has the divine power to give life and to judge (; ; ; ). Through his death, resurrection, and ascension the Son is glorified by the Father (), but it is not a glory that is thereby essentially enhanced. His glory not only existed from the time of the incarnation to reveal the Father (), but also pre-existed the creation of the world (). Where paul and the author of Hebrews picture Jesus almost as the elder brother or the first-born of God's new eschatological family (; ), John insists even more on the clear qualitative difference between Jesus' sonship and that of others. Being God's "only Son" (; ), he enjoys a truly unique and exclusive relationship with the Father.
At least four of these themes go back to the earthly Jesus himself. First, although one has no real evidence for holding that he was humanly aware of his eternal pre-existence as Son, his "Abba-consciousness" revealed an intimate loving relationship with the Father. The full Johannine development of the Father-Son relationship rests on an authentic basis in the Jesus-tradition (; ; ; ). Second, Jesus not only thought of himself as God's Son, but also spoke of himself as sent by God. Once again, John develops the theme of the Son's mission, which is already present in sayings that at least partly go back to Jesus (; ; ), especially in , where it is a question of the sending of a "beloved Son". Third, the Johannine theme of the Son with power to judge in the context of eternal life finds its original historical source in the sayings of Jesus about his power to dispose of things in the kingdom assigned to him by "my Father" () and about one's relationship to him deciding one's final destiny before God (). Fourth, albeit less insistently, when inviting his audience to accept a new filial relationship with God, Jesus — as previously seen — distinguished his own relationship to God from theirs. The exclusive Johannine language of God's "only Son" has its real source in Jesus' preaching. All in all, Johannine theology fully deploys Jesus' divine sonship, but does so by building up what one already finds in the Synoptic Gospels and what, at least in part, derives from the earthly Jesus himself.
New Testament narrative.
The Gospel of Mark begins by calling Jesus the Son of God and reaffirms the title twice when a voice from Heaven calls Jesus: "my Son" in and .
In after Jesus walks on water, the disciples tell Jesus: "You really are the Son of God!" In response to the question by Jesus, "But who do you say that I am?", Peter replied: "You are Christ, the Son of the living God". And Jesus answered him, "Blessed are you, Simon Bar-Jonah! For flesh and blood has not revealed this to you, but my Father who is in heaven" (). In , while Jesus hangs on the cross, the Jewish leaders mock him to ask God help, "for he said, I am the Son of God", referring to the claim of Jesus to be the Son of God. and include the exclamation by the Roman commander: "He was surely the Son of God!" after the earthquake following the Crucifixion of Jesus.
In , in the Annunciation, before the birth of Jesus, the angel tells Mary that her child "shall be called the Son of God". In (and ), when Jesus casts out demons, they fall down before him, and declare: "You are the Son of God."
In John the Baptist bears witness that Jesus is the Son of God and in Martha calls him the Messiah and the Son of God. In several passages in the Gospel of John assertions of Jesus being the Son of God are usually also assertions of his unity with the Father, as in : "If you know me, then you will also know my Father" and "Whoever has seen me has seen the Father".
In the Jews cry out to Pontius Pilate "Crucify him" based on the charge that Jesus "made himself the Son of God." The charge that Jesus had declared himself "Son of God" was essential to the argument of the Jews from a religious perspective, as the charge that he had called himself King of the Jews was important to Pilate from a political perspective, for it meant possible rebellion against Rome.
Towards the end of his Gospel (in ) John declares that the purpose for writing it was "that you may believe that Jesus is the Messiah, the Son of God".
In , after the Conversion of Paul the Apostle, and following his recovery, "straightway in the synagogues he proclaimed Jesus, that he is the Son of God."
Jesus' own assertions.
When in Matthew 16:15–16 Apostle Peter states: "You are Christ, the Son of the living God", Jesus not only accepts the titles, but calls Peter "blessed" because his declaration had been revealed him by "my Father who is in Heaven". According to John Yieh, in this account the evangelist Matthew is unequivocally stating this as the church's view of Jesus.
In the Sanhedrin trial of Jesus in Mark 14:61 when the high priest asked Jesus: "Are you the Messiah, the Son of the Blessed One?" Jesus responded "I am". Jesus' claim here was emphatic enough to make the high priest tear his robe.
In the new Testament Jesus uses the term "my Father" as a direct and unequivocal assertion of his sonship, and a unique relationship with the Father beyond any attribution of titles by others:
In a number of other episodes Jesus claims sonship by referring to the Father, e.g. in Luke 2:49 when he is found in the temple a young Jesus calls the temple "my Father's house", just as he does later in John 2:16 in the Cleansing of the Temple episode. In Matthew 1:11 and Luke 3:22 Jesus allows himself to be called the Son of God by the voice from above, not objecting to the title.
References to "my Father" by Jesus in the New Testament are distinguished in that he never includes other individuals in them and only refers to "his" Father, however when addressing the disciples he uses "your" Father, excluding himself from the reference.
New Testament references.
Humans, including the New Testament writers, calling Jesus Son of God
Attributed to Jesus himself
Unclear whether attributed to Jesus himself or only a comment of the evangelist
God the Father referring to Jesus as his son
Angels calling Jesus Son of God
The devil or demons calling Jesus Son of God
Jesus referred to as the Son:
The God and Father of Jesus
Theological development.
Through the centuries, the theological development of the concept of Son of God has interacted with other Christological elements such as Pre-existence of Christ, Son of man, the hypostatic union, etc. For instance, in Johannine "Christology from above" which begins with the Pre-existence of Christ, Jesus did not become Son of God through the Virgin Birth, he always 'was' the Son of God.
By the 2nd century, differences had developed among various Christian groups and to defend the mainstream view in the early Church, St. Irenaeus introduced the confession: "One Christ only, Jesus the Son of God incarnate for our salvation". By referring to incarnation, this professes Jesus as the pre-existing Logos, i.e. The Word. It also professes him as both Christ and the only-begotten Son of God.
To establish a common ground, the Nicene Creed of 325 began with the profession of the Father Almighty and then states the belief:
Saint Augustine wrote at length on the Son of God and its relationship with the Son of man, positioning the two issues in terms of the dual nature of Jesus as both divine and human in terms of the hypostatic union. He wrote:
Christ Jesus, the Son of God, is God and Man: God before all worlds, man in our world...
But since he is the only Son of God, by nature and not by grace, he became also the Son of Man that he might be full of grace as well.
However, unlike Son of God, the proclamation of Jesus as the Son of man has never been an article of faith in Christianity. The interpretation of the use of "the Son of man" and its relationship to Son of God has remained challenging and after 150 years of debate no consensus on the issue has emerged among scholars.
Just as in Romans 10:9–13 Paul emphasized the salvific value of "professing by mouth" that Jesus is Lord (Kyrion Iesoun) Augustine emphasized the value of "professing that Jesus is the Son of God" as a path to salvation.
For Saint Thomas Aquinas (who also taught the Perfection of Christ) the "'Son of God' is God as known to God". Aquinas emphasized the crucial role of the Son of God in bringing forth all of creation and taught that although humans are created in the "image of God" they fall short and only the Son of God is truly like God, and hence divine.
Islam.
In Islam, Jesus (Arabic: عيسى, translit.: "ʿĪsā") is considered to be the Messiah and a highly respected prophet sent to the Children of Israel,#Redirect but not the son of God. As in Christianity, Jesus had no earthly father, but is instead seen as born through the breathing of the "Spirit of God" on Mary. The Qur'an compares the nature of his birth to the birth of Adam, who had neither mother nor father. The Qur'an also asserts that God has no begotten son as in the verse "He begets not, nor is He begotten." The birth of Jesus without a father, is stated in the following verse of the Quran:
She (Mary) said, "My Lord, how will I have a child when no man has touched me?" [The angel] said, "Such is Allah ; He creates what He wills. When He decrees a matter, He only says to it, 'Be,' and it is.
The Quran challenges the acceptance of Jesus or other person as the son of God in the following verse:
O People of the Scripture, do not commit excess in your religion or say about Allah except the truth. The Messiah, Jesus, the son of Mary, was but a messenger of Allah and His word which He directed to Mary and a soul [created at a command] from Him. So believe in Allah and His messengers. And do not say, "Three"; desist - it is better for you. Indeed, Allah is but one God. Exalted is He above having a son. To Him belongs whatever is in the heavens and whatever is on the earth. And sufficient is Allah as Disposer of affairs.
Bahá'í Faith.
In the writings of the Bahá'í Faith, the term "Son of God" is applied to Jesus, but does not indicate a literal physical relationship between Jesus and God, but is symbolic and is used to indicate the very strong spiritual relationship between Jesus and God and the source of his authority. Shoghi Effendi, the head of the Bahá'í Faith in the first half of the 20th century, also noted that the term does not indicate that the station of Jesus is superior to other prophets and messengers that Bahá'ís name Manifestations of God, including Buddha, Muhammad and Baha'u'llah among others. Shoghi Effendi notes that, since all Manifestations of God share the same intimate relationship with God and reflect the same light, the term Sonship can in a sense be attributable to all the Manifestations.
Bibliography.
</dl>

</doc>
<doc id="28171" url="http://en.wikipedia.org/wiki?curid=28171" title="SA">
SA

Sa or SA may refer to:

</doc>
<doc id="28172" url="http://en.wikipedia.org/wiki?curid=28172" title="Saint Boniface">
Saint Boniface

Saint Boniface (Latin: "Bonifatius") (c. 675? – 5 June 754), born Winfrid, Wynfrith, or Wynfryth in the kingdom of Wessex in Anglo-Saxon England, was a leading figure in the Anglo-Saxon mission to the German parts of the Frankish Empire during the 8th century. He established the first organized Christianity in many parts of Germany. He is the patron saint of Germany, the first archbishop of Mainz and the "Apostle of the Germans". He was killed in Frisia in 754, along with 52 others. His remains were returned to Fulda, where they rest in a sarcophagus which became a site of pilgrimage. Facts about Boniface's life and death as well as his work became widely known, since there is a wealth of material available—a number of "vitae", especially the near-contemporary "Vita Bonifatii auctore Willibaldi", and legal documents, possibly some sermons, and above all his correspondence.
Norman F. Cantor notes the three roles Boniface played that made him "one of the truly outstanding creators of the first Europe, as the apostle of Germany, the reformer of the Frankish church, and the chief fomentor of the alliance between the papacy and the Carolingian family." Through his efforts to reorganize and regulate the church of the Franks, he helped shape Western Christianity, and many of the dioceses he proposed remain today. After his martyrdom, he was quickly hailed as a saint in Fulda and other areas in Germany and in England. His cult is still notably strong today. Boniface is celebrated (and criticized) as a missionary; he is regarded as a unifier of Europe, and he is seen (mainly by Catholics) as a German national figure.
Early life and first mission to Frisia.
The earliest Bonifacian "vita", Willibald's, does not mention his place of birth but says that at an early age he attended a monastery ruled by Abbot Wulfhard in "escancastre", or "Examchester", which seems to denote Exeter, and may have been one of many "monasteriola" built by local landowners and churchmen; nothing else is known of it outside the Bonifacian "vitae". Later tradition places his birth at Crediton, but the earliest mention of Crediton in connection to Boniface is from the early fourteenth century, in John Grandisson's "Legenda Sanctorum: The Proper Lessons for Saints' Days according to the use of Exeter". In one of his letters Boniface mentions he was "born and reared...[in] the synod of London", but he may have been speaking metaphorically.
According to the "vitae", Winfrid was of a respected and prosperous family. Against his father's wishes he devoted himself at an early age to the monastic life. He received further theological training in the Benedictine monastery and minster of Nhutscelle (Nursling), not far from Winchester, which under the direction of abbot Winbert had grown into an industrious centre of learning in the tradition of Aldhelm. Winfrid taught in the abbey school and at the age of 30 became a priest; in this time, he wrote a Latin grammar, the "Ars Grammatica", besides a treatise on verse and some Aldhelm-inspired riddles. While little is known about Nursling outside of Boniface's "vitae", it seems clear that the library there was significant. In order to supply Boniface with the materials he needed, it would have contained works by Donatus, Priscian, Isidore, and many others. Around 716, when his abbot Wynberth of Nursling died, he was invited (or expected) to assume his position—it is possible that they were related, and the practice of hereditary right among the early Anglo-Saxons would affirm this. Winfrid, however, declined the position and in 716 set out on a missionary expedition to Frisia.
Early missionary work in Frisia and Germania.
Boniface first left for the continent in 716. He traveled to Utrecht, where Willibrord, the "Apostle of the Frisians," had been working since the 690s. He spent a year with Willibrord, preaching in the countryside, but their efforts were frustrated by the war then being carried on between Charles Martel and Radbod, King of the Frisians. Willibrord fled to the abbey he had founded in Echternach (in modern-day Luxembourg) while Boniface returned to Nursling.
Boniface returned to the continent the next year and went straight to Rome, where Pope Gregory II renamed him "Boniface", after the (legendary) fourth-century martyr Boniface of Tarsus, and appointed him missionary bishop for Germania—he became a bishop without a diocese for an area that lacked any church organization. He would never return to England, though he remained in correspondence with his countrymen and kinfolk throughout his life.
According to the "vitae" Boniface felled the Donar Oak, Latinized by Willibald as "Jupiter's oak," near the present-day town of Fritzlar in northern Hesse. According to his early biographer Willibald, Boniface started to chop the oak down, when suddenly a great wind, as if by miracle, blew the ancient oak over. When the god did not strike him down, the people were amazed and converted to Christianity. He built a chapel dedicated to Saint Peter from its wood at the site—the chapel was the beginning of the monastery in Fritzlar. This account from the "vita" is stylized to portray Boniface as a singular character who alone acts to root out paganism. Lutz von Padberg and others point out that what the "vitae" leave out is that the action was most likely well-prepared and widely publicized in advance for maximum effect, and that Boniface had little reason to fear for his personal safety since the Frankish fortified settlement of Büraburg was nearby. According to Willibald, Boniface later had a church with an attached monastery built in Fritzlar, on the site of the previously built chapel, according to tradition.
Boniface and the Carolingians.
The support of the Frankish mayors of the palace (maior domos), and later the early Pippinid and Carolingian rulers, was essential for Boniface's work. Boniface had been under the protection of Charles Martel from 723 on. The Christian Frankish leaders desired to defeat their rival power, the non-Christian Saxons, and to incorporate the Saxon lands into their own growing empire. Boniface's destruction of indigenous Germanic pagan sites may have benefited the Franks in their campaign against the Saxons.
In 732, Boniface traveled again to Rome to report, and Pope Gregory III conferred upon him the pallium as archbishop with jurisdiction over Germany. Boniface again set out for what is now Germany, baptized thousands, and dealt with the problems of many other Christians who had fallen out of contact with the regular hierarchy of the Roman Catholic Church. During his third visit to Rome in 737–38, he was made papal legate for Germany.
After Boniface's third trip to Rome, Charles Martel erected four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them to Boniface as archbishop and metropolitan over all Germany east of the Rhine. In 745, he was granted Mainz as metropolitan see. In 742, one of his disciples, Sturm (also known as Sturmi, or Sturmius), founded the abbey of Fulda not far from Boniface's earlier missionary outpost at Fritzlar. Although Sturm was the founding abbot of Fulda, Boniface was very involved in the foundation. The initial grant for the abbey was signed by Carloman, the son of Charles Martel, and a supporter of Boniface's reform efforts in the Frankish church. The saint himself explained to his old friend, Daniel of Winchester, that without the protection of Charles Martel he could "neither administer his church, defend his clergy, nor prevent idolatry."
According to German historian Gunther Wolf, the high point of Boniface's career was the Concilium Germanicum, organized by Carloman in an unknown location in April 743. While Boniface was not able to safeguard the church from property seizures by the local nobility, he did achieve one goal, the adoption of stricter guidelines for the Frankish clergy, which often hailed directly from the nobility. After Carloman's resignation in 747 he maintained a sometimes turbulent relationship with the king of the Franks, Pepin; the claim that he would have crowned Pepin at Soissons in 751 is now generally discredited.
Boniface balanced this support and attempted to maintain some independence, however, by attaining the support of the papacy and of the Agilolfing rulers of Bavaria. In Frankish, Hessian, and Thuringian territory, he established the dioceses of Würzburg, and Erfurt. By appointing his own followers as bishops, he was able to retain some independence from the Carolingians, who most likely were content to give him leeway as long as Christianity was imposed on the Saxons and other Germanic tribes.
Last mission to Frisia.
According to the "vitae", Boniface had never relinquished his hope of converting the Frisians, and in 754 he set out with a retinue for Frisia. He baptized a great number and summoned a general meeting for confirmation at a place not far from Dokkum, between Franeker and Groningen. Instead of his converts, however, a group of armed robbers appeared who slew the aged archbishop. The "vitae" mention that Boniface persuaded his (armed) comrades to lay down their arms: "Cease fighting. Lay down your arms, for we are told in Scripture not to render evil for good but to overcome evil by good."
Having killed Boniface and his company, the Frisian bandits ransacked their possessions but found that the company's luggage did not contain the riches they had hoped for: "they broke open the chests containing the books and found, to their dismay, that they held manuscripts instead of gold vessels, pages of sacred texts instead of silver plates." They attempted to destroy these books, the earliest "vita" already says, and this account underlies the status of the Ragyndrudis Codex, now held as a Bonifacian relic in Fulda, and supposedly one of three books found on the field by the Christians who inspected it afterward. Of those three books, the Ragyndrudis Codex shows incisions that could have been made by sword or axe; its story appears confirmed in the Utrecht hagiography, the "Vita altera", which reports that an eye-witness saw that the saint at the moment of death held up a gospel as spiritual protection. The story was later repeated by Otloh's "vita"; at that time, the Ragyndrudis Codex seems to have been firmly connected to the martyrdom.
Boniface's remains were moved from the Frisian countryside to Utrecht, and then to Mainz, where sources contradict each other regarding the behavior of Lullus, Boniface's successor as archbishop of Mainz. According to Willibald's "vita" Lullus allowed the body to be moved to Fulda, while the (later) "Vita Sturmi", a hagiography of Sturm by Eigil of Fulda, Lullus attempted to block the move and keep the body in Mainz.
His remains were eventually buried in the abbey church of Fulda after resting for some time in Utrecht, and they are entombed within a shrine beneath the high altar of Fulda Cathedral, previously the abbey church.
Veneration.
Fulda.
Veneration of Boniface in Fulda began immediately after his death; his grave was equipped with a decorative tomb around ten years after his burial, and the grave and relics became the center of the abbey and of the cult of the saint. Fulda monks prayer for newly elected abbots at the grave site before greeting him, and every Monday the saint was remembered in prayer, the monks prostrating themselves and reciting Psalm 50. After the abbey church was rebuilt to become the Ratgar Basilica (dedicated 791), Boniface's remains were translated to a new grave: since the church had been enlarged, his grave, originally in the west, was now in the middle; his relics were moved to a new apse in 819. From then on Boniface, as patron of the abbey, was regarded as both spiritual intercessor for the monks and legal owner of the abbey and its possessions, and all donations to the abbey were done in his name. He was honored on the date of his martyrdom, 5 June (with a mass written by Alcuin), and (around the year 1000) with a mass dedicated to his appointment as bishop, on 1 December.
Dokkum.
Willibald's "vita" describes how a visitor on horseback come to the site of the martyrdom, and a hoof of his horse got stuck in the mire. When it was pulled loose, a well sprang up. By the time of the "Vita altera Bonifatii" (9th century), there was a church on the site, and the well had become a "fountain of sweet water" used to sanctify people. The "Vita Liudgeri", a hagiographical account of the work of Ludger, describes how Ludger himself had built the church, sharing duties with two other priests. According to James Palmer, the well was of great importance since the saint's body was hundreds of miles away; the physicality of the well allowed for an ongoing connection with the saint. In addition, Boniface signified Dokkum's and Frisia's "connect[ion] to the rest of (Frankish) Christendom".
Memorials.
Saint Boniface's feast day is celebrated on 5 June in the Roman Catholic Church, the Lutheran Church, the Anglican Communion and the Eastern Orthodox Church.
A famous statue of Saint Boniface stands on the grounds of Mainz Cathedral, seat of the archbishop of Mainz. A more modern rendition stands facing the cathedral of Fritzlar.
The UK National Shrine is located at the Catholic church at Crediton, Devon, which has a bas-relief of the felling of Thor's Oak, by sculptor Kenneth Carter. The sculpture was unveiled by Princess Margaret in his native Crediton, located in Newcombes Meadow Park. There is also a series of paintings there by Timothy Moore. There are quite a few churches dedicated to St. Boniface in the United Kingdom: Bunbury, Cheshire; Chandler's Ford and Southampton Hampshire; Adler Street, London; Papa Westray, Orkney; St Budeaux, Plymouth (now demolished); Bonchurch, Isle of Wight; Cullompton, Devon.
Bishop George Errington founded St Boniface's Catholic College, Plymouth in 1856. The school celebrates Saint Boniface on 5 June each year.
In 1818, Father Norbert Provencher founded a mission on the east bank of the Red River in what was then Rupert's Land, building a log church and naming it after St. Boniface. The log church was consecrated as Saint Boniface Cathedral after Provencher was himself consecrated as a bishop and the diocese was formed. The community that grew around the cathedral eventually became the city of St. Boniface, which merged into the city of Winnipeg in 1971. In 1844, four Grey Nuns arrived by canoe in Manitoba, and in 1871, built Western Canada's first hospital: St. Boniface Hospital, where the Assiniboine and Red Rivers meet. Today, St. Boniface Hospital is the second-largest hospital in Manitoba.
Legends.
Some traditions credit Saint Boniface with the invention of the Christmas tree. The "vitae" mention nothing of the sort. However, it is mentioned on a BBC-Devon website, in an account which places Geismar in Bavaria, and in a number of educational books, including "St. Boniface and the Little Fir Tree", "The Brightest Star of All: Christmas Stories for the Family", "The American normal readers". and a short story by Henry van Dyke, "The First Christmas Tree".
Sources and writings.
"Vitae".
The earliest "Life" of Boniface was written by a certain Willibald, an Anglo-Saxon priest who came to Mainz after Boniface's death, around 765. Willibald's biography was widely dispersed; Levison lists some forty manuscripts. According to his lemma, a group of four manuscripts including Codex Monacensis 1086 are copies directly from the original.
Listed second in Levison's edition is the entry from a late ninth-century Fulda document: Boniface's status as a martyr is attested by his inclusion in the "Fulda Martyrology" which also lists, for instance, the date (1 November) of his translation in 819, when the Fulda Cathedral had been rebuilt.
The next "vita", chronologically, is the "Vita altera Bonifatii auctore Radbodo", which originates in the Bishopric of Utrecht, and was probably revised by Radboud of Utrecht (899–917). Mainly agreeing with Willibald, it adds an eye-witness who presumably saw the martyrdom at Dokkum. The "Vita tertia Bonifatii" likewise originates in Utrecht. It is dated between 917 (Radboud's death) and 1075, the year Adam of Bremen wrote his "Gesta Hammaburgensis ecclesiae pontificum", which used the "Vita tertia".
A later "vita", written by Otloh of St. Emmeram (1062–1066), is based on Willibald's and a number of other "vitae" as well as the correspondence, and also includes information from local traditions.
Correspondence.
Boniface engaged in regular correspondence with fellow churchmen all over Western Europe, including the three popes he worked with, and with some of his kinsmen back in England. Many of these letters contain questions about church reform and liturgical or doctrinal matters. In most cases, what remains is one half of the conversation, either the question or the answer. The correspondence as a whole gives evidence of Boniface's widespread connections; some of the letters also prove an intimate relationship especially with female correspondents.
There are 150 letters in what is generally called the Bonifatian correspondence, though not all them are by Boniface or addressed to him. They were assembled by order of archbishop Lullus, Boniface's successor in Mainz, and were initially organized into two parts, a section containing the papal correspondence and another with his private letters. They were reorganized in the eighth century, in a roughly chronological ordering. Otloh of St. Emmeram, who worked on a new "vita" of Boniface in the eleventh century, is credited with compiling the complete correspondence as we have it.
The correspondence was edited and published already in the seventeenth century, by Nicolaus Serarius. Stephan Alexander Würdtwein's 1789 edition, "Epistolae S. Bonifacii Archiepiscopi Magontini", was the basis for a number of (partial) translations in the nineteenth century. The first version to be published by Monumenta Germaniae Historica (MGH) was the edition by Ernst Dümmler (1892); the most authoritative version until today is Michael Tangl's 1912 "Die Briefe des Heiligen Bonifatius, Nach der Ausgabe in den Monumenta Germaniae Historica", published by MGH in 1916. This edition is the basis of Ephraim Emerton's selection and translation in English, "The Letters of Saint Boniface", first published in New York in 1940; it was republished most recently with a new introduction by Thomas F.X. Noble in 2000.
Sermons.
Some fifteen preserved sermons are traditionally associated with Boniface, but that they were actually his is not generally accepted.
Grammar and poetry.
Early in his career, before he left for the continent, Boniface wrote an "Ars Grammatica", a grammatical treatise presumably for his students in Nursling. Helmut Gneuss reports that one manuscript copy of the treatise originates from (the south of) England, mid-eighth century; it is now held in Marburg, in the Hessisches Staatsarchiv. He also wrote a treatise on verse, the "Caesurae uersuum", and a collection of riddles, the "Enigmata", influenced greatly by Aldhelm and containing many references to works of Vergil (the "Aeneid", the "Georgics", and the "Eclogues"). Three octosyllabic poems written in clearly Aldhelmian fashion (according to Andy Orchard) are preserved in his correspondence, all composed before he left for the continent.
Additional materials.
A letter by Boniface charging Aldebert and Clement with heresy is preserved in the records of the Roman Council of 745 that condemned the two. Boniface had an interest in the Irish canon law collection known as "Collectio canonum Hibernensis", and a late 8th/early 9th-century manuscript in Würzburg contains, besides a selection from the "Hibernensis", a list of rubrics that mention the heresies of Clemens and Aldebert. The relevant folios containing these rubrics were most likely copied in Mainz, Würzburg, or Fulda, all places associated with Boniface. Michael Glatthaar suggested that the rubrics should be seen as Boniface's contribution to the agenda for a synod.
Anniversary and other celebrations.
Boniface's death (and birth) has given rise to a number of noteworthy celebrations. The dates for some of these celebrations have undergone some changes: in 1805, 1855, and 1905 (and in England in 1955) anniversaries were calculated with Boniface's death dated in 755, the "Mainz tradition"; Michael Tangl's dating of the martyrdom in 754 was not accepted until after 1955. Celebrations in Germany centered on Fulda and Mainz, in the Netherlands on Dokkum and Utrecht, and in England on Crediton and Exeter.
Celebrations in Germany: 1805, 1855, 1905.
The first German celebration on a fairly large scale was held in 1805 (the 1150th anniversary of his death), followed by a similar celebration in a number of towns in 1855; both of these were predominantly Catholic affairs, which emphasized the role of Boniface in German history as opposed to Protestant views on the role of Martin Luther, and especially the 1855 celebrations were an expression of German Catholic nationalism. In 1905, when strife between Catholic and Protestant factions had eased (one Protestant church published a celebratory pamphlet, Gerhard Ficker's "Bonifatius, der "Apostel der Deutschen""), there were modest celebrations and a publication for the occasion on historical aspects of Boniface and his work, the 1905 "Festgabe" by Gregor Richter and Carl Scherer. In all, the content of these early celebrations showed evidence of the continuing question about the meaning of Boniface for Germany, though the importance of Boniface in cities associated with him was without question.
1954 celebrations.
In 1954, celebrations were widespread, in England, Germany, and the Netherlands, and a number of these celebrations were international affairs. Especially in Germany, these celebrations had a distinctly political note to them and often stressed Boniface as a kind of founder of Europe, such as when Konrad Adenauer, the (Catholic) German chancellor, addressed a crowd of 60,000 in Fulda, celebrating the feast day of the saint in a European context: "Das, was wir in Europa gemeinsam haben, [ist] gemeinsamen Ursprungs" ("What we have in common in Europe comes from the same source").
Papal visit, 1980.
When Pope John Paul II visited Germany in November 1980, he spent two days in Fulda (17 and 18 November). He celebrated mass in Fulda Cathedral with 30,000 gathered on the square in front of the building, and met with the German Bishops' Conference (held in Fulda since 1867). The pope next celebrated mass outside the cathedral, in front of an estimated crowd of 100,000, and hailed the importance of Boniface for German Christianity: "Der heilige Bonifatius, Bischof und Märtyrer, "bedeutet" den 'Anfang' des Evangeliums und der Kirche in Eurem Land" ("The holy Boniface, bishop and martyr, "signifies" the beginning of the gospel and the church in your country"). A photograph of the pope praying at Boniface's grave became the centerpiece of a prayer card distributed from the cathedral.
2004 celebrations.
In 2004, anniversary celebrations were held throughout Northwestern Germany and Utrecht, and Fulda and Mainz—generating a great amount of academic and popular interest. The event occasioned a number of scholarly studies, esp. biographies (for instance, by Auke Jelsma in Dutch, Lutz von Padberg in German, and Klaas Bruinsma in Frisian), and a fictional completion of the Boniface correspondence (Lutterbach, "Mit Axt und Evangelium"). A German musical proved a great commercial success, and in the Netherlands an opera was staged.
Scholarship on Boniface.
The literature on the saint and his work is extensive. At the time of the various anniversaries, edited collections were published containing essays by some of the best-known scholars of the time, such as the 1954 collection "Sankt Bonifatius: Gedenkgabe zum Zwölfhundertsten Todestag" and the 2004 collection "Bonifatius—Vom Angelsächsischen Missionar zum Apostel der Deutschen". In the modern era, published a number of biographies and articles on the saint focusing on his missionary praxis and his relics. The most authoritative biography is still Theodor Schieffer's "Winfrid-Bonifatius und die Christliche Grundlegung Europas" (1954).

</doc>
<doc id="28174" url="http://en.wikipedia.org/wiki?curid=28174" title="Data storage device">
Data storage device

A data storage device is a device for recording (storing) information (data). Recording can be done using virtually any form of energy, spanning from manual muscle power in handwriting, to acoustic vibrations in phonographic recording, to electromagnetic energy modulating magnetic tape and optical discs.
A storage device may hold information, process information, or both. A device that only holds information is a recording medium. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.
Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require vision and a brain to read data fall into this category. Electromagnetic data may be stored in either an analog data or digital data format on a variety of media. This type of data is considered to be electronically encoded data, whether or not it is electronically stored in a semiconductor device, for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of computer data storage) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) microcircuits are volatile memory, for it vanishes if power is removed.
With the exception of barcodes and OCR data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium. However, the durability of methods such as printed data is still superior to that of most electronic storage media. The durability limitations may be overcome with the ease of duplicating (backing-up) electronic data.
Terminology.
Devices that are not used exclusively for recording (e.g. hands, mouths, musical instruments) and devices that are intermediate the storing/retrieving process (e.g. eyes, ears, cameras, scanners, microphones, speakers, monitors, video projectors) are not usually considered storage devices. Devices that are exclusively for recording (e.g. printers), exclusively for reading (e.g. barcode readers), or devices that process only one form of information (e.g. phonographs) may or may not be considered storage devices. In computing these are known as input/output devices.
All information is data. However, not all data is information.
Many data storage devices are also media players. Any device that can store and playback multimedia may also be considered a media player such as in the case with the HD media player. Designated hard drives are used to play saved or streaming media on home cinemas or home theater PCs.
Global capacity, digitization, and trends.
In a recent study in "Science" it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than 3 (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007, doubling roughly every 3 years.
In a less comprehensive study, the International Data Corporation estimated that the total amount of digital data was 281 exabytes in 2007, and had for the first time exceeded the amount of storage.
It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information on digital than on analog storage devices. In 1986 only 1% of the world's capacity to store information was in digital format, which grew to 3% by 1993, 25% in the year 2000, and exploded to 97% of the world's storage capacity by 2007.
Data storage equipment.
Any input/output equipment may be considered data storage equipment if it writes to and reads from a data storage medium.
Data storage methods.
Data storage equipment uses either:
The following are examples of those methods:
Recording medium.
A recording medium is a physical material that holds data expressed in any of the existing recording formats. With electronic media, the data and the recording medium is sometimes\s part of the surface of the medium.
Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time. Data such as smoke signals or skywriting are temporary by nature.
Modern examples by shape.
A typical way to classify data storage media is to consider its shape and type of movement (or non-movement) relative to the read/write device(s) of the storage apparatus as listed:
Bekenstein (2003) foresees that miniaturization might lead to the invention of devices that store bits on a single atom.
Weight and volume.
Especially for carrying around data, the weight and volume per MB are relevant. They are quite large for written and printed paper compared with modern electronic media. On the other hand, written and printer paper do not require (the weight and volume of) reading equipment, and handwritten edits only require simple writing equipment, such as a pen.
With mobile data connections the data need not be carried around to be available.

</doc>
<doc id="28175" url="http://en.wikipedia.org/wiki?curid=28175" title="Sinn Féin">
Sinn Féin

Sinn Féin ( ) is an Irish republican political party active in both the Republic of Ireland and Northern Ireland. The name is Irish for "ourselves" or "we ourselves", although it is frequently mistranslated as "ourselves alone". Originating in the Sinn Féin organisation founded in 1905 by Arthur Griffith, it took its current form in 1970 after a split within the party (the other party is the Workers' Party of Ireland), and has been associated with the Provisional Irish Republican Army. Gerry Adams has been party president since 1983.
Sinn Féin is currently the second-largest party behind the Democratic Unionist Party in the Northern Ireland Assembly, where it has four ministerial posts in the power-sharing Northern Ireland Executive, and the fourth-largest party in the Oireachtas, the parliament of the Republic. Sinn Féin also received the second highest number of Northern Ireland votes and seats in the 2015 United Kingdom general election, behind the DUP.
History.
Pre-1970.
Sinn Féin was founded on 28 November 1905, when, at the first annual Convention of the National Council, Arthur Griffith outlined the Sinn Féin policy. That policy was "to establish in Ireland's capital a national legislature endowed with the moral authority of the Irish nation". Sinn Féin contested the Leitrim North by-election of 1908 and secured 27% of the vote. Thereafter, both support and membership fell. At the 1910 Ard Fheis (party conference) the attendance was poor and there was difficulty finding members willing to take seats on the executive.
In 1914, Sinn Féin members, including Griffith, joined the anti-Redmond Irish Volunteers, which was referred to by Redmondites and others as the "Sinn Féin Volunteers". Although Griffith himself did not take part in the Easter Rising of 1916, many Sinn Féin members, who were also members of both the Volunteers and the Irish Republican Brotherhood, did. Government and newspapers dubbed the Rising "the Sinn Féin Rising". After the Rising, republicans came together under the banner of Sinn Féin, and at the 1917 Ard Fheis the party committed itself for the first time to the establishment of an Irish Republic. In the 1918 general election, Sinn Féin won 73 of Ireland's 105 seats, and in January 1919, its MPs assembled in Dublin and proclaimed themselves Dáil Éireann, the parliament of Ireland. The party supported the Irish Republican Army during the War of Independence, and members of the Dáil government negotiated the Anglo-Irish Treaty with the British Government in 1921. In the Dáil debates that followed, the party divided on the Treaty. Anti-Treaty members led by Éamon de Valera walked out, and pro- and anti-Treaty members took opposite sides in the ensuing Civil War.
Pro-Treaty Dáil deputies and other Treaty supporters formed a new party, Cumann na nGaedheal, on 27 April 1923 at a meeting in Dublin where delegates agreed on a constitution and political programme. Cumann na nGaedheal governed the new Irish Free State for ten years. It merged with two other organisations to form Fine Gael in 1933. Anti-Treaty Sinn Féin members continued to boycott the Dáil. At a special "Ard Fheis" in March 1926 de Valera proposed that elected members be allowed to take their seats in the Dáil if and when the controversial oath of allegiance was removed. When his motion was defeated, de Valera resigned from Sinn Féin and on 16 May 1926 founded his own party, Fianna Fáil, which was dedicated to republicanising the Free State from within. He took most Sinn Féin TDs with him. De Valera's resignation meant also the loss of financial support from America. The Sinn Féin party could field no more than fifteen candidates, and won only six seats in the June election, a level support not seen since pre-1916. Vice-President and de facto leader Mary MacSwiney announced that the party simply did not have the funds to contest the second election called that year, declaring "no true Irish citizen can vote for any of the other parties".
An attempt in the 1940s to access funds which had been put in the care of the High Court led to the Sinn Féin Funds Case, which the party lost and in which the judge ruled that it was not the direct successor of the Sinn Féin of 1917. At the Westminister 1959 general election, the Sinn Féin vote dropped almost 60% from the 1955 number 152,000 to 63,000. In the 1960s, Sinn Féin moved to the left. It became involved in campaigns over the provision of housing and social services during the sixties. It also adopted a "National Liberation Strategy" which was the brainchild of Roy Johnston. In 1967 the Garland commission was set up to investigate the possibility of ending abstentionism. Its report angered many within the party, notably Seán Mac Stíofáin and Ruairí Ó Brádaigh.
1970–83.
The Sinn Féin party split in two at the beginning of 1970. At the party's "Ard Fheis" on 11 January the proposal to end abstentionism and take seats, if elected, in the Dáil, the Parliament of Northern Ireland and the Parliament of the United Kingdom was put before the members. A similar motion had been adopted at an IRA convention the previous month, leading to the formation of a Provisional Army Council by Mac Stíofáin and other members opposed to the leadership. When the motion was put to the "Ard Fheis", it failed to achieve the necessary two-thirds majority. The Executive attempted to circumvent this by introducing a motion in support of IRA policy, at which point the dissenting delegates walked out of the meeting. These members reconvened at another place, appointed a Caretaker Executive and pledged allegiance to the Provisional Army Council. The Caretaker Executive declared itself opposed to the ending of abstentionism, the drift towards "extreme forms of socialism", the failure of the leadership to defend the nationalist people of Belfast during the 1969 Northern Ireland riots, and the expulsion of traditional republicans by the leadership during the 1960s. At the October 1970 "Ard Fheis" delegates were informed that an IRA convention had been held and had regularised its structure, bringing to an end the 'provisional' period. By then, however, the label "Provisional" or "Provo" was already being applied to them by the media. The opposing, anti-abstentionist party became known as "Official Sinn Féin". It changed its name in 1977 to "Sinn Féin, the Workers' Party", and in 1982 to "The Workers' Party".
Initially, because the "Provisionals" were committed to military rather than political action, Sinn Féin's membership was largely confined, in Danny Morrison's words, to people "over military age or women". A Belfast Sinn Féin organiser of the time described the party's role as "agitation and publicity". New "cumainn" (branches) were established in Belfast, and a new newspaper, "Republican News", was published. Sinn Féin took off as a protest movement after the introduction of internment in August 1971, organising marches and pickets. The party launched its platform, "Éire Nua" (a New Ireland) at the 1971 "Ard Fheis". In general, however, the party lacked a distinct political philosophy. In the words of Brian Feeney, "Ó Brádaigh would use Sinn Féin "ard fheiseanna" to announce republican policy, which was, in effect, IRA policy, namely that Britain should leave the North or the 'war' would continue". Sinn Féin was given a concrete presence in the community when the IRA declared a ceasefire in 1975. 'Incident centres' were set up to communicate potential confrontations to the British authorities. They were manned by Sinn Féin, which had been legalised the year before by Secretary of State, Merlyn Rees.
After the ending of the truce another issue arose—that of political status for prisoners. Rees released the last of the internees but introduced the Diplock courts, and ended 'Special Category Status' for all prisoners convicted after 1 March 1976. This led first to the blanket protest, and then to the dirty protest . Around the same time, Gerry Adams began writing for "Republican News", calling for Sinn Féin to become more involved politically . The prisoners' protest climaxed with the 1981 hunger strike, during which striker Bobby Sands was elected Member of Parliament for Fermanagh and South Tyrone as an Anti H-Block candidate. After his death on hunger strike, his seat was held, with an increased vote, by his election agent, Owen Carron. These successes convinced republicans that they should contest every election. Danny Morrison expressed the mood at the 1981 "Ard Fheis" when he said:
This was the origin of what became known as the Armalite and ballot box strategy. "Éire Nua" was dropped in 1982, and the following year Ó Brádaigh stepped down as leader, to be replaced by Adams.
1983–present.
Under Adams' leadership electoral politics became increasingly important. In 1983 Alex Maskey was elected to Belfast City Council, the first Sinn Féin member to sit on that body. Sinn Féin polled over 100,000 votes in the Westminster elections that year, with Adams winning the West Belfast seat previously held by the Social Democratic and Labour Party (SDLP). By 1985 it had fifty-nine seats on seventeen of the twenty-six Northern Ireland councils, including seven on Belfast City Council.
The party began a reappraisal of the policy of abstention from the Dáil. At the 1983 "Ard Fheis" the constitution was amended to remove the ban on the discussion of abstentionism, so as to allow Sinn Féin to run a candidate in the forthcoming European elections, although in his address Adams said, "We are an abstentionist party. It is not my intention to advocate change in this situation." A motion to permit entry into the Dáil was allowed at the 1985 "Ard Fheis", but without the active support of the leadership, and Adams did not speak. The motion failed narrowly. By October of the following year an IRA Convention had indicated its support for elected Sinn Féin Teachtaí Dála (TDs) taking their seats. Thus, when the motion to end abstention was put to the "Ard Fheis" on 1 November 1986, it was clear that there would not be a split in the IRA as there had been in 1970. The motion was passed with a two-thirds majority. Ó Brádaigh and about twenty other delegates walked out, and met in a Dublin hotel to form Republican Sinn Féin.
Multi-party negotiations began in 1994 in Northern Ireland, without Sinn Féin. The Provisional IRA declared a ceasefire in the autumn of 1994. The John Major-led Conservative government had asked that the IRA decommission all of their weapons before Sinn Féin be admitted to the talks, but the Labour government of Tony Blair later let them in on the basis of the ceasefire.
The talks led to the Good Friday Agreement of 10 April 1998 (officially known as the "Belfast Agreement"), which set up an inclusive devolved government in the North, and altered the Southern government's constitutional claim to the whole island in Articles 2 and 3 of the Constitution of Ireland. Republicans opposed to the direction taken by Sinn Féin in the peace process formed the 32 County Sovereignty Movement in the late 1990s.
The party expelled Denis Donaldson, a party official, in December 2005, with him stating publicly that he had been in the employ of the British government as an agent since the 1980s. Mr. Donaldson told reporters that the British security agencies who employed him were behind the collapse of the Assembly and set up Sinn Féin to take the blame for it, a claim disputed by the British Government. Donaldson was found fatally shot in his home in County Donegal on 4 April 2006, and a murder inquiry was launched. In April 2009, the Real IRA released a statement taking responsibility for the killing.
When Sinn Féin and the DUP became the largest parties, it was clear that no deal could be made without the support of both parties. They nearly reached a deal in November 2004, but the DUP had a requirement for visible evidence that decommissioning had been carried out.
On 2 September 2006 Martin McGuinness publicly stated that Sinn Féin would refuse to participate in a shadow assembly at Stormont, asserting that his party would only take part in negotiations that were aimed at restoring a power-sharing government within Northern Ireland. This development follows a decision on the part of members of Sinn Féin to refrain from participating in debates since the Assembly's recall this past May. The relevant parties to these talks were given a deadline of 24 November 2006 to decide upon whether or not they would ultimately form the executive.
The 86-year Sinn Féin boycott of policing in Northern Ireland ended on 28 January 2007 when the Ard Fheis voted overwhelmingly to support the PSNI. Sinn Féin members will sit on Policing Boards and District Policing Partnerships. There has been some opposition to this decision from people such as former IRA prisoner Gerry McGeough, who stood in the 2007 Assembly Elections against Sinn Féin in the assembly constituency of Fermanagh and South Tyrone.
Links with the IRA.
Sinn Féin is the largest group in the Republican wing of Irish nationalism and is closely associated with the Provisional IRA, with the Irish Government alleging that senior members of Sinn Féin have held posts on the IRA Army Council. However, the SF leadership has denied these claims. The US Government also alleged that Sinn Féin and the IRA were linked.
A republican document of the early 1980s states, "Both Sinn Féin and the IRA play different but converging roles in the war of national liberation. The Irish Republican Army wages an armed campaign... Sinn Féin maintains the propaganda war and is the public and political voice of the movement".
The British Government stated in 2005 that "we had always said all the way through we believed that Sinn Féin and the IRA were inextricably linked and that had obvious implications at leadership level".
The robbery of £26.5 million from the Northern Bank in Belfast in December 2004 further scuppered chances of a deal. The IRA were blamed for the robbery though Sinn Féin denied this and stated that party officials had not known of the robbery nor sanctioned it. Because of the timing of the robbery, it is considered that the plans for the robbery must have been laid whilst Sinn Féin was engaged in talks about a possible peace settlement. This undermined confidence within the unionist community about the sincerity of republicans towards reaching agreement. In the aftermath of the row over the robbery, a further controversy erupted when, on RTÉ's "Questions and Answers" programme, the chairman of Sinn Féin, Mitchel McLaughlin, insisted that the IRA's controversial killing of a mother of ten young children, Jean McConville, in the early 1970s though "wrong", was not a crime, as it had taken place in the context of the political conflict. Politicians from the Republic, along with the Irish media strongly attacked McLaughlin's comments.
On 10 February 2005, the government-appointed Independent Monitoring Commission reported that it firmly supported the Police Service of Northern Ireland (PSNI) and Garda assessments that the IRA was responsible for the Northern Bank robbery and that certain senior members of Sinn Féin were also senior members of the IRA and would have had knowledge of and given approval to the carrying out of the robbery. Sinn Féin have argued that the IMC is not independent and the inclusion of former Alliance Party Leader John Alderdice and a British security head was proof of this. The IMC recommended further financial sanctions against Sinn Féin members of the Northern Ireland Assembly. The British government responded by saying it would ask MPs to vote to withdraw the parliamentary allowances of the four Sinn Féin MPs elected in 2001.
Gerry Adams responded to the IMC report by challenging the Irish Government to have him arrested for IRA membership, a crime in both jurisdictions, and conspiracy.
On 20 February 2005, Irish Minister for Justice, Equality and Law Reform Michael McDowell publicly accused three of the Sinn Féin leadership, Gerry Adams, Martin McGuinness and Martin Ferris (TD for Kerry North) of being on the seven-man IRA Army Council which they later denied.
On 27 February 2005, a demonstration against the murder of Robert McCartney on 30 January 2005 was held in East Belfast. Alex Maskey, a former Sinn Féin Lord Mayor of Belfast, was told by relatives of McCartney to demand that Maskey "hand over the 12" IRA members involved. The McCartney family, though formerly Sinn Féin voters themselves, urged witnesses to the crime to contact the PSNI. Three IRA men were expelled from the organisation, and a man was charged with McCartney's murder.
Irish Taoiseach Bertie Ahern subsequently called Sinn Féin and the IRA "both sides of the same coin". The ostracism of Sinn Féin was shown in February 2005 when Dáil Éireann passed a motion condemning the party's alleged involvement in illegal activity. US President George W. Bush and Senator Edward Kennedy refused to meet Gerry Adams while meeting the family of Robert McCartney.
On 10 March 2005, the British House of Commons in London passed a motion placed by the British Government to withdraw the allowances of the four Sinn Féin MPs for one year in response to the Northern Bank Robbery without significant opposition. This measure cost the party approximately £400,000. However, the debate prior to the vote mainly surrounded the more recent events connected with the murder of Robert McCartney. Conservatives and Unionists put down amendments to have the Sinn Féin MPs evicted from their offices at the House of Commons but these were defeated.
In March 2005, Mitchell Reiss, the United States special envoy to Northern Ireland, condemned the party's links to the IRA, saying "it is hard to understand how a European country in the year 2005 can have a private army associated with a political party".
Policy and ideology.
Most of the party's policies are intended to be implemented on an 'all-Ireland' basis which further emphasises their central aim of creating a united Ireland.
Sinn Féin is considered a democratic socialist or left-wing party. In the European parliament, the party aligns itself with the European United Left–Nordic Green Left (GUE/NGL) parliamentary group. The party pledges support for minority rights, migrants' rights, and eradicating poverty. Although it is not in favour of the extension of legalised abortion (British 1967 Act) to Northern Ireland, Sinn Féin state they are opposed to the attitudes in society which "pressurise women" to have abortions and "criminalise" women who make this decision. The party does state that in cases of incest, rape, sexual abuse, or when a woman's life and health are at risk or in danger that the final decision must rest with the woman.
Sinn Féin is considered to be Eurosceptic, and urged a "No" vote in the referendum held in Ireland in 2008 on the Lisbon Treaty. In 2015, Sinn Féin are beginning to become more Pro-European - their 2015 Westminster manifesto pledges that the party will campaign to stay within the EU and that there will be a separate and binding referendum for Northern Ireland if the rest of the United Kingdom are to ever have one. Sinn Féin has said that if Northern Ireland were to exit the European Union "that would be absolutely economically disastrous for this island and particularly for us here in the North." 
Social and cultural.
Sinn Féin's main political goal is a united Ireland. Other key policies from their most recent election manifesto are listed below:
International relations.
Sinn Féin supports the creation of a 'Minister for Europe' – likely to be used in the Dáil. They support the independence of the Basque Country from Spain and France. Sinn Féin support the Palestinians in the Israeli–Palestinian conflict.
Organisational structure.
Sinn Féin is organised throughout Ireland, and membership is open to all Irish residents over the age of 16. The party is organised hierarchically into cumainn (branches), comhairle ceantair (district executives), cúigí (regional executives). At national level, the Coiste Seasta (Standing Committee) oversees the day-to-day running of Sinn Féin. It is an eight-member body nominated by the Sinn Féin Ard Chomhairle (National Executive) and also includes the chairperson of each cúige. The Sinn Féin Ard Chomhairle meets at least once a month. It directs the overall implementation of Sinn Féin policy and activities of the party.
The Ard Chomhairle also oversees the operation of various departments of Sinn Féin, viz Administration, Finance, National Organiser, Campaigns, Sinn Féin Republican Youth, Women's Forum, Culture, Publicity and International Affairs. It is made up of the following: Officer Board and nine other members, all of whom are elected by delegates to the Ard Fheis, fifteen representing the five Cúige regions (three delegates each). The Ard Chomhairle can co-opt eight members for specific posts and additional members can be co-opted, if necessary, to ensure that at least thirty per cent of Ard Chomhairle members are women.
The Ardfheis (national delegate conference) is the ultimate policy-making body of the party where delegates – directly elected by members of cumainn – can decide on and implement policy. It is held at least once a year but a special Ard Fheis can be called by the Ard Chomhairle or the membership under special circumstances.
Ard Chomhairle (National Executive) Officer Board.
2010–2011:
Ard Chomhairle (Sinn Féin Leadership) Members elected at the Ard Fhéis 2012.
Six Men
Six Women
General election results.
Northern Ireland.
Westminister elections.
Sinn Féin returned to Northern Ireland politics at the 1982 Assembly elections, winning five seats with 64,191 votes (10.1%). The party narrowly missed winning additional seats in Belfast North and Fermanagh and South Tyrone. In the 1983 Westminster elections eight months later Sinn Féin increased its support, breaking the hundred thousand vote barrier for the first time by polling 102,701 votes (13.4%). Gerry Adams won the Belfast West constituency with Danny Morrison only 78 votes short of victory in Mid Ulster.
The 1984 European elections proved to be a disappointment with Sinn Féin's candidate Danny Morrison polling 91,476 (13.3%) and falling well behind the SDLP candidate John Hume.
By the beginning of 1985 Sinn Féin had won their first representation on local councils due to three by-election wins in Omagh (Seamus Kerr, May 1983) and Belfast (Alex Maskey in June 1983 and Sean McKnight in March 1984). Three sitting councillors also defected to Sinn Féin in Dungannon, Fermanagh and Derry (the last defecting from the SDLP). Sinn Féin succeeded in winning 59 seats in the 1985 local government elections. Originally the party had predicted winning only 40 seats. However the results continued to show a decline from the peak of 1983 as the party won 75,686 votes (11.8%). The party failed to gain any seats in the 1986 by-elections caused by the resignation of Unionist MPs in protest at the Anglo-Irish Agreement, partly this was due to an electoral pact between Unionist candidates, however the SF vote fell in the four constituencies they contested.
In the 1987 election Gerry Adams held his Belfast West seat but the party elsewhere failed to make breakthroughs and overall polled 83,389 votes (11.4%). The same year saw the party contest the Dáil election in the Republic of Ireland, however they failed to win any seats and polled less than 2%.
The 1989 local government elections saw a drop in support for Sinn Féin. Defending 58 seats (the 59 won in 1985 plus two 1987 by-election gains in West Belfast minus three councillors who had defected to Republican Sinn Féin in 1986) the party lost 15 seats. In the aftermath of the election Mitchell McLaughlin admitted that recent IRA activity had affected the Sinn Féin vote.
In the 1989 European elections, candidate Danny Morrison again failed to win a seat, polling at 48,914 votes (9%).
The nadir for SF in this period came in 1992, with Gerry Adams losing his Belfast West seat to the SDLP and the SF vote falling in the other constituencies that they had contested relative to 1987.
In the 1997 British General Election, Gerry Adams regained his Belfast West seat. Martin McGuinness also won a seat in Mid Ulster. In Irish elections the same year the party won its first seat since the 1957 elections with Caoimhghín Ó Caoláin gaining a seat in the Cavan-Monaghan constituency. In the Irish local elections in 1999 the party increased its number of councillors from 7 to 23.
The party overtook its nationalist rival, the Social Democratic and Labour Party as the largest nationalist party in the 2001 Westminster general election and local elections, winning four Westminster seats to the SDLP's three. The party continues to subscribe, however, to an abstentionist policy towards the Westminster British parliament, on account of opposing that parliament's jurisdiction in Northern Ireland, as well as its oath to the Queen.
Results in Northern Ireland from UK General Elections. Sinn Féin increased its number of seats from two in 1997 to five in 2005, four of them in the west. It retained its five seats in 2010, but was reduced to four in 2015.
Sinn Féin increased its share of the nationalist vote in the 2003, 2007, and 2011 Assembly elections, with Martin McGuinness, former Minister for Education, taking the post of deputy First Minister in the Northern Ireland power-sharing Executive Committee. The party has three ministers in the Executive Committee.
In the 2010 General Election, the party retained its five seats, and for the first time topped the poll at a Westminster Election in Northern Ireland, winning 25.5% of the vote. All Sinn Féin MPs increased their share of the vote and with the exception of Fermanagh and South Tyrone, increased their majorities. In Fermanagh and South Tyrone, Unionist parties agreed a joint candidate, this resulted in the closest contest of the election, with Sinn Féin MP Michelle Gildernew holding her seat by 4 votes after 3 recounts and an election petition challenging the result.
Republic of Ireland.
Dáil Éireann elections.
The party had five TDs elected in the 2002 Republic general election, an increase of four from the previous election. At the general election in 2007 the party had expectations of substantial gains, with poll predictions that they would gain five to ten seats. However, the party lost one of its seats to Fine Gael. Seán Crowe, who had topped the poll in Dublin South–West fell to fifth place, with his first preference vote reduced from 20.28% to 12.16%.
On 26 November 2010, Pearse Doherty won a seat in the Donegal South–West by-election. It was the party's first by-election victory in the Republic of Ireland since 1925. After negotiations with the left wing Independent TDs Finian McGrath and Maureen O'Sullivan, a Technical Group was formed in the Dáil to give its members more speaking time.
In the 2011 Irish General Election the party made gains. All its sitting TDs were returned with Seán Crowe regaining the seat in Dublin South–West he lost in 2007. In addition to winning long time targeted seats such as Dublin Central and Dublin North–West the party gained unexpected seats in Cork East and Sligo–North Leitrim. It ultimately won 14 seats, the best performance for the party's current incarnation. The party went on to win three seats in the Seanad election which followed their success at the General Election.
Local Government elections.
Sinn Féin is represented on most county and city councils. It made large gains in the local elections of 2004, increasing its number of councillors from 21 to 54, and replacing the Progressive Democrats as the fourth-largest party in local government. At the local elections of June 2009, the party's vote fell by 0.95% to 7.34%, with no change in the number of seats. Losses in Dublin and urban areas were balanced by gains in areas such as Limerick, Wicklow, Cork, Tipperary and Kilkenny and the border counties . However three of Sinn Féin's seven representatives on Dublin City Council resigned within six months of the June 2009 elections, one of them defecting to the Labour Party.
European elections.
In the 2004 European Parliament election, Bairbre de Brún won Sinn Féin's first seat in the European Parliament, at the expense of the Social Democratic and Labour Party (SDLP). She came in second behind Jim Allister, then of the Democratic Unionist Party (DUP). In the 2009 election, de Brún was re-elected with 126,184 first preference votes, the only candidate to reach the quota on the first count. This was the first time since elections began in 1979 that the DUP failed to take the first seat, and was the first occasion Sinn Féin topped a poll in any Northern Ireland election.
Sinn Féin made a breakthrough in the Dublin constituency in 2004. The party's candidate, Mary Lou McDonald, was elected on the sixth count as one of four MEPs for Dublin, effectively taking the seat of Patricia McKenna of the Green Party. In the 2009 election, when Dublin's representation was reduced to three MEPs, she failed to hold her seat. In the South constituency their candidate, Councillor Toireasa Ferris, managed to nearly double the number of first preference votes, lying third after the first count, but failed to get enough transfers to win a seat.
In the 2014 election, Martina Anderson topped the poll in Northern Ireland, as did Lynn Boylan in Dublin. Liadh Ní Riada was elected in the South constituency, and Matt Carthy in Midlands–North-West.

</doc>
<doc id="28176" url="http://en.wikipedia.org/wiki?curid=28176" title="Willis Tower">
Willis Tower

The Willis Tower, built as and still commonly referred to as Sears Tower, is a 108-story, 1451 ft skyscraper in Chicago, Illinois, United States. At completion in 1973, it surpassed the World Trade Center towers in New York to become the tallest building in the world, a title it held for nearly 25 years. The Willis Tower is the second-tallest building in the United States and the 12th-tallest in the world. More than one million people visit its observation deck each year, making it one of Chicago's most popular tourist destinations. The structure was renamed in 2009 by the Willis Group as part of its lease on a portion of the tower's space.
s of 2013[ [update]], the building's largest tenant is United Airlines, which moved its corporate headquarters from the United Building at 77 West Wacker Drive in 2012 and today occupies around 20 floors with its headquarters and operations center.
The building's official address is 233 South Wacker Drive, Chicago, Illinois 60606.
History.
Planning and construction.
In 1969, Sears, Roebuck & Co. was the largest retailer in the world, with about 350,000 employees. Sears executives decided to consolidate the thousands of employees in offices distributed throughout the Chicago area into one building on the western edge of Chicago's Loop. Sears asked its outside counsel, Arnstein, Gluck, Weitzenfeld & Minow (now known as Arnstein & Lehr, LLP) to suggest a location. The firm consulted with local and federal authorities and the applicable law, then offered Sears two options: an area known as Goose Island and a two-block area bounded by Franklin Street on the east, Jackson Boulevard on the south, Wacker Drive on the west and Adams Street on the north, with Quincy Street running through the middle from east to west.
This latter site was decided upon, and preliminary inquiries determined that the necessary permits could be obtained and Quincy Street vacated. The next step was to acquire the property; a team of attorneys from the Arnstein law firm, headed by Andrew Adsit, began buying the property parcel by parcel. Sears purchased 15 old buildings from 100 owners and paid $2.7 million to the City of Chicago for the portion of Quincy Street that divided the property.
Sears, which needed 3000000 sqft of office space for its planned consolidation and predicted that growth would require yet more, commissioned architects Skidmore, Owings & Merrill (SOM) to produce a structure to be one of the largest office buildings in the world. Their team of architect Bruce Graham and structural engineer Fazlur Rahman Khan designed the building as nine square "tubes" (each essentially a separate building), clustered in a 3×3 matrix forming a square base with 225 ft sides. All nine tubes would rise up to the 50th floor of the building. At the 50th floor, the northwest and southeast tubes end, and the remaining seven continue up. At the 66th floor, the northeast and the southwest tubes end. At the 90th floor, the north, east, and south tubes end. The remaining west and center tubes continue up to the 108th floor.
The Willis Tower was the first building to use Khan's bundled tube structure. This innovative design was structurally efficient and economic: at 1,450 feet, it provided more space and rose higher than the Empire State Building, yet cost much less per unit area. This structural system would prove highly influential in skyscraper construction. It has been used in most supertall buildings since then, including the world's tallest building, the Burj Khalifa. To honor Khan's contributions, the Structural Engineers Association of Illinois commissioned a sculpture of him for the lobby of the Willis Tower.
Sears executives decided that the space they would immediately occupy should be efficiently designed to house their Merchandise Group, and that floor space for future growth would be rented out to smaller firms and businesses until Sears could retake it. The latter floor areas had to be designed to a smaller plate, with a high window-space to floor-space ratio, to be attractive and marketable to prospective lessees. Smaller floorplates required a taller structure to yield sufficient square footage. Skidmore architects proposed a tower with large 55000 sqft floors in the lower part of the building, and gradually tapered areas of floorplates in a series of setbacks, which would give the Sears Tower its distinctive look.
As Sears continued to offer optimistic projections for growth, the tower's proposed floor count rapidly increased into the low hundreds, surpassing the height of New York's unfinished World Trade Center to become the world's tallest building. The height was restricted by a limit imposed by the Federal Aviation Administration (FAA) to protect air traffic. The financing of the tower was provided by the Sears company. It was topped with two antennas to permit local television and radio broadcasts. Sears and the City of Chicago approved the design, and the first steel was put in place in April 1971. The structure was completed in May 1973. The construction cost about US$150 million at the time, equivalent to $ million in 2015. By comparison, Taipei 101, built in 2004 in Taiwan, cost around the equivalent of US$2.14 billion in 2014 dollars.
Black bands appear on the tower around the 29th–32nd, 64th–65th, 88th–89th, and 104th–108th floors. These are louvres that allow ventilation for service equipment and obscure the structure's belt trusses. Even though regulations did not require a fire sprinkler system, the building was equipped with one from the beginning. There are around 40,000 sprinkler heads in the building, installed at a cost of $4 million.
In February 1982, two television antennas were added to the structure, increasing its total height to 1707 ft. The western antenna was later extended, bringing the overall height to 1730 ft on June 5, 2000 to improve reception of local NBC station WMAQ-TV.
Suits filed to halt construction.
As the construction of the building neared the 50th floor, lawsuits for an injunction were filed seeking to stop the building from exceeding 67 floors. The suits alleged that above that point television reception would deteriorate and cause property values to plummet. The first suit was filed by the State’s Attorney in neighboring Lake County on March 17, 1972. A second suit was filed on March 28 in the Cook County Circuit Court by the Villages of Skokie, Northbrook and Deerfield, Illinois.
Sears filed motions to dismiss the Lake County and the Cook County lawsuits and on May 17, 1972, Judge LaVerne Dickson, Chief of the Lake County Circuit Court dismissed the suit, saying, “I find nothing that gives television viewers the right to reception without interference. They will have to find some other means of ensuring reception such as taller antennas.” The Lake County State’s Attorney filed a Notice of Appeal and the Supreme Court agreed to permit bypassing the appellate court and to hear the matter on an expedited basis. The State’s Attorney then asked the Illinois Supreme Court for a temporary injunction to stop the construction and his request was denied. On June 12, Judge Charles R. Barrett granted Sears’ motion to dismiss the suit filed by three Chicago suburbs on the ground that interference with television reception caused by construction of the Sears building did not violate constitutional rights and that the suburbs involved in the suit do not have any right to undistorted television reception. This decision, too, also was appealed and consolidated with the Lake County appeal with the Supreme Court of Illinois.
Meanwhile, an Illinois Citizens Committee for Broadcasting requested the Federal Communications Commission to halt construction on the grounds so that it would not interfere with area television reception. On May 26, 1972, the Commission declined to take action on the ground that it did not have jurisdiction to do so.
On June 30, 1972, the Illinois Supreme Court affirmed the previous rulings by Lake and Cook County Circuit Courts, by a letter order with a written opinion to follow. On September 8, 1972, the United States Court of Appeals for the Seventh Circuit upheld the decision by the Federal Communications Commission to dismiss the complaint brought by the Illinois Citizens Committee for Broadcasting charging that the building would drastically affect reception in Chicago land and requesting the FCC to halt construction. The Supreme Court of Illinois written opinion was filed on September 20, 1972. In affirming the judgments of lower courts the Court held, “Considering the foregoing, it is clear to us that absent legislation to the contrary defendant has a propriety right to construct a building to its desired height and that completion of the project would not constitute a nuisance under the circumstances of this case.” 
Post-opening.
Sears' optimistic growth projections were not met. Competition from its traditional rivals (like Montgomery Ward) continued, with new competition by retailing giants such as Kmart, Kohl's, and Walmart. The fortunes of Sears & Roebuck declined in the 1970s as the company lost market share; its management grew more cautious. Nor did the Sears Tower draw as many tenants as Sears had hoped. The tower stood half-vacant for a decade as a surplus of office space was erected in Chicago in the 1980s.
In 1990, the law firm of Keck, Mahin & Cate decided to move out of its space in the Sears Tower and into a development that would become 77 West Wacker Drive, rebuffing Sears' attempts to entice the firm to stay. Two years later, Sears began moving its own offices out of the Sears Tower.
In 1994, Sears sold the building to Boston-based AEW Capital Management, with financing from MetLife. At the time, it was one-third vacant. By 1995, Sears had completely left the building, moving to a new office campus in Hoffman Estates, Illinois.
In 1997, Toronto-based TrizecHahn Corporation (the owner of the CN Tower at the time) purchased the building for $110 million, and assumption of $4 million in liabilities, and a $734 million mortgage. In 2003, Trizec surrendered the building to lender MetLife.
In 2004, MetLife sold the building to a group of investors, including New York-based Joseph Chetrit, Joseph Moinian, Lloyd Goldman, Joseph Cayre and Jeffrey Feil, and Skokie, Illinois-based American Landmark Properties. The quoted price was $840 million, with $825 million held in a mortgage.
In June 2006, seven men were arrested by the FBI and charged with plotting to destroy the tower. Deputy FBI Director John Pistole described their plot as "more aspirational than operational". The case went to court in October 2007; after three trials, five of the suspects were convicted and two were acquitted. The alleged leader of the group, Narseal Batiste, was sentenced to 13½ years in prison in November 2009.
Plans.
In February 2009, the owners announced they were considering a plan to paint the structure silver; this plan was later dropped. The paint would have "rebranded" the building and highlighted its advances in energy efficiency. The estimated cost was $50 million.
Since 2007, the building owners have been considering building a hotel on the north side of Jackson, between Wacker and Franklin, at the plaza that is the entrance to the tower's observation deck. The tower's parking garage is beneath the plaza. Building owners say the second building was considered in the original design. The plan was eventually cancelled as city zoning does not permit construction of such a tall tower there.
Although Sears' naming rights expired in 2003, the building continued to be called the Sears Tower for several years. In March 2009, London-based insurance broker Willis Group Holdings agreed to lease a portion of the building, and obtained the building's naming rights. On July 16, 2009, the building was officially renamed Willis Tower. On August 13, 2012, United Airlines announced it would move its corporate headquarters from 77 West Wacker Drive to Willis Tower.
Skydeck.
The Willis Tower observation deck, called the Skydeck, opened on June 22, 1974. Located on the 103rd floor of the tower, it is 1353 ft high and is one of the most famous tourist attractions in Chicago. Tourists can experience how the building sways on a windy day. They can see far over the plains of Illinois and across Lake Michigan to Indiana, Michigan and Wisconsin on a clear day. Elevators take tourists to the top in about 60 seconds, and allow tourists to feel the pressure change as they rise up. The Skydeck competes with the John Hancock Center's observation floor a mile and a half away, which is 323 ft lower. Some 1.3 million tourists visit the Skydeck annually. A second Skydeck on the 99th floor is also used if the 103rd floor is closed. The tourist entrance can be found on the south side of the building along Jackson Boulevard.
In January 2009, Willis Tower's owners began a major renovation of the Skydeck, including the installation of retractable glass balconies, which can be extended approximately 4 ft from the facade of the 103rd floor, overlooking South Wacker Drive. The all-glass boxes, informally dubbed "The Ledge", allow visitors to look through the glass floor to the street 1353 ft below. The boxes, which can bear 5 ST of weight, opened to the public on July 2, 2009. However on May 29, 2014 the laminated glass covering the floor of one of the glass boxes shattered while visitors were sitting on it, but caused no injuries. The broken glass was replaced within days, and tourist operations resumed as before.
Panorama of Chicago skyline as seen from Willis Tower Skydeck
Height.
Willis Tower remains the second tallest building in the Americas (after One World Trade Center) and the Western Hemisphere. With a pinnacle height of 1729 ft, it is the third tallest freestanding structure in the Americas, as it is 86 ft shorter than Toronto's CN Tower. Willis Tower is the eighth-tallest freestanding structure in the world by pinnacle height.
At 1482.6 ft tall, including decorative spires, the Petronas Twin Towers in Kuala Lumpur, Malaysia, laid claim to replacing the Sears Tower as the tallest building in the world in 1998. Not everyone agreed, and in the ensuing controversy four different categories of "tallest building" were created. Of these, Petronas was the tallest in the first category (height to top of architectural elements, meaning spires but not antennas) giving it the title of world's tallest building.
Taipei 101 in Taiwan claimed the record in three of the four categories in 2004 to become recognized as the tallest building in the world. Taipei 101 surpassed the Petronas Twin Towers in spire height and the Sears Tower in roof height and highest occupied floor. The Sears Tower retained one record: its antenna exceeded Taipei 101's spire in height. In 2008, the Shanghai World Financial Center claimed the records of tallest building by roof and highest occupied floor.
On August 12, 2007, the Burj Khalifa in Dubai, United Arab Emirates was reported by its developers to have surpassed the Sears Tower in all height categories.
Upon completion, One World Trade Center in New York City surpassed Willis Tower through its structural and pinnacle heights, but not by roof, observation deck elevation or highest occupied floor.
Until 2000, the Sears Tower did not hold the record for the tallest building by pinnacle height. From 1969 to 1978, this record was held by the John Hancock Center, whose antenna reached a height of 1500 ft, or 49 ft taller than the Sears Tower's original height of 1451 ft. In 1978, One World Trade Center became taller by pinnacle height due to the addition of a 359 ft antenna, which brought its total height to 1727 ft. In 1982, two antennas were installed on top of the Sears Tower which brought its total height to 1707 ft, making it taller than the John Hancock Center but not One World Trade Center. However, the extension of the Sears Tower's western antenna in June 2000 to 1730 ft allowed it to just barely claim the title of tallest building by pinnacle height.
Climbing.
On May 25, 1981, Dan Goodwin, wearing a homemade Spider-Man suit while using suction cups, camming devices, and sky hooks, and despite several attempts by the Chicago Fire Department to stop him, made the first successful outside ascent of the Sears Tower. Goodwin was arrested at the top after the seven-hour climb and charged with trespassing. Goodwin stated that the reason he made the climb was to call attention to shortcomings in high-rise rescue and firefighting techniques. After a lengthy interrogation by Chicago's District Attorney and Fire Commissioner, Goodwin was released.
In August 1999, French urban climber Alain "Spiderman" Robert, using only his bare hands and bare feet, scaled the building's exterior glass and steel wall all the way to the top. A thick fog settled in near the end of his climb, making the last 20 stories of the building's glass and steel exterior slippery.
Naming rights.
Although Sears sold the Tower in 1994 and had completely vacated it by 1995, the company retained the naming rights to the building through 2003. The new owners were rebuffed in renaming deals with CDW Corp in 2005 and the U.S. Olympic Committee in 2008. London-based insurance broker Willis Group Holdings, Ltd. leased more than 140000 sqft of space on three floors in 2009. A Willis spokesman said the naming rights were obtained as part of the negotiations at no cost to Willis, and the building was renamed Willis Tower on July 16, 2009.
The naming rights are valid for 15 years, so it is possible that the building’s name could change again in 2024 or later. The "Chicago Tribune" joked that the building’s new name reminded them of the oft-repeated "What you talkin' 'bout, Willis?" catchphrase from the 1980s American television sitcom "Diff'rent Strokes" and considered the name-change ill-advised in "a city with a deep appreciation of tradition and a healthy ego, where some Chicagoans still mourn the switch from Marshall Field's to Macy's". This feeling was confirmed in a July 16, 2009 CNN article in which some Chicago area residents expressed reluctance to accept the Willis Tower name, and in an article that appeared in the October 2010 issue of "Chicago" magazine that ranked the building among Chicago's 40 most important, the author pointedly refused to acknowledge the name change and referred to the building as the "Sears Tower". "Time" magazine called the name change one of the top 10 worst corporate name changes and pointed to negative press coverage by local news outlets and online petitions from angry residents. The naming rights issue continued into 2013, when Eric Zorn noted in the "Chicago Tribune" that "We're stubborn about such things. This month marked four years since the former Sears Tower was re-christened Willis Tower, and the new name has yet to stick."
Broadcasting.
Many broadcast station transmitters are located at the top of Willis Tower. Each list is ranked by height from the top down. Stations at the same height on the same mast indicate the use of a diplexer into the same shared antenna. Due to its extreme height, FM stations (all class B) are very limited in power output.
Radio stations.
Also, NOAA Weather Radio station KWO39 transmits off the top of Willis Tower, at 162.550 MHz. KWO39, programmed by the National Weather Service Weather Forecast Office in Chicago, is equipped with Specific Area Message Encoding (SAME), which sets off a siren on specially-programmed weather radios to alert of an impending hazard, such as a tornado or civil emergency.
Cultural depictions.
Film and television.
The building has appeared in numerous films and television shows set in Chicago such as "Ferris Bueller's Day Off", where Ferris and company watch the streets of Chicago from the observation deck. The television show "Late Night with Conan O'Brien" introduced a character called The Sears Tower Dressed In Sears Clothing when the show visited Chicago in 2006. The building is also featured in History Channel's "Life After People", in which it and other human-made landmarks suffer from neglect without humans around, and it collapses two hundred years after people are gone. In an episode of the television series "Monk", Adrian Monk tries to conquer his fear of heights by imagining that he is on top of the Sears Tower. Also, in an episode of "Kenan and Kel", Kenan Rockmore and Kel Kimble decide to climb to the top of the Sears Tower, so that Kenan can declare his love for a girl.
In the movie "", the tower is damaged by a tornado.
In "1969", a Season 2 episode of the science-fiction series "Stargate SG-1", the SG-1 team accidentally travels back in time to the titular year. At one point, the team travels though Chicago and the Sears Tower is shown (erroneously, since construction did not begin on the tower until two years later in 1971).
In the 2004 film "I, Robot", the tower is shown updated in the year 2035 with new triangular antennas. The tower is shown surpassed in height by the USR (United States Robotics) Building.
In the 2011 film "", the tower is featured in a number of scenes. The most notable one is when the N.E.S.T team tries to enter the city using V-22 Osprey helicopters. They use Willis Tower for cover before using wing suits to descend into the city streets. In the movie, the tower is shown to be severely damaged by the Decepticon invasion of the city.
In the 2013 film "Man of Steel", the tower's interior and parts of its exterior portrayed the offices of the "Daily Planet".
In the film "Divergent", the tower is shown abandoned and decayed in a future Chicago.
Other.
Older versions of "Microsoft Flight Simulator" would begin with the player on the runway of Meigs Field, facing a virtual version of the tower.
In Sufjan Stevens' 2005 album "Illinois", the tower is referenced in the track "Seer's Tower", whose title is a play on the tower's now-former name, Sears Tower.

</doc>
<doc id="28177" url="http://en.wikipedia.org/wiki?curid=28177" title="Simony">
Simony

Simony (pron. [ˈsaɪ.mə.ni] or [ˈsɪ.mə.ni]) is the act of selling church offices and roles. The practice is named after Simon Magus, who is described in the Acts of the Apostles 8:9–24 as having offered two disciples of Jesus, Peter and John, payment in exchange for their empowering him to impart the power of the Holy Spirit to anyone on whom he would place his hands. The term also extends to other forms of trafficking for money in "spiritual things". Simony was also one of the important issues during the Investiture Controversy.
History.
Catholic church.
Although an offence against canon law, Simony became widespread in the Catholic Church in the 9th and 10th centuries. In the canon law, the word bears a more extended meaning than in English law. "Simony according to the canonists", says John Ayliffe in his "Parergon",
"...is defined to be a deliberate act or a premeditated will and desire of selling such things as are spiritual, or of anything annexed unto spirituals, by giving something of a temporal nature for the purchase thereof; or in other terms it is defined to be a commutation of a thing spiritual or annexed unto spirituals by giving something that is temporal."
In the "Corpus Juris Canonici" the Decretum and the Decretals deal with the subject. The offender whether "simoniacus" (one who had bought his orders) or "simoniace promotus" (one who had bought his promotion), was liable to deprivation of his benefice and deposition from orders if a secular priest, or to confinement in a stricter monastery if a regular. No distinction seems to have been drawn between the sale of an immediate and of a reversionary interest. The innocent "simoniace promotus" was, apart from dispensation, liable to the same penalties as though he were guilty.
Certain matters were simoniacal by the canon law which would not be so regarded in English law. So grave was the crime of simony considered that even infamous persons (deprived of citizens' rights due to conviction) could accuse another of it. English provincial and legatine constitutions continually assailed simony.
Church of England.
The Church of England also struggled with the practice after its separation from Rome. For the purposes of English law, simony is defined by William Blackstone as the corrupt presentation of any person to an ecclesiastical benefice for money, gift or reward. While English law recognized simony as an offence, it treated it as merely an ecclesiastical matter, rather than a crime, for which the punishment was forfeiture of the office or any advantage from the offence and severance of any patronage relationship with the person who bestowed the office. Both Edward VI of England and Elizabeth I promulgated statutes against simony. The cases of Bishop of St. David's Thomas Watson in 1699 and of Dean of York William Cockburn in 1841 were particularly notable.
By the Benefices Act 1892, a person guilty of simony is guilty of an offence for which he may be proceeded against under the Clergy Discipline Act 1892. An innocent clerk is under no disability, as he might be by the canon law. Simony may be committed in three ways – in promotion to orders, in presentation to a benefice, and in resignation of a benefice. The common law (with which the canon law is incorporated, as far as it is not contrary to the common or statute law or the prerogative of the Crown) has been considerably modified by statute. Where no statute applies to the case, the doctrines of the canon law may still be of authority.
s of 2011[ [update]], simony remains an offence. An unlawfully bestowed office can be declared void by the Crown, and the offender can be disabled from making future appointments and fined up to £1000. Clergy are no longer required to make a declaration as to simony on ordination, but offences are now likely to be dealt with under the Clergy Discipline Measure 2003, r.8.

</doc>
<doc id="28178" url="http://en.wikipedia.org/wiki?curid=28178" title="September 26">
September 26

September 26 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28179" url="http://en.wikipedia.org/wiki?curid=28179" title="Samaritans">
Samaritans

The Samaritans (Samaritan Hebrew: שוֹ‏מְרִים "Samerim" "Guardians/Keepers/Watchers [of the Law/Torah]", Jewish Hebrew: שומרונים‎ "Shomronim", Arabic: السامريون‎ "Sāmeriyyūn") are an ethnoreligious group of the Levant, descended from ancient Semitic inhabitants of the region.
The Samaritans are adherents of Samaritanism, an Abrahamic religion closely related to Judaism. Based on the Samaritan Pentateuch, Samaritans say that their worship is the true religion of the ancient Israelites prior to the Babylonian Exile, preserved by those who remained in the Land of Israel, as opposed to Judaism, which they say is a related but altered and amended religion, brought back by those returning from the Babylonian exile.
Ancestrally, Samaritans claim descent from the Israelite tribes of Ephraim and Manasseh (the two sons of Joseph) as well as from the priestly tribe of Levi, who have links to ancient Samaria from the period of their entry into the land of Canaan, while some suggest that it was from the beginning of the Babylonian Exile up to the Samaritan polity of Baba Rabba. Samaritans used to include a line of Benjamin tribe, but it went extinct during the decline period of the Samaritan demographics. The split between them and their brothers, the children of Judah (the Jews), began during the time of Eli the priest, and the culmination was during the Kingdom of Israel and Kingdom of Judah when the Samaritans (then Kingdom of Israel) refused to accept Jerusalem as the elect, and remained on Mount Gerizim. The Samaritans say that Mount Gerizim was the original Holy Place of Israel from the time that Joshua conquered Israel. The major issue between Jews and Samaritans has always been the location of the chosen place to worship God; Jerusalem according to the Jewish faith or Mount Gerizim according to the Samaritan version.
In the Talmud, a central post-exilic religious text of Judaism, the Samaritans are called "Cutheans" (Hebrew: כותים‎, "Kutim"), referring to the ancient city of Kutha, geographically located in what is today Iraq. In the biblical account, however, Cuthah was one of several cities from which people were brought to Samaria, and they worshiped Nergal. Modern genetics suggests some truth to both the claims of the Samaritans and the account in the Talmud.
Once a large community of over a million in late Roman times, the Samaritans shrank to several tens of thousands in the wake of the bloody suppression of the Third Samaritan Revolt (529 AD) against the Byzantine Christian rulers and mass conversion to Christianity under Byzantine rulers and to Islam under Arab and Turkish rulers.
As of January 1, 2015, the population was 775, divided between Kiryat Luza on Mount Gerizim and the city of Holon, just outside Tel Aviv. Most Samaritans in Israel and the West Bank today speak Hebrew and Arabic. For liturgical purposes, Samaritan Hebrew, Samaritan Aramaic, and Samaritan Arabic are used, all written in the Samaritan alphabet, a variant of the Old Hebrew alphabet, which is distinct from the Hebrew alphabet. Hebrew and later Aramaic were languages in use by the Jewish and Samaritan inhabitants of Judea prior to the Roman exile.
Although they are drafted into the Israel Defense Forces and considered by Rabbinical Judaism to be a branch of Jews, the Israeli Rabbinate requires Samaritans to officially go through formal Orthodox conversion in order to be recognized as Halakhic Jews in Israel. One example is Israeli TV personality Sofi Tsedaka, who formally converted to Judaism at the age of 18.
Etymology.
In Samaritan Hebrew the Samaritans call themselves "Samerim", which according to the Anchor Bible Dictionary, is derived from the Ancient Hebrew term "Šamerim/Samerim" שַמֶרִים, meaning "Guardians/Keepers/Watchers [of the Law/Torah]". Thus, it may suggest Samaria is named after the Samaritans, rather than the Samaritans being named after Samaria. In Jewish tradition, Mount Samaria, meaning "Watch Mountain", is named so because watchers used to watch from those mountains for approaching armies from Egypt from ancient times. Historically, Samaria was the key geographical concentration of the Samaritan community. In Jewish Hebrew however, the Samaritans are called "Shomronim", which would appear to simply mean "Samarians" ("inhabitants of Samaria", Samaria in Jewish Hebrew being Shomron).
The Ancient Hebrew "Šamerim/Samerim" ("Samerin" سامرين in Arabic which have the same meaning), which in the Bible means "Guardians" (singular Šameri/Sameri) comes from the Hebrew root verb S-M-R שמר which means: "to watch", or "to guard".
That the etymology of the Samaritans' ethnonym in Samaritan Hebrew is derived from "Guardians/Keepers/Watchers [of the Law/Torah]" (to protect it from alteration against the Talmudic Rabbinic school), as opposed to Samaritans being named after the region of Samaria, is supported by Christian Church fathers Epiphanius of Salamis in Panarion, Jerome and Eusibius in Chronicon and Origen in The Commentary of Origen on S. John's Gospel, and in some Ancient Jewish Talmudic Bible Interpretations of Midrash Tanhuma on Genesis chapter 31, and Pirke De-Rabbi Eliezer chapter 38 Page 21.
History and origin.
Samaritan sources.
According to Samaritan tradition, Mount Gerizim was the original Holy Place of the Israelites from the time that Joshua conquered Canaan and the tribes of Israel settled the land. The reference to Mount Gerizim derives from the biblical story of Moses ordering Joshua to take the Twelve Tribes of Israel, to the mountains by Nablus and place half of the tribes, six in number, on the top of Mount Gerizim, the Mount of the Blessing, and the other half in Mount Ebal, the Mount of the Curse. The two mountains were used to symbolize the significance of the commandments and serve as a warning to whoever disobeyed them (Deut. 11:29; 27:12; Josh. 8:33).
Samaritans claim they are Israelite descendants of the Northern Israelite tribes of Ephraim and Manasseh, who survived the destruction of the Northern Kingdom of Israel by the Assyrians in 722 BC.
Samaritan historiography places the basic schism from the remaining part of Israel after the tribes of Israel conquered and returned to the land of Canaan, led by Joshua. After Joshua's death, Eli the priest left the tabernacle which Moses erected in the desert and established on Mount Gerizim, and built another one under his own rule in the hills of Shiloh.
Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:
A terrible civil war broke out between Eli son of Yafni, of the line of Ithamar, and the sons of Pincus (Phinehas), because Eli son of Yafni resolved to usurp the High Priesthood from the descendants of Pincus. He used to offer sacrifices on an altar of stones. He was 50 years old, endowed with wealth and in charge of the treasury of the children of Israel. ...
He offered a sacrifice on the altar, but without salt, as if he were inattentive. When the Great High Priest Ozzi learned of this, and found the sacrifice was not accepted, he thoroughly disowned him; and it is (even) said that he rebuked him.
Thereupon he and the group that sympathized with him, rose in revolt and at once he and his followers and his beasts set off for Shiloh. Thus Israel split in factions. He sent to their leaders saying to them, "Anyone who would like to see wonderful things, let him come to me." Then he assembled a large group around him in Shiloh, and built a Temple for himself there; he constructed a place like the Temple (on Mount Gerizim). He built an altar, omitting no detail—it all corresponded to the original, piece by piece.
At this time the Children of Israel split into three factions. A loyal faction on Mount Gerizim; a heretical faction that followed false gods; and the faction that followed Eli son of Yafni on Shiloh.
Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources states:
And the children of Israel in his days divided into three groups. One did according to the abominations of the Gentiles and served other gods; another followed Eli the son of Yafni, although many of them turned away from him after he had revealed his intentions; and a third remained with the High Priest Uzzi ben Bukki, the chosen place.
Jewish sources.
The emergence of the Samaritans as an ethnic and religious community distinct from other Levant peoples appears to have occurred at some point after the Assyrian conquest of the Israelite Kingdom of Israel in approximately 721 BC. The records of Sargon II of Assyria indicate that he deported 27,290 inhabitants of the former kingdom.
Jewish tradition affirms the Assyrian deportations and replacement of the previous inhabitants by forced resettlement by other peoples, but claims a different ethnic origin for the Samaritans. The Talmud accounts for a people called "Cuthim" on a number of occasions, mentioning their arrival by the hands of the Assyrians. According to 2 Kings and Josephus the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians then brought people from Babylon, Cuthah, Avah, Emath, and Sepharvaim to place in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshipped both the God of the land and their own gods from the countries from which they came.
This account is contradicted by the version in Chronicles, where, following Samaria's destruction, King Hezekiah is depicted as endeavouring to draw the Ephraimites and Manassites closer to Judah. Temple repairs at the time of Josiah were financed by money from all "the remnant of Israel" in Samaria, including from Manasseh, Ephraim and Benjamin. Jeremiah likewise speaks of people from Shechem, Shiloh and Samaria who brought offerings of frankincense and grain to the house of the Lord. Chronicles makes no mention of an Assyrian resettlement. Yitzakh Magen argues that the version of Chronicles is perhaps closer to the historical truth, and that the Assyrian settlement was unsuccessful, a notable Israelite population remained in Samaria, part of which, following the conquest of Judah, fled south and settled there as refugees.
A Midrash (Genesis Rabbah Sect. 94) relates about an encounter between Rabbi Meir and a Samaritan. The story that developed includes the following dialogue:
Zertal dates the Assyrian onslaught at 721 BC to 647 BC and discusses three waves of imported settlers. He shows that Mesopotamian pottery in Samaritan territory cluster around the lands of Menasheh and that the type of pottery found was produced around 689 BC. Some date their split with the Jews to the time of Nehemiah, Ezra, and the building of the Second Temple in Jerusalem after the Babylonian exile. Returning exiles considered the Samaritans to be non-Israelites and, thus, not fit for this religious work.
The "Encyclopaedia Judaica" (under "Samaritans") summarizes both past and the present views on the Samaritans' origins. It says:
Until the middle of the 20th century it was customary to believe that the Samaritans originated from a mixture of the people living in Samaria and other peoples at the time of the conquest of Samaria by Assyria (722–721 BC). The Biblical account in II Kings 17 had long been the decisive source for the formulation of historical accounts of Samaritan origins. Reconsideration of this passage, however, has led to more attention being paid to the Chronicles of the Samaritans themselves. With the publication of Chronicle II (Sefer ha-Yamim), the fullest Samaritan version of their own history became available: the chronicles, and a variety of non-Samaritan materials.
According to the former, the Samaritans are the direct descendants of the Joseph tribes, Ephraim and Manasseh, and until the 17th century AD they possessed a high priesthood descending directly from Aaron through Eleazar and Phinehas. They claim to have continuously occupied their ancient territory and to have been at peace with other Israelite tribes until the time when Eli disrupted the Northern cult by moving from Shechem to Shiloh and attracting some northern Israelites to his new followers there. For the Samaritans, this was the 'schism' par excellence.("Samaritans" in "Encyclopaedia Judaica", 1972, Volume 14, op. cit., col. 727.)
Furthermore, to this day the Samaritans claim descent from the tribe of Joseph:
The laymen also possess their traditional claims. They are all of the tribe of Joseph, except those of the tribe of Benjamin, but this traditional branch of people, which, the Chronicles assert, was established at Gaza in earlier days, seems to have disappeared. There exists an aristocratic feeling amongst the different families in this community, and some are very proud over their pedigree and the great men it had produced.(J. A. Montgomery, "The Samaritans The Earliest Jewish Sect: Their History, Theology And Literature", 1907, op. cit., p. 32.)
Dead Sea scrolls.
The Dead Sea scroll 4Q372 hopes that the northern tribes will return to the land of Joseph. The current dwellers in the north are referred to as fools, an enemy people. However they are not referred to as foreigners. It goes on to say that the Samaritans mocked Jerusalem and built a temple on a high place to provoke Israel.
Tensions between the Samaritans and the Judeans.
The narratives in Genesis about the rivalries among the twelve sons of Jacob describe tensions between north and south. They were temporarily united in the United Monarchy, but after the death of Solomon the kingdom split in two, the Kingdom of Israel with its capital Samaria and the Kingdom of Judea with its capital Jerusalem.
The Deuteronomistic history, written in Judah, portrayed Israel as a sinful kingdom, divinely punished for its idolatry and iniquity by being destroyed by the Assyrians in 720 BC.
The tensions continued in the postexilic period. Chronicles is more inclusive than Ezra–Nehemiah since for the Chronicler the ideal is of one Israel with twelve tribes; the Chronicler concentrates on Judah and ignores northern Israel.
The Samaritans claimed that they were the true Israel who were descendants of the "Ten Lost Tribes" taken into Assyrian captivity. They had their own temple on Mount Gerizim and claimed that it was the original sanctuary. Moreover, they claimed that their version of the Pentateuch was the original and that the Jews had a falsified text produced by Ezra during the Babylonian exile.
Both Jewish and Samaritan religious leaders taught that it was wrong to have any contact with the opposite group, and neither was to enter each other's territories or even to speak to one another. During the New Testament period, although the tensions went unrecognized by Roman authorities, Josephus reports numerous violent confrontations between Jews and Samaritans throughout the first half of the first century.
Rejection by Judeans.
According to the Jewish version of events, when the Judean exile ended in 538 BC and the exiles began returning home from Babylon, they found their former homeland populated by other people who claimed the land as their own and Jerusalem, their former glorious capital, in ruins. The inhabitants worshiped the Pagan gods, but when the then-sparsely populated areas became infested with dangerous wild beasts, they appealed to the king of Assyria for Israelite priests to instruct them on how to worship the "God of that country." The result was a syncretistic religion, in which national groups worshiped the Hebrew God, but they also served their own gods in accordance with the customs of the nations from which they had been brought.
According to , the Persian emperor, Cyrus the Great (reigned 559 BC – 530 BC), permitted the return of the exiles to their homeland and ordered the rebuilding of the Temple in Jerusalem (Zion). The prophet Isaiah identified Cyrus as "the Lord's Messiah" (Mashiach; see ). The word "Messiah" refers to an anointed one, such as a king or priest.
 says that the local inhabitants of the land offered to assist with the building of the new temple during the time of Zerubbabel, but their offer was rejected. According to Ezra, this rejection precipitated a further interference not only with the rebuilding of the temple but also with the reconstruction of Jerusalem.
The text is not clear on this matter, but one possibility is that these "people of the land" were thought of as Samaritans. We do know that Samaritan and Jewish alienation increased, and that the Samaritans eventually built their own temple on Mount Gerizim, near Shechem.
The rebuilding of the Jewish Temple in Jerusalem took several decades. The project was first led by Sheshbazzar (about 538 BC), later by Zerubbabel and Jeshua, and later still by Haggai and Zechariah (520–515 BC). The work was completed in 515 BC.
The term "Cuthim" applied by Jews to the Samaritans had clear pejorative connotations, implying that they were interlopers brought in from Kutha in Mesopotamia and rejecting their claim of descent from the ancient Tribes of Israel.
Assyrian account of the conquest and settlement of Samaria.
However, the following account of the Assyrian kings, which was among the archaeological discoveries in Babylon, differs from the Samaritan account, and confirms much of the Jewish biblical account but may differ in regard to the ethnicity of the foreigners settled in Samaria by Assyria. At one point it is simply said that they were from Arabia, while at another, that they were brought from a number of countries conquered by Sargon II:
[the Samar]ians [who had agreed with a hostile king] ... I fought with them and decisively defeated them] ... carried off as spoil. 50 chariots for my royal force ... [the rest of them I settled in the midst of Assyria]. ... The Tamudi, Ibadidi, Marsimani and Hayappa, who live in distant Arabia, in the desert, who knew neither overseer nor commander, who never brought tribute to any king--with the help of Ashshur my lord, I defeated them. I deported the rest of them. I settled them in Samaria/Samerina. (Sargon II Inscriptions, COS 2.118A, p. 293)
Also,
The inhabitants of Samaria/Samerina, who agreed [and plotted] with a king [hostile to] me, not to do service and not to bring tribute [to Ashshur] and who did battle, I fought against them with the power of the great gods, my lords. I counted as spoil 27,280 people, together with their chariots, and gods, in which they trusted. I formed a unit with 200 of [their] chariots for my royal force. I settled the rest of them in the midst of Assyria. I repopulated Samaria/Samerina more than before. I brought into it people from countries conquered by my hands. I appointed my eunuch as governor over them. And I counted them as Assyrians. (Nimrud Prisms, COS 2.118D, pp. 295-296)
Further history.
Temple on Mount Gerizim.
Archaeological excavations at Mount Gerizim indicate that a Samaritan temple was built there in the first half of the 5th century BC. The date of the schism between Samaritans and Jews is unknown, but by the early 4th century BC the communities seem to have had distinctive practices and communal separation.
According to Samaritans, it was on Mount Gerizim that Abraham was commanded by God to offer Isaac, his son, as a sacrifice . In both narratives, God then causes the sacrifice to be interrupted, explaining that this was the ultimate test of Abraham's obedience, as a result of which all the world would receive blessing.
The Torah mentions the place where God shall choose to establish His name (Deut 12:5), and Judaism takes this to refer to Jerusalem. However, the Samaritan text speaks of the place where God "has chosen" to establish His name, and Samaritans identify it as Mount Gerizim, making it the focus of their spiritual values.
The legitimacy of the Samaritan temple was attacked by Jewish scholars including Andronicus ben Meshullam.
In the Christian Bible, the Gospel of John relates an encounter between a Samaritan woman and Jesus in which she says that the mountain was the center of their worship .
Antiochus IV Epiphanes and Hellenization.
In the 2nd century BC a series of events led to a revolution of some Judeans against Antiochus IV.
Antiochus IV Epiphanes was on the throne of the Seleucid Empire from 175 to 163 BC. His policy was to Hellenize his entire kingdom and standardize religious observance. According to 1 Maccabees 1:41-50 he proclaimed himself the incarnation of the Greek god Zeus and mandated death to anyone who refused to worship him.
The universal peril led the Samaritans, eager for safety, to repudiate all connection and kinship with the Jews. The request was granted. This was put forth as the final breach between the two groups, being alleged at a much later date in the Christian Bible (John 4:9), "For Jews have no dealings with Samaritans" - or not "alleged" if the Greek sunchrasthai merely refers to not sharing utensils (NABRE).
Anderson notes that during the reign of Antiochus IV (175–164 BC):
the Samaritan temple was renamed either Zeus Hellenios (willingly by the Samaritans according to Josephus) or, more likely, Zeus Xenios, (unwillingly in accord with 2 Macc. 6:2) Bromiley, 4.304).
Josephus Book 12, Chapter 5 quotes the Samaritans as saying:
We therefore beseech thee, our benefactor and saviour, to give order to Apolonius, the governor of this part of the country, and to Nicanor, the procurator of thy affairs, to give us no disturbances, nor to lay to our charge what the Jews are accused for, since we are aliens from their nation and from their customs, but let our temple which at present hath no name at all, be named the Temple of Jupiter Hellenius.
Shortly afterwards, the Greek king sent Gerontes the Athenian to force the Jews of Israel to violate their ancestral customs and live no longer by the laws of God; and to profane the Temple in Jerusalem and dedicate it to Olympian Zeus, and the one on Mount Gerizim to Zeus, Patron of Strangers, as the inhabitants of the latter place had requested. —II Maccabees 6:1–2
This Samaritan Temple at Mount Gerizim was destroyed by John Hyrcanus in about 128 BC, having existed about 200 years. Only a few stone remnants of it exist today.
164 BC and after.
During the Hellenistic period, Samaria was largely divided between a Hellenizing faction based in Samaria (Sebastaea) and a pious faction, led by the High Priest and based largely around Shechem and the rural areas. Samaria was a largely autonomous state nominally dependent on the Seleucid Empire until around 129 BC, when the Jewish Hasmonean king Yohanan Girhan (John Hyrcanus) destroyed the Samaritan temple and devastated Samaria.
Roman period.
Under the Roman Empire, Samaria was a part of the Roman-ruled province of Judaea.
Samaritans appear briefly in the Christian gospels, most notably in the account of the Samaritan woman at the well and the parable of the Good Samaritan. In the latter it is only the Samaritan who helped the man stripped of clothing, beaten, and left on the road half dead, his Abrahamic covenantal circumcision implicitly evident. The priest and Levite each ignored his obligation and walked past. But the Samaritan helped the naked man regardless of which Israelite sect he was.
This period is considered as something of a golden age for the Samaritan community, the population thought to number up to a million. The Temple of Gerizim was rebuilt after the Bar Kochba revolt against the Romans, around 135 AD. Much of Samaritan liturgy was set by the high priest Baba Rabba in the 4th century.
A building excavated on Delos, dating to the 2nd century BC, is commonly identified as a Samaritan synagogue, which would make it the oldest known Jewish or Samaritan synagogue. On the other hand, Matassa argues that, although there is evidence of Samaritans on Delos, there is no evidence the building was a synagogue.
There were some Samaritans in the Persian Empire, where they served in the Sassanid army.
Byzantine times.
According to Samaritan sources, Eastern Roman Emperor Zeno (who ruled 474-491 and whom the sources call "Zait the King of Edom") persecuted the Samaritans. The Emperor went to Sichem ("Neapolis"), gathered the elders and asked them to convert; when they refused, Zeno had many Samaritans killed, and re-built the synagogue to a church. Zeno then took for himself Mount Gerizim, where the Samaritans worshipped God, and built several edifices, among whom a tomb for his recently deceased son, on which he put a cross, so that the Samaritans, worshipping God, would prostrate in front of the tomb. Later, in 484, the Samaritans revolted. The rebels attacked Sichem, burnt five churches built on Samaritan holy places and cut the finger of bishop Terebinthus, who was officiating the ceremony of Pentecost. They elected a Justa (or Justasa/Justasus) as their king and moved to Caesarea, where a noteworthy Samaritan community lived. Here several Christians were killed and the church of St. Sebastian was destroyed. Justa celebrated the victory with games in the circus. According to John Malalas, the "dux Palaestinae" Asclepiades, whose troops were reinforced by the Caesarea-based Arcadiani of Rheges, defeated Justa, killed him and sent his head to Zeno. According to Procopius, Terebinthus went to Zeno to ask for revenge; the Emperor personally went to Samaria to quell the rebellion.
Modern historians believe that the order of the facts preserved by Samaritan sources should be inverted, as the persecution of Zeno was a consequence of the rebellion rather than its cause, and should have happened after 484, around 489. Zeno rebuilt the church of St. Procopius in Neapolis (Sichem) and the Samaritans were banned from Mount Gerizim, on whose top a signalling tower was built to alert in case of civil unrest.
Under a charismatic, messianic figure named Julianus ben Sabar (or ben Sahir), the Samaritans launched a war to create their own independent state in 529. With the help of the Ghassanid Arabs, Emperor Justinian I crushed the revolt; tens of thousands of Samaritans died or were enslaved. The Samaritan faith was virtually outlawed thereafter by the Christian Byzantine Empire; from a population once at least in the hundreds of thousands, the Samaritan community dwindled to near extinction.
After the Muslim Conquests.
By the time of the Muslim Conquests, Samaritans were living in an area stretching between Egypt, Syria, and Iran. Like other non-Muslims in the empire, such as Jews, Samaritans were considered to be People of the Book.
Their minority status was protected by the Muslim rulers, and they had the right to practice their religion, but, as dhimmi, adult males had to pay the jizya or "protection tax".
It has been suggested that they were forced to wear red colored turbans as a result of the terms of a document known as the Pact of Umar II, but this stipulation is not explicitly mentioned in the document, the authenticity has been questioned by contemporary scholars, and the tradition cannot be independently verified.
During the Crusades, Samaritans, like the other non-Latin Christian inhabitants of the Kingdom of Jerusalem, were second-class citizens, but they were tolerated and perhaps favoured because they were docile and had been mentioned positively in the Christian New Testament.
Over the centuries of Byzantine, Arab and Turkish rule, the Samaritans suffered many hardships which included forced conversion to Christianity, forced conversion to Islam, harsh religious decrees, massacre and persecution.
While the majority of the Samaritan population in Damascus was killed or converted during the reign of the Ottoman Pasha Mardam Beq in the early 17th century, the remainder of the Samaritan communities from Damascus and the other cities where they had a presence moved to Shechem, due to its close proximity to Mount Gerizim.
The Shechem community endured because most of the surviving diaspora returned, and they have maintained a tiny presence there to this day. In 1624, the last Samaritan High Priest of the line of Eleazar son of Aaron died without issue, but descendants of Aaron's other son, Ithamar, remained and took over the office.
The situation of the Samaritan community improved significantly during the British Mandate of Palestine. At that time, they began to work in the public sector, like many other groups. During the thirties one of the Samaritans, Tawfeek Khadir al-Kahen, was nominated as member of the Shechem Municipality. The censuses of 1922 and 1931 recorded 163 and 182 Samaritans in Palestine, respectively. The majority of them lived in Nablus.
Samaritan origins of Palestinian Muslims in Nablus.
Much of the local Palestinian population of Nablus is believed to be descended from Samaritans who converted to Islam. According to the historian Fayyad Altif, large numbers of Samaritans converted due to persecution under various Muslim rulers, and because the monotheistic nature of Islam made it easy for them to accept it. The Samaritans themselves describe the Ottoman period as the worst period in their modern history, as many Samaritan families were forced to convert their religion to Islam during that time. Even today, certain Nabulsi family names such as Muslimani, Yaish, and Shakshir among others, are associated with Samaritan ancestry.
For the Samaritans in particular, the passing of the al-Hakem Edict by the Fatimids in 1021, under which all Jews and Christians in the Fatimid ruled southern Levant were ordered to either convert to Islam or leave, along with another notable forced conversion to Islam imposed at the hands of the rebel Ibn Firāsa, would contribute to their rapid unprecedented decrease, and ultimately almost complete extinction as a separate religious community. As a result, they have decreased from more than a million in late Roman (Byzantine) times to 150 people by the end of the Ottoman Era.
In 1940, the future Israeli president and historian Yitzhak Ben-Zvi wrote an article in which he stated that two thirds of the residents of Nablus and the surrounding neighboring villages are of Samaritan origin. He mentioned the name of several Palestinian Muslim families as having Samaritan origins, including the Buwarda and Kasem families, who protected Samaritans from Muslim persecution in the 1850s. He further claimed that these families had written records testifying to their Samaritan ancestry, which were maintained by their priests and elders.
Genetic studies.
Demographic investigation.
Demographic investigations of the Samaritan community were carried out in the 1960s. Detailed pedigrees of the last 13 generations show that the Samaritans comprise four lineages:
Y-DNA and mtDNA comparisons.
Recently several genetic studies on the Samaritan population were made using haplogroup comparisons as well as wide-genome genetic studies. Of the 12 Samaritan males used in the analysis, 10 (83%) had Y chromosomes belonging to haplogroup J, which includes three of the four Samaritan families. The Joshua-Marhiv family belongs to haplogroup J1, while the Danfi and Tsedakah families belong to haplogroup J2, and can be further distinguished by M67, the derived allele of which has been found in the Danfi family. The only Samaritan family not found in haplogroup J was the Cohen family (Tradition: Tribe of Levi) which was found in haplogroup E3b1a M78. This article predated the change of the classification of haplogroup E3b1-M78 to E3b1a-M78 and the further subdivision of E3b1a-M78 into 6 subclades based on the research of Cruciani, et al.
The 2004 article on the genetic ancestry of the Samaritans by Shen "et al." concluded from a sample comparing Samaritans to several Jewish populations, all currently living in Israel—representing Ethiopian Jews, Ashkenazi Jews, Iraqi Jews, Libyan Jews, Moroccan Jews, and Yemenite Jews, as well as Israeli Druze and Palestinian Arabs—that "the principal components analysis suggested a common ancestry of Samaritan and Jewish patrilineages. Most of the former may be traced back to a common ancestor in what is today identified as the paternally inherited Israelite high priesthood (Cohanim) with a common ancestor projected to the time of the Assyrian conquest of the kingdom of Israel."
Archaeologists Aharoni, et al., estimated that this "exile of peoples to and from Israel under the Assyrians" took place during ca. 734 BC to 712 BC. The authors speculated that when the Assyrians conquered the northern kingdom of Israel, resulting in the exile of many of the Israelites, a subgroup of the Israelites that remained in the Land of Israel "married Assyrian and female exiles relocated from other conquered lands, which was a typical Assyrian policy to obliterate national identities." The study goes on to say that "Such a scenario could explain why Samaritan Y chromosome lineages cluster tightly with Jewish Y lineages, while their mitochondrial lineages are closest to Iraqi Jewish and Palestinian mtDNA sequences." Non-Jewish Iraqis were not sampled in this study; however, mitochondrial lineages of Jewish communities tend to correlate with their non-Jewish host populations, unlike paternal lineages which almost always correspond to Israelite lineages.
Genetic differences between the Samaritans and neighboring Jewish and non-Jewish populations are corroborated in that study of 7,280 bp of non-recombining Y-chromosome and 5,622 bp of coding and hypervariable segment (HVS-I) mitochondrial DNA (mtDNA) sequences. Comparative sequence analysis was carried out on 12 Samaritan Y-chromosome and mtDNA samples from 9 male and 7 female Samaritans separated by at least two generations. The four Samaritan families clustered to four distinct Y-chromosome haplogroups according to their patrilineal identity. Of the 16 Samaritan mtDNA samples, 14 carry either of two mitochondrial haplotypes that are rare or absent among other worldwide ethnic groups.
Modern times.
As of January 1, 2012, there were 751 Samaritans, half of whom reside in their modern homes at Kiryat Luza on Mount Gerizim, which is sacred to them, and the rest in the city of Holon, just outside Tel Aviv. There are also four Samaritan families residing in Binyamina-Giv'at Ada, Matan and Ashdod.
After the end of the British Mandate of Palestine and the subsequent establishment of the State of Israel, some of the Samaritans who were living in Jaffa emigrated to the West Bank and lived in Nablus. But by the late 1950s, around 100 Samaritans left the West Bank for Israel under an agreement with the Jordanian authorities in the West Bank.
Until the 1980s, most of the Samaritans resided in the Samarian town of Nablus below Mount Gerizim. They relocated to the mountain itself near the Israeli settlement neighborhood of Har Brakha as a result of violence during the First Intifada (1987–1990). Consequently, all that is left of the Samaritan community in Nablus/Shechem itself is an abandoned synagogue. The Israeli army maintains a presence in the area.
Relations of Samaritans with Jewish Israelis and Muslim and Christian Palestinians in neighboring areas have been mixed. In 1954, Israeli President Yitzhak Ben-Zvi fostered a Samaritan enclave in Holon, Israel. Samaritans living in both Israel and in the West Bank enjoy Israeli citizenship. Samaritans in the Palestinian Authority-ruled territories are a minority in the midst of a Muslim majority, although the Samaritans are a recognized minority along with Christians and Jews. In Israel the Samaritans operate without the status of a recognised religion. They had a reserved seat in the Palestinian Legislative Council in the election of 1996, but they no longer have one. Palestinian Samaritans have been granted passports by both Israel and the Palestinian Authority.
Samaritan communities tend to be more politically aligned with Israel, regardless of whether they live in Jewish-majority or Arab-majority areas. However, Al-Kahen Wasef al-Samery, a Samaritan leader, declared in 1960 that Israel is an enemy for them as it is an enemy for the Arabs. The Samaritans in Nablus often try to show the differences between them and the Jews, more so than those who live in Holon. Samaritans have stated that the military authorities do not treat them as a minority. On the contrary, they felt that they were treated like West Bank Arabs. Prior to 1948, the Samaritans were divided politically into two factions. The first was led by Sadaqa al-Kahen, who supported the Palestinian Arab leader Mohammad Amin al-Husayni, while the second faction was led by Wasef al-kahen, who supported another Palestinian Arab leader, Raghib al-Nashashibi.
As a small community physically divided between neighbors in a hostile region, Samaritans have been hesitant overtly to take sides in the Arab–Israeli conflict, fearing that doing so could lead to negative repercussions. While the Samaritan communities in both the West Bank's Nablus and Israeli Holon have assimilated to the surrounding culture, Hebrew has become the primary domestic language for Samaritans. Samaritans who are Israeli citizens are drafted into the military, along with the Jewish citizens of Israel.
Survival.
One of the biggest problems facing the community today is the issue of continuity. With such a small population, divided into only four families (Cohen, Tsedakah, Danfi and Marhib, a fifth family died out in the twentieth century) and a general refusal to accept converts, there has been a history of genetic disease within the group due to the small gene pool. To counter this, the Samaritan community has recently agreed that men from the community may marry non-Samaritan (primarily, Israeli Jewish) women, provided that the women agree to follow Samaritan religious practices. There is a six-month trial period prior to officially joining the Samaritan community to see whether this is a commitment that the woman would like to take. This often poses a problem for the women, who are typically less than eager to adopt the strict interpretation of biblical (Levitical) laws regarding menstruation, by which they must live in a separate dwelling during their periods and after childbirth. There have been a few instances of intermarriage. In addition, all marriages within the Samaritan community are first approved by a geneticist at Tel HaShomer Hospital, in order to prevent the spread of genetic disease. In meetings arranged by "international marriage agencies", a small number of Ukrainian women have recently been allowed to marry into the community in an effort to expand the gene pool.
The Israeli Ministry of Interior has refused to officially recognize inter-marriages between Jews and Samaritans.
The head of the community is the Samaritan High Priest, who is selected by age from the priestly family, and resides on Mount Gerizim. The current high priest is Aabed-El ben Asher ben Matzliach who assumed the office in 2013.
Samaritanism.
The Samaritan religion is based on some of the same books used as the basis of rabbinic Judaism, but differs from the latter. Samaritan scriptures include the Samaritan version of the Torah, the Memar Markah, the Samaritan liturgy, and Samaritan law codes and biblical commentaries. Samaritans appear to have texts of the Torah as old as the Masoretic Text and the Septuagint; scholars have various theories concerning the actual relationships between these three texts.
Religious beliefs.
The Samaritans retained the Ancient Hebrew script, the high priesthood, animal sacrifices, the eating of lambs at Passover, and the celebration of Aviv in spring as the New Year. Yom Teruah (the biblical name for Rosh Hashanah), at the beginning of Tishrei, is not considered a new year as it is in Judaism. Their main Torah text differs from the Masoretic Text, as well. Some differences are doctrinal: for example, the Samaritan Torah explicitly states that Mount Gerizim is "the place that God "has chosen"" for the Temple, as opposed to the Jewish Torah that refers to "the place that God "will" choose". Other differences are minor and seem more or less accidental.
Relationship to rabbinic Judaism.
Samaritans refer to themselves as "Bene Yisrael" ("Children of Israel") which is a term used by all Jewish denominations as a name for the Jewish people as a whole. They however do not refer to themselves as "Yehudim" (Judeans), the standard Hebrew name for Jews.
The Talmudic attitude expressed in tractate Kutim is that they are to be treated as Jews in matters where their practice coincides with rabbinic Judaism but as non-Jews where their practice differs. Since the 19th century, rabbinic Judaism has regarded the Samaritans as a Jewish sect and the term "Samaritan Jews" has been used for them.
Religious texts.
Samaritan law is not the same as halakha (Rabbinical Jewish law). The Samaritans have several groups of religious texts, which correspond to Jewish halakhah. A few examples of such texts are:
Christian sources: New Testament.
Samaria or Samaritans are mentioned in the New Testament books of Matthew, Luke, John and Acts. The Gospel of Mark contains no mention of Samaritans or Samaria. The best known reference to the Samaritans is the Parable of the Good Samaritan, found in the book of Luke. The following references are found:
The rest of the New Testament makes no specific mention of Samaria or Samaritans.
Media.
"The Samaritan News", a monthly magazine started in 1969, is written in Samaritan, Hebrew, Arabic, and English and deals with current and historical issues with which the Samaritan community is concerned. The "Samaritan Update" is a bi-monthly e-newsletter for Samaritan Studies.
Further reading.
</dl>

</doc>
<doc id="28180" url="http://en.wikipedia.org/wiki?curid=28180" title="Seneca Lake (New York)">
Seneca Lake (New York)

Seneca Lake is the largest of the glacial Finger Lakes of the U.S. state of New York, and the deepest lake entirely within the state. It is promoted as being the lake trout capital of the world, and is host of the National Lake Trout Derby. Because of its depth and relative ease of access, the US Navy uses Seneca Lake to perform test and evaluation of equipment ranging from single element transducers to complex sonar arrays and systems.
The lake takes its name from the Seneca nation of Native Americans. At the north end of Seneca Lake is the city of Geneva, New York, home of Hobart and William Smith Colleges and the New York State Agricultural Experiment Station, a division of Cornell University. At the south end of the lake is the village of Watkins Glen, New York, famed for auto racing and waterfalls.
Due to Seneca Lake's unique macroclimate it is home to over 50 wineries, many of them farm wineries and is the location of the Seneca Lake AVA. (See Seneca Lake wine trail).
Description.
At 38 mi long, it is the second longest of the Finger Lakes and has the largest volume, estimated at 4.2 trillion US gallons (16 km³), roughly half of the water in all the Finger Lakes. It has a maximum depth of 618 ft, and a mean depth of 291 ft. It has a surface area of 42800 acre.
The two main inlets are Catharine Creek at the southern end and the Keuka Lake Outlet. Seneca Lake outlets into the Cayuga-Seneca Canal, which joins Seneca and Cayuga Lakes at their northern ends.
It is fed by underground springs and replenished at a rate of 328,000 gallons (29,520 m³) per minute. These springs keep the water moving in a constant circular motion, giving it little chance to freeze over. Because of Seneca Lake's great depth its temperature remains a near-constant 39 °F. During the summer months however, the top 10 to does warm up to a pleasant 70 –.
Ecology.
Seneca lake has a typical aquatic population for large deep lakes in the northeast, with coldwater fish such as Lake Trout and Atlantic Salmon inhabiting the deeper waters, and warmwater fish such as Smallmouth Bass and Yellow Perch inhabiting the shallower areas. The lake is also home to a robust population of "Sawbellies", the local term for Gizzard shad.
History.
Over 200 years ago, there were Iroquois villages on Seneca Lake's surrounding hillsides. During the American Revolutionary War, their villages, including Kanadaseaga ("Seneca Castle"), were wiped out during the 1779 Sullivan Expedition by Continental troops under order by General George Washington to invade their homeland, destroy their dwellings and crops, and end their threat to the patriots. They destroyed nearly 50 Seneca and Cayuga villages. Today roadside signs trace Sullivan and Clinton's route along the east side of Seneca Lake where the burning of villages and crops occurred.
After the war, the Iroquois were forced to cede their land when Britain was defeated. Their millions of acres were sold and some lands in this area were granted to veterans of the army in payment for their military service. A slow stream of European-American settlers began to arrive circa 1790. Initially the settlers were without a market nearby or a way to get their crops to market. The settlers' isolation ended in 1825 with the opening of the Erie Canal.
The canal linked the Finger Lakes Region to the outside world. Steamships, barges and ferries quickly became Seneca Lake's ambassadors of commerce and trade. The former, short Crooked Lake Canal linked Seneca Lake to Keuka Lake.
Numerous canal barges sank during operations and rest on the bottom of the lake. A collection of barges at the southwest end of the lake, near the village of Watkins Glen, is being preserved and made accessible for scuba diving by the Finger Lakes Underwater Preserve Association.
Painted rocks.
The painted rocks located at the southern end of the lake on the eastern cliff face depict an American flag, Tee-pee, and several Native Americans. The older paintings, located on the bottom of the cliff, were said to have been drawn in 1779 after the Senecas escaped men from John Sullivan's campaign. However, this account is questioned by historian Barbara Bell, arguing that it is unlikely that the Senecas would have returned to paint the paintings having just escaped from Sullivan's men. She suggests instead that these paintings may have been made much later, for tourists on Seneca Lake boat tours. 
It is known that the more visible and prominent paintings of the Native Americans, American flag, and Tee-pee were added in 1929 during the Sullivan Sesquicentennial. There are two mistakes in these 1929 additions: firstly the Native Americans in the Seneca Region used longhouses and not Tee-pees, and secondly the flag is displayed pointing to the left which is never to be done on a horizontal surface.
Guns of the Seneca.
Seneca Lake is also the site of a strange and currently unexplained phenomenon known as Mistpouffers. In this area, they are called the Seneca Guns, Lake Drums, or Lake Guns. These are mysterious cannon-like booms and shakes that are heard and felt in the surrounding area. The term Lake Guns originated in the short story "Lake Gun" by James Fenimore Cooper in 1851. The most widely accepted explanation is sonic booms from military aircraft, though this does not explain the sounds heard during Cooper's time.
Sampson Navy and Air Force bases.
The east side of Seneca Lake was once home to a military training ground called Sampson Naval Base, primarily used during World War II. It became Sampson Air Force Base during the Korean War and was used for basic training. After Sampson AFB closed, the airfield remained as Seneca Army Airfield but was closed in 2000. The training grounds of Sampson have since been converted to a civilian picnic area called Sampson State Park. 
There is still a Naval facility at Seneca Lake, the Naval Undersea Warfare Center (NUWC) Sonar test facility, where a scale model of the sonar section of the nuclear submarine USS Seawolf (SSN 21) was tested during the development of this ship. 
Water quality buoy.
There is a YSI EMM-2500 Buoy Platform located in the north end of Seneca Lake roughly in the center. Its coordinates are: latitude: 42°41'49.99"N, longitude: 76°55'29.93"W. The buoy has cellular modem communications and measures wind speed and direction, relative humidity, air temperature, barometric pressure, light intensity, and the water's depth and temperature, conductivity, turbidity, and chlorophyll-a levels.
The buoy was initially deployed in June 2006. The water depth where it is located is about 200 ft.
Wine.
Viticulture and winemaking in the area date back to the 19th century, with the foundation of the Seneca Lake Wine Company in 1866 marking the first major winery in the area. The modern era of wine production began in the 1970s with the establishment of several wineries and the passage of the New York Farm Winery Act of 1976. The region was established as an American Viticultural Area in 1988.
Seneca Lake Wine Trail hosts many events on and around the lake, annually. With more than 30 wineries currently located on the shores of Seneca Lake, the winter 'Deck the Halls' event is a great time at the lake with participating wineries showcasing their vintages and pairing these wines with distinctive, tasty treats. Wineries also provide participants with an ornament at each stop to commemorate the event. 2011 marks the 20th anniversary of this event.

</doc>
<doc id="28181" url="http://en.wikipedia.org/wiki?curid=28181" title="Strait of Gibraltar">
Strait of Gibraltar

The Strait of Gibraltar (Arabic: مضيق جبل طارق, Spanish: "Estrecho de Gibraltar") is a narrow strait that connects the Atlantic Ocean to the Mediterranean Sea and separates Gibraltar and Peninsular Spain in Europe from Morocco and Ceuta (Spain) in Africa. The name comes from the Rock of Gibraltar, which in turn originates from the Arabic "Jebel Tariq" (meaning "Tariq's mountain") named after Tariq ibn Ziyad. It is also known as the Straits of Gibraltar, the Gut of Gibraltar (although this is mostly archaic), STROG (Strait Of Gibraltar) in naval use, and in the ancient world as the "Pillars of Hercules" (Ancient Greek: αἱ Ἡράκλειοι στῆλαι).
Europe and Africa are separated by 7.7 nmi of ocean at the strait's narrowest point. The Strait's depth ranges between 300 and which possibly interacted with the lower mean sea level of the last major glaciation 20,000 years ago when the level of the sea is believed to have been lower by 110 -. Ferries cross between the two continents every day in as little as 35 minutes. The Spanish side of the Strait is protected under El Estrecho Natural Park.
Location.
On the northern side of the Strait are Spain and Gibraltar (a British overseas territory in the Iberian Peninsula), while on the southern side are Morocco and Ceuta (a Spanish exclave in North Africa). Its boundaries were known in antiquity as the Pillars of Hercules. There are several islets, such as the disputed Isla Perejil, that are claimed by both Morocco and Spain.
Due to its location, the Strait is commonly used for illegal immigration from Africa to Europe.
Extent.
The International Hydrographic Organization defines the limits of the Strait of Gibraltar as follows:
Geology.
Around 5.9 million years ago, the connection between the Mediterranean Sea and the Atlantic Ocean along the Betic and Rifan Corridor was progressively restricted until its total closure, effectively causing the salinity of the Mediterranean to periodically rise within the gypsum and salt deposition range, during what is known as the Messinian Salinity Crisis. In this water chemistry environment, dissolved mineral concentrations, temperature and stilled water currents combined properly and occurred regularly to precipitate many mineral salts in sea floor bedded layers. The resultant accumulation of various huge salt and mineral deposits about the Mediterranean basin are directly linked to this era. It is believed that this process took a short time, by geological standards, lasting between 500,000 and 600,000 years.
It is estimated that, were the straits closed even at today's higher sea level, most water in the Mediterranean basin would evaporate within only a thousand years, as it is believed to have done then, and such an event would lay down mineral deposits like the salt deposits now found under the sea floor all over the Mediterranean.
After a lengthy period of restricted intermittent or no water exchange between the Atlantic Ocean and Mediterranean basin, approximately 5.33 million years ago, the Atlantic-Mediterranean connection was completely reestablished through the Strait of Gibraltar by the Zanclean flood, and has remained open ever since. The erosion produced by the incoming waters seem to be the main cause for the present depth of the strait (900 m at the narrows, 280 m at the Camarinal Sill). The strait is expected to close again as the African Plate moves northward relative to the Eurasian Plate, but on geological rather than human timescales.
Important Bird Area.
The Strait has been identified as an Important Bird Area by BirdLife International because hundreds of thousands of seabirds use it every year to pass between the Mediterranean and the Atlantic, including large numbers of Cory's and Balearic shearwaters, Audouin's, yellow-legged and lesser black-backed gulls, razorbills and Atlantic puffins.
History.
Evidence of the first human habitation of the area by Neanderthals dates back to 125,000 years ago. In fact, it is believed that the Rock of Gibraltar may have been one of the last outposts of Neanderthal habitation in the world, with evidence of their presence there dating to as recently as only 24,000 years ago. Archaeological evidence of Homo sapiens habitation of the area dates back c. 40,000 years.
The relatively short distance between the two shores has served as a quick hop-over point for various groups and civilizations throughout history, i.e., Hannibal's invasion of Rome, Roman travel between the provinces of Hispania and Mauritania, Vandals raiding down from Germania through Western Rome and into North Africa in the 5th century, Moors/Berbers in the 8th - 11th centuries, Spain & Portugal in the 16th century, etc.
Beginning in 1492, the straits began to play a certain cultural role in acting as a barrier against cross-strait conquest and the flow of culture and language that would naturally follow such a conquest. In that year, the last Muslim government north of the straits was overthrown by a Spanish force. Since that time, the straits have come to foster the development of two very distinct and varied cultures on either side of the straits after sharing much the same culture and greater degrees of tolerance for over 300+ years from the 8th century to the early 13th century.
On the northern side, Christian/European culture has remained dominant since the expulsion of the last Muslim kingdom in 1492, along with the Latin-based Spanish language, while on the southern side, Muslim-Arabic/Mediterranean has been dominant since the spread of Islam into North Africa in the 700's, along with the Arabic language. For the last 500 years, religious and cultural intolerance, more than the small travel barrier that the straits present, has come to act as a powerful enforcing agent of the cultural separation that exists between these two groups.
The small British enclave of the city of Gibraltar presents a third cultural group found in the straits. This enclave was first established in 1704 and has since been used by Britain to act as a surety for control of the sea lanes into and out of the Mediterranean.
Following the Spanish coup of July 1936 the Spanish Republican Navy tried to blockade the Strait of Gibraltar to hamper the transport of Army of Africa troops from Spanish Morocco to Peninsular Spain. But on 5 August 1936 the so-called Convoy de la victoria was able to bring at least 2,500 men across the strait breaking the republican blockade.
Communications.
The Strait is an important shipping route from the Mediterranean to the Atlantic. There are ferries that operate between Spain and Morocco across the strait, as well as between Spain and Ceuta and Gibraltar to Tangier.
Tunnel across the strait.
In December 2003, Spain and Morocco agreed to explore the construction of an undersea rail tunnel to connect their rail systems across the Strait. The gauge of the rail would be 1435 mm to match the proposed construction and conversion of significant parts of the existing broad gauge system to standard gauge. While the project remains in a planning phase, Spanish and Moroccan officials have met to discuss it as recently as 2012, and proposals predict it could be completed by 2025.
Special flow and wave patterns.
The Strait of Gibraltar links the Atlantic Ocean directly to the Mediterranean Sea. This direct linkage creates certain unique flow and wave patterns. These unique patterns are created due to the interaction of various regional and global evaporative forces, tidal forces, and wind forces.
Inflow and outflow.
Through the strait, water generally flows more or less continually in both an eastward and a westward direction. A smaller amount of deeper saltier and therefore denser waters continually work their way westwards (the Mediterranean outflow), while a larger amount of surface waters with lower salinity and density continually work their way eastwards (the Mediterranean inflow). These general flow tendencies may be occasionally interrupted for brief periods to accommodate temporary tidal flow requirements, depending on various lunar and solar alignments. Still, on the whole and over time, the balance of the water flow is eastwards, due to an evaporation rate within the Mediterranean basin higher than the combined inflow of all the rivers that empty into it. The shallow Camarinal Sill of the Strait of Gibraltar, which forms the shallowest point within the strait, acts to limit mixing between the cold, less saline Atlantic water and the warm Mediterranean waters. The Camarinal Sill is located at the far western end of the strait.
The Mediterranean waters are so much saltier than the Atlantic waters that they sink below the constantly incoming water and form a highly saline ("thermohaline", both warm and salty) layer of bottom water. This layer of bottom-water constantly works its way out into the Atlantic as the Mediterranean outflow. On the Atlantic side of the strait, a density boundary separates the Mediterranean outflow waters from the rest at about 100 m depth. These waters flow out and down the continental slope, losing salinity, until they begin to mix and equilibrate more rapidly, much further out at a depth of about 1000 m. The Mediterranean outflow water layer can be traced for thousands of kilometres west of the strait, before completely losing its identity.
During the Second World War, German U-boats used the currents to pass into the Mediterranean Sea without detection, by maintaining silence with engines off. From September 1941 to May 1944 Germany managed to send 62 U-boats into the Mediterranean. All these boats had to navigate the British-controlled Strait of Gibraltar where nine U-boats were sunk while attempting passage and 10 more had to break off their run due to damage. No U-boats ever made it back into the Atlantic and all were either sunk in battle or scuttled by their own crews.
Internal waves.
Internal waves (waves at the density boundary layer) are often produced by the strait. Like traffic merging on a highway, the water flow is constricted in both directions because it must pass over the Camarinal Sill. When large tidal flows enter the Strait and the high tide relaxes, internal waves are generated at the Camarinal Sill and proceed eastwards. Even though the waves may occur down to great depths, occasionally the waves are almost imperceptible at the surface, at other times they can be seen clearly in satellite imagery. These "internal waves" continue to flow eastward and to refract around coastal features. They can sometimes be traced for as much as 100 km, and sometimes create interference patterns with refracted waves.
Territorial waters.
The Strait lies mostly within the territorial waters of Spain and Morocco, except for at the far eastern end. The United Kingdom (through Gibraltar) claims 3 nautical miles around Gibraltar putting part of the Strait inside British territorial waters, and the smaller-than-maximal claim also means that part of the Strait therefore lies in international waters according to the British claim. However, the ownership of Gibraltar and its territorial waters is disputed by Spain.
Power generation.
Some studies have proposed the possibility of erecting tidal power generating stations within the strait, to be powered from the predictable current at the strait.
In the 1920s and 1930s, the Atlantropa project proposed damming the strait to generate large amounts of electricity and lower the sea level of the Mediterranean by several hundreds of meters to create large new lands for settlement.

</doc>
<doc id="28182" url="http://en.wikipedia.org/wiki?curid=28182" title="Social epistemology">
Social epistemology

Social epistemology refers to a broad set of approaches to the study of knowledge that construes human knowledge as a collective achievement. Another way of characterizing social epistemology is as the study of the social dimensions of knowledge. One of the enduring difficulties with defining "social epistemology" is that of determining what the word "knowledge" means in this context. There is also a challenge in arriving at a definition of "social" which satisfies academics from different disciplines. Social epistemologists may be found working in many of the disciplines of the humanities and social sciences, most commonly in philosophy and sociology. In addition to marking a distinct movement in traditional, analytic epistemology, social epistemology is associated with the interdisciplinary field of Science and Technology Studies (STS).
The emergence of social epistemology.
The term "social epistemology" was first used by the library scientists Margaret Egan and Jesse Shera in the 1950s. Steven Shapin also used it in 1979. But its current sense began to emerge in the late 1980s. In 1987, the philosophical journal "Synthese" published a special issue on "social epistemology" which included two authors that have since taken "social epistemology" in two divergent directions: Alvin Goldman and Steve Fuller3. Fuller founded a journal called "Social Epistemology: a journal of knowledge, culture, and policy" in 1987 and published his first book, "Social Epistemology", in 1988. Goldman's "Knowledge in a Social World" came out in 1999. Goldman advocates for a type of epistemology which is sometimes called "veritistic epistemology" because of its large emphasis on truth. This type of epistemology is sometimes seen to side with "essentialism" as opposed to "multiculturalism". But Goldman has argued that this association between veritistic epistemology and essentialism is not necessary.
In 2012, on the occasion of the 25th anniversary of "Social Epistemology", Fuller reflected on the history and the prospects of the field, including the need for social epistemology to re-connect with the larger issues of knowledge production first identified by Charles Sanders Peirce as "cognitive economy" and nowadays often pursued by library and information science. As for the "analytic social epistemology", to which Goldman has been a significant contributor, Fuller concludes that it has "failed to make significant progress owing, in part, to a minimal understanding of actual knowledge practices, a minimised role for philosophers in ongoing inquiry, and a focus on maintaining the status quo of epistemology as a field."
The basic view of knowledge that motivated the emergence of social epistemology can be traced to the work of Thomas Kuhn and Michel Foucault, which gained in prominence at the end of the 1960s. Both brought historical concerns directly to bear on problems long associated with the philosophy of science. Perhaps the most notable issue here was the nature of truth, which both Kuhn and Foucault described as a relative and contingent notion. On this background, ongoing work in the sociology of scientific knowledge (SSK) and the history and philosophy of science (HPS) was able to assert its epistemological consequences, leading most notably to the establishment of the "Strong Programme" at the University of Edinburgh. In terms of the two strands of social epistemology, Fuller is more sensitive and receptive to this historical trajectory (if not always in agreement) than Goldman, whose "veritistic" social epistemology can be reasonably read as a systematic rejection of the more extreme claims associated with Kuhn and Foucault.
Social Epistemology as a field within analytic philosophy.
As a field within analytic philosophy, Social Epistemology foregrounds the social aspects of knowledge creation and dissemination. What precisely these social aspects are, and whether they have beneficial or detrimental effects upon the possibilities to create, acquire and spread knowledge is a subject of continuous debate.
Within the field, `the social' is approached in two complementary and not mutually exclusive ways: `the social' character of knowledge can either be approached through inquiries in "inter-individual" epistemic relations or through inquiries focusing on epistemic "communities". The inter-individual approach typically focuses on issues such as testimony, epistemic trust as a form of trust placed by one individual in another, epistemic dependence, epistemic authority etc. The community approach typically focuses on issues such as community standards of justification, community procedures of critique, diversity, epistemic justice, and collective knowledge.
Social Epistemology as a field within analytic philosophy has close ties to, and often overlaps with Feminist Epistemology and Philosophy of Science. While parts of the field engage in abstract, normative considerations of knowledge creation and dissemination, other parts of the field are `naturalized epistemology' in the sense that they draw on empirically gained insights---be that natural science research from, e.g., cognitive psychology, be that qualitative or quantitative social science research. (For the notion of `naturalized epistemology see Willard Van Orman Quine.) And while parts of the field are concerned with analytic considerations of rather general character, case-based and domain-specific inquiries in, e.g., knowledge creation in collaborative scientific practice or knowledge exchange on online platforms, play an increasing role.
Important academic journals for Social Epistemology as a field within analytic philosophy are, e.g., Episteme, Hypatia, Social Epistemology, and Synthese. However, major work within this field is also published in journals that predominantly address philosophers of science or in interdisciplinary journals which focus on particular domains of inquiry (such as, e.g., Ethics and Information Technology).
Present and future concerns.
At this stage, both varieties of "social epistemology" remain largely "academic" or "theoretical" projects. But both emphasize the social significance of knowledge and therefore the cultural value of social epistemology itself. A range of journals publishing Social Epistemology welcome papers that include a policy dimension. More practical applications of social epistemology can be found in the areas of library science, academic publishing, guidelines for scientific authorship and collaboration, knowledge policy and debates over the role over the Internet in knowledge transmission and creation.
Notes.
1. "What Is Social Epistemology? A Smorgasbord of projects", in "Pathways to Knowledge: Private and Public", Oxford University Press, Pg:182-204, ISBN 0-19-517367-8
2. "Relativism, Rationality and Sociality of Knowledge", Barry Barnes and David Bloor, in "Rationality and Relativism", Pg:22 ISBN 0-262-58061-6
3. A comparison of Goldman and Fuller can be found in "Legitimizing Scientific Knowledge: An Introduction to Steve Fuller's Social Epistemology", Francis Remedios, Lexington Books, 2003. pp. 106 –112.<br>
http://social-epistemology.com/2013/07/12/orienting-social-epistemology-francis-remedios/
4. "Social Epistemology", Steve Fuller, Indiana University Press, p. 3.

</doc>
<doc id="28184" url="http://en.wikipedia.org/wiki?curid=28184" title="Sound card">
Sound card

A sound card (also known as an audio card) is an internal computer expansion card that facilitates economical input and output of audio signals to and from a computer under control of computer programs. The term "sound card" is also applied to external audio interfaces that use software to generate sound, as opposed to using hardware inside the PC. Typical uses of sound cards include providing the audio component for multimedia applications such as music composition, editing video or audio, presentation, education and entertainment (games) and video projection.
Sound functionality can also be integrated onto the motherboard, using basically the same components as a plug-in card. The best plug-in cards, which use better and more expensive components, can achieve higher quality than integrated sound. The integrated sound system is often still referred to as a "sound card".
General characteristics.
Most sound cards use a digital-to-analog converter (DAC), which converts recorded or generated digital data into an analog format. The output signal is connected to an amplifier, headphones, or external device using standard interconnects, such as a TRS phone connector or an RCA connector. If the number and size of connectors is too large for the space on the backplate the connectors will be off-board, typically using a breakout box, an auxiliary backplate, or a panel mounted at the front. More advanced cards usually include more than one sound chip to support higher data rates and multiple simultaneous functionality, for example digital production of synthesized sounds, usually for real-time generation of music and sound effects using minimal data and CPU time.
Digital sound reproduction is usually done with multichannel DACs, which are capable of simultaneous and digital samples at different pitches and volumes, and application of real-time effects such as filtering or deliberate distortion. Multichannel digital sound playback can also be used for music synthesis, when used with a compliance, and even multiple-channel emulation. This approach has become common as manufacturers seek simpler and lower-cost sound cards.
Most sound cards have a line in connector for an input signal
from a cassette tape or other sound source that has higher voltage levels than a microphone. The sound card digitizes this signal. The DMAC transfers the samples to the main memory, from where a recording software may write it to the hard disk for storage, editing, or further processing. Another common external connector is the "microphone" connector, for signals from a microphone or other low-level input device. Input through a microphone jack can be used, for example, by speech recognition or voice over IP applications.
Sound channels and polyphony.
An important sound card characteristic is polyphony, which refers to its ability to process and output multiple "independent" voices or sounds "simultaneously". These distinct "channels" are seen as the number of audio outputs, which may correspond to a speaker configuration such as 2.0 (stereo), 2.1 (stereo and sub woofer), 5.1 (surround), or other configuration. Sometimes, the terms "voice" and "channel" are used interchangeably to indicate the degree of polyphony, not the output speaker configuration.
For example, many older sound chips could accommodate three voices, but only one audio channel (i.e., a single mono output) for output, requiring all voices to be mixed together. Later cards, such as the AdLib sound card, had a 9-voice polyphony combined in 1 mono output channel.
For some years, most PC sound cards have had multiple FM synthesis voices (typically 9 or 16) which were usually used for MIDI music. The full capabilities of advanced cards are often not fully used; only one (mono) or two (stereo) voice(s) and channel(s) are usually dedicated to playback of digital sound samples, and playing back more than one digital sound sample usually requires a software downmix at a fixed sampling rate. Modern low-cost integrated soundcards (i.e., those built into motherboards) such as audio codecs like those meeting the AC'97 standard and even some lower-cost expansion sound cards still work this way. These devices may provide more than two sound output channels (typically 5.1 or 7.1 surround sound), but they usually have no actual hardware polyphony for either sound effects or MIDI reproduction – these tasks are performed entirely in software. This is similar to the way inexpensive softmodems perform modem tasks in software rather than in hardware.
Also, in the early days of 'wavetable' sample-based synthesis, some sound card manufacturers advertised polyphony solely on the MIDI capabilities alone. In this case, the card's output channel is irrelevant; typically, the card is only capable of two channels of digital sound. Instead, the polyphony measurement solely applies to the number of MIDI instruments the sound card is capable of producing at one given time.
Today, a sound card providing actual hardware polyphony, regardless of the number of output channels, is typically referred to as a "hardware audio accelerator", although actual voice polyphony is not the sole (or even a necessary) prerequisite, with other aspects such as hardware acceleration of 3D sound, positional audio and real-time DSP effects being more important.
Since digital sound playback has become available and provided better performance than synthesis, modern soundcards with hardware polyphony do not actually use DACs with as many channels as voices; instead, they perform voice mixing and effects processing in hardware, eventually performing digital filtering and conversions to and from the frequency domain for applying certain effects, inside a dedicated DSP. The final playback stage is performed by an external (in reference to the DSP chip(s)) DAC with significantly fewer channels than voices (e.g., 8 channels for 7.1 audio, which can be divided among 32, 64 or even 128 voices).
Color codes.
Connectors on the sound cards are color-coded as per the PC System Design Guide. They will also have symbols with arrows, holes and soundwaves that are associated with each jack position, the meaning of each is given below:
History of sound cards for the IBM PC architecture.
Sound cards for computers compatible with the IBM PC were very uncommon until 1988, which left the single internal PC speaker as the only way early PC software could produce sound and music. The speaker hardware was typically limited to square waves, which fit the common nickname of "beeper". The resulting sound was generally described as "beeps and boops". Several companies, most notably Access Software, developed techniques for digital sound reproduction over the PC speaker (see RealSound); the resulting audio, while barely functional, suffered from distorted output and low volume, and usually required all other processing to be stopped while sounds were played. Other home computer models of the 1980s included hardware support for digital sound playback, or music synthesis (or both), leaving the IBM PC at a disadvantage to them when it came to multimedia applications such as music composition or gaming. The initial design and marketing focuses of sound cards for the IBM PC platform were not based on gaming, but rather on specific audio applications such as music composition (AdLib Personal Music System, IBM Music Feature Card, Creative Music System), or on speech synthesis (Digispeech "DS201", Covox Speech Thing, Street Electronics "Echo").
In 1988 a panel of computer-game CEOs stated at the Consumer Electronics Show that the PC's limited sound capability prevented it from becoming the leading home computer, that it needed a $49–79 sound card with better capability than current products, and that once such hardware was widely installed their companies would support it. Sierra On-Line, which had pioneered supporting EGA and VGA video, and 3 1/2" disks, that year promised to support AdLib, IBM Music Feature, and Roland MT-32 in its games; the cards cost $195 to $600. A 1989 "Computer Gaming World" survey found that 18 of 25 game companies planned to support AdLib, six Roland and Covox, and seven Creative Music System/Game Blaster.
Hardware manufacturers.
One of the first manufacturers of sound cards for the IBM PC was AdLib, which produced a card based on the Yamaha YM3812 sound chip, also known as the OPL2. The AdLib had two modes: A 9-voice mode where each voice could be fully programmed, and a less frequently used "percussion" mode with 3 regular voices producing 5 independent percussion-only voices for a total of 11. (The percussion mode was considered inflexible by most developers; it was used mostly by AdLib's own composition software.)
Creative Labs also marketed a sound card about the same time called the Creative Music System. Although the "C/MS " had twelve voices to AdLib's nine, and was a stereo card while the AdLib was mono, the basic technology behind it was based on the Philips SAA1099 chip which was essentially a square-wave generator. It sounded much like twelve simultaneous PC speakers would have except for each channel having amplitude control, and failed to sell well, even after Creative renamed it the Game Blaster a year later, and marketed it through Radio Shack in the US. The Game Blaster retailed for under $100 and was compatible with many popular games, such as Silpheed.
A large change in the IBM PC compatible sound card market happened when Creative Labs introduced the Sound Blaster card. Recommended by Microsoft to developers creating software based on the Multimedia PC standard, the Sound Blaster cloned the AdLib and added a sound coprocessor for recording and play back of digital audio (likely to have been an Intel microcontroller relabeled by Creative). It was incorrectly called a "DSP" (to suggest it was a digital signal processor), a game port for adding a joystick, and capability to interface to MIDI equipment (using the game port and a special cable). With more features at nearly the same price, and compatibility as well, most buyers chose the Sound Blaster. It eventually outsold the AdLib and dominated the market.
Roland also made sound cards in the late 1980s, most of them being high quality "prosumer" cards, such as the MT-32 and LAPC-I. Roland cards often sold for hundreds of dollars, and sometimes over a thousand. Many games had music written for their cards, such as Silpheed and Police Quest II. The cards were often poor at sound effects such as laughs, but for music were by far the best sound cards available until the mid nineties. Some Roland cards, such as the SCC, and later versions of the MT-32 were made to be less expensive, but their quality was usually drastically poorer than the other Roland cards.
By 1992 one sound card vendor advertised that its product was "Sound Blaster, AdLib, Disney Sound Source and Covox Speech Thing Compatible!". The Sound Blaster line of cards, together with the first inexpensive CD-ROM drives and evolving video technology, ushered in a new era of multimedia computer applications that could play back CD audio, add recorded dialogue to video games, or even reproduce full motion video (albeit at much lower resolutions and quality in early days). The widespread decision to support the Sound Blaster design in multimedia and entertainment titles meant that future sound cards such as Media Vision's Pro Audio Spectrum and the Gravis Ultrasound had to be Sound Blaster compatible if they were to sell well. Until the early 2000s (by which the AC'97 audio standard became more widespread and eventually usurped the SoundBlaster as a standard due to its low cost and integration into many motherboards), Sound Blaster compatibility is a standard that many other sound cards still support to maintain compatibility with many games and applications released.
Industry adoption.
When game company Sierra On-Line opted to support add-on music hardware (instead of built-in hardware such as the PC speaker and built-in sound capabilities of the IBM PCjr and Tandy 1000), what could be done with sound and music on the IBM PC changed dramatically. Two of the companies Sierra partnered with were Roland and AdLib, opting to produce in-game music for King's Quest 4 that supported the MT-32 and AdLib Music Synthesizer. The MT-32 had superior output quality, due in part to its method of sound synthesis as well as built-in reverb. Since it was the most sophisticated synthesizer they supported, Sierra chose to use most of the MT-32's custom features and unconventional instrument patches, producing background sound effects (e.g., chirping birds, clopping horse hooves, etc.) before the Sound Blaster brought playing real audio clips to the PC entertainment world. Many game companies also supported the MT-32, but supported the Adlib card as an alternative because of the latter's higher market base. The adoption of the MT-32 led the way for the creation of the MPU-401/Roland Sound Canvas and General MIDI standards as the most common means of playing in-game music until the mid-1990s.
Feature evolution.
Early ISA bus soundcards were half-duplex, meaning they couldn't record and play digitized sound simultaneously, mostly due to inferior card hardware (e.g., DSPs). Later, ISA cards like the SoundBlaster AWE series and Plug-and-play Soundblaster clones eventually became full-duplex and supported simultaneous recording and playback, but at the expense of using up two IRQ and DMA channels instead of one, making them no different from having two half-duplex sound cards in terms of configuration. Towards the end of the ISA bus' life, ISA soundcards started taking advantage of IRQ sharing, thus reducing the IRQs needed to one, but still needed two DMA channels. Many PCI bus cards do not have these limitations and are mostly full-duplex. It should also be noted that many modern PCI bus cards also do not require free DMA channels to operate.
Also, throughout the years, soundcards have evolved in terms of digital audio sampling rate (starting from 8-bit 11 025 Hz, to 32-bit, 192 kHz that the latest solutions support). Along the way, some cards started offering 'wavetable' sample-based synthesis, which provides superior MIDI synthesis quality relative to the earlier OPL-based solutions, which uses FM-synthesis. Also, some higher end cards started having their own RAM and processor for user-definable sound samples and MIDI instruments as well as to offload audio processing from the CPU.
For years, soundcards had only one or two channels of digital sound (most notably the Sound Blaster series and their compatibles) with the exception of the E-MU card family, the Gravis GF-1 and AMD Interwave, which had hardware support for up to 32 independent channels of digital audio. Early games and MOD-players needing more channels than a card could support had to resort to mixing multiple channels in software. Even today, the tendency is still to mix multiple sound streams in software, except in products specifically intended for gamers or professional musicians, with a sensible difference in price from "software based" products. Also, in the early era of 'wavetable' sample-based synthesis, soundcard companies would also sometimes boast about the card's polyphony capabilities in terms of MIDI synthesis. In this case polyphony solely refers to the count of MIDI notes the card is capable of synthesizing simultaneously at one given time and not the count of digital audio streams the card is capable of handling.
In regards to physical sound output, the number of physical sound channels has also increased. The first soundcard solutions were mono. Stereo sound was introduced in the early 1980s, and quadraphonic sound came in 1989. This was shortly followed by 5.1 channel audio. The latest soundcards support up to 8 physical audio channels in the 7.1 speaker setup.
Crippling of features.
Most new soundcards no longer have the audio loopback device commonly called "Stereo Mix"/"Wave out mix"/"Mono Mix"/"What U Hear" that was once very prevalent and that allows users to digitally record speaker output to the microphone input.
Many users suspect the RIAA is responsible for colluding with or pressuring computer and soundcard manufacturers to disable this and other features because of their ability to be used for copyright infringement (although many legitimate uses exist for this feature), but no proof of this currently exists.
However, virtually no other answers exist as to why computer and soundcard manufacturers have been discontinuing this feature. No notice or information is usually given to consumers of the exclusion or inclusion of the feature when purchasing or in specifications.
Lenovo and other manufacturers fail to implement the chipset feature in hardware, while other manufacturers disable the driver from supporting it. In some cases loopback can be reinstated with driver updates (as in the case of some Dell computers); alternatively software (Total Recorder) can be purchased to enable the functionality. According to Microsoft, the functionality was hidden by default in Windows Vista (to reduce user confusion), but is still available, as long as the underlying sound card drivers and hardware support it. Ultimately, the user can connect the line out directly to the line in (analog hole).
Professional soundcards (audio interfaces).
Professional soundcards are special soundcards optimized for low-latency multichannel sound recording and playback, including studio-grade fidelity. Their drivers usually follow the Audio Stream Input Output protocol for use with professional sound engineering and music software, although ASIO drivers are also available for a range of consumer-grade soundcards.
Professional soundcards are usually described as "audio interfaces", and sometimes have the form of external rack-mountable units using USB, FireWire, or an optical interface, to offer sufficient data rates. The emphasis in these products is, in general, on multiple input and output connectors, direct hardware support for multiple input and output sound channels, as well as higher sampling rates and fidelity as compared to the usual consumer soundcard. In that respect, their role and intended purpose is more similar to a specialized multi-channel data recorder and real-time audio mixer and processor, roles which are possible only to a limited degree with typical consumer soundcards.
On the other hand, certain features of consumer soundcards such as support for environmental audio extensions (EAX), optimization for hardware acceleration in video games, or real-time ambience effects are secondary, nonexistent or even undesirable in professional soundcards, and as such audio interfaces are not recommended for the typical home user.
The typical "consumer-grade" soundcard is intended for generic home, office, and entertainment purposes with an emphasis on playback and casual use, rather than catering to the needs of audio professionals. In response to this, Steinberg (the creators of audio recording and sequencing software, Cubase and Nuendo) developed a protocol that specified the handling of multiple audio inputs and outputs.
In general, consumer grade soundcards impose several restrictions and inconveniences that would be unacceptable to an audio professional. One of a modern soundcard's purposes is to provide an AD/DA converter (analog to digital/digital to analog). However, in professional applications, there is usually a need for enhanced recording (analog to digital) conversion capabilities.
One of the limitations of consumer soundcards is their comparatively large sampling latency; this is the time it takes for the AD Converter to complete conversion of a sound sample and transfer it to the computer's main memory.
Consumer soundcards are also limited in the "effective" sampling rates and bit depths they can actually manage (compare analog versus digital sound) and have lower numbers of less flexible input channels: professional studio recording use typically requires more than the two channels that consumer soundcards provide, and more accessible connectors, unlike the variable mixture of internal—and sometimes virtual—and external connectors found in consumer-grade soundcards.
Sound devices other than expansion cards.
Integrated sound hardware on PC motherboards.
In 1984, the first IBM PCjr had a rudimentary 3-voice sound synthesis chip (the SN76489) which was capable of generating three square-wave tones with variable amplitude, and a pseudo-white noise channel that could generate primitive percussion sounds. The Tandy 1000, initially a clone of the PCjr, duplicated this functionality, with the Tandy TL/SL/RL models adding digital sound recording and playback capabilities. Many games during the 1980s that supported the PCjr's video standard (described as "Tandy-compatible", "Tandy graphics", or "TGA") also supported PCjr/Tandy 1000 audio.
In the late 1990s many computer manufacturers began to replace plug-in soundcards with a "codec" chip (actually a combined audio AD/DA-converter) integrated into the motherboard. Many of these used Intel's AC'97 specification. Others used inexpensive ACR slot accessory cards.
From around 2001 many motherboards incorporated integrated "real" (non-codec) soundcards, usually in the form of a custom chipset providing something akin to full Sound Blaster compatibility, providing relatively high-quality sound.
However, these features were dropped when AC'97 was superseded by Intel's HD Audio standard, which was released in 2004, again specified the use of a codec chip, and slowly gained acceptance. As of 2011, most motherboards have returned to using a codec chip, albeit a HD Audio compatible one, and the requirement for Sound Blaster compatibility relegated to history.
Integrated sound on other platforms.
Various non-IBM PC compatible computers, such as early home computers like the Commodore 64 (1982) and Amiga (1985), NEC's PC-88 and PC-98, Fujitsu's FM-7 and FM Towns, the MSX, Apple's Macintosh, and workstations from manufacturers like Sun, have had their own motherboard integrated sound devices. In some cases, most notably in those of the Amiga, C64, PC-88, PC-98, MSX, FM-7, and FM towns, they provide very advanced capabilities (as of the time of manufacture), in others they are only minimal capabilities. Some of these platforms have also had sound cards designed for their bus architectures that cannot be used in a standard PC.
Several Japanese computer platforms, including the PC-88, PC-98, MSX, and FM-7, featured built-in FM synthesis sound from Yamaha by the mid-1980s. By 1989, the FM Towns computer platform featured built-in PCM sample-based sound and supported the CD-ROM format.
The custom sound chip on Amiga, named Paula, had four digital sound channels (2 for the left speaker and 2 for the right) with 8 bit resolution (although with patches, 14/15bit was accomplishable at the cost of high CPU usage) for each channel and a 6 bit volume control per channel. Sound playback on Amiga was done by reading directly from the chip-RAM without using the main CPU.
Most arcade games have integrated sound chips, the most popular being the Yamaha OPL chip for BGM coupled with a variety of DACs for sampled audio and sound effects.
Sound cards on other platforms.
The earliest known soundcard used by computers was the Gooch Synthetic Woodwind, a music device for PLATO terminals, and is widely hailed as the precursor to sound cards and MIDI. It was invented in 1972.
Certain early arcade machines made use of sound cards to achieve playback of complex audio waveforms and digital music, despite being already equipped with onboard audio. An example of a sound card used in arcade machines is the Digital Compression System card, used in games from Midway. For example, Mortal Kombat II on the Midway T Unit hardware. The T-Unit hardware already has an onboard YM2151 OPL chip coupled with an OKI 6295 DAC, but said game uses an added on DCS card instead. The card is also used in the arcade version of Midway and Aerosmith's Revolution X for complex looping BGM and speech playback (Revolution X used fully sampled songs from the band's album that transparently looped- an impressive feature at the time the game was released).
MSX computers, while equipped with built-in sound capabilities, also relied on sound cards to produce better quality audio. The card, known as Moonsound, uses a Yamaha OPL4 sound chip. Prior to the Moonsound, there were also soundcards called "MSX Music" and "MSX Audio", which uses OPL2 and OPL3 chipsets, for the system.
The Apple II series of computers, which did not have sound capabilities beyond a beep until the IIGS, could use plug-in sound cards from a variety of manufacturers. The first, in 1978, was ALF's Apple Music Synthesizer, with 3 voices; two or three cards could be used to create 6 or 9 voices in stereo. Later ALF created the Apple Music II, a 9-voice model. The most widely supported card, however, was the Mockingboard. Sweet Micro Systems sold the Mockingboard in various models. Early Mockingboard models ranged from 3 voices in mono, while some later designs had 6 voices in stereo. Some software supported use of two Mockingboard cards, which allowed 12-voice music and sound. A 12-voice, single card clone of the Mockingboard called the Phasor was made by Applied Engineering. In late 2005 a company called ReactiveMicro.com produced a 6-voice clone called the Mockingboard v1 and also had plans to clone the Phasor and produce a hybrid card user-selectable between Mockingboard and Phasor modes plus support both the SC-01 or SC-02 speech synthesizers.
External sound devices.
Devices such as the Covox Speech Thing could be attached to the parallel port of an IBM PC and feed 6- or 8-bit PCM sample data to produce audio. Also, many types of professional soundcards (audio interfaces) have the form of an external FireWire or USB unit, usually for convenience and improved fidelity.
Sound cards using the PCMCIA Cardbus interface were available before laptop and notebook computers routinely had onboard sound. Cardbus audio may still be used if onboard sound quality is poor. When Cardbus interfaces were superseded by Expresscard on computers since about 2005, manufacturers followed. Most of these units are designed for mobile DJs, providing separate outputs to allow both playback and monitoring from one system, however some also target mobile gamers, providing high-end sound to gaming laptops who are usually well-equipped when it comes to graphics and processing power, but tend to have audio codecs that are no better than the ones found on regular laptops.
USB sound cards.
USB sound "cards", sometimes called "audio interfaces", are usually external boxes that plug into the computer via USB.
A USB audio interface may describe a device allowing a computer which has a sound-card, yet lacks a standard audio socket, to be connected to an external device which requires such a socket, via its USB socket.
The USB specification defines a standard interface, the USB audio device class, allowing a single driver to work with the various USB sound devices and interfaces on the market.
Even cards meeting the older, slow, USB 1.1 specification are capable of high quality sound with a limited number of channels, or limited sampling frequency or bit depth, but USB 2.0 or later is more capable.
Uses.
The main function of a sound card is to play audio, usually music, with varying formats (monophonic, stereophonic, various multiple speaker setups) and degrees of control. The source may be a CD or DVD, a file, streamed audio, or any external source connected to a sound card input.
Audio may be recorded. Sometimes sound card hardware and drivers do not support recording a source that is being played.
A card can also be used, in conjunction with software, to generate arbitrary waveforms, acting as an audio-frequency function generator. Free and commercial software is available for this purpose; there are also online services that generate audio files for any desired waveforms, playable through a sound card.
A card can be used, again in conjunction with free or commercial software, to analyse input waveforms. For example, a very-low-distortion sinewave oscillator can be used as input to equipment under test; the output is sent to a sound card's line input and run through Fourier transform software to find the amplitude of each harmonic of the added distortion. Alternatively, a less pure signal source may be used, with circuitry to subtract the input from the output, attenuated and phase-corrected; the result is distortion and noise only, which can be analysed.
There are programs which allow a sound card to be used as an audio-frequency oscilloscope.
For all measurement purposes a sound card must be chosen with good audio properties. It must itself contribute as little distortion and noise as possible, and attention must be paid to bandwidth and sampling. A typical integrated sound card, the Realtek ALC887, according to its data sheet has distortion of about 80dB below the fundamental; cards are available with distortion better than -100dB.
Driver architecture.
To use a sound card, the operating system (OS) typically requires a specific device driver, a low-level program that handles the data connections between the physical hardware and the operating system. Some operating systems include the drivers for many cards; for cards not so supported, drivers are supplied with the card, or available for download.
References.
This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the "relicensing" terms of the GFDL, version 1.3 or later.

</doc>
<doc id="28186" url="http://en.wikipedia.org/wiki?curid=28186" title="Symmetry group">
Symmetry group

In abstract algebra, the symmetry group of an object (image, signal, etc.) is the group of all isometries under which the object is invariant with composition as the operation. It is a subgroup of the isometry group of the space concerned. If not stated otherwise, this article considers symmetry groups in Euclidean geometry, but the concept may also be studied in wider contexts; see below.
Introduction.
The "objects" may be geometric figures, images, and patterns, such as a wallpaper pattern. The definition can be made more precise by specifying what is meant by image or pattern, e.g., a function of position with values in a set of colors. For symmetry of physical objects, one may also want to take physical composition into account. The group of isometries of space induces a group action on objects in it.
The symmetry group is sometimes also called full symmetry group in order to emphasize that it includes the orientation-reversing isometries (like reflections, glide reflections and improper rotations) under which the figure is invariant. The subgroup of orientation-preserving isometries (i.e. translations, rotations, and compositions of these) that leave the figure invariant is called its proper symmetry group. The proper symmetry group of an object is equal to its full symmetry group if and only if the object is chiral (and thus there are no orientation-reversing isometries under which it is invariant).
Any symmetry group whose elements have a common fixed point, which is true for all finite symmetry groups and also for the symmetry groups of bounded figures, can be represented as a subgroup of orthogonal group O(n) by choosing the origin to be a fixed point. The proper symmetry group is a subgroup of the special orthogonal group SO(n) then, and therefore also called rotation group of the figure.
Discrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversion and rotoinversion – they are in fact just the finite subgroups of O(n), (2) infinite lattice groups, which include only translations, and (3) infinite space groups which combines elements of both previous types, and may also include extra transformations like screw axis and glide reflection. There are also "continuous" symmetry groups, which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. The group of all symmetries of a sphere O(3) is an example of this, and in general such continuous symmetry groups are studied as Lie groups. With a categorization of subgroups of the Euclidean group corresponds a categorization of symmetry groups.
Two geometric figures are considered to be of the same symmetry type if their symmetry groups are conjugate subgroups of the Euclidean group "E"("n") (the isometry group of Rn), where two subgroups "H"1, "H"2 of a group "G" are "conjugate", if there exists "g" ∈ "G" such that "H"1 = g−1"H"2"g". For example:
When considering isometry groups, one may restrict oneself to those where for all points the set of images under the isometries is topologically closed. This excludes for example in 1D the group of translations by a rational number. A "figure" with this symmetry group is non-drawable and up to arbitrarily fine detail homogeneous, without being really homogeneous.
One dimension.
The isometry groups in 1D where for all points the set of images under the isometries is topologically closed are:
See also symmetry groups in one dimension.
Two dimensions.
Up to conjugacy the discrete point groups in 2-dimensional space are the following classes:
"C"1 is the trivial group containing only the identity operation, which occurs when the figure has no symmetry at all, for example the letter F. "C"2 is the symmetry group of the letter Z, "C"3 that of a triskelion, "C"4 of a swastika, and "C"5, "C"6 etc. are the symmetry groups of similar swastika-like figures with five, six etc. arms instead of four.
"D"1 is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter A. "D"2, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral rectangle, and "D"3, "D"4 etc. are the symmetry groups of the regular polygons.
The actual symmetry groups in each of these cases have two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.
The remaining isometry groups in 2D with a fixed point, where for all points the set of images under the isometries is topologically closed are:
For non-bounded figures, the additional isometry groups can include translations; the closed ones are:
Three dimensions.
Up to conjugacy the set of 3D point groups consists of 7 infinite series, and 7 separate ones. In crystallography they are restricted to be compatible with the discrete translation symmetries of a crystal lattice. This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 from the 7 infinite series, and 5 of the 7 others).
The continuous symmetry groups with a fixed point include those of:
For objects and scalar fields the cylindrical symmetry implies vertical planes of reflection. However, for vector fields it does not: in cylindrical coordinates with respect to some axis, 
formula_1 has cylindrical symmetry with respect to the axis if and only if formula_2 and formula_3 have this symmetry, i.e., they do not depend on φ. Additionally there is reflectional symmetry if and only if formula_4.
For spherical symmetry there is no such distinction, it implies planes of reflection.
The continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.
Symmetry groups in general.
In wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Once we know what kind of mathematical structure we are concerned with, we should be able to pinpoint what mappings preserve the structure. Conversely, specifying the symmetry can define the structure, or at least clarify what we mean by an invariant, geometric language in which to discuss it; this is one way of looking at the Erlangen programme.
For example, automorphism groups of certain models of finite geometries are not "symmetry groups" in the usual sense, although they preserve symmetry. They do this by preserving "families" of point-sets rather than point-sets (or "objects") themselves.
Like above, the group of automorphisms of space induces a group action on objects in it.
For a given geometric figure in a given geometric space, consider the following equivalence relation: two automorphisms of space are equivalent if and only if the two images of the figure are the same (here "the same" does not mean something like e.g. "the same up to translation and rotation", but it means "exactly the same"). Then the equivalence class of the identity is the symmetry group of the figure, and every equivalence class corresponds to one isomorphic version of the figure.
There is a bijection between every pair of equivalence classes: the inverse of a representative of the first equivalence class, composed with a representative of the second.
In the case of a finite automorphism group of the whole space, its order is the order of the symmetry group of the figure multiplied by the number of isomorphic versions of the figure.
Examples:
Compare Lagrange's theorem (group theory) and its proof.

</doc>
<doc id="28187" url="http://en.wikipedia.org/wiki?curid=28187" title="Singular they">
Singular they

Singular "they" is the use of "they", or its inflected or derivative forms, such as "them", "their", or "themselves", to refer to a single person or an antecedent that is grammatically singular. It typically occurs with an antecedent of indeterminate gender, as in sentences such as:
A reason for its use is that English has no dedicated singular personal pronoun of indeterminate gender.
In some cases, its use can be explained by notional agreement because words like "everyone", though singular in form, are plural in meaning.
Its use in formal English has increased in recent times with the trend toward gender-inclusive language, but it has been used by respected writers for centuries.
Though singular "they" has a long history of usage and is common in everyday English, its use has been criticized since the late nineteenth century, and acceptance varies.
Inflected forms and derivative pronouns.
Singular "they" has the same inflected forms as the "normal", plural "they", i.e. "them" and "their". They are usually both used with the same verb forms, i.e. "when I tell someone a joke "they" laughs" would be non-standard. In other words, singular "they" is a grammatically plural pronoun with a singular antecedent.
The reflexive form "themselves" is sometimes used but there is an alternative reflexive form "themself". Although "themself" has a long history and re-emerged in the 1980s, it is still fairly rare and is accepted only by a minority. It is sometimes used when referring to a single person of indeterminate gender, where the plural form "themselves" might seem incongruous, as in
Singular "themself" is used systematically in Canadian federal legislative texts in opposition to the plural "themselves".
Usage.
Older usage by respected authors.
"They" was already being used with a singular antecedent in the Middle English of the 14th century.
It is found in the writings of many respected authors, including Chaucer, Shakespeare, Jane Austen, Thackeray, and Shaw:
Alongside "they", however, it was also acceptable to use the pronoun "he" as a (purportedly) gender-neutral pronoun, as in the following:
In Thackeray's writings, we find both
and
And Caxton writes
alongside
Trend to prescription of generic "he" from 19th century.
Preferring "he" as a purportedly gender-neutral pronoun, nineteenth-century grammarians insisted on a singular pronoun on the grounds of number agreement, though permitting the apparent lack of gender agreement, and rejecting "he or she" as clumsy.
A recommendation to use the generic "he", rather than "they", in formal English can be found as early as the mid-18th century, in Ann Fisher's "A New Grammar", where she writes:
The "Masculine Person" answers to the "general Name", which comprehends both "Male" and "Female"; as, "any Person who knows what he says" (as quoted by Ostade)
An 1895 grammar (Baskervill, W.M. and Sewell, J.W.: "An English Grammar for the Use of High School, Academy and College Class") notes the common use of the singular "they" but recommends use of the generic "he", on the basis of number agreement:
Another way of referring to an antecedent which is a distributive pronoun [e.g. "everybody"] or a noun modified by a distributive adjective [e.g. every], is to use the plural of the pronoun following. This is not considered the best usage, the logical analysis requiring the singular pronoun in each case; but the construction is frequently found "when the antecedent includes or implies both genders". The masculine does not really represent a feminine antecedent, and the expression "his or her" is avoided as being cumbrous.—Baskervill, An English Grammar
Baskervill gives a number of examples of recognized authors using the singular "they", including
but prefers the use of "he":
[…] when the antecedent includes both masculine and feminine, or is a distributive word, taking in each of many persons,—the preferred method is to put the pronoun following in the masculine singular […]
—Baskervill, An English Grammar
In 1850, the British Parliament passed an act which provided that, when used in acts of Parliament "words importing the masculine gender shall be deemed and taken to include females".
It has been argued that the real motivation for promoting the "generic" "he" was an androcentic world view, with the default sex of humans being male – and the default gender therefore being masculine.
As Wilson wrote in 1560
and Poole wrote in 1646
In spite of continuous attempts on the part of educationalists to proscribe singular "they" in favour of "he", its use remained widespread, and the advice was largely ignored, even by writers of the period, though the advice may have been observed more by American writers.
Use of the purportedly gender-neutral "he" remained acceptable until at least the 1960s, though some uses of "he" were later criticized as being awkward or silly, for instance when referring to:
Contemporary use of "he" to refer to a generic or indefinite antecedent.
"He" is still sometimes found in contemporary writing when referring to a generic or indeterminate antecedent.
In some cases it is clear from the situation that the persons potentially referred to are likely to be male, as in
In some cases the antecedent may refer to persons who are only "probably" male or to occupations traditionally thought of as male:
In other situations, the antecedent may refer to:
Even in 2010, we find the use of generic "he" recommended:
Trend to gender-neutral language from the 20th century.
In the second half of the 20th century, feminists expressed concern at the use of sexist and male-oriented language.
Such usage included not only the use of "man" as a false generic but also the use of "he" as a generic pronoun.
It was argued that "he" could not sensibly be used as a generic pronoun understood to include men and women.
William Safire in his "On Language" column in "The New York Times" approved of the use of generic "he", mentioning the mnemonic phrase "the male embraces the female".
C. Adendyck from Brooklyn wrote to the "New York Times" in a reply:
"The average American needs the small routines of getting ready for work. As he shaves or blow-dries his hair or pulls on his panty-hose, he is easing himself by small stages into the demands of the day."—C. Badendyck ["sic"], New York Times (1985)
By 1980, the movement had gained wide support, and many organizations, including most publishers, had issued guidelines on the use of gender-neutral language.
Use for specific, known people.
In some situations, an individual may be known but referred to using the pronoun "they" because their gender is unknown or because "they" is their pronoun; social media applications, for example, may permit account holders to select a nonbinary gender such as "gender fluid" or "bigender" and a pronoun, including "they"/"them" which they wish to be used when referring to them.
Contemporary usage.
The use of masculine generic nouns and pronouns in written and spoken language has decreased since the 1960s.
In a corpus of spontaneous speech collected in Australia in the 1990s, singular "they" had become the most frequently used generic pronoun. Similarly, a study from 2002 looking at a corpus of American and British newspapers showed a preference for "they" to be used (rather than generic "he" or "he or she") as a singular epicene pronoun.
The increased use of singular "they" may be at least partly due to an increasing desire for gender-neutral language. While writers a hundred years ago might have had no qualm using "he" with a referent of indeterminate gender, writers today often feel uncomfortable with this. One solution in formal writing has often been to write "he or she", or something similar, but this is considered awkward when used excessively, overly politically correct, or both.
In contemporary usage, singular "they" is used—at least by some—to refer to an indeterminate antecedent, for instance when the sex (social gender) or number of the antecedent is indeterminate, unknown or unrevealed.
Examples include different types of usage.
Use with a pronoun antecedent.
The singular antecedent can be a pronoun such as everybody, someone, anybody, or an interrogative pronoun such as "who":
Use with a generic noun as antecedent.
The singular antecedent can also be a noun such as "person", "patient", or "student":
Some uses are more acceptable than others, and in some case attempts to replace "they" with a (morphologically) singular pronoun can lead to absurd results, as can be tested with the above examples.
Acceptability and prescriptive guidance.
Though both generic "he" and generic "they" have long histories of use, and both are still used, both are also systematically avoided by particular groups.
Style guides that avoid expressing a preference for either approach sometimes recommend recasting a problem sentence, for instance replacing generic expressions with plurals to avoid the criticisms of either party.
The use of singular "they" may be more accepted in British English than in American English, or vice versa.
Usage guidance in British–American style guides.
The Handbook of Non-Sexist Writing by Casey Miller and Kate Swift was first published in the United States but, because of differences in culture and vocabulary, separate British editions have since been published.
These authors accept or recommend singular uses of "they" not just in cases where there is an element of semantic plurality expressed by a word such as "everyone" but also where an indeterminate "person" is referred to, citing examples of such usage even in formal speech. For instance, they quote Ronald Reagan:
In addition to use of singular "they", they – and others – also suggest a number of ways to avoid "the pronoun problem" in gender-neutral writing.
One strategy is to rewrite the sentence to use a plural "they". For instance, in a newspaper story
could have been changed to
Another strategy is to eliminate the pronoun; so
becomes
Other methods of avoiding gender preference include recasting a sentence to use "one", or (for babies) "it".
Usage guidance in American style guides.
Garner's Modern American Usage (2003) recommends cautious use of singular "they", and avoidance where possible because its use is stigmatized.
Garner suggests that use of singular "they" is more acceptable in British English:
and apparently regrets the resistance by the American language community:
He regards the trend toward using singular "they" with antecedents like "everybody", "anyone" and "somebody" as inevitable:
In the 14th edition (1993) of The Chicago Manual of Style, the University of Chicago Press explicitly recommended use of singular use of "they" and "their", noting a "revival" of this usage and citing "its venerable use by such writers as Addison, Austen, Chesterfield, Fielding, Ruskin, Scott, and Shakespeare."
From the 15th edition, this was changed. In Chapter 5 of the 16th edition, now written by Bryan A. Garner, the recommendations are:
and
According to The American Heritage Book of English Usage, many Americans avoid use of "they" to refer to a singular antecedent out of respect for a "traditional" grammatical rule, despite use of singular "they" by modern writers of note and mainstream publications:
The Publication Manual of the American Psychological Association explicitly reject the use of singular "they" and gives the following example as "incorrect" usage:
while also specifically taking the position that generic "he" is unacceptable. The APA recommends using "he or she", recasting the sentence with a plural subject to allow correct use of "they", or simply rewriting the sentence to avoid issues with gender or number.
Strunk & White, the authors of The Elements of Style find use of "they" with a singular antecedent unacceptable:
Their assessment, in 1979, was
Joseph M. Williams, who wrote a number of books on writing with "", discusses the advantages and disadvantages of various solutions when faced with the problem of referring to an antecedent such as "someone", "everyone", "no one" or a noun that does not indicate gender and suggests that this will continue to be a problem for some time. He "suspect[s] that eventually we will accept the plural "they" as a correct singular" but states that currently "formal usage requires a singular pronoun".
According to The Little, Brown Handbook, most experts—and some teachers and employers—find use of singular "they" unacceptable:
It recommends using "he or she" or avoiding the problem by rewriting the sentence to use a plural or omit the pronoun.
The Purdue Online Writing Lab (OWL) maintains that singular "they" is incorrect:
Usage guidance in British style guides.
In the first edition of A Dictionary of Modern English Usage (published in 1926) it is stated that singular "they" is disapproved of by grammarians and should be avoided in favour of the generic "he". Examples of its use by eminent writers are given, but it is suggested that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray", whose sentences are described as having an "old-fashioned sound".
In the second edition of Fowler's, Fowler's Modern English Usage (edited by Sir Ernest Gowers and published in 1965), it is stated that singular "they" is disapproved of by grammarians and, while common in colloquial speech, should preferably be avoided in favour of the generic "he" in prose. Numerous examples of its use by eminent writers are given, but it is still suggested that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray".
According to the third edition of Fowler's (The New Fowler's Modern English Usage, edited by Burchfield and published in 1996) singular "they" has not only been widely used by good writers for centuries, but is now generally accepted, except by some conservative grammarians, including the Fowler of 1926, who ignored the evidence:
The Complete Plain Words was originally written in 1948 by Sir Ernest Gowers, a civil servant, in an attempt by the British civil service to improve "official English". A second edition, edited by Sir Bruce Fraser, was published in 1973. It refers to "they" or "them" as the "equivalent of a singular pronoun of common sex" as "common in speech and not unknown in serious writing " but "stigmatized by grammarians as usage grammatically indefensible. The book's advice for "official writers" (civil servants) is to avoid its use and not to be tempted by its "greater convenience", though "necessity may eventually force it into the category of accepted idiom".
A new edition of Plain Words, revised and updated by Sir Ernest Gowers' great granddaughter, Rebecca Gowers, was published in 2014.
It notes that singular "they" and "them" have become much more widespread since Gowers' original comments, but still finds it "safer" to treat a sentence like 'The reader may toss their book aside' as incorrect "in formal English", while rejecting even more strongly sentences like
The Times Style and Usage Guide (first published in 2003 by "The Times" of London) recommends avoiding sentences like
by using a plural construction:
"The Cambridge Guide to English Usage" (2004) finds singular "they" "unremarkable":
It expresses several preferences.
The Economist Style Guide refers to the use of "they" in sentences like
as "scrambled syntax that people adopt because they cannot bring themselves to use a singular pronoun".
The New Hart's Rules is aimed at those engaged in copy editing, and the emphasis is on the formal elements of presentation including punctuation and typeface, rather than on linguistic style but—like "The Chicago Manual of Style"—makes occasional forays into matters of usage.
It advises against use of the purportedly gender-neutral "he", and suggests cautious use of "they" where "he or she" presents problems.
The 2011 edition of the New International Version Bible uses singular "they" instead of the traditional "he" when translating pronouns that apply to both genders in the original Greek or Hebrew. This decision was based on research by a commission that studied modern English usage and determined that singular "they" ("them"/"their") was by far the most common way that English-language speakers and writers today refer back to singular antecedents such as "whoever", "anyone", "somebody", "a person", "no one", and the like."
Australian usage guidance.
The Australian Federation Press Style Guide for use in preparation of book manuscripts recommends "Gender-neutral language should be used", stating that use of "they" and "their" as singular pronouns is acceptable.
Usage guidance in English grammars.
According to A Comprehensive Grammar of the English Language (1985):
The Cambridge Grammar of the English Language discusses the prescriptivist argument that "they" is a plural pronoun and that the use of "they" with a singular "antecedent" therefore violates the rule of agreement between antecedent and pronoun, but takes the view that "they", though "primarily" plural, can also be singular in a secondary "extended" sense, comparable to the purportedly extended sense of "he" to include female gender.
Use of singular "they" is stated to be "particularly common", even "stylistically neutral" with antecedents such as "everyone", "someone", and "no one", but more restricted when referring to common nouns as antecedents, as in
Use of the pronoun "themself" is described as being "rare" and "acceptable only to a minority of speakers", while use of the morphologically plural "themselves" is considered problematic when referring to "someone" rather that "everyone" (since only the latter implies a plural set).
There are also issues of grammatical acceptability when reflexive pronouns refer to singular noun phrases joined by "or", the following all being problematic:
On the motivation for using singular "they", A Student's Introduction to English Grammar states
The alternative "he or she" can be "far too cumbersome", as in
or even " flatly ungrammatical", as in
"Among younger speakers", use of singular "they" even with definite noun-phrase antecedents finds increasing acceptance, "sidestepping any presumption about the sex of the person referred to", as in
Grammatical and logical analysis.
Steven Pinker suggests that "singular" "they" and plural "they" can be regarded as a pair of homonyms — two words with different meanings but the same spelling and sound. However, this analysis is not extended to "you", another originally plural pronoun that has come to have singular use.
Distribution.
Distributive constructions apply a "single" idea to "multiple" members of a group.
They are typically marked in English by words like "each", "every" and "any". The simplest examples are applied to groups of two, and use words like "either" and "or"—"Would you like tea or coffee?". Since distributive constructions apply an idea relevant to each individual in the group, rather than to the group as a whole, they are most often conceived of as singular, and a singular pronoun is used.
However, many languages, including English, show ambivalence in this regard. Because distribution also requires a group with more than one member, plural forms are sometimes used.
Referential and non-referential anaphors.
According to the traditional analysis, English personal pronouns (e.g. "his", "her", "their") are typically used to refer backward (or forward) within a sentence to a noun phrase (which may be a simple noun). This reference is called an "anaphoric" reference, and the referring pronoun is termed an "anaphor".
The so-called singular "they" is morphologically plural, and is accompanied by a plural verb. However, it is often used in circumstances where an indeterminate antecedent is signified by an indefinite singular antecedent; for example,
In some sentences, typically those including words like "every" or "any", the morphologically singular antecedent does not refer to a single entity but is "anaphorically linked" to the associated pronoun to indicate a set of pairwise relationships, as in the sentence:
One explanation given for the use of "they" to refer to a singular antecedent is "notional agreement", when the antecedent is seen as semantically plural, as in the Shaw quotation
In other words, in the Shaw quotation "no man" is syntactically singular, demonstrated by taking the singular form "goes"; however, it is semantically plural ("all" go [to kill] not to be killed), hence idiomatically requiring "they".
Linguists like Pinker and Huddleston explain sentences like this (and others) in terms of bound variables, a term borrowed from logic. Pinker prefers the terms "quantifier" and "bound variable" to "antecedent" and " pronoun".
The word "reference" is traditionally used in two different senses:
With a morphologically singular antecedent, there are a number of possibilities, including the following:
Cognitive efficiency.
In the light of increasing use of the plural pronoun "they" to refer to morphologically singular antecedents, there have been a few studies that have attempted to determine whether such usage is more "difficult" to understand.
One such study, "In Search of Gender Neutrality: Is Singular "They" a Cognitively Efficient Substitute for Generic "He"?" by Foertsch and Gernsbacher found that "singular "they" is a cognitively efficient substitute for generic "he" or "she", particularly when the antecedent is nonreferential" (e.g. "anybody" or "a nurse") rather than referring to a specific person (e.g. "a runner I knew" or "my nurse"). Clauses with singular "they" were read "just as quickly as clauses containing a gendered pronoun that matched the stereotype of the antecedent" (e.g. "she" for a nurse and "he" for a truck driver) and "much more quickly than clauses containing a gendered pronoun that went against the gender stereotype of the antecedent".
Comparison with other pronouns.
The singular and plural use of "they" can be compared with the pronoun "you", which originally was only plural, but by about 1700 replaced "thou" for singular referents, while retaining the plural verb form.

</doc>
<doc id="28189" url="http://en.wikipedia.org/wiki?curid=28189" title="Space Shuttle">
Space Shuttle

The Space Shuttle was a partially reusable low Earth orbital spacecraft system operated by the U.S. National Aeronautics and Space Administration (NASA). Its official program name was "Space Transportation System", taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982. They were used on a total of 135 missions from 1981 to 2011, launched from the Kennedy Space Center (KSC) in Florida. Operational missions launched numerous satellites, interplanetary probes, and the Hubble Space Telescope (HST); conducted science experiments in orbit; and participated in construction and servicing of the International Space Station. The Shuttle fleet totaled 1322 days, 19 hours, 21 minutes and 23 seconds during missions.
Shuttle components included the Orbiter Vehicle (OV), a pair of recoverable solid rocket boosters (SRBs), and the expendable external tank (ET) containing liquid hydrogen and liquid oxygen. The Shuttle was launched vertically, like a conventional rocket, with the two SRBs operating in parallel with the OV's three main engines, which were fueled from the ET. The SRBs were jettisoned before the vehicle reached orbit, and the ET was jettisoned just before orbit insertion, which used the orbiter's two Orbital Maneuvering System (OMS) engines. At the conclusion of the mission, the orbiter fired its OMS to de-orbit and re-enter the atmosphere. The orbiter glided to a runway landing on Rogers Dry Lake at Edwards Air Force Base in California or at the Shuttle Landing Facility at the KSC. After the landings at Edwards, the orbiter was flown back to KSC on the Shuttle Carrier Aircraft, a specially modified Boeing 747.
The first orbiter, "Enterprise", was built for Approach and Landing Tests and had no orbital capability. Four fully operational orbiters were initially built: "Columbia", "Challenger", "Discovery", and "Atlantis". Of these, "Challenger" and "Columbia" were destroyed in mission accidents in 1986 and 2003 respectively, in which a total of fourteen astronauts were killed. A fifth operational orbiter, "Endeavour", was built in 1991 to replace "Challenger". The Space Shuttle was retired from service upon the conclusion of "Atlantis"‍ '​s final flight on July 21, 2011.
Overview.
The Space Shuttle was a partially reusable human spaceflight vehicle capable of reaching low Earth orbit, commissioned and operated by the US National Aeronautics and Space Administration (NASA) from 1981 to 2011. It resulted from shuttle design studies conducted by NASA and the US Air Force in the 1960s and was first proposed for development as part of an ambitious second-generation Space Transportation System (STS) of space vehicles to follow the Apollo program in a September 1969 report of a Space Task Group headed by Vice President Spiro Agnew to President Richard Nixon. Nixon's post-Apollo NASA budgeting withdrew support of all system components except the Shuttle, to which NASA applied the STS name.
The vehicle consisted of a spaceplane for orbit and re-entry, fueled by expendable liquid hydrogen and liquid oxygen tanks, with reusable strap-on solid booster rockets. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982, all launched from the Kennedy Space Center, Florida. The system was retired from service in 2011 after 135 missions, with "Atlantis" making the final launch of the three-decade Shuttle program on July 8, 2011. The program ended after "Atlantis" landed at the Kennedy Space Center on July 21, 2011. Major missions included launching numerous satellites and interplanetary probes, conducting space science experiments, and servicing and construction of space stations. The first orbiter vehicle, named "Enterprise", was built for the initial Approach and Landing Tests phase and lacked engines, heat shielding, and other equipment necessary for orbital flight. A total of five operational orbiters were built, and of these, two were destroyed in accidents.
It was used for orbital space missions by NASA, the US Department of Defense, the European Space Agency, Japan, and Germany. The United States funded Shuttle development and operations except for the Spacelab modules used on D1 and D2—sponsored by Germany. SL-J was partially funded by Japan.
At launch, it consisted of the "stack", including the dark orange external tank (ET); two white, slender solid rocket boosters (SRBs); and the Orbiter Vehicle, which contained the crew and payload. Some payloads were launched into higher orbits with either of two different upper stages developed for the STS (single-stage Payload Assist Module or two-stage Inertial Upper Stage). The Space Shuttle was stacked in the Vehicle Assembly Building, and the stack mounted on a mobile launch platform held down by four frangible nuts on each SRB, which were detonated at launch.
The Shuttle stack launched vertically like a conventional rocket. It lifted off under the power of its two SRBs and three main engines, which were fueled by liquid hydrogen and liquid oxygen from the ET. The Space Shuttle had a two-stage ascent. The SRBs provided additional thrust during liftoff and first-stage flight. About two minutes after liftoff, frangible nuts were fired, releasing the SRBs, which then parachuted into the ocean, to be retrieved by ships for refurbishment and reuse. The orbiter and ET continued to ascend on an increasingly horizontal flight path under power from its main engines. Upon reaching 17,500 mph (7.8 km/s), necessary for low Earth orbit, the main engines were shut down. The ET, attached by two frangible nuts was then jettisoned to burn up in the atmosphere. After jettisoning the external tank, the orbital maneuvering system (OMS) engines were used to adjust the orbit.
The orbiter carried astronauts and payloads such as satellites or space station parts into low Earth orbit, the Earth's upper atmosphere or thermosphere. Usually, five to seven crew members rode in the orbiter. Two crew members, the commander and pilot, were sufficient for a minimal flight, as in the first four "test" flights, STS-1 through STS-4. The typical payload capacity was about 50045 lb but could be increased depending on the choice of launch configuration. The orbiter carried its payload in a large cargo bay with doors that opened along the length of its top, a feature which made the Space Shuttle unique among spacecraft. This feature made possible the deployment of large satellites such as the Hubble Space Telescope and also the capture and return of large payloads back to Earth.
When the orbiter's space mission was complete, it fired its OMS thrusters to drop out of orbit and re-enter the lower atmosphere. During descent, the orbiter passed through different layers of the atmosphere and decelerated from hypersonic speed primarily by aerobraking. In the lower atmosphere and landing phase, it was more like a glider but with reaction control system (RCS) thrusters and fly-by-wire-controlled hydraulically actuated flight surfaces controlling its descent. It landed on a long runway as a conventional aircraft. The aerodynamic shape was a compromise between the demands of radically different speeds and air pressures during re-entry, hypersonic flight, and subsonic atmospheric flight. As a result, the orbiter had a relatively high sink rate at low altitudes, and it transitioned during re-entry from using RCS thrusters at very high altitudes to flight surfaces in the lower atmosphere.
Early history.
The formal design of what became the Space Shuttle began with the "Phase A" contract design studies issued in the late 1960s. Conceptualization had begun two decades earlier, before the Apollo program of the 1960s. One of the places the concept of a spacecraft returning from space to a horizontal landing originated was within NACA, in 1954, in the form of an aeronautics research experiment later named the X-15. The NACA proposal was submitted by Walter Dornberger.
In 1958, the X-15 concept further developed into proposal to launch an X-15 into space, and another X-series spaceplane proposal, named X-20 Dyna-Soar, as well as variety of aerospace plane concepts and studies. Neil Armstrong was selected to pilot both the X-15 and the X-20. Though the X-20 was not built, another spaceplane similar to the X-20 was built several years later and delivered to NASA in January 1966 called the HL-10 ("HL" indicated "horizontal landing").
In the mid-1960s, the US Air Force conducted classified studies on next-generation space transportation systems and concluded that semi-reusable designs were the cheapest choice. It proposed a development program with an immediate start on a "Class I" vehicle with expendable boosters, followed by slower development of a "Class II" semi-reusable design and possible "Class III" fully reusable design later. In 1967, George Mueller held a one-day symposium at NASA headquarters to study the options. Eighty people attended and presented a wide variety of designs, including earlier US Air Force designs such as the X-20 Dyna-Soar.
In 1968, NASA officially began work on what was then known as the Integrated Launch and Re-entry Vehicle (ILRV). At the same time, NASA held a separate Space Shuttle Main Engine (SSME) competition. NASA offices in Houston and Huntsville jointly issued a Request for Proposal (RFP) for ILRV studies to design a spacecraft that could deliver a payload to orbit but also re-enter the atmosphere and fly back to Earth. For example, one of the responses was for a two-stage design, featuring a large booster and a small orbiter, called the DC-3, one of several Phase A Shuttle designs. After the aforementioned "Phase A" studies, B, C, and D phases progressively evaluated in-depth designs up to 1972. In the final design, the bottom stage was recoverable solid rocket boosters, and the top stage used an expendable external tank.
In 1969, President Richard Nixon decided to support proceeding with Space Shuttle development. A series of development programs and analysis refined the basic design, prior to full development and testing. In August 1973, the X-24B proved that an unpowered spaceplane could re-enter Earth's atmosphere for a horizontal landing.
Across the Atlantic, European ministers met in Belgium in 1973 to authorize Western Europe's manned orbital project and its main contribution to Space Shuttle—the Spacelab program. Spacelab would provide a multidisciplinary orbital space laboratory and additional space equipment for the Shuttle.
Description.
The Space Shuttle was the first operational orbital spacecraft designed for reuse. It carried different payloads to low Earth orbit, provided crew rotation and supplies for the International Space Station (ISS), and performed satellite servicing and repair. The orbiter could also recover satellites and other payloads from orbit and return them to Earth. Each Shuttle was designed for a projected lifespan of 100 launches or ten years of operational life, although this was later extended. The person in charge of designing the STS was Maxime Faget, who had also overseen the Mercury, Gemini, and Apollo spacecraft designs. The crucial factor in the size and shape of the Shuttle orbiter was the requirement that it be able to accommodate the largest planned commercial and military satellites, and have over 1,000 mile cross-range recovery range to meet the requirement for classified USAF missions for a once-around abort from a launch to a polar orbit. The militarily specified 1,085 nm cross range requirement was one of the primary reasons for the Shuttle's large wings, compared to modern commercial designs with very minimal control surfaces and glide capability. Factors involved in opting for solid rockets and an expendable fuel tank included the desire of the Pentagon to obtain a high-capacity payload vehicle for satellite deployment, and the desire of the Nixon administration to reduce the costs of space exploration by developing a spacecraft with reusable components.
Each Space Shuttle was a reusable launch system composed of three main assemblies: the reusable OV, the expendable ET, and the two reusable SRBs. Only the OV entered orbit shortly after the tank and boosters are jettisoned. The vehicle was launched vertically like a conventional rocket, and the orbiter glided to a horizontal landing like an airplane, after which it was refurbished for reuse. The SRBs parachuted to splashdown in the ocean where they were towed back to shore and refurbished for later Shuttle missions.
Five operational OVs were built: "Columbia" (OV-102), "Challenger" (OV-099), "Discovery" (OV-103), "Atlantis" (OV-104), and "Endeavour" (OV-105). A mock-up, "Inspiration", currently stands at the entrance to the Astronaut Hall of Fame. An additional craft, "Enterprise" (OV-101), was built for atmospheric testing gliding and landing; it was originally intended to be outfitted for orbital operations after the test program, but it was found more economical to upgrade the structural test article STA-099 into orbiter "Challenger" (OV-099). "Challenger" disintegrated 73 seconds after launch in 1986, and "Endeavour" was built as a replacement from structural spare components. Building "Endeavour" cost about US$1.7 billion. "Columbia" broke apart over Texas during re-entry in 2003. A Space Shuttle launch cost around $450 million.
Roger A. Pielke, Jr. has estimated that the Space Shuttle program cost about US$170 billion (2008 dollars) through early 2008; the average cost per flight was about US$1.5 billion. Two missions were paid for by Germany, Spacelab D1 and D2 (D for "Deutschland") with a payload control center in Oberpfaffenhofen. D1 was the first time that control of a manned STS mission payload was not in U.S. hands.
At times, the orbiter itself was referred to as the Space Shuttle. This was not technically correct as the "Space Shuttle" was the combination of the orbiter, the external tank, and the two solid rocket boosters. These components, once assembled in the Vehicle Assembly Building originally built to assemble the Apollo Saturn V rocket, were commonly referred to as the "stack".
Responsibility for the Shuttle components was spread among multiple NASA field centers. The Kennedy Space Center was responsible for launch, landing and turnaround operations for equatorial orbits (the only orbit profile actually used in the program), the US Air Force at the Vandenberg Air Force Base was responsible for launch, landing and turnaround operations for polar orbits (though this was never used), the Johnson Space Center served as the central point for all Shuttle operations, the Marshall Space Flight Center was responsible for the main engines, external tank, and solid rocket boosters, the John C. Stennis Space Center handled main engine testing, and the Goddard Space Flight Center managed the global tracking network.
Orbiter vehicle.
The orbiter resembles a conventional aircraft, with double-delta wings swept 81° at the inner leading edge and 45° at the outer leading edge. Its vertical stabilizer's leading edge is swept back at a 50° angle. The four elevons, mounted at the trailing edge of the wings, and the rudder/speed brake, attached at the trailing edge of the stabilizer, with the body flap, controlled the orbiter during descent and landing.
The orbiter's payload bay measures 15 by, comprising most of the fuselage. Information declassified in 2011 showed that the payload bay was designed specifically to accommodate the KH-9 HEXAGON spy satellite operated by the National Reconnaissance Office. Two mostly symmetrical lengthwise payload bay doors hinged on either side of the bay comprise its entire top. Payloads were generally loaded horizontally into the bay while the orbiter is oriented vertically on the launch pad and unloaded vertically in the near-weightless orbital environment by the orbiter's robotic remote manipulator arm (under astronaut control), EVA astronauts, or under the payloads' own power (as for satellites attached to a rocket "upper stage" for deployment.)
Three Space Shuttle Main Engines (SSMEs) are mounted on the orbiter's aft fuselage in a triangular pattern. The engine nozzles can gimbal 10.5 degrees up and down, and 8.5 degrees from side to side during ascent to change the direction of their thrust to steer the Shuttle. The orbiter structure is made primarily from aluminum alloy, although the engine structure is made primarily from titanium alloy.
The operational orbiters built were OV-102 "Columbia", OV-099 "Challenger", OV-103 "Discovery", OV-104 "Atlantis", and OV-105 "Endeavour".
External tank.
The main function of the Space Shuttle external tank was to supply the liquid oxygen and hydrogen fuel to the main engines. It was also the backbone of the launch vehicle, providing attachment points for the two solid rocket boosters and the orbiter. The external tank was the only part of the Shuttle system that was not reused. Although the external tanks were always discarded, it would have been possible to take them into orbit and re-use them (such as for incorporation into a space station).
Solid rocket boosters.
Two solid rocket boosters (SRBs) each provided 2800000 lbf of thrust at liftoff, which was 83% of the total thrust at liftoff. The SRBs were jettisoned two minutes after launch at a height of about 150000 ft, and then deployed parachutes and landed in the ocean to be recovered. The SRB cases were made of steel about ½ inch (13 mm) thick. The solid rocket boosters were re-used many times; the casing used in Ares I engine testing in 2009 consisted of motor cases that had been flown, collectively, on 48 Shuttle missions, including STS-1.
Astronauts who have flown on multiple spacecraft report that Shuttle delivers a rougher ride than Apollo or Soyuz. The additional vibration is caused by the solid rocket boosters, as solid fuel does not burn as evenly as liquid fuel. The vibration dampens down after the solid rocket boosters have been jettisoned.
Orbiter add-ons.
The orbiter could be used in conjunction with a variety of add-ons depending on the mission. This included orbital laboratories (Spacelab, Spacehab), boosters for launching payloads farther into space (Inertial Upper Stage, Payload Assist Module), and other functions, such as provided by Extended Duration Orbiter, Multi-Purpose Logistics Modules, or Canadarm (RMS). An upper stage called Transfer Orbit Stage (Orbital Science Corp. TOS-21) was also used once. Other types of systems and racks were part of the modular Spacelab system —pallets, igloo, IPS, etc., which also supported special missions such as SRTM.
Spacelab.
A major component of the Space Shuttle Program was Spacelab, primarily contributed by a consortium of European countries, and operated in conjunction with the United States and international partners. Supported by a modular system of pressurized modules, pallets, and systems, Spacelab missions executed on multidisciplinary science, orbital logistics, and international cooperation. Over 29 missions flew on subjects ranging from astronomy, microgravity, radar, and life sciences, to name a few. Spacelab hardware also supported missions such as Hubble (HST) servicing and space station resupply. STS-2 and STS-3 provided testing, and the first full mission was Spacelab-1 (STS-9) launched on November 28, 1983.
Spacelab formally began in 1973, after a meeting in Brussels, Belgium, by European heads of state. Within the decade, Spacelab went into orbit and provided Europe and the United States with an orbital workshop and hardware system. International cooperation, science, and exploration were realized on Spacelab.
Flight systems.
The Shuttle was one of the earliest craft to use a computerized fly-by-wire digital flight control system. This means no mechanical or hydraulic linkages connected the pilot's control stick to the control surfaces or reaction control system thrusters. The control algorithm, which used a classical Proportional Integral Derivative (PID) approach, was developed and maintained by Honeywell. The Shuttle's fly-by-wire digital flight control system was composed of 4 control systems each addressing a different mission phase: Ascent, Descent, On-Orbit and Aborts. Honeywell is also credited with the design and implementation of the Shuttle's Nose Wheel Steering Control Algorithm that allowed the Orbiter to safely land at Kennedy Space Center's Shuttle Runway.
A concern with using digital fly-by-wire systems on the Shuttle was reliability. Considerable research went into the Shuttle computer system. The Shuttle used five identical redundant IBM 32-bit general purpose computers (GPCs), model AP-101, constituting a type of embedded system. Four computers ran specialized software called the Primary Avionics Software System (PASS). A fifth backup computer ran separate software called the Backup Flight System (BFS). Collectively they were called the Data Processing System (DPS).
The design goal of the Shuttle's DPS was fail-operational/fail-safe reliability. After a single failure, the Shuttle could still continue the mission. After two failures, it could still land safely.
The four general-purpose computers operated essentially in lockstep, checking each other. If one computer provided a different result than the other three (i.e. the one computer failed), the three functioning computers "voted" it out of the system. This isolated it from vehicle control. If a second computer of the three remaining failed, the two functioning computers voted it out. A very unlikely failure mode would have been where two of the computers produced result A, and two produced result B (a two-two split). In this unlikely case, one group of two was to be picked at random.
The Backup Flight System (BFS) was separately developed software running on the fifth computer, used only if the entire four-computer primary system failed. The BFS was created because although the four primary computers were hardware redundant, they all ran the same software, so a generic software problem could crash all of them. Embedded system avionic software was developed under totally different conditions from public commercial software: the number of code lines was tiny compared to a public commercial software product, changes were only made infrequently and with extensive testing, and many programming and test personnel worked on the small amount of computer code. However, in theory it could have still failed, and the BFS existed for that contingency. While the BFS could run in parallel with PASS, the BFS never engaged to take over control from PASS during any Shuttle mission.
The software for the Shuttle computers was written in a high-level language called HAL/S, somewhat similar to PL/I. It is specifically designed for a real time embedded system environment.
The IBM AP-101 computers originally had about 424 kilobytes of magnetic core memory each. The CPU could process about 400,000 instructions per second. They had no hard disk drive, and loaded software from magnetic tape cartridges.
In 1990, the original computers were replaced with an upgraded model AP-101S, which had about 2.5 times the memory capacity (about 1 megabyte) and three times the processor speed (about 1.2 million instructions per second). The memory was changed from magnetic core to semiconductor with battery backup.
Early Shuttle missions, starting in November 1983, took along the Grid Compass, arguably one of the first laptop computers. The GRiD was given the name SPOC, for Shuttle Portable Onboard Computer. Use on the Shuttle required both hardware and software modifications which were incorporated into later versions of the commercial product. It was used to monitor and display the Shuttle's ground position, path of the next two orbits, show where the Shuttle had line of sight communications with ground stations, and determine points for location-specific observations of the Earth. The Compass sold poorly, as it cost at least US$8000, but it offered unmatched performance for its weight and size. NASA was one of its main customers.
During its service life, the Shuttle's Control System never experienced a failure. Many of the lessons learned have been used to design today's high speed control algorithms.
Orbiter markings and insignia.
The prototype orbiter "Enterprise" originally had a flag of the United States on the upper surface of the left wing and the letters "USA" in black on the right wing. The name "Enterprise" was painted in black on the payload bay doors just above the hinge and behind the crew module; on the aft end of the payload bay doors was the NASA "worm" logotype in gray. Underneath the rear of the payload bay doors on the side of the fuselage just above the wing is the text "United States" in black with a flag of the United States ahead of it.
The first operational orbiter, "Columbia", originally had the same markings as "Enterprise", although the letters "USA" on the right wing were slightly larger and spaced farther apart. "Columbia" also had black markings which "Enterprise" lacked on its forward RCS module, around the cockpit windows, and on its vertical stabilizer, and had distinctive black "chines" on the forward part of its upper wing surfaces, which none of the other orbiters had.
"Challenger" established a modified marking scheme for the shuttle fleet that was matched by "Discovery", "Atlantis" and "Endeavour". The letters "USA" in black above an American flag were displayed on the left wing, with the NASA "worm" logotype in gray centered above the name of the orbiter in black on the right wing. The name of the orbiter was inscribed not on the payload bay doors, but on the forward fuselage just below and behind the cockpit windows. This would make the name visible when the shuttle was photographed in orbit with the doors open.
In 1983, "Enterprise" had its wing markings changed to match "Challenger", and the NASA "worm" logotype on the aft end of the payload bay doors was changed from gray to black. Some black markings were added to the nose, cockpit windows and vertical tail to more closely resemble the flight vehicles, but the name "Enterprise" remained on the payload bay doors as there was never any need to open them. "Columbia" had its name moved to the forward fuselage to match the other flight vehicles after STS-61-C, during the 1986–88 hiatus when the shuttle fleet was grounded following the loss of "Challenger", but retained its original wing markings until its last overhaul (after STS-93), and its unique black wing "chines" for the remainder of its operational life.
Beginning in 1998, the flight vehicles' markings were modified to incorporate the NASA "meatball" insignia. The "worm" logotype, which the agency had phased out, was removed from the payload bay doors and the "meatball" insignia was added aft of the "United States" text on the lower aft fuselage. The "meatball" insignia was also displayed on the left wing, with the American flag above the orbiter's name, left-justified rather than centered, on the right wing. The three surviving flight vehicles, "Discovery", "Atlantis" and "Endeavour", still bear these markings as museum displays. "Enterprise" became the property of the Smithsonian Institution in 1985 and was no longer under NASA's control when these changes were made, hence the prototype orbiter still has its 1983 markings and still has its name on the payload bay doors.
Upgrades.
The Space Shuttle was initially developed in the 1970s, but received many upgrades and modifications afterward to improve performance, reliability and safety. Internally, the Shuttle remained largely similar to the original design, with the exception of the improved avionics computers. In addition to the computer upgrades, the original analog primary flight instruments were replaced with modern full-color, flat-panel display screens, called a glass cockpit, which is similar to those of contemporary airliners. To facilitate construction of ISS, the internal airlocks of each orbiter except "Columbia" were replaced with external docking systems to allow for a greater amount of cargo to be stored on the Shuttle's mid-deck during station resupply missions.
The Space Shuttle Main Engines (SSMEs) had several improvements to enhance reliability and power. This explains phrases such as "Main engines throttling up to 104 percent." This did not mean the engines were being run over a safe limit. The 100 percent figure was the original specified power level. During the lengthy development program, Rocketdyne determined the engine was capable of safe reliable operation at 104 percent of the originally specified thrust. NASA could have rescaled the output number, saying in essence 104 percent is now 100 percent. To clarify this would have required revising much previous documentation and software, so the 104 percent number was retained. SSME upgrades were denoted as "block numbers", such as block I, block II, and block IIA. The upgrades improved engine reliability, maintainability and performance. The 109% thrust level was finally reached in flight hardware with the Block II engines in 2001. The normal maximum throttle was 104 percent, with 106 percent or 109 percent used for mission aborts.
For the first two missions, STS-1 and STS-2, the external tank was painted white to protect the insulation that covers much of the tank, but improvements and testing showed that it was not required. The weight saved by not painting the tank resulted in an increase in payload capability to orbit. Additional weight was saved by removing some of the internal "stringers" in the hydrogen tank that proved unnecessary. The resulting "light-weight external tank" was first flown on STS-6 and used on the majority of Shuttle missions. STS-91 saw the first flight of the "super light-weight external tank". This version of the tank was made of the 2195 aluminum-lithium alloy. It weighed 3.4 metric tons (7,500 lb) less than the last run of lightweight tanks, allowing the Shuttle to deliver heavy elements to ISS's high inclination orbit. As the Shuttle was not operated without a crew, each of these improvements was first flown on operational flights.
The solid rocket boosters underwent improvements as well. Design engineers added a third O-ring seal to the joints between the segments after the 1986 Space Shuttle "Challenger" disaster.
Several other SRB improvements were planned to improve performance and safety, but never came to be. These culminated in the considerably simpler, lower cost, probably safer and better-performing Advanced Solid Rocket Booster. These rockets entered production in the early to mid-1990s to support the Space Station, but were later canceled to save money after the expenditure of $2.2 billion. The loss of the ASRB program resulted in the development of the Super LightWeight external Tank (SLWT), which provided some of the increased payload capability, while not providing any of the safety improvements. In addition, the US Air Force developed their own much lighter single-piece SRB design using a filament-wound system, but this too was canceled.
STS-70 was delayed in 1995, when woodpeckers bored holes in the foam insulation of "Discovery"'s external tank. Since then, NASA has installed commercial plastic owl decoys and inflatable owl balloons which had to be removed prior to launch. The delicate nature of the foam insulation had been the cause of damage to the Thermal Protection System, the tile heat shield and heat wrap of the orbiter. NASA remained confident that this damage, while it was the primary cause of the Space Shuttle "Columbia" disaster on February 1, 2003, would not jeopardize the completion of the International Space Station (ISS) in the projected time allotted.
A cargo-only, unmanned variant of the Shuttle was variously proposed and rejected since the 1980s. It was called the Shuttle-C, and would have traded re-usability for cargo capability, with large potential savings from reusing technology developed for the Space Shuttle. Another proposal was to convert the payload bay into a passenger area, with versions ranging from 30 to 74 seats, three days in orbit, and cost US$1.5 million per seat.
On the first four Shuttle missions, astronauts wore modified US Air Force high-altitude full-pressure suits, which included a full-pressure helmet during ascent and descent. From the fifth flight, STS-5, until the loss of "Challenger", one-piece light blue nomex flight suits and partial-pressure helmets were worn. A less-bulky, partial-pressure version of the high-altitude pressure suits with a helmet was reinstated when Shuttle flights resumed in 1988. The Launch-Entry Suit ended its service life in late 1995, and was replaced by the full-pressure Advanced Crew Escape Suit (ACES), which resembled the Gemini space suit in design, but retained the orange color of the Launch-Entry Suit.
To extend the duration that orbiters could stay docked at the ISS, the Station-to-Shuttle Power Transfer System (SSPTS) was installed. The SSPTS allowed these orbiters to use power provided by the ISS to preserve their consumables. The SSPTS was first used successfully on STS-118.
Specifications.
Orbiter (for "Endeavour", OV-105)
External tank (for SLWT)
Solid Rocket Boosters
System Stack
Mission profile.
Launch preparation.
All Space Shuttle missions were launched from Kennedy Space Center (KSC). The weather criteria used for launch included, but were not limited to: precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Shuttle was not launched under conditions where it could have been struck by lightning. Aircraft are often struck by lightning with no adverse effects because the electricity of the strike is dissipated through its conductive structure and the aircraft is not electrically grounded. Like most jet airliners, the Shuttle was mainly constructed of conductive aluminum, which would normally shield and protect the internal systems. However, upon liftoff the Shuttle sent out a long exhaust plume as it ascended, and this plume could have triggered lightning by providing a current path to ground. The NASA Anvil Rule for a Shuttle launch stated that an anvil cloud could not appear within a distance of 10 nautical miles. The Shuttle Launch Weather Officer monitored conditions until the final decision to scrub a launch was announced. In addition, the weather conditions had to be acceptable at one of the Transatlantic Abort Landing sites (one of several Space Shuttle abort modes) to launch as well as the solid rocket booster recovery area. While the Shuttle might have safely endured a lightning strike, a similar strike caused problems on Apollo 12, so for safety NASA chose not to launch the Shuttle if lightning was possible (NPR8715.5).
Historically, the Shuttle was not launched if its flight would run from December to January (a year-end rollover or YERO). Its flight software, designed in the 1970s, was not designed for this, and would require the orbiter's computers be reset through a change of year, which could cause a glitch while in orbit. In 2007, NASA engineers devised a solution so Shuttle flights could cross the year-end boundary.
Launch.
After the final hold in the countdown at T-minus 9 minutes, the Shuttle went through its final preparations for launch, and the countdown was automatically controlled by the Ground Launch Sequencer (GLS), software at the Launch Control Center, which stopped the count if it sensed a critical problem with any of the Shuttle's onboard systems. The GLS handed off the count to the Shuttle's on-board computers at T minus 31 seconds, in a process called auto sequence start.
At T-minus 16 seconds, the massive sound suppression system (SPS) began to drench the Mobile Launcher Platform (MLP) and SRB trenches with 300000 USgal of water to protect the Orbiter from damage by acoustical energy and rocket exhaust reflected from the flame trench and MLP during lift off.
At T-minus 10 seconds, hydrogen igniters were activated under each engine bell to quell the stagnant gas inside the cones before ignition. Failure to burn these gases could trip the onboard sensors and create the possibility of an overpressure and explosion of the vehicle during the firing phase. The main engine turbopumps also began charging the combustion chambers with liquid hydrogen and liquid oxygen at this time. The computers reciprocated this action by allowing the redundant computer systems to begin the firing phase.
The three main engines (SSMEs) started at "T"-6.6 seconds. The main engines ignited sequentially via the Shuttle's general purpose computers (GPCs) at 120 millisecond intervals. All three SSMEs were required to reach 90% rated thrust within three seconds, otherwise the onboard computers would initiate an RSLS abort. If all three engines indicated nominal performance by "T"-3 seconds, they were commanded to gimbal to liftoff configuration and the command would be issued to arm the SRBs for ignition at "T"-0. Between "T"-6.6 seconds and "T"-3 seconds, while the SSMEs were firing but the SRBs were still bolted to the pad, the offset thrust caused the entire launch stack (boosters, tank and Shuttle) to pitch down 25.5 in measured at the tip of the external tank. The three second delay after confirmation of SSME operation was to allow the stack to return to nearly vertical. At "T"-0 seconds, the 8 frangible nuts holding the SRBs to the pad were detonated, the SSMEs were commanded to 100% throttle, and the SRBs were ignited. By "T"+0.23 seconds, the SRBs built up enough thrust for liftoff to commence, and reached maximum chamber pressure by "T"+0.6 seconds. The Johnson Space Center's Mission Control Center assumed control of the flight once the SRBs had cleared the launch tower.
Shortly after liftoff, the Shuttle's main engines were throttled up to 104.5% and the vehicle began a combined roll, pitch and yaw maneuver that placed it onto the correct heading (azimuth) for the planned orbital inclination and in a heads down attitude with wings level. The Shuttle flew upside down during the ascent phase. This orientation allowed a trim angle of attack that was favorable for aerodynamic loads during the region of high dynamic pressure, resulting in a net positive load factor, as well as providing the flight crew with a view of the horizon as a visual reference. The vehicle climbed in a progressively flattening arc, accelerating as the weight of the SRBs and main tank decreased. To achieve low orbit requires much more horizontal than vertical acceleration. This was not visually obvious, since the vehicle rose vertically and was out of sight for most of the horizontal acceleration. The near circular orbital velocity at the 380 km altitude of the International Space Station is 27650 km/h, roughly equivalent to Mach 23 at sea level. As the International Space Station orbits at an inclination of 51.6 degrees, missions going there must set orbital inclination to the same value in order to rendezvous with the station.
Around 30 seconds into ascent, the SSMEs were throttled down—usually to 72%, though this varied—to reduce the maximum aerodynamic forces acting on the Shuttle at a point called Max Q. Additionally, the propellant grain design of the SRBs caused their thrust to drop by about 30% by 50 seconds into ascent. Once the Orbiter's guidance verified that Max Q would be within Shuttle structural limits, the main engines were throttled back up to 104.5%; this throttling down and back up was called the "thrust bucket". To maximize performance, the throttle level and timing of the thrust bucket was shaped to bring the Shuttle as close to aerodynamic limits as possible.
At around "T"+126 seconds, pyrotechnic fasteners released the SRBs and small separation rockets pushed them laterally away from the vehicle. The SRBs parachuted back to the ocean to be reused. The Shuttle then began accelerating to orbit on the main engines. Acceleration at this point would typically fall to .9 "g", and the vehicle would take on a somewhat nose-up angle to the horizon – it used the main engines to gain and then maintain altitude while it accelerated horizontally towards orbit. At about five and three-quarter minutes into ascent, the orbiter's direct communication links with the ground began to fade, at which point it rolled heads up to reroute its communication links to the Tracking and Data Relay Satellite system.
At about seven and a half minutes into ascent, the mass of the vehicle was low enough that the engines had to be throttled back to limit vehicle acceleration to 3 "g" (29.34 m/s², equivalent to accelerating from zero to 105 kph each second). The Shuttle would maintain this acceleration for the next minute, and main engine cut off (MECO) occurred at about eight and a half minutes after launch. The main engines were shut down before complete depletion of propellant, as running dry would have destroyed the engines. The oxygen supply was terminated before the hydrogen supply, as the SSMEs reacted unfavorably to other shutdown modes. (Liquid oxygen has a tendency to react violently, and supports combustion when it encounters hot engine metal.) A few seconds after MECO, the external tank was released by firing pyrotechnic fasteners.
At this point the Shuttle and external tank were on a slightly suborbital trajectory, coasting up towards apogee. Once at apogee, about half an hour after MECO, the Shuttle's Orbital Maneuvering System (OMS) engines were fired to raise its perigee and achieve orbit, while the external tank fell back into the atmosphere and burned up over the Indian Ocean or the Pacific Ocean depending on launch profile. The sealing action of the tank plumbing and lack of pressure relief systems on the external tank helped it break up in the lower atmosphere. After the foam burned away during re-entry, the heat caused a pressure buildup in the remaining liquid oxygen and hydrogen until the tank exploded. This ensured that any pieces that fell back to Earth were small.
Ascent tracking.
The Shuttle was monitored throughout its ascent for short range tracking (10 seconds before liftoff through 57 seconds after), medium range (7 seconds before liftoff through 110 seconds after) and long range (7 seconds before liftoff through 165 seconds after). Short range cameras included 22 16mm cameras on the Mobile Launch Platform and 8 16mm on the Fixed Service Structure, 4 high speed fixed cameras located on the perimeter of the launch complex plus an additional 42 fixed cameras with 16mm motion picture film. Medium range cameras included remotely operated tracking cameras at the launch complex plus 6 sites along the immediate coast north and south of the launch pad, each with 800mm lens and high speed cameras running 100 frames per second. These cameras ran for only 4–10 seconds due to limitations in the amount of film available. Long range cameras included those mounted on the external tank, SRBs and orbiter itself which streamed live video back to the ground providing valuable information about any debris falling during ascent. Long range tracking cameras with 400-inch film and 200-inch video lenses were operated by a photographer at Playalinda Beach as well as 9 other sites from 38 miles north at the Ponce Inlet to 23 miles south to Patrick Air Force Base (PAFB) and additional mobile optical tracking camera was stationed on Merritt Island during launches. A total of 10 HD cameras were used both for ascent information for engineers and broadcast feeds to networks such as NASA TV and HDNet The number of cameras significantly increased and numerous existing cameras were upgraded at the recommendation of the Columbia Accident Investigation Board to provide better information about the debris during launch. Debris was also tracked using a pair of Weibel Continuous Pulse Doppler X-band radars, one on board the SRB recovery ship MV Liberty Star positioned north east of the launch pad and on a ship positioned south of the launch pad. Additionally, during the first 2 flights following the loss of Columbia and her crew, a pair of NASA WB-57 reconnaissance aircraft equipped with HD Video and Infrared flew at 60000 ft to provide additional views of the launch ascent. Kennedy Space Center also invested nearly $3 million in improvements to the digital video analysis systems in support of debris tracking.
In orbit.
Once in orbit, the Shuttle usually flew at an altitude of 320 kilometers (200 miles), and occasionally as high as 650 kilometers. In the 1980s and 1990s, many flights involved space science missions on the NASA/ESA Spacelab, or launching various types of satellites and science probes. By the 1990s and 2000s the focus shifted more to servicing the space station, with fewer satellite launches. Most missions involved staying in orbit several days to two weeks, although longer missions were possible with the Extended Duration Orbiter add-on or when attached to a space station.
Re-entry and landing.
Almost the entire Space Shuttle re-entry procedure, except for lowering the landing gear and deploying the air data probes, was normally performed under computer control. However, the re-entry could be flown entirely manually if an emergency arose. The approach and landing phase could be controlled by the autopilot, but was usually hand flown.
The vehicle began re-entry by firing the Orbital maneuvering system engines, while flying upside down, backside first, in the opposite direction to orbital motion for approximately three minutes, which reduced the Shuttle's velocity by about 200 mph. The resultant slowing of the Shuttle lowered its orbital perigee down into the upper atmosphere. The Shuttle then flipped over, by pushing its nose down (which was actually "up" relative to the Earth, because it was flying upside down). This OMS firing was done roughly halfway around the globe from the landing site.
The vehicle started encountering more significant air density in the lower thermosphere at about 400000 ft, at around Mach 25, 8200 m/s. The vehicle was controlled by a combination of RCS thrusters and control surfaces, to fly at a 40-degree nose-up attitude, producing high drag, not only to slow it down to landing speed, but also to reduce reentry heating. As the vehicle encountered progressively denser air, it began a gradual transition from spacecraft to aircraft. In a straight line, its 40-degree nose-up attitude would cause the descent angle to flatten-out, or even rise. The vehicle therefore performed a series of four steep S-shaped banking turns, each lasting several minutes, at up to 70 degrees of bank, while still maintaining the 40-degree angle of attack. In this way it dissipated speed sideways rather than upwards. This occurred during the 'hottest' phase of re-entry, when the heat-shield glowed red and the G-forces were at their highest. By the end of the last turn, the transition to aircraft was almost complete. The vehicle leveled its wings, lowered its nose into a shallow dive and began its approach to the landing site.
The orbiter's maximum glide ratio/lift-to-drag ratio varies considerably with speed, ranging from 1:1 at hypersonic speeds, 2:1 at supersonic speeds and reaching 4.5:1 at subsonic speeds during approach and landing.
In the lower atmosphere, the orbiter flies much like a conventional glider, except for a much higher descent rate, over 50 m/s or 9,800 fpm. At approximately Mach 3, two air data probes, located on the left and right sides of the orbiter's forward lower fuselage, are deployed to sense air pressure related to the vehicle's movement in the atmosphere.
Final approach and landing phase.
When the approach and landing phase began, the orbiter was at a 3000 m altitude, 12 km from the runway. The pilots applied aerodynamic braking to help slow down the vehicle. The orbiter's speed was reduced from 682 to, approximately, at touch-down (compared to 260 km/h for a jet airliner). The landing gear was deployed while the Orbiter was flying at 430 km/h. To assist the speed brakes, a 12 m drag chute was deployed either after main gear or nose gear touchdown (depending on selected chute deploy mode) at about 343 km/h. The chute was jettisoned once the orbiter slowed to 110 km/h.
Post-landing processing.
After landing, the vehicle stayed on the runway for several hours for the orbiter to cool. Teams at the front and rear of the orbiter tested for presence of hydrogen, hydrazine, monomethylhydrazine, nitrogen tetroxide and ammonia (fuels and by-products of the reaction control system and the orbiter's three APUs). If hydrogen was detected, an emergency would be declared, the orbiter powered down and teams would evacuate the area. A convoy of 25 specially designed vehicles and 150 trained engineers and technicians approached the orbiter. Purge and vent lines were attached to remove toxic gases from fuel lines and the cargo bay about 45–60 minutes after landing. A flight surgeon boarded the orbiter for initial medical checks of the crew before disembarking. Once the crew left the orbiter, responsibility for the vehicle was handed from the Johnson Space Center back to the Kennedy Space Center
If the mission ended at Edwards Air Force Base in California, White Sands Space Harbor in New Mexico, or any of the runways the orbiter might use in an emergency, the orbiter was loaded atop the Shuttle Carrier Aircraft, a modified 747, for transport back to the Kennedy Space Center, landing at the Shuttle Landing Facility. Once at the Shuttle Landing Facility, the orbiter was then towed 2 mi along a tow-way and access roads normally used by tour buses and KSC employees to the Orbiter Processing Facility where it began a months-long preparation process for the next mission.
Landing sites.
NASA preferred Space Shuttle landings to be at Kennedy Space Center. If weather conditions made landing there unfavorable, the Shuttle could delay its landing until conditions are favorable, touch down at Edwards Air Force Base, California, or use one of the multiple alternate landing sites around the world. A landing at any site other than Kennedy Space Center meant that after touchdown the Shuttle must be mated to the Shuttle Carrier Aircraft and returned to Cape Canaveral. Space Shuttle "Columbia" (STS-3) once landed at the White Sands Space Harbor, New Mexico; this was viewed as a last resort as NASA scientists believe that the sand could potentially damage the Shuttle's exterior.
There were many alternative landing sites that were never used.
Risk contributors.
An example of technical risk analysis for a STS mission is SPRA iteration 3.1 top risk contributors for STS-133:
An internal NASA risk assessment study (conducted by the Shuttle Program Safety and Mission Assurance Office at Johnson Space Center) released in late 2010 or early 2011 concluded that the agency had seriously underestimated the level of risk involved in operating the Shuttle. The report assessed that there was a 1 in 9 chance of a catastrophic disaster during the first nine flights of the Shuttle but that safety improvements had later improved the risk ratio to 1 in 90.
Fleet history.
Below is a list of major events in the Space Shuttle orbiter fleet.
Sources: NASA launch manifest, NASA Space Shuttle archive
Shuttle disasters.
On January 28, 1986, "Challenger" disintegrated 73 seconds after launch due to the failure of the right SRB, killing all seven astronauts on board. The disaster was caused by low-temperature impairment of an O-ring, a mission critical seal used between segments of the SRB casing. The failure of a lower O-ring seal allowed hot combustion gases to escape from between the booster sections and burn through the adjacent external tank, causing it to explode. Repeated warnings from design engineers voicing concerns about the lack of evidence of the O-rings' safety when the temperature was below 53 °F (12 °C) had been ignored by NASA managers.
On February 1, 2003, "Columbia" disintegrated during re-entry, killing its crew of seven, because of damage to the carbon-carbon leading edge of the wing caused during launch. Ground control engineers had made three separate requests for high-resolution images taken by the Department of Defense that would have provided an understanding of the extent of the damage, while NASA's chief thermal protection system (TPS) engineer requested that astronauts on board "Columbia" be allowed to leave the vehicle to inspect the damage. NASA managers intervened to stop the Department of Defense's assistance and refused the request for the spacewalk, and thus the feasibility of scenarios for astronaut repair or rescue by "Atlantis" were not considered by NASA management at the time.
Retirement.
NASA retired the Space Shuttle in 2011, after 30 years of service. The Shuttle was originally conceived of and presented to the public as a "Space Truck", which would, among other things, be used to build a United States space station in low earth orbit in the early 1990s. When the US space station evolved into the International Space Station project, which suffered from long delays and design changes before it could be completed, the service life of the Space Shuttle was extended several times until 2011, serving at least 15 years longer than it was originally designed to do. "Discovery" was the first of NASA's three remaining operational Space Shuttles to be retired.
The final Space Shuttle mission was originally scheduled for late 2010, but the program was later extended to July 2011 when Michael Suffredini of the ISS program said that one additional trip was needed in 2011 to deliver parts to the International Space Station. The Shuttle's final mission consisted of just four astronauts—Christopher Ferguson (Commander), Douglas Hurley (Pilot), Sandra Magnus (Mission Specialist 1), and Rex Walheim (Mission Specialist 2); they conducted the 135th and last space Shuttle mission on board "Atlantis", which launched on July 8, 2011, and landed safely at the Kennedy Space Center on July 21, 2011, at 5:57 AM EDT (09:57 UTC).
Distribution of orbiters and other hardware.
NASA announced it would transfer orbiters to education institutions or museums at the conclusion of the Space Shuttle program. Each museum or institution is responsible for covering the US$ cost of preparing and transporting each vehicle for display. Twenty museums from across the country submitted proposals for receiving one of the retired orbiters. NASA also made Space Shuttle thermal protection system tiles available to schools and universities for less than US$25 each. About 7,000 tiles were available on a first-come, first-served basis, limited to one per institution.
On April 12, 2011, NASA announced selection of locations for the remaining Shuttle orbiters:
Flight and mid-deck training hardware will be taken from the Johnson Space Center and will go to the National Air and Space Museum and the National Museum of the U.S. Air Force. The full fuselage mockup, which includes the payload bay and aft section but no wings, is to go to the Museum of Flight in Seattle. Mission Simulation and Training Facility's fixed simulator will go to the Adler Planetarium in Chicago, and the motion simulator will go to the Texas A&M Aerospace Engineering Department in College Station, Texas. Other simulators used in Shuttle astronaut training will go to the Wings of Dreams Aviation Museum in Starke, Florida and the Virginia Air and Space Center in Hampton, Virginia.
In August 2011, the NASA Office of Inspector General (OIG) published a "Review of NASA's Selection of Display Locations for the Space Shuttle Orbiters"; the review had four main findings: 
The NASA OIG had three recommendations, saying NASA should:
In September 2011, the CEO and two board members of Seattle's Museum of Flight met with NASA Administrator Charles Bolden, pointing out "significant errors in deciding where to put its four retiring Space Shuttles"; the errors alleged include inaccurate information on Museum of Flight's attendance and international visitor statistics, as well as the readiness of the Intrepid Sea-Air-Space Museum's exhibit site.
Space Shuttle successors and legacy.
Until another US manned spacecraft is ready, crews will travel to and from the International Space Station (ISS) exclusively aboard the Russian Soyuz spacecraft.
A planned successor to STS was the "Shuttle II", during the 1980s and 1990s, and later the Constellation program during the 2004–2010 period. CSTS was a proposal to continue to operate STS commercially, after NASA. In September 2011, NASA announced the selection of the design for the new Space Launch System that is planned to launch the Orion spacecraft and other hardware to missions beyond low earth-orbit.
The Commercial Orbital Transportation Services program began in 2006 with the purpose of creating commercially operated unmanned cargo vehicles to service the ISS. The Commercial Crew Development (CCDev) program was started in 2010 to create commercially operated manned spacecraft capable of delivering at least four crew members to the ISS, to stay docked for 180 days, and then return them back to Earth. These spacecraft were to become operational in the 2010s.
In culture.
Space Shuttles have been features of fiction and nonfiction, from children's movies to documentaries. Early examples include the 1979 James Bond film, "Moonraker", the 1982 Activision videogame "Space Shuttle: A Journey into Space" (1982) and G. Harry Stine's 1981 novel "Shuttle Down". In the 1986 film "SpaceCamp", "Atlantis" accidentally launched into space with a group of U.S. Space Camp participants as its crew. The 1998 film "Armageddon" portrayed a combined crew of offshore oil rig workers and US military staff who pilot two modified Shuttles to avert the destruction of Earth by an asteroid. Retired American test pilots visited a Russian satellite in the 2000 Clint Eastwood adventure film "Space Cowboys". In the 2003 film "The Core," the "Endeavour"'s landing is disrupted by the earth's magnetic core, and its crew is selected to pilot the vehicle designed to restart the core. The 2004 Bollywood movie "Swades", where a Space Shuttle was used to launch a special rainfall monitoring satellite, was filmed at Kennedy Space Center in the year following the Columbia disaster that had taken the life of Indian-American astronaut KC Chawla. On television, the 1996 drama "The Cape" portrayed the lives of a group of NASA astronauts as they prepared for and flew Shuttle missions. "Odyssey 5" was a short lived sci-fi series that featured the crew of a Space Shuttle as the last survivors of a disaster that destroyed Earth. The 2013 film "Gravity" features the fictional space shuttle "Explorer", whose crew are killed or left stranded after it is destroyed by a shower of high speed orbital debris.
The Space Shuttle has also been the subject of toys and models; for example, a large Lego Space Shuttle model was constructed by visitors at Kennedy Space Center, and smaller models have been sold commercially as a standard "LegoLand" set. A 1980 pinball machine "Space Shuttle" was produced by Zaccaria and a 1984 pinball machine "Space Shuttle: Pinball Adventure" was produced by Williams and features a plastic Space Shuttle model among other artwork of astronauts on the play field. The Space Shuttle also appears in a number of flight simulator and space flight simulator games such as "Microsoft Space Simulator", "Orbiter", "FlightGear" and "X-Plane".
US postage commemorations.
The U.S. Postal Service has released several postage issues that depict the Space Shuttle. The first such stamps were issued in 1981, and are on display at the National Postal Museum.

</doc>
<doc id="28191" url="http://en.wikipedia.org/wiki?curid=28191" title="Snow">
Snow

Snow is precipitation in the form of flakes of crystalline water ice that falls from clouds. Since snow is composed of small ice particles, it is a granular material. It has an open and therefore soft, white, and fluffy structure, unless subjected to external pressure. Snowflakes come in a variety of sizes and shapes. Types that fall in the form of a ball due to melting and refreezing, rather than a flake, are hail, ice pellets or snow grains.
The process of precipitating snow is called snowfall. Snowfall tends to form within regions of upward movement of air around a type of low-pressure system known as an extratropical cyclone. Snow can fall poleward of these systems' associated warm fronts and within their comma head precipitation patterns (called such due to the comma-like shape of the cloud and precipitation pattern around the poleward and west sides of extratropical cyclones). Where relatively warm water bodies are present, for example because of water evaporation from lakes, lake-effect snowfall becomes a concern downwind of the warm lakes within the cold cyclonic flow around the backside of extratropical cyclones. Lake-effect snowfall can be heavy locally. Thundersnow is possible within a cyclone's comma head and within lake effect precipitation bands. In mountainous areas, heavy snow is possible where upslope flow is maximized within windward sides of the terrain at elevation, if the atmosphere is cold enough. Snowfall amount and its related liquid equivalent precipitation amount are measured using a variety of different rain gauges.
Forms.
Once on the ground, snow can be categorized as powdery when light and fluffy, fresh when recent but heavier, granular when it begins the cycle of melting and refreezing, and eventually ice once it comes down, after multiple melting and refreezing cycles, into a dense mass called snow pack. When powdery, snow moves with the wind from the location where it originally landed, forming deposits called snowdrifts that may have a depth of several meters. After attaching itself to hillsides, blown snow can evolve into a snow slab—an avalanche hazard on steep slopes. The existence of a snowpack keeps temperatures lower than they would be otherwise, as the whiteness of the snow reflects most sunlight, and any absorbed heat goes into melting the snow rather than increasing its temperature. The water equivalent of snowfall is measured to monitor how much liquid is available to flood rivers from meltwater that will occur during the following spring. Snow cover can protect crops from extreme cold. If snowfall stays on the ground for a series of years uninterrupted, the snowpack develops into a mass of ice called glacier. Fresh snow absorbs sound, lowering ambient noise over a landscape because the trapped air between snowflakes attenuates vibration. These acoustic qualities quickly minimize and reverse, once a layer of freezing rain falls on top of snow cover. Walking across snowfall produces a squeaking sound at low temperatures.
The energy balance of the snowpack itself is dictated by several heat exchange processes. The snowpack absorbs solar shortwave radiation that is partially blocked by cloud cover and reflected by snow surface. A long-wave heat exchange takes place between the snowpack and its surrounding environment that includes overlying air mass, tree cover and clouds. Heat exchange takes place by convection between the snowpack and the overlaying air mass, and it is governed by the temperature gradient and wind speed. Moisture exchange between the snowpack and the overlying air mass is accompanied by latent heat transfer that is influenced by vapor pressure gradient and air wind. Rain on snow can add significant amounts of thermal energy to the snowpack. A generally insignificant heat exchange takes place by conduction between the snowpack and the ground. The small temperature change from before to after a snowfall is a result of the heat transfer between the snowpack and the air. As snow degrades, its surface can develop characteristic ablation textures such as suncups or penitentes.
The term "snow storm" can describe a heavy snowfall, while a "blizzard" involves snow and wind, obscuring visibility. "Snow shower" is a term for an intermittent snowfall, while "flurry" is used for very light, brief snowfalls. Snow can fall more than a meter at a time during a single storm in flat areas, and meters at a time in rugged terrain, such as mountains. When snow falls in significant quantities, travel by foot, car, airplane and other means becomes severely restricted, but other methods of mobility become possible, such as the use of snowmobiles, snowshoes and skis. When heavy snow occurs early in the fall (or, on rarer occasions, late in the spring), significant damage can occur to trees still in leaf. Areas with significant snow each year can store the winter snow within an ice house, which can be used to cool structures during the following summer. A variation on snow has been observed on Venus, though composed of metallic compounds and occurring at a substantially higher temperature.
Cause.
Extratropical cyclones can bring cold and dangerous conditions with heavy rain and snow with winds exceeding 119 km/h, (sometimes referred to as windstorms in Europe). The band of precipitation that is associated with their warm front is often extensive, forced by weak upward vertical motion of air over the frontal boundary, which condenses as it cools off and produces precipitation within an elongated band, which is wide and stratiform, meaning falling out of nimbostratus clouds. When moist air tries to dislodge an arctic air mass, overrunning snow can result within the poleward side of the elongated precipitation band. In the Northern Hemisphere, poleward is towards the North Pole, or north. Within the Southern Hemisphere, poleward is towards the South Pole, or south.
Within the "cold sector", poleward and west of the cyclone center, small scale or mesoscale bands of heavy snow can occur within a cyclone's comma head pattern. The cyclone's comma head pattern is a comma-shaped area of clouds and precipitation found around mature extratropical cyclones. These snow bands typically have a width of 20 to. These bands in the comma head are associated with areas of frontogenesis, or zones of strengthening temperature contrast.
Southwest of extratropical cyclones, curved cyclonic flow bringing cold air across the relatively warm water bodies can lead to narrow lake-effect snow bands. Those bands bring strong localized snowfall, which can be understood as follows: Large water bodies such as lakes efficiently store heat that results in significant temperature differences (larger than 13 °C [23 °F]) between the water surface and the air above. Because of this temperature difference, warmth and moisture are transported upward, condensing into vertically oriented clouds (see satellite picture) that produce snow showers. The temperature decrease with height and cloud depth are directly affected by both the water temperature and the large-scale environment. The stronger the temperature decrease with height, the deeper the clouds get, and the greater the precipitation rate becomes.
In mountainous areas, heavy snowfall accumulates when air is forced to ascend the mountains and squeeze out precipitation along their windward slopes, which in cold conditions, falls in the form of snow. Because of the ruggedness of terrain, forecasting the location of heavy snowfall remains a significant challenge.
Snowflakes.
Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than -18 °C, because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice. Then the droplet freezes around this "nucleus". Experiments show that this "homogeneous" nucleation of cloud droplets only occurs at temperatures lower than -35 °C. In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Ice nuclei are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei include particles of silver iodide and dry ice, and these are used to stimulate precipitation in cloud seeding.
Once a droplet has frozen, it grows in the supersaturated environment—one where air is saturated with respect to ice when the temperature is below the freezing point. The droplet then grows by diffusion of water molecules in the air (vapor) onto the ice crystal surface where they are collected. Because water droplets are so much more numerous than the ice crystals due to their sheer abundance, the crystals are able to grow to hundreds of micrometers or millimeters in size at the expense of the water droplets by a process known as the Wegner-Bergeron-Findeison process. The corresponding depletion of water vapor causes the ice crystals to grow at the droplets' expense. These large crystals are an efficient source of precipitation, since they fall through the atmosphere due to their mass, and may collide and stick together in clusters, or aggregates. These aggregates are snowflakes, and are usually the type of ice particle that falls to the ground. Guinness World Records list the world's largest snowflakes as those of January 1887 at Fort Keogh, Montana; allegedly one measured 38 cm wide. Although the ice is clear, scattering of light by the crystal facets and hollows/imperfections mean that the crystals often appear white in color due to diffuse reflection of the whole spectrum of light by the small ice particles.
The shape of the snowflake is determined broadly by the temperature and humidity at which it is formed. The most common snow particles are visibly irregular. Planar crystals (thin and flat) grow in air between 0 C and -3 C. Between -3 C and -8 C, the crystals will form needles or hollow columns or prisms (long thin pencil-like shapes). From -8 C to -22 C the shape reverts to plate-like, often with branched or dendritic features. At temperatures below -22 C, the crystal development becomes column-like, although many more complex growth patterns also form such as side-planes, bullet-rosettes and also planar types depending on the conditions and ice nuclei. If a crystal has started forming in a column growth regime, at around -5 C, and then falls into the warmer plate-like regime, then plate or dendritic crystals sprout at the end of the column, producing so called "capped columns".
A snowflake consists of roughly 1019 water molecules, which are added to its core at different rates and in different patterns, depending on the changing temperature and humidity within the atmosphere that the snowflake falls through on its way to the ground. As a result, it is extremely difficult to encounter two identical snowflakes. Initial attempts to find identical snowflakes by photographing thousands their images under a microscope from 1885 onward by Wilson Alwyn Bentley found the wide variety of snowflakes we know about today. It is more likely that two snowflakes could become virtually identical if their environments were similar enough. Matching snow crystals were discovered in Wisconsin in 1988. The crystals were not flakes in the usual sense but rather hollow hexagonal prisms.
Types.
Types of snow can be designated by the shape of the flakes, the rate of accumulation, and the way the snow collects on the ground. Types that fall in the form of a ball due to melting and refreezing cycles, rather than a flake, are known as graupel, with ice pellets and snow pellets as types of graupel associated with wintry precipitation. Once on the ground, snow can be categorized as "powdery" when fluffy, "granular" when it begins the cycle of melting and refreezing, and eventually "ice" once it packs down into a dense drift after multiple melting and refreezing cycles. When powdery, snow drifts with the wind from the location where it originally fell, forming deposits with a depth of several meters in isolated locations. Snow fences are constructed in order to help control snow drifting in the vicinity of roads, to improve highway safety. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes. A frozen equivalent of dew known as hoar frost forms on a snow pack when winds are light and there is ample low-level moisture over the snow pack.
Snowfall's intensity is determined by visibility. When the visibility is over 1 km, snow is considered light. Moderate snow describes snowfall with visibility restrictions between 0.5 and 1 km. Heavy snowfall describes conditions when visibility is less than 0.5 km. Steady snows of significant intensity are often referred to as "snowstorms". When snow is of variable intensity and short duration, it is described as a "snow shower". The term snow flurry is used to describe the lightest form of a snow shower.
A blizzard is a weather condition involving snow and has varying definitions in different parts of the world. In the United States, a blizzard occurs when two conditions are met for a period of three hours or more: A sustained wind or frequent gusts to 35 mph, and sufficient snow in the air to reduce visibility to less than 0.4 km. In Canada and the United Kingdom, the criteria are similar. While heavy snowfall often occurs during blizzard conditions, falling snow is not a requirement, as blowing snow can create a ground blizzard.
Density.
Snow remains on the ground until it melts or sublimates. Sublimation of snow directly into water vapor is most likely to occur on a dry and windy day such as when a strong downslope wind, such as a Chinook wind, exists.
Once the snow is on the ground, it will settle under its own weight (largely due to differential evaporation) until its density is approximately 30% of water. Increases in density above this initial compression occur primarily by melting and refreezing, caused by temperatures above freezing or by direct solar radiation. In colder climates, snow lies on the ground all winter. By late spring, snow densities typically reach a maximum of 50% of water. When the snow does not all melt in the summer it evolves into firn, where individual granules become more spherical in nature, evolving into a glacier as the ice flows downhill.
Snow Water Equivalent (SWE).
The "snow water equivalent" is the product of snow depth and the snow bulk density. It is a quantity of type columnar mass density, having units of area density (kg/m2), though it is usually reported normalized by the volumetric density of liquid water (units kg/m3), thus being expressed in units of length (e.g., millimeter or inches). It corresponds to the depth of a layer of water that would accumulate in an area, if all the snow and ice were melted in that given area. For example, if the snow covering a given area has a water equivalent of 50 cm, then it will melt into a pool of water 50 cm deep covering the same area. This is a much more useful measurement to hydrologists than snow "depth", as the density of cool freshly fallen snow widely varies. New snow commonly has a density of around 8% of water. This means that 33 cm of snow melts down to 2.5 cm of water. Cloud temperatures and physical processes in the cloud affect the shape of individual snow crystals. Highly branched or dendritic crystals tend to have more space between the arms of ice that form the snowflake and this snow will therefore have a lower density, often referred to as "dry" snow. Conditions that create columnar or plate-like crystals will have much less air space within the crystal and will therefore be denser and feel "wetter".
Acoustic properties.
Newly fallen snow acts as a sound-absorbing material, which minimizes sound over its surface. This is due to the trapped air between the individual crystalline flakes, trapping sound waves and dampening vibrations. Once it is blown around by the wind and exposed to sunshine, snow hardens and its sound-softening quality diminishes. Snow cover as thin as 2 cm thick changes the acoustic properties of a landscape. Studies concerning the acoustic properties of snow have revealed that loud sounds, such as from a pistol, can be used to measure snow cover permeability and depth. Within motion pictures, the sound of walking through snow is simulated using cornstarch, salt, or cat litter. When the temperature falls below -10 C, snow will squeak when walked upon due to the crushing of the ice crystals within the snow. If covered by a layer of freezing rain, the hardened frozen surface acts to echo sounds, similar to concrete.
From under water, snowfall has a unique sound when compared to other forms of precipitation, and the sound varies little with differences in the snowflakes' size and shape.
Snowfall measurement.
Snowfall is defined by the U.S. National Weather Service as a being the maximum depth of snow on a snowboard (typically a piece of plywood painted white) observed during a six-hour period. At the end of the six-hour period, all snow is cleared from the measuring surface. For a daily total snowfall, four six-hour snowfall measurements are summed. Snowfall can be very difficult to measure due to melting, compacting, blowing and drifting.
The liquid equivalent of snowfall may be evaluated using a snow gauge or with a standard rain gauge having a diameter of 100 mm (4 in; plastic) or 200 mm (8 in; metal). Rain gauges are adjusted to winter by removing the funnel and inner cylinder and allowing the snow/freezing rain to collect inside the outer cylinder. Antifreeze liquid may be added to melt the snow or ice that falls into the gauge. In both types of gauges once the snowfall/ice is finished accumulating, or as its height in the gauge approaches 300 mm, the snow is melted and the water amount recorded.
Another type of gauge used to measure the liquid equivalent of snowfall is the weighing precipitation gauge. The wedge and tipping bucket gauges will have problems with snow measurement. Attempts to compensate for snow/ice by warming the tipping bucket meet with limited success, since snow may sublimate if the gauge is kept much above the freezing temperature. Weighing gauges with antifreeze should do fine with snow, but again, the funnel needs to be removed before the event begins. At some automatic weather stations an ultrasonic snow depth sensor may be used to augment the precipitation gauge.
Spring snow melt is a major source of water supply to areas in temperate zones near mountains that catch and hold winter snow, especially those with a prolonged dry summer. In such places, water equivalent is of great interest to water managers wishing to predict spring runoff and the water supply of cities downstream. Measurements are made manually at marked locations known as "snow courses", and remotely using special scales called "snow pillows".
When a snow measurement is made, various networks exist across the United States and elsewhere where rainfall measurements can be submitted through the Internet, such as CoCoRAHS or GLOBE. If a network is not available in the area where one lives, the nearest local weather office will likely be interested in the measurement.
Records.
The world record for the highest seasonal total snowfall was measured in the United States at Mount Baker Ski Area, outside of the town Bellingham, Washington during the 1998–1999 season. Mount Baker received 2896 cm of snow, thus surpassing the previous record holder, Mount Rainier, Washington, which during the 1971–1972 season received 2850 cm of snow.
The world record for the highest average yearly snowfall is 1764 cm, measured in Sukayu Onsen, Japan for the period of 1981-2010.
The North American record for the highest average yearly snowfall is 1630 cm, measured on Mount Rainier, Washington.
The world record for snow depth is 1182 cm. It was measured on the slope of Mt. Ibuki in Shiga Prefecture, Japan at altitude of 1200 m on February 14, 1927.
The North American record for snow depth is 1150 cm. It was measured at Tamarack, California at altitude of 7000 foot in March 1911.
The world's snowiest city with a population over one million is Sapporo, Japan, with an average yearly snowfall of 595 cm.
Snow blindness.
Fresh snow reflects 90% or more of ultraviolet radiation, which causes snow blindness, also reducing absorption of sunlight by the ground. Snow blindness (also known as ultraviolet keratitis, photokeratitis or niphablepsia) is a painful eye condition, caused by exposure of unprotected eyes to the ultraviolet (UV) rays in bright sunlight reflected from snow or ice. This condition is a problem in polar regions and at high altitudes, as with every 300 m of elevation (above sea level), the intensity of UV rays increases by 4%. Snow's large reflection of light makes night skies much brighter, since reflected light is directed back up into the sky. However, when there is also cloud cover, light is then reflected back to the ground. This greatly amplifies light emitted from city lights, causing the 'bright night' effect. A similar brightening effect occurs when no snow is falling and there is a full moon and a large amount of snow.
Relation to river flow.
Many rivers originating in mountainous or high-latitude regions receive a significant portion of their flow from snowmelt. This often makes the river's flow highly seasonal resulting in periodic flooding during the spring months and at least in dry mountainous regions like the mountain West of the US or most of Iran and Afghanistan, very low flow for the rest of the year. In contrast, if much of the melt is from glaciated or nearly glaciated areas, the melt continues through the warm season, with peak flows occurring in mid to late summer.
Effects on human society.
Substantial snowfall can disrupt public infrastructure and services, slowing human activity even in regions that are accustomed to such weather. Air and ground transport may be greatly inhibited or shut down entirely. Populations living in snow-prone areas have developed various ways to travel across the snow, such as skis, snowshoes, and sleds pulled by horses, dogs, or other animals and later, snowmobiles. Basic utilities such as electricity, telephone lines, and gas supply can also fail. In addition, snow can make roads much harder to travel and vehicles attempting to use them can easily become stuck. Snowfall can have a small negative effect on yearly yield from solar photovoltaic systems.
The combined effects can lead to a "snow day" on which gatherings such as school or work are officially canceled. In areas that normally have very little or no snow, a snow day may occur when there is only light accumulation or even the threat of snowfall, since those areas are unprepared to handle any amount of snow. In some areas, such as some states in the United States, schools are given a yearly quota of snow days (or "calamity days"). Once the quota is exceeded, the snow days must be made up. In other states, all snow days must be made up. For example, schools may extend the remaining school days later into the afternoon, shorten spring break, or delay the start of summer vacation.
Accumulated snow is removed to make travel easier and safer, and to decrease the long-term impact of a heavy snowfall. This process utilizes shovels, snowplows and snow blowers and is often assisted by sprinkling salt or other chloride-based chemicals, which reduce the melting temperature of snow. In some areas with abundant snowfall, such as Yamagata Prefecture, Japan, people harvest snow and store it surrounded by insulation in ice houses. This allows the snow to be used through the summer for refrigeration and air conditioning, which requires far less electricity than traditional cooling methods.
Agriculture.
Snowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth. If it melts into water and refreezes upon sensitive crops, such as oranges, the resulting ice will protect the fruit from exposure to lower temperatures.
Recreation.
Many winter sports, such as skiing, snowboarding, snowmobiling, and snowshoeing depend upon snow. Where snow is scarce but the temperature is low enough, snow cannons may be used to produce an adequate amount for such sports. Children and adults can play on a sled or ride in a sleigh. Although a person's footsteps remain a visible lifeline within a snow-covered landscape, snow cover is considered a general danger to hiking since the snow obscures landmarks and makes the landscape itself appear uniform.
One of the recognizable recreational uses of snow is in building snowmen. A snowman is created by making a man shaped figure out of snow – often using a large, shaped snowball for the body and a smaller snowball for the head which is often decorated with simple household items – traditionally including a carrot for a nose, and coal for eyes, nose and mouth; occasionally including old clothes such as a top hat or scarf.
Snow can be used to make snow cones, also known as snowballs, which are usually eaten in the summer months. Flat areas of snow can be used to make snow angels, a popular pastime for children.
Snow can be used to alter the format of outdoor games such as Capture the flag, or for snowball fights. The world's biggest snowcastle, the SnowCastle of Kemi, is built in Kemi, Finland every winter. Since 1928 Michigan Technological University in Houghton, Michigan has held an annual Winter Carnival in mid-February, during which a large Snow Sculpture Contest takes place between various clubs, fraternities, and organizations in the community and the university. Each year there is a central theme, and prizes are awarded based on creativity. Snowball softball tournaments are held in snowy areas, usually using a bright orange softball for visibility, and burlap sacks filled with snow for the bases.
Damage.
When heavy, wet snow with a snow-water equivalent (SWE) ratio of between 6:1 and 12:1 (in extreme cases, as heavy as 4:1) and a weight in excess of 10 pounds per square foot (~40 kg/m2) piles onto trees or electricity lines – particularly if the trees have full leaves or are not adapted to snow – significant damage may occur on a scale usually associated with hurricanes. An avalanche can occur upon a sudden thermal or mechanical impact upon snow that has accumulated on a mountain, which causes the snow to rush downhill en masse. Preceding an avalanche is a phenomenon known as an avalanche wind caused by the approaching avalanche itself, which adds to its destructive potential. Large amounts of snow which accumulate on top of man-made structures can lead to structural failure. During snowmelt, acidic precipitation which previously fell into the snow pack is released, which harms marine life.
There is a popular misconception that snow becomes heavier when it starts to melt, so many people take risks by climbing on roofs to remove snow when the weather starts to get warmer, for fear that the roofs will collapse. In fact, when snow starts to melt, its volume decreases as the ice crystals and meltwater move into the spaces between the crystals, which makes the "density" of wet, melting snow greater than that of freshly-fallen snow. This makes it feel heavier to shovel, but its mass does not increase. In fact, it decreases when meltwater runs off the roof, so the weight of snow on a roof actually decreases when it starts to melt.
Design of structures considering snow load.
The designs of all structures and buildings use the ground snow load determined by professional engineers and designers. Data on ground snow in the U.S.A. are provided by the American Society of Civil Engineers (ASCE7-latest edition) for most jurisdictions. This load is typically the governing design factor on roofs and structural elements exposed to the effects of snow in the northern United States. Closer to the Equator, the snow load becomes less important and may or may not be the governing factor.
Extraterrestrial snow.
Very light snow is known to occur at high latitudes on Mars. A "snow" of hydrocarbons is also theorized to occur on Saturn's moon Titan.
While there is little or no water on Venus, there is a phenomenon which is quite similar to snow. The Magellan probe imaged a highly reflective substance at the tops of Venus's highest mountain peaks which bore a strong resemblance to terrestrial snow. This substance arguably formed from a similar process to snow, albeit at a far higher temperature. Too volatile to condense on the surface, it rose in gas form to cooler higher elevations, where it then fell as precipitation. The identity of this substance is not known with certainty, but speculation has ranged from elemental tellurium to lead sulfide (galena).

</doc>
<doc id="28195" url="http://en.wikipedia.org/wiki?curid=28195" title="Symbolics">
Symbolics

Symbolics refers to two companies: now-defunct computer manufacturer Symbolics, Inc., and a privately held company that acquired the assets of the former company and continues to sell and maintain the Open Genera Lisp system and the Macsyma computer algebra system.
The symbolics.com domain was originally registered on March 15, 1985, making it the first .com-domain in the world. In August 2009, it was sold to XF.com Investments.
History.
Symbolics, Inc. was a computer manufacturer headquartered in Cambridge, Massachusetts, and later in Concord, Massachusetts, with manufacturing facilities in Chatsworth, California (a suburban section of Los Angeles). Its first CEO, chairman, and founder was Russell Noftsker. Symbolics designed and manufactured a line of Lisp machines, single-user computers optimized to run the Lisp programming language. Symbolics also made significant advances in software technology, and offered one of the premier software development environments of the 1980s and 1990s, now sold commercially as Open Genera for Tru64 UNIX on the HP Alpha. The Lisp Machine was the first commercially available "workstation" (although that word had not yet been coined).
Symbolics was a spinoff from the MIT AI Lab, one of two companies to be founded by AI Lab staffers and associated hackers for the purpose of manufacturing Lisp machines. The other was Lisp Machines, Inc., although Symbolics attracted most of the hackers, and more funding.
Symbolics' initial product, the LM-2 (introduced in 1981), was a repackaged version of the MIT CADR Lisp machine design. The operating system and software development environment, over 500,000 lines, was written in Lisp from the microcode up, based on MIT's Lisp Machine Lisp.
The software bundle was later renamed ZetaLisp, to distinguish the Symbolics' product from other vendors who had also licensed the MIT software. Symbolics' Zmacs text editor, a variant of Emacs, was implemented in a text-processing package named "ZWEI", an acronym for "Zwei was Eine initially", with "Eine" being an acronym for "Eine Is Not Emacs". Both are recursive acronyms and puns on the German words for "One" ("Eins", "Eine") and "Two" ("Zwei").
The Lisp Machine system software was then copyrighted by MIT, and was licensed to Symbolics. Until 1981, they shared all the source code with MIT and kept it on an MIT server. According to Richard Stallman, Symbolics engaged in a business tactic in which it forced MIT to make all fixes and improvements to the Lisp Machine OS available only to Symbolics, and thereby choke off its competitor LMI, which at that time had insufficient resources to independently maintain or develop the OS and environment.
Symbolics felt that they no longer had sufficient control over their product. At that point, Symbolics began using their own copy of the software, located on their company servers — while Stallman says that Symbolics did that to prevent its Lisp improvements from flowing to Lisp Machines, Inc. From that base, Symbolics made extensive improvements to every part of the software, and continued to deliver almost all the source code to their customers (including MIT). However, the policy prohibited MIT staff from distributing the Symbolics version of the software to others. With the end of open collaboration came the end of the MIT hacker community. As a reaction to this, Stallman initiated the GNU project to make a new community. Eventually, Copyleft and the GNU General Public License would ensure that a hacker's software could remain free software. In this way Symbolics played a key, albeit adversarial, role in instigating the free software movement.
The 3600 series.
In 1983, a year later than planned, Symbolics introduced the 3600 family of Lisp machines. Code-named the "L-machine" internally, the 3600 family was an innovative new design, inspired by the CADR architecture but sharing few of its implementation details. The main processor had a 36 bit word (divided up as 4 or 8 bits of tags, and 32 bits of data or 28 bits of memory address). Memory words were 44 bits, the additional 8 bits being used for error-correcting code (ECC). The instruction set was that of a stack machine. The 3600 architecture provided 4,096 hardware registers, of which half were used as a cache for the top of the control stack; the rest were used by the microcode and time-critical routines of the operating system and Lisp run-time environment. Hardware support was provided for virtual memory, which was common for machines in its class, and for garbage collection, which was unique.
The original 3600 processor was a microprogrammed design like the CADR, and was built on several large circuit boards from standard TTL integrated circuits, both features being common for commercial computers in its class at the time. CPU clock speed varied depending on the particular instruction being executed, but was typically around 5 MHz. Many Lisp primitives could be executed in a single clock cycle. Disk I/O was handled by multitasking at the microcode level. A 68000 processor (known as the "Front-End Processor", or FEP) started the main computer up, and handled the slower peripherals during normal operation. An Ethernet interface was standard equipment, replacing the Chaosnet interface of the LM-2.
The 3600 was roughly the size of a household refrigerator. This was partly due to the size of the processor — the cards were widely spaced to allow wire-wrap prototype cards to fit without interference — and partly due to the limitations of the disk drive technology in the early 1980s. At the 3600's introduction, the smallest disk that could support the ZetaLisp software was 14 inches (356 mm) across (most 3600s shipped with the 10½-inch Fujitsu Eagle). The 3670 and 3675 were slightly shorter in height, but were essentially the same machine packed a little tighter. The advent of 8 in, and later 5+1/4 in, disk drives that could hold hundreds of megabytes led to the introduction of the 3640 and 3645, which were roughly the size of a two-drawer file cabinet.
Later versions of the 3600 architecture were implemented on custom integrated circuits, reducing the 5 cards of the original processor design to 2, at a large manufacturing cost savings but with performance slightly better than the old design. The 3650, first of the "G machines" (as they were known within the company), was housed in a cabinet derived from the 3640s. Denser memory and smaller disk drives enabled the introduction of the 3620, about the size of a modern full-size tower PC. The 3630 was a "fat 3620" with room for more memory and video interface cards. The 3610 was a lower priced variant of the 3620, essentially identical in every way except that it was licensed for application deployment rather than general development.
The various models of the 3600 family were popular for AI research and commercial applications throughout the 1980s. The AI commercialization boom of the 1980s led directly to Symbolics' success during the decade. Symbolics computers were widely believed to be the best platform available for developing AI software. The LM-2 used a Symbolics-branded version of the complex space-cadet keyboard, while later models used a simplified version (at right), known simply as the Symbolics keyboard. The Symbolics keyboard featured the many modifier keys used in Emacs, notably Control/Meta/Super/Hyper in a block, but did not feature the complex symbol set of the space-cadet keyboard.
Also contributing to the 3600 series' success was a line of bit-mapped graphics color video interfaces, combined with extremely powerful animation software. Symbolics' Graphics Division, headquartered in Westwood, California, a stone's throw from the major Hollywood movie and TV studios, made its S-Render and S-Paint software into industry leaders in the animation business.
Symbolics developed the first workstations capable of processing HDTV quality video, which enjoyed a popular following in Japan. A 3600 — with the standard black-and-white monitor — made a cameo appearance in the movie Real Genius. The company was also referenced in Michael Crichton's novel Jurassic Park.
Symbolics' Graphics Division was sold to Nichimen Trading Company in the early 90s, and the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) ported to Franz Allegro Common Lisp on SGI and PC computers running Windows NT. Today it is sold as Mirai by Izware LLC, and continues to be used in major motion pictures (most famously in New Line Cinema's "The Lord of the Rings"), video games, and military simulations.
Symbolic's 3600 series computers were also used as the first front end "controller" computers for the Connection Machine massively parallel computers manufactured by Thinking Machines Inc., another MIT spinoff based in Cambridge, Massachusetts. The Connection Machine ran a parallel variant of Lisp and, initially, was used primarily by the AI community, so the Symbolics Lisp machine was a particularly good fit as a front-end machine.
For a long time, the operating system didn't have a name, but was finally named "Genera" around 1984. The system included a number of advanced dialects of Lisp. Its heritage was MACLISP on the PDP-10, but it included more data types, and multiple-inheritance object-oriented programming features.
Initially called Lisp Machine Lisp, then ZetaLisp, it finally acquired the name "Symbolics Common Lisp" during the creation of Common Lisp in 1987. Common Lisp is a subset of the dialect available on the Lisp Machine.
Ivory and Open Genera.
In the late 1980s (2 years later than planned), the Ivory family of single-chip Lisp Machine processors superseded the G-Machine 3650, 3620, and 3630 systems. The Ivory 390k transistor VLSI implementation designed in Symbolics Common Lisp using NS, a custom Symbolics Hardware Design Language (HDL), addressed a 40-bit word (8 bits tag, 32 bits data/address). Since it only addressed full words and not bytes or half-words, this allowed addressing of 4 Gigawords (GW) or 16 gigabytes (GB) of memory; the increase in address space reflected the growth of programs and data as semiconductor memory and disk space became cheaper. The Ivory processor had 8 bits of ECC attached to each word, so each word fetched from external memory to the chip was actually 48 bits wide. Each Ivory instruction was 18 bits wide and two instructions plus a 2-bit CDR code and 2-bit Data Type were in each instruction word fetched from memory. Fetching two instruction words at a time from memory enhanced the Ivory's performance. Unlike the 3600's microprogrammed architecture, the Ivory instruction set was still microcoded, but was stored in a 1200 x 180-bit ROM inside the Ivory chip. The initial Ivory processors were fabricated by VLSI Technology Inc in San Jose, California, on a 2 µm CMOS process, with later generations fabricated by Hewlett Packard in Corvallis, Oregon, on 1.25 µm and 1 µm CMOS processes. The Ivory had a stack architecture and operated a 4-stage pipeline: Fetch, Decode, Execute and Write Back. Ivory processors were marketed in stand-alone Lisp Machines (the XL400, XL1200, and XL1201), headless Lisp Machines (NXP1000), and on add-in cards for Sun Microsystems (UX400, UX1200) and Apple Macintosh (MacIvory I, II, III) computers. The Lisp Machines with Ivory processors operated at speeds that were between two and six times faster than a 3600 depending on the model and the revision of the Ivory chip.
The Ivory instruction set was later emulated in software for microprocessors implementing the 64-bit Alpha architecture. The "Virtual Lisp Machine" emulator, combined with the operating system and software development environment from the XL machines, is sold as Open Genera.
Sunstone.
Sunstone was a RISC-like processor that was to be released shortly after the Ivory. It was designed by Ron Lebel's group at the Symbolics Westwood office. However, the project was canceled the day it was supposed to tape out.
Endgame.
As quickly as the commercial AI boom of the mid-1980s had propelled Symbolics to success, the "AI Winter" of the late 1980s and early 1990s, combined with the slow down of Reagan's "Star Wars" missile defense program, for which DARPA had invested heavily in AI solutions, severely damaged Symbolics. An internal war between Noftsker and the CEO the board had hired in 1986, Brian Sear, over whether to follow Sun's suggested lead and focus on selling their software, or to re-emphasize their superior hardware, and the ensuing lack of focus when both Noftsker and Sear were fired from the company caused sales to plummet. This fact, combined with some ill-advised real estate deals by company management during the boom years (they had entered into large long-term lease obligations in California), drove Symbolics into bankruptcy. Rapid evolution in mass-market microprocessor technology (the "PC revolution"), advances in Lisp compiler technology, and the economics of manufacturing custom microprocessors severely diminished the commercial advantages of purpose-built Lisp machines. By 1995, the Lisp machine era had ended, and with it Symbolics' hopes for success.
Symbolics continued as an enterprise with very limited revenues, supported mainly by service contracts on the remaining MacIvory, UX-1200, UX-1201, and other machines still used by commercial customers. Symbolics also sold Virtual Lisp Machine (VLM) software for DEC, Compaq, and HP Alpha-based workstations (AlphaStation) and servers (AlphaServer), refurbished MacIvory IIs, and Symbolics keyboards.
In July 2005, Symbolics closed its Chatsworth, California, maintenance facility. The reclusive owner of the company, Andrew Topping, died that same year. The current legal status of Symbolics software is uncertain. An assortment of Symbolics hardware was still available for purchase as of August 2007. The US DoD is still paying Symbolics for regular maintenance work.
First .com domain.
On March 15, 1985, symbolics.com became the first (and currently, since it is still registered, the oldest) registered .com domain of the Internet. The symbolics.com domain was purchased by XF.com in 2009.
Networking.
Genera also featured the most extensive networking interoperability software seen to that point. A local area network system called Chaosnet had been invented for the Lisp Machine (predating the commercial availability of Ethernet). The Symbolics system supported Chaosnet, but also had one of the first TCP/IP implementations. It also supported DECnet and IBM's SNA network protocols. A Dialnet protocol used phone lines and modems. Genera would, using hints from its distributed "namespace" database (somewhat similar to DNS, but more comprehensive, like parts of Xerox's Grapevine), automatically select the best protocol combination to use when connecting to network service. An application program (or a user command) would only specify the name of the host and the desired service. For example, a host name and a request for "Terminal Connection" might yield a connection over TCP/IP using the Telnet protocol (although there were many other possibilities). Likewise, requesting a file operation (such as a Copy File command) might pick NFS, FTP, NFILE (the Symbolics network file access protocol), or one of several others, and it might execute the request over TCP/IP, Chaosnet, or whatever other network was most suitable.
Application programs.
The most popular application program for the Symbolics Lisp Machine was the ICAD computer-aided engineering system. One of the first networked multi-player video games, a version of Spacewar, was developed for the Symbolics Lisp Machine in 1983. Electronic CAD software on the Symbolics Lisp Machine was used to develop the first implementation of the Hewlett-Packard Precision Architecture.
Contributions to computer science.
Symbolics' research and development staff (first at MIT, and then later at the company) produced a number of major innovations in software technology:
Symbolics Graphics Division.
The Symbolics Graphics Division (SGD, founded in 1982, sold to Nichimen Graphics in 1992) developed the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) for Symbolics Genera.
Movies.
This software was also used to create a few computer animated movies and was used for some popular movies.
Further reading.
</dl>

</doc>
<doc id="28198" url="http://en.wikipedia.org/wiki?curid=28198" title="Surfing">
Surfing

Surfing is a surface water sport in which the wave rider, referred to as a surfer, rides on the forward or deep face of a moving wave, which is usually carrying the surfer toward the shore. Waves suitable for surfing are primarily found in the ocean, but can also be found in lakes or in rivers in the form of a standing wave or tidal bore. However, surfers can also utilize artificial waves such as those from boat wakes and the waves created in artificial wave pools.
The term "surfing" refers to the act of riding a wave, regardless of whether the wave is ridden with a board or without a board, and regardless of the stance used (goofy or regular stance). The native peoples of the Pacific, for instance, surfed waves on alaia, paipo, and other such craft, and did so on their belly and knees. The modern-day definition of surfing, however, most often refers to a surfer riding a wave standing up on a surfboard; this is also referred to as stand-up surfing.
One variety of stand-up surfing is paddle boarding. Another prominent form of surfing is body boarding, when a surfer rides a wave on a bodyboard, either lying on their belly, drop knee, or sometimes even standing up on a body board. Other types of surfing include knee boarding, surf matting (riding inflatable mats), and using foils. Body surfing, where the wave is surfed without a board, using the surfer's own body to catch and ride the wave, is very common and is considered by some to be the purest form of surfing.
Three major subdivisions within standing-up surfing are long boarding, short boarding, and stand up paddle surfing (SUP), and these three have several major differences, including the board design and length, the riding style, and the kind of wave that is ridden.
In tow-in surfing (most often, but not exclusively, associated with big wave surfing), a motorized water vehicle, such as a personal watercraft, tows the surfer into the wave front, helping the surfer match a large wave's speed, which is generally a higher speed than a self-propelled surfer can produce. Surfing-related sports such as paddle boarding and sea kayaking do not require waves, and other derivative sports such as kite surfing and windsurfing rely primarily on wind for power, yet all of these platforms may also be used to ride waves. Recently with the use of V-drive boats, Wakesurfing, in which one surfs on the wake of a boat, has emerged. The Guinness Book of World Records recognized a 78 ft wave ride by Garrett McNamara at Nazaré, Portugal as the largest wave ever surfed, although this remains an issue of much contention amongst many surfers, given the difficulty of measuring a constantly changing mound of water.
Origins and history.
For centuries, surfing was a central part of ancient Polynesian culture. Surfing may have first been observed by Europeans at Tahiti in 1767 by Samuel Wallis and the crew members of the "Dolphin" who were the first Europeans to visit the island in June of that year. Another candidate is the botanist Joseph Banks being part of the first voyage of James Cook on the HMS "Endeavour", who arrived on Tahiti on 10 April 1769. Lieutenant James King was the first person to write about the art of surfing on Hawaii when he was completing the journals of Captain James Cook upon Cook's death in 1779.
When Mark Twain visited Hawaii in 1866 he wrote,
In one place we came upon a large company of naked natives, of both sexes and all ages, amusing themselves with the national pastime of surf-bathing.
References to surf riding on planks and single canoe hulls are also verified for pre-contact Samoa, where surfing was called "fa'ase'e" or "se'egalu" (see Augustin Krämer, "The Samoa Islands") and Tonga far pre-dating the practice of surfing by Hawaiians and eastern Polynesians by over a thousand years.
In July 1885, three teenage Hawaiian princes took a break from their boarding school, St. Mathew’s Hall in San Mateo, and came to cool off in Santa Cruz, California. There, David Kawananakoa, Edward Keliiahonui and Jonah Kuhio Kalaniana'ole surfed the mouth of the San Lorenzo River on custom-shaped redwood boards, according to surf historians Kim Stoner and Geoff Dunn.
George Freeth (8 November 1883 – 7 April 1919) is often credited as being the "Father of Modern Surfing". He is thought to have been the first modern surfer.
In 1907, the eclectic interests of the land baron Henry Huntington brought the ancient art of surfing to the California coast. While on vacation, Huntington had seen Hawaiian boys surfing the island waves. Looking for a way to entice visitors to the area of Redondo Beach, where he had heavily invested in real estate, he hired a young Hawaiian to ride surfboards. George Freeth decided to revive the art of surfing, but had little success with the huge 16-foot hardwood boards that were popular at that time. When he cut them in half to make them more manageable, he created the original "Long board", which made him the talk of the islands. To the delight of visitors, Freeth exhibited his surfing skills twice a day in front of the Hotel Redondo.
In 1975, professional contests started. That year Margo Oberg became the first female professional surfer.
Surf waves.
Swell is generated when wind blows consistently over a large area of open water, called the wind's fetch. The size of a swell is determined by the strength of the wind and the length of its fetch and duration. Because of this, surf tends to be larger and more prevalent on coastlines exposed to large expanses of ocean traversed by intense low pressure systems.
Local wind conditions affect wave quality, since the surface of a wave can become choppy in blustery conditions. Ideal conditions include a light to moderate "offshore" wind, because it blows into the front of the wave, making it a "barrel" or "tube" wave. Waves are Left handed and Right Handed depending upon the breaking formation of the wave.
Waves are generally recognized by the surfaces over which they break. For example, there are Beach breaks, Reef breaks and Point breaks.
The most important influence on wave shape is the topography of the seabed directly behind and immediately beneath the breaking wave. The contours of the reef or bar front becomes stretched by diffraction. Each break is different, since each location's underwater topography is unique. At beach breaks, sandbanks change shape from week to week. Surf forecasting is aided by advances in information technology. Mathematical modeling graphically depicts the size and direction of swells around the globe.
Swell regularity varies across the globe and throughout the year. During winter, heavy swells are generated in the mid-latitudes, when the North and South polar fronts shift toward the Equator. The predominantly Westerly winds generate swells that advance Eastward, so waves tend to be largest on West coasts during winter months. However, an endless train of mid-latitude cyclones cause the isobars to become undulated, redirecting swells at regular intervals toward the tropics.
East coasts also receive heavy winter swells when low-pressure cells form in the sub-tropics, where slow moving highs inhibit their movement. These lows produce a shorter fetch than polar fronts, however they can still generate heavy swells, since their slower movement increases the duration of a particular wind direction. The variables of fetch and duration both influence how long wind acts over a wave as it travels, since a wave reaching the end of a fetch behaves as if the wind died.
During summer, heavy swells are generated when cyclones form in the tropics. Tropical cyclones form over warm seas, so their occurrence is influenced by El Niño & La Niña cycles. Their movements are unpredictable.
Surf travel and some surf camps offer surfers access to remote, tropical locations, where tradewinds ensure offshore conditions. Since winter swells are generated by mid-latitude cyclones, their regularity coincides with the passage of these lows. Swells arrive in pulses, each lasting for a couple of days, with a few days between each swell.
The availability of free model data from the NOAA has allowed the creation of several surf forecasting websites.
Artificial reefs.
The value of good surf in attracting surf tourism has prompted the construction of artificial reefs and sand bars. Artificial surfing reefs can be built with durable sandbags or concrete, and resemble a submerged breakwater. These artificial reefs not only provide a surfing location, but also dissipate wave energy and shelter the coastline from erosion. Ships such as Seli 1 that have accidentally stranded on sandy bottoms, can create sandbanks that give rise to good waves.
An artificial reef known as Chevron Reef was constructed in El Segundo, California in hopes of creating a new surfing area. However, the reef failed to produce any quality waves and was removed in 2008. In Kovalam, South West India, an artificial reef has, however, successfully provided the local community with a quality lefthander, stabilized coastal soil erosion, and provided good habitat for marine life. ASR Ltd., a New Zealand-based company, constructed the Kovalam reef and is working on another reef in Boscombe, England.
Even with artificial reefs in place, a tourist's vacation time may coincide with a "flat spell", when no waves are available. Completely artificial Wave pools aim to solve that problem by controlling all the elements that go into creating perfect surf, however there are only a handful of wave pools that can simulate good surfing waves, owing primarily to construction and operation costs and potential liability. Most wave pools generate waves that are too small and lack the power necessary to surf. The Seagaia Ocean Dome, located in Miyazaki, Japan, was an example of a surfable wave pool. Able to generate waves with up to 10-foot faces, the specialized pump held water in 20 vertical tanks positioned along the back edge of the pool. This allowed the waves to be directed as they approach the artificial sea floor. Lefts, Rights, and A-frames could be directed from this pump design providing for rippable surf and barrel rides. The Ocean Dome cost about $2 billion to build and was expensive to maintain. The Ocean Dome was closed in 2007. In England, construction is nearing completion on the The Wave, situated near Bristol, which will enable people unable to get to the coast to enjoy the waves in a controlled environment, set in the heart of nature.
Surfers and surf culture.
Surfers represent a diverse culture based on riding the waves. Some people practice surfing as a recreational activity while others make it the central focus of their lives. Within the United States, surfing culture is most dominant in Hawaii and California because these two states offer the best surfing conditions. However, waves can be found wherever there is coastline, and a tight-knit yet far-reaching subculture of surfers has emerged throughout America. Some historical markers of the culture included the woodie, the station wagon used to carry surfers' boards, as well as boardshorts, the long swim shorts typically worn while surfing. Surfers also wear wetsuits in colder regions.
The sport of surfing now represents a multi-billion dollar industry especially in clothing and fashion markets. The Association of Surfing Professionals (ASP) runs the world tour, hosting top competitors in some of the best surf spots around the globe. A small number of people make a career out of surfing by receiving corporate sponsorships and performing for photographers and videographers in far-flung destinations; they are typically referred to as freesurfers.
When the waves were flat, surfers persevered with sidewalk surfing, which is now called skateboarding. Sidewalk surfing has a similar feel to surfing and requires only a paved road or sidewalk. To create the feel of the wave, surfers even sneak into empty backyard swimming pools to ride in, known as pool skating. Eventually, surfing made its way to the slopes with the invention of the Snurfer, later credited as the first snowboard. Many other board sports have been invented over the years, but all can trace their heritage back to surfing.
Many surfers claim to have a spiritual connection with the ocean, describing surfing, the surfing experience, both in and out of the water, as a type of spiritual experience or a religion.
Maneuvers.
Standup surfing begins when the surfer paddles toward shore in an attempt to match the speed of the wave (The same applies whether the surfer is standup paddling, bodysurfing, boogie-boarding or using some other type of watercraft, such as a waveski or kayak.). Once the wave begins to carry the surfer forward, the surfer stands up and proceeds to ride the wave. The basic idea is to position the surfboard so it is just ahead of the breaking part (whitewater) of the wave. A common problem for beginners is being able to catch the wave at all.
Surfers' skills are tested by their ability to control their board in difficult conditions, riding challenging waves, and executing maneuvers such as strong turns and cutbacks (turning board back to the breaking wave) and "carving" (a series of strong back-to-back maneuvers). More advanced skills include the "floater" (riding on top of the breaking curl of the wave), and "off the lip" (banking off the breaking wave). A newer addition to surfing is the progression of the "air" whereby a surfer propels off the wave entirely up into the air, and then successfully lands the board back on the wave.
The tube ride is considered to be the ultimate maneuver in surfing. As a wave breaks, if the conditions are ideal, the wave will break in an orderly line from the middle to the shoulder, enabling the experienced surfer to position themselves inside the wave as it is breaking. This is known as a tube ride. Viewed from the shore, the tube rider may disappear from view as the wave breaks over the rider's head. The longer the surfer remains in the tube, the more successful the ride. This is referred to as getting tubed, barreled, shacked or pitted. Some of the world's best known waves for tube riding include Pipeline on the North shore of Oahu, Teahupoo in Tahiti and G-Land in Java. Other names for the tube include "the barrel", and "the pit".
Hanging ten and hanging five are moves usually specific to long boarding. Hanging Ten refers to having both feet on the front end of the board with all of the surfer's toes off the edge, also known as nose-riding. Hanging Five is having just one foot near the front, with five toes off the edge.
Cutback: Generating speed down the line and then turning back to reverse direction.
Floater: Suspending the board atop the wave. Very popular on small waves.
Top-Turn: Turn off the top of the wave. Sometimes used to generate speed and sometimes to shoot spray.
Air / Aerial: Launching the board off the wave entirely, then re-entering the wave. Various airs include ollies, lien airs, method airs, and other skateboard-like maneuvers.
Terms.
The Glossary of surfing includes some of the extensive vocabulary used to describe various aspects of the sport of surfing as described in literature on the subject. In some cases terms have spread to a wider cultural use. These terms were originally coined by people who were directly involved in the sport of surfing.
Learning.
Many popular surfing destinations have surf schools and surf camps that offer lessons. Surf camps for beginners and intermediates are multi-day lessons that focus on surfing fundamentals. They are designed to take new surfers and help them become proficient riders. All-inclusive surf camps offer overnight accommodations, meals, lessons and surfboards. Most surf lessons begin with instruction and a safety briefing on land, followed by instructors helping students into waves on longboards or "softboards". The softboard is considered the ideal surfboard for learning, due to the fact it is safer, and has more paddling speed and stability than shorter boards. Funboards are also a popular shape for beginners as they combine the volume and stability of the longboard with the manageable size of a smaller surfboard.
New and inexperienced surfers typically learn to catch waves on softboards around the 7–8 foot funboard size. Due to the softness of the surfboard the chance of getting injured is substantially minimized. Costco's alaia design Wavestorm surfboard is a cheap and cost efficient option; unlike many surfboards, this one comes with rubber fins, stock leash, pre-installed traction.
Typical surfing instruction is best performed one-on-one, but can also be done in a group setting. The most popular surf locations offer perfect surfing conditions for beginners, as well as challenging breaks for advanced students. The ideal conditions for learning would be small waves that crumble and break softly, as opposed to the steep, fast-peeling waves desired by more experienced surfers. When available, a sandy seabed is generally safer.
Surfing can be broken into several skills: Paddling strength, Positioning to catch the wave, timing, and balance. Paddling out requires strength, but also the mastery of techniques to break through oncoming waves ("duck diving", "eskimo roll"). Take-off positioning requires experience at predicting the wave set and where they will break. The surfer must pop up quickly as soon as the wave starts pushing the board forward. Preferred positioning on the wave is determined by experience at reading wave features including where the wave is breaking. Balance plays a crucial role in standing on a surfboard. Thus, balance training exercises are a good preparation. Practicing with a Balance board or swing boarding helps novices master the art.
Equipment.
Surfing can be done on various equipment, including surfboards, longboards, Stand Up Paddle boards (SUP's), bodyboards, wave skis, skimboards, kneeboards, surf mats and macca's trays. Surfboards were originally made of solid wood and were large and heavy (often up to 12 ft long and 150 lb). Lighter balsa wood surfboards (first made in the late 1940s and early 1950s) were a significant improvement, not only in portability, but also in increasing maneuverability.
Most modern surfboards are made of fiberglass foam (PU), with one or more wooden strips or "stringers", fiberglass cloth, and polyester resin (PE). An emerging board material is epoxy resin and Expanded Polystyrene foam (EPS) which is stronger and lighter than traditional PU/PE construction. Even newer designs incorporate materials such as carbon fiber and variable-flex composites in conjunction with fiberglass and epoxy or polyester resins. Since epoxy/EPS surfboards are generally lighter, they will float better than a traditional PU/PE board of similar size, shape and thickness. This makes them easier to paddle and faster in the water. However, a common complaint of EPS boards is that they do not provide as much feedback as a traditional PU/PE board. For this reason, many advanced surfers prefer that their surfboards be made from traditional materials.
Other equipment includes a leash (to stop the board from drifting away after a wipeout, and to prevent it from hitting other surfers), surf wax, traction pads (to keep a surfer's feet from slipping off the deck of the board), and fins (also known as "skegs") which can either be permanently attached ("glassed-on") or interchangeable. Sportswear designed or particularly suitable for surfing may be sold as "boardwear" (the term is also used in snowboarding). In warmer climates, swimsuits, surf trunks or boardshorts are worn, and occasionally rash guards; in cold water surfers can opt to wear wetsuits, boots, hoods, and gloves to protect them against lower water temperatures. A newer introduction is a rash vest with a thin layer of titanium to provide maximum warmth without compromising mobility. In recent years, there have been advancements in technology that have allowed surfers to pursue even bigger waves with added elements of safety. Big wave surfers are now experimenting with inflatable vests or colored dye packs to help decrease their odds of drowning.
There are many different surfboard sizes, shapes, and designs in use today. Modern longboards, generally 9 to in length, are reminiscent of the earliest surfboards, but now benefit from modern innovations in surfboard shaping and fin design. Competitive longboard surfers need to be competent at traditional "walking" manoeuvres, as well as the short-radius turns normally associated with shortboard surfing. The modern shortboard began life in the late 1960s and has evolved into today's common "thruster" style, defined by its three fins, usually around 6 to in length. The thruster was invented by Australian shaper Simon Anderson.
Midsize boards, often called funboards, provide more maneuverability than a longboard, with more flotation than a shortboard. While many surfers find that funboards live up to their name, providing the best of both surfing modes, others are critical.
There are also various niche styles, such as the "Egg", a longboard-style short board targeted for people who want to ride a shortboard but need more paddle power. The "Fish", a board which is typically shorter, flatter, and wider than a normal shortboard, often with a split tail (known as a "swallow tail"). The Fish often has two or four fins and is specifically designed for surfing smaller waves. For big waves there is the "Gun", a long, thick board with a pointed nose and tail (known as a pin tail) specifically designed for big waves.
Famous locations.
In Australia.
Margaret River – Yallingup and Prevelly Park, WA (Western Australia).
260 km south of Perth, the tiny resort village of Yallingup marks the beginning of the famed Margaret River winery region, where wine enthusiasts and ‘waxheads’ (board-riders) have long converged in equal numbers. With several breaks that range from mild to monstrous depending on the swell, Yallingup is considered the best all-round surfing destination on Australia’s west coast.
Further south, Prevelly Park is the heart of serious Margaret River surfing territory, where swells up to six metres get spun into perfect barrels across the treacherous offshore reef. No place for beginners or the faint-of-heart, "Surfers Point" at Prevelly even attracts the big-name big-wave lunatics from the US and Hawaii, and it’s one of the few places in Australia where board-riders wear helmets and nobody laughs at them.
Crescent Head, NSW.
The coastline beginning just north of Port Macquarie through to Crescent Head is accessed via Point Plomer Road, which ribbons the coast for 25 km. Along this route are four perfect right-hand point-breaks, tailor-made for long-board riders, grommets and beginners and capable of generating miracle rides of 200 metres. The point break at Crescent Head itself is revered by long-boarders the world over, and some of the sport’s best have been filmed here "Hanging Ten" or cross-stepping the length of their 10-foot planks, however, the accumulated sand necessary to enable this wave to run has been significantly depleted in recent years, the result being that the break has suffered in consistency. There is no longer an actual ride-able wave on the point, its glory days only living on in the memory of local surfers. Halfway between Crescent Head and Point Plomer is the brilliantly named Delicate Nobby, a wedge-shaped rock formation that starts just off the beach and spears out into the Pacific, creating beach breaks on either side.
Northern Beaches, NSW.
Beginning at Manly Beach and running 20 km north to Palm Beach, Sydney’s northern peninsula offers a succession of surf beaches unmatched by a city environment anywhere else on earth. Manly itself has playful beach breaks and punchy barrels, plus the offshore Queenscliff "Bommie" (bombora), joy for big wave riders. Neighbouring Freshwater Beach is much loved by bodysurfers and youngsters on body-boards; this is also where surfboard-riding was first introduced to Australia by Hawaiian surfer Duke Kahanamoku, on 15 January 1915. Continuing north, the 6 km coastal corridor between Dee Why Beach and North Narrabeen is widely considered Sydney’s blue-ribbon surfing belt, with the legendary Long Reef bombora (known locally as "Butter-box") situated smack in the middle. The surfing tribes of Mona Vale Beach, Newport Beach, Whale Beach and Avalon Beach can all make a good case for choosing their own local breaks over their neighbours’, or you could try all four beaches in a lazy half-day. Finally, the distinctive burnt-orange sands of Palm Beach mark the end of the peninsula, its 1.5 km procession of beach breaks offering thrills and spills for surfers, body-boarders and wave-ski paddlers.
Seal Rocks and Pacific Palms, NSW.
Lighthouse Beach and Treachery Beach at Seal Rocks are south-facing and known for generating epic waves when a south swell rolls in. Just 22 km up the road at Pacific Palms, Boomerang Beach and Bluey’s Beach are blessed with their own postcard waves shaped by prominent headlands, and often visited by cheeky dolphins that love showing the rest of us how surfing should really be done. This part of the NSW coast has remained miraculously undeveloped too; there’s nary a high-rise, nightclub or casino in sight, making it the perfect place for a true ‘soul surfer’ experience.
Snapper Rocks, QLD.
Snapper Rocks is a sand bottom point break considered as a world renowned surfing spot on the Gold Coast. Snapper, located at Rainbow Bay, is home to the world-famous ‘Super Bank’, regarded in surfing circles as the longest, most consistent and most hollow wave in the world. The swell here often reaches six to eight feet, and one good, clean wave can transport you from Snapper to Kirra, a distance of almost two kilometres. Snapper Rocks hosts elite international surfing events such as the Quiksilver and Roxy Pro, Rip Curl Masters, and MP Classic. It is also a favourite surfing spot of local world champs, Mick Fanning, Joel Parkinson and Stephanie Gilmore, who enjoy nothing more than surfing their own ‘local’ break when they’re at home.
Noosa – Point Break, QLD.
One of the best and most photogenic long-board breaks in the world, the point at Noosa is capable of producing a genuine 200 metre ride on its best days. In a decent swell especially there’s always a big crew of locals riding it who really know how to "walk the plank", but when it’s smaller it’s perfect for beginners – a long, easy-rolling cruise.
North Coast – Angourie to Byron Bay, NSW.
When the surfing counter-culture took hold in Australia in the late 1960s, the NSW north coast quickly became the promised land for anyone with a board and a hankering for an alternative lifestyle communing with the waves. "Discovered" in the early 1970s, the point break at Angourie remained relatively unheralded for the next two decades, but it’s world famous nowadays as home break of Aussie surfing legend Nat Young. Endlessly filmed and fawned over, the right-hand point-break at Lennox Head rates a mention in any discussion of Australia’s best wave.
Bells Beach, Victoria.
Although the final scene of the film "Point Break" is supposedly set at Bells Beach, the scene was not filmed there. Bells Beach is a straight stretch and the beach in the film is a cove with spruce trees atop a hill. The actual location of the film was a beach called Indian Beach, in Ecola State Park, located in Cannon Beach, Oregon, USA. Bells Beach is visited in the 1966 documentary film The Endless Summer.
Bells Beach is the home of the world's longest-running surfing competition – the Rip Curl Pro Surf & Music Festival. The event was formerly known as the Bells Beach Surf Classic. The competition was first held in January 1961 and then at Easter every year since although occasionally, when conditions at Bells are unsuitable, the competition has been transferred to other breaks such as Johanna.
As early as 1939, surfers from Torquay made their way to Bells, but access was a considerable problem until 1960 when Torquay surfers and Olympic wrestler Joe Sweeney hired a bulldozer and cleared a road along the Bells cliff [1] from the Cobb & Co Road, where the concrete wave now stands, down to the beach. He charged one pound per surfer to recover his expenses. This is now part of the Torquay to Anglesea walking track.
Nearby surf breaks include "Southside", "Centreside", "Rincon", "Winki Pop", (Uppers and Lowers), Boobs and Steps. Although Bells is known internationally as one of the best breaks in Victoria, "Winki Pop" often works better under more diverse conditions than the other nearby breaks.
In 1988, a group of local surfers who were concerned about the human impact that tourism was having on the Bells Beach Surfing Reserve started a group called Surfers Appreciating the Natural Environment. Since 1988 they have met monthly to revegetate the reserve in an effort to bring it back to its original state. They have planted over 100,000 plants there.
In Asia.
Arugam Bay, Ullae (Pottuvil, Sri Lanka).
Arugam Bay is a small fishing village that was, for many years, only known to a small group of surfers, who considered the area to be Asia's surfing "mecca" ever since the 1960s. Due to Sri Lanka's long-running civil war, this remote half-moon-shaped bay was mostly unknown to visitors and tourists. The consistent swell; shark-free, permanently warm (28 C), clear water; and budget accommodation brought Arugam Bay to the attention of international surfers.
In June 2010, the ASP held an international competition—the "6-Star SriLankan Airlines Pro"—at Arugam Bay that was won by Australian Julian Wilson. Prior to the commencement of the inaugural ASP event, the location's warm waters and "high performance sand bottom point waves" were highlighted.
Following a tsunami in 2004, most of the hotels in Arugam Bay were destroyed. By 2008, most of the tourism infrastructure was restored and the Community Eco-Guide Association (CEGA)—a thirteen-member collaboration between local community-based organizations (CBOs), cooperatives and associations—promotes sustainable community-based tourism in the area.
In the South Pacific.
Teahupoʻo (Tahiti).
Teahupoʻo (pronounced cho-po) is a world-renowned surf spot off the South West of the island of Tahiti, French Polynesia, southern Pacific Ocean. It is known for its heavy, glassy waves, often reaching 2 to and higher. It is the site of the annual Billabong Pro Tahiti surf competition, part of the World Championship Tour (WCT) of the professional surfing circuit.
In South Africa.
Jeffreys Bay (Eastern Cape).
The break is regarded as one of the best right-hand point breaks in the entire world, in both consistency and quality, in season. It has been divided up into several sections, including, from the top of the point, Kitchen Windows, Magna tubes, Boneyards, Supertubes, Impossibles, Tubes, the Point, and Albatross. "Supertubes", which itself breaks for about 300m or more, is regarded as the best part of the wave. On rare occasions (large wave sizes, wide-breaking waves, and even swells), Boneyards can link up all the way to the Point for a ride over one kilometer long. The most consistent waves occur between about May to mid September, also often coinciding with offshore winds, although good waves can occasionally occur at other times of the year.
The initial discovery and promotion of the wave is curious. Another nearby right-hand point wave at St Francis Bay (Bruce's Beauty) was first idolised and promoted in the cult classic surf movie "The Endless Summer" in the 1960s (although both Jeffreys Bay and St. Francis Bay had probably been surfed much earlier). Surfers who travelled to the area soon stumbled upon the nearby Jeffreys Bay surf break, which was found to be not only a faster, more powerful, and hollower wave, but also much more consistent.
In North America.
Zicatela Beach (Mexican Pipeline).
Zicatela is a beach located in the town of Puerto Escondido, Oaxaca. Nicknamed the "Mexican Pipeline" due to the similar power and shape of the Banzai Pipeline on the North Shore of Oahu, the wave that breaks on Zicatela Beach draws an international crowd of surfers, bodyboarders and their entourages. Mid to late summer is low season for tourists, but a prime time for waves and international tournaments. A number of international competitions such as the ESPN X Games, and the MexPipe Challenge have taken place.
In Central America.
La Libertad (El Salvador).
El Puerto is home to one of the best right points in Central America, known for its fast hollow, pulsing, over 30-second ride waves. Punta Roca (also called "La Punta" by local surfers) has been the perfect spot for many known surfers who back in the 1970s discovered the point with only a few local surfers brave enough to venture into its rocky bottom plane. It is known that legend Gerry Lopez, traveled frequently to this surf spot back in the 1970s encouraging a new wave of locals to get into the sport.
By the 1980s, El Salvador went through a civil war, and getting to the point was rather dangerous slowing visitors, and with that, a scarcity of surf boards to the locals whose only means of getting a surf board was by travelers leaving them behind in exchange of guidance and accommodations. Local legend, "Yepi" was one of the first of his generation to take on full self-support and help maintain the sport, a popular activity among locals. Locals have also been increasing the popularity of the sport throughout the country by offering custom surf tours to tourists and visitors in the region.
The main wave extends from La Punta to the township, a distance of about 800m, although single rides do not normally connect along this whole distance. On a good 6 to 8 feet day (Hawaiian scale), the top part of the point produces the best waves, giving a ride of about 300m or more. The wave features a relatively easy takeoff with long, fast, powerful walls, with longer hollow barrels on the best days. This wave works from about 3 to 12 feet (Hawaiian scale), and can barrel anywhere along the point, but most often closest to the takeoff area. The main takeoff is close to a dangerous rock which often sticks out of the water, and has caused injuries. It works on all tides, although low tide probably has more barrels. The wave is unusual in that it often breaks at a slight angle to the shoreline, hitting it slightly squarely, creating powerful and fast walls. It can be difficult to get out the back in large swells, and the rocky shoreline is notorious for its rather difficult entry.
Further down the point are a few other breaks, including next to the cemetery and in the town itself. These are less crowded and can produce waist-high waves on occasions, but the world-class section of the point is way on the outside. Other surf spots around the region include: Conchalio, La Paz, San Diego, El Tunco, El Zunzal, La Bocana, El Zonte.
In South America.
Chicama (La Libertad-Peru).
This is one of the longest waves in the world with up to 4 km of left waves over more than 3 separate sections of surf. The different sections on the long cape don't link up, and the longest rides are usually only up to about 1 km.
In the USA.
Mavericks (California).
Maverick's or Mavericks is a world-famous surfing location in Northern California. It is located approximately one-half mile (0.8 km) from shore in Pillar Point Harbor, just North of Half Moon Bay at the village of Princeton-By-The-Sea. After a strong winter storm in the Northern Pacific Ocean, waves can routinely crest at over 25 ft and top out at over 50 ft. The break is caused by an unusually-shaped underwater rock formation.
Trestles (San Clemente).
Located at the northern end of San Diego County, Trestles is the name given to a series of breaks known for their unique quality (wave shape), particularly Lowers (also called Lower Trestles). Each break is popular depending on swell direction, season, and each surfer's preferred riding style. Lowers is frequently the venue of world-class surfing events, including the top-level of professional surfing. Lowers is often considered the best summertime high-performance wave in California.
Pipeline (Hawaii).
Pipeline is a surf reef break located in Hawaii, off Ehukai Beach Park in Pupukea on O'ahu's North Shore. The spot is notorious and famous for its huge waves breaking in shallow water just above its sharp and cavernous reef, forming large, hollow and thick curls of water that surfers can ride inside of. There are three reefs at Pipeline in progressively deeper water further out to sea that activate at various power levels applied by ocean swells.
In Europe.
Costa da Caparica (Almada, Portugal).
Caparica Beaches are popular Atlantic beaches located on Portugal's Almada coast, near Lisbon. The Caparica Coast, with part of the "Protected Landscape of the Ancient Beach of Costa da Caparica", is visible the Convent of the Capuchos. The beach has preferred surfing conditions and is also popular for windsurfing, and kitesurfing. The International Surf Center is based in Caparica.
Supertubos (Peniche, Portugal).
The little fishing town of Peniche is probably the most renowned surfing area in Portugal. Originally an island, Peniche became one with the mainland due to the silting up of the shallow channel that divided it from the rest of the country. Today that short and narrow spit of land contains an obscene amount of wave variety that can provide the goods in almost any conditions. Most famous is Supertubos, regarded by many as one of Europe’s best beach breaks, but there are plenty of other barrels to pull into around Peniche. Peniche is a year round destination with swell exposure on the north side of the town and shelter on the south. The town also sits at the dividing point between the cooler and wetter north and the dry, sunny south meaning that summers are long but tempered by cool sea breezes and the winters mild though occasionally stormy. Supertubos is considered the best wave in Portugal and one of the best in Europe. It is a fast and tubular wave which breaks on a hollow sand bank. It works best with SW swells and N, NE or NW winds. Andy Irons, Kelly Slater and Mick Fanning made frequent appearances in the Supertubos surf competitions.
Nazaré (Portugal).
Nazaré has become a popular tourist attraction, advertising itself, internationally, as a picturesque seaside village. Located on the Atlantic coast, it has long sandy beaches (considered by some to be among the best beaches in Portugal), with lots of tourists in the summer. Nazaré is where the largest wave ever surfed, in 2011, by the American surfer Garrett McNamara. The town used to be known for its traditional costumes worn by the fishermen and their wives who wore a traditional headscarf and embroidered aprons over seven flannel skirts in different colours. These dresses can still occasionally be seen.
Dangers.
Drowning.
Surfing, like all water sports, carries the inherent danger of drowning. Anyone at any age can learn to surf, but should have at least intermediate swimming skills. Although the board assists a surfer in staying buoyant, it can become separated from the user. A leash, attached to the ankle or knee, can keep a board from being swept away, but does not keep a rider on the board or above water. In some cases, possibly including the drowning of professional surfer Mark Foo, a leash can even be a cause of drowning by snagging on a reef or other object and holding the surfer underwater. By keeping the surfboard close to the surfer during a wipeout, a leash also increases the chances that the board may strike the rider, which could knock him or her unconscious and lead to drowning. A fallen rider's board can become trapped in larger waves, and if the rider is attached by a leash, he or she can be dragged for long distances underwater. Surfers should be careful to remain in smaller surf until they have acquired the advanced skills and experience necessary to handle bigger waves and more challenging conditions. However, even world-class surfers have drowned in extremely challenging conditions.
Collisions.
Under the wrong set of conditions, anything that a surfer's body can come in contact with is potentially a danger, including sand bars, rocks, small ice, reefs, surfboards, and other surfers. Collisions with these objects can sometimes cause injuries such as cuts and scrapes and in rare instances, death.
A large number of injuries, up to 66%, are caused by collision with a surfboard (nose or fins). Fins can cause deep lacerations and cuts, as well as bruising. While these injuries can be minor, they can open the skin to infection from the sea; groups like Surfers Against Sewage campaign for cleaner waters to reduce the risk of infections. Local bugs and disease can be a dangerous factor when surfing around the globe.
Falling off a surfboard or colliding with others is commonly referred to as a "wipeout".
Marine life.
Sea life can sometimes cause injuries and even fatalities. Animals such as sharks, stingrays, Weever fish, seals and jellyfish can sometimes present a danger. Warmer-water surfers often do the "stingray shuffle" as they walk out through the shallows, shuffling their feet in the sand to scare away stingrays that may be resting on the bottom.
Rip currents.
Rip currents are water channels that flow away from the shore. Under the wrong circumstances these currents can endanger both experienced and inexperienced surfers. Since a rip current appears to be an area of flat water, tired or inexperienced swimmers or surfers may enter one and be carried out beyond the breaking waves. Although many rip currents are much smaller, the largest rip currents have a width of forty or fifty feet. However, by paddling parallel to the shore, a surfer can easily exit a rip current. Alternatively, some surfers actually ride on a rip current because it is a fast and effortless way to get out beyond the zone of breaking waves.
Seabed.
The seabed can pose dangers for surfers. If a surfer falls while riding a wave, the wave tosses and tumbles the surfer around, often in a downwards direction. At reef breaks and beach breaks, surfers have been seriously injured and even killed because of a violent collision with the sea bed, the water above which can sometimes be very shallow, especially at beach breaks or reef breaks during low tide. Cyclops, Western Australia, for example is one of the biggest and thickest reef breaks in the world, with waves measuring up to 10 metres high, but the reef below is only about 2 m below the surface of the water.

</doc>
<doc id="28202" url="http://en.wikipedia.org/wiki?curid=28202" title="September 24">
September 24

September 24 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28203" url="http://en.wikipedia.org/wiki?curid=28203" title="September 25">
September 25

September 25 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28204" url="http://en.wikipedia.org/wiki?curid=28204" title="September 29">
September 29

September 29 is the day of the year in the Gregorian calendar.

</doc>
<doc id="28207" url="http://en.wikipedia.org/wiki?curid=28207" title="Short Message Service">
Short Message Service

Short Message Service (SMS) is a text messaging service component of phone, Web, or mobile communication systems. It uses standardized communications protocols to allow fixed line or mobile phone devices to exchange short text messages.
SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile phone subscribers at the end of 2010. The term "SMS" is used for both the user activity and all types of short text messaging in many parts of the world. SMS is also employed in direct marketing, known as SMS marketing. As of September 2014, global SMS messaging business is said to be worth over USD 100 billion, and SMS accounts for almost 50 percent of all the revenue generated by mobile messaging.
SMS as used on modern handsets originated from radio telegraphy in radio memo pagers using standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards<ref name="GSM 28/85">GSM Doc 28/85 "Services and Facilities to be provided in the GSM System" rev2, June 1985</ref> as a means of sending messages of up to 160 characters to and from GSM mobile handsets. Though most SMS messages are mobile-to-mobile text messages, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS, as well as satellite and landline networks.
History.
Initial concept.
Adding text messaging functionality to mobile devices began in the early 1980s. The first action plan of the CEPT Group GSM was approved in December 1982, requesting that, "The services and facilities offered in the public switched telephone networks and public data networks ... should be available in the mobile system." This plan included the exchange of text messages either directly between mobile stations, or transmitted via message handling systems in use at that time.
The SMS concept was developed in the Franco-German GSM cooperation in 1984 by Friedhelm Hillebrand and Bernard Ghillebaert. The GSM is optimized for telephony, since this was identified as its main application. The key idea for SMS was to use this telephone-optimized system, and to transport messages on the signalling paths needed to control the telephone traffic during periods when no signalling traffic existed. In this way, unused resources in the system could be used to transport messages at minimal cost. However, it was necessary to limit the length of the messages to 128 bytes (later improved to 160 seven-bit characters) so that the messages could fit into the existing signalling formats. Based on his personal observations and on analysis of the typical lengths of postcard and Telex messages, Hillebrand argued that 160 characters was sufficient to express most messages succinctly.
SMS could be implemented in every mobile station by updating its software. Hence, a large base of SMS-capable terminals and networks existed when people began to use SMS. A new network element required was a specialized short message service centre, and enhancements were required to the radio capacity and network transport infrastructure to accommodate growing SMS traffic.
Early development.
The technical development of SMS was a multinational collaboration supporting the framework of standards bodies. Through these organizations the technology was made freely available to the whole world.
The first proposal which initiated the development of SMS was made by a contribution of Germany and France into the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result was approved by the main GSM group in a June '85 document which was distributed to industry. The input documents on SMS had been prepared by Friedhelm Hillebrand (Deutsche Telekom) with contributions from Bernard Ghillebaert (France Télécom). The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the prevailing messaging paging that some in GSM might have had in mind.
SMS was considered in the main GSM group as a possible service for the new digital cellular system. In GSM document "Services and Facilities to be provided in the GSM System," both mobile-originated and mobile-terminated short messages appear on the table of GSM teleservices.
The discussions on the GSM services were concluded in the recommendation GSM 02.03 "TeleServices supported by a GSM PLMN." Here a rudimentary description of the three services was given:
The material elaborated in GSM and its WP1 subgroup was handed over in Spring 1987 to a new GSM body called IDEG (the Implementation of Data and Telematic Services Experts Group), which had its kickoff in May 1987 under the chairmanship of Friedhelm Hillebrand (German Telecom). The technical standard known today was largely created by IDEG (later WP4) as the two recommendations GSM 03.40 (the two point-to-point services merged) and GSM 03.41 (cell broadcast).
WP4 created a Drafting Group Message Handling (DGMH), which was responsible for the specification of SMS. Finn Trosby of Telenor chaired the draft group through its first 3 years, in which the design of SMS was established. DGMH had five to eight participants, and Finn Trosby mentions as major contributors Kevin Holley, Eija Altonen, Didier Luizard and Alan Cox. The first action plan mentions for the first time the Technical Specification 03.40 "Technical Realisation of the Short Message Service". Responsible editor was Finn Trosby. The first and very rudimentary draft of the technical specification was completed in November 1987. However, drafts useful for the manufacturers followed at a later stage in the period. A comprehensive description of the work in this period is given in.
The work on the draft specification continued in the following few years, where Kevin Holley of Cellnet (now Telefónica O2 UK) played a leading role. Besides the completion of the main specification GSM 03.40, the detailed protocol specifications on the system interfaces also needed to be completed.
Support in other architectures.
The Mobile Application Part (MAP) of the SS7 protocol included support for the transport of Short Messages through the Core Network from its inception. MAP Phase 2 expanded support for SMS by introducing a separate operation code for Mobile Terminated Short Message transport. Since Phase 2, there have been no changes to the Short Message operation packages in MAP, although other operation packages have been enhanced to support CAMEL SMS control.
From 3GPP Releases 99 and 4 onwards, CAMEL Phase 3 introduced the ability for the Intelligent Network (IN) to control aspects of the Mobile Originated Short Message Service, while CAMEL Phase 4, as part of 3GPP Release 5 and onwards, provides the IN with the ability to control the Mobile Terminated service. CAMEL allows the gsmSCP to block the submission (MO) or delivery (MT) of Short Messages, route messages to destinations other than that specified by the user, and perform real-time billing for the use of the service. Prior to standardized CAMEL control of the Short Message Service, IN control relied on switch vendor specific extensions to the Intelligent Network Application Part (INAP) of SS7.
Early implementations.
The first SMS message was sent over the Vodafone GSM network in the United Kingdom on 3 December 1992, from Neil Papworth of Sema Group (now Mavenir Systems) using a personal computer to Richard Jarvis of Vodafone using an Orbitel 901 handset. The text of the message was "Merry Christmas."
The first commercial deployment of a short message service center (SMSC) was by Aldiscon part of Logica (now part of Acision) with Telia (now TeliaSonera) in Sweden in 1993, followed by Fleet Call (now Nextel) in the US, Telenor in Norway and BT Cellnet (now O2 UK) later in 1993. All first installations of SMS gateways were for network notifications sent to mobile phones, usually to inform of voice mail messages. The first commercially sold SMS service was offered to consumers, as a person-to-person text messaging service by Radiolinja (now part of Elisa) in Finland in 1993. Most early GSM mobile phone handsets did not support the ability to send SMS text messages, and Nokia was the only handset manufacturer whose total GSM phone line in 1993 supported user-sending of SMS text messages.
Initial growth was slow, with customers in 1995 sending on average only 0.4 messages per GSM customer per month. One factor in the slow takeup of SMS was that operators were slow to set up charging systems, especially for prepaid subscribers, and eliminate billing fraud which was possible by changing SMSC settings on individual handsets to use the SMSCs of other operators. Initially, networks in the UK only allowed customers to send messages to other users on the same network, limiting the usefulness of the service. This restriction was lifted in 1999.
Over time, this issue was eliminated by switch billing instead of billing at the SMSC and by new features within SMSCs to allow blocking of foreign mobile users sending messages through it. By the end of 2000, the average number of messages reached 35 per user per month, and on Christmas Day 2006, over 205 million messages were sent in the UK alone.
It is also alleged that the fact that roaming customers, in the early days, rarely received bills for their SMSs after holidays abroad which gave a boost to text messaging as an alternative to voice calls.
Text messaging outside GSM.
SMS was originally designed as part of GSM, but is now available on a wide range of networks, including 3G networks. However, not all text messaging systems use SMS, and some notable alternative implementations of the concept include J-Phone's "SkyMail" and NTT Docomo's "Short Mail", both in Japan. Email messaging from phones, as popularized by NTT Docomo's i-mode and the RIM BlackBerry, also typically uses standard mail protocols such as SMTP over TCP/IP.
SMS today.
In 2010[ [update]], 6.1 trillion (6.1 × 1012) SMS text messages were sent. This translates into an average of 193,000 SMS per second. SMS has become a huge commercial industry, earning $114.6 billion globally in 2010. The global average price for an SMS message is US$0.11, while mobile networks charge each other interconnect fees of at least US$0.04 when connecting between different phone networks.
In 2015, the actual cost of sending an SMS in Australia was found to be $0.00016 per SMS. 
In 2014, developed the world's first SMS-based voter registration system in Libya. So far, more than 1.5 million people have registered using that system, providing Libyan voters with unprecedented access to the democratic process.
While SMS is still a growing market, traditional SMS is becoming increasingly challenged by alternative messaging services such as Facebook Messenger, WhatsApp and Viber available on smart phones with data connections, especially in Western countries where these services are growing in popularity.
Technical details.
GSM.
The "Short Message Service—Point to Point (SMS-PP)"—was originally defined in GSM recommendation 03.40, which is now maintained in 3GPP as TS 23.040. GSM 03.41 (now 3GPP TS 23.041) defines the "Short Message Service—Cell Broadcast (SMS-CB)", which allows messages (advertising, public information, etc.) to be broadcast to all mobile users in a specified geographical area.
Messages are sent to a short message service center (SMSC), which provides a "store and forward" mechanism. It attempts to send messages to the SMSC's recipients. If a recipient is not reachable, the SMSC queues the message for later retry. Some SMSCs also provide a "forward and forget" option where transmission is tried only once. Both mobile terminated (MT, for messages sent "to" a mobile handset) and mobile originating (MO, for those sent "from" the mobile handset) operations are supported. Message delivery is "best effort," so there are no guarantees that a message will actually be delivered to its recipient, but delay or complete loss of a message is uncommon, typically affecting less than 5 percent of messages. Some providers allow users to request delivery reports, either via the SMS settings of most modern phones, or by prefixing each message with *0# or *N#. However, the exact meaning of confirmations varies from reaching the network, to being queued for sending, to being sent, to receiving a confirmation of receipt from the target device, and users are often not informed of the specific type of success being reported.
SMS is a stateless communication protocol in which every SMS message is considered entirely independent of other messages. Enterprise applications using SMS as a communication channel for stateful dialogue (where an MO reply message is paired to a specific MT message) requires that session management be maintained external to the protocol through proprietary methods as .
Message size.
Transmission of short messages between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length is limited by the constraints of the signaling protocol to precisely 140 octets (140 octets * 8 bits / octet = 1120 bits). Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit data alphabet, and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.
Larger content (concatenated SMS, multipart or segmented SMS, or "long SMS") can be sent using multiple messages, in which case each message will start with a User Data Header (UDH) containing segmentation information. Since UDH is part of the payload, the number of available characters per segment is lower: 153 for 7-bit encoding, 134 for 8-bit encoding and 67 for 16-bit encoding. The receiving handset is then responsible for reassembling the message and presenting it to the user as one long message. While the standard theoretically permits up to 255 segments, 6 to 8 segment messages are the practical maximum, and long messages are often billed as equivalent to multiple SMS messages. Some providers have offered length-oriented pricing schemes for messages, however, the phenomenon is disappearing.
Gateway providers.
SMS gateway providers facilitate SMS traffic between businesses and mobile subscribers, including SMS for enterprises, content delivery, and entertainment services involving SMS, e.g. TV voting. Considering SMS messaging performance and cost, as well as the level of messaging services, SMS gateway providers can be classified as aggregators or SS7 providers.
The aggregator model is based on multiple agreements with mobile carriers to exchange two-way SMS traffic into and out of the operator's SMSC, also known as local termination model. Aggregators lack direct access into the SS7 protocol, which is the protocol where the SMS messages are exchanged. SMS messages are delivered to the operator's SMSC, but not the subscriber's handset; the SMSC takes care of further handling of the message through the SS7 network.
Another type of SMS gateway provider is based on SS7 connectivity to route SMS messages, also known as international termination model. The advantage of this model is the ability to route data directly through SS7, which gives the provider total control and visibility of the complete path during SMS routing. This means SMS messages can be sent directly to and from recipients without having to go through the SMSCs of other mobile operators. Therefore, it is possible to avoid delays and message losses, offering full delivery guarantees of messages and optimized routing. This model is particularly efficient when used in mission-critical messaging and SMS used in corporate communications. Moreover, these SMS gateway providers are providing branded SMS services with masking but after misuse of these gateways most countries's Governments have taken serious steps to block these gateways.
Interconnectivity with other networks.
Message Service Centers communicate with the Public Land Mobile Network (PLMN) or PSTN via Interworking and Gateway MSCs.
Subscriber-originated messages are transported from a handset to a service center, and may be destined for mobile users, subscribers on a fixed network, or Value-Added Service Providers (VASPs), also known as application-terminated. Subscriber-terminated messages are transported from the service center to the destination handset, and may originate from mobile users, from fixed network subscribers, or from other sources such as VASPs.
On some carriers nonsubscribers can send messages to a subscriber's phone using an Email-to-SMS gateway. Additionally, many carriers, including AT&T Mobility, T-Mobile USA, Sprint, and Verizon Wireless, offer the ability to do this through their respective websites.
For example, an AT&T subscriber whose phone number was 555-555-5555 would receive e-mails addressed to 5555555555@txt.att.net as text messages. Subscribers can easily reply to these SMS messages, and the SMS reply is sent back to the original email address. Sending email to SMS is free for the sender, but the recipient is subject to the standard delivery charges. Only the first 160 characters of an email message can be delivered to a phone, and only 160 characters can be sent from a phone.
Text-enabled fixed-line handsets are required to receive messages in text format. However, messages can be delivered to nonenabled phones using text-to-speech conversion.
Short messages can send binary content such as ringtones or logos, as well as Over-the-air programming (OTA) or configuration data. Such uses are a vendor-specific extension of the GSM specification and there are multiple competing standards, although Nokia's Smart Messaging is common. An alternative way for sending such binary content is EMS messaging, which is standardized and not dependent on vendors.
SMS is used for M2M (Machine to Machine) communication. For instance, there is an LED display machine controlled by SMS, and some vehicle tracking companies use SMS for their data transport or telemetry needs. SMS usage for these purposes is slowly being superseded by GPRS services owing to their lower overall cost. GPRS is offered by smaller telco players as a route of sending SMS text to reduce the cost of SMS texting internationally.
AT commands.
Many mobile and satellite transceiver units support the sending and receiving of SMS using an extended version of the Hayes command set, a specific command language originally developed for the Hayes Smartmodem 300-baud modem in 1977.
The connection between the terminal equipment and the transceiver can be realized with a serial cable (e.g., USB), a Bluetooth link, an infrared link, etc. Common AT commands include AT+CMGS (send message), AT+CMSS (send message from storage), AT+CMGL (list messages) and AT+CMGR (read message).
However, not all modern devices support receiving of messages if the message storage (for instance the device's internal memory) is not accessible using AT commands.
Premium-rated short messages.
Short messages may be used normally to provide premium rate services to subscribers of a telephone network.
Mobile-terminated short messages can be used to deliver digital content such as news alerts, financial information, logos, and ring tones. The first premium-rate media content delivered via the SMS system was the world's first paid downloadable ringing tones, as commercially launched by Saunalahti (later Jippii Group, now part of Elisa Grous), in 1998. Initially only Nokia branded phones could handle them. By 2002 the ringtone business globally had exceeded $1 billion of service revenues, and nearly $5 billion by 2008. Today, they are also used to pay smaller payments online—for example, for file-sharing services, in mobile application stores, or VIP section entrance. Outside the online world, one can buy a bus ticket or beverages from ATM, pay a parking ticket, order a store catalog or some goods (e.g., discount movie DVDs), make a donation to charity, and much more.
Premium-rated messages are also used in Donors Message Service to collect money for charities and foundations. DMS was first launched at April 1, 2004, and is very popular in the Czech Republic. For example, the Czech people sent over 1.5 million messages to help South Asia recover from the 2004 Indian Ocean Earthquake.
The Value-added service provider (VASP) providing the content submits the message to the mobile operator's SMSC(s) using an TCP/IP protocol such as the short message peer-to-peer protocol (SMPP) or the External Machine Interface (EMI). The SMSC delivers the text using the normal Mobile Terminated delivery procedure. The subscribers are charged extra for receiving this premium content; the revenue is typically divided between the mobile network operator and the VASP either through revenue share or a fixed transport fee. Submission to the SMSC is usually handled by a third party.
Mobile-originated short messages may also be used in a premium-rated manner for services such as televoting. In this case, the VASP providing the service obtains a short code from the telephone network operator, and subscribers send texts to that number. The payouts to the carriers vary by carrier; percentages paid are greatest on the lowest-priced premium SMS services. Most information providers should expect to pay about 45 percent of the cost of the premium SMS up front to the carrier. The submission of the text to the SMSC is identical to a standard MO Short Message submission, but once the text is at the SMSC, the Service Center (SC) identifies the Short Code as a premium service. The SC will then direct the content of the text message to the VASP, typically using an IP protocol such as SMPP or EMI. Subscribers are charged a premium for the sending of such messages, with the revenue typically shared between the network operator and the VASP. Short codes only work within one country, they are not international.
An alternative to inbound SMS is based on long numbers (international number format, e.g. +44 762 480 5000), which can be used in place of short codes for SMS reception in several applications, such as TV voting, product promotions and campaigns. Long numbers work internationally, allow businesses to use their own numbers, rather than short codes, which are usually shared across many brands. Additionally, long numbers are nonpremium inbound numbers.
Threaded SMS.
Threaded SMS is a visual styling orientation of SMS message history that arranges messages to and from a contact in chronological order on a single screen. It was first invented by a developer working to implement the SMS client for the BlackBerry, who was looking to make use of the blank screen left below the message on a device with a larger screen capable of displaying far more than the usual 160 characters, and was inspired by threaded Reply conversations in email. Visually, this style of representation provides a back-and-forth chat-like history for each individual contact. Hierarchical-threading at the conversation-level (as typical in blogs and on-line messaging boards)is not widely supported by SMS messaging clients. This limitation is due to the fact that there is no session identifier or subject-line passed back and forth between sent and received messages in the header data (as specified by SMS protocol) from which the client device can properly thread an incoming message to a specific dialogue, or even to a specific message within a dialogue. Most smart phone text-messaging-clients are able to create some contextual threading of "group messages" which narrows the context of the thread around the common interests shared by group members. On the other hand, advanced enterprise messaging applications which push messages from a remote server often display a dynamically changing reply number (multiple numbers used by the same sender), which is used along with the sender's phone number to create session-tracking capabilities analogous to the functionality that cookies provide for web-browsing. As one pervasive example, this technique is used to extend the functionality of many Instant Messenger (IM) applications such that they are able to communicate over two-way dialogues with the much larger SMS user-base. In cases where multiple reply numbers are used by the enterprise server to maintain the dialogue, the visual conversation threading on the client may be separated into multiple threads.
Application-to-Person (A2P) SMS.
While SMS reached its popularity as a person-to-person messaging, another type of SMS is growing fast: application-to-person (A2P) messaging. A2P is a type of SMS sent from a subscriber to an application or sent from an application to a subscriber. It is commonly used by financial institutions, airlines, hotel booking sites, social networks, and other organizations sending SMS from their systems to their customers. According to research in 2011, A2P traffic is growing faster than P2P messaging traffic.
Satellite phone networks.
All commercial satellite phone networks except ACeS and OptusSat support SMS. While early Iridium handsets only support incoming SMS, later models can also send messages. The price per message varies for different networks. Unlike some mobile phone networks, there is no extra charge for sending international SMS or to send one to a different satellite phone network. SMS can sometimes be sent from areas where the signal is too poor to make a voice call.
Satellite phone networks usually have web-based or email-based SMS portals where one can send free SMS to phones on that particular network.
Unreliability.
Unlike dedicated texting systems like the Simple Network Paging Protocol and Motorola's ReFLEX protocol, SMS message delivery is not guaranteed, and many implementations provide no mechanism through which a sender can determine whether an SMS message has been delivered in a timely manner. SMS messages are generally treated as lower-priority traffic than voice, and various studies have shown that around 1% to 5% of messages are lost entirely, even during normal operation conditions, and others may not be delivered until long after their relevance has passed. The use of SMS as an emergency notification service in particular has been starkly criticized.
Vulnerabilities.
The Global Service for Mobile communications (GSM), with the greatest worldwide number of users, succumbs to several security vulnerabilities. In the GSM, only the airway traffic between the Mobile Station (MS) and the Base Transceiver Station (BTS) is optionally encrypted with a weak and broken stream cipher (A5/1 or A5/2). The authentication is unilateral and also vulnerable. There are also many other security vulnerabilities and shortcomings. Such vulnerabilities are inherent to SMS as one of the superior and well-tried services with a global availability in the GSM networks. SMS messaging has some extra security vulnerabilities due to its store-and-forward feature, and the problem of fake SMS that can be conducted via the Internet. When a user is roaming, SMS content passes through different networks, perhaps including the Internet, and is exposed to various vulnerabilities and attacks. Another concern arises when an adversary gets access to a phone and reads the previous unprotected messages.
In October 2005, researchers from Pennsylvania State University published an analysis of vulnerabilities in SMS-capable cellular networks. The researchers speculated that attackers might exploit the open functionality of these networks to disrupt them or cause them to fail, possibly on a nationwide scale.
SMS spoofing.
The GSM industry has identified a number of potential fraud attacks on mobile operators that can be delivered via abuse of SMS messaging services. The most serious threat is SMS Spoofing, which occurs when a fraudster manipulates address information in order to impersonate a user that has roamed onto a foreign network and is submitting messages to the home network. Frequently, these messages are addressed to destinations outside the home network—with the home SMSC essentially being "hijacked" to send messages into other networks.
The only sure way of detecting and blocking spoofed messages is to screen incoming mobile-originated messages to verify that the sender is a valid subscriber and that the message is coming from a valid and correct location. This can be implemented by adding an intelligent routing function to the network that can query originating subscriber details from the HLR before the message is submitted for delivery. This kind of intelligent routing function is beyond the capabilities of legacy messaging infrastructure. While there is no way to entirely prevent spoofed messages, enterprise applications can verify origination of MO reply messages (for example, confirming a purchase made on a credit card) using the .
Limitation.
In an effort to limit telemarketers who had taken to bombarding users with hordes of unsolicited messages India introduced new regulations in September 2011, including a cap of 3,000 SMS messages per subscriber per month, or an average of 100 per subscriber per day. Due to representations received from some of the service providers and consumers, TRAI (Telecom Regulatory Authority of India) has raised this limit to 200 SMS messages per SIM per day in case of prepaid services, and up to 6,000 SMS messages per SIM per month in case of postpaid services with effect from 1 November 2011. However, it was ruled unconstitutional by the Delhi high court, but there are some limitations.
Flash SMS.
A Flash SMS is a type of SMS that appears directly on the main screen without user interaction and is not automatically stored in the inbox. It can be useful in emergencies, such as a fire alarm or cases of confidentiality, as in delivering one-time passwords.
Silent SMS.
In Germany in 2010 almost half a million "silent SMS" messages were sent by the federal police, customs and the secret service "Verfassungsschutz" (offices for protection of the constitution). These silent messages, also known as "silent TMS", "stealth SMS" or "stealth ping", are used to locate a person and thus to create a complete movement profile. They do not show up on a display, nor trigger any acoustical signal when received. Their primary purpose was to deliver special services of the network operator to any cell phone. The mobile provider, often at the behest of the police, will capture data such as subscriber identification IMSI.

</doc>
<doc id="28208" url="http://en.wikipedia.org/wiki?curid=28208" title="Santa Monica, California">
Santa Monica, California

Santa Monica is a beachfront city in western Los Angeles County, California, United States. The city is named after the Christian saint, Monica. Situated on Santa Monica Bay, it is bordered on three sides by the city of Los Angeles – Pacific Palisades to the north, Brentwood on the northeast, West Los Angeles and Mar Vista on the east, and Venice on the southeast. Santa Monica is home to many Hollywood celebrities and executives and is a mixture of affluent single-family neighborhoods, renters, surfers, professionals, and students. The Census Bureau 2010 population for Santa Monica is 89,736.
Partly because of its agreeable climate, Santa Monica had become a famed resort town by the early 20th century. The city has experienced a boom since the late 1980s through the revitalization of its downtown core and significant job growth and increased tourism.
History.
Santa Monica was long inhabited by the Tongva people. Santa Monica was called Kecheek in the Tongva language. The first non-indigenous group to set foot in the area was the party of explorer Gaspar de Portolà, who camped near the present day intersection of Barrington and Ohio Avenues on August 3, 1769. There are two different versions of the naming of the city. One says that it was named in honor of the feast day of Saint Monica (mother of Saint Augustine), but her feast day is actually May 4. Another version says that it was named by Father Juan Crespí on account of a pair of springs, the Kuruvungna Springs (Serra Springs), that were reminiscent of the tears that Saint Monica shed over her son's early impiety.
The Californios valiantly defended their territories against the Manifest Destiny expansion of the United States westward during the Mexican–American War. In Los Angeles, several battles were fought by the Californios. However in the end, the US came out victorious. Mexico signed the Treaty of Guadalupe Hidalgo, which gave Mexicans and Californios living in state certain unalienable rights. US government sovereignty in California began on February 2, 1848.
In the 1870s the Los Angeles and Independence Railroad, connected Santa Monica with Los Angeles, and a wharf out into the bay. The first town hall was a modest 1873 brick building, later a beer hall, and now part of the Santa Monica Hostel. It is Santa Monica's oldest extant structure. By 1885, the town's first hotel, was the Santa Monica Hotel.
Amusement piers became enormously popular in the first decades of the 20th century and the extensive Pacific Electric Railroad brought people to the city's beaches from across the Greater Los Angeles Area.
Around the start of the 20th century, a growing population of Asian Americans lived in or near Santa Monica and Venice. A Japanese fishing village was located near the Long Wharf while small numbers of Chinese lived or worked in both Santa Monica and Venice. The two ethnic minorities were often viewed differently by White Americans who were often well-disposed towards the Japanese but condescending towards the Chinese. The Japanese village fishermen were an integral economic part of the Santa Monica Bay community.
Donald Wills Douglas, Sr. built a plant in 1922 at Clover Field (Santa Monica Airport) for the Douglas Aircraft Company. In 1924, four Douglas-built planes took off from Clover Field to attempt the first aerial circumnavigation of the world. Two planes made it back, after having covered 27553 miles in 175 days, and were greeted on their return September 23, 1924, by a crowd of 200,000 (generously estimated). The Douglas Company (later McDonnell Douglas) kept facilities in the city until the 1960s.
The Great Depression hit Santa Monica deeply. One report gives citywide employment in 1933 of just 1,000. Hotels and office building owners went bankrupt. In the 1930s, corruption infected Santa Monica (along with neighboring Los Angeles).The federal Works Project Administration helped build several buildings in the city, most notably "City Hall". The main "Post Office" and "Barnum Hall" (Santa Monica High School auditorium) were also among several other WPA projects.
Douglas's business grew astronomically with the onset of World War II, employing as many as 44,000 people in 1943. To defend against air attack set designers from the Warner Brothers Studios prepared elaborate camouflage that disguised the factory and airfield. The RAND Corporation began as a project of the Douglas Company in 1945, and spun off into an independent think tank on May 14, 1948. RAND eventually acquired a 15-acre (61,000 m²) campus centrally located between the Civic Center and the pier entrance.
The completion of the Santa Monica Freeway in 1966 brought the promise of new prosperity, though at the cost of decimating the Pico neighborhood that had been a leading African American enclave on the Westside.
Beach volleyball is believed to have been developed by Duke Kahanamoku in Santa Monica during the 1920s.
Attractions and cultural resources.
The Santa Monica Looff Hippodrome (carousel) is a National Historic Landmark. It sits on the Santa Monica Pier, which was built in 1909. The La Monica Ballroom on the pier was once the largest ballroom in the US and the source for many New Year's Eve national network broadcasts.
The Santa Monica Civic Auditorium was an important music venue for several decades and hosted the Academy Awards in the 1960s. McCabe's Guitar Shop is still a leading acoustic performance space as well as retail outlet. Bergamot Station is a city-owned art gallery compound that includes the Santa Monica Museum of Art. The city is also home to the California Heritage Museum and the Angels Attic dollhouse and toy museum.
Santa Monica has three main shopping districts, Montana Avenue on the north side of the city, the Downtown District in the city's core, and Main Street on the south end of the city. Each of these districts has its own unique feel and personality. Montana Avenue is a stretch of luxury boutique stores, restaurants, and small offices that generally features more upscale shopping. The Main Street district offers an eclectic mix of clothing, restaurants, and other specialty retail.
The Downtown District is the home of the Third Street Promenade, a major outdoor pedestrian-only shopping district that stretches for three blocks between Wilshire Blvd. and Broadway (not the same Broadway in downtown and south Los Angeles).
Third Street is closed to vehicles for those three blocks to allow people to stroll, congregate, shop and enjoy street performers.
Santa Monica Place, featuring Bloomingdale's and Nordstrom in a three-level outdoor environment, is located at the south end of the Promenade. After a period of redevelopment, the mall reopened in the fall of 2010 as a modern shopping-entertainment complex with more outdoor space.
Santa Monica hosts the annual Santa Monica Film Festival.
The oldest movie theater in the city is the Majestic. Also known as the Mayfair Theatre, the theater which opened in 1912 has been closed since the 1994 Northridge earthquake. The Aero Theater (now operated by the American Cinematheque) and Criterion Theater were built in the 1930s and still show movies. The Santa Monica Promenade alone supports more than a dozen movie screens.
Palisades Park stretches out along the crumbling bluffs overlooking the Pacific and is a favorite walking area to view the ocean. It includes a totem pole, camera obscura, artwork, benches, picnic areas, pétanque courts, and restrooms.
Tongva Park occupies 6 acres between Ocean Avenue and Main Street, just south of Colorado Avenue. The park includes an overlook, amphitheater, playground, garden, fountains, picnic areas, and restrooms.
The Santa Monica Stairs, a long, steep staircase that leads from north of San Vicente down into Santa Monica Canyon, is a popular spot for all-natural outdoor workouts. Some area residents have complained that the stairs have become too popular, and attract too many exercisers to the wealthy neighborhood of multimillion-dollar properties.
Natives and tourists alike have enjoyed the Santa Monica Rugby Club since 1972. The club has been very successful since its conception, most recently winning back-to-back national championships in 2005 and 2006. Santa Monica defeated the Boston Irish Wolfhounds 57-19 in the Division 1 final, convincingly claiming its second consecutive American title on June 4, 2006, in San Diego. They offer Men's, Women's and a thriving children's programs. The club recently joined the Rugby Super League.
Every fall the Santa Monica Chamber of Commerce hosts "The Taste of Santa Monica" on the Santa Monica Pier. Visitors can sample food and drinks from Santa Monica restaurants. Other annual events include the Business and Consumer Expo, Sustainable Quality Awards, Santa Monica Cares Health and Wellness Festival, and the State of the City. The swanky Shutters on the Beach Hotel offers a trip to the famous Santa Monica Farmers Market to select and influence the materials that will become that evening's special "Market Dinner."
Santa Monica is a mecca for skateboarding culture.
Santa Monica has two hospitals: Saint John's Health Center and Santa Monica-UCLA Medical Center. Its cemetery is Woodlawn Memorial.
Santa Monica has several newspapers and magazines, including the "Santa Monica Star", "Santa Monica Daily Press", the "Santa Monica Mirror", the "Santa Monica Observer", "Santa Monica Magazine", and the "Santa Monica Sun".
Geography.
The city of Santa Monica rests on a mostly flat slope that angles down towards Ocean Avenue and towards the south. High bluffs separate the north side of the city from the beaches.
Climate.
Classified as a moderate Mediterranean climate (Köppen Csb), Santa Monica enjoys an average of 310 days of sunshine a year. It is located in USDA plant hardiness zone 11a. Because of its location, nestled on the vast and open Santa Monica Bay, morning fog is a common phenomenon in May, June and early July (caused by ocean temperature variations and currents). Like other inhabitants of the greater Los Angeles area, residents have a particular terminology for this phenomenon: the "May Gray" and the "June Gloom". Overcast skies are common for June mornings, but usually the strong sun burns the fog off by noon. In the late winter/early summer, daily fog is a phenomenon too. It happens suddenly and it may last some hours or past sunset time. Nonetheless, it will sometimes stay cloudy and cool all day during June, even as other parts of the Los Angeles area enjoy sunny skies and warmer temperatures. At times, the sun can be shining east of 20th Street, while the beach area is overcast. As a general rule, the beach temperature is from 5 to 10 degrees Fahrenheit (3 to 6 degrees Celsius) cooler than it is inland during summer days, and 5-10 degrees warmer during winter nights.
It is also in September that highest temperatures tend to be reached. It is winter, however, when the hot, dry winds of the Santa Anas are most common. In contrast, temperatures exceeding 10 degrees below average are rare.
The rainy season is from late October through late March. Winter storms usually approach from the northwest and pass quickly through the Southland. There is very little rain during the rest of the year. Yearly rainfall totals are unpredictable as rainy years are occasionally followed by droughts. There has never been any snow or frost, but there has been hail.
Santa Monica usually enjoys cool breezes blowing in from the ocean, which tend to keep the air fresh and clean. Therefore, smog is less of a problem for Santa Monica than elsewhere around Los Angeles. However, in the autumn months of September through November, the Santa Ana winds will sometimes blow from the east, bringing smoggy inland air to the beaches.
Environment.
Santa Monica is one of the most environmentally activist municipalities in the nation. The city proposed a Sustainable City Plan in 1992 and in 1994, was one of the first cities in the nation to adopt a comprehensive sustainability plan, setting waste reduction and water conservation policies for both public and private sector through its Office of Sustainability and the Environment. Eighty-two percent of the city's public works vehicles now run on alternative fuels, including nearly 100% of the municipal bus system, making it among the largest such fleets in the country.
Santa Monica has adopted a Community Energy Independence Initiative, with a goal of achieving complete energy independence by 2020 (vs. California's already ambitious 33% renewables goal). In the last 15 years, greenhouse gas emissions have been cut city-wide by nearly 10% relative to 1990 levels, with further reductions being planned by the Office of Sustainability.
An urban runoff facility (SMURFF), the first of its kind in the US, catches and treats 3.5 e6USgal of water each week that would otherwise flow into the bay via storm-drains and sells it back to end-users within the city for reuse as gray-water, while bio-swales throughout the city allow rainwater to percolate into and replenish the groundwater supply. The groundwater supply will play an increasingly central role in the city's Sustainable Water Master Plan, whereby Santa Monica has set a goal of attaining 100% water independence by 2020. The city has numerous programs designed to promote water conservation among residents, including a rebate of $1.50 per square foot for those who convert water intensive lawns to more local drought-tolerant gardens that require less water.
Santa Monica also has a green building-code whereby constructing to code implies the equivalent of the US Green Building Council's LEED Silver certification. The city's LEED certified Main Library, for example, is built over a 200,000 gallon cistern that collects filtered storm water from the roof. The water is used for landscape irrigation.
Since 2009, Santa Monica has been developing the Zero Waste Strategic Operations Plan by which the city will set a goal of diverting at least 95% of all waste away from landfills, and toward recycling and composting, by 2030. The plan includes a food waste composting program, which diverts 3 million pounds of restaurant food waste away from landfills annually. Currently, 77% of all solid waste produced city-wide is diverted from landfills.
The city is also in the process of implementing a 5-year and 20 year Bike Action Plan with a goal of attaining 14 to 35% bicycle mode share by 2030 through the installation of enhanced bicycle infrastructure throughout the city. Other environmental features include curbside recycling, curbside composting bins (in addition to trash, yard-waste, and recycle bins), farmers' markets, community gardens, garden-share, an urban forest initiative, a hazardous materials home-collection service, green business certification, and a municipal bus system which is currently being revamped to integrate with the soon-to-open Expo Line.
Cityscape.
Santa Monica beach and pier viewed from the end of Santa Monica Pier. Note that the bluff is highest at the north end, to the left of the image
Santa Monica
Demographics.
Santa Monica's population has grown from 417 in 1880 to 89,736 in 2010.
2010.
The 2010 United States Census reported that Santa Monica had a population of 89,736. The population density was 10,662.6 people per square mile (4,116.9/km²). The racial makeup of Santa Monica was 69,663 (77.6%) White (70.1% Non-Hispanic White), 3,526 (3.9%) African American, 338 (0.4%) Native American, 8,053 (9.0%) Asian, 124 (0.1%) Pacific Islander, 4,047 (4.5%) from other races, and 3,985 (4.4%) from two or more races. Hispanic or Latino of any race were 11,716 persons (13.1%).
The Census reported that 87,610 people (97.6% of the population) lived in households, 1,299 (1.4%) lived in non-institutionalized group quarters, and 827 (0.9%) were institutionalized.
There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were opposite-sex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. There were 2,867 (6.1%) unmarried opposite-sex partnerships, and 416 (0.9%) same-sex married couples or partnerships. 22,716 households (48.4%) were made up of individuals and 5,551 (11.8%) had someone living alone who was 65 years of age or older. The average household size was 1.87. There were 17,929 families (38.2% of all households); the average family size was 2.79.
The population was spread out with 12,580 people (14.0%) under the age of 18, 6,442 people (7.2%) aged 18 to 24, 32,552 people (36.3%) aged 25 to 44, 24,746 people (27.6%) aged 45 to 64, and 13,416 people (15.0%) who were 65 years of age or older. The median age was 40.4 years. For every 100 females there were 93.2 males. For every 100 females age 18 and over, there were 91.2 males.
There were 50,912 housing units at an average density of 6,049.5 per square mile (2,335.7/km²), of which 13,315 (28.4%) were owner-occupied, and 33,602 (71.6%) were occupied by renters. The homeowner vacancy rate was 1.1%; the rental vacancy rate was 5.1%. 30,067 people (33.5% of the population) lived in owner-occupied housing units and 57,543 people (64.1%) lived in rental housing units.
According to the 2010 United States Census, Santa Monica had a median household income of $72,271, with 11.3% of the population living below the federal poverty line.
2000.
As of the census of 2000, there are 84,084 people, 44,497 households, and 16,775 families in the city. The population density is 10,178.7 inhabitants per square mile (3,930.4/km²). There are 47,863 housing units at an average density of 5,794.0 per square mile (2,237.3/km²). The racial makeup of the city is 78.29% White, 7.25% Asian, 3.78% African American, 0.47% Native American, 0.10% Pacific Islander, 5.97% from other races, and 4.13% from two or more races. 13.44% of the population are Hispanic or Latino of any race.
There are 44,497 households, out of which 15.8% have children under the age of 18, 27.5% are married couples living together, 7.5% have a female householder with no husband present, and 62.3% are non-families. 51.2% of all households are made up of individuals and 10.6% have someone living alone who is 65 years of age or older. The average household size is 1.83 and the average family size is 2.80.
The city of Santa Monica is consistently among the most educated cities in the United States, with 23.8 percent of all residents holding graduate degrees.
The population is diverse in age, with 14.6% under 18, 6.1% from 18 to 24, 40.1% from 25 to 44, 24.8% from 45 to 64, and 14.4% 65 years or older. The median age is 39 years. For every 100 females, there are 93.0 males. For every 100 females age 18 and over, there are 91.3 males.
According to a 2009 estimate, the median income for a household in the city is $71,095, and the median income for a family is $109,410. Males have a median income of $55,689 versus $42,948 for females. The per capita income for the city is $42,874. 10.4% of the population and 5.4% of families are below the poverty line. Out of the total population, 9.9% of those under the age of 18 and 10.2% of those 65 and older are living below the poverty line.
Education.
Elementary and secondary schools.
The Santa Monica-Malibu Unified School District provides public education at the elementary and secondary levels. In addition to the traditional model of early education school houses, SMASH (Santa Monica Alternative School House) is "a K-8 public school of choice with team teachers and multi-aged classrooms".
Elementary schools.
The district maintains eight public elementary schools in Santa Monica:
Middle schools.
The district maintains two public middle schools in Santa Monica: John Adams Middle School and Lincoln Middle School.
High schools.
The district maintains two high schools in Santa Monica: Olympic High School and Santa Monica High School.
Private schools.
Private schools in the city include:
Miscellaneous education.
Asahi Gakuen, a weekend Japanese supplementary school system, operates its Santa Monica campus (サンタモニカ校･高等部 "Santamonika-kō kōtōbu") at Webster Middle in the Sawtelle neighborhood of Los Angeles. All high school classes in the Asahi Gakuen system are held at the Santa Monica campus. As of 1986 students take buses from as far away as Orange County to go to the high school classes of the Santa Monica campus.
Post-secondary.
Santa Monica College is a junior college originally founded in 1929. Many SMC graduates transfer to the University of California system. It occupies 35 acres (14 hectares) and enrolls 30,000 students annually. The Frederick S. Pardee RAND Graduate School, associated with the RAND Corporation, is the U.S.'s largest producer of public policy PhDs. The Art Institute of California – Los Angeles is also located in Santa Monica near the Santa Monica Airport. L.A. Leadership College, an online institution, for underprivileged young adults, is located in Santa Monica.
Universities and colleges within a 22 mi radius from Santa Monica include Santa Monica College, Antioch University Los Angeles, Loyola Marymount University, Mount St. Mary's College, Pepperdine University, California State University, Northridge, California State University, Los Angeles, UCLA, USC, West Los Angeles College, California Institute of Technology (Caltech), Occidental College (Oxy), Los Angeles City College, Los Angeles Southwest College, Los Angeles Valley College, and Emperor's College of Traditional Oriental Medicine.
Public library system.
The Santa Monica Public Library consists of a Main Library in the downtown area, plus four neighborhood branches: Fairview, Montana Avenue, Ocean Park, and Pico Boulevard.
Transportation.
Bicycles.
Santa Monica has a bike action plan and expects to launch a Bicycle sharing system in 2015. The city is traversed by the Marvin Braude Bike Trail. Santa Monica has received the Bicycle Friendly Community Award (Bronze in 2009, Silver in 2013) by the League of American Bicyclists. Local bicycle advocacy organizations include Santa Monica Spoke, a local chapter of the Los Angeles County Bicycle Coalition. Santa Monica is thought to be one of the leaders for bicycle infrastructure and programming in Los Angeles County.
In terms of number of bicycle accidents, Santa Monica ranks as one of the worst (#2) out of 102 California cities with population 50,000-100,000, a ranking that is consistent with the composite ranking for the city.
In 2007 and 2008, local police cracked down on Santa Monica Critical Mass rides that had become controversial, putting a damper on the tradition.
Motorized vehicles.
The Santa Monica Freeway (Interstate 10) begins in Santa Monica near the Pacific Ocean and heads east. The Santa Monica Freeway between Santa Monica and downtown Los Angeles has the distinction of being one of the busiest highways in all of North America. After traversing Los Angeles County, I-10 crosses seven more states, terminating at Jacksonville, Florida. In Santa Monica, there is a road sign designating this route as the Christopher Columbus Transcontinental Highway. State Route 2 (Santa Monica Boulevard) begins in Santa Monica, barely grazing State Route 1 at Lincoln Boulevard, and continues northeast across Los Angeles County, through the Angeles National Forest, crossing the San Gabriel Mountains as the Angeles Crest Highway, ending in Wrightwood. Santa Monica is also the western (Pacific) terminus of historic U.S. Route 66. Close to the eastern boundary of Santa Monica, Sepulveda Boulevard reaches from Long Beach at the south, to the northern end of the San Fernando Valley. Just east of Santa Monica is Interstate 405, the "San Diego Freeway", a major north-south route in Los Angeles County and Orange County, California.
The City of Santa Monica has purchased the first ZeroTruck all-electric medium-duty truck. The vehicle will be equipped with a Scelzi utility body, it is based on the Isuzu N series chassis, a UQM PowerPhase 100 advanced electric motor and is the only US built electric truck offered for sale in the United States in 2009.
Bus.
The city of Santa Monica runs its own bus service, the Big Blue Bus, which also serves much of West Los Angeles and the University of California, Los Angeles (UCLA). A Big Blue Bus was featured prominently in the action movie "Speed".
The city of Santa Monica is also served by the Los Angeles County Metropolitan Transportation Authority's bus lines. Metro also complements Big Blue service, as when Big Blue routes are not operational overnight, Metro buses make many Big Blue Bus stops, in addition to MTA stops.
Light rail.
Design and construction on the 6.6 mile of the Expo Line from Culver City to Santa Monica started in September 2011 with service scheduled to begin in 2016. Santa Monica stations include 26th Street/Bergamot, 17th Street/Santa Monica College, and Downtown Santa Monica.
Historical aspects of the Expo line route are noteworthy. It uses the right-of-way for the Santa Monica Air Line that provided electric-powered freight and passenger service between Los Angeles and Santa Monica beginning in the 1920s. Service was discontinued in 1953 but diesel-powered freight deliveries to warehouses along the route continued until March 11, 1988. The abandonment of the line spurred concerns within the community and the entire right-of-way was purchased from Southern Pacific by Los Angeles Metropolitan Transportation Authority. The line was built in 1875 as the steam-powered Los Angeles and Independence Railroad to bring mining ore to ships in Santa Monica harbor and as a passenger excursion train to the beach.
Subway.
Since the mid-1980s, various proposals have been made to extend the Purple Line subway to Santa Monica under Wilshire Boulevard. There are no current plans to complete the "subway to the sea," an estimated $5 billion project.
Airport and ports.
The city owns and operates a general aviation airport, Santa Monica Airport, which has been the site of several important aviation achievements. Commercial flights are available for residents at Los Angeles International Airport, a few miles south of Santa Monica.
Like other cities in Los Angeles County, Santa Monica is dependent upon the Port of Long Beach and the Port of Los Angeles for international ship cargo. In the 1890s, Santa Monica was once in competition with Wilmington, California, and San Pedro for recognition as the "Port of Los Angeles" (see History of Santa Monica, California).
Emergency services.
Two major hospitals are within the Santa Monica city limits, UCLA Santa Monica Hospital and St. John's Hospital. There are four fire stations providing medical and fire response within the city staffed with 6 Paramedic Engines, 1 Truck company, 1 Hazardous Materials team and 1 Urban Search & Rescue team. Santa Monica Fire Department has its own Dispatch Center. Ambulance transportation is provided by AmeriCare Ambulance Services.
The Los Angeles County Department of Health Services operates the Simms/Mann Health and Wellness Center in Santa Monica. The Department's West Area Health Office is in the Simms/Mann Center.
Government.
Santa Monica is governed by the Santa Monica City Council, a Council-Manager governing body with seven members elected at-large. The current mayor is Kevin McKeown.
In the California State Legislature, Santa Monica is in the 26 Senate District, represented by Democrat Ben Allen, and in the 50 Assembly District, represented by Democrat Richard Bloom.
In the United States House of Representatives, Santa Monica is in California's 33 congressional district, represented by Democrat Ted Lieu.
Economy.
Santa Monica is home to the headquarters of many notable businesses, including Universal Music Group, Lionsgate Films, the RAND Corporation, Beachbody, Macerich, Supermarine, now Atlantic Aviation, is at the Santa Monica Airport. National Public Radio member station KCRW is located at the Santa Monica College campus.
A number of game development studios are based in Santa Monica, making it a major location for the industry. These include:
Fatburger's headquarters are in Santa Monica. TOMS Shoes has its headquarters in Santa Monica.
Former Santa Monica businesses include Douglas Aircraft (now merged with Boeing), MySpace (now headquartered in Beverly Hills), and Metro-Goldwyn-Mayer. In December 1996, GeoCities was headquartered on the third floor of 1918 Main Street in Santa Monica.
Santa Monica is part of the Los Angeles area called Silicon Beach, and serves as the home of hundreds of startup companies.
Top employers.
According to the City's 2012-2013 Comprehensive Annual Financial Report, the top employers in the city are:
Crime.
In 2006, crime in Santa Monica affected 4.41% of the population, slightly lower than the national average crime rate that year of 4.48%.
The majority of this was property crime,
which affected 3.74% of Santa Monica's population in 2006;
this was higher than
the rates for Los Angeles County (2.76%)
and California (3.17%),
but lower than the national average (3.91%).
These per-capita crime rates are computed based on Santa Monica's full-time population of about 85,000.
However,
the Santa Monica Police Department has suggested the actual per-capita crime rate is much lower,
as tourists, workers, and beachgoers can increase the city's daytime population to between 250,000 and
450,000 people.
Violent crimes affected 0.67% of the population in Santa Monica in 2006,
in line with Los Angeles County (0.65%),
but higher than the averages for California (0.53%)
and the nation (0.55%).
Hate crime has typically been minimal in Santa Monica, with only one reported incident in 2007.
However, the city experienced a spike of anti-Islamic hate crime in 2001, following the attacks of September 11.
Hate crime levels returned to their minimal 2000 levels by 2002.
In 2006, Santa Monica voters passed "Measure Y" with a 65% majority,
which moved the issuance of citations for marijuana smoking to the bottom of the police priority list. A 2009 study by the Santa Monica Daily Press showed that since the law took effect in 2007, the Santa Monica Police had
"not issued any citations for offenses involving the adult, personal use of marijuana inside private residences."
In June 2011, the infamous Boston gangster Whitey Bulger was arrested in Santa Monica after being a fugitive for 16 years. He had been living in the area for 15 years.
A shooting in Santa Monica in 2013 left six (including the perpetrator) dead and five more injured.
Gang activity.
The Pico neighborhood of Santa Monica (south of the Santa Monica Freeway) experiences some gang activity. The city estimates that there are about 50 gang members based in Santa Monica, although some community organizers dispute this claim. Gang activity has been prevalent for decades in the Pico neighborhood.
In October 1998, alleged Culver City 13 gang member Omar Sevilla, 21, of Culver City was killed. A couple of hours after the shooting of Sevilla, German tourist Horst Fietze was killed. Several days later Juan Martin Campos, age 23, a Santa Monica city employee, was shot and killed. Police believe this was a retaliatory killing in response to the death of Omar Sevilla. Less than twenty-four hours later, Javier Cruz was wounded in a drive-by shooting outside his home on 17th and Michigan.
In 1999, there was a double homicide in the Westside Clothing store on Lincoln Boulevard. During the incident, Culver City gang members David "Puppet" Robles and Jesse "Psycho" Garcia entered the store masked and began opening fire, killing Anthony and Michael Juarez. They then ran outside to a getaway vehicle driven by a third Culver City gang member, who is now also in custody. The clothing store was believed to be a local hang out for Santa Monica gang members. The dead included two men from Northern California who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area. Police say the incident was in retaliation for a shooting committed by the Santa Monica 13 gang days before the Juarez brothers were gunned down.
Aside from the rivalry with the Culver City gang, gang members also feud with the Venice and West Los Angeles gangs. The main rivals in these regions include Venice 13, Graveyard Gangster Crips, and Venice Shoreline Crips gangs located in the Oakwood area of Venice, California.
Sport.
The men's and women's marathon ran through parts of Santa Monica during the 1984 Summer Olympics. The Santa Monica Track Club has many prominent track athletes, including many Olympic gold medalists. Santa Monica is also home to the Santa Monica Rugby Club, a semi-professional team that competes in the Pacific Rugby Premiership, the highest-level rugby club competition in the United States.
In popular culture.
Film and television.
Hundreds of movies have been shot or set in part within the city of Santa Monica. One of the oldest exterior shots in Santa Monica is Buster Keaton's "Spite Marriage" (1929) which shows much of 2nd Street. The comedy "It's a Mad, Mad, Mad, Mad World" (1963) included several scenes shot in Santa Monica, including those along the California Incline, which led to the movie's treasure spot, "The Big W". The Sylvester Stallone film "Rocky III" (1982) shows Rocky Balboa and Apollo Creed training to fight Clubber Lang by running on the Santa Monica Beach, and Stallone's "Demolition Man" (1993) includes Santa Monica settings. Henry Jaglom's indie "Someone to Love" (1987), the last film in which Orson Welles appeared, takes place in Santa Monica's venerable Mayfair Theatre. "Heathers" (1989) used Santa Monica's John Adams Middle School for many exterior shots. "The Truth About Cats & Dogs" (1996) is set entirely in Santa Monica, particularly the Palisades Park area, and features a radio station that resembles KCRW at Santa Monica College. "17 Again" (2009) was shot at Samohi. Other films that show significant exterior shots Santa Monica include "Fletch" (1985), "Species" (1995),"Get Shorty" (1995), and "Ocean's Eleven" (2001). Richard Rossi's biopic "Aimee Semple McPherson" opens and closes at the beach in Santa Monica. Iron Man (I) features the Santa Monica pier and surrounding communities as Tony Stark tests his experimental flight suit.
The documentary "Dogtown and Z-Boys" (2001) and the related dramatic film "Lords of Dogtown" (2005) are both about the influential skateboarding culture of Santa Monica's Ocean Park neighborhood in the 1970s.
The Santa Monica Pier is shown in many movies, including "They Shoot Horses, Don't They?" (1969), "The Sting" (1973), "Ruthless People" (1986), "Beverly Hills Cop III" (1994), "Clean Slate" (1994), "Forrest Gump" (1994), "The Net" (1995), "Love Stinks" (1999), "Cellular" (2004), "Iron Man" (2008) and "" (2009).
A number of television series have been set in Santa Monica, including "Baywatch", "Three's Company", "Pacific Blue", and "Private Practice". The Santa Monica pier is shown in the main theme of CBS series "". In "Buffy the Vampire Slayer", the main exterior set of the town of Sunnydale, including the infamous "sun sign", was located in Santa Monica in a lot on Olympic Boulevard.
The film "The Doors" (1991) and "Speed" (1994) featured vehicles from Santa Monica's Big Blue Bus line, relative to the eras depicted in the films.
The city of Santa Monica (and in particular the Santa Monica Airport) was featured in Roland Emmerich's disaster film "2012" (2009). A magnitude 10.9 earthquake destroys the airport and the surrounding area as a group of survivors escape in a personal plane. The Santa Monica Pier and the whole city sinks into the Pacific Ocean after the earthquake.
Literature.
Raymond Chandler's most famous character, private detective Philip Marlowe, frequently has a portion of his adventures in a place called "Bay City", which is modeled on depression-era Santa Monica. In Marlowe's world, Bay City is "a wide-open town", where gambling and other crimes thrive due to a massively corrupt and ineffective police force.
The setting on a certain portion of Mitch Albom's book, The Five People You Meet in Heaven, has similarities to the Pacific Pier located along the Santa Monica beach. In the book, it is named Ruby Pier. Mitch Albom even acknowledged the Pacific Pier for its cooperation.
The main character from Edgar Rice Burroughs' The Land That Time Forgot (novel) was a shipbuilder from Santa Monica.
In "Al Capone Does My Shirts", the Flanagans move to Alcatraz from Santa Monica.
Tennessee Williams lived (while working at MGM Studios) in a hotel on Ocean Avenue in the 1940s. At that location he wrote The Glass Menagerie. His short story titled "" was set near Santa Monica Beach, and mentions the clock visible in much of the city, high up on The Broadway Building, on Broadway near 2nd Street.
Also featured in Rick Riordains "Percy Jackson" Novels, specifically the Santa Monica pier.
Video games.
Santa Monica is featured in the video games
' (2003), ' (2004), "Grand Theft Auto San Andreas" (2004), " Destroy All Humans! " (2004), "Tony Hawk's American Wasteland" (2005), "" (2008), "Cars Race-O-Rama" (2009), "Grand Theft Auto V" (2013) as a fictional district - Del Perro
and "" (2013) as a fictional U.S. military base - Fort Santa Monica

</doc>
<doc id="28209" url="http://en.wikipedia.org/wiki?curid=28209" title="Shot put">
Shot put

The shot put (pronounced ) is a track and field event involving "throwing"/"putting" (throwing in a pushing motion) a heavy spherical object —the "shot"—as far as possible. The shot put competition for men has been a part of the modern Olympics since their revival in 1896, and women's competition began in 1948.
History.
Homer makes mention of competitions of rock throwing by soldiers during the Siege of Troy but there is no record of any dead weights being thrown in Greek competitions. The first evidence for stone- or weight-throwing events were in the Scottish Highlands, and date back to approximately the first century. In the 16th century King Henry VIII was noted for his prowess in court competitions of weight and hammer throwing. 
The first events resembling the modern shot put likely occurred in the Middle Ages when soldiers held competitions in which they hurled cannonballs. Shot put competitions were first recorded in early 19th century Scotland, and were a part of the British Amateur Championships beginning in 1866.
Competitors take their throw from inside a marked circle 2.135 m in diameter, with a stopboard about 10 cm high at the front of the circle. The distance thrown is measured from the inside of the circumference of the circle to the nearest mark made in the ground by the falling shot, with distances rounded down to the nearest centimetre under IAAF and WMA rules.
Legal throws.
The following rules are adhered to for a legal throw:
The athlete may enter the ring wherever they chose.
Foul throws occur when an athlete:
At any time if the shot loses contact with the neck then it is technically an illegal throw.
Regulation Misconceptions.
The following are either obsolete or non-existent but commonly believed rules within professional competition:
Competition.
Shot put competitions have been held at the modern Summer Olympic Games since their inception in 1896, and it is also included as an event in the World Athletics Championships.
Each competition has a set number of rounds of throws. Typically there are three preliminary rounds to determine qualification for the final, and then three more rounds in the final. Each competitor is credited with their longest throw, regardless of whether it was achieved in the preliminary or final rounds. The competitor with the longest legal put is declared the winner.
In open competitions the men's shot weighs 7.260 kg, and the women's shot weighs 4 kg. Junior, school, and masters competitions often use different weights of shots, typically below the weights of those used in open competitions; the individual rules for each competition should be consulted in order to determine the correct weights to be used.
Putting styles.
Two putting styles are in current general use by shot put competitors: the "glide" and the "spin". With all putting styles, the goal is to release the shot with maximum forward velocity at an angle of approximately forty degrees.
Glide.
The origin of the glide dates to 1951, when Parry O'Brien from the United States invented a technique that involved the putter facing backwards, rotating 180 degrees across the circle, and then tossing the shot.
With this technique, a right-hand thrower would begin facing the rear of the circle, and then kick to the front with the left leg, while pushing off forcefully with the right. As the thrower crosses the circle, the hips twist toward the front, the left arm is swung out then pulled back tight, followed by the shoulders, and they then strike in a putting motion with their right arm. The key is to move quickly across the circle with as little air under the feet as possible, hence the name 'glide'.
Spin.
In 1972 Aleksandr Baryshnikov set his first USSR record using a new putting style, the spin ("круговой мах" in Russian), invented by his coach Viktor Alexeyev. The spin involves rotating like a discus thrower and using rotational momentum for power. In 1976 Baryshnikov went on to set a world record of 22.00 m with his spin style, and was the first shot putter to cross the 22 meter mark.
With this technique, a right-hand thrower faces the rear, and begins to spin on the ball of the left foot. The thrower comes around and faces the front of the circle and drives the right foot into the middle of the circle. Finally, the thrower reaches for the front of the circle with the left foot, twisting the hips and shoulders like in the glide, and puts the shot.
When the athlete executes the spin, the upper body is twisted hard to the right, so the imaginary lines created by the shoulders and hips are no longer parallel. This action builds up torque, and stretches the muscles, creating an involuntary elasticity in the muscles, providing extra power and momentum. When the athlete prepares to release, the left foot is firmly planted, causing the momentum and energy generated to be conserved, pushing the shot in an upward and outward direction.
Another purpose of the spin is to build up a high rotational speed, by swinging the right leg initially, then to bring all the limbs in tightly, similar to a figure skater bringing in their arms while spinning to increase their speed. Once this fast speed is achieved the shot is released, transferring the energy into the shot put.
Usage.
Currently, most top male shot putters use the spin. However the glide remains popular, especially among Olympic and World Champions and among women, since the technique leads to greater consistency compared to the rotational technique. Almost all throwers start by using the glide. Tomasz Majewski notes that although most athletes use the spin, he and some other top shot putters achieved success using this classic method (for example he became first to defend the Olympic title in 56 years).
The world record by a male putter of 23.120 m by Randy Barnes was completed with the spin technique, while the second-best all-time put of 23.063 m by Ulf Timmermann was completed with the glide technique.
Measuring which technique can provide the most potential is difficult, as many of the best throws recorded with each technique have been completed by athletes under doping suspicions, or with a record of drug violations. The decision to glide or spin may need to be decided on an individual basis, determined by the thrower's size and power. Short throwers may benefit from the spin and taller throwers may benefit from the glide, but many throwers do not follow this guideline.
Types of shots.
The shot put ball is made of different kinds of materials depending on its intended use. Materials used include sand, iron, cast iron, solid steel, stainless steel, brass, and synthetic materials like polyvinyl. Some metals are more dense than others making the size of the shot vary, for example, indoor shots are larger than outdoor shots, so different materials are used to make them. There are various size and weight standards for the implement that depend on the age and gender of the competitors as well as the national customs of the governing body.
World records.
The current world record holders are:
Continental records.
The current records held on each continent are:

</doc>
<doc id="28211" url="http://en.wikipedia.org/wiki?curid=28211" title="Stan Kelly-Bootle">
Stan Kelly-Bootle

Stanley Bootle, known as Stan Kelly-Bootle (15 September 1929  – 16 April 2014) was a British author, singer-songwriter and computer scientist. His best-known song is the "Liverpool Lullaby", which Judy Collins recorded in 1966 for her album, "In My Life". Cilla Black recorded it three years later as the B-side to her pop hit "Conversations". Kelly-Bootle achieved the first postgraduate diploma in computer science in 1954.
Education.
Born in Liverpool, Lancashire, Stan Kelly-Bootle was schooled at the Liverpool Institute. He spent 1948–1950 as a conscript in the British Army, achieving the rank of Sgt. Instructor in RADAR. He attended Downing College, Cambridge, graduating with a first class degree in Numerical Analysis and Automatic Computing in 1954, the first postgraduate degree in computer science.
Folk singing career.
In 1950, Kelly-Bootle helped found the St. Lawrence Folk Song Society at Cambridge University. As a folk singer-songwriter, he performed under the name Stan Kelly. He wrote some of his own tunes and also wrote lyrics set to traditional tunes. In the course of his musical career, he made over 200 radio and television appearances, and released several recordings, as well as having his songs recorded by others.
Discography.
Solo releases include:
Other audio recordings include:
Computing career.
He started his computing career programming the pioneering EDSAC computer, designed and built at Cambridge University. He worked for IBM in the United States and the UK from 1955 to 1970. From 1970 to 1973, he worked as Manager for University Systems for Sperry-UNIVAC. He also lectured at the University of Warwick.
Writing career.
In 1973, Kelly-Bootle left Sperry-UNIVAC and became a freelance consultant, writer and programmer. He was known in the computer community for "The Devil's DP Dictionary" and its second edition, "The Computer Contradictionary" (1995), which he authored. These works are cynical lexicographies in the vein of Ambrose Bierce's "The Devil's Dictionary". Kelly-Bootle authored or co-authored several serious textbooks and tutorials on subjects such as the Motorola 68000 family of CPUs, programming languages including various C compilers, and the Unix operating system. He authored the "Devil's Advocate" column in "UNIX Review" from 1984-2000, and had columns in "Computer Language" ("Bit by Bit", 1989-1994), "OS/2 Magazine" ("End Notes", 1994–97) and "Software Development" ("Seamless Quanta", October 1995–May 1997). He contributed columns and articles to several other computer industry magazines, as well.
Kelly-Bootle's articles for magazines such as "ACM Queue", "AI/Expert", and "UNIX Review" contain stunning examples of word-play, criticism of silly marketing and usage (he refers often to the computer "laxicon") and commentary on the industry in general. He wrote an online monthly column posted on the Internet. While most of his writing was oriented towards the computer industry, he wrote a few books relating to his other interests, including 
Death.
Stan Kelly-Bootle died on 16 April 2014, aged 84, in hospital in Oswestry, Shropshire from undisclosed causes.

</doc>
<doc id="28212" url="http://en.wikipedia.org/wiki?curid=28212" title="Skewness">
Skewness

In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or even undefined.
The qualitative interpretation of the skew is complicated. For a unimodal distribution, negative skew indicates that the "tail" on the left side of the probability density function is longer or fatter than the right side – it does not distinguish these shapes. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value indicates that the tails on both sides of the mean balance out, which is the case both for a symmetric distribution, and for asymmetric distributions where the asymmetries even out, such as one tail being long but thin, and the other being short but fat. Further, in multimodal distributions and discrete distributions, skewness is also difficult to interpret. Importantly, the skewness does not determine the relationship of mean and median.
Introduction.
Consider the two distributions in the figure just below. Within each graph, the bars on the right side of the distribution taper differently than the bars on the left side. These tapering sides are called "tails", and they provide a visual means for determining which of the two kinds of skewness a distribution has:
Skewness in a data series may be observed not only graphically but by simple inspection of the values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of (50). We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, as in e.g. (40, 49, 50, 51). Similarly, we can make the sequence positively skewed by adding a value far above the mean, as in e.g. (49, 50, 51, 60).
Relationship of mean and median.
The skewness is not strictly connected with the relationship between the mean and median: a distribution with negative skew can have the mean greater than or less than the median, and likewise for positive skew.
In the older notion of nonparametric skew, defined as formula_1 where "µ" is the mean, "ν" is the median, and "σ" is the standard deviation, the skewness is defined in terms of this relationship: positive/right nonparametric skew means the mean is greater than (to the right of) the median, while negative/left nonparametric skew means the mean is less than (to the left of) the median. However, the modern definition of skewness and the traditional nonparametric definition do not in general have the same sign: while they agree for some families of distributions, they differ in general, and conflating them is misleading.
If the distribution is symmetric then the mean is equal to the median and the distribution will have zero skewness. If, in addition, the distribution is unimodal, then the mean = median = mode. This is the case of a coin toss or the series 1,2,3,4... Note, however, that the converse is not true in general, i.e. zero skewness does not imply that the mean is equal to the median.
""Many textbooks," a 2005 article points out, "teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in multimodal distributions, or in distributions where one tail is long but the other is heavy. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median.""
Definition.
Pearson's moment coefficient of skewness.
The skewness of a random variable "X" is the moment coefficient of skewness. It is sometimes referred to as Pearson's moment coefficient of skewness, not to be confused with Pearson's other skewness statistics (see below). It is the third standardized moment. It is denoted "γ"1 and defined as
where "μ"3 is the third central moment, "μ" is the mean, "σ" is the standard deviation, and "E" is the expectation operator. The last equality expresses skewness in terms of the ratio of the third cumulant "κ"3 and the 1.5th power of the second cumulant "κ"2. This is analogous to the definition of kurtosis as the fourth cumulant normalized by the square of the second cumulant.
The skewness is also sometimes denoted Skew["X"].
The formula expressing skewness in terms of the non-central moment E["X"3] can be expressed by expanding the previous formula,
Properties.
Skewness can be infinite, as when
or undefined, as when
In this latter example, the third cumulant is undefined. One can also have distributions such as
where both the second and third cumulants are infinite, so the skewness is again undefined.
If "Y" is the sum of "n" independent and identically distributed random variables, all with the distribution of "X", then the third cumulant of "Y" is "n" times that of "X" and the second cumulant of "Y" is "n" times that of "X", so formula_7. This shows that the skewness of the sum is smaller, as it approaches a Gaussian distribution in accordance with the central limit theorem.
Sample skewness.
For a sample of "n" values, a natural method of moments estimator of the population skewness is
where formula_9 is the sample mean, "s" is the sample standard deviation, and the numerator "m"3 is the sample third central moment.
Another common definition of the "sample skewness" is
where formula_11 is the unique symmetric unbiased estimator of the third cumulant and formula_12 is the symmetric unbiased estimator of the second cumulant (i.e. the variance).
In general, the ratios formula_13 and formula_14 are both biased estimators of the population skewness formula_15; their expected values can even have the opposite sign from the true skewness. (For instance a mixed distribution consisting of very thin Gaussians centred at −99, 0.5, and 2 with weights 0.01, 0.66, and 0.33 has a skewness of about −9.77, but in a sample of 3, formula_14 has an expected value of about 0.32, since usually all three samples are in the positive-valued part of the distribution, which is skewed the other way.) Nevertheless, formula_13 and formula_14 each have obviously the correct expected value of zero for any symmetric distribution with a finite third moment, including a normal distribution.
The variance of the skewness of a random sample of size "n" from a normal distribution is
An approximate alternative is 6/"n" but this is inaccurate for small samples.
In normal samples, formula_13 has the smaller variance of the two estimators, with
where "m"2 in the denominator is the (biased) sample second central moment.
The adjusted Fisher–Pearson standardized moment coefficient formula_22 is the version found in Excel and several statistical packages including Minitab, SAS and SPSS.
Applications.
Skewness has benefits in many areas. Many models assume normal distribution; i.e., data are symmetric about the mean. The normal distribution has a skewness of zero. But in reality, data points may not be perfectly symmetric. So, an understanding of the skewness of the dataset indicates whether deviations from the mean are going to be positive or negative.
D'Agostino's K-squared test is a goodness-of-fit normality test based on sample skewness and sample kurtosis.
Other measures of skewness.
Other measures of skewness have been used, including simpler calculations suggested by Karl Pearson (not to be confused with Pearson's moment coefficient of skewness, see above). These other measures are:
Pearson's first skewness coefficient (mode skewness).
The Pearson mode skewness, or first skewness coefficient, is defined by
Pearson's second skewness coefficient (median skewness).
The Pearson median skewness, or second skewness coefficient, is defined by
The latter is a simple multiple of the nonparametric skew.
Other.
Starting from a standard cumulant expansion around a Normal distribution, one can actually show that 
skewness = 6 (mean − median) / standard deviation ( 1 + kurtosis / 8) + O(skewness2). One should keep in mind that above given equalities often don't hold even approximately and these empirical formulas are abandoned nowadays. There is no guarantee that these will be the same sign as each other or as the ordinary definition of skewness.
Quantile-based measures.
A skewness function
can be defined, where "F" is the cumulative distribution function. This leads to a corresponding overall measure of skewness defined as the supremum of this over the range 1/2 ≤ "u" < 1. Another measure can be obtained by integrating the numerator and denominator of this expression. The function "γ"("u") satisfies −1 ≤ "γ"("u") ≤ 1 and is well defined without requiring the existence of any moments of the distribution.
Galton's measure of skewness is γ("u") evaluated at "u" = 3/4. Other names for this same quantity are the Bowley Skewness, the Yule–Kendall index and the quartile skewness.
Kelley's measure of skewness uses u = 0.1.
L-moments.
Use of L-moments in place of moments provides a measure of skewness known as the L-skewness.
Distance skewness.
A value of skewness equal to zero does not imply that the probability distribution is symmetric. Thus there is a need for another measure of asymmetry which has this property: such a measure was introduced in 2000. It is called distance skewness and denoted by dSkew. If "X" is a random variable which takes values in the d-dimensional Euclidean space, X has finite expectation, X' is an independent identically distributed copy of X and formula_24 denotes the norm in the Euclidean space then a simple "measure of asymmetry" is
and dSkew("X") := 0 for "X" =  (with probability 1). Distance skewness is always between 0 and 1, equals 0 if and only if "X" is diagonally symmetric ("X" and −"X" has the same probability distribution) and equals 1 if and only if X is a nonzero constant with probability one. Thus there is a simple consistent statistical test of diagonal symmetry based on the sample distance skewness:
Groeneveld & Meeden’s coefficient.
Groeneveld & Meeden have suggested, as an alternative measure of skewness,
where "μ" is the mean, "ν" is the median, |…| is the absolute value and "E"() is the expectation operator.
Medcouple.
The medcouple is a scale-invariant robust measure of skewness, with a breakdown point of 25%. It is the median of the values of the kernel function 
taken over all couples formula_29 such that formula_30, where formula_31 is the median of the sample formula_32.

</doc>
<doc id="28215" url="http://en.wikipedia.org/wiki?curid=28215" title="Saint Columba (disambiguation)">
Saint Columba (disambiguation)

Saint Columba may refer to:

</doc>
<doc id="28217" url="http://en.wikipedia.org/wiki?curid=28217" title="Serial Experiments Lain">
Serial Experiments Lain

Serial Experiments Lain (シリアルエクスペリメンツレイン, Shiriaru Ekusuperimentsu Rein) is an anime series directed by Ryutaro Nakamura, with character design by Yoshitoshi ABe, screenplay written by Chiaki J. Konaka, and produced by Yasuyuki Ueda for Triangle Staff. It was broadcast on TV Tokyo from July to September 1998. A PlayStation game with the same title was released in November 1998 by Pioneer LDC.
"Serial Experiments Lain" is an avant-garde anime influenced by themes such as reality, identity, and communication. The series focuses on Lain Iwakura, an adolescent girl living in suburban Japan, and her introduction to the Wired, a global communications network similar to the Internet. Lain lives with her middle-class family, which consists of her inexpressive older sister Mika, her emotionally distant mother, and her computer-obsessed father. Lain herself is somewhat awkward, introverted, and socially isolated from most of her school peers, but the status-quo of her life becomes upturned by a series of bizarre incidents which start to take place after she learns that girls from her school have received an e-mail from Chisa Yomoda, a schoolmate who had committed suicide. When Lain receives the message herself at home, Chisa tells her (apparently in real time) that she is not dead, but has merely "abandoned her physical body and flesh", and is alive deep within the virtual reality-world of the Wired itself where she has found the almighty and divine God. From this point, Lain is caught up in a series of cryptic and surreal events that see her delving deeper into the mystery of the network in a narrative that explores themes of consciousness, perception, and the nature of reality.
The anime series was licensed in North America by Geneon (formerly Pioneer Entertainment) on DVD, VHS, and LaserDisc. However, Geneon closed its USA division in December 2007 and the series went out-of-print as a result. However, at Anime Expo 2010, North American distributor Funimation Entertainment announced that it had obtained the license to the series and re-released it in 2012. Funimation has since released all 13 episodes on YouTube. It was also released in Singapore by Odex. The video game, which shares only the themes and protagonist with the series, was never released outside Japan.
The series demonstrates influences embracing philosophy, computer history, cyberpunk literature, and conspiracy theory, and it was made the subject of several academic articles. English language anime reviewers found it to be "weird" and unusual, with generally positive reviews. Producer Ueda said he intended Japanese and American audiences to form conflicting views on the series, but was disappointed in this regard, as the impressions turned out to be similar.
Plot.
"Serial Experiments Lain" describes the 'Wired' as a virtual reality-world that contains and supports the very sum of "all" human communication and networks, created with the telegraph, televisions, and telephone services, and expanded with the Internet, cyberspace, and subsequent networks. The anime assumes that the Wired could be linked to a system that enables unconscious communication between people and machines without physical interface. The storyline introduces such a system with the Schumann resonance, a property of the Earth's magnetic field that theoretically allows for unhindered long distance communications. If such a link was created, the network would become equivalent to Reality as the general consensus of all perceptions and knowledge. The increasingly thin invisible line between what is real and what is virtual/digital begins to slowly shatter.
Masami Eiri is introduced as the project director on Protocol Seven (the next generation internet protocol in the series' time-frame) for major computer company Tachibana General Laboratories. He had secretly included code of his very own creation to give himself absolute control of the Wired through the wireless system described above. He then "uploaded" his own brain, conscience, consciousness, memory, feelings, emotions; his very self into the Wired and "died" a few days after, leaving only his physical, living body behind. These details are unveiled around the middle of the series, but this is the point where the story of "Serial Experiments Lain" begins. Masami later explains that Lain is the artifact by which the wall between the virtual and material worlds is to fall, and that he needs her to get to the Wired and "abandon the flesh", as he did, to achieve his plan. The series sees him trying to convince her through interventions, using the promise of unconditional love, romantic seduction and charm, and even, when all else fails, threats and force.
In the meantime, the anime follows a complex game of hide-and-seek between the "Knights of the Eastern Calculus", hackers whom Masami claims are "believers that enable him to be a God in the Wired", and Tachibana General Laboratories, who try to regain control of Protocol Seven. In the end, the viewer sees Lain realizing, after much introspection, that she has absolute control over everyone's mind and over reality itself. Her dialogue with different versions of herself shows how she feels shunned from the material world, and how she is afraid to live in the Wired, where she has the possibilities and responsibilities of an almighty goddess. The last scenes feature her erasing everything connected to herself from everyone's memories. She is last seen, unchanged, encountering her oldest and closest friend Arisu once again, who is now married. Lain promises herself that she and Arisu will surely meet again anytime as Lain can literally go and be anywhere she desires between both worlds.
Production.
"Serial Experiments Lain" was conceived, as a series, to be original to the point of it being considered "an enormous risk" by its producer Yasuyuki Ueda.
Producer Ueda had to answer repeated queries about a statement made in an Animerica interview. The controversial statement said "Lain" was "a sort of cultural war against American culture and the American sense of values we [Japan] adopted after World War II".
He later explained in numerous interviews that he created "Lain" with a set of values he took as distinctly Japanese; he hoped Americans would not understand the series as the Japanese would. This would lead to a "war of ideas" over the meaning of the anime, hopefully culminating in new communication between the two cultures. When he discovered that the American audience held the same views on the series as the Japanese, he was disappointed.
The "Lain" franchise was originally conceived to connect across forms of media (anime, video games, manga). Producer Yasuyuki Ueda said in an interview, "the approach I took for this project was to communicate the essence of the work by the total sum of many media products." The scenario for the video game was written first, and the video game was produced at the same time as the anime series, though the series was released first. A doujinshi named "The Nightmare of Fabrication" was produced by Yoshitoshi ABe and released in Japanese in the artbook "Omnipresence in The Wired". Ueda and Konaka declared in an interview that the idea of a multimedia project was not unusual in Japan, as opposed to the contents of "Lain", and the way they are exposed.
Writing.
The authors were asked in interviews if they had been influenced by "Neon Genesis Evangelion", in the themes and graphic design. This was strictly denied by writer Chiaki J. Konaka in an interview, arguing that he had not seen "Evangelion" until he finished the fourth episode of "Lain". Being primarily a horror movies writer, his stated influences are Godard (especially for using typography on screen), "The Exorcist", "Hell House", and Dan Curtis's "House of Dark Shadows". Alice's name, like the names of her two friends Julie and Reika, came from a previous production from Konaka, "Alice in Cyberland", which in turn was largely influenced by "Alice in Wonderland". As the series developed, Konaka was "surprised" by how close Alice's character became to the original "Wonderland" character.
Vannevar Bush (and Memex), John C. Lilly, Timothy Leary and his 8-Circuit Model of Consciousness, Ted Nelson and Project Xanadu are cited as precursors to the Wired. Douglas Rushkoff and his book "Cyberia" were originally to be cited as such, and in "Lain" Cyberia became the name of a nightclub populated with hackers and techno-punk teenagers. Likewise, the series' Deus ex machina lies in the conjunction of the Schumann resonance and Jung's collective unconscious (the authors chose this term over Kabbalah and Akashic Record). Majestic 12 and the Roswell UFO incident are used as examples of how a hoax might still have an impact on history, even after having been exposed as such, by creating sub-cultures. This links again to Vannevar Bush, the alleged "brains" of MJ12. Two of the literary references in "Lain" are quoted through Lain's father: he first logs onto a website with the password "Think Bule Count One Tow" ("Think Blue, Count Two" is an Instrumentality of Man story featuring virtual persons projected as real ones in people's minds); and his saying that "madeleines would be good with the tea" in the last episode makes "Lain" "perhaps the only cartoon to allude to Proust".
Character design.
Yoshitoshi ABe confesses to have never read manga as a child, as it was "off-limits" in his household. His major influences are "nature and everything around him". Specifically speaking about "Lain"‍‍ '​‍s character, ABe was inspired by Kenji Tsuruta, Akihiro Yamada, Range Murata, and Yukinobu Hoshino. In a broader view, he has been influenced in his style and technique by Japanese artists Chinai-san and Tabuchi-san.
The character design of Lain was not ABe's sole responsibility. Her distinctive left forelock for instance was a demand from Yasuyuki Ueda. The goal was to produce asymmetry to reflect Lain's unstable and disconcerting nature. It was designed as a mystical symbol, as it is supposed to prevent voices and spirits from being heard by the left ear. The bear pajamas she wears were a demand from character animation director Takahiro Kishida. Though bears are a trademark of the Konaka brothers, Chiaki Konaka first opposed the idea. Director Nakamura then explained how the bear motif could be used as a shield for confrontations with her family. It is a key element of the design of the shy "real world" Lain ("see "mental illness" under themes"). When she first goes to the Cyberia night club, she wears a bear hat for similar reasons. The pajamas were finally considered as possible fan-service by Konaka, in the way they enhance Lain's nymph aspect.
ABe's original design was generally more complicated than what finally appeared on screen. As an example, the X-shaped hairclip was to be an interlocking pattern of gold links. The links would open with a snap, or rotate around an axis until the moment the " X " became a " = ". This was not used as there is no scene where Lain takes her hairclip off.
Themes.
"Serial Experiments Lain" is not a conventionally linear story, but "an alternative anime, with modern themes and realization". Themes range from theological to psychological and are dealt with in a number of ways: from classical dialogue to image-only introspection, passing by direct interrogation of imaginary characters.
Communication, in its wider sense, is one of the main themes of the series, not only as opposed to loneliness, but also as a subject in itself. Writer Konaka said he wanted to directly "communicate human feelings". Director Nakamura wanted to show the audience — and particularly viewers between 14 and 15 — "the multidimensional wavelength of the existential self: the relationship between self and the world".
Loneliness, if only as representing a lack of communication, is recurrent through "Lain". Lain herself (according to Anime Jump) is "almost painfully introverted with no friends to speak of at school, a snotty, condescending sister, a strangely apathetic mother, and a father who seems to want to care but is just too damn busy to give her much of his time". Friendships turn on the first rumor; and the only insert song of the series is named "Kodoku no shigunaru", literally "signal of loneliness".
Mental illness, especially dissociative identity disorder, is a significant theme in "Lain": the main character is constantly confronted with alter-egos, to the point where writer Chiaki Konaka and Lain's voice actress Kaori Shimizu had to agree on subdividing the character's dialogues between three different orthographs. The three names designate distinct "versions" of Lain: the real-world, "childish" Lain has a shy attitude and bear pajamas. The "advanced" Lain, her Wired personality, is bold and questioning. Finally, the "evil" Lain is sly and devious, and does everything she can to harm Lain or the ones close to her. As a writing convention, the authors spelled their respective names in kanji, katakana, and roman characters (see picture).
Reality never has the pretense of objectivity in "Lain". Acceptations of the term are battling throughout the series, such as the "natural" reality, defined through normal dialogue between individuals; the material reality; and the tyrannic reality, enforced by one person onto the minds of others. A key debate to all interpretations of the series is to decide whether matter flows from thought, or the opposite. The production staff carefully avoided "the so-called God's Eye Viewpoint" to make clear the "limited field of vision" of the world of "Lain".
Theology plays its part in the development of the story too. "Lain" has been viewed as a questioning of the possibility of an infinite spirit in a finite body. From self-realization as a goddess to deicide, religion (the title of a layer) is an inherent part of "Lain"‍‍ '​‍s background.
Apple computers.
"Lain" contains extensive references to Apple computers, as the brand was used at the time by most of the creative staff, such as writers, producers, and the graphical team. As an example, the title at the beginning of each episode is announced by the Apple Computer Speech synthesis program PlainTalk, using the voice "Whisper", e.g. codice_1. Tachibana Industries, the company that creates the NAVI computers, is a reference to Apple computers: "tachibana" means "Mandarin orange" in Japanese. NAVI is the abbreviation of Knowledge Navigator, and the HandiNAVI is based on the Apple Newton, one of the world's first PDAs. The NAVIs are seen to run "Copland OS Enterprise" (this reference to Copland was an initiative of Konaka, a declared Apple fan), and Lain's and Alice's NAVIs closely resembles the Twentieth Anniversary Macintosh and the iMac respectively. The HandiNAVI programming language, as seen on the seventh episode, is a dialect of Lisp. Notice that the Newton also used a Lisp dialect (NewtonScript). The program being typed by Lain can be found in the CMU AI repository, it is a simple implementation of Conway's Game of Life in Common Lisp.
During a series of disconnected images, an iMac and the Think Different advertising slogan appears for a short time, while the "Whisper" voice says it. This was an unsolicited insertion from the graphic team, also Mac-enthusiasts. Other subtle allusions can be found: "Close the world, Open the nExt" is the slogan for the "Serial Experiments Lain" video game. NeXT was the company that produced NeXTSTEP, which later evolved into Mac OS X after Apple bought NeXT. Another example is "To Be Continued." at the end of episodes 1–12, with a blue "B" and a red "e" on "Be": "this" "Be" is the original logo of Be Inc., a company founded by ex-Apple employees and NeXT's main competitor in its time.
Media.
Anime.
"Serial Experiments Lain" was first aired on TV Tokyo on July 6, 1998 and concluded on September 28, 1998 with the thirteenth and final episode. The series consists of 13 episodes (referred to in the series as "Layers") of 24 minutes each, except for the sixth episode, "Kids" (23 minutes 14 seconds). In Japan, the episodes were released in LD, VHS, and DVD with a total of five volumes. A DVD compilation named ""Serial Experiments Lain DVD-BOX Яesurrection" was released along with a promo DVD called "LPR-309" in 2000. As this box set is now discontinued, a rerelease was made in 2005 called "Serial Experiments Lain TV-BOX". A 4-volume DVD box set was released in the US by Pioneer/Geneon. A Blu-ray release of the anime was made on December 2009 called "Serial Experiments Lain Blu-ray Box | RESTORE"". The anime series returned to US television on October 15, 2012 on the Funimation Channel.
The series' opening theme, "Duvet", was written and performed by Jasmine Rodgers and the British band Bôa. The ending theme, "Distant Scream" (遠い叫び, Tōi Sakebi), was written and composed by Reichi Nakaido.
Soundtracks.
The first original soundtrack, "Serial Experiments Lain Soundtrack", features music by Reichi Nakaido: the ending theme and part of the television series' score. The series' opening theme, "Duvet", was written and performed in English by the British rock band Bôa. The second, "Serial Experiments Lain Soundtrack: Cyberia Mix", features electronica songs inspired by the television series, including a remix of the opening theme 'Duvet'. The third, "lain BOOTLEG", consists of two CDs with more than forty-five tracks, containing ambient music from the series. One of the CDs is a mixed-mode data and audio disk, containing a clock program and a game. It was released by Pioneer Records. Because the word "bootleg" appears in its title, it is easily confused with the Sonmay counterfeit edition of itself, which contains one CD of forty-five tracks, some of which are shorter than on the original.
Video game.
On November 26, 1998, Pioneer LDC released a video game with the same name as the anime for the PlayStation. It was designed by Konaka and Yasuyuki, and made to be a "network simulator" in which the player would navigate to explore Lain's story. The creators themselves did not call it a game, but "Psycho-Stretch-Ware", and it has been described as being a kind of graphic novel: the gameplay is limited to unlocking pieces of information, and then reading/viewing/listening to them, with little or no puzzle needed to unlock. "Lain" distances itself even more from classical games by the random order in which information is collected. The aim of the authors was to let the player get the feeling that there are myriads of informations that he would have to sort through, and that he would have to do with less than what exists to understand. As with the anime, the creative team's main goal was to let the player "feel" Lain, and "to understand her problems, and to love her". A guidebook to the game called "Serial Experiments Lain Official Guide" (ISBN 4-07-310083-1) was released the same month by MediaWorks.
Reception.
"Serial Experiments Lain" was first broadcast in Tokyo at 1:15 a.m. JST. The word "weird" appears almost systematically in English language reviews of the series, or the alternatives "bizarre", and "atypical", due mostly to the freedoms taken with the animation and its unusual science fiction themes, and due to its philosophical and psychological context. Critics responded positively to these thematic and stylistic characteristics, and it was awarded an Excellence Prize by the 1998 Japan Media Arts Festival for "its willingness to question the meaning of contemporary life" and the "extraordinarily philosophical and deep questions" it asks.
According to Christian Nutt from "Newtype USA", the main attraction to the series is its keen view on "the interlocking problems of identity and technology". Nutt saluted ABe's "crisp, clean character design" and the "perfect soundtrack" in his 2005 review of series, saying that ""Serial Experiments Lain" might not yet be considered a true classic, but it's a fascinating evolutionary leap that helped change the future of anime." "Anime Jump" gave it 4.5/5, and Anime on DVD gave it A+ on all criteria for volume 1 and 2, and a mix of A and A+ for volume 3 and 4.
"Lain" was subject to commentary in the literary and academic worlds. The "Asian Horror Encyclopedia" calls it "an outstanding psycho-horror anime about the psychic and spiritual influence of the Internet". It notes that the red spots present in all the shadows look like blood pools (see picture). It notes the death of a girl in a train accident is "a source of much ghost lore in the twentieth century", more so in Tokyo. 
The "Anime Essentials" anthology by Gilles Poitras describes it as a "complex and somehow existential" anime that "pushed the envelope" of anime diversity in the 1990s, alongside the much better known "Neon Genesis Evangelion" and "Cowboy Bebop". Professor Susan J. Napier, in her 2003 reading to the American Philosophical Society called "The Problem of Existence in Japanese Animation" (published 2005), compared "Serial Experiments Lain" to "Ghost in the Shell" and Hayao Miyazaki's "Spirited Away". According to her, the main characters of the two other works cross barriers; they can cross back to our world, but Lain cannot. Napier asks whether there is something to which Lain should return, "between an empty 'real' and a dark 'virtual'".
Unlike the anime, the video game drew little attention from the public. Criticized for its (lack of) gameplay, as well as for its "clunky interface", interminable dialogues, absence of music and very long loading times, it was nonetheless remarked for its (at the time) remarkable CG graphics, and its beautiful backgrounds.
Despite the positive feedback the television series had received, Anime Academy gave this series a 75%, partly due to the "lifeless" setting it had. Michael Poirier of EX magazine stated that the last three episodes fail to resolve the questions in other DVD volumes. Justin Sevakis of Anime News Network noted that the english dub was decent, but that the show relied so little on dialogue that it hardly mattered.
Cultural references.
Lain's artificial intelligent personal assistant "NAVI" was the basis of the Siri-like guide in the audio tour by White Rabbit Press.

</doc>
<doc id="28219" url="http://en.wikipedia.org/wiki?curid=28219" title="Spontaneous emission">
Spontaneous emission

Spontaneous emission is the process by which a quantum system such as an atom, molecule, nanocrystal or nucleus in an excited state undergoes a transition to a state with a lower energy (e.g., the ground state) and emits quanta of energy. Light or luminescence from an atom is a fundamental process that plays an essential role in many phenomena in nature and forms the basis of many applications, such as fluorescent tubes, older television screens (cathode ray tubes), plasma display panels, lasers, and light emitting diodes. Lasers start by spontaneous emission, and then normal continuous operation works by stimulated emission.
Introduction.
If a light source ('the atom') is in the excited state with energy formula_1, it may spontaneously decay to a lower lying level (e.g., the ground state) with energy formula_2, releasing the difference in energy between the two states as a photon. The photon will have angular frequency formula_3 and energy formula_4 (= formula_5, where formula_6 is the Planck constant and formula_7 is the frequency):
where formula_9 is the reduced Planck constant. The phase of the photon in spontaneous emission is random as is the direction in which the photon propagates. This is not true for stimulated emission. An energy level diagram illustrating the process of spontaneous emission is shown below:
If the number of light sources in the excited state at time formula_10 is given by formula_11, the rate at which formula_12 decays is:
where formula_14 is the rate of spontaneous emission. In the rate-equation formula_14 is a proportionality constant for this particular transition in this particular light source. The constant is referred to as the "Einstein A coefficient", and has units formula_16. 
The above equation can be solved to give:
where formula_18 is the initial number of light sources in the excited state, formula_10 is the time and formula_20 is the radiative decay rate of the transition. The number of excited states formula_12 thus decays exponentially with time, similar to radioactive decay. After one lifetime, the number of excited states decays to 36.8% of its original value (formula_22-time). The radiative decay rate formula_20 is inversely proportional to the lifetime formula_24:
Theory.
Spontaneous transitions were not explainable within the framework of the Schroedinger equation, in which the electronic energy levels were quantized, but the electromagnetic field was not. Given that the eigenstates of an atom are properly diagonalized, the overlap of the wavefunctions between the excited state and the ground state of the atom is zero. Thus, in the absence of a quantized electromagnetic field, the excited state atom can not decay to the ground state. In order to explain spontaneous transitions, quantum mechanics must be extended to a quantum field theory, wherein the electromagnetic field is quantized at every point in space. The quantum field theory of electrons and electromagnetic fields is known as quantum electrodynamics.
In quantum electrodynamics (or QED), the electromagnetic field has a ground state, the QED vacuum, which can mix with the excited stationary states of the atom (for more information, see Ref. [2]). As a result of this interaction, the "stationary state" of the atom is no longer a true eigenstate of the combined system of the atom plus electromagnetic field. In particular, the electron transition from the excited state to the electronic ground state mixes with the transition of the electromagnetic field from the ground state to an excited state, a field state with one photon in it. Spontaneous emission in free space depends upon vacuum fluctuations to get started.
Although there is only one electronic transition from the excited state to ground state, there are many ways in which the electromagnetic field may go from the ground state to a one-photon state. That is, the electromagnetic field has infinitely more degrees of freedom, corresponding to the different directions in which the photon can be emitted. Equivalently, one might say that the phase space offered by the electromagnetic field is infinitely larger than that offered by the atom. This infinite degrees of freedom for the emission of the photon results in the apparent irreversible decay, i.e., spontaneous emission. 
In presence of the electromagnetic vacuum modes, the combined atom-vacuum system is explained by the superposition of the wavefunctions of the excited state atom with no photon and the ground state atom with a single emitted photon:
 formula_26
where formula_27 and formula_28 are the atomic excited state-electromagnetic vacuum wavefunction and its probability amplitude, formula_29 and formula_30 are the ground state atom with a single photon (of mode formula_31) wavefunction and its probability amplitude, formula_32 is the atomic transition frequency, and formula_33 is the frequency of the photon. The sum is over formula_34 and formula_35, which are the wavenumber and polarization of the emitted photon, respectively. As mentioned above, the emitted photon has a chance to be emitted with different wavenumbers and polarizations, and the resulting wavefunction is a superposition of these possibilities. To calculate the probability of the atom at the ground state (formula_36), one needs to solve the time evolution of the wavefunction with an appropriate Hamiltonian (see external link 1 for the detailed calculations). To solve for the transition amplitude, one needs to average over (integrate over) all the vacuum modes, since one must consider the probabilities that the emitted photon occupies various parts of phase space equally. The "spontaneously" emitted photon has infinite different modes to propagate into, thus the probability of the atom re-absorbing the photon and returning to the original state is negligible, making the atomic decay practically irreversible. Such irreversible time evolution of the atom-vacuum system is responsible for the apparent spontaneous decay of an excited atom. If one were to keep track of all the vacuum modes, the combined atom-vacuum system would undergo unitary time evolution, making the decay process reversible. Cavity quantum electrodynamics is one such system where the vacuum modes are modified resulting in the reversible decay process, see also Quantum revival. The theory of the spontaneous emission under the QED framework was first calculated by Weisskopf and Wigner. 
In spectroscopy one can frequently find that atoms or molecules in the excited states dissipate their energy in the absence of any external source of photons. This is not spontaneous emission, but is actually nonradiative relaxation of the atoms or molecules caused by the fluctuation of the surrounding molecules present inside the bulk.
Rate of spontaneous emission.
The rate of spontaneous emission (i.e., the radiative rate) can be described by Fermi's golden rule. The rate of emission depends on two factors: an 'atomic part', which describes
the internal structure of the light source and a 'field part', which describes the density of electromagnetic modes of the environment. The atomic part describes the strength of a transition between two states in terms of transition moments. In a homogeneous medium, such as free space, the rate of spontaneous emission in the dipole approximation is given by: 
where formula_3 is the emission frequency, formula_40 is the index of refraction, formula_41 is the transition dipole moment, formula_42 is the vacuum permittivity, formula_9 is the reduced Planck constant, formula_44 is the vacuum speed of light, and formula_45 is the fine structure constant. (This approximation breaks down in the case of inner shell electrons in high-Z atoms.) Clearly, the rate of spontaneous emission in free space increases with formula_46. 
In contrast with atoms, which have a discrete emission spectrum, quantum dots can be tuned continuously by changing their size. This property has been used to check the formula_46-frequency dependence of the spontaneous emission rate as described by Fermi's golden rule.
Radiative and nonradiative decay: the quantum efficiency.
In the rate-equation above, it is assumed that decay of the number of excited states formula_12 only occurs under emission of light. In this case one speaks of full radiative decay and this means that the quantum efficiency is 100%. Besides radiative decay, which occurs under the emission of light, there is a second decay mechanism; nonradiative decay. To determine the total decay rate formula_49, radiative and nonradiative rates should be summed:
where formula_49 is the total decay rate, formula_20 is the radiative decay rate and formula_53 the nonradiative decay rate. The quantum efficiency (QE) is defined as the fraction of emission processes in which emission of light is involved:
In nonradiative relaxation, the energy is released as phonons, more commonly known as heat. Nonradiative relaxation occurs when the energy difference between the levels is very small, and these typically occur on a much faster time scale than radiative transitions. For many materials (for instance, semiconductors), electrons move quickly from a high energy level to a meta-stable level via small nonradiative transitions and then make the final move down to the bottom level via an optical or radiative transition. This final transition is the transition over the bandgap in semiconductors. Large nonradiative transitions do not occur frequently because the crystal structure generally cannot support large vibrations without destroying bonds (which generally doesn't happen for relaxation). Meta-stable states form a very important feature that is exploited in the construction of lasers. Specifically, since electrons decay slowly from them, they can be deliberately piled up in this state without too much loss and then stimulated emission can be used to boost an optical signal.

</doc>
<doc id="28220" url="http://en.wikipedia.org/wiki?curid=28220" title="Nicolas Léonard Sadi Carnot">
Nicolas Léonard Sadi Carnot

Nicolas Léonard Sadi Carnot (]; 1 June 1796 – 24 August 1832) was a French military engineer and physicist, often described as the "father of thermodynamics". In his only publication, the 1824 monograph "Reflections on the Motive Power of Fire", Carnot gave the first successful theory of the maximum efficiency of heat engines. Carnot's work attracted little attention during his lifetime, but it was later used by Rudolf Clausius and Lord Kelvin to formalize the second law of thermodynamics and define the concept of entropy.
Life.
Nicolas Léonard Sadi Carnot was born in Paris into a family that was distinguished in both science and politics. He was the first son of Lazare Carnot, an eminent mathematician, military engineer and leader of the French Revolutionary Army. Lazare chose his son's third given name (by which he would always be known) after the Persian poet Sadi of Shiraz. Sadi was the elder brother of statesman Hippolyte Carnot and the uncle of Marie François Sadi Carnot, who would serve as President of France from 1887 to 1894.
At the age of 16, Sadi Carnot became a cadet in the École Polytechnique in Paris, where his classmates included Michel Chasles and Gaspard-Gustave Coriolis. The École Polytechnique was intended to train engineers for military service, but its professors included such eminent scientists as André-Marie Ampère, François Arago, Joseph Louis Gay-Lussac, Louis Jacques Thénard and Siméon Denis Poisson, and the school had become renowned for its mathematical instruction. After graduating in 1814, Sadi became an officer in the French army's corps of engineers. His father Lazare had served as Napoleon's minister of the interior during the "Hundred Days", and after Napoleon's final defeat in 1815 Lazare was forced into exile. Sadi's position in the army, under the restored Bourbon monarchy of Louis XVIII, became increasingly difficult.
Sadi Carnot was posted to different locations, he inspected fortifications, tracked plans and wrote many reports. It appears his recommendations were ignored and his career was stagnating. On September 15, 1818 he took a six-month leave to prepare for the entrance examination of Royal Corps of Staff and School of Application for the Service of the General Staff.
In 1819, Sadi transferred to the newly formed General Staff, in Paris. He remained on call for military duty, but from then on he dedicated most of his attention to private intellectual pursuits and received only two-thirds pay. Carnot befriended the scientist Nicolas Clément and attended lectures on physics and chemistry. He became interested in understanding the limitation to improving the performance of steam engines, which led him to the investigations that became his "Reflections on the Motive Power of Fire", published in 1824.
Carnot retired from the army in 1828, without a pension. He was interned in a private asylum in 1832 as suffering from "mania" and "general delirum", and he died of cholera shortly thereafter, aged 36, at the hospital in Ivry-sur-Seine.
"Reflections on the Motive Power of Fire".
Background.
When Carnot began working on his book, steam engines had achieved widely recognized economic and industrial importance, but there had been no real scientific study of them. Newcomen had invented the first piston-operated steam engine over a century before, in 1712; some 50 years after that, James Watt made his celebrated improvements, which were responsible for greatly increasing the efficiency and practicality of steam engines. Compound engines (engines with more than one stage of expansion) had already been invented, and there was even a crude form of internal-combustion engine, with which Carnot was familiar and which he described in some detail in his book. Although there existed some intuitive understanding of the workings of engines, scientific theory for their operation was almost nonexistent. In 1824 the principle of conservation of energy was still poorly developed and controversial, and an exact formulation of the first law of thermodynamics was still more than a decade away; the mechanical equivalence of heat would not be formulated for another two decades. The prevalent theory of heat was the caloric theory, which regarded heat as a sort of weightless and invisible fluid that flowed when out of equilibrium.
Engineers in Carnot's time had tried, by means such as highly pressurized steam and the use of fluids, to improve the efficiency of engines. In these early stages of engine development, the efficiency of a typical engine — the useful work it was able to do when a given quantity of fuel was burned — was only 3%.
Carnot cycle.
Carnot sought to answer two questions about the operation of heat engines: "Is the work available from a heat source potentially unbounded?" and "Can heat engines in principle be improved by replacing the steam with some other working fluid or gas?" He attempted to answer these in a memoir, published as a popular work in 1824 when he was only 28 years old. It was entitled "Réflexions sur la Puissance Motrice du Feu" ("Reflections on the Motive Power of Fire"). The book was plainly intended to cover a rather wide range of topics about heat engines in a rather popular fashion; equations were kept to a minimum and called for little more than simple algebra and arithmetic, except occasionally in the footnotes, where he indulged in a few arguments involving some calculus. He discussed the relative merits of air and steam as working fluids, the merits of various aspects of steam engine design, and even included some ideas of his own regarding possible improvements of the practical nature. The most important part of the book was devoted to an abstract presentation of an idealized engine that could be used to understand and clarify the fundamental principles that are generally applied to all heat engines, independent of their design.
Perhaps the most important contribution Carnot made to thermodynamics was his abstraction of the essential features of the steam engine, as they were known in his day, into a more general and idealized heat engine. This resulted in a model thermodynamic system upon which exact calculations could be made, and avoided the complications introduced by many of the crude features of the contemporary steam engine. By idealizing the engine, he could arrive at clear and indisputable answers to his original two questions.
He showed that the efficiency of this idealized engine is a function only of the two temperatures of the reservoirs between which it operates. He did not, however, give the exact form of the function, which was later shown to be (T1−T2)⁄T1, where T1 is the absolute temperature of the hotter reservoir. (Note: This equation probably came from Kelvin.) No thermal engine operating any other cycle can be more efficient, given the same operating temperatures.
The Carnot cycle is the most efficient possible engine, not only because of the (trivial) absence of friction and other incidental wasteful processes; the main reason is that it assumes no conduction of heat between parts of the engine at different temperatures. Carnot knew that the conduction of heat between bodies at different temperatures is a wasteful and irreversible process, which must be eliminated if the heat engine is to achieve maximum efficiency.
Regarding the second point, he also was quite certain that the maximum efficiency attainable did not depend upon the exact nature of the working fluid. He stated this for emphasis as a general proposition:
The motive power of heat is independent of the agents employed to realize it; its quantity is fixed solely by the temperatures of the bodies between which is effected, finally, the transfer of caloric.
For his "motive power of heat", we would today say "the efficiency of a reversible heat engine", and rather than "transfer of caloric" we would say "the reversible transfer of heat." He knew intuitively that his engine would have the maximum efficiency, but was unable to state what that efficiency would be.
He concluded:
The production of motive power is therefore due in steam engines not to actual consumption of caloric but to its transportation from a warm body to a cold body.
and
In the fall of caloric, motive power evidently increases with the difference of temperature between the warm and cold bodies, but we do not know whether it is proportional to this difference.
The second law of thermodynamics.
In Carnot's idealized model, the caloric transported from a hot to a cold body, yielding work, could be transported back by reversing the motion of the cycle, a concept subsequently known as thermodynamic reversibility. Carnot further postulated that no caloric is lost. The process being completely reversible, a real heat engine using cycle's reversibility is the most efficient heat machine. The proof for this is as follows: imagine we have two large bodies, a hot and a cold one. If we couple a Carnot machine to this that makes heat flow from hot to cold, an amount Q for each cycle, yielding an amount of work denoted W. If we use this work to power another machine, but one that is more efficient than the Carnot machine, it could, using the amount of work W each cycle, make an amount of heat, Q'>Q flow from the cold to the hot body. The net effect is a flow of Q'-Q of heat from the cold to the hot body, while no net work is done. This would violate the second law of thermodynamics and is thus impossible. This proves that the Carnot engine is the most efficient heat engine.
Though formulated in terms of caloric, rather than entropy, this was an early statement of the second law of thermodynamics.
Reception and later life.
Carnot's book received very little attention from his contemporaries. The only reference to it within a few years after its publication was in a review in the periodical "Revue Encyclopédique", which was a journal that covered a wide range of topics in literature. The impact of the work had only become apparent once it was modernized by Émile Clapeyron in 1834 and then further elaborated upon by Clausius and Kelvin, who together derived from it the concept of entropy and the second law of thermodynamics.
On Carnot's religious views, he was a Philosophical theist. As a deist, he believed in divine causality, stating that "what to an ignorant man is chance, cannot be chance to one better instructed," but he did not believe in divine punishment. He criticized established religion, though at the same time spoke in favor of "the belief in an all-powerful Being, who loves us and watches over us."
He was a reader of Blaise Pascal, Molière and Jean de La Fontaine.
Death.
Carnot died during a cholera epidemic in 1832, at the age of 36. 
Because of the contagious nature of cholera, many of Carnot's belongings and writings were buried together with him after his death. As a consequence, only a handful of his scientific writings survived.
After the publication of "Reflections on the Motive Power of Fire", the book quickly went out of print and for some time was very difficult to obtain. Kelvin, for one, had a difficult time getting a copy of Carnot's book. In 1890 an English translation of the book was published by R. H. Thurston; this version has been reprinted in recent decades by Dover and by Peter Smith, most recently by Dover in 2005. Some of Carnot's posthumous manuscripts have also been translated into English.
Carnot published his book in the heyday of steam engines. His theory explained why steam engines using superheated steam were better because of the higher temperature of the consequent hot reservoir. Carnot's theories and efforts did not immediately help improve the efficiency of steam engines; his theories only helped to explain why one existing practice was superior to others. It was only towards the end of the nineteenth century that Carnot's ideas, namely that a heat engine can be made more efficient if the temperature of its hot reservoir is increased, were put into practice. Carnot's book did, however, eventually have a real impact on the design of practical engines. Rudolf Diesel, for example, used Carnot's theories to design the diesel engine, in which the temperature of the hot reservoir is much higher than that of a steam engine, resulting in an engine which is more efficient.
Bibliography.
The text of part of an earlier version of this article was taken from the public domain resource "A Short Account of the History of Mathematics" by W. W. Rouse Ball (4th Edition, 1908)

</doc>
<doc id="28222" url="http://en.wikipedia.org/wiki?curid=28222" title="Sydney Opera House">
Sydney Opera House

The Sydney Opera House is a multi-venue performing arts centre in Sydney, New South Wales, Australia. Situated on Bennelong Point in Sydney Harbour, close to the Sydney Harbour Bridge, the facility is adjacent to the Sydney central business district and the Royal Botanic Gardens, between Sydney and Farm Coves.
Designed by Danish architect Jørn Utzon, the facility formally opened on 20 October 1973 after a gestation beginning with Utzon's 1957 selection as winner of an international design competition. The Government of New South Wales, led by the premier, Joseph Cahill, authorised work to begin in 1958 with Utzon directing construction. The government's decision to build Utzon's design is often overshadowed by circumstances that followed, including cost and scheduling overruns as well as the architect's ultimate resignation.
Though its name suggests a single venue, the project comprises multiple performance venues which together are among the busiest performing arts centres in the world — hosting over 1,500 performances each year attended by some 1.2 million people. The venues produce and present a wide range of in-house productions and accommodate numerous performing arts companies, including four key resident companies: Opera Australia, The Australian Ballet, the Sydney Theatre Company and the Sydney Symphony Orchestra. As one of the most popular visitor attractions in Australia, more than seven million people visit the site each year, with 300,000 people participating annually in a guided tour of the facility.
Identified as one of the 20th century's most distinctive buildings and one of the most famous performing arts centres in the world, the facility is managed by the Sydney Opera House Trust, under the auspices of the New South Wales Ministry of the Arts. The Sydney Opera House became a UNESCO World Heritage Site on 28 June 2007.
Description.
The facility features a modern expressionist design, with a series of large precast concrete "shells", each composed of sections of a sphere of 75.2 m radius, forming the roofs of the structure, set on a monumental podium. The building covers 1.8 ha of land and is 183 m long and 120 m wide at its widest point. It is supported on 588 concrete piers sunk as much as 25 m below sea level.
Although the roof structures are commonly referred to as "shells" (as in this article), they are precast concrete panels supported by precast concrete ribs, not shells in a strictly structural sense. Though the shells appear uniformly white from a distance, they actually feature a subtle chevron pattern composed of 1,056,006 tiles in two colours: glossy white as well as matte cream. The tiles were manufactured by the Swedish company Höganäs AB which generally produced stoneware tiles for the paper-mill industry.
Apart from the tile of the shells and the glass curtain walls of the foyer spaces, the building's exterior is largely clad with aggregate panels composed of pink granite quarried at Tarana. Significant interior surface treatments also include off-form concrete, Australian white birch plywood supplied from Wauchope in northern New South Wales, and brush box glulam.
Of the two larger spaces, the Concert Hall is in the western group of shells, the Joan Sutherland Theatre in the eastern group. The scale of the shells was chosen to reflect the internal height requirements, with low entrance spaces, rising over the seating areas up to the high stage towers. The smaller venues (the Drama Theatre, the Playhouse and the Studio) are within the podium, beneath the Concert Hall. A smaller group of shells set to the western side of the Monumental Steps houses the Bennelong Restaurant. The podium is surrounded by substantial open public spaces, and the large stone-paved forecourt area with the adjacent monumental steps is regularly used as a performance space.
Performance venues and facilities.
The Sydney Opera House includes a number of performance venues:
Other areas (for example the northern and western foyers) are also used for performances on an occasional basis. Venues are also used for conferences, ceremonies and social functions.
Other facilities.
The building also houses a recording studio, cafes, restaurants, bars and retail outlets. Guided tours are available, including a frequent tour of the front-of-house spaces, and a daily backstage tour that takes visitors backstage to see areas normally reserved for performers and crew members.
Construction history.
Origins.
Planning began in the late 1940s, when Eugene Goossens, the Director of the NSW State Conservatorium of Music, lobbied for a suitable venue for large theatrical productions. The normal venue for such productions, the Sydney Town Hall, was not considered large enough. By 1954, Goossens succeeded in gaining the support of NSW Premier Joseph Cahill, who called for designs for a dedicated opera house. It was also Goossens who insisted that Bennelong Point be the site: Cahill had wanted it to be on or near Wynyard Railway Station in the northwest of the CBD.
An international design competition was launched by Cahill on 13 September 1955 and received 233 entries, representing architects from 32 countries. The criteria specified a large hall seating 3,000 and a small hall for 1,200 people, each to be designed for different uses, including full-scale operas, orchestral and choral concerts, mass meetings, lectures, ballet performances and other presentations.
The winner, announced in 1957, was Jørn Utzon, a Danish architect. According to legend the Utzon design was rescued from a final cut of 30 "rejects" by the noted Finnish American architect Eero Saarinen. The prize was £5,000. Utzon visited Sydney in 1957 to help supervise the project. His office moved to Palm Beach, Sydney in February 1963.
Utzon received the Pritzker Architecture Prize, architecture's highest honour, in 2003. The Pritzker Prize citation read:
There is no doubt that the Sydney Opera House is his masterpiece. It is one of the great iconic buildings of the 20th century, an image of great beauty that has become known throughout the world – a symbol for not only a city, but a whole country and continent.
Design and construction.
The Fort Macquarie Tram Depot, occupying the site at the time of these plans, was demolished in 1958 and construction began in March 1959. It was built in three stages: stage I (1959–1963) consisted of building the upper podium; stage II (1963–1967) the construction of the outer shells; stage III (1967–1973) interior design and construction.
Stage I: Podium.
Stage I commenced on 2 March 1959 with the construction firm Civil & Civic, monitored by the engineers Ove Arup and Partners. The government had pushed for work to begin early, fearing that funding, or public opinion, might turn against them. However, Utzon had still not completed the final designs. Major structural issues still remained unresolved. By 23 January 1961, work was running 47 weeks behind, mainly because of unexpected difficulties (inclement weather, unexpected difficulty diverting stormwater, construction beginning before proper construction drawings had been prepared, changes of original contract documents). Work on the podium was finally completed in February 1963. The forced early start led to significant later problems, not least of which was the fact that the podium columns were not strong enough to support the roof structure, and had to be re-built.
Stage II: Roof.
The shells of the competition entry were originally of undefined geometry, but, early in the design process, the "shells" were perceived as a series of parabolas supported by precast concrete ribs. However, engineers Ove Arup and Partners were unable to find an acceptable solution to constructing them. The formwork for using in-situ concrete would have been prohibitively expensive, and, because there was no repetition in any of the roof forms, the construction of precast concrete for each individual section would possibly have been even more expensive.
From 1957 to 1963, the design team went through at least 12 iterations of the form of the shells trying to find an economically acceptable form (including schemes with parabolas, circular ribs and ellipsoids) before a workable solution was completed. The design work on the shells involved one of the earliest uses of computers in structural analysis, to understand the complex forces to which the shells would be subjected.The computer system was also used in the assembly of the arches. The pins in the arches were surveyed at the end of each day, and the information was entered into the computer so the next arch could be properly placed the following day. In mid-1961, the design team found a solution to the problem: the shells all being created as sections from a sphere. This solution allows arches of varying length to be cast in a common mould, and a number of arch segments of common length to be placed adjacent to one another, to form a spherical section. With whom exactly this solution originated has been the subject of some controversy. It was originally credited to Utzon. Ove Arup's letter to Ashworth, a member of the Sydney Opera House Executive Committee, states: "Utzon came up with an idea of making all the shells of uniform curvature throughout in both directions." Peter Jones, the author of Ove Arup's biography, states that "the architect and his supporters alike claimed to recall the precise "eureka" moment ... ; the engineers and some of their associates, with equal conviction, recall discussion in both central London and at Ove's house."
He goes on to claim that "the existing evidence shows that Arup's canvassed several possibilities for the geometry of the shells, from parabolas to ellipsoids and spheres." Yuzo Mikami, a member of the design team, presents an opposite view in his book on the project, "Utzon's Sphere". It is unlikely that the truth will ever be categorically known, but there is a clear consensus that the design team worked very well indeed for the first part of the project and that Utzon, Arup, and Ronald Jenkins (partner of Ove Arup and Partners responsible for the Opera House project) all played a very significant part in the design development.
As Peter Murray states in "The Saga of the Sydney Opera House":
... the two men—and their teams—enjoyed a collaboration that was remarkable in its fruitfulness and, despite many traumas, was seen by most of those involved in the project as a high point of architect/engineer collaboration.
The design of the roof was tested on scale models in wind tunnels at Southampton University and later NPL in order to establish the wind-pressure distribution around the roof shape in very high winds, which helped in the design of the roof tiles and their fixtures.
The shells were constructed by Hornibrook Group Pty Ltd, who were also responsible for construction in Stage III. Hornibrook manufactured the 2400 precast ribs and 4000 roof panels in an on-site factory and also developed the construction processes. The achievement of this solution avoided the need for expensive formwork construction by allowing the use of precast units (it also allowed the roof tiles to be prefabricated in sheets on the ground, instead of being stuck on individually at height). Ove Arup and Partners' site engineer supervised the construction of the shells, which used an innovative adjustable steel-trussed "erection arch" to support the different roofs before completion. On 6 April 1962, it was estimated that the Opera House would be completed between August 1964 and March 1965.
Stage III: Interiors.
Stage III, the interiors, started with Utzon moving his entire office to Sydney in February 1963. However, there was a change of government in 1965, and the new Robert Askin government declared the project under the jurisdiction of the Ministry of Public Works. This ultimately led to Utzon's resignation in 1966 (see below).
The cost of the project so far, even in October 1966, was still only $22.9 million, less than a quarter of the final $102 million cost. However, the projected costs for the design were at this stage much more significant.
The second stage of construction was progressing toward completion when Utzon resigned. His position was principally taken over by Peter Hall, who became largely responsible for the interior design. Other persons appointed that same year to replace Utzon were E. H. Farmer as government architect, D. S. Littlemore and Lionel Todd.
Following Utzon's resignation, the acoustic advisor, Lothar Cremer, confirmed to the Sydney Opera House Executive Committee (SOHEC) that Utzon's original acoustic design allowed for only 2000 seats in the main hall and further stated that increasing the number of seats to 3000 as specified in the brief would be disastrous for the acoustics. According to Peter Jones, the stage designer, Martin Carr, criticised the "shape, height and width of the stage, the physical facilities for artists, the location of the dressing rooms, the widths of doors and lifts, and the location of lighting switchboards."
Completion and cost.
The Opera House was formally completed in 1973, having cost $102 million. H.R. "Sam" Hoare, the Hornibrook director in charge of the project, provided the following approximations in 1973:
Stage I: podium Civil & Civic Pty Ltd approximately $5.5m.
Stage II: roof shells M.R. Hornibrook (NSW) Pty Ltd approximately $12.5m.
Stage III: completion The Hornibrook Group $56.5m.
Separate contracts: stage equipment, stage lighting and organ $9.0m.
Fees and other costs: $16.5m.
The original cost and scheduling estimates in 1957 projected a cost of £3,500,000 ($7 million) and completion date of 26 January 1963 (Australia Day). In actuality, the project was completed ten years late and 1,457% over budget in real terms.
Jørn Utzon and his resignation.
Before the Sydney Opera House competition, Jørn Utzon had won seven of the 18 competitions he had entered but had never seen any of his designs built. Utzon's submitted concept for the Sydney Opera House was almost universally admired and considered groundbreaking. The Assessors Report of January 1957, stated:
The drawings submitted for this scheme are simple to the point of being diagrammatic. Nevertheless, as we have returned again and again to the study of these drawings, we are convinced that they present a concept of an Opera House which is capable of becoming one of the great buildings of the world.
For the first stage, Utzon worked very successfully with the rest of the design team and the client, but, as the project progressed, the Cahill government insisted on progressive revisions. They also did not fully appreciate the costs or work involved in design and construction. Tensions between the client and the design team grew further when an early start to construction was demanded despite an incomplete design. This resulted in a continuing series of delays and setbacks while various technical engineering issues were being refined. The building was unique, and the problems with the design issues and cost increases were exacerbated by commencement of work before the completion of the final plans.
After the 1965 election of the Liberal Party, with Robert Askin becoming Premier of New South Wales, the relationship of client, architect, engineers and contractors became increasingly tense. Askin had been a "vocal critic of the project prior to gaining office."
His new Minister for Public Works, Davis Hughes, was even less sympathetic. Elizabeth Farrelly, Australian architecture critic, has written that:
at an election night dinner party in Mosman, Hughes' daughter Sue Burgoyne boasted that her father would soon sack Utzon. Hughes had no interest in art, architecture or aesthetics. A fraud, as well as a philistine, he had been exposed before Parliament and dumped as Country Party leader for 19 years of falsely claiming a university degree. The Opera House gave Hughes a second chance. For him, as for Utzon, it was all about control; about the triumph of homegrown mediocrity over foreign genius.
Differences ensued. One of the first was that Utzon believed the clients should receive information on all aspects of the design and construction through his practice, while the clients wanted a system (notably drawn in sketch form by Davis Hughes) where architect, contractors, and engineers each reported to the client directly and separately. This had great implications for procurement methods and cost control, with Utzon wishing to negotiate contracts with chosen suppliers (such as Ralph Symonds for the plywood interiors) and the New South Wales government insisting contracts be put out to tender.
Utzon was highly reluctant to respond to questions or criticism from the client's Sydney Opera House Executive Committee (SOHEC). However, he was greatly supported throughout by a member of the committee and one of the original competition judges, Harry Ingham Ashworth. Utzon was unwilling to compromise on some aspects of his designs that the clients wanted to change.
Utzon's ability was never in doubt, despite questions raised by Davis Hughes, who attempted to portray Utzon as an impractical dreamer. Ove Arup actually stated that Utzon was "probably the best of any I have come across in my long experience of working with architects" and: "The Opera House could become the world's foremost contemporary masterpiece if Utzon is given his head."
In October 1965, Utzon gave Hughes a schedule setting out the completion dates of parts of his work for stage III. Utzon was at this time working closely with Ralph Symonds, a manufacturer of plywood based in Sydney and highly regarded by many, despite an Arup engineer warning that Ralph Symonds's "knowledge of the design stresses of plywood, was extremely sketchy" and that the technical advice was "elementary to say the least and completely useless for our purposes." Australian architecture critic Elizabeth Farrelly has referred to Ove Arup's project engineer Michael Lewis as having "other agendas". In any case, Hughes shortly after withheld permission for the construction of plywood prototypes for the interiors, and the relationship between Utzon and the client never recovered. By February 1966, Utzon was owed more than $100,000 in fees. Hughes then withheld funding so that Utzon could not even pay his own staff. The government minutes record that following several threats of resignation, Utzon finally stated to Davis Hughes: "If you don't do it, I resign." Hughes replied: "I accept your resignation. Thank you very much. Goodbye."
Utzon left the project on 28 February 1966. He said that Hughes's refusal to pay him any fees and the lack of collaboration caused his resignation and later famously described the situation as "Malice in Blunderland". In March 1966, Hughes offered him a subordinate role as "design architect" under a panel of executive architects, without any supervisory powers over the House's construction, but Utzon rejected this. Utzon left the country never to return.
Following the resignation, there was great controversy about who was in the right and who was in the wrong. The "Sydney Morning Herald" initially opined: "No architect in the world has enjoyed greater freedom than Mr Utzon. Few clients have been more patient or more generous than the people and the Government of NSW. One would not like history to record that this partnership was brought to an end by a fit of temper on the one side or by a fit of meanness on the other." On 17 March 1966, the "Herald" offered the view that: "It was not his [Utzon's] fault that a succession of Governments and the Opera House Trust should so signally have failed to impose any control or order on the project ... his concept was so daring that he himself could solve its problems only step by step ... his insistence on perfection led him to alter his design as he went along."
The Sydney Opera House opened the way for the immensely complex geometries of some modern architecture. The design was one of the first examples of the use of computer-aided design to design complex shapes. The design techniques developed by Utzon and Arup for the Sydney Opera House have been further developed and are now used for architecture, such as works of Gehry and blobitecture, as well as most reinforced concrete structures. The design is also one of the first in the world to use araldite to glue the precast structural elements together and proved the concept for future use.
It was also a first in mechanical engineering. Another Danish firm, Steensen Varming, was responsible for designing the new air-conditioning plant, the largest in Australia at the time, supplying over 600000 cuft of air per minute, using the innovative idea of harnessing the harbour water to create a water-cooled heat pump system that is still in operation today.
Architectural design role of Peter Hall.
After the resignation of Utzon, the Minister for Public Works, Davis Hughes, and the Government Architect, Ted Farmer, organised a team to bring the Sydney Opera House to completion. The architectural work was divided between three appointees who became the Hall, Todd, Littlemore partnership. David Littlemore would manage construction supervision, Lionel Todd contract documentation, while the crucial role of design became the responsibility of Peter Hall.
Peter Hall (1931-1995) completed a combined arts and architecture degree at Sydney University. Upon graduation a travel scholarship enabled him to spend twelve months in Europe during which time he visited Utzon in Hellebæk. Returning to Sydney, Hall worked for the Government Architect, a branch of the NSW Public Works Department. While there he established himself as a talented design architect with a number of court and university buildings, including the Goldstein Hall at the University of New South Wales, which won the Sir John Sulman Medal in 1964.
Hall resigned from the Government Architects office in early 1966 to pursue his own practice. When approached to take on the design role, (after at least two prominent Sydney architects had declined), Hall spoke with Utzon by phone before accepting the position. Utzon reportedly told Hall: he (Hall) would not be able to finish the job and the Government would have to invite him back. Hall also sought the advice of others, including architect Don Gazzard who warned him acceptance would be a bad career move as the project would "never be his own".
Hall agreed to accept the role on the condition there was no possibility of Utzon returning. Even so, his appointment did not go down well with many of his fellow architects who considered that no one but Utzon should complete the Sydney Opera House. Upon Utzon's dismissal, a rally of protest had marched to Bennelong Point. A petition was also circulated, including in the Government Architects office. Peter Hall was one of the many who had signed the petition that called for Utzon's reinstatement.
When Hall agreed to the design role and was appointed in April 1966, he imagined he would find the design and documentation for the Stage III well advanced. What he found was an enormous amount of work ahead of him with many aspects completely unresolved by Utzon in relation to seating capacity, acoustics and structure. In addition Hall found the project had proceeded for nine years without the development of a concise client brief. To bring himself up to speed, Hall investigated concert and opera venues overseas and engaged stage consultant Ben Schlange and acoustic consultant Wilhelm Jordan, while establishing his team. In consultation with all the potential building users the first Review of Program was completed in January 1967. The most significant conclusion reached by Hall was that concert and opera were incompatible in the same hall. Although Utzon had sketched ideas using plywood for the great enclosing glass walls their structural viability was unresolved when Hall took on the design role. With the ability to delegate tasks and effectively coordinate the work of consultants, Hall guided the project for over five years until the opening day in 1973.
A former Government Architect, Peter Webber, in his book "Peter Hall: the Phantom of the Opera House", concludes: when Utzon resigned no one was better qualified (than Hall) to rise to the challenge of completing the design of the Opera House.
Opening.
The Sydney Opera House was formally opened by Elizabeth II, Queen of Australia, on 20 October 1973. A large crowd attended. Utzon was not invited to the ceremony, nor was his name mentioned. The opening was televised and included fireworks and a performance of Beethoven's Symphony No. 9.
Performance firsts.
During construction, lunchtime performances were often arranged for the workers, with Paul Robeson the first artist to perform, in 1960.
Various performances were presented prior to the official opening:
After the opening:
Reconciliation with Utzon; building refurbishment.
In the late 1990s, the Sydney Opera House Trust resumed communication with Utzon in an attempt to effect a reconciliation and to secure his involvement in future changes to the building. In 1999, he was appointed by the Trust as a design consultant for future work.
In 2004, the first interior space rebuilt to an Utzon design was opened, and renamed "The Utzon Room" in his honour. In April 2007, he proposed a major reconstruction of the Opera Theatre, as it was then known. Utzon died on 29 November 2008.
A state memorial service, attended by Utzon's son Jan and daughter Lin, celebrating his creative genius, was held in the Concert Hall on 25 March 2009 featuring performances, readings and recollections from prominent figures in the Australian performing arts scene.
Refurbished Western Foyers and Accessibility improvements were commissioned on 17 November 2009, the largest building project completed since Utzon was re-engaged in 1999. Designed by Utzon and his son Jan, the project provided improved ticketing, toilet and cloaking facilities. New escalators and a public lift enabled enhanced access for the disabled and families with prams. The prominent paralympian athlete Louise Sauvage was announced as the building's "accessibility ambassador" to advise on further improvements to aid people with disabilities.
Public and commemorative events.
In 1993, Constantine Koukias was commissioned by the Sydney Opera House Trust in association with REM Theatre to compose "Icon", a large-scale music theatre piece for the 20th anniversary of the Sydney Opera House.
During the 2000 Summer Olympics, the venue served as the focal point for the triathlon events. The event had a 1.5 km swimming loop at Farm Cove, along with competitions in the neighbouring Royal Botanical Gardens for the cycling and running portions of the event. In 2012, Louise Herron was appointed as the first female Chief Executive Officer of the Sydney Opera House in its history.
The Sydney Opera House sails formed a graphic projection-screen in a lightshow mounted in connection with the International Fleet Review in Sydney Harbour on 5 October 2013.
On 31 December 2013, the venue's 40th anniversary year, a New Year firework display was mounted for the first time in a decade.

</doc>
<doc id="28223" url="http://en.wikipedia.org/wiki?curid=28223" title="Selim II">
Selim II

Selim II (Ottoman Turkish: سليم ثانى "Selīm-i sānī", Turkish:"II.Selim"; 28 May 1524 – 12 December/15 December 1574), also known as "Selim the Sot (Mest)" in west and as "Sarı Selim" (Selim the Blond) in east, was the Sultan of the Ottoman Empire from 1566 until his death in 1574. He was a son of Suleiman the Magnificent and Haseki Hürrem Sultan.
Biography.
He was born in Constantinople a son of Suleiman the Magnificent and his legal Rusyn wife, Hürrem Sultan. She later became the first Haseki Sultan and was a prominent figure during the era known as the Sultanate of Women.
In 1545, at Konya, he married Nurbanu Sultan whose background is disputed. It is said that she was originally named Cecelia Venier Baffo, or Rachel, (or Kale Katenou). She was the mother of Murad III. She later became the first Valide Sultan who acted as co-regent with the sultan in the Sultanate of Women.
After gaining the throne after palace intrigue and fraternal dispute, he succeeded as Sultan on 7 September 1566, According to one source Selim II became the first Sultan devoid of active military interest and willing to abandon power to his ministers, provided he was left free to pursue his orgies and debauches. His Grand Vizier, Mehmed Sokollu, from what is now Bosnia and Herzegovina, controlled much of state affairs, and two years after Selim's accession succeeded in concluding at Constantinople an honourable treaty (17 February 1568) with the Habsburg Holy Roman Emperor, Maximilian II, whereby the Emperor agreed to pay an annual "present" of 30,000 ducats and essentially granted the Ottomans authority in Moldavia and Walachia.
Against Russia, Selim was less fortunate and the first encounter between the Ottoman Empire and her future northern rival gave presage of disaster to come. A plan had been prepared in Istanbul for uniting the Volga and Don by a canal, and in the summer of 1569 a large force of Janissaries and cavalry were sent to lay siege to Astrakhan and begin the canal works, while an Ottoman fleet besieged Azov. But a sortie of the garrison of Astrakhan drove back the besiegers; a Russian relief army of 15,000 attacked and scattered the workmen and the Tatar force sent for their protection; and finally, the Ottoman fleet was destroyed by a storm. Early in 1570 the ambassadors of Ivan IV of Russia concluded at Constantinople a treaty which restored friendly relations between the Sultan and the Tsar.
Expeditions in the Hejaz and Yemen were more successful, but the conquest of Cyprus in 1571, which provided Selim with his favourite vintage, led to the calamitous naval defeat against Spain and Italian states in the Battle of Lepanto in the same year, freeing the Mediterranean Sea from corsairs.
The Empire's shattered fleets were soon restored (in just six months; it consisted of about 150 galleys and 8 galleasses) and the Ottomans maintained control of the Mediterranean (1573). In August 1574, months before Selim's death, the Ottomans regained control of Tunisia from Spain who had controlled it since 1572.
During his reign, his older sister Mihrimah Sultan, acted as his Valide Sultan, (equivalent to Queen Mother), due to the fact that their mother Hürrem Sultan, died before his reign began. She was the most powerful woman in the empire, and often lent him large sums of gold, to fight his wars.
Selim II between reality and myths.
The discussion about the personality of Sultan Selim II goes back centuries, he was most likely the center of discussion even in the period of time in which he lived. This would be understandable, as his father, Sultan Suleyman the Magnificent, would be difficult to surpass in greatness.
Actual imperial orders from the Sultan seem hardly able to fit within the uncited fictional character seeking to escape Islam’s commandments to pursue Western frivolities. On the contrary, imperial orders show firm resolve to ease the burden of those engaging in the strenuous Hajj pilgrimage, and special consideration for Muslims living under the subjugation of intolerant colonialists.
An excerpt of an imperial order from the Sultan:
"..because the accursed Portuguese are everywhere owing to their hostilities against India, and the routes by which Muslims come to the Holy Places are obstructed and moreover, it is not considered lawful for the people of Islam to live under the power of miserable infidels … you are to gather together all the expert architects and engineers of that place and investigate the land between the Mediterranean and the Red Seas and report where it is possible to make a canal in that desert place and how long it would be and how many boats could pass side by side."
Many Western sources created the common story that Sultan Selim II was actually controlled by his Grand Vizier. This theory has left out important information contained within Ottoman Archives where Sultan Selim II was often deciding between various viziers and creating his own hierarchies of authority:
"In 1568 a strong expedition was sent to pacify the province under the command of Sultan Selim’s former tutor and confidant Lala Mustafa Pasha, a choice which showed that Selim was not entirely the pawn of his grand vezir, for Sokullu Mehmed resented Lala Mustafa’s place in the Sultan’s affections. To put down the uprising in Yemen Lala Mustafa needed men and supplies from Egypt but the provincial governor, another rival Koca Sinan Pasha, refused his requests and made it impossible for him to pursue the campaign. In a spate of petitions to the Sultan the two defended their respective positions. Koca Sinan proved the stronger and Lala Mustafa was dismissed from command of the Yemen campaign. To mark his continuing favour, however, Selim created for him the position of sixth vezir of the governing council of the empire. -Osman’s Dream by Caroline Finkel".
Marks of decay.
Scottish historian Lord Kinross, in his "The Seeds of Decline", sees the massive outlay for the fleet-rebuilding following the Battle of Lepanto as the start of the Empire's slow decay, although many historians confirm the Empire's decay started with Murad III his son. Sultan Selim II died in Topkapi Palace after a period of fever brought on when he slipped over on the wet floor of an unfinished bath-house, getting a head injury.
External links.
 Media related to at Wikimedia Commons
<BR>

</doc>
<doc id="28224" url="http://en.wikipedia.org/wiki?curid=28224" title="Smith">
Smith

Smith may refer to:

</doc>
<doc id="28226" url="http://en.wikipedia.org/wiki?curid=28226" title="Show business">
Show business

Show business, sometimes shortened to show biz or showbiz (since ca. 1945), is a vernacular term for all aspects of entertainment, especially light entertainment. The word applies to all aspects of the entertainment industry from the business side (including managers, agents, producers and distributors) to the creative element (including artists, performers, writers, musicians and technicians). The term was in common usage throughout the 20th century, but the first known use in print dates from 1850. 
 At that time and for several decades it always included an initial "the". By the latter part of the century it had acquired a slightly arcane quality associated with the era of variety, but the term is still in active use.

</doc>
<doc id="28230" url="http://en.wikipedia.org/wiki?curid=28230" title="Speaker for the Dead">
Speaker for the Dead

Speaker for the Dead is a 1986 science fiction novel by Orson Scott Card and an indirect sequel to the novel "Ender's Game". This book takes place around the year 5270, some 3,000 years after the events in "Ender's Game". However, because of relativistic space travel, Ender himself is only about 35 years old.
This is the first book to discuss the Starways Congress, a high standpoint Legislation for the human colonies. It is also the first to describe the Hundred Worlds, the planets with human colonies that are tightly intertwined by Ansible technology.
Like "Ender's Game", the book won the Nebula Award in 1986 and the Hugo Award in 1987. "Speaker for the Dead" was published in a slightly revised edition in 1991. It was followed by "Xenocide" and "Children of the Mind".
Meaning of the term "Speaker for the Dead".
In this novel's precursor, "Ender's Game", the last surviving member of 'the Buggers' contacts the lead character (Ender Wiggin), who had unwittingly wiped out the rest of the species. Ender tells the story of the Buggers as it is related to him, and publishes it as "The Hive Queen" under the pseudonym "Speaker for the Dead." The audience of "The Hive Queen" is not aware of the identity of the author (or that the work is factual and not speculative). However, Hegemon Peter Wiggin (Ender's brother) recognized the writing and requested that Ender also act as 'his' "Speaker". Ender complies with the request by writing a second book titled "The Hegemon", giving a parallel, but uniquely human, perspective to the ideas and lessons of "The Hive Queen".
The two books become classics and inspire the rise of a movement of Speakers for the Dead. The movement is not a religion, although Speakers are treated with the respect afforded to a priest or cleric. Any citizen has the legal right to summon a Speaker (or a priest of any faith, which Speakers are legally considered) to mark the death of a family member. Speakers research the dead person's life and give a speech that attempts to speak for them, describing the person's life as he or she tried to live it. This speech is not given in order to persuade the audience to condemn or forgive the deceased, but rather a way to understand the person as a whole, including any flaws or misdeeds.
Plot summary.
The Hive Queen/Continuation of Ender's Game.
At the close of "Ender's Game", Ender reports to be transporting "precious cargo". "Speaker for the Dead" reveals and confirms his precious cargo to be the Hive Queen whom he meets at the very end of the first novel. He wishes to restore her species, and ashamed of the Xenocide, he composes "The Hive Queen" under the name Speaker for the Dead and begins to seek out a new planet for her to thrive. After receiving call to speak on the planet of Lusitania, Ender is hopeful that this planet may fit the Hive Queen.
Lusitania.
On Novinha's request for a Speaker, Andrew Wiggin leaves for Lusitania, a colony turned into a virtual prison, with its expansion severely limited and its whole existence devoted to the work of xenologers (alien anthropology) and xenobiologists (alien biology) who study the Pequeninos ("piggies"), the first sentient beings found since the destruction of the Formics. Lusitania itself is remarkably lacking in biodiversity, featuring thousands of unfilled ecological niches. The other outstanding feature of Lusitania is the "Descolada", a native virus which almost wipes out the colony, until husband-and-wife xenobiologists Gusta and Cida, known as Os Venerados, succeed in developing counters. Unfortunately, they did not find the cure soon enough to save themselves, leaving their orphaned daughter Novinha to strike out for herself.
Novinha.
At the age of thirteen, Novinha, a cold and distant girl, successfully petitions to be made an official xenobiologist of the colony. From then on, she contributes to the work of father-and-son xenologers Pipo and Libo, and for a short time there is family and camaraderie. One day, however, she makes a discovery about the Descolada (later revealed to be a virus that is in every native life-form) and Pipo rushes out to talk to the piggies without telling her or Libo why it is important. They cannot figure it out on their own, and never learn. A few hours later, Pipo is found vivisected in the grass; his corpse does not even have the benefit of a tree (the symbol of honor placed among all dead piggies). To protect Libo from suffering the same fate, Novinha erases all the lab work, but can only lock the information because of regulations. Libo demands to see it, but even their love for each other does not convince her. It appears to be a secret the piggies will kill to keep. Novinha is determined to ensure they never marry, for if they do, Libo will gain access to those locked files. In anguish, Novinha calls for a Speaker for the Dead, hoping that perhaps the original Speaker may arrive.
Ender/Andrew Wiggin.
At the start of the novel, Andrew Wiggin is a Speaker for the Dead on the planet Trondheim, where his sister Valentine resides. He doesn't dare let himself be known as Ender anymore; the name is now an epithet and considered taboo because of the Xenocide of the Formics ("buggers"). Upon receiving the call from Novinha to speak for Pipo's death at Lusitania, Ender decides to leave his sister behind because she just married and is pregnant. His only companion on the journey is Jane, an artificial sentience existing within the ansible computer network by which spaceships and planets communicate instantly across galactic distances.
He arrives on Lusitania after twenty-two years in transit (only around two weeks to him) to discover that Novinha has canceled her call, or rather tried to, as a call for a speaker cannot legally be canceled after the speaker has begun the journey. However, two other people have called for a speaker, making Ender's trip not entirely in vain: they are Novinha's eldest son Miro, calling for Libo, who was killed in the same manner as his father four years before Ender's arrival; and Novinha's eldest daughter Ela, calling for Novinha's husband Marcos Ribeira, who died not six weeks ago from a terminal disease. Besides attempting to unravel the question of why Novinha married Marcão when she really loved Libo (Marcão was sterile, and a quick genetic scan on Jane's part reveals that Novinha's children are all, in fact, Libo's), Ender also takes responsibility for attempting to heal the Ribeira family, and manages to adopt (or perhaps is adopted by) most of the children within their first meeting.
He also takes a strong interest in the pequeninos, and eventually (in direct violation of Starways Congress law) meets with them in person. The Hive Queen has also managed to make contact with the pequeninos philotically, and has told them a number of things—including the fact that "Andrew Wiggin" is not only the original Speaker for the Dead, but the original Xenocide as well. The Hive Queen very emphatically wants to be revived and freed on Lusitania.
Jane realizes the tension between Ender and the Catholic colonists, especially Bishop Peregrino. To spawn cooperation, she creates a common enemy by revealing to the Starways Congress that Miro and Ouanda have been committing Questionable activities, including teaching the pequeninos farming techniques.
The Speaking.
Ender holds a speaking for Marcão, Novinha's late husband. As a speaker's job, Ender reveals secrets from the lives of Libo, Pipo, and even Novinha herself. He explains how Novinha blamed herself for Pipo's death, and underwent a life of suffering and deception. She married Marcão to prevent Libo from accessing the information which killed Pipo, but she trusted with Libo—because their love for each other never truly died. Knowing about the affair, Marcão took out his anger on his wife and children.
The Severing of the Ansible/The Covenant.
In response to Jane's article, the Starways Congress revokes Lusitania colony's charter and orders an immediate evacuation. In addition, they place Miro and Ouanda under arrest for committing Questionable Activities and order them to report to Trondheim for trial, a journey that would take twenty-two years. Convincing him that he can hide in the forest, the pequeninos urge Miro to cross the electric fence. In his attempt, he suffers significant neurological damage that partially paralyzes him. The colony declares itself in rebellion, and Jane severs the ansible connection, shutting down the electric fence and allowing Ender and the others to save Miro.
With pequenino Human's help, Ender is able to work out a treaty with the Wives, the women and leaders of the piggies, so that humans and pequeninos might live in peace. The meaning of Pipo's and Libo's murders is revealed as well: the trees are the "third stage" in the life of the piggies. Trees grown from piggies normally become brothertrees, but the ritually dissected ones are done so in order to make them fathertrees—sentient, living trees that are, unlike animal pequeninos, capable of reproduction (the Descolada is proved to be instrumental in these transformations). Both Pipo and Libo gained honor with the help of their partners, Mandachuva and Leaf-Eater respectively. The pequenino reward for such an honor is to be brought into the third life. Mandachuva and Leaf-Eater bring Libo and Pipo into the third life because the xenobiologists couldn't muster the strength to commit the act. Human begs Ender to do the same to him, and after struggling with the decision, Ender does the deed. Our final image of Human is of a small sapling growing out of his spine. Ender later adds to the treaty that no humans may be brought to the third life, as the colonists would view it as murder.
At the conclusion of the novel, Valentine and her family plan to travel to Lusitania to help in the rebellion. Miro, with his crippled and partially paralyzed body, is sent into space to meet them. For the first time in his life, someone (Novinha) is prepared to receive the Xenocide with compassion instead of revulsion, and she and Ender marry. In the final chapter, Ender releases the Hive Queen, ready to begin the continuation of her species.
The novel begins 3,081 years after the events of the first book, by which time the works of "The Hive Queen" and "The Hegemon" have caused the human race to let go of its hatred of the Buggers and instead revile Ender as "The Xenocide", who exterminated an entire species. Ender himself, now using his real name of Andrew Wiggin, is still alive because of relativistic space travel, and still acting as a Speaker for the Dead. No one connects "Andrew Wiggin" with "Ender Wiggin", nor do they connect him (as "Andrew" or "Ender") with the original Speaker for the Dead.
Relation to "Ender's Game".
Whereas the previous novel focused on armies and space warfare, "Speaker for the Dead" is philosophical in nature, although it still advances a xenology for the planetary setting. Its story finds Andrew in a human colony on the colony planet Lusitania, believed to be the only remaining planet in Card's universe with an intelligent alien race after the xenocide of the "Buggers" in "Ender's Game". The novel deals with the difficult relationship between the humans and the "piggies" (or "pequeninos", since the action is set in a Catholic Brazilian research installation) and with Andrew's attempts to bring peace to a brilliant but troubled family whose history is intertwined with that of the pequeninos.
The Pequeninos.
The Pequeninos (also known as "piggies") are a native species on Lusitania. They are the only sentient alien species discovered since the xenocide of the buggers. Many provisions are taken by the Starways Congress to prevent contaminating the Pequeninos culture with any human technological advances or human culture. At the beginning, not much is known about them other than that they worship the trees and call the trees their fathers. Later on in the book, it is learned that the Pequeninos have what is called a "third life", where they are reborn as trees. It is a great honor to be allowed to enter the third life, one of the reasons being that only when they are in the third life are they able to reproduce. The Pequeninos have a special language reserved for speaking to the trees, and the trees can be manipulated to build wooden structures and tools as a favor to the piggies. The Pequeninos want to learn much more about human culture and "murder" Xenologers Pipo and Libo. Pipo and Libo both contributed significantly to their survival, so the Pequeninos wanted them to vivisect a Pequenino, which is necessary to attain the third life. However, Pipo and Libo both refused and the Pequeninos proceeded to vivisect them instead, with intentions of their entering the third life. However, their endeavor failed as Libo and Pipo are humans and have no third life. As the nature of the third life and the necessary procedure was unknown to humans, it appeared to the human population that Pipo and Libo were brutally murdered, thus the main and first reason that Ender came to Lusitania.
Lack of film adaptation.
At the Los Angeles Times Book Festival (April 20, 2013), Card stated why he does not want "Speaker for the Dead" made into a film: 
""Speaker for the Dead" is unfilmable," Card said in response to a question from the audience. "It consists of talking heads, interrupted by moments of excruciating and unwatchable violence. Now, I admit, there's plenty of unwatchable violence in film, but never attached to my name. "Speaker for the Dead", I don't want it to be filmed. I can't imagine it being filmed."

</doc>
<doc id="28232" url="http://en.wikipedia.org/wiki?curid=28232" title="Star catalogue">
Star catalogue

A star catalogue, or star catalog, is an astronomical catalogue that lists stars. In astronomy, many stars are referred to simply by catalogue numbers. There are a great many different star catalogues which have been produced for different purposes over the years, and this article covers only some of the more frequently quoted ones. Star catalogues were compiled by many different ancient peoples, including the Babylonians, Greeks, Chinese, Persians, and Arabs. Most modern catalogues are available in electronic format and can be freely downloaded from NASA's Astronomical Data Center.
Completeness and accuracy is described by the weakest apparent magnitude V (largest number) and the accuracy of the positions.
Historical catalogues.
Ancient Near East.
From their existing records, it is known that the ancient Egyptians recorded the names of only a few identifiable constellations and a list of thirty-six decans that were used as a star clock. The Egyptians called the circumpolar star 'the star that cannot perish' and, although they made no known formal star catalogues, they nonetheless created extensive star charts of the night sky which adorn the coffins and ceilings of tomb chambers.
Although the ancient Sumerians were the first to record the names of constellations on clay tablets, the earliest known star catalogues were compiled by the ancient Babylonians of Mesopotamia in the late 2nd millennium BC, during the Kassite Period ("ca". 1531 BC to "ca". 1155 BC). They are better known by their Assyrian-era name 'Three Stars Each'. These star catalogues, written on clay tablets, listed thirty-six stars: twelve for 'Anu' along the celestial equator, twelve for 'Ea' south of that, and twelve for 'Enlil' to the north. The Mul.Apin lists, dated to sometime before the Neo-Babylonian Empire (626-539 BC), are direct textual descendants of the 'Three Stars Each' lists and their constellation patterns show similarities to those of later Greek civilization.
Hellenistic world and Roman Empire.
In Ancient Greece, the astronomer and mathematician Eudoxus laid down a full set of the classical constellations around 370 BC. His catalogue "Phaenomena", rewritten by Aratus of Soli between 275 and 250 BC as a didactic poem, became one of the most consulted astronomical texts in antiquity and beyond. It contains descriptions of the positions of the stars, the shapes of the constellations and provided information on their relative times of rising and setting.
Approximately in the 3rd century BC, the Greek astronomers Timocharis of Alexandria and Aristillus created another star catalogue. Hipparchus (c. 190 – c. 120 BC) completed his star catalogue in 129 BC, which he compared to Timocharis' and discovered that the longitude of the stars had changed over time. This led him to determine the first value of the precession of the equinoxes. In the 2nd century, Ptolemy (c. 90 - c. 186 AD) of Roman Egypt published a star catalogue as part of his "Almagest", which listed 1,022 stars visible from Alexandria. Ptolemy's catalogue was based almost entirely on an earlier one by Hipparchus (Newton 1977; Rawlins 1982). It remained the standard star catalogue in the Western and Arab worlds for over eight centuries. The Islamic astronomer al-Sufi updated it in 964, and the star positions were redetermined by Ulugh Beg in 1437, but it was not fully superseded until the appearance of the thousand-star catalogue of Tycho Brahe in 1598.
Although the ancient Vedas of India specified how the ecliptic was to be divided into twenty-eight "nakshatra", Indian constellation patterns were ultimately borrowed from Greek ones sometime after Alexander's conquests in Asia in the 4th century BC.
Ancient China.
The earliest known inscriptions for Chinese star names were written on oracle bones and date to the Shang Dynasty (c. 1600 - c. 1050 BC). Sources dating from the Zhou Dynasty (c. 1050 - 256 BC) which provide star names include the "Zuo Zhuan", the "Shi Jing", and the "Canon of Yao" (堯典) in the "Book of Documents". The "Lüshi Chunqiu" written by the Qin statesman Lü Buwei (d. 235 BC) provides most of the names for the twenty-eight mansions (i.e. asterisms across the ecliptic belt of the celestial sphere used for constructing the calendar). An earlier lacquerware chest found in the Tomb of Marquis Yi of Zeng (interred in 433 BC) contains a complete list of the names of the twenty-eight mansions. Star catalogues are traditionally attributed to Shi Shen and Gan De, two rather obscure Chinese astronomers who may have been active in the 4th century BC of the Warring States period (403-221 BC). The "Shi Shen astronomy" (石申天文, Shi Shen tienwen) is attributed to Shi Shen, and the "Astronomic star observation" (天文星占, Tianwen xingzhan) to Gan De.
It was not until the Han Dynasty (202 BC - 220 AD) that astronomers started to observe and record names for all the stars that were apparent (to the naked eye) in the night sky, not just those around the ecliptic. A star catalogue is featured in one of the chapters of the late 2nd-century-BC history work "Records of the Grand Historian" by Sima Qian (145-86 BC) and contains the "schools" of Shi Shen and Gan De's work (i.e. the different constellations they allegedly focused on for astrological purposes). Sima's catalogue—the "Book of Celestial Offices" (天官書 Tianguan shu)—includes some 90 constellations, the stars therein named after temples, ideas in philosophy, locations such as markets and shops, and different people such as farmers and soldiers. For his "Spiritual Constitution of the Universe" (靈憲, Ling Xian) of 120 AD, the astronomer Zhang Heng (78-139 AD) compiled a star catalogue comprising 124 constellations. Chinese constellation names were later adopted by the Koreans and Japanese.
Islamic world.
A large number of star catalogues were published by Muslim astronomers in the medieval Islamic world. These were mainly "Zij" treatises, including Arzachel's "Tables of Toledo" (1087), the Maragheh observatory's "Zij-i Ilkhani" (1272) and Ulugh Beg's "Zij-i-Sultani" (1437). Other famous Arabic star catalogues include Alfraganus' "A compendium of the science of stars" (850) which corrected Ptolemy's "Almagest"; and Azophi's "Book of Fixed Stars" (964) which described observations of the stars, their positions, magnitudes, brightness and colour, drawings for each constellation, and the first descriptions of Andromeda Galaxy and the Large Magellanic Cloud. Many stars are still known by their Arabic names (see List of Arabic star names).
Pre-Columbian Americas.
The "Motul Dictionary", compiled in the 16th century by an anonymous author (although attributed to Fray Antonio de Ciudad Real), contains a list of stars originally observed by the ancient Mayas. The Maya Paris Codex also contain symbols for different constellations which were represented by mythological beings.
Bayer and Flamsteed catalogues.
Two systems introduced in historical catalogues remain in use to the present day. The first system comes from the German astronomer Johann Bayer's (1572–1625) "Uranometria" published in 1603 and is for bright stars. These are given a Greek letter followed by the genitive case of the constellation in which they are located; examples are Alpha Centauri or Gamma Cygni. The major problem with Bayer's naming system was the number of letters in the Greek alphabet (24). It was easy to run out of letters before running out of stars needing names, particularly for large constellations such as Argo Navis. Bayer extended his lists up to 67 stars by using lower-case Roman letters ("a" through "z") then upper-case ones ("A" through "Q"). Few of those designations have survived. It is worth mentioning, however, as it served as the starting point for variable star designations, which start with "R" through "Z", then "RR", "RS", "RT"..."RZ", "SS", "ST"..."ZZ" and beyond.
The second system comes from the English astronomer John Flamsteed's (1646–1719) "Historia coelestis Britannica". It kept the genitive-of-the-constellation rule for the back end of his catalog names, but used numbers instead of the Greek alphabet for the front half. Examples include 61 Cygni and 47 Ursae Majoris.
Full-sky catalogues.
Bayer and Flamsteed covered only a few thousand stars between them. In theory, full-sky catalogues try to list every star in the sky. There are, however, billions of stars resolvable by telescopes, so this is an impossible goal; these kind of catalogs generally try to get every star brighter than a given magnitude.
LAL.
Jérôme Lalande published the "Histoire Céleste Française" in 1801, which contained an extensive star catalog, among other things. The observations made were made from the Paris Observatory and so it describes mostly northern stars. This catalog contained the positions and magnitudes of 47,390 stars, out to magnitude 9, and was the most complete catalog up to that time. A significant reworking of this catalog in 1846 added reference numbers to the stars that are used to refer to some of these stars to this day. The decent accuracy of this catalog kept it in common use as a reference by observatories around the world throughout the 19th century.
HD/HDE.
The Henry Draper Catalogue was published in the period 1918–1924. It covers the whole sky down to about ninth or tenth magnitude, and is notable as the first large-scale attempt to catalogue spectral types of stars.
The catalogue was compiled by Annie Jump Cannon and her co-workers at Harvard College Observatory under the supervision of Edward Charles Pickering, and was named in honour of Henry Draper, whose widow donated the money required to finance it.
HD numbers are widely used today for stars which have no Bayer or Flamsteed designation. Stars numbered 1–225300 are from the original catalogue and are numbered in order of right ascension for the 1900.0 epoch. Stars in the range 225301–359083 are from the 1949 extension of the catalogue. The notation HDE can be used for stars in this extension, but they are usually denoted HD as the numbering ensures that there can be no ambiguity.
SAO.
The Smithsonian Astrophysical Observatory catalogue was compiled in 1966 from various previous astrometric catalogues, and contains only the stars to about ninth magnitude for which accurate proper motions were known. There is considerable overlap with the Henry Draper catalogue, but any star lacking motion data is omitted. The epoch for the position measurements in the latest edition is J2000.0. The SAO catalogue contains this major piece of information not in Draper, the proper motion of the stars, so it is often used when that fact is of importance. The cross-references with the Draper and Durchmusterung catalogue numbers in the latest edition are also useful.
Names in the SAO catalogue start with the letters SAO, followed by a number. The numbers are assigned following 18 ten-degree bands in the sky, with stars sorted by right ascension within each band.
BD/CD/CPD.
The "Bonner Durchmusterung" ("German": Bonn sampling) and follow-ups were the most complete of the pre-photographic star catalogues.
The "Bonner Durchmusterung" itself was published by Friedrich Wilhelm Argelander, Adalbert Krüger, and Eduard Schönfeld between 1852 and 1859. It covered 320,000 stars in epoch 1855.0.
As it covered only the northern sky and some of the south (being compiled from the Bonn observatory), this was then supplemented by the "Südliche Durchmusterung " (SD), which covers stars between declinations -1 and -23 degrees
(1886, 120,000 stars). It was further supplemented by the "Cordoba Durchmusterung" (580,000 stars), which began to be compiled at Córdoba, Argentina in 1892 under the initiative of John M. Thome and covers declinations -22 to -90. Lastly, the "Cape Photographic Durchmusterung" (450,000 stars, 1896), compiled at the Cape, South Africa, covers declinations -18 to -90.
Astronomers preferentially use the HD designation of a star, as that catalogue also gives spectroscopic information, but as the Durchmusterungs cover more stars they occasionally fall back on the older designations when dealing with one not found in Draper. Unfortunately, a lot of catalogues cross-reference the Durchmusterungs without specifying which one is used in the zones of overlap, so some confusion often remains.
Star names from these catalogues include the initials of which of the four catalogues they are from (though the "Southern" follows the example of the "Bonner" and uses BD; CPD is often shortened to CP), followed by the angle of declination of the star (rounded towards zero, and thus ranging from +00 to +89 and -00 to -89), followed by an arbitrary number as there are always thousands of stars at each angle. Examples include BD+50°1725 or CD-45°13677.
AC.
The "Catalogue astrographique" (Astrographic Catalogue) was part of the international "Carte du Ciel" programme designed to photograph and measure the positions of all stars brighter than magnitude 11.0. In total, over 4.6 million stars were observed, many as faint as 13th magnitude. This project was started in the late 19th century. The observations were made between 1891 and 1950. To observe the entire celestial sphere without burdening too many institutions, the sky was divided among 20 observatories, by declination zones. Each observatory exposed and measured the plates of its zone, using a standardized telescope (a "normal astrograph") so each plate photographed had a similar scale of approximately 60 arcsecs/mm. The U.S. Naval Observatory took over custody of the catalogue, now in its 2000.2 edition.
USNO-B1.0.
USNO-B1.0 is an all-sky catalog created by research and operations astrophysicists at the U.S. Naval Observatory (as developed at the United States Naval Observatory Flagstaff Station), that presents positions, proper motions, magnitudes in various optical passbands, and star/galaxy estimators for 1,042,618,261 objects derived from 3,643,201,733 separate observations. The data was obtained from scans of 7,435 Schmidt plates taken for the various sky surveys during the last 50 years. USNO-B1.0 is believed to provide all-sky coverage, completeness down to V = 21, 0.2 arcsecond astrometric accuracy at J2000.0, 0.3 magnitude photometric accuracy in up to five colors, and 85% accuracy for distinguishing stars from non-stellar objects. USNO-B is now followed by NOMAD; both can be found on the Naval Observatory server. The Naval Observatory is currently working on B2 and C variants of the USNO catalog series.
GSC.
The "Guide Star Catalog" is an online catalog of stars produced for the purpose of accurately positioning and identifying stars satisfactory for use as "guide stars" by the Hubble Space Telescope program. The first version of the catalog was produced in the late 1980s by digitizing photographic plates and contained about 20 million stars, out to about magnitude 15. The latest version of this catalog contains information for 945,592,683 stars, out to magnitude 21. The latest version continues to be used to accurately position the Hubble Space Telescope.
Specialized catalogues.
Specialized catalogs make no effort to list all the stars in the sky, working instead to highlight a particular type of star, such as variables or nearby stars.
ADS.
Aitken's double star catalogue
This lists 17,180 double stars north of declination -30 degrees.
BS, BSC, HR.
First published in 1930 as the "Yale Catalog of Bright Stars", this catalog contained information on all stars brighter than visual magnitude 6.5 in the "Harvard Revised Photometry Catalogue". The list was revised in 1983 with the publication of a supplement that listed additional stars down to magnitude 7.1. The catalog detailed each star's coordinates, proper motions, photometric data, spectral types, and other useful information.
The last printed version of the Bright Star Catalogue was the 4th revised edition, released in 1982. The 5th edition is in electronic form and is available online.
Carbon Stars.
Stephenson's General Catalogue of galactic Carbon stars is a catalogue of 7000+ carbon stars.
Gl, GJ, Wo.
The Gliese (later Gliese-Jahreiß) catalogue attempts to list all star systems within 20 pc of Earth ordered by right ascension (see the List of nearest stars). Later editions expanded the coverage to 25 pc. Numbers in the range 1.0–915.0 (Gl numbers) are from the second edition, which was
The integers up to 915 represent systems which were in the first edition. Numbers with a decimal point were used to insert new star systems for the second edition without destroying the desired order (by right ascension). This catalogue is referred to as CNS2, although this name is never used in catalogue numbers.
Numbers in the range 9001–9850 (Wo numbers) are from the supplement
Numbers in the ranges 1000–1294 and 2001–2159 (GJ numbers) are from the supplement
The range 1000–1294 represents nearby stars, while 2001–2159 represents suspected nearby stars. In the literature, the GJ numbers are sometimes retroactively extended to the Gl numbers (since there is no overlap). For example, Gliese 436 can be interchangeably referred to as either Gl 436 or GJ 436.
Numbers in the range 3001–4388 are from
Although this version of the catalogue was termed "preliminary", it is still the current one as of March 2006[ [update]], and is referred to as CNS3. It lists a total of 3,803 stars. Most of these stars already had GJ numbers, but there were also 1,388 which were not numbered. The need to give these 1,388 "some" name has resulted in them being numbered 3001–4388 (NN numbers, for "no name"), and data files of this catalogue now usually include these numbers. An example of a star which is often referred to by one of these unofficial GJ numbers is GJ 3021.
GCTP.
The General Catalogue of Trigonometric Parallaxes, first published in 1952 and later superseded by the New GCTP (now in its fourth edition), covers nearly 9,000 stars. Unlike the Gliese, it does not cut off at a given distance from the Sun; rather it attempts to catalogue all known measured parallaxes. It gives the co-ordinates in 1900 epoch, the secular variation, the proper motion, the weighted average absolute parallax and its standard error, the number of parallax observations, quality of interagreement of the different values, the visual magnitude and various cross-identifications with other catalogues. Auxiliary information, including UBV photometry, MK spectral types, data on the variability and binary nature of the stars, orbits when available, and miscellaneous information to aid in determining the reliability of the data are also listed.
HIP.
The Hipparcos catalogue was compiled from the data gathered by the European Space Agency's astrometric satellite "Hipparcos", which was operational from 1989 to 1993. The catalogue was published in June 1997 and contains 118,218 stars; an updated version with re-processed data was published in 2007. It is particularly notable for its parallax measurements, which are considerably more accurate than those produced by ground-based observations. See Stellar parallax and List of stars in the Hipparcos Catalogue.
PPM.
The PPM Star Catalogue is one of best, both in the proper motion and star position till 1999. Not as precise as Hipparcos catalogue but with many more stars. The PPM was built from BD, SAO, HD and more, with sophisticated algorithm and is an extension for the Fifth Fundamental Catalogue, "Catalogues of Fundamental Stars".
Proper motion catalogues.
A common way of detecting nearby stars is to look for relatively high proper motions. Several catalogues exist, of which we'll mention a few. The Ross and Wolf catalogues pioneered the domain:
Willem Jacob Luyten later produced a series of catalogues:
L - Luyten, Proper motion stars and White dwarfs
LFT - Luyten Five-Tenths catalogue
LHS - Luyten Half-Second catalogue
LTT - Luyten Two-Tenths catalogue
NLTT - New Luyten Two-Tenths catalogue
LPM - Luyten Proper-Motion catalogue
Around the same time period, Henry Lee Giclas worked on a similar series of catalogs:
uvby98.
The "ubvyβ Photoelectric Photometric Catalogue" is a compilation of previously published photometric data. Published in 1998, the catalogue includes 63,316 stars surveyed through 1996.
Successors to USNO-A, USNO-B, NOMAD, UCAC and Others.
Stars evolve and move over time, making catalogs evolving, impermanent databases at even the most rigorous levels of production. The USNO catalogs are the most current and widely used astrometric catalogs available at present, and include USNO products such as USNO-B (the successor to USNO-A), NOMAD, UCAC and others in production or narrowly released. Some users may see specialized catalogs (more recent versions of the above), tailored catalogs, interferometrically-produced cataloges, dynamic catalogs, and those with updated positions, motions, colors, and improved errors. Catalog data is continually collected at the Naval Observatory dark-sky facility, NOFS; and the latest refined, updated catalogs are reduced and produced by NOFS and the USNO. See the USNO Catalog and Image Servers for more information and access.

</doc>
<doc id="28233" url="http://en.wikipedia.org/wiki?curid=28233" title="Stellar designation">
Stellar designation

Designations of stars (and other celestial bodies) is done by the International Astronomical Union (IAU). Many of the star names in use today were inherited from the time before the IAU existed. Other names, mainly for variable stars (including novae and supernovae), are being added all the time.
Approximately 10,000 stars are visible to the naked eye. Pre-modern catalogues listed only the brightest of these. Hipparchus in the 2nd century BC enumerated about 850 stars. Johann Bayer in 1603 listed about twice this number. Only a minority of these have proper names, all others are designated by catalogization schemes.
Only in the 19th century did star catalogues list the naked-eye stars exhaustively.
The most voluminous modern catalogues list of the order of a billion stars, out of an estimated total of 200 to 400 billion in the Milky Way.
Proper names.
Several hundred of the brightest stars have traditional names, most of which derive from Arabic, but a few from Latin.
There are a number of problems with these names, however:
In practice, the traditional names are only universally used for the very brightest stars (Sirius, Arcturus, Vega, etc.) and for a small number of slightly less bright but "interesting" stars (Algol, Polaris, Mira, etc.). For other naked eye stars, the Bayer designation is often preferred.
In addition to the traditional names, a small number of stars that are "interesting" can have modern English names. For instance Barnard's star has the highest known proper motion of any star and is thus notable even though it is far too faint to be seen with the naked eye. See stars named after people.
Two second-magnitude stars, Alpha Pavonis and Epsilon Carinae, were assigned the proper names Peacock and Avior respectively in 1937 by Her Majesty's Nautical Almanac Office during the creation of The Air Almanac, a navigational almanac for the Royal Air Force. Of the fifty-seven stars included in the new almanac, these two had no classical names. The RAF insisted that all of the stars must have names, so new names were invented for them.
The book "" by R.H.Allen (1899) has had effects on star names:
A few stars are named for individuals. These are mostly unofficial names that became official at some juncture.
The first such case (discounting characters from Greek mythology) was Cor Caroli (α CVn), named in the 17th century for Charles I of England. The remaining examples are mostly stars named after astronomers or astronauts.
Catalogue numbers.
In the absence of any better means of designating a star, catalogue numbers are generally used. A great many different star catalogues are used for this purpose, see star catalogues.
By constellation.
The first modern schemes for designating stars systematically labelled them within their constellation.
Full-sky catalogues.
Full-sky star catalogues detach the star designation from the star's constellation and aim at enumerating all stars with apparent magnitude greater than a given value.
Variable designations.
Variable stars which do not have Bayer designations are given special designations which mark them out as variable stars.
Exoplanet searches.
When a planet is detected around a star, the star is often given a name and number based on the name of the telescope or survey mission that discovered it and based on how many planets have already been discovered by that mission e.g. HAT-P-9, WASP-1, COROT-1, Kepler-4.
Sale of star names.
There are a number of companies that sell naming rights to obscure stars for commemorative purposes. These sales of star names are not recognised by astronomers, nor by any international scientific or registration body. As a result, a single star can potentially be named independently by multiple companies or multiple times by the same company.

</doc>
<doc id="28235" url="http://en.wikipedia.org/wiki?curid=28235" title="Space Shuttle Challenger">
Space Shuttle Challenger

Space Shuttle "Challenger" (NASA Orbiter Vehicle Designation: OV-099) was the second orbiter of NASA's space shuttle program to be put into service following "Columbia". The shuttle was built by Rockwell International's Space Transportation Systems Division in Downey, California. Its maiden flight, STS-6, started on April 4, 1983. It launched and landed nine times before breaking apart 73 seconds into its tenth mission, STS-51-L, on January 28, 1986, resulting in the death of all seven crew members. It was the first of two shuttles to be destroyed in flight. The accident led to a two-and-a-half year grounding of the shuttle fleet; flights resumed in 1988 with STS-26 flown by "Discovery". "Challenger" itself was replaced by "Endeavour" which was built using structural spares ordered by NASA as part of the construction contracts for "Discovery" and "Atlantis". "Endeavour" was launched for the first time in May 1992.
History.
"Challenger" was named after HMS "Challenger", a British corvette that was the command ship for the Challenger Expedition, a pioneering global marine research expedition undertaken from 1872 through 1876. The Apollo 17 lunar module that landed on the Moon in 1972 was also named "Challenger".
Construction.
Because of the low production of orbiters, the Space Shuttle program decided to build a vehicle as a Structural Test Article, STA-099, that could later be converted to a flight vehicle. The contract for STA-099 was awarded to North American Rockwell on July 26, 1972, and its construction was completed in February 1978. After STA-099's rollout, it was sent to a Lockheed test site in Palmdale, where it spent over 11 months in vibration tests designed to simulate entire shuttle flights, from launch to landing. In order to prevent damage during structural testing, qualification tests were performed to a factor of safety of 1.2 times the design limit loads. The qualification tests were used to validate computational models, and compliance with the required 1.4 factor of safety was shown by analysis. STA-099 was essentially a complete airframe of a Space Shuttle orbiter, with only a mockup crew module installed and thermal insulation placed on its forward fuselage.
NASA planned to refit the prototype orbiter "Enterprise" (OV-101), used for flight testing, as the second operational orbiter, but design changes made during construction of the first orbiter, "Columbia" (OV-102), would have required extensive work. Because STA-099's qualification testing prevented damage, NASA found that rebuilding STA-099 as OV-099 would be less expensive than refitting "Enterprise." Work on converting STA-099 into "Challenger" began in January 1979, starting with just the crew module (the pressurized portion of the vehicle) as the rest of the orbiter was still used by Lockheed. STA-099 returned to the Rockwell plant in November 1979, and the original unfinished crew module was replaced with the newly constructed model. Major portions of STA-099, including the payload bay doors, body flap, wings and vertical stabilizer, also had to be returned to their individual subcontractors for rework. By early 1981, most of these components had returned to Palmdale and were reinstalled on the orbiter. Work continued on the conversion until July 1982.
"Challenger" (and the orbiters built after it) had fewer tiles in its Thermal Protection System than Columbia, though it still made heavy use of the white LRSI tiles on the cabin and main fuselage compared to the later orbiters. Most of the tiles on the payload bay doors, upper wing surfaces, and rear fuselage surfaces were replaced with DuPont white Nomex felt insulation. These modifications as well as an overall lighter structure allowed "Challenger" to carry 2,500 lb (1,100 kg) more payload than "Columbia." "Challenger's" fuselage and wings were also stronger than "Columbia's" despite being lighter. The hatch and vertical stabilizer tile patterns were also different from that of the other orbiters. "Challenger" was also the first orbiter to have a head-up display system for use in the descent phase of a mission, and the first to feature Phase I main engines rated for 104% maximum thrust.
Flights and modifications.
After its first flight in April 1983, "Challenger" quickly became the workhorse of NASA's Space Shuttle fleet, flying far more missions per year than "Columbia." In 1983 and 1984, "Challenger" flew on 85% of all Space Shuttle missions. Even when the orbiters "Discovery" and "Atlantis" joined the fleet, "Challenger" flew three missions a year from 1983 to 1985. "Challenger," along with "Discovery," was modified at Kennedy Space Center to be able to carry the Centaur-G upper stage in its payload bay. If flight STS-51-L had been successful, "Challenger"'s next mission would have been the deployment of the Ulysses probe with the Centaur to study the polar regions of the Sun.
"Challenger" flew the first American woman, African-American, and Canadian into space; three Spacelab missions; and performed the first night launch and night landing of a Space Shuttle. "Challenger" was also the first space shuttle to be destroyed in an accident during a mission. The collected debris of the vessel is currently buried in decommissioned missile silos at Launch Complex 31, Cape Canaveral Air Force Station. From time to time, further pieces of debris from the orbiter wash up on the Florida coast. When this happens, they are collected and transported to the silos for storage. Because of its early loss, "Challenger" was the only space shuttle that never wore the NASA "meatball" logo, and was never modified with the MEDS "glass cockpit". The tail was never fitted with a drag chute – it was fitted to the remaining orbiters in 1992.
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="28236" url="http://en.wikipedia.org/wiki?curid=28236" title="Space Shuttle Enterprise">
Space Shuttle Enterprise

The Space Shuttle "Enterprise" (NASA Orbiter Vehicle Designation: OV-101) was the first Space Shuttle. It was built for NASA as part of the Space Shuttle program to perform test flights in the atmosphere, aided by a modified Boeing 747. It was constructed without engines or a functional heat shield, and was therefore not capable of spaceflight. It was unveiled on September 17, 1976.
Originally, "Enterprise" had been intended to be refitted for orbital flight to become the second space-rated orbiter in service. However, during the construction of "Columbia", details of the final design changed, particularly with regard to the weight of the fuselage and wings. Refitting "Enterprise" for spaceflight would have involved dismantling the orbiter and returning the sections to subcontractors across the country. As this was an expensive proposition, it was determined to be less costly to build "Challenger" around a body frame (STA-099) that had been built as a test article. Similarly, "Enterprise" was considered for refit to replace "Challenger" after the latter was destroyed, but "Endeavour" was built from structural spares instead.
Differences between "Enterprise" and future shuttles.
The design of OV-101 was not the same as that planned for OV-102, the first flight model; the aft fuselage was constructed differently, and it did not have the interfaces to mount OMS pods. A large number of subsystems—ranging from main engines to radar equipment—were not installed on "Enterprise", but the capacity to add them in the future was retained, as NASA originally intended to refit the orbiter for spaceflight at the conclusion of its testing. Instead of a thermal protection system, its surface was primarily covered with simulated tiles made from polyurethane foam. Fiberglass was used for the leading edge panels in place of the reinforced carbon-carbon ones of spaceflight-worthy orbiters. Only a few sample thermal tiles and some Nomex blankets were real. "Enterprise" used fuel cells to generate its electrical power, but these were not sufficient to power the orbiter for spaceflight.
"Enterprise" also lacked RCS thrusters (which were useless in atmospheric flight) and hydraulic mechanisms for the landing gear; the landing gear doors were simply opened through the use of explosive bolts and the gear dropped down solely by gravity. As it was only used for atmospheric testing, "Enterprise" featured a large air spike mounted on its nose cap which had been used on the U-2 spy plane.
"Enterprise" was equipped with Lockheed-manufactured zero-zero ejection seats like those its sister "Columbia" had carried on its first four missions.
Service.
Construction began on "Enterprise" on June 4, 1974. Designated OV-101, it was originally planned to be named "Constitution" and unveiled on Constitution Day, September 17, 1976. A letter-writing campaign by fans to President Gerald Ford asked that the orbiter be named after the Starship "Enterprise". Although Ford did not mention the campaign, the president, saying he was "partial to the name" "Enterprise", directed NASA officials to change the name.
In mid-1976 the orbiter was used for ground vibration tests, allowing engineers to compare data from an actual flight vehicle with theoretical models.
On September 17, 1976, "Enterprise" was rolled out of Rockwell's plant at Palmdale, California. In recognition of its fictional namesake, "Star Trek" creator Gene Roddenberry and most of the principal cast of the original series of "Star Trek" were on hand at the dedication ceremony.
Approach and landing tests (ALT).
On January 31, 1977, it was taken by road to Dryden Flight Research Center at Edwards Air Force Base, to begin operational testing.
While at NASA Dryden "Enterprise" was used by NASA for a variety of ground and flight tests intended to validate aspects of the shuttle program. The initial nine-month testing period was referred to by the acronym ALT, for "Approach and Landing Test". These tests included a maiden "flight" on February 18, 1977, atop a Boeing 747 Shuttle Carrier Aircraft (SCA) to measure structural loads and ground handling and braking characteristics of the mated system. Ground tests of all orbiter subsystems were carried out to verify functionality prior to atmospheric flight.
The mated "Enterprise"/SCA combination was then subjected to five test flights with "Enterprise" unmanned and unactivated. The purpose of these test flights was to measure the flight characteristics of the mated combination. These tests were followed with three test flights with "Enterprise" manned to test the shuttle flight control systems.
On August 12, 1977, the space shuttle "Enterprise" flew on its own for the first time. "Enterprise" underwent four more free flights where the craft separated from the SCA and was landed under astronaut control. These tests verified the flight characteristics of the orbiter design and were carried out under several aerodynamic and weight configurations. The first three flights were flown with a tailcone placed at the end of "Enterprise's" aft fuselage, which reduced drag and turbulence when mated to the SCA. The final two flights saw the tailcone removed and mockup main engines installed. On the fifth and final glider flight, pilot-induced oscillation problems were revealed, which had to be addressed before the first orbital launch occurred.
Mated Vertical Ground Vibration Test (MGVT).
Following the conclusion of the ALT test flight program, on March 13, 1978, "Enterprise" was flown once again, but this time half way across the country to NASA's Marshall Space Flight Center (MSFC) in Alabama for the Mated Vertical Ground Vibration Testing (MGVT). The orbiter was lifted up on a sling very similar to the one used at Kennedy Space Center and placed inside the Dynamic Test Stand building, and there mated to the Vertical Mate Ground Vibration Test tank (VMGVT-ET), which in turn was attached to a set of inert Solid Rocket Boosters (SRB) to form a complete shuttle launch stack, and marked the first time in the program's history that all Space Shuttle elements, an Orbiter, an External Tank (ET), and two SRBs, were mated together. During the course of the program, "Enterprise" and the rest of the launch stack would be exposed to a punishing series of vibration tests simulating as closely as possible those expected during various phases of launch, some tests with and others without the SRBs in place.
Planned preparations for spaceflight.
At the conclusion of this testing, "Enterprise" was supposed to be taken back to Palmdale for retrofitting as a fully spaceflight capable vehicle. Under this arrangement, "Enterprise" would be launched on its maiden spaceflight in July 1981 to launch a communications satellite and retrieve the Long Duration Exposure Facility, then planned for a 1980 release on the first operational orbiter, "Columbia". Afterwards, "Enterprise" would conduct two Spacelab missions. However, in the period between the rollout of "Enterprise" and the rollout of "Columbia", a number of significant design changes had taken place that meant retrofitting the prototype would end up being a much more intensive and expensive process than previously realized. As a consequence, NASA took the option to convert the incomplete Structural Test Article, numbered STA-099, which had been built to undergo a variety of stress tests, into a full up flight worthy orbiter, which became "Challenger".
Preparation for STS-1.
Following the MGVT program and with the decision to not use "Enterprise" for orbital missions, it was ferried on April 10, 1979, to KSC. By June 1979, it was again mated with an external tank and solid rocket boosters (known as a boilerplate configuration) and tested in a launch configuration at Kennedy Space Center Launch Pad 39A for a series of fit checks of the facilities there.
Retirement.
With the completion of critical testing, "Enterprise" was returned to Rockwell's plant in Palmdale in October 1979 and was partially disassembled to allow certain components to be reused in other shuttles. After this period, "Enterprise" was returned to NASA's Dryden Flight Research Facility in September 1981. During 1983 and 1984, "Enterprise" underwent an international tour visiting France, Germany, Italy, the UK, Canada, and the American states of California, Alabama, and Louisiana (during the 1984 Louisiana World Exposition). It was also used to fit-check the never-used shuttle launch pad at Vandenberg AFB, California. Finally, on November 18, 1985, "Enterprise" was ferried to Washington, D.C., where it became property of the Smithsonian Institution.
Post-"Challenger".
After the "Challenger" disaster, NASA considered using "Enterprise" as a replacement. Refitting the shuttle with all of the necessary equipment for it to be used in space was considered, but NASA decided to use spares constructed at the same time as "Discovery" and "Atlantis" to build "Endeavour".
Post-"Columbia".
In 2003 after the breakup of "Columbia" during re-entry, the Columbia Accident Investigation Board conducted tests at Southwest Research Institute, which used an air gun to shoot foam blocks of similar size, mass and speed to that which struck "Columbia" at a test structure which mechanically replicated the orbiter wing leading edge. They removed a fiberglass panel from "Enterprise"'s wing to perform analysis of the material and attached it to the test structure, then shot a foam block at it. While the panel was not broken as a result of the test, the impact was enough to permanently deform a seal. Since the reinforced carbon-carbon (RCC) panel on "Columbia" had only 40% of the strength of the test panel from "Enterprise," this result suggested that the RCC leading edge would have been shattered. Additional tests on the fiberglass were canceled in order not to risk damaging the test apparatus, and a panel from "Discovery" was tested to determine the effects of the foam on a similarly-aged RCC leading edge. On July 7, 2003, a foam impact test created a hole 41 by in the protective RCC panel. The tests clearly demonstrated that a foam impact of the type "Columbia" sustained could seriously breach the protective RCC panels on the wing leading edge.
The board determined that the probable cause of the accident was that the foam impact caused a breach of a reinforced carbon-carbon panel along the leading edge of "Columbia's" left wing, allowing hot gases generated during re-entry to enter the wing and cause structural collapse. This caused "Columbia" to tumble out of control, breaking up with the loss of the entire crew.
Museum exhibit.
Washington, D.C..
"Enterprise" was stored at the Smithsonian's hangar at Washington Dulles International Airport before it was restored and moved to the newly built Smithsonian's National Air and Space Museum's Steven F. Udvar-Hazy Center at Dulles International Airport, where it was the centerpiece of the space collection. On April 12, 2011, NASA announced that Space Shuttle "Discovery", the most traveled orbiter in the fleet, would be added to the collection once the Shuttle fleet was retired. On April 17, 2012, "Discovery" was transported by Shuttle Carrier Aircraft to Dulles from Kennedy Space Center, where it made several passes over the Washington D.C. metro area.
New York.
On December 12, 2011, ownership of the "Enterprise" was officially transferred to the Intrepid Sea, Air & Space Museum in New York City. In preparation for the anticipated relocation, engineers evaluated the vehicle in early 2010 and determined that it was safe to fly on the Shuttle Carrier Aircraft once again. At approximately 9:40 am Eastern Daylight Time on April 27, 2012 "Enterprise" took off from Dulles International Airport en-route to a fly-by over the Hudson River, New York's JFK International Airport, the Statue of Liberty, the George Washington and Verrazano-Narrows Bridges, and several other landmarks in the city, in an approximately 45-minute "final tour". At 11:23 am Eastern Daylight Time "Enterprise" touched down at JFK International Airport.
The mobile Mate-Demate Device and cranes were transported from Dulles to the ramp at JFK and the shuttle was removed from the SCA overnight on May 12, 2012, placed on a specially designed flat bed trailer and returned to Hangar 12. On June 3 a Weeks Marine barge took Enterprise to Jersey City. The Shuttle sustained cosmetic damage to a wingtip when a gust of wind blew the barge towards a piling. It was hoisted June 6 onto the Intrepid Museum in Manhattan.
The "Enterprise" went on public display on July 19, 2012, at the Intrepid Museum's new Space Shuttle Pavilion, a temporary shelter consisting of a pressurized, air-supported fabric bubble constructed on the aft end of the carrier's flight deck.
On October 29, 2012, storm surges from Hurricane Sandy caused Pier 86, including the Intrepid Museum's visitor center, to flood, and knocked out the museum's electrical power and both backup generators. The loss of power caused the Space Shuttle Pavilion to deflate, and high winds from the hurricane caused the fabric of the Pavilion to tear and collapse around the orbiter. Minor damage was spotted on the vertical stabilizer of the orbiter, as a portion of the tail fin above the rudder/speedbrake had broken off. The broken section was recovered by museum staff. While the pavilion itself could not be replaced for some time in 2013, the museum erected scaffolding and sheeting around "Enterprise" to protect it from the environment.
By April 2013, the damage sustained to "Enterprise"'s vertical stabilizer had been fully repaired, and construction work on the structure for a new pavilion was under way. The exhibit was closed due to damage from Hurricane Sandy. The pavilion and exhibit reopened on July 10, 2013.
The "Enterprise" was listed on the National Register of Historic Places on March 13, 2013, reference number 13000071, in recognition of its role in the development of the Space Shuttle Program. The historic significance criteria are in space exploration, transportation, and engineering.
References.
 This article incorporates  from websites or documents of the .
</dl>

</doc>
<doc id="28237" url="http://en.wikipedia.org/wiki?curid=28237" title="Space Shuttle Columbia">
Space Shuttle Columbia

Space Shuttle "Columbia" (NASA Orbiter Vehicle Designation: OV-102) was the first space-rated Space Shuttle in NASA's orbiter fleet. It launched for the first time on mission STS-1 on 12 April 1981, the first flight of the Space Shuttle program. Over 22 years of service it completed 27 missions before disintegrating during re-entry near the end of its 28th mission, STS-107 on 1 February 2003, resulting in the deaths of all seven crew members.
History.
Construction began on "Columbia" in 1975 at Rockwell International's (formerly North American Aviation/North American Rockwell) principal assembly facility in Palmdale, California, a suburb of Los Angeles. "Columbia" was named after the historical poetic name for the United States of America, like the explorer ship of Captain Robert Gray and the Command Module of Apollo 11, the first manned landing on another celestial body. "Columbia" was also the female symbol of the United States. After construction, the orbiter arrived at Kennedy Space Center on March 25, 1979, to prepare for its first launch. "Columbia" was originally scheduled to lift off in late 1979, however the launch date was delayed by problems with both the SSME components, as well as the thermal protection system (TPS). On March 19, 1981, during preparations for a ground test, workers were asphyxiated while working in Columbia's nitrogen-purged aft engine compartment, resulting in (variously reported) two or three fatalities.
The first flight of "Columbia" (STS-1) was commanded by John Young, a veteran from the Gemini and Apollo programs who was the ninth person to walk on the Moon in 1972, and piloted by Robert Crippen, a rookie astronaut originally selected to fly on the military's Manned Orbital Laboratory (MOL) spacecraft, but transferred to NASA after its cancellation, and served as a support crew member for the Skylab and Apollo-Soyuz missions.
"Columbia" spent 610 days in the Orbiter Processing Facility (OPF), another 35 days in the Vehicle Assembly Building (VAB), and 105 days on Pad 39A before finally lifting off. "Columbia" was successfully launched on April 12, 1981, the 20th anniversary of the first human spaceflight (Vostok 1), and returned on April 14, 1981, after orbiting the Earth 36 times, landing on the dry lakebed runway at Edwards Air Force Base in California. "Columbia" then undertook three further research missions to test its technical characteristics and performance. Its first operational mission, with a four-man crew, was STS-5, which launched on November 11, 1982. At this point "Columbia" was joined by "Challenger", which flew the next three shuttle missions, while "Columbia" underwent modifications for the first Spacelab mission.
In 1983, "Columbia", under the command of John Young on what was his sixth spaceflight, undertook its second operational mission (STS-9), in which the Spacelab science laboratory and a six-person crew was carried, including the first non-American astronaut on a space shuttle, Ulf Merbold. After the flight, "Columbia" spent 18 months at the Rockwell Palmdale facility beginning in January 1984, undergoing modifications that removed the Orbiter Flight Test hardware and bringing it up to similar specifications as that of its sister orbiters. At that time the shuttle fleet was expanded to include "Discovery" and "Atlantis".
"Columbia" returned to space on January 12, 1986, with the launch of STS-61-C. The mission's crew included Dr. Franklin Chang-Diaz, as well as the first sitting member of the House of Representatives to venture into space, Bill Nelson.
The next shuttle mission, STS-51-L, was undertaken by "Challenger". It was launched on January 28, 1986, ten days after STS-61-C had landed, and ended in disaster 73 seconds after launch. In the aftermath NASA's shuttle timetable was disrupted, and "Columbia" was not flown again until 1989 (on STS-28), after which it resumed normal service as part of the shuttle fleet.
STS-93, launched on July 23, 1999, was the first U.S. space mission with a female commander, Lt. Col. Eileen Collins. This mission deployed the Chandra X-ray Observatory.
"Columbia"'s final successful mission was STS-109, the fourth servicing mission for the Hubble Space Telescope. Its next mission, STS-107, culminated in the orbiter's loss when it disintegrated during reentry, killing all seven of its crew.
Consequently, President Bush decided to retire the Shuttle orbiter fleet by 2010 in favor of the Constellation program and its manned Orion spacecraft. The Constellation program was later cancelled with the NASA Authorization Act of 2010 signed by President Obama on October 11.
Prototype orbiter.
Weight.
As the second orbiter to be constructed, and the first able to fly into space, "Columbia" was roughly 8000 lb heavier than subsequent orbiters such as "Endeavour", which were of a slightly different design, and had benefited from advances in materials technology. In part, this was due to heavier wing and fuselage spars, the weight of early test instrumentation that remained fitted to the avionics suite, and an internal airlock that, originally fitted into the other orbiters, was later removed in favor of an external airlock to facilitate Shuttle/Mir and Shuttle/International Space Station dockings. Due to its weight, "Columbia" could not have used the planned Centaur-G booster (cancelled after the loss of "Challenger"). The retention of the internal airlock allowed NASA to use "Columbia" for the STS-109 Hubble Space Telescope servicing mission, along with the Spacehab double module used on STS-107. Due to "Columbia's" heavier weight, it was less ideal for NASA to use it for missions to the International Space Station, though modifications were made to the Shuttle during its last refit in case the spacecraft was needed for such tasks.
Thermal protection system.
Externally, "Columbia" was the first orbiter in the fleet whose surface was mostly covered with High & Low Temperature Reusable Surface Insulation (HRSI/LRSI) tiles as its main thermal protection system (TPS), with white silicone rubber-painted Nomex — known as Felt Reusable Surface Insulation (FRSI) blankets – in some areas on the wings, fuselage and payload bay doors. FRSI once covered almost 25% of the orbiter; the first upgrade resulted in its removal from many areas, and in later flights it was only used on the upper section of the payload bay doors and inboard sections of the upper wing surfaces. The upgrade also involved replacing many of the white LRSI tiles on the upper surfaces with Advanced Flexible Reusable Surface Insulation (AFRSI) blankets (also known as Fibrous Insulation Blankets, or FIBs) that had been used on "Discovery" and "Atlantis". Originally, "Columbia" had 32,000 tiles – the upgrade reduced this to 24,300. The AFRSI blankets consisted of layers of pure silica felt sandwiched between a layer of silica fabric on the outside and S-Glass fabric on the inside, stitched together using pure silica thread in a 1-inch grid, then coated with a high-purity silica coating. The blankets were semi-rigid and could be made as large as 30" by 30". Each blanket replaced as many as 25 tiles and was bonded directly to the orbiter. The direct application of the blankets to the orbiter resulted in weight reduction, improved durability, reduced fabrication and installation cost, and reduced installation schedule time. All of this work was performed during "Columbia's" first retrofitting and the post-"Challenger" stand-down.
Despite refinements to the orbiter's thermal protection system and other enhancements, "Columbia" would never weigh as little unloaded as the other orbiters in the fleet. The next-oldest shuttle, "Challenger", was also relatively heavy, although 2200 lb lighter than "Columbia".
Markings and insignia.
Until its last refit, "Columbia" was the only operational orbiter with wing markings consisting of an American flag on the port (left) wing and the letters "USA" on the starboard (right) wing. "Challenger", "Discovery", "Atlantis" and "Endeavour" all, until 1998, bore markings consisting of the letters "USA" below an American flag on the left wing, and the pre-1998 NASA "worm" logo afore the respective orbiter's name on the right wing. ("Enterprise", the test vehicle which was the prototype for "Columbia", originally had the same wing markings as "Columbia" but with the letters "USA" on the right wing spaced closer together; "Enterprise"'s markings were modified to match "Challenger" in 1983.) The name of the orbiter was originally placed on the payload bay doors much like "Enterprise" but was placed on the crew cabin after the "Challenger" disaster so that the orbiter could be easily identified while in orbit. From its last refit to its destruction, "Columbia" bore markings identical to those of its operational sister orbiters – the NASA "meatball" logo on the left wing and the American flag afore the orbiter's name on the right; only "Columbia's" distinctive wing "chines" remained. These black areas on the upper surfaces of the shuttle's forward wing were added because, at first, shuttle designers did not know how reentry heating would affect the craft's upper wing surfaces. The "chines" allowed "Columbia" to be easily recognized at a distance, as opposed to the subsequent orbiters. The "chines" were added after "Columbia" arrived at KSC in 1979.
SILTS pod.
Another unique external feature, termed the "SILTS" pod, was located on the top of "Columbia's" vertical stabilizer, and was installed after STS-9 to acquire infrared and other thermal data. Though the pod's equipment was removed after initial tests, NASA decided to leave it in place, mainly to save costs, along with the agency's plans to use it for future experiments. The vertical stabilizer was later modified to incorporate the drag chute first used on "Endeavour" in 1992.
Other upgrades.
"Columbia" was also originally fitted with Lockheed-built ejection seats identical to those found on the SR-71 Blackbird. These were active for the four orbital test flights, but deactivated after STS-4, and removed entirely after STS-9. "Columbia" was also the only spaceworthy orbiter not delivered with head-up displays for the Commander and Pilot, although these were incorporated after STS-9. Like its sister ships, "Columbia" was eventually retrofitted with the new MEDS "glass cockpit" display and lightweight seats.
Future.
Had "Columbia" not been destroyed, it would have been fitted with the external airlock/docking adapter for STS-118, an International Space Station assembly mission, originally planned for November 2003. "Columbia" was scheduled for this mission due to "Discovery" being out of service for its Orbital Maintenance Down Period, and because the ISS assembly schedule could not be adhered to with only "Endeavour" and "Atlantis".
"Columbia"’s 'career' would have started to wind down after STS-118. It was to service the Hubble Space Telescope two more times between 2004 and 2005, but no more missions were planned for it again except for a mission designated STS-144 where it would retrieve the Hubble Space Telescope from orbit and bring it back to Earth. Following the "Columbia" accident, NASA flew the STS-125 mission using "Atlantis", combining the planned fourth and fifth servicing missions into one final message to Hubble. Because of the retirement of the Space Shuttle fleet, the batteries and gyroscopes that keep the telescope pointed will eventually fail also because of the magnifier screen, which would result in its reentry and break-up in Earth's atmosphere. A "Soft Capture Docking Mechanism", based on the docking adapter that was to be used on the Orion spacecraft, was installed during the last servicing mission in anticipation of this event.
"Columbia" was also scheduled to launch the X-38 V-201 Crew Return Vehicle prototype as the next mission after STS-118, until the cancellation of the project in 2002.
Flights.
"Columbia" flew 28 missions, gathering 300.74 days spent in space with 4,808 orbits and a total distance of 125204911 mi up until STS-107.
Despite being in service during the Shuttle-Mir and International Space Station programs, "Columbia" did not fly any missions that visited a space station. The other three active orbiters at the time had visited both "Mir" and the ISS at least once. "Columbia" was not suited for high-inclination missions.
Final mission and destruction.
"Columbia" was destroyed at about 09:00 EST on February 1, 2003 while re-entering the atmosphere after a 16-day scientific mission. The Columbia Accident Investigation Board determined that a hole was punctured in the leading edge on one of "Columbia's" wings, made of a carbon composite. The hole had formed when a piece of insulating foam from the external fuel tank peeled off during the launch 16 days earlier and struck the shuttle's left wing. During the intense heat of re-entry, hot gases penetrated the interior of the wing, destroying the support structure and causing the rest of the shuttle to break apart. The nearly 84,000 pieces of collected debris of the vessel are stored in a 16th floor office suite in the Vehicle Assembly Building at the Kennedy Space Center. The collection was opened to the media once and has since been open only to researchers. Unlike "Challenger", which had a replacement orbiter built, "Columbia" did not.
The seven crew members who died aboard this final mission were: Rick Husband, Commander; William C. McCool, Pilot; Michael P. Anderson, Payload Commander; David M. Brown, Mission Specialist 1; Kalpana Chawla, Mission Specialist 2; Laurel Clark, Mission Specialist 3; and Ilan Ramon, Payload Specialist 1.
Tributes and memorials.
The debris field encompassed hundreds of miles across Northeast Texas and into Louisiana. The nose cap and remains of all seven crew members were found in Sabine County, East Texas.
To honor those who lost their lives aboard the shuttle and during the recovery efforts, the "Remembering Columbia" was opened in Hemphill, Sabine County, Texas. The museum tells the story of Space Shuttle Columbia explorations throughout all its missions, including the final STS-107. Its exhibits also show the efforts of local citizens during the recovery period of the "Columbia" shuttle debris and its crew's remains. An area is dedicated to each STS-107 crew member, and also to the Texas Forest Service helicopter pilot who died in the recovery effort. The museum houses many objects and artifacts from: NASA and its contractors; the families of the STS-107 crew; and other individuals. The crew's families contributed personal items of the crew members to be on permanent display. The museum features two interactive simulator displays that emulate activities of the shuttle and orbiter. The digital learning center and its classroom provide educational opportunities for all ages.
The is the U.S. national memorial for the Space Shuttle Columbia’s seven crew members. It is located in Downey on the site of the Space Shuttle's origins and production, the former North American Aviation plant in Los Angeles County, southern California. The facility is also a hands-on learning center with interactive exhibits, workshops, and classes about space science, astronautics, and the Space Shuttle program's legacy — providing educational opportunities for all ages.
The shuttle's final crew was honored in 2003 when the United States Board on Geographic Names approved the name Columbia Point for a 13980 ft mountain in Colorado's Sangre de Cristo Mountains, less than a half-mile from Challenger Point, a peak named after America's other lost shuttle. The Columbia Hills on Mars were also named in honor of the crew, and a host of other memorials were dedicated in various forms.
The Columbia supercomputer at the NASA Advanced Supercomputing (NAS) Division located at Ames Research Center in California was named in honor of the crew lost in the 2003 disaster. Built as a joint effort between NASA and technical partners SGI and Intel in 2004, the supercomputer was used in scientific research of space, the Earth's climate, and aerodynamic design of space launch vehicles and aircraft. The first part of the system, built in 2003, was dedicated to STS-107 astronaut and engineer Kalpana Chawla, who prior to joining the Space Shuttle program worked at Ames Research Center.
Media tributes.
Guitarist Steve Morse of the rock band Deep Purple wrote the instrumental "Contact Lost" in response to the news of the tragedy, recorded by Deep Purple and featured as the closing track on their 2003 album "Bananas". It was dedicated to the astronauts whose lives were lost in the disaster. Morse donated songwriting royalties to the families of lost astronauts. Astronaut and mission specialist engineer Kalpana Chawla, one of the victims of the accident, was a fan of Deep Purple and had exchanged e-mails with the band during the flight, making the tragedy even more personal for the group. She took three CDs into space with her, two of which were Deep Purple albums ("Machine Head" and "Purpendicular"). Both CDs survived the destruction of the shuttle and the 39-mile plunge.
The musical group Echo's Children included singer-songwriter Cat Faber's "Columbia" on their final album "From the Hazel Tree."
The Long Winters band's 2005 album "Ultimatum" features the song "The Commander Thinks Aloud", a tribute to the final "Columbia" crew.
The Eric Johnson instrumental "Columbia" from his 2005 album "Bloom" was written as a commemoration and tribute to the lives that were lost. Johnson said "I wanted to make it more of a positive message, a salute, a celebration rather than just concentrating on a few moments of tragedy, but instead the bigger picture of these brave people’s lives."
The graphic novel Orbiter by Warren Ellis and Colleen Doran was dedicated to the "lives, memories and legacies of the seven astronauts lost on space shuttle Columbia during mission STS-107."
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="28238" url="http://en.wikipedia.org/wiki?curid=28238" title="Space Shuttle Discovery">
Space Shuttle Discovery

Space Shuttle "Discovery" (Orbiter Vehicle Designation: OV-103) is one of the three orbiters from NASA's Space Shuttle program and the third of five built. Its first mission, STS-41-D, flew from August 30 to September 5, 1984. Over 27 years of service it launched and landed 39 times, gathering more spaceflights than any other spacecraft to date.
"Discovery" became the third operational orbiter to enter service, preceded by "Columbia" and "Challenger". It embarked on its last mission, STS-133, on February 24, 2011 and touched down for the final time at Kennedy Space Center on March 9, having spent a cumulative total of almost a full year in space. "Discovery" performed both research and International Space Station (ISS) assembly missions. It also carried the Hubble Space Telescope into orbit. "Discovery" was the first operational shuttle to be retired, followed by "Endeavour" and then "Atlantis".
History.
The name "Discovery" was chosen to carry on a tradition based on ships of exploration, primarily HMS "Discovery", one of the ships commanded by Captain James Cook during his third and final major voyage from 1776 to 1779, and Henry Hudson's "Discovery", which was used in 1610–1611 to explore the Hudson Bay and search for a Northwest Passage. Other ships bearing the name have included the HMS "Discovery" of the 1875–1876 British Arctic Expedition to the North Pole and RRS "Discovery", which led the 1901–1904 "Discovery Expedition" to Antarctica.
"Discovery" launched the Hubble Space Telescope and conducted the second and third Hubble service missions. It also launched the Ulysses probe and three TDRS satellites. Twice "Discovery" was chosen as the "Return To Flight" Orbiter, first in 1988 after the loss of "Challenger" in 1986, and then again for the twin "Return To Flight" missions in July 2005 and July 2006 after the "Columbia" disaster in 2003. Project Mercury astronaut John Glenn, who was 77 at the time, flew with "Discovery" on STS-95 in 1998, making him the oldest person to go into space.
Had plans to launch United States Department of Defense payloads from Vandenberg Air Force Base gone ahead, "Discovery" would have become the dedicated US Air Force shuttle. Its first West Coast mission, STS-62-A, was scheduled for 1986, but cancelled in the aftermath of "Challenger".
"Discovery" was retired after completing its final mission, STS 133 on March 9, 2011. The spacecraft is now on display in Virginia at the Steven F. Udvar-Hazy Center, an annex of the Smithsonian Institution's National Air and Space Museum.
Upgrades and features.
"Discovery" weighed 6870 lb less than "Columbia" when it was brought into service due to optimizations determined during the construction and testing of "Enterprise", "Columbia" and "Challenger". Part of these weight optimizations included the greater use of quilted AFRSI blankets rather than the white LRSI tiles on the fuselage, and the use of graphite epoxy instead of aluminum for the payload bay doors and some of the wing spars and beams.
Upon its delivery to the Kennedy Space Center in 1983, "Discovery" was modified alongside "Challenger" to accommodate the liquid-fueled Centaur-G booster, which had been planned for use beginning in 1986 but was cancelled in the wake of the "Challenger" disaster.
Beginning in late 1995, the orbiter underwent a nine-month Orbiter Maintenance Down Period (OMDP) in Palmdale, California. This included outfitting the vehicle with a 5th set of cryogenic tanks and an external airlock to support missions to the International Space Station. As with all the orbiters, it could be attached to the top of specialized aircraft and did so in June 1996 when it returned to the Kennedy Space Center, and later in April 2012 when sent to the Udvar-Hazy Center, riding piggy-back on a modified Boeing 747.
After STS-105, "Discovery" became the first of the orbiter fleet to undergo Orbiter Major Modification (OMM) period at the Kennedy Space Center. Work began in September 2002 to prepare the vehicle for Return to Flight. This included scheduled upgrades and additional safety modifications. "Discovery" is 6 lb heavier than "Atlantis" and 363 lb heavier than "Endeavour".
Decommissioning and display.
"Discovery" was decommissioned on March 9, 2011.
NASA offered "Discovery" to the Smithsonian Institution's National Air and Space Museum for public display and preservation, after a month-long decontamination process, as part of the national collection. "Discovery" replaced "Enterprise" in the Smithsonian's display at the Steven F. Udvar-Hazy Center in Virginia. Discovery was transported to Washington Dulles International Airport on April 17, 2012, and was transferred to the Udvar-Hazy on April 19 where a welcome ceremony was held. Afterwards, at around 5: 30 pm, Discovery was rolled to its "final wheels stop" in the Udvar Hazy Center.
Flights.
By its last mission, "Discovery" had flown 149 million miles (238 million km) in 39 missions, completed 5,830 orbits, and spent 365 days in orbit in over 27 years. "Discovery" flew more flights than any other Orbiter Shuttle, including four in 1985 alone. "Discovery" flew all three "return to flight" missions after the "Challenger" and "Columbia" disasters: STS-26 in 1988, STS-114 in 2005, and STS-121 in 2006. "Discovery" flew the ante-penultimate mission of the Space Shuttle program, STS-133, having launched on (NET) February 24, 2011. "Endeavour" flew STS-134 and "Atlantis" performed STS-135, NASA's last Space Shuttle mission. On February 24, 2011, Space Shuttle "Discovery" launched from Kennedy Space Center's Launch Complex 39-A to begin its final orbital flight.
Notable missions:
Flow directors.
The Flow Director was responsible for the overall preparation of the shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space shuttle "Discovery"'s Flow Directors were:
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="28239" url="http://en.wikipedia.org/wiki?curid=28239" title="Space Shuttle Atlantis">
Space Shuttle Atlantis

The Space Shuttle "Atlantis" (Orbiter Vehicle Designation: OV‑104) was a Space Shuttle orbiter belonging to the National Aeronautics and Space Administration (NASA), the spaceflight and space exploration agency of the United States. Constructed by the Rockwell International company in Southern California and delivered to the Kennedy Space Center in Eastern Florida in April 1985, "Atlantis" was the fourth operational and the second-to-last Space Shuttle built. Its maiden flight was STS-51-J from 3 to 7 October 1985.
"Atlantis" embarked on its 33rd and final mission, also the final mission of a space shuttle, STS-135, on 8 July 2011. STS-134 by "Endeavour was expected to be the final flight before STS-135 was authorized in October 2010. STS-135 took advantage of the processing for the STS-335 Launch On Need mission that would have been necessary if STS-134's crew became stranded in orbit.
"Atlantis" landed for the final time at the Kennedy Space Center on 21 July 2011.
By the end of its final mission, "Atlantis" had orbited the Earth a total of 4,848 times, traveling nearly 126000000 mi or more than 525 times the distance from the Earth to the Moon.
"Atlantis" was named after RV "Atlantis", a two-masted sailing ship that operated as the primary research vessel for the Woods Hole Oceanographic Institution from 1930 to 1966.
Notable missions.
Space Shuttle "Atlantis" lifted off on its maiden voyage on 3 October 1985, on mission STS-51-J, the second dedicated Department of Defense flight. It flew one other mission, STS-61-B, the second night launch in the shuttle program, before the Space Shuttle Challenger disaster temporarily grounded the shuttle fleet in 1986. Among the five Space Shuttles flown into space, "Atlantis" conducted a subsequent mission in the shortest time after the previous mission (turnaround time) when it launched in November 1985 on STS-61-B, only 50 days after its previous mission, STS-51-J in October 1985. "Atlantis" was then used for ten flights between 1988 and 1992. Two of these, both flown in 1989, deployed the planetary probes Magellan to Venus (on STS-30) and Galileo to Jupiter (on STS-34). With STS-30 "Atlantis" became the first shuttle to launch an interplanetary probe. During another mission, STS-37 flown in 1991, "Atlantis" deployed the Compton Gamma Ray Observatory. Beginning in 1995 with STS-71, "Atlantis" made seven straight flights to the former Russian space station Mir as part of the Shuttle-Mir Program. STS-71 marked a number of firsts in human spaceflight: 100th U.S. manned space flight; first U.S. shuttle-Russian Space Station Mir docking and joint on-orbit operations; and first on-orbit changeout of shuttle crew. When linked, "Atlantis" and "Mir" together formed the largest spacecraft in orbit at the time.
Shuttle "Atlantis" also delivered several vital components for the construction of the International Space Station (ISS). During the February 2001 mission STS-98 to the ISS, "Atlantis" delivered the Destiny Module, the primary operating facility for U.S. research payloads aboard the ISS. The five hour 25 minute third spacewalk performed by astronauts Robert Curbeam and Thomas Jones during STS-98 marked NASA's 100th extra vehicular activity in space. The Quest Joint Airlock, was flown and installed to the ISS by "Atlantis" during the mission STS-104 in July 2001. The successful installation of the airlock gave on-board space station crews the ability to stage repair and maintenance spacewalks outside the ISS using U.S. EMU or Russian Orlan space suits. The first mission flown by "Atlantis" after the Space Shuttle Columbia disaster was STS-115, conducted during September 2006. The mission carried the P3/P4 truss segments and solar arrays to the ISS. On ISS assembly flight STS-122 in February 2008, "Atlantis" delivered the Columbus laboratory to the ISS. Columbus laboratory is the largest single contribution to the ISS made by the European Space Agency (ESA).
In May 2009 "Atlantis" flew a seven member crew to the Hubble Space Telescope for its Servicing Mission 4, STS-125. The mission was a success, with the crew completing five space walks totaling 37 hours to install new cameras, batteries, a gyroscope and other components to the telescope.
The longest mission flown using "Atlantis" was STS-117 which lasted almost 14 days in June 2007. During STS-117, Atlantis' crew added a new starboard truss segment and solar array pair (the S3/S4 truss), folded the P6 array in preparation for its relocation and performed four spacewalks. "Atlantis" was not equipped to take advantage of the Station-to-Shuttle Power Transfer System so missions could not be extended by making use of power provided by ISS.
During the STS-129 post-flight interview on 16 November 2009 shuttle launch director Mike Leinbach said that "Atlantis" officially beat shuttle "Discovery" on the record low amount of Interim Problem Reports, with a total of just 54 listed since returning from the STS-125. He continued to add "It is due to the team and the hardware processing. They just did a great job. The record will probably never be broken again in the history of the Space Shuttle Program, so congratulations to them". During the STS-132 post-launch interview on 14 May 2010, shuttle launch director Mike Leinbach said that "Atlantis" beat its previous record low amount of Interim Problem Reports, with a total of 46 listed between STS-129 and STS-132.
Orbiter Maintenance Down Periods.
"Atlantis" went through two overhauls of scheduled Orbiter Maintenance Down Periods (OMDPs) during its operational history.
"Atlantis" arrived at Palmdale, California in October 1992 for OMDP-1. During that visit 165 modifications were made over the next 20 months. These included the installation of a drag chute, new plumbing lines to configure the orbiter for extended duration, more than 800 new heat tiles and blankets and new insulation for main landing gear and structural modifications to the airframe.
On 5 November 1997, "Atlantis" again arrived at Palmdale for OMDP-2 which was completed on 24 September 1998. The 130 modifications carried out during OMDP-2 included glass cockpit displays, replacement of TACAN navigation with GPS and ISS airlock and docking installation. Several weight reduction modifications were also performed on the orbiter including replacement of Advanced Flexible Reusable Surface Insulation (AFRSI) insulation blankets on upper surfaces with FRSI. Lightweight crew seats were installed and the Extended Duration Orbiter (EDO) package installed on OMDP-1 was removed to lighten "Atlantis" to better serve its prime mission of servicing the ISS.
During the stand down period post "Columbia" accident, "Atlantis" went through over 75 modifications to the orbiter ranging from very minor bolt change-outs to window change-outs and different fluid systems.
Atlantis was known among the shuttle workforce as being more prone than the others in the fleet to problems that needed to be addressed while readying the vehicle for launch leading to some nicknaming it "Britney".
Planned decommissioning.
NASA had planned to withdraw "Atlantis" from service in 2008, as the orbiter would have been due to undergo its third scheduled OMDP. However, because of the final retirement of the shuttle fleet in 2010, this was deemed uneconomical. It was planned that "Atlantis" would be kept in near-flight condition to be used as a parts hulk for "Discovery" and "Endeavour". However, with the significant planned flight schedule up to 2010, the decision was taken to extend the time between OMDPs, allowing "Atlantis" to be retained for operations. "Atlantis" had been swapped for one flight of each "Discovery" and "Endeavour" in the current flight manifest. "Atlantis" had completed what was meant to be its last flight, STS-132, prior to the end of the shuttle program, but the extension of the Shuttle program into 2011 led to "Atlantis" being selected for STS-135, the final Space Shuttle mission in July 2011.
"Atlantis" is currently displayed at the Kennedy Space Center Visitor Complex. NASA Administrator Charles Bolden announced the decision at an employee event held on 12 April 2011 to commemorate the 30th anniversary of the first shuttle flight: "First, here at the Kennedy Space Center where every shuttle mission and so many other historic human space flights have originated, we'll showcase my old friend, Atlantis."
The Visitor Complex displays "Atlantis" suspended with its payload bay doors opened such that it appears to be back in orbit around the Earth. A multi-story digital projection of Earth rotates behind the orbiter in a 64,000 sqft indoor facility. Ground breaking of the facility occurred in 2012.
The exhibit opened on 29 June 2013.
Crews.
A total of 156 individuals flew with Space Shuttle "Atlantis" over the course of its 33 missions. Because the shuttle sometimes flew crew members arriving and departing Mir and the ISS, not all of them launched and landed on "Atlantis".
Astronaut Clayton Anderson, ESA astronaut Leopold Eyharts and Russian cosmonauts Nikolai Budarin and Anatoly Solovyev only launched on "Atlantis". Similarly, astronauts Daniel Tani and Sunita Williams, as well as cosmonauts Vladimir Dezhurov and Gennady Strekalov only landed with "Atlantis". Only 146 men and women both launched and landed aboard "Atlantis".
Some of those people flew with "Atlantis" more than once. Taking them into account, 203 total seats were filled over "Atlantis"‍ '​ 32 missions. Astronaut Jerry Ross holds the record for the most flights aboard "Atlantis" at five.
Astronaut Rodolfo Neri Vela who flew aboard Atlantis on STS-61-B mission in 1985 became the first and so far only Mexican to have traveled to space. ESA astronaut Dirk Frimout who flew on STS-45 as a payload specialist was the first Belgian in space. STS-46 mission specialist Claude Nicollier was the first astronaut from Switzerland. On the same flight, astronaut Franco Malerba became the first citizen of Italy to travel to space.
Astronaut Michael Massimino who flew on STS-125 mission became the first person to use Twitter in space in May 2009.
Having flown aboard "Atlantis" as part of the STS-132 crew in May 2010 and "Discovery" as part of the STS-133 crew in February/March 2011, Stephen Bowen became the first NASA astronaut to be launched on consecutive missions.
Problems.
Composite overwrapped pressure vessels.
NASA announced in 2007 that 24 helium and nitrogen gas tanks in "Atlantis" were older than their designed lifetime. These composite overwrapped pressure vessels (COPV) were designed for a 10-year life and later cleared for an additional 10 years; they exceeded this life in 2005. NASA said it could not guarantee any longer that the vessels on "Atlantis" would not burst or explode under full pressure. Failure of these tanks could have damaged parts of the orbiter and even wound or kill ground personnel. An in-flight failure of a pressure vessel could have even resulted in the loss of the orbiter and its crew. NASA analyses originally assumed that the vessels would leak before they burst, but new tests showed that they could in fact burst before leaking.
Because the original vendor was no longer in business, and a new manufacturer could not be qualified before 2010, when the shuttles were scheduled to be retired, NASA decided to continue operations with the existing tanks. Therefore, to reduce the risk of failure and the cumulative effects of load, the vessels were maintained at 80 percent of the operating pressure as late in the launch countdown as possible, and the launch pad was cleared of all but essential personnel when pressure was increased to 100 percent. The new launch procedure was employed during some the remaining launches of "Atlantis", but was resolved when the two COPVs deemed to have the highest risk of failure were replaced.
Window damage.
After the STS-125 mission, a work light knob was discovered jammed in the space between one of "Atlantis"‍ '​s front interior windows and the Orbiter dashboard structure. The knob was believed to have entered the space during flight, when the pressurized Orbiter was expanded to its maximum size. Then, once back on Earth, the Orbiter contracted, jamming the knob in place. Leaving "as-is" was considered unsafe for flight, and some options for removal (including window replacement) would have included a 6 month delay of "Atlantis"‍ '​s next mission (planned to be STS-129). Had the removal of the knob been unsuccessful, the worst-case scenario was that "Atlantis" could have been retired from the fleet, leaving "Discovery" and "Endeavour" to complete the manifest alone. On 29 June 2009, "Atlantis" was pressurized to 17 psi (3 psi above ambient), which forced the Orbiter to expand slightly. The knob was then frozen with dry ice, and successfully removed. Small areas of damage to the window were discovered where the edges of the knob had been embedded into the pane. Subsequent investigation of the window damage discovered a maximum defect depth of approximately 0.0003 in, less than the reportable depth threshold of 0.0015 in and not serious enough to warrant the pane’s replacement.
In media.
"Atlantis" was the shuttle shown in the 1986 film "SpaceCamp", starring Kate Capshaw, Lea Thompson, Tom Skerritt and Joaquin Phoenix. The premise of the film was a crew of students at United States Space Camp that are accidentally launched into space on-board "Atlantis". "Atlantis" was also featured in the 1998 film "Deep Impact" as the spacecraft used to shuttle the crew to the fictional "Messiah" spacecraft. It is also featured in "Armageddon", a film with a similar plot, in which the shuttle is destroyed in a rogue meteor shower. 
She is involved as well in The Dig, a 1995 science fiction adventure game.

</doc>
<doc id="28240" url="http://en.wikipedia.org/wiki?curid=28240" title="Space Shuttle Endeavour">
Space Shuttle Endeavour

Space Shuttle "Endeavour" (Orbiter Vehicle Designation: OV-105) is a retired orbiter from NASA's Space Shuttle program and the fifth and final operational shuttle built. It embarked on its first mission, STS-49, in May 1992 and its 25th and final mission, STS-134, in May 2011. STS-134 was expected to be the final mission of the Space Shuttle program, but with the authorization of STS-135, "Atlantis" became the last shuttle to fly.
The United States Congress authorized the construction of "Endeavour" in 1987 to replace "Challenger", which was lost in 1986 when it broke up 73 seconds after launch. Structural spares built during the construction of "Discovery" and "Atlantis" were used in its assembly. NASA chose to build "Endeavour" from spares rather than refitting "Enterprise" or accepting a Rockwell International proposal to build two shuttles for the price of one on cost grounds.
History.
The orbiter is named after the British HMS "Endeavour", the ship which took Captain James Cook on his first voyage of discovery (1768–1771). This is why the name is spelled in the British English manner, rather than the American English ("Endeavor"). This has caused confusion, including when NASA itself misspelled a sign on the launch pad in 2007. The name also honored "Endeavour", the Command Module of Apollo 15, which was also named after Cook's ship.
"Endeavour" was named through a national competition involving students in elementary and secondary schools. Entries included an essay about the name, the story behind it and why it was appropriate for a NASA shuttle, and the project that supported the name. "Endeavour" was the most popular entry, accounting for almost one-third of the state-level winners. The national winners were Senatobia Middle School in Senatobia, Mississippi, in the elementary division and Tallulah Falls School in Tallulah Falls, Georgia, in the upper school division. They were honored at several ceremonies in Washington, D.C., including a White House ceremony where then-President George H. W. Bush presented awards to each school.
"Endeavour" was delivered by Rockwell International Space Transportation Systems Division in May 1991 and first launched a year later, in May 1992, on STS-49. Rockwell International claimed that it had made no profit on Space Shuttle "Endeavour", despite construction costing US$2.2 billion.
Service.
On its first mission, it captured and redeployed the stranded "INTELSAT VI" communications satellite. The first African-American woman astronaut, Mae Jemison, was brought into space on the mission STS-47 on September 12, 1992.
"Endeavour" flew the first servicing mission STS-61 for the Hubble Space Telescope in 1993. In 1997 it was withdrawn from service for eight months for a retrofit, including installation of a new airlock. In December 1998, it delivered the Unity Module to the International Space Station.
"Endeavour"'s last Orbiter Major Modification period began in December 2003 and ended on October 6, 2005. During this time, "Endeavour" received major hardware upgrades, including a new, multi-functional, electronic display system, often referred to as a glass cockpit, and an advanced GPS receiver, along with safety upgrades recommended by the Columbia Accident Investigation Board (CAIB) for the shuttle's return to flight following the loss of "Columbia" during reentry on 1 February 2003.
The STS-118 mission, "Endeavour"'s first since the refit, included astronaut Barbara Morgan, formerly assigned to the Teacher in Space project, but now a full member of the Astronaut Corps, as part of the crew. Morgan was the backup for Christa McAuliffe who was on the ill-fated mission STS-51-L in 1986.
Upgrades and features.
As it was constructed later, "Endeavour" was built with new hardware designed to improve and expand orbiter capabilities. Most of this equipment was later incorporated into the other three orbiters during out-of-service major inspection and modification programs. "Endeavour"’s upgrades include:
Modifications resulting from a 2005–2006 refit of "Endeavour" included:
Final flights.
"Endeavour" flew its final mission, STS-134, to the International Space Station (ISS) in May 2011. After the conclusion of STS-134, "Endeavour" was formally decommissioned.
STS-134 was intended to launch in late 2010, but on July 1 NASA released a statement saying the "Endeavour" mission was rescheduled for February 27, 2011.
"The target dates were adjusted because critical payload hardware for STS-133 will not be ready in time to support the previously planned 16 September launch," NASA said in a statement. With the "Discovery" launch moving to November, "Endeavour" mission "cannot fly as planned, so the next available launch window is in February 2011," NASA said, adding that the launch dates were subject to change.
The launch was further postponed until April to avoid a scheduling conflict with a Russian supply vehicle heading for the International Space Station. STS-134 did not launch until 16 May at 08:56 EST.
"Endeavour" landed at the Kennedy Space Center at 06:34 UTC on June 1, 2011, completing its final mission. It was the 25th night landing of a shuttle. Over its flight career, "Endeavour" flew 122,853,151 miles and spent 299 days in space. During "Endeavour's" last mission, the Russian spacecraft Soyuz TMA-20 departed from the ISS and paused at a distance of 200 meters. Italian astronaut Paolo Nespoli took a series of photographs and videos of the ISS with "Endeavour" docked. This was the second time a shuttle was photographed docked and the first time since 1996. Commander Mark Kelly was the last astronaut off "Endeavour" after the landing, and the crew stayed on the landing strip to sign autographs and pose for pictures.
STS-134 was the penultimate space shuttle mission; STS-135 was added to the schedule in January 2011, and in July "Atlantis" flew for the final time.
Decommissioning.
After more than twenty organizations submitted proposals to NASA for the display of an orbiter, NASA announced that "Endeavour" would go to the California Science Center in Los Angeles.
After low level flyovers above NASA and civic landmarks across the country and in California, it was delivered to Los Angeles International Airport (LAX) on September 21, 2012. The orbiter was slowly and carefully transported through the streets of Los Angeles and Inglewood three weeks later, from October 11–14 along La Tijera, Manchester, Crenshaw, and Martin Luther King, Jr. Boulevards to its final destination at the California Science Center in Exposition Park.
"Endeavour" encountered a few obstacles while transiting the streets narrowly missing telephone poles, apartment buildings and other structures. The lack of obstacles was due, in part, to the fact that over 400 old-growth shade trees had been cut down beforehand. The power had to be turned off and power carrying poles had to be removed temporarily as the orbiter crept along Manchester to Prairie Avenue then Crenshaw Boulevard. News crews lined the streets along the path with visible news personalities in the news trucks. There were several police escorts as well as considerable security helping to control the large crowds gathered. "Endeavour" was parked for a few hours at the Great Western Forum where it was available for viewing. The journey was famous for an unmodified Toyota Tundra pickup truck pulling the space shuttle across the Manchester Boulevard Bridge. The space shuttle was mainly carried by four self-propelled robotic dollies throughout the 12 mile journey. However, due to bridge weight restrictions, the space shuttle was moved onto the dolly towed by the Tundra. After it had completely crossed the bridge, the Space Shuttle was returned to the robotic dollies. The footage was later used in a commercial for the 2013 Super Bowl. Having taken longer than expected, "Endeavour" finally reached the Science Center on October 14.
The exhibit was opened to the public on October 30, 2012 at the temporary Samuel Oschin Space Shuttle "Endeavour" Display Pavilion of the museum. A new addition to the Science Center, called the Samuel Oschin Air and Space Center, is under construction as "Endeavour's" permanent home. Planned for a 2017 opening, the "Endeavour" will be mounted vertically with an external tank and a pair of solid rocket boosters in the shuttle stack configuration. One payload door will be open to reveal a demonstration payload inside.
After its decommissioning, "Endeavour's" Canadarm (formally the 'Shuttle Remote Manipulator System') was removed in order to be sent to the Canadian Space Agency’s John H. Chapman Space Centre in Longueuil, Quebec, a suburb of Montreal, where it was to be placed on display. In a Canadian poll on which science or aerospace museum should be selected to display the Canadarm, originally built by SPAR Aerospace, the Canadian Space Agency’s headquarters placed third to last with only 35 out of 638 votes. "Endeavour's" Canadarm has since gone on permanent display at the Canada Aviation and Space Museum in Ottawa.
Flow Directors.
The Flow Director was responsible for the overall preparation of the shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space shuttle "Endeavour"'s Flow Directors were:
California Science Center.
Endeavour is currently housed in the Samuel Oschin Pavilion. A companion exhibit, Endeavour: The California Story, features images and artifacts that relate the shuttle program to California, where the orbiters were originally constructed.

</doc>
<doc id="28242" url="http://en.wikipedia.org/wiki?curid=28242" title="Sports Car Club of America">
Sports Car Club of America

The Sports Car Club of America (SCCA) is a club and sanctioning body supporting road racing, rallying, and autocross in the United States. Formed in 1944, it runs many programs for both amateur and professional racers.
History.
The SCCA traces its roots to the Automobile Racing Club of America (not to be confused with the current stock car series of the same name). ARCA was founded in 1933 by brothers Miles and Sam Collier, and dissolved in 1941 at the outbreak of World War II. The SCCA was formed in 1944, at first as only an enthusiast group. The SCCA began sanctioning road racing in 1948, with the inaugural Watkins Glen Grand Prix. Cameron Argetsinger, an SCCA member and local enthusiast who would later become Director of Pro Racing and Executive Director of the SCCA, helped organize the event for the SCCA.
In 1951, the SCCA National Sports Car Championship was formed from existing marquee events around the nation, including Watkins Glen, Pebble Beach, and Elkhart Lake. Many early SCCA events were held on disused air force bases, organized with the help of Air Force General Curtis LeMay, a renowned enthusiast of sports car racing. LeMay loaned out facilities of Strategic Air Command bases for the SCCA's use; the SCCA relied heavily on these venues during the early and mid-1950s during the transition from street racing to permanent circuits.
By 1962, the SCCA was tasked with managing the U.S. World Sportscar Championship rounds at Daytona, Sebring, Bridgehampton and Watkins Glen. The club was also involved in the Formula 1 U.S. Grand Prix. SCCA Executive Director John Bishop helped to create the United States Road Racing Championship series for Group 7 sports cars to recover races that had been taken by rival USAC Road Racing Championship. Bishop was also instrumental in founding the SCCA Trans-Am Series and the SCCA/CASC Can-Am series. In 1969, tension and in-fighting over Pro Racing's autonomy caused Bishop to resign and help form the International Motor Sports Association.
Sanctioned racing.
Professional racing.
The SCCA dropped its amateur policy in 1962 and began sanctioning professional racing. In 1963, the United States Road Racing Championship was formed. In 1966 the Canadian-American Challenge Cup (Can-Am) was created for Group 7 open-top sportscars. The Trans-Am Series for pony cars also began in 1966. Today, Trans-Am uses GT-1 class regulations, giving amateur drivers a chance to race professionally. A professional series for open-wheel racing cars was introduced in 1967 as the SCCA Grand Prix Championship. This series was then held under various names through to the 1976 SCCA/USAC Formula 5000 Championship.
Current SCCA-sanctioned series include Trans Am, the Pirelli World Challenge for GT and touring cars, the Battery Tender Mazda MX-5 Cup Presented by BFGoodrich Tires, F2000 Championship Series, F1600 Championship Series and the Atlantic Championship Series. SCCA Pro Racing has also sanctioned professional series for some amateur classes such as Spec Racer Ford Pro and Formula Enterprises Pro. SCCA Pro Racing also sanctioned the Volkswagen Jetta TDI Cup during its time.
Club racing.
The club racing program is the road racing division where drivers race on either dedicated race tracks or on temporary street circuits. Competitors require either a regional or a national racing license. Both modified production cars (ranging from lightly modified cars with only extra safety equipment to heavily modified cars that retain only the basic shape of the original vehicle) and designed-from-scratch "formula" and "sports racer" cars can be used in club racing. Most of the participants in the Club Racing program are unpaid amateurs, but some go on to professional racing careers. The club is also the source for race workers in all specialties.
The annual national championship for Club Racing is called the SCCA National Championship Runoffs and has been held at Riverside International Raceway (1964, 1966, 1968), Daytona International Speedway (1965, 1967, 1969), Road Atlanta (1970–1993), Mid-Ohio Sports Car Course (1994–2005), Heartland Park Topeka (2006–2008), Road America in Elkhart Lake, Wisconsin (2009-2013), Mazda Raceway Laguna Seca (2014), and next year the runoffs will be held at Daytona International Speedway (2015). In 2016, the runoffs will go back to Mid-Ohio Sports Car Course. The current SCCA record holder is Jerry Hansen, (former owner of Brainerd International Raceway), with 27 national titles.
Autocross.
The Solo program is the autocross program. up to four cars at a time run on a course without interfering with one another on a course laid out with traffic cones on a large paved surface, such as a parking lot or airport runway.
Competitions are held at the Regional, Divisional, and National levels. Each Division typically crowns a Divisional Champion in each class, awarded by winning a single event. Similarly, a National Champion in each class is awarded by winning the class at the National Championship (usually referred to as "Nationals") held in September. In 2009, the Solo Nationals moved to the Lincoln Airpark in Lincoln, Nebraska. Individual National-level events, called "National Tours," are held throughout the racing season. They have now introduced "match tours" which are a combination of a pro solo event and an autocross event. At these tours is where our kart drivers became champions among adults.A prime example of this is Kate Regganie and Kevin Teague. They are the youngest km and kml drivers to have ever driven, they complimented this succession with both of them trophying at the 2013 National Championships. 
The SCCA also holds National-level events in an alternate format called ProSolo. In ProSolo, two cars compete at the same time on mirror-image courses with drag racing-style starts, complete with reaction and 60-foot times. Class winners and other qualifiers (based on time differential against the class winner) then compete in a handicapped elimination round called the "Challenge". Points are awarded both in class competition and in Challenge competition, and an annual champion is crowned each September at the Pro Finale event in Lincoln, Nebraska.
Rallying.
The SCCA sanctions RallyCross events, similar to autocross, but on a non-paved course. SCCA ProRally was a national performance rally series similar to the World Rally Championship. At the end of the 2004 season SCCA dropped ProRally and ClubRally. A new organization, Rally America, picked up both series starting in 2005.
Road rallies are run on open, public roads. These are not races in the sense of speed, but of precision and navigation. The object is to drive on time, arriving at checkpoints with the proper amount of elapsed time from the previous checkpoint. Competitors do not know where the checkpoints are.
Time Trials.
In recent years, SCCA has expanded and re-organized some of the higher-speed events under the "Time Trials" banner. These include Performance Driving eXperience (PDX), Club Trials, Track Trials, and Hill Climb events. PDX events are non-competition HPDE-type events and consist of drivers-education and car control type of classroom learning combined with on-track instruction.
Divisions and regions.
The SCCA is organized into nine divisions and 115 regions, each organizing events in that area to make the events more accessible to people throughout the country. The number of divisions has increased since the SCCA's foundation. Northern Pacific and Southern Pacific started as a single Pacific Coast Division until dividing in 1966. Rocky Mountain Division is a relatively recent split. The Great Lakes Division was split from the Central Division at the end of 2006.
The Northern Pacific Division consists of Alaska, Northern California, Idaho, Western Montana, Northern Nevada, Oregon, and Washington. The Southern Pacific Division consists of Arizona, Southern California, Hawaii, and Nevada. The Rocky Mountain Division consists of Colorado, Eastern Montana, New Mexico, Utah, and Wyoming. The Southwest Division consists of Louisiana, Texas, and coastal Mississippi. The Midwest Division consists of Arkansas, Southern Illinois, Western Iowa, Kansas, Northern Mississippi, Missouri, Nebraska, Oklahoma, and Western Tennessee. The Central Division consists of Northern Illinois, Minnesota, Eastern Iowa, North Dakota, South Dakota, Upper Peninsula of Michigan and Wisconsin. The Great Lakes Division consists of Kentucky, Ohio, Indiana, Lower Peninsula of Michigan, and Southern West Virginia. The Southeast Division consists of Alabama, Florida, Georgia, North Carolina, South Carolina, Eastern Tennessee, and Southern Virginia. The Northeast Division consists of Connecticut, Delaware, Maine, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, Vermont, and Northern Virginia.

</doc>
<doc id="28244" url="http://en.wikipedia.org/wiki?curid=28244" title="Star network">
Star network

Star networks are one of the most common computer network topologies. In its simplest form, a star network consists of one central switch, hub or computer, which acts as a conduit to transmit messages. This consists of a central node, to which all other nodes are connected; this central node provides a common connection point for all nodes through a hub. In star topology, every node (computer workstation or any other peripheral) is connected to a central node called a hub or switch. The switch is the server and the peripherals are the clients. Thus, the hub and leaf nodes, and the transmission lines between them, form a graph with the topology of a star. If the central node is "passive", the originating node must be able to tolerate the reception of an echo of its own transmission, delayed by the two-way transmission time (i.e. to and from the central node) plus any delay generated in the central node. An "active" star network has an active central node that usually has the means to prevent echo-related problems.
The star topology reduces the damage caused by line failure by connecting all of the systems to a central node. When applied to a bus-based network, this central hub rebroadcasts all transmissions received from any peripheral node to all peripheral nodes on the network, sometimes including the originating node. All peripheral nodes may thus communicate with all others by transmitting to, and receiving from, the central node only. The failure of a transmission line linking any peripheral node to the central node will result in the isolation of that peripheral node from all others, but the rest of the systems will be unaffected. 
It is also designed with each node (file servers, workstations, and peripherals) connected directly to a central network hub, switch, or concentrator.
Data on a star network passes through the hub, switch, or concentrator before continuing to its destination. The hub, switch, or concentrator manages and controls all functions of the network. It also acts as a repeater for the data flow. This configuration is common with twisted pair cable. However, it can also be used with coaxial cable or optical fibre cable.

</doc>
<doc id="28245" url="http://en.wikipedia.org/wiki?curid=28245" title="SQL Server">
SQL Server

SQL Server may refer to:

</doc>
