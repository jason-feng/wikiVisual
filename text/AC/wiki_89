<doc id="40288" url="http://en.wikipedia.org/wiki?curid=40288" title="John Wentworth (Illinois)">
John Wentworth (Illinois)

John Wentworth (nicknamed "Long John") (March 5, 1815 – October 16, 1888) was the editor of the "Chicago Democrat," publisher of an extensive Wentworth family genealogy, a two-term mayor of Chicago, and a six-term member of the United States House of Representatives, both before and after his service as mayor.
After growing up in New Hampshire, he joined the migration west and moved to the developing city of Chicago in 1836, where he made his adult life. Wentworth was affiliated with the Democratic Party until 1855; then he changed to the Republican Party. After retiring from politics, he wrote a three-volume genealogy of the Wentworth family in the United States.
Early life and education.
John Wentworth was born in Sandwich, New Hampshire. He was educated at the New Hampton Literary Institute and at the academy of Dudley Leavitt. He graduated from Dartmouth College in 1836.
Migration west and career.
Later that year, Wentworth joined a migration west and moved to Chicago, arriving in the city on October 25, 1836. He became managing editor of Chicago's first newspaper, the "Chicago Democrat," eventually becoming its owner and publisher.
He started a law practice and entered politics. He was a business partner of Illinois financier Jacob Bunn, and the two men were two of the incorporators of the Chicago Secure Depository Company.
Marriage and family.
In 1844 he married Roxanna Marie Loomis. 
In later years, his nephew Moses J. Wentworth handles his business affairs, and would eventually manage his estate as well.
Political career.
After becoming active with the Democrats, Wentworth was elected to the U.S. House of Representatives, where he served for a total of six terms, five of them as a Democrat: (March 4, 1843 – March 3, 1851 and March 4, 1853 – March 3, 1855.
He returned to Chicago and affiliated with the Republican Party. Wentworth was elected as mayor of Chicago for two terms, 1857–1858 and 1860–1861. Wentworth instituted the use of chain gangs of prisoners in the city as laborers.
In his effort to clean up the city's morals, he hired spies to determine who was frequenting Chicago's brothels. In 1857, Wentworth led a raid on "the Sands," Chicago's red-light district, which resulted in the burning of the area. 
In 1864 Wentworth ran again for Congress, as a Republican, and was elected for his last term, serving March 4, 1865 – March 3, 1867. While he was in the House, there was a controversial vote to settle a boundary issue between Wisconsin and Illinois, with Wisconsin claiming land as far as the tip of Lake Michigan. Wentworth was promised that if he voted to give the land including Chicago to Wisconsin, he would be appointed to the US Senate. Wentworth declined the offer.
After retiring from Congress, from 1868 Wentworth lived at his country estate at 5441 South Harlem Avenue in Chicago. He owned about 5000 acre of land in what is today part of the Chicago neighborhood of Garfield Ridge and suburban Summit.
When an author left a manuscript of a history of Chicago with Wentworth for his suggestions, he reportedly removed what did not refer to him and returned the manuscript to its author with the note, "Here is your expurgated and correct history of Chicago."
Family historian.
He researched and wrote "The Wentworth Genealogy – English and American" - twice, which he published privately. The first two-volume edition, also known as the "private edition", published in 1871, was followed by a second, corrected, edition in 1878, which was published in three volumes, for a total of 2241 pages. The total reported cost for both editions was $40,000. The first of the 1878 volumes chronicles the ancestry of Elder William Wentworth, the first of this family in New England, and his first five generations of New World descendants. The second and third volumes discuss the "Elder's" many descendants and others of the name. John was a fourth great grandson of William.
Death.
Wentworth died at his estate in 1888, aged 73. He was buried in Rosehill Cemetery in Chicago.

</doc>
<doc id="40289" url="http://en.wikipedia.org/wiki?curid=40289" title="Hiram College">
Hiram College

Hiram College ( ) is a private liberal arts college located in Hiram, Ohio. It was founded in 1850 as the Western Reserve Eclectic Institute by Amos Sutton Hayden and other members of the Disciples of Christ Church. The college is nonsectarian and coeducational. It is accredited by The Higher Learning Commission of the North Central Association of Colleges and Schools. Hiram's most famous alumnus is James A. Garfield, who also served as a college instructor and principal, and was subsequently elected the 20th President of the United States.
History.
On June 12, 1849, representatives of the Disciples of Christ voted to establish an academic institution, which would later become Hiram College. On November 7 that year, they chose the village of Hiram as the site for the school because the founders considered this area of the Western Reserve to be "healthful and free of distractions". The following month, on December 20, the founders accepted the suggestion of Isaac Errett and named the school the Western Reserve Eclectic Institute.
The Institute's original charter was authorized by the state legislature on March 1, 1850, and the school opened several months later, on November 27. Many of the students came from the surrounding farms and villages of the Western Reserve, but Hiram soon gained a national reputation and students began arriving from other states. On February 20, 1867, the Institute incorporated as a college and changed its name to Hiram College.
During the years before it was renamed Hiram College, 1850–1867, the school had seven principals, the equivalent of today's college presidents. The two that did the most in establishing and defining the nature of the institution were Disciple minister Amos Sutton Hayden, who led the school through its first six years, and James A. Garfield, who had been a student at the Institute from 1851–1853 and then returned in 1856 as a teacher. As principal, Garfield expanded the Institute's curriculum. He left the Institute in 1861 and in 1880 was elected the 20th President of the United States.
In 1870, one of Garfield's best friends and former students, Burke A. Hinsdale, was appointed Hiram's president. Although there were two before him, Hinsdale is considered the college's first permanent president because the others served only briefly. The next president to have a major impact on the college was Ely V. Zollars, who increased enrollment significantly, established a substantial endowment and created a program for the construction of campus buildings. Later presidents who served for at least 10 years were Miner Lee Bates, Kenneth I. Brown, Paul H. Fall, Elmer Jagow, and G. Benjamin Oliver.
In 1931, shortly before Hiram celebrated the 100th anniversary of Garfield's birth, there was a debate in the community about changing the name of the school to Garfield College. There were strong advocates on both sides of the issue. Among the 2,000 guests at the centennial celebration were three generations of Garfield's family, including two of his sons. The idea of changing the college's name was not mentioned at the event and the idea was abandoned.
Principals and presidents.
The following is a list of the school's leaders since its founding in 1850.
Profile.
As of the 2011–12 academic year, Hiram's student body consists of 1,334 undergraduates from 35 states and 30 foreign countries Of the 73 full-time faculty, 95-percent hold a Ph.D. or other terminal degree in their field.
Rankings.
Hiram was ranked #167 among National Liberal Arts Colleges by U.S. News and World Report in 2012. At the same time, Hiram is currently ranked #67 among Liberal Arts Colleges by Washington Monthly. Also, in 2012, Forbes ranked Hiram at #197 among all colleges and universities in the U.S., and #39 in the Midwest. Hiram has regularly been included in The Princeton Review Best Colleges guide, and is one of only 40 schools included in Loren Pope's book "Colleges That Change Lives".
Hiram is a member of the Annapolis Group, which has been critical of the college rankings process. Hiram is among the signatories of the "Presidents Letter". 
Academics.
Hiram specializes in the education of undergraduate students, though the college does have a small graduate program. Hiram confers the BA, BSN, and MA degrees. The college offers 30 majors and 36 minors for traditional undergraduates, in addition to pre-professional programs for specific fields. Interdisciplinary studies have also been a part of Hiram's curriculum for decades.
Hiram's curriculum requires all students to complete one course in each of eight academic areas: creative methods, interpretive methods, modeling methods, experimental scientific methods, social and cultural analysis, experiencing the world, understanding diversity at home, and ethics and social responsibility. Its education plan also includes international study and independent study opportunities, and faculty-guided research projects. Currently, almost all majors require some form of extensive independent project or apprenticeship experience.
Hiram has seven Centers of Distinction for interdisciplinary studies. They include: Center for Deciphering Life's Languages, Center for Engaged Ethics, Center for Integrated Entrepreneurship, Center for Literature and Medicine, Center for the Study of Nature and Society, Garfield Institute for Public Leadership, and Lindsay-Crane Center for Writing and Literature.
Athletics.
The school's sports teams are called the Terriers. They participate in the NCAA's Division III and the North Coast Athletic Conference. Men's sports include baseball, basketball, cross country, football, golf, lacrosse, soccer, swimming & diving, and track & field. Women's sports include basketball, cross country, golf, lacrosse, soccer, softball, swimming & diving, track & field,volleyball and, tennis.
The Hiram College basketball team won the gold medal in the collegiate division of the 1904 Summer Olympics in St. Louis. It was the first time that basketball was part of an Olympics; it was included as a demonstration sport and no foreign teams participated.
The Cleveland Browns held their training camp at Hiram College from 1952 through 1974, making it the longest-tenured training site in the team's history.
Residence life.
The college's residential complexes include The Quad (Agler Hall, Dean Hall, New Quad, Peckham Hall, and Gray-Bancroft), The Hill (Bowler Hall, Henry Hall, Miller Hall, and Whitcomb Hall), Booth-Centennial, East Hall, and the Townhouses. They are managed by resident directors (RDs), resident managers (RMs), and resident assistants (RAs).
Student clubs and organizations.
Student Senate is the elected student governing body of the college. It serves as a liaison between students and the school's administration, and oversees all student clubs and organizations, collectively called the Associated Student Organizations (ASO). The Kennedy Center Programming Board (KCPB) falls under the auspices of Student Senate, and is responsible for planning educational, social, recreational, and cultural programs.
Hiram has close to 100 registered student clubs and organizations in eight categories: Academic, Greek Social, Musical, Political and Activisim, Publications and Communications, Religious, Special Interest and Service, and Sports and Recreation. Fraternities and sororities are not permitted to have housing on campus, but there are six Greek social clubs: Delta Chi Lambda, Kappa Sigma Pi, Lambda Lambda Lambda, Phi Beta Gamma, Phi Gamma Epsilon, and Phi Kappa Chi, for which Greek Council serves as an umbrella organization. The school newspaper is "The Advance".
Since 1971, Hiram has maintained a chapter of Phi Beta Kappa, the national honor society for the liberal arts. The school has also had a chapter of Omicron Delta Kappa (ODK), a national leadership honor society, since 1962.

</doc>
<doc id="40290" url="http://en.wikipedia.org/wiki?curid=40290" title="Joseph Medill">
Joseph Medill

Joseph Medill (April 6, 1823 – March 16, 1899) was an American newspaper editor, publisher, and politician. He was co-owner and managing editor of the "Chicago Tribune", and was Mayor of Chicago.
Personal life.
Joseph Medill was born April 6, 1823 in Saint John, New Brunswick, Canada. He read law in Ohio and was admitted to the Ohio Bar in 1846.
Medill married Katherine "Kitty" Patrick on September 2, 1852 and they had three daughters, Katherine, Elinor and Josephine (1866 - 1892).
Publishing career.
In 1853, Medill and Edwin Cowles started the "Leader" a newspaper in Cleveland, Ohio. (It was later absorbed by "The Plain Dealer"). In 1854, the "Tribune"'s part-owner, Captain J. D. Webster, asked Medill to become the paper's managing editor. Medill was further encouraged to come to Chicago by Dr. Charles H. Ray of Galena, Illinois, and editor Horace Greeley of the "New York Tribune".
In 1855, Medill sold his interest in the "Leader" to Cowles, and bought the "Tribune" in partnership with Dr. Ray and Cowles' brother Alfred.
Under Medill's management, the "Tribune" flourished, becoming one of the largest newspapers in Chicago. Medill served as its managing editor until 1864, when Horace White became editor-in-chief. At that time Medill left day-to-day operations of the "Tribune" for political activities.
But White clashed with Medill over the Presidential election of 1872. So, in 1873 Medill bought additional equity from Cowles and from White, becoming majority owner. In 1874, he replaced White as editor-in-chief. Medill served as editor-in-chief until his death.
Political activity.
Under Medill, the "Tribune" became the leading Republican newspaper in Chicago. Medill was strongly anti-slavery, supporting both the Free-Soil cause and Abolitionism. Medill was a major supporter of Abraham Lincoln in the 1850s. Medill and the "Tribune" were instrumental in Lincoln's presidential nomination, and were equally supportive of the Union cause during the American Civil War. The "Tribune"'s chief adversary through this period was the "Chicago Times", which supported the Democrats.
In 1864, Medill left the "Tribune" editorship for political activity, which occupied him for the next ten years. He was appointed by President Grant to the first Civil Service Commission. In 1870, he was elected as a delegate to the Illinois Constitutional convention. In 1871, after the Great Chicago Fire, Medill was elected mayor of Chicago as candidate of the temporary "Fireproof" party, serving for two years. As mayor, Medill gained more power for the mayor's office, created Chicago's first public library, enforced blue laws, and reformed the police and fire departments. But the stress of the job impaired his health. In August 1873, he appointed Lester L. Bond as Acting Mayor for the remaining 3½ months of his term, and went to Europe on a convalescent tour.
Medill was a strong Republican loyalist who supported President Grant for re-election in 1872. The breach with White came because White supported the breakaway Liberal Republicans, reformists who nominated Horace Greeley for President. It was also at this time that Medill broke with Greeley.
Family tree.
The tree omits Medill's third daughter, Josephine, who died in 1892.

</doc>
<doc id="40291" url="http://en.wikipedia.org/wiki?curid=40291" title="Carter Harrison, Sr.">
Carter Harrison, Sr.

Carter Henry Harrison, Sr. (February 15, 1825 – October 28, 1893) was an American politician who served as mayor of Chicago, Illinois from 1879 until 1887; he was subsequently elected to a fifth term in 1893 but was assassinated before completing his term. He previously served two terms in the United States House of Representatives. Harrison was the first cousin twice removed of President William Henry Harrison.
Born near Lexington, Kentucky to Carter Henry Harrison II and Caroline Russell, he was only a few months old when his father died. He was educated by private tutors, and was graduated from Yale College in 1845 as a member of Scroll and Key. Following graduation, he traveled and studied in Europe from 1851 to 1853 before entering Transylvania College in Lexington, where he earned a law degree in 1855. He was admitted to the bar in 1855 and commenced practice in Chicago; Harrison came to Chicago because he saw it as a land of opportunity.
Harrison ran an unsuccessful campaign in 1872 for election to the Forty-third Congress. Beginning in 1874, he served as a member of the board of commissioners of Cook County. He was elected as a Democrat to the Forty-fourth and Forty-fifth Congresses, and delegate to the 1880 and 1884 Democratic National Conventions.
Harrison married Margarette (or Margaret) E. Stearns in 1882, following the death of his first wife in 1876. She was the daughter of Chicago pioneer Marcus C. Stearns. 
"A Summer's Outing".
In 1890, Harrison and his daughter took a vacation trip from Chicago to Yellowstone National Park and Alaska. His letters from the trip were first published in the "Chicago Tribune" and later compiled into the book (1891): "A Summer's Outing and The Old Man's Story."
Assassination.
The night of the Haymarket Riot in 1886, Harrison walked unmolested through the crowd of anarchists and advised the police to leave the demonstrators alone. The riot was sparked by a bomb, reportedly thrown at police by anarchists (killing seven police officers). After leaving office, Harrison was owner and editor of the "Chicago Times" from 1891 to 1893. He was re-elected in 1893, in time for the World's Columbian Exposition. His desire was to show the world the true Chicago, and he appointed 1st Ward Alderman "Bathhouse" John Coughlin to sit on the reception committee. On October 28, 1893, two days before the close of the Exposition, Harrison was murdered in his home by Patrick Eugene Prendergast, a disgruntled office seeker. Harrison was buried in Chicago's Graceland Cemetery. Prendergast was hanged on July 13, 1894. Harrison was Chicago's first five-time elected mayor; eventually his son, Carter Harrison, Jr., was also elected mayor five times.
Harrison's career and assassination are closely connected with the World's Columbian Exposition, and are discussed at some length as a subplot to the two main stories (about the fair and serial killer H. H. Holmes) in "The Devil in the White City". The celebration of the close of the Exposition was cancelled in lieu of a large public memorial service for Harrison.

</doc>
<doc id="40292" url="http://en.wikipedia.org/wiki?curid=40292" title="Carter Harrison, Jr.">
Carter Harrison, Jr.

Carter Henry Harrison, Jr. (April 23, 1860 – December 25, 1953) was an American politician who served as Mayor of Chicago, Illinois (1897–1905 and 1911–1915). The City's 30th mayor, he was the first actually born in Chicago.
Biography.
He was born on April 23, 1860 in Chicago.
Like his father, Carter Harrison, Sr., Carter Harrison, Jr. gained election to five terms as Chicago's mayor. Educated in Saxe-Altenburg, Germany, Harrison returned to Chicago to help his brother run the "Chicago Times", which their father bought in 1891. Under the Harrisons the paper became a resolute supporter of the Democratic Party, and was the only local newspaper to support the Pullman strikers in the mid-1890s.
As with his father, Harrison did not believe in trying to legislate morality. As mayor, Harrison believed that Chicagoans' two major desires were to make money and to spend it. During his administrations, Chicago's vice districts blossomed, and special maps were printed to enable tourists to find their way from brothel to brothel. The name of one Chicago saloon-keeper of the time supposedly entered the English language as a term for a strong or laced drink intended to render unconsciousness: Mickey Finn.
However, Harrison was seen as more of a reformer than his father, which helped him garner the middle class votes his father had lacked. One of Harrison's biggest enemies was Charles Yerkes, whose plans to monopolize Chicago's streetcar lines were vigorously attacked by the mayor. During his final term in office, Harrison established the Chicago Vice Commission and worked to close down the Levee district, starting with the Everleigh Club brothel on October 24, 1911.
Harrison was a hopeful for the 1904 Democratic nomination for President, but was unable to negotiate his way through a tangle of conflicting loyalies to different Party bosses; the nomination went to Alton B. Parker, who was soundly defeated by Theodore Roosevelt.
In 1915, when Harrison left office, Chicago had essentially reached its modern size in land area, and had a population of 2,400,000; the city was moving inexorably into its status as a major modern metropolis. He and his father had collectively been mayor of the city for 21 of the previous 36 years. 
He died in Chicago on December 25, 1953, and is buried in Graceland Cemetery.
Legacy.
Harrison wrote his autobiography, not once but twice; his wife, Edith Ogden Harrison, was a well-known writer of children's books and fairy tales in the first two decades of the twentieth century.
He was a member of many organizations including the Freemasons, Knights Templar, the Society of the Cincinnati, Sons of the Revolution, Sons of the American Revolution, Society of Colonial Wars, Veterans of Foreign Wars, American Legion and the Military Order of the World Wars. 
Ancestry.
Harrison was a descendant of Robert Carter I, Benjamin Harrison IV, William Randolph, and Isham Randolph of Dungeness.

</doc>
<doc id="40293" url="http://en.wikipedia.org/wiki?curid=40293" title="Nikkei 225">
Nikkei 225

The Nikkei 225 (日経平均株価, Nikkei heikin kabuka, 日経225), more commonly called the Nikkei, the Nikkei index, or the Nikkei Stock Average (, , or ), is a stock market index for the Tokyo Stock Exchange (TSE). It has been calculated daily by the "Nihon Keizai Shimbun" ("Nikkei") newspaper since 1950. It is a price-weighted index (the unit is yen), and the components are reviewed once a year. Currently, the Nikkei is the most widely quoted average of Japanese equities, similar to the Dow Jones Industrial Average. In fact, it was known as the "Nikkei Dow Jones Stock Average" from 1975 to 1985.
The Nikkei 225 began to be calculated on September 7, 1950, retroactively calculated back to May 16, 1949. Since January 2010 the index is updated every 15 seconds during trading sessions.
The Nikkei 225 Futures, introduced at Singapore Exchange (SGX) in 1986, the Osaka Securities Exchange (OSE) in 1988, Chicago Mercantile Exchange (CME) in 1990, is now an internationally recognized futures index.
The Nikkei average has deviated sharply from the textbook model of stock averages which grow at a steady exponential rate. The average hit its all-time high on December 29, 1989, during the peak of the Japanese asset price bubble, when it reached an intra-day high of 38,957.44 before closing at 38,915.87, having grown sixfold during the decade. Subsequently, it lost nearly all these gains, closing at 7,054.98 on March 10, 2009—81.9% below its peak twenty years earlier.
Another major index for the Tokyo Stock Exchange is the Topix.
On March 15, 2011, the second working day after the massive earthquake in the northeast part of Japan, the index dropped over 10% to finish at 8605.15, a loss of 1,015 points. The index continued to drop throughout 2011, eventually bottoming out at 8160.01 on November 25, putting it at its lowest close since March 10, 2009. The Nikkei fell over 17% in 2011, finishing the year at 8455.35, its lowest year-end closing value in nearly thirty years, when the index finished at 8016.70 in 1982.
The Nikkei started 2013 near 10,600, hitting a peak of 15,942 in May. However, shortly afterward, it plunged by almost 10% before rebounding, making it the most volatile stock market index among the developed markets.
Weighting.
The index is a price-weighted index. As of late 2014, the company with the largest influence on the index is Fast Retailing.
Investing.
Since it is not possible to invest directly in an index, the best way to gain exposure to the stocks in the Nikkei 225 is via an exchange-traded fund (ETF) that tracks the original index. There are several Japanese exchange-traded funds that track the Nikkei 225 index, including iShares Nikkei 225 ETF, MAXIS Nikkei 225 Index ETF, Nomura ETF - Nikkei 225 Exchange Traded Fund, and Daiwa ETF - Nikkei 225. There is also an Irish exchange-traded fund, iShares Nikkei 225 UCITS, which is traded on several European exchanges, in euros, pounds, and yen. In the United States, a fund tracking the MSCI Japan index is most popular.
Components.
As of February 2015, the Nikkei 225 consists of the following companies: (Japanese securities identification code in parentheses)
Components.
As of February 2015, the Nikkei 225 consists of the following companies: (Japanese securities identification code in parentheses)

</doc>
<doc id="40294" url="http://en.wikipedia.org/wiki?curid=40294" title="Stephen Smale">
Stephen Smale

Stephen Smale (born July 15, 1930) is an American mathematician from Flint, Michigan. His research concerns topology, dynamical systems and mathematical economics. He was awarded the Fields Medal in 1966, and spent more than three decades on the mathematics faculty of the University of California, Berkeley (1960–61 and 1964–1995).
Education and career.
Smale entered the University of Michigan in 1948. Initially, he was a good student, placing into an honors calculus sequence taught by Bob Thrall and earning himself A's. However, his sophomore and junior years were marred with mediocre grades, mostly Bs, Cs and even an F in nuclear physics. However, with some luck, Smale was accepted as a graduate student at the University of Michigan's mathematics department. Yet again, Smale performed poorly in his first years, earning a C average as a graduate student. It was only when the department chair, Hildebrandt, threatened to kick out Smale, that he began to work hard. Smale finally earned his Ph.D. in 1957, under Raoul Bott.
Smale began his career as an instructor at the college at the University of Chicago. In 1958, he astounded the mathematical world with a proof of a sphere eversion. He then cemented his reputation with a proof of the Poincaré conjecture for all dimensions greater than or equal to 5, published in 1961; in 1962 he generalized the ideas in a 107 page paper that established the h-cobordism theorem.
After having made great strides in topology, he then turned to the study of dynamical systems, where he made significant advances as well. His first contribution is the Smale horseshoe that jumpstarted significant research in dynamical systems. He also outlined a research program carried out by many others. Smale is also known for injecting Morse theory into mathematical economics, as well as recent explorations of various theories of computation.
In 1998 he compiled a list of 18 problems in mathematics to be solved in the 21st century, known as Smale's problems. This list was compiled in the spirit of Hilbert's famous list of problems produced in 1900. In fact, Smale's list contains some of the original Hilbert problems, including the Riemann hypothesis and the second half of Hilbert's sixteenth problem, both of which are still unsolved. Other famous problems on his list include the Poincaré conjecture (Now a theorem, proved by Grigori Perelman), the P = NP problem, and the Navier–Stokes equations, all of which have been designated Millennium Prize Problems by the Clay Mathematics Institute. 
Earlier in his career, Smale was involved in controversy over remarks he made regarding his work habits while proving the higher-dimensional Poincaré conjecture. He said that his best work had been done "on the beaches of Rio". This led to the withholding of his grant money from the NSF. He has been politically active in various movements in the past, such as the Free Speech movement. At one time he was subpoenaed by the House Un-American Activities Committee.
In 1960 Smale was appointed an associate professor of mathematics at the University of California, Berkeley, moving to a professorship at Columbia University the following year. In 1964 he returned to a professorship at UC Berkeley where he has spent the main part of his career. He retired from UC Berkeley in 1995 and took up a post as professor at the City University of Hong Kong. He also amassed over the years one of the finest private mineral collections in existence. Many of Smale's mineral specimens can be seen in the book—"The Smale Collection: Beauty in Natural Crystals".
Since 2002 Smale is a Professor at the Toyota Technological Institute at Chicago; starting August 1, 2009, he is also a Distinguished University Professor at the City University of Hong Kong.
In 2007, Smale was awarded the Wolf Prize in mathematics.

</doc>
<doc id="40295" url="http://en.wikipedia.org/wiki?curid=40295" title="Hawker Siddeley Nimrod">
Hawker Siddeley Nimrod

The Hawker Siddeley Nimrod was a maritime patrol aircraft developed and operated by the United Kingdom. It is an extensive modification of the de Havilland Comet, the world's first operational jet airliner. It was originally designed by de Havilland's successor firm, Hawker Siddeley; further development and maintenance work was undertaken by Hawker Siddeley's own successor companies, British Aerospace and BAE Systems, respectively.
Designed in response to a requirement issued by the Royal Air Force (RAF) to replace its fleet of ageing Avro Shackletons, the Nimrod MR1/MR2s were primarily fixed-wing aerial platforms for anti-submarine warfare (ASW) operations; secondary roles included maritime surveillance and anti-surface warfare. It served from the early 1970s until March 2010. The intended replacement was to be extensively rebuilt Nimrod MR2s, designated as Nimrod MRA4; however due to considerable delays, repeated cost overruns, and financial cutbacks, the development of the MRA4 was abandoned in 2010.
In addition to the three Maritime Reconnaissance variants, two further Nimrod types were developed. The RAF operated a small number of Nimrod R1, an electronic intelligence gathering (ELINT) variant. A dedicated airborne early warning platform, the Nimrod AEW3 was in development from late 1970s to the mid-1980s; however, much like the MRA4, considerable problems were encountered in development and thus the project was cancelled in 1986 in favour of an off-the-shelf solution in the Boeing E-3 Sentry. All Nimrod variants had been retired by mid-2011.
Development.
MR1.
On 4 June 1964, the British Government issued Air Staff Requirement 381 to replace the Avro Shackleton. Such a replacement was necessitated by the rapidly approaching fatigue life limits of the RAF's existing Shackleton fleet. A great deal of interest in the requirement was received from both British and foreign manufacturers, offered aircraft including the Lockheed P-3 Orion, the Breguet Atlantic and derivatives of the Hawker Siddeley Trident, BAC One-Eleven, Vickers VC10 and de Havilland Comet. On 2 February 1965, British Prime Minister Harold Wilson announced the intention to order Hawker Siddeley's maritime patrol version of the Comet, the HS.801.
The Nimrod design was based on that of the Comet 4 civil airliner which had reached the end of its commercial life (the first two prototype Nimrods, XV148 & XV147 were built from two final unfinished Comet 4C airframes). The Comet's turbojet engines were replaced by Rolls-Royce Spey turbofans for better fuel efficiency, particularly at the low altitudes required for maritime patrol. Major fuselage changes were made, including an internal weapons bay, an extended nose for radar, a new tail with electronic warfare (ESM) sensors mounted in a bulky fairing, and a MAD (magnetic anomaly detector) boom. After the first flight in May 1967, the RAF ordered a total of 46 Nimrod MR1s. The first example (XV230) entered service in October 1969. A total of five squadrons using the type were established; four were permanently based in the UK and a fifth was initially based in Malta.
R1.
Three Nimrod aircraft were adapted for the signals intelligence role, replacing the Comet C2s and Canberras of No. 51 Squadron in May 1974. The R1 was visually distinguished from the MR2 by the lack of a MAD boom. It was fitted with an array of rotating dish aerials in the aircraft's bomb bay, with further dish aerials in the tailcone and at the front of the wing-mounted fuel tanks. It had a flight crew of four (two pilots, a flight engineer and one navigator) and up to 25 crew operating the SIGINT equipment.
Only since the end of the Cold War has the role of the aircraft been officially acknowledged; they were once described as "radar calibration aircraft". The R1s have not suffered the same rate of fatigue and corrosion as the MR2s. One R1 was lost in a flying accident since the type's introduction; this occurred in May 1995 during a flight test after major servicing, at RAF Kinloss. To replace this aircraft an MR2 was selected for conversion to R1 standard, and entered service in December 1996.
The Nimrod R1 was based initially at RAF Wyton, Cambridgeshire, and later at RAF Waddington in Lincolnshire, England, and flown by 51 Sqn. The two remaining Nimrod R1s were originally planned to be retired at the end of March 2011, but operational requirements forced the RAF to deploy one to RAF Akrotiri, Cyprus on 16 March in support of Operation Ellamy. The last flight of the type was on 28 June 2011 from RAF Waddington, in the presence of the Chief of the Air Staff, ACM Sir Stephen Dalton. XV 249, the former MR2, is now on display at the RAF Museum Cosford, West Midlands. The R1 is being replaced by three Boeing RC-135W "Rivet Joint" aircraft, acquired under the Airseeker project; the first aircraft was delivered in late 2013.
MR2.
Starting in 1975, 35 aircraft were upgraded to MR2 standard, being re-delivered from August 1979. The upgrade included extensive modernisation of the aircraft's electronic suite. Changes included the replacement of the obsolete ASV Mk 21 radar used by the Shackleton and Nimrod MR1 with the new EMI Searchwater radar, a new acoustic processor (GEC-Marconi AQS-901) capable of handling more modern sonobouys, a new mission data recorder (Hanbush) and a new Electronic Support Measures (Yellow Gate) which included new pods on the wingtips.
Provision for in-flight refuelling was introduced during the Falklands War (as the "MR2P"), as well as hardpoints to allow the Nimrod to carry the AIM-9 Sidewinder missile to counter enemy Argentine Air Force maritime surveillance aircraft. In preparation for operations in the Gulf War theatre, several MR2s were fitted with new communications and ECM equipment to deal with anticipated threats; at the time these modified aircraft were given the designation "MR2P(GM) (Gulf Mod)".
The Nimrod MR2 carried out three main roles – Anti-Submarine Warfare (ASW), Anti-Surface Unit Warfare (ASUW) and Search and Rescue (SAR). Its extended range enabled the crew to monitor maritime areas far to the north of Iceland and up to 4,000 km out into the Western Atlantic. With Air-to-Air Refuelling (AAR), range and endurance was greatly extended. The crew consisted of two pilots and one flight engineer, two navigators (one tactical navigator and a routine navigator), one Air Electronics Officer (AEO), the sonobuoy sensor team of two Weapon System Operators (WSOp ACO) and four Weapon System Operators (WSOp EW) to manage passive and active electronic warfare systems.
Until 1992, the Nimrod MR2 was based at RAF Kinloss in Scotland (120, 201 and 206 Squadrons), and RAF St Mawgan in Cornwall (42 and 38(R) Squadrons). Following Options for Change, 42 Squadron was disbanded and its number reassigned to 38(R) Squadron. The Nimrod MR2 aircraft was withdrawn on 31 March 2010, a year earlier than planned, for financial reasons. The last official flight of a Nimrod MR2 took place on 26 May 2010, with XV229 flying from RAF Kinloss to Kent International Airport to be used as an evacuation training airframe at the nearby MOD Defence Fire Training and Development Centre.
AEW3.
In the mid-1970s a modified Nimrod was proposed for the Airborne Early Warning (AEW) mission – again as a replacement for the Lancaster-derived, piston-engined Shackleton AEW.2. Eleven existing Nimrod airframes were to be converted by British Aerospace at the former Avro plant at Woodford to house the GEC Marconi radars in a bulbous nose and tail. The Nimrod AEW3 project was plagued by cost over-runs and problems with the GEC 4080M computer used. Eventually, the MoD recognised that the cost of developing the radar system to achieve the required level of performance was prohibitive and the probability of success very uncertain, and in December 1986 the project was cancelled. The RAF eventually received seven Boeing E-3 Sentry aircraft instead.
MRA4.
The Nimrod MRA4 was intended to replace the capability provided by the MR2. It was essentially a new aircraft, with current-generation Rolls-Royce BR710 turbofan engines, a new larger wing, and fully refurbished fuselage. However the project was subject to delays, cost over-runs, and contract re-negotiations; the type had been originally intended to enter service in 2003. The MRA4 was cancelled in 2010 as a result of the Strategic Defence and Security Review at which point it was £789 million over-budget and nine years late; the development airframes were also scrapped. The cancellation of the MRA4 marked an abortive end of the Nimrod's era; the functions it provided were largely abandoned leading to a significant UK capability gap. A few functions were dispersed to other assets, including the use of unmanned aerial vehicles (UAVs) to conduct limited maritime surveillance.
Design.
Overview.
The Nimrod was the first jet-powered maritime patrol aircraft (MPA) to enter service, being powered by the Rolls-Royce Spey turbofan engine. Aircraft in this role have been commonly propelled by piston or turboprop powerplants instead to maximise fuel economy and enable maximum patrol time on station; advantages of the Nimrod's turbofan engines included greater speed and altitude capabilities, it was also more capable of evading detection methods by submarines, whereas propeller-driven aircraft are more detectable underwater to standard acoustic sensors. Inflight, the Nimrods had a flight endurance of ten hours without aerial refuelling; the MR2s were later fitted to receive mid-air refuelling in response to demands in the Falklands War.
At the start of a patrol mission all four engines would normally be running, but as the aircraft's weight was reduced by the consumption of onboard fuel up to two engines could be intentionally shut down, allowing the remaining engines to be operated in a more efficient manner. Instead of relying on ram air to restart an inactive engine, compressor air could be crossfed from a live engine to a starter turbine; the crossfeed duct was later discovered to be a potential fire hazard. Similarly, the two hydraulic systems onboard were designed to be powered by the two inner engines that would always be running. Electrical generation was designed to far exceed the consumption of existing equipment to accommodate additional systems installed over the Nimrod's operational service.
The standard Nimrod fleet carried out three basic operational roles during their RAF service: Anti-Submarine Warfare duties typically involved surveillance over an allocated area of the North Atlantic to detect the presence of Soviet submarines in that area and to track their movements. In the event of war, reconnaissance information gathered during these patrols would be shared with other allied aircraft to enable coordinated strikes at both submarines and surface targets. Search and rescue (SAR) missions were another important duty of the RAF's Nimrod fleet, operating under the Air Rescue Coordination Center at RAF Kinloss, and were a common sight in both military and civil maritime incidents. Throughout the Nimrod's operational life, a minimum of one aircraft was being held in a state of readiness to respond to SAR demands at all times.
Avionics.
The Nimrod featured a large crew of up to 25 personnel, although a typical crew numbered roughly 12 members, most of which operated the various onboard sensor suites and specialist detection equipment. A significant proportion of the onboard sensor equipment was housed outside the pressure shell inside the Nimrod's distinctive pannier lower fuselage. Sensor systems included radar, sonar, and the magnetic anomaly detector; a 'sniffer' could detect exhaust fumes from diesel submarines as well. The Nimrod and its detection capabilities was an important component of Britain's military defence during the height of the Cold War.
The Nimrod's navigational functions were computerised, and were managed from a central tactical compartment housed in the forward cabin; various aircraft functions such as weapons control and information from sensors such as the large forward doppler radar were displayed and controlled at the tactical station. The flight systems and autopilot could be directly controlled by navigator's stations in the tactical compartment, giving the navigator nearly complete aircraft control. The navigational systems comprised digital, analogue, and electro-mechanical elements; the computers were directly integrated with most of the Nimrod's guidance systems such as the air data computer, astrocompass, inertial guidance and doppler radar, navigation information could also be manually input by the operators.
Upon its introduction to service, the Nimrod was hailed as possessing advanced electronic equipment such as onboard digital computers; the increased capability of these electronic systems allowed the RAF's fleet of 46 Nimrod aircraft to provide equal coverage to that of the larger fleet of retiring Avro Shackletons. The design philosophy of these computerised systems was that of a 'man-machine partnership'; while onboard computers performed much of the data sift and analysis processes, decisions and actions on the basis of that data remained in the operator's hands. To support the Nimrod's anticipated long lifespan, onboard computers were designed to be capable of integrating with various new components, systems, and sensors that could be added in future upgrades. After a mission, gathered information could be extracted for review purposes and for further analysis.
Armaments and equipment.
The Nimrod featured a sizeable bomb bay in which, in addition to armaments such as torpedos and missiles, could be housed a wide variety of specialist equipment for many purposes, such as up to 150 sonobuoys for ASW purposes or multiple air-deployed dinghies and droppable survival packs such as Lindholme Gear for SAR missions; additional fuel tanks and cargo could also be carried in the bomb bay during ferrying flights. Other armaments equippable in the bomb bay include mines, bombs, and nuclear depth charges; later munitions included the Sting Ray torpedo and Harpoon missile for increased capabilities.
The Nimrod could also be fitted with two detachable pylons mounted underneath the wings to be used with missiles such as the Martel; two specialised pylons were later added to enable the equipping of Sidewinder missiles, used for self-defence purposes from hostile aircraft. A powerful remote-controlled searchlight was installed underneath the starboard wing for SAR operations. For reconnaissance missions, a pair of downward-facing cameras suited to low and high-altitude photography were also equipped on the Nimrod; in later years a newer electro-optical camera system was installed for greater imaging quality.
Various new ECMs and electronic support systems were retrofitted onto the Nimrod fleet in response to new challenges and to increase the type's defensive capabilities; additional equipment also provided more effective means of identification and communication. A number of modifications were introduced during the 1991 Gulf War, a small number of MR2s were fitted with improved Link 11 datalinks, new defensive ECM equipment including the first operational use of a towed radar decoy, and a forward looking infrared turret under the starboard wing.
Operational history.
Introduction to service.
The Nimrod first entered squadron service with the RAF at RAF St Mawgan in October 1969. These initial aircraft, designated as Nimrod MR1, were intended as a stop-gap measure, and thus were initially equipped with many of the same sensors and equipment as the Shackletons they were supplementing. While some improvements were implemented on the MR1 fleet to enhance their detection capabilities, the improved Nimrod MR2 variant entered service in August 1979 following a lengthy development process. The majority of the Nimrod fleet operated from RAF Kinloss.
Operationally, each active Nimrod would form a single piece of a complex submarine detection and monitoring mission. An emphasis on real-time intelligence sharing was paramount to these operations; upon detecting a submarine, Nimrod aircrews would inform Royal Navy frigates and other NATO-aligned vessels to pursuit in an effort to continuously monitor Soviet submarines. The safeguarding of the Royal Navy's Resolution-class ballistic missile submarines, which were the launch platform for Britain's nuclear deterrent, was viewed as being of the upmost priority.
Falklands War.
Nimrods were first deployed to Wideawake airfield on Ascension Island on 5 April 1982, the type at first being used to fly local patrols around Ascension to guard against potential Argentine attacks, and to escort the British Task Force as it sailed south towards the Falkands, with Nimrods also being used to provide search and rescue as well as communications relay support of the Operation Black Buck bombing raids by Avro Vulcans. As the Task Force neared what would become the combat theatre and the threat from Argentine submarines rose, the more capable Nimrod MR2s took on operations initially performed by older Nimrod MR1s. Aviation author Chris Chant has claimed that the Nimrod R1 also conducted electronic intelligence missions operating from Punta Arenas in neutral Chile.
The addition of air-to-air refuelling probes allowed operations to be carried out in the vicinity of the Falklands, while the aircraft's armament was supplemented by the addition of 1,000 lb (450 kg) general purpose bombs, BL755 cluster bombs and AIM-9 Sidewinder air-to-air missiles. The use of air-to-air refuelling allowed extremely long reconnaissance missions to be mounted, one example being a 19-hour 15-minute patrol conducted on 15 May 1982, which passed within 60 miles (97 km) of the Argentine coast to confirm that Argentine surface vessels were not at sea. Another long-range flight was carried out by an MR2 on the night of 20/21 May, covering a total of 8,453 miles (13,609 km), the longest distance flight carried out during the Falklands War. In all, Nimrods flew 111 missions from Ascension in support of British operations during the Falklands War.
Gulf War.
A detachment of three Nimrod MR2s was deployed to Seeb in Oman in August 1990 as a result of the Iraqi invasion of Kuwait, carrying out patrols over the Gulf of Oman and Persian Gulf. Due to the level of threats present in the Gulf theatre, operational Nimrods were quickly retrofitted with a Marconi-towed active decoy. Once hostilities commenced, the Nimrod detachment, by now increased to five aircraft, concentrated on night patrols, with daylight patrols carried out by US Navy Lockheed P-3 Orions. Nimrods were used to guide Westland Lynx helicopters and Grumman A-6 Intruder attack aircraft against Iraqi patrol vessels, being credited with assisting in sinking or damaging 16 Iraqi vessels.
After the ground offensive against Iraqi forces had ended, Britain elected to maintain an RAF presence in the region through assets such as the Nimrod and other aircraft. Nimrod R1s operated from August 1990 to March 1991 from Cyprus, providing almost continuous flying operations from the start of the ground offensive. Each R1 was retrofitted with the same Marconi towed active decoy as well as under wing chaff/flare dispensers, reportedly sourced from the Tornado fleet.
Afghanistan and Iraq War.
Nimrods were again deployed to the Middle East as part of the British contribution to the US-led invasion of Afghanistan; missions in this theatre involved the Nimrods performing lengthy overland flights for intelligence-gathering purposes. On 2 September 2006, 12 RAF personnel were killed when a Nimrod MR2 was destroyed in a midair explosion following an onboard fire over Afghanistan, it was the single greatest loss of British life since the Falklands War. The outbreak of the Iraq War in March 2003 saw the RAF's Nimrods being used for operations over Iraq, using the aircraft's sensors to detect hostile forces and to direct attacks by friendly coalition forces.
Search and rescue.
While the Nimrod MR1/MR2 was in service, one aircraft from each of the squadrons on rotation was available for search and rescue operations at one-hour standby. The standby aircraft carried two sets of Lindholme Gear in the weapons bay. Usually one other Nimrod airborne on a training mission would also carry a set of Lindholme Gear. As well as using the aircraft sensors to find aircraft or ships in trouble, it was used to find survivors in the water, with a capability to search areas of up to 20000 sqmi. The main role would normally be to act as on-scene rescue coordinator to control ships, fixed-wing aircraft, and helicopters in the search area.
The Nimrod was most often featured in the media in relation to its search-and-rescue role, such as in the reporting of major rescue incidents. In August 1979, several Nimrods were involved in locating yachting competitors during the disaster-stricken 1979 Fastnet race and coordinated with helicopters in searches for survivors from lost vessels. In March 1980, the "Alexander L. Kielland" was a Norwegian semi-submersible drilling rig that capsized whilst working in the Ekofisk oil field killing 123 people; six different Nimrods searched for survivors and took turns to provide rescue co-ordination, involving the control of 80 surface ships and 20 British and Norwegian helicopters. In an example of the search capabilities, in September 1977 when an attempted crossing of the North Atlantic in a Zodiac inflatable dinghy went wrong, a Nimrod found the collapsed dinghy and directed a ship to it.
Operation Tapestry.
The Nimrods were often used to enforce Operation Tapestry. Tapestry is a codeword for the activities by ships and aircraft that protect the United Kingdom's Sovereign Sea Areas, including the protection of fishing rights and oil and gas extraction. Following the establishment of a 200 nmi Exclusive Economic Zone (EEZ) at the beginning of 1977 the Nimrod fleet was given the task of patrolling the 270000 sqmi area. The aircraft would locate, identify, and photograph vessels operating in the EEZ. The whole area was routinely patrolled; in addition to surveillance, the aircraft would communicate with all oil and gas platforms. In 1978, an airborne Nimrod arrested an illegal fishing vessel in the Western Approaches and made the vessel proceed to Milford Haven for further investigation. During the Icelandic Cod Wars of 1972 and 1975–1976, the Nimrod fleet closely cooperated with Royal Navy surface vessels to protect British civilian fishing ships.
Accidents and incidents.
Five Nimrods were lost in accidents during the type's service with the RAF:
References.
Bibliography.
</dl>

</doc>
<doc id="40298" url="http://en.wikipedia.org/wiki?curid=40298" title="Jean-Claude Killy">
Jean-Claude Killy

Jean-Claude Killy (born 30 August 1943) also known as Gilette is a former French World Cup alpine ski racer. Born in Saint-Cloud, Hauts-de-Seine, he dominated the sport in the late 1960s. He was a triple Olympic champion, winning the three alpine events at the 1968 Winter Olympics, becoming the most successful athlete there. He also won the first two World Cup titles, in 1967 and 1968.
Early life.
Killy was born in Saint-Cloud, a suburb of Paris, during the Nazi occupation of World War II, but was brought up in Val-d'Isère in the Alps, where his family had relocated in 1945 following the war. His father, Robert, was a former Spitfire pilot for the Free French, and opened a ski shop in the Savoie village, and would later operate a hotel. In 1950, his mother Madeline abandoned the family for another man, leaving Robert to raise Jean-Claude, age 7, his older sister (France), and their infant brother (Mic). Jean-Claude was sent to boarding school in Chambéry, 80 mi down the valley, but he despised being shut up in a classroom.
Early career.
Killy turned his attention to skiing rather than school. His father allowed him to drop out at age 15, and he made the French national junior team a year later. As a young racer, Killy was fast, but did not usually complete his races, and the early 1960s were not entirely successful for him.
In December 1961, at age 18, Killy won his first international race, a giant slalom. The event took place in his home village of Val-d'Isere. Killy had started 39th, a position that should have been a severe disadvantage.
The French coach picked Killy for the giant slalom in the 1962 World Championships in Chamonix, France, 50 miles (80 km) away in the shadow of Mont Blanc. But Killy, unaware of his selection, was still attempting to qualify for the downhill event in northeastern Italy at Cortina d'Ampezzo. Only three weeks before the world championships, he skied in his typical reckless style. About two hundred yards (180 m) from the finish, Killy hit a stretch of ice in a compression and went down, rose immediately, then crossed the finish on just one ski—and the fastest time. Unfortunately, his other leg was broken, and he watched the 1962 World Championships on crutches.
Two years later, at age 20, Killy was entered in all three of the men's events at the 1964 Olympics, because his coach wanted to prepare him for 1968. Unfortunately, Killy was plagued by recurrences of amoebic dysentery and hepatitis, ailments that he had contracted in 1962 during a summer of compulsory service with the French Army in Algeria. His form was definitely off, and he fell a few yards after the start of the downhill, lost a binding in the slalom, and finished fifth in the giant slalom, in which he had been the heavy favorite. Yet a few weeks later, he dominated a giant slalom race at Garmisch-Partenkirchen, in Bavaria, counting for the prestigious Arlberg-Kandahar events, the oldest 'Classic' in the sport. A year later, he also triumphed at another major competition, the slalom of the Hahnenkamm races at Kitzbühel that he clinched three times in a row until 1967.
Although the first half of the decade was a relative disappointment, Killy began to strongly improve his results afterwards to become one of the best technical ski racers. In August 1966, the Frenchman, nicknamed 'Toutoune' by some of his colleagues and friends, scored his first win in a downhill race against an international field at the 1966 World Championships in Portillo, Chile, and also took gold in the combined. Killy was peaking as the first World Cup season was launched in January 1967, with the 1968 Winter Olympics in France only a year away.
Dominance – 1967–68.
World Cup results.
Individual races.
Killy was the first World Cup champion in 1967, winning 12 of 17 races to easily take the overall title. He also won the season standings in each of the three "Classic" alpine disciplines; he won all five of the downhill races and four of the five giant slalom races.
The following year, Killy won the Triple Crown of Alpine Skiing with a sweep of all three Olympic gold medals (downhill, giant slalom, and slalom) in controversial circumstances at the 1968 Winter Olympics in Grenoble, France. By finishing first in all races, he also captured the FIS world championship title in the combined event.
Killy wasn't just faster than the other skiers, he was smarter. Electrical timing by Omega was accurate to one-hundredth of a second. The starting official counted aloud, "3-2-1-Go" and the skier's boot moved forward to push a pivoting rod aside and start the timer. Everyone knew that the closer they got to the bar, the less distance they would travel. Killy, however, relied on enormous upper-body strength and outwitted his opponents. Rather than crowd as close as possible to the bar, Killy knew that he was allowed a 6-second window to push it aside. When the official began counting, he could trip the lever any time he chose from the beginning of the "3-" call and up to 3 seconds after the "Go" signal. Therefore, he rose backward, raised his body completely off the ground with his arms and poles, pulled his feet backwards, and propelled himself forward. Instead of beginning from a standing start right at the bar, as everyone else did, he hit the bar while already moving forward, giving himself a slight edge. This spectacular start certainly helped him to beat his teammate Guy Perillat by a few hundredths in the Olympic downhill despite ruining the wax covering the base of his skis moving over a plate of icy snow an hour prior to his start.
With the Olympic events included (for the only time) in the World Cup standings, Killy easily defended his title in 1968 as the overall champion, placing first in the giant slalom and second in the downhill and slalom season standings. He retired following the 1968 season, and moved to Geneva, Switzerland, in 1969.
World Championship results.
1962: injured
Post-Olympic career.
In May 1968, Killy signed with International Management Group, the sports management firm headed by Mark McCormack. After racing on Dynamic VR17 and Rossignol skis during the part of his career when he was dominant, Killy signed a deal with Head Skis in the fall 1968 to endorse a metal and fiberglass ski named for him, the "Killy 800". Head, which was acquired by AMF the following year, manufactured a line of Killy skis for at least two years.
In television advertisements, Killy promoted the American Express card. He also became a spokesman for Schwinn bicycles, United Airlines, and Chevrolet automobiles; the latter, a role detailed by journalist Hunter S. Thompson in his 1970 article "The Temptations of Jean-Claude Killy" for "Scanlan's Monthly".
Killy starred as a ski instructor in the 1972 crime movie "Snow Job", released in the UK as "The Ski Raiders", and US TV as "The Great Ski Caper". American children in the early 1970s knew Killy from a TV commercial where he introduces himself, his thick accent making his name into "Chocolate Kitty." Killy played himself in the 1983 movie Copper Mountain: A Club Med Experience, starring Jim Carrey and Alan Thicke, set at Copper Mountain, Colorado.
Jean-Claude Killy also had a short career as a racing driver between 1967 and 1970, participating in several car races including at Monza. In team with fellow Frenchman Bernard Cahier, Killy was 7th overall in the 1967 Targa-Florio in a Porsche 911 S and first in the GT classification.
In November 1972, Killy came out of ski racing retirement at age 29 to compete on the pro circuit in the U.S. for two seasons. After a spirited challenge from two-time defending champion Spider Sabich, Killy won the 1973 season title, taking $28,625 in race winnings and a $40,000 bonus for the championship.
He missed the next season, won by Hugo Nindl, due to a recurring stomach ailment, then returned in the fall of 1974. Injuries slowed him and he finished well out of the 1975 standings, won by Hank Kashiwa.
In addition to trying his skill as a car racer, Killy made two television series. One, "The Killy Style", was a thirteen-week series that showcased various ski resorts, and the other, "The Killy Challenge", featured him racing against celebrities, who were all given handicaps. He was also sponsored by a champagne company, Moët & Chandon, which paid him to be seen with a bottle of their champagne on his table everywhere he went.
In 1974 Killy, as part of this sponsorship deal was paid to ski down the previously unskied eastern slope of Mt Ngauruhoe (Peter Jackson's "Mt Doom") in New Zealand. The average slope on this side of the active volcano is 35 degrees. Radar recorded his speed at over 100 mph, and it took two takes, as cloud cover spoiled the first.
From 1977 to 1994, he was a member of the Executive board of the Alpine Skiing Committee of the FIS. Killy served as co-president of the 1992 Winter Olympics, held in Albertville, France, and as the President of the Société du Tour de France cycling race between 1992 and 2001. From 1995 to 2014 he was a member of the International Olympic Committee and chaired the coordination committee for Turin 2006 and Sochi 2014. He is a Honorary Member since them.
The ski area of Val d'Isère and Tignes in the French Alps was given the name l'Espace Killy, in his honor.
Killy became Grand Officer of the Légion d'honneur in 2000.
Intrawest credits Killy with the design of a ski trail, "Cupp Run," at their Snowshoe resort in West Virginia.
Personal life.
From 1973 to 1987, he was married to French actress Danielle Gaubert, until her death from cancer. Together they had a daughter, Emilie; he also adopted her two children from her first marriage to Rhadamés Trujillo, the son of Rafael Trujillo, the assassinated dictator of the Dominican Republic. Gaubert and Trujillo were divorced in 1968 and later that year she met Killy.

</doc>
<doc id="40302" url="http://en.wikipedia.org/wiki?curid=40302" title="William Hale Thompson">
William Hale Thompson

William Hale Thompson (May 14, 1869 – March 19, 1944) was Mayor of Chicago from 1915 to 1923 and again from 1927 to 1931. Known as "Big Bill", Thompson was the last Republican to serve as Mayor of Chicago (as of 2015). He ranks among the most unethical mayors in American history.
Biography.
Thompson was born in Boston, Massachusetts to William Hale and Mary Ann Thompson, but his family moved to Chicago when he was only nine days old. Instead of college, he travelled in Europe and then took up ranching in Texas and New Mexico, returning to Chicago in 1892 after his father's death.
Thompson began his political career in 1900, when he ran for and narrowly won a position as alderman of the 2nd Ward.
In 1915 he was elected as the 41st Mayor of Chicago. He was also the last Republican to be elected Mayor of Chicago. Early in his mayoral career, Thompson began to amass a war chest to support an eventual run for the Presidency by charging city drivers and inspectors $3 per month. He was mayor during the Chicago Race Riot of 1919 and was said to have had control of the 75,000 African-American voters in his day.
He declined to run for reelection in 1923 and he was succeeded to the office by William Emmett Dever. While out of office, Thompson organized a "scientific" expedition to search for tree-climbing fish in the South Seas (actually just a crude attempt to keep his name in the public eye—the expedition never got farther than New Orleans).
He ran again in 1927 during city wide gang war. Always a flamboyant campaigner, Thompson held a debate between himself and two live rats which he used to portray his opponents. Pledging to clean up Chicago and remove the crooks, Thompson instead turned his attention to the reformers, whom he considered the real criminals. According to Thompson, at this time the biggest enemy the United States had was King George V of the United Kingdom. Thompson promised his supporters that if they ever met, Thompson would punch the king in the nose. Al Capone's support allowed Thompson to return to the mayor's office using such tactics as the "Pineapple Primary" which occurred April 10, 1928, so-called because of the hand grenades thrown at polling places to disrupt voting. The St. Valentine's Day Massacre also took place while Thompson was mayor.
Thompson blamed Ruth Hanna McCormick's lack of support for his loss at the 1928 Republican National Convention, and he returned the favor during her 1930 campaign for the United States Senate. Thompson had had a longstanding rivalry with the McCormicks. He intensely disliked Robert Rutherford McCormick who published the "Chicago Tribune". U.S. Senator Joseph Medill McCormick, was the publisher's brother, and after his death, his widow ran against Thompson for the vacant seat.
Amid growing discontent with Thompson's leadership, particularly in the area of cleaning up Chicago's reputation as the capital of organized crime, he was defeated in 1931 by Democrat Anton Cermak. Cermak was an immigrant from Bohemia, and Thompson used this fact to belittle him with ethnic slurs such as:
Cermak replied to these with, "He doesn't like my name...It's true I didn't come over on the Mayflower, but I came over as soon as I could," which was a sentiment to which ethnic Chicagoans could relate and Thompson's slurs largely backfired. 
After Thompson's defeat, the "Chicago Tribune" wrote that
Upon Thompson's death, two safe deposit boxes in his name were discovered to contain nearly $1.5 million in cash.

</doc>
<doc id="40303" url="http://en.wikipedia.org/wiki?curid=40303" title="Anton Cermak">
Anton Cermak

Anton "Tony" Joseph Cermak (Czech: Antonín Josef Čermák, ]; May 9, 1873 – March 6, 1933) was an American politician of Czech origin who served as the mayor of Chicago, Illinois from 1931 until his assassination in 1933.
Early life and career.
Born in Kladno, Austria-Hungary (now in the Czech Republic), Cermak emigrated with his parents to the United States in 1874. Cermak grew up in the town of Braidwood, Illinois, southwest of Chicago, and later moved to Chicago. He began his political career as a precinct captain and in 1902 was elected to the Illinois House of Representatives. Seven years later, he would take his place as alderman of the 12th Ward. Cermak was elected president of the Cook County Board of Commissioners in 1922, chairman of the Cook County Democratic Party in 1928, and mayor of Chicago in 1931. In 1928 he ran for the United States Senate and was defeated by Republican Otis F. Glenn, receiving 46% of the vote.
Campaign for Mayor.
His mayoral victory came in the wake of the Great Depression and the deep resentment many Chicagoans had of Prohibition and the increasing violence resulting from organized crime's control of Chicago, typified by the St. Valentine's Day Massacre.
The many ethnic groups such as Poles, Czechs, Ukrainians, Jews, Italians, and African Americans that began to settle in Chicago in the early 1900s were mostly detached from the political system, due in part to lack of organization which led to underrepresentation in the City Council. As an immigrant himself, Cermak recognized Chicago's relatively new immigrants as a significant population of disenfranchised voters and a large power base for Cermak and his local Democratic organization.
Before Cermak, the Democratic party in Cook County was run by Irish Americans. As Cermak climbed the local political ladder, the resentment of the Party leadership grew. When the bosses rejected his bid to become the mayoral candidate, Cermak swore revenge. He formed his political army from the non-Irish elements, and even persuaded black politician William L. Dawson to switch from the Republican to the Democratic Party. Dawson later became U.S. Representative (from the 1st District) and soon the most powerful black politician in Illinois.
Cermak's political and organizational skills helped create one of the most powerful political organizations of his day. With support from Franklin D. Roosevelt on the national level, Cermak gradually wooed members of Chicago's growing black community into the Democratic fold. Walter Wright, the superintendent of parks and aviation for the city of Chicago also aided Cermak in stepping into office.
When Cermak challenged the incumbent "Big Bill" Thompson in the 1931 mayor's race, Thompson, representative of Chicago's existing power structure, responded with ethnic slurs:
Cermak's reply, "He doesn't like my name... it's true I didn't come over on the "Mayflower", but I came over as soon as I could." It was a sentiment to which ethnic Chicagoans could relate and Thompson's slur largely backfired. 
The flamboyant Thompson's reputation as a buffoon and the voters' disgust with the corruption of his machine and his inability or unwillingness to clean up organized crime in Chicago were cited as major factors in Cermak capturing 58% of the vote in the mayoral election on April 6, 1931. Cermak's victory finished Thompson as a political power and largely ended the Republican Party's power in Chicago; indeed, all of the mayors of Chicago since 1931 have been members of the Democratic Party.
Mayor.
For nearly his entire administration, Cermak had to deal with a major tax revolt. From 1931 to 1933, the Association of Real Estate Taxpayers mounted a "tax strike." At its height, ARET, which was headed by John M. Pratt and James E. Bistor, had over thirty thousand members. Much to Cermak's dismay, it successfully slowed down the collection of real estate taxes through litigation and promoting refusal to pay. In the meantime, the city found it difficult to pay teachers and maintain services.
Assassination.
While shaking hands with President-elect Franklin D. Roosevelt at Bayfront Park in Miami, Florida, on February 15, 1933, Cermak was shot in the lung and seriously wounded when Giuseppe Zangara, who at the time was believed to have been engaged in an attempt to assassinate Roosevelt, hit Cermak instead. At the critical moment, Lilian Cross, a doctor's wife, hit Zangara's arm with her purse and spoiled his aim. In addition to Cermak, Zangara hit four other people, one of whom, a woman, also died of her injuries. Zangara told the police that he hated rich and powerful people, but not Roosevelt personally.
Later, rumors circulated that Cermak, not Roosevelt, had been the intended target, as his promise to clean up Chicago's rampant lawlessness posed a threat to Al Capone and the Chicago organized crime syndicate. According to Roosevelt biographer Jean Edward Smith, there is no proof for this theory. One of the first people to suggest the organized crime theory was reporter Walter Winchell, who happened to be in Miami the evening of the shooting.
Long-time Chicago newsman Len O'Connor offers a different view of the events surrounding Cermak's death. He has written that aldermen "Paddy" Bauler and Charlie Weber informed him that relations between Cermak and FDR were strained because Cermak fought FDR's nomination at the Democratic convention in Chicago, and the legend that his last words were "I'm glad it was me instead of you" was, according to O'Connor, totally fabricated by Weber and Bauler.
Author Ronald Humble offers his view as to why Cermak was killed. In his book "Frank Nitti: The True Story of Chicago's Notorious Enforcer", Humble contends that Cermak was as corrupt as Thompson and that the Chicago Outfit hired Zangara to kill Cermak in retaliation for Cermak's attempt to murder Frank Nitti.
Death.
Cermak died at Jackson Memorial Hospital in Miami on March 6, partly because of his wounds. On March 30, however, his personal physician, Dr. Karl A. Meyer, said that Cermak's primary cause of death was ulcerative colitis, commenting, "The mayor would have recovered from the bullet wound had it not been for the complication of colitis. The autopsy disclosed the wound had healed.. the other complications were not directly due to the bullet wound." 
He was interred in a mausoleum at Bohemian National Cemetery in Chicago. The mayor's death was followed by a struggle for succession to his party chairmanship and to the mayor's office.
A plaque honoring Cermak still lies at the site of the assassination in Miami's Bayfront Park. It is inscribed with Cermak's alleged words to FDR after he was shot, "I'm glad it was me instead of you."
Following Cermak's death, 22nd Street, a major east-west artery that traversed Chicago's West Side and the close-in suburbs of Cicero and Berwyn, areas with a significant Czech population, was renamed Cermak Road. Zangara was electrocuted in Florida's electric chair on March 20, 1933, for he could not be charged with murder until Cermak died.
In 1943, a Liberty ship, the SS "A. J. Cermak" was named in Cermak's honor. It was scrapped in 1964.
Family.
Cermak's son-in-law, Otto Kerner Jr., served as the 33rd Governor of Illinois and a federal circuit judge. His grandson, Frank J. Jirka, Jr., was with him in Miami when he was assassinated, later becoming a highly decorated UDT naval officer from wounds suffered at Iwo Jima (double amputee below the knee), after WWII he became a physician, and in 1983 president of the American Medical Association. Cermak's great niece Kajon Cermak is a broadcaster for a Southern California radio station.
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="40304" url="http://en.wikipedia.org/wiki?curid=40304" title="Jane Byrne">
Jane Byrne

Jane Margaret Byrne (née Burke; May 24, 1933 – November 14, 2014) was an American politician who was Mayor of Chicago from April 16, 1979 to April 29, 1983. She was the first and only female mayor of Chicago, the second largest city in the United States at the time, and the largest U.S. city to have had a female mayor as of 2014. Byrne first entered politics to volunteer in John F. Kennedy's campaign for president in 1960. During that campaign she first met Mayor Richard J. Daley. 
In 1968, Daley appointed her head of Chicago's consumer affairs department. Byrne held that post until she was fired by Mayor Michael Bilandic in 1977. She challenged Bilandic in the 1979 Democratic mayoral primary, the real contest in this heavily Democratic city. At first, political observers believed her to have little chance of winning. A memorandum inside the Bilandic campaign said it should portray her as, “a shrill, charging, vindictive person — and nothing makes a woman look worse.” However, a series of major snowstorms in January paralyzed the city and caused Bilandic to be seen as an ineffective leader. Jesse Jackson endorsed Byrne. Many Republican voters voted in the Democratic primary to beat Bilandic and the "Machine". Infuriated voters in the North Side and Northwest Side retaliated against Bilandic for the Democratic Party's slating of only South Side candidates for the mayor, clerk, and treasurer (the outgoing city clerk, John C. Marcin, was from the Northwest Side). These four factors combined to give Byrne a razor-thin 51% to 49% victory over Bilandic in the primary. She then won the general election with 82% of the vote, still the largest margin in a Chicago mayoral election. 
Mayor of Chicago (1979–1983).
Byrne positioned herself as a reformer in her first campaign. She made inclusive moves as mayor, such as hiring the first black school superintendent Ruth B. Love, and she was the first mayor to recognize the gay community. In March 1981, she moved into the crime-ridden Cabrini–Green Homes housing project for a 3-week period to bring attention and resources to its high crime rate. In her first three months in office, she faced strikes by labor unions as the city’s transit workers, public school teachers and firefighters all went on strike. She effectively banned handgun possession for guns unregistered or purchased after the enactment of an ordinance. That two-year re-registration program effectively banned handgun possession. Byrne used special events, such as ChicagoFest, to revitalize Navy Pier and the downtown Chicago Theatre. She endorsed Senator Edward Kennedy for president in 1980, but could not stop President Jimmy Carter from winning the Illinois Democratic Primary. However, her attempt to block the election of Richard M. Daley, the son of her late mentor, to the prominent position of Cook County States' Attorney (chief local prosecutor) in 1980 failed as Daley defeated Byrne's candidate, 14th Ward Alderman Edward M. Burke in the Democratic Primary and GOP incumbent Bernard Carey in the general election. In 1982, she supported the Cook County Democratic Party's replacement of its chairman, County Board President George Dunne, with her city-council ally, Alderman Edward Vrdolyak.
The "Chicago Sun Times" reported that her enemies publicly mocked her as “that crazy broad” and “that skinny bitch” and worse.
On November 11, 1981, Dan Goodwin, who had successfully climbed the Sears Tower, battled for his life on the side of the John Hancock Center. William Blair, Chicago's fire commissioner, had ordered the Chicago Fire Department to stop Goodwin by directing a full power fire hose at him and by using fire axes to break window glass in Goodwin's path. Mayor Byrne rushed to the scene and ordered the fire department to stand down. Then, through a smashed out 38th floor window, she told Goodwin, who was hanging from the building's side a floor below, that though she did not agree with his climbing of the John Hancock Center, she certainly opposed the fire department knocking him to the ground below. Byrne then allowed Goodwin to continue to the top.
Byrne was narrowly defeated in the 1983 Democratic primary for mayor by Harold Washington; the younger Daley ran a close third. Washington won the Democratic primary with just 36% of the vote; Byrne had 33%. Washington went on to win the general election. 
Later Career.
Byrne ran against Washington again in the 1987 Democratic primary, but was narrowly defeated. She endorsed Washington for the general election, in which he defeated two Democrats running under other parties' banners (Edward Vrdolyak and Thomas Hynes) and a Republican.
Byrne next ran in the 1988 Democratic primary for Cook County Circuit Court Clerk. She faced the Democratic Party's slated candidate, Aurelia Pucinski (who was endorsed by Mayor Washington and is the daughter of then-Alderman Roman Pucinski). Pucinski defeated Byrne in the primary and Vrdolyak, by then a Republican, in the general election.
Byrne's fourth run for mayor involved a rematch against Daley in 1991. Byrne received only 5.9% of the vote, a distant third behind Daley and Alderman Danny K. Davis.
Personal life.
Byrne was born Jane Margaret Burke on May 24, 1933. In 1956, she married William P. Byrne, a Marine. The couple had a daughter, Katherine C. Byrne (born 1957). On May 31, 1959, while flying from Marine Corps Air Station Cherry Point to Naval Air Station Glenview in a Skyraider, Lt. Byrne attempted to land in a dense fog. After being waved off for landing twice, his plane's wing struck the porch of a nearby house and the plane crashed into Sunset Memorial Park, killing him. Byrne married journalist Jay McMullen in 1978, and they remained married until his death from lung cancer in 1992. Byrne lived in the same apartment building from the 1970s until her death in 2014. She has one grandchild, Willie. Her daughter, Kathy, is a lawyer with a Chicago firm. Mayor Byrne's book, "My Chicago" (ISBN 0-8101-2087-9), was published in 1992, and covers her life through her political career. On May 16, 2011, Byrne attended the inauguration of the city's new mayor, Rahm Emanuel.
Death and legacy.
Byrne had entered hospice care and died on November 14, 2014 in Chicago, aged 81, from complications of a stroke she suffered in January 2013. She was survived by her daughter Katherine and her grandson Willie. Her funeral Mass was held at St. Vincent de Paul on Monday, November 17, 2014. She was buried at Interment Calvary Cemetery in Evanston, Illinois. In a dedication ceremony held on August 29, 2014, Governor Pat Quinn renamed the Circle Interchange in Chicago the Jane Byrne Interchange. In July 2014, the Chicago City Council voted to rename the plaza surrounding the historic Chicago Water Tower on North Michigan Avenue the Jane M. Byrne Plaza in her honor.

</doc>
<doc id="40305" url="http://en.wikipedia.org/wiki?curid=40305" title="Harold Washington">
Harold Washington

Harold Lee Washington (April 15, 1922  – November 25, 1987) was an American lawyer, politician and elected in 1983 as the 51st Mayor of Chicago. He was the first African-American Mayor of Chicago, serving from April 29, 1983 until his death on November 25, 1987. He was also a member of the U.S. House of Representatives from 1981 to 1983 representing the Illinois first district, and also previously served in the Illinois State Senate and the Illinois House of Representatives.
Early years.
Harold Washington was born on April 15, 1922, to Roy and Bertha Washington. His father had been one of the first precinct captains in the city, a lawyer and a Methodist minister. His mother, Bertha, left a small farm near Centralia, Illinois, to seek her fortune in Chicago as a singer. She married Roy Washington soon after arriving in Chicago and the couple had three children, one named Kevin and the other named Ramon Price (from a later marriage), who was a former artist and eventually became chief curator of The DuSable Museum of African American History.
Washington grew up in Bronzeville, a Chicago neighborhood that was the center of black culture for the entire Midwest in the early and middle 20th century. Washington attended DuSable High School, then a newly established racially segregated public high school, and was a member of its first graduating class. In a 1939 citywide track meet, Washington placed first in the 110 meter high hurdles event, and second in the 220 meter low hurdles event. Between his junior and senior year of high school, Washington dropped out, claiming that he no longer felt challenged by the coursework.
He worked at a meat-packing plant for a time before his father helped him get a job at the U.S. Treasury branch in the city. There he met Dorothy Finch, whom he married soon after; Washington was 20 years old and Dorothy was 17 years old. Seven months later, the U.S. was drawn into World War II with the bombing of Pearl Harbor by the Japanese on Sunday, December 7, 1941.
Military service.
In 1942, Washington was drafted into the United States Army for the war effort and after basic training, sent overseas as part of a racially-segregated unit of the U.S. Army Air Corps unit of Engineers. After the American invasion of the Philippines in 1944, on Leyte Island and later the main Luzon island, Washington was part of a unit building runways for bombers, protective fighter aircraft, refueling planes, and returning damaged aircraft. Eventually, Washington rose to the rank of First Sergeant in the Army Air Corps (later in the war renamed the U.S. Army Air Forces).
Roosevelt College.
In the summer of 1946, Washington, aged 24 and a war veteran, enrolled at Roosevelt College (now Roosevelt University). Washington joined other groups of students not permitted to enroll in other local colleges. Local estimates placed the student population of Roosevelt College at about 1/8 black and 1/2 Jewish. A full 75% of the students had enrolled because of the "nondiscriminatory progressive principles." He chaired a fund-raising drive by students, and then was named to a committee that supported city-wide efforts to outlaw "restrictive covenants" in housing, the legal means by which minorities (especially blacks ("negroes") and, to a lesser extent, Jews) were prohibited from purchasing real estate in predominantly white neighborhoods of the city.
In 1948, after the college had moved to the Auditorium Building, Washington was elected the third president of Roosevelt's student council. Under his leadership, the student council successfully petitioned the college to have student representation on Roosevelt's faculty committees. At the first regional meeting of the newly- founded National Student Association in the spring of 1948, Washington and nine other delegates proposed student representation on college faculties, and a "Bill of Rights" for students; both measures were roundly defeated.
The next year, Washington went to the state capital at Springfield to protest Illinois legislators' coming probe of "subversives". The probe of investigation would outlaw the Communist Party and require "loyalty oaths" for teachers. He led students' opposition to the bills, although they would pass later in 1949.
During his Roosevelt College years, Washington came to be known for his stability. His friends said that he had a "remarkable ability to keep cool", reason carefully and walk a middle line. Washington intentionally avoided extremist activities, including street actions and sit-ins against racially segregated restaurants and businesses. Overall, Washington and other radical activists ended up sharing a mutual respect for each other, acknowledging both Washington's pragmatism and the activists' idealism. With the opportunities found only at Roosevelt College in the late 1940s, Washington's time at the Roosevelt College proved to be pivotal. Washington graduated in August 1949, with a Bachelor of Arts (B.A.) degree. In addition to his activities at Roosevelt, he was a member of Phi Beta Sigma fraternity.
Northwestern University School of Law.
Washington then applied and was admitted to study law at the Northwestern University School of Law in Chicago. During this time, Washington was divorced from Dorothy Finch. By some accounts, Harold and Dorothy had simply grown apart after Washington was sent overseas during the war during the first year of his marriage. Others saw both as young and headstrong, the relationship doomed from the beginning. Another friend of Washington's deemed Harold "not the marrying kind." He would not marry again, but continued to have relationships with other women; his longtime secretary is said to have said, "If every woman Harold slept with stood at one end of City Hall, the building would sink five inches into LaSalle Street!".
At Northwestern Law School, Washington was the only black student in his class (there were six women in the class, one of them being Dawn Clark Netsch). As at Roosevelt, he entered school politics. In 1951, his last year, he was elected treasurer of the Junior Bar Association (JBA). The election was largely symbolic, however, and Washington's attempts to give the JBA more authority at Northwestern were largely unsuccessful. On campus, Washington joined the Nu Beta Epsilon fraternity, largely because he and the other minorities which constituted the fraternity were blatantly excluded from the other fraternities on campus. Overall, Washington stayed away from the activism that defined his years at Roosevelt. During the evenings and weekends, he worked to supplement his GI Bill income. He received his J.D. in 1952.
Legislative political career.
Working for Metcalfe.
From 1951 until he was first slated for election in 1965, Washington worked in the offices of the 3rd Ward Alderman, former Olympic athlete Ralph Metcalfe. Richard J. Daley was elected party chairman in 1952. Daley replaced C.C. Wimbush, an ally of William Dawson, on the party committee with Metcalfe. Under Metcalfe, the 3rd Ward was a critical factor in Mayor Daley's 1955 mayoral election victory and ranked first in the city in the size of its Democratic plurality in 1961. While working under Metcalfe, Washington began to organize the 3rd Ward's Young Democrats (YD) organization. At YD conventions, the 3rd Ward would push for numerous resolutions in the interest of blacks. Eventually, other black YD organizations would come to the 3rd Ward headquarters for advice on how to run their own organizations. Like he had at Roosevelt College, Washington avoided radicalism and preferred to work through the party to engender change. While working with the Young Democrats, Washington met Mary Ella Smith. They dated for the next 20 years, and in 1983 Washington proposed to Smith. In an interview with the Chicago Sun-Times, Smith said that she never pressed Washington for marriage because she knew Washington's first love was politics, saying, "He was a political animal. He thrived on it, and I knew any thoughts of marriage would have to wait. I wasn't concerned about that. I just knew the day would come."
In 1960, with Lemuel Bentley, Bennett Johnson, Luster Jackson and others, Washington founded the Chicago League of Negro Voters, one of the first African-American political organizations in the city. In its first election, Bentley drew 60,000 votes for city clerk. After dropping out of view after the elections, it resurfaced as the group Protest at the Polls in 1963. Washington participated in the planning process to further the goals of 3rd Ward YDs. By 1967, the independent candidates had gained traction within the black community, winning several aldermanic seats; in 1983, the League of Negro Voters were instrumental in Washington's run for mayor. By then, the YDs were losing influence in the party, as more black voters supported independent candidates.
Illinois House (1965–1976).
After the state legislature failed to reapportion districts as required by the census every ten years, an at-large election was held in January 1965 to elect 177 representatives. With the Republicans and Democrats combining to slate only 118 candidates, independent voting groups seized the opportunity to slate candidates. The League of Negro Voters created a "Third Slate" of 59 candidates, announcing the slate on June 27, 1964. Shortly afterwards, Daley put together a slate including Adlai Stevenson III and Washington. The Third Slate was then thrown out by the Illinois Election Board because of "insufficient signatures" on the nominating petitions. In the election, Washington received the second-largest amount of ballots, behind Stevenson.
Washington's years in the Illinois House were marked by tension with Democratic Party leadership. In 1967, he was ranked by the Independent Voters of Illinois (IVI) as the fourth-most independent legislator in the Illinois House and named Best Legislator of the Year. His defiance of the "idiot card", a sheet of paper that directed legislators' votes on every issue, attracted the attention of party leaders, who moved to remove Washington from his legislative position. Daley often told Metcalfe to dump Washington as a candidate, but Metcalfe did not want to risk losing the 3rd Ward's Young Democrats, who were mostly aligned with Washington. Washington backed Renault Robinson, a black police officer and one of the founders of the Afro-American Patrolmen’s League (AAPL). The aim of the AAPL was to fight racism directed against minority officers by the rest of the predominately white department. Soon after the creation of the group, Robinson was written up for minor infractions, suspended, reinstated, and then placed on the graveyard shift to a single block behind central police headquarters. Robinson approached Washington to fashion a bill creating a civilian review board, consisting of both patrolmen and officers, to monitor police brutality. Both black independent and white liberal legislators refused to back the bill, afraid to challenge Daley's grip on the police force.
After Washington announced he would support the AAPL, Metcalfe refused to protect him from Daley. Washington believed he had the support of John Touhy, Speaker of the House and a former party chair. Instead, Touhy criticized Washington and then allayed Daley's anger. In exchange for the party's backing, Washington would serve on the Chicago Crime Commission, the group Daley formed to investigate the AAPL's charges. The commission promptly found the AAPL's charges "unwarranted". An angry and humiliated Washington admitted that on the commission, he felt like Daley's "showcase nigger". In 1969, Daley removed Washington's name from the slate; only by the intervention of Cecil Partee, a party loyalist, was Washington reinstated. The Democratic Party supported Jim Taylor, a former professional boxer, Streets and Sanitation worker, over Washington. With Partee and his own ward's support, Washington defeated Taylor.
His years in the House of Representatives were focused on becoming an advocate for black rights. He continued work on the Fair Housing Act, and worked to strengthen the state's Fair Employment Practices Commission (FEPC). In addition, he worked on a state Civil Rights Act, which would strengthen employment and housing provisions in the federal Civil Rights Act of 1964. In his first session, all of his bills were sent to committee or tabled. Like his time in Roosevelt College, Washington relied on parliamentary tactics (e.g., writing amendments guaranteed to fail in a vote) to enable him to bargain for more concessions.
Washington also passed bills honoring civil rights figures. He passed a resolution honoring Metcalfe, his mentor. He also passed a resolution honoring James J. Reeb, a Unitarian minister who was beaten to death in Selma, Alabama by a segregationist mob. After the assassination of Martin Luther King, Jr., he introduced a bill aimed at making King's birthday a state holiday; it was tabled and later vetoed. It was not until 1973 that Washington was able, with Partee's help in the Senate, to have the bill enacted and signed by the governor. In 1975, Washington was named chairman of the Judiciary Committee with the election of William A. Redmond as Speaker of the House. The same year, Partee, now President of the Senate and eligible for his pension, decided to retire from the Senate. Although Daley and Taylor declined at first, at Partee's insistence, Washington was slated for the seat and received the party's support. In 1976, Washington was elected to the Illinois Senate.
Legal issues.
In addition to Daley's strong-armed tactics, Washington's time in the Illinois House was also marred by problems with tax returns and allegations of not performing services owed to his clients. In her biography, Levinsohn questions whether the timing of Washington's legal troubles was politically motivated. In November 1966, Washington was re-elected to the house over Daley's strong objections; the first complaint was filed in 1964; the second was filed by January 1967.
A letter asking Washington to explain the matter was sent on January 5, 1967. After failing to respond to numerous summons and subpoenas, the commission recommend a five-year suspension on March 18, 1968. A formal response to the charges did not occur until July 10, 1969. In his reply, Washington said that "sometimes personal problems are enlarged out of proportion to the entire life picture at the time and the more important things are abandoned." In 1970, the Board of Managers of the Chicago Bar Association ruled that Washington's license be suspended for only one year, not the five recommended; the total amount in question between all six clients was $205.
In 1971, Washington was charged with failure to file tax returns for four years, although the Internal Revenue Service (IRS) claimed to have evidence for nineteen years; top campaign aides later said that nineteen was closer to the truth. Judge Sam Perry noted that he was "disturbed that this case ever made it to my courtroom" — while Washington had paid his taxes, he ended up owing the government a total of $508 as a result of not filing his returns. Typically, the IRS handled such cases in civil court, or within its bureaucracy. Washington pleaded "no contest" and was sentenced to forty days in Cook County Jail, a $1,000 fine, and three years probation.
Illinois Senate (1976–1980).
Human Rights Act of 1980.
In the Illinois Senate, Washington's main focus worked to pass 1980's Illinois Human Rights Act. Legislators rewrote all of the human rights laws in the state, restricting discrimination based on "race, color, religion, sex, national origin, ancestry, age, marital status, physical or mental disability, military status, sexual orientation, or unfavorable discharge from military service in connection with employment, real estate transactions, access to financial credit, and the availability of public accommodations." The bill's origins began in 1970 with the rewriting of the Illinois Constitution. The new constitution required all governmental agencies and departments to be reorganized for efficiency. Republican governor James R. Thompson reorganized low-profile departments before his re-election in 1978. In 1979, during the early stages of his second term and immediately in the aftermath of the largest vote for a gubernatorial candidate in the state's history, Thompson called for human rights reorganization. The bill would consolidate and remove some agencies, eliminating a number of political jobs. Some Democratic legislators would oppose any measure backed by Washington, Thompson and other Republican legislators.
For many years, human rights had been a campaign issue brought up and backed by Democrats. Thompson's staffers brought the bill to Washington and other black legislators before it was presented to the legislature. Washington made adjustments in anticipation of some legislators' concerns regarding the bill, before speaking for it in April 1979. On May 24, 1979, the bill passed the Senate by a vote of 59 to one, with two voting present and six absent. The victory in the Senate was attributed by a Thompson staffer to Washington's "calm noncombative presentation".
However, the bill stalled in the house. State Representative Susan Catania insisted on attaching an amendment to allow women guarantees in the use of credit cards. This effort was assisted by Carol Moseley Braun, a representative from Hyde Park. State Representatives Jim Taylor and Larry Bullock introduced over one hundred amendments, including the text of the first ten amendments to the U.S. Constitution, to try to stall the bill. With Catania's amendment, the bill passed the House, but the Senate refused to accept the amendment. On June 30, 1979, the legislature adjourned.
U.S. House (1980–1983).
In 1980, Washington was elected to the U.S. House of Representatives in Illinois' 1st Congressional District. He defeated incumbent Representative Bennett Stewart in the Democratic primary. Anticipating that the Democratic Party would challenge him in his bid for re-nomination in 1982, Washington spent much of his first term campaigning for re-election, often travelling back to Chicago to campaign. Washington missed many House votes, an issue that would come up in his campaign for mayor in 1983. Washington's major congressional accomplishment involved legislation to extend the Voting Rights Act, legislation that opponents had argued was only necessary in an emergency. Others, including Congressman Henry Hyde, had submitted amendments designed to seriously weaken the power of the Voting Rights Act.
Although he had been called "crazy" for railing in the House of Representatives against deep cuts to social programs, Associated Press political reporter Mike Robinson noted that Washington worked "quietly and thoughtfully" as the time came to pass the act. During hearings in the South regarding the Voting Rights Act, Washington asked questions that shed light on tactics used to prevent African Americans from voting (among them, closing registration early, literacy tests, and gerrymandering). After the amendments were submitted on the floor, Washington spoke from prepared speeches that avoided rhetoric and addressed the issues. As a result, the amendments were defeated, and Congress passed the Voting Rights Act Extension. By the time Washington faced re-election in 1982, he had cemented his popularity in the 1st Congressional District. Jane Byrne could not find one serious candidate to run against Washington for his re-election campaign. He had collected 250,000 signatures to get on the ballot, although only 610 signatures (0.5% of the voters in the previous election) were required. With his re-election to Congress locked up, Washington turned his attention to the next Chicago mayoral election.
Mayor of Chicago (1983–1987).
In the February 22, 1983, Democratic mayoral primary, more than 100,000 new voters registered to vote. On the North- and Northwest Sides, the incumbent mayor Jane Byrne led and future mayor Richard M. Daley, son of the late Mayor Richard J. Daley, finished a close second. Harold Washington had massive majorities on the South- and West Sides. Southwest Side voters overwhelmingly supported Daley. Washington won with 37% of the vote, versus 33% for Byrne and 30% for Daley.
Although winning the Democratic primary is normally tantamount to election in heavily Democratic Chicago, after his primary victory Washington found that his Republican opponent, former state legislator Bernard Epton (earlier considered a nominal stand-in), was supported by many high-ranking Democrats and their ward organizations, including the chairman of the Cook County Democratic Party, Alderman Edward "Fast Eddie" Vrdolyak. Epton's campaign referred to, among other things, Washington's conviction for failure to file income tax returns (he had paid the taxes, but had not filed a return). Washington, on the other hand, stressed reforming the Chicago patronage system and the need for a jobs program in a tight economy. In the April 12, 1983, mayoral general election, Washington defeated Epton by 3.7%, 51.7% to 48.0%, to become mayor of Chicago. Washington was sworn in as mayor on April 29, 1983, and resigned his Congressional seat the following day.
During his tenure as mayor, Washington lived at the Hampton House apartments in the Hyde Park neighborhood of Chicago. He created the city's first environmental-affairs department under the management of longtime Great Lakes environmentalist Lee Botts. Washington's first term in office was characterized by conflict with the city council dubbed "Council Wars", referring to the then-recent "Star Wars" films and caused Chicago to be nicknamed "Beirut on the Lake". A 29-alderman City Council majority refused to enact Washington's legislation and prevented him from appointing nominees to boards and commissions. First-term challenges included city population loss, increased crime, and a massive decrease in ridership on the Chicago Transit Authority (CTA).
The 29, also known as the "Vrdolyak 29", were led by Alderman Ed Vrdolyak and Finance Chair Edward Burke. Parks superintendent Edmund Kelly also opposed the mayor. The three were known as "the Eddies" and were supported by the younger Daley (now State's Attorney), U.S. Congressmen Dan Rostenkowski and William Lipinski, and much of the Democratic Party. During his first city council meeting, Washington and the 21 supportive aldermen walked out of the meeting after a quorum had been established. Vrdolyak and the other 28 then chose committee chairmen and assigned aldermen to the various committees. Later lawsuits submitted by Washington and others were dismissed because it was determined that the appointments were legally made.
Washington ruled by veto. The 29 lacked the 30th vote they needed to override Washington's veto; female and African American aldermen supported Washington despite pressure from the Eddies. Meanwhile, in the courts, Washington kept the pressure on to reverse the redistricting of city council wards that the city council had created during the Byrne years. During special elections in 1986, victorious Washington-backed candidates in the first round ensured at least 24 supporters in the city council. Six weeks later, when Marlene Carter and Luís Gutiérrez won run-off elections, Washington had the 25 aldermen he needed. His vote as chairman of the City Council enabled him to break 25-25 tie-votes and enact his programs.
Washington defeated former mayor Jane Byrne in the February 24, 1987 Democratic mayoral primary by 7.2%, 53.5% to 46.3%, and in the April 7, 1987 mayoral general election defeated Vrdolyak (Illinois Solidarity Party) by 11.8%, 53.8% to 42.8%, with Northwestern University business professor Donald Haider (Republican) getting 4.3%, to win reelection to a second term as mayor. Cook County Assessor Thomas Hynes (Chicago First Party), a Daley ally, dropped out of the race 36 hours before the mayoral general election. During Washington's short second term, the Eddies fell from power: Vrdolyak became a Republican, Kelly was removed from his powerful parks post, and Burke lost his Finance Committee chairmanship.
Harold Washington's Political Education Project.
From 1984 to 1987, Harold Washington's Political Education Project (PEP) served as Washington’s political arm, organizing both Washington’s campaigns and the campaigns of his political allies.
Harold Washington established the Political Education Project in 1984. This organization supported Washington’s interests in electoral politics beyond the Office of the Mayor. PEP helped organize political candidates for statewide elections in 1984 and managed Washington's participation in the 1984 Democratic National Convention as a "favorite son" presidential candidate.
PEP used its political connections to support candidates such as Luís Gutiérrez and Jesús "Chuy" García through field operations, voter registration and Election Day poll monitoring. Once elected, these aldermen helped break the stalemate between Washington and his opponents in the city council. Due to PEP’s efforts, Washington’s City Council legislation gained ground and his popularity grew as the 1987 mayoral election approached.
In preparation for the 1987 mayoral election, PEP formed the Committee to Re-Elect Mayor Washington. This organization carried out fundraising for the campaign, conducted campaign events, and coordinated volunteers. PEP staff members, such as Joseph Gardner and Helen Shiller, went on to play leading roles in Chicago politics. The organization disbanded upon Harold Washington’s death.
Harold Washington's Political Education Project Records is an archival collection detailing the organization's work. It is located in the Chicago Public Library Special Collections, Harold Washington Library Center, Chicago, Illinois.
Death.
On November 25, 1987, at 11:00 a.m., Chicago Fire Department paramedics were called to City Hall. Washington's press secretary, Alton Miller, had been discussing school board issues with the mayor when Washington suddenly slumped over on his desk, falling unconscious. After failing to revive Washington in his office, paramedics rushed him to Northwestern Memorial Hospital. Further resuscitation attempts failed, and Washington was pronounced dead at 1:36 p.m. At Daley Plaza, Richard Keen, project director for the Westside Habitat for Humanity, announced Washington's official time of death to a separate gathering of Chicagoans. Initial reactions to the pronouncement of his death were of shock and sadness, as many blacks believed that Washington was the only top Chicago official who would address their concerns. Thousands of Chicagoans attended his wake in the lobby of City Hall between November 27 and November 29, 1987. On November 30, Rev. B. Herbert Martin officiated Washington's "upbeat, hard-clapping funeral service" in Christ Universal Temple at 119th Street and Ashland Avenue in Chicago. After the service, Washington was buried in Oak Woods Cemetery on the South Side of Chicago.
Immediately after Washington's death, rumors about how Washington died began to surface. On January 6, 1988, Dr. Antonio Senat, Washington's personal physician, denied "unfounded speculations" that Washington had cocaine in his system at the time of his death, or that foul play was involved. Cook County Medical Examiner Robert J. Stein performed an autopsy on Washington and concluded that Washington had died of a heart attack. Washington had weighed 284 lb, and suffered from hypertension, high cholesterol levels, and an enlarged heart. On June 20, 1988, Alton Miller again indicated that drug reports on Washington had come back negative, and that Washington had not been poisoned prior to his death. Dr. Stein stated that the only drug in Washington's system had been lidocaine, which is used to stabilize the heart after a heart attack takes place. The drug was given to Washington either by paramedics, or by doctors at Northwestern Memorial Hospital.
School of the Art Institute of Chicago student David Nelson painted "Mirth & Girth", a caricature that depicted Washington wearing women's lingerie and holding a pencil, which was briefly displayed in a hallway at the school on May 11, 1988. The painting kicked off a First Amendment and civil rights controversy between Art Institute students and black aldermen. Nelson and the ACLU eventually split a US$95,000 (1994, US$138,000 in 2008) settlement from the city. Coincidentally, Bernard Epton, Washington's opponent in the 1983 general election, followed him in death just 18 days later, on December 13, 1987.
Legacy.
Despite the bickering in City Council, Washington seemed to relish his role as Chicago's ambassador to the world. At a party held shortly after his re-election on April 7, 1987, he said to a group of supporters, "In the old days, when you told people in other countries that you were from Chicago, they would say, 'Boom-boom! Rat-a-tat-tat!' Nowadays, they say [crowd joins with him], 'How's Harold?'!"
In later years, various city facilities and institutions were named or renamed after the late mayor to commemorate his legacy. The new building housing the main branch of the Chicago Public Library, located at 400 South State Street, was named the Harold Washington Library Center. The Chicago Public Library Special Collections, located on the building's 9th floor, house the Harold Washington Archives and Collections. These archives hold numerous collections related to Harold Washington's life and political career.
Five months after Mayor Washington's sudden death in office, a ceremony was held on April 19, 1988, changing the name of Loop College, one of the City Colleges of Chicago, to Harold Washington College. Harold Washington Elementary School in Chicago's Chatham neighborhood is also named after the former mayor. In August 2004, the 40000 sqft Harold Washington Cultural Center was opened to the public in the Bronzeville neighborhood. Across from the Hampton House apartments where Washington lived, a city park was renamed Harold Washington Park, which was known for "Harold's Parakeets", a colony of feral monk parakeets that inhabited Ash Trees in the park. A building on the campus of Chicago State University is named Harold Washington Hall.
In 2007, Washington was inducted into the Chicago Gay and Lesbian Hall of Fame as a Friend of the Community.
External links.
88558

</doc>
<doc id="40307" url="http://en.wikipedia.org/wiki?curid=40307" title="Plasma stability">
Plasma stability

An important field of plasma physics is the stability of the plasma. It usually only makes sense to analyze the stability of a plasma once it has been established that the plasma is in equilibrium. "Equilibrium" asks whether there are net forces that will accelerate any part of the plasma. If there are not, then "stability" asks whether a small perturbation will grow, oscillate, or be damped out.
In many cases a plasma can be treated as a fluid and its stability analyzed with magnetohydrodynamics (MHD). MHD theory is the simplest representation of a plasma, so MHD stability is a necessity for stable devices to be used for nuclear fusion, specifically magnetic fusion energy. There are, however, other types of instabilities, such as velocity-space instabilities in magnetic mirrors and systems with beams. There are also rare cases of systems, e.g. the Field-Reversed Configuration, predicted by MHD to be unstable, but which are observed to be stable, probably due to kinetic effects.
Plasma instabilities.
Plasma instabilities can be divided into two general groups:
Plasma instabilities are also categorised into different modes:
Source: Andre Gsponer, "Physics of high-intensity high-energy particle beam propagation in open air and outer-space plasmas" (2004)
MHD Instabilities.
Beta is a ratio of the plasma pressure over the magnetic field strength. 
formula_1 
MHD stability at high beta is crucial for a compact, cost-effective magnetic fusion reactor. Fusion power density varies roughly as formula_2 at constant magnetic field, or as formula_3 at constant bootstrap fraction in configurations with externally driven plasma current. (Here formula_4 is the normalized beta.) In many cases MHD stability represents the primary limitation on beta and thus on fusion power density. MHD stability is also closely tied to issues of creation and sustainment of certain magnetic configurations, energy confinement, and steady-state operation. Critical issues include understanding and extending the stability limits through the use of a
variety of plasma configurations, and developing active means for reliable operation near those limits. Accurate predictive capabilities are needed, which will require the addition of new physics to existing MHD models. Although a wide range of magnetic configurations exist, the underlying MHD physics is common to all. Understanding of MHD stability gained in one configuration can benefit others, by verifying analytic theories, providing benchmarks for predictive MHD stability codes, and advancing the development of active control techniques.
The most fundamental and critical stability issue for magnetic fusion is simply that MHD instabilities often limit performance at high beta. In most cases the important instabilities are long wavelength, global modes, because of their ability to cause severe degradation of energy confinement or termination of the plasma. Some important examples that are common to many magnetic configurations are ideal kink modes, resistive wall modes, and neoclassical tearing modes. A possible consequence of violating stability boundaries is a disruption, a sudden loss of thermal energy often followed by termination of the discharge. The key issue thus includes understanding the nature of the beta limit in the various configurations, including the associated thermal and magnetic stresses, and finding ways to avoid the limits or mitigate the consequences. A wide range of approaches to preventing such instabilities is under investigation, including optimization of the configuration of the plasma and its confinement device, control of the internal structure of the plasma, and active control of the MHD instabilities.
Ideal Instabilities.
Ideal MHD instabilities driven by current or pressure gradients represent
the ultimate operational limit for most configurations. The long-wavelength kink mode and short-wavelength
ballooning mode limits are generally well understood and can in principle be avoided.
Intermediate-wavelength modes (n ~ 5–10 modes encountered in tokamak edge plasmas, for
example) are less well understood due to the computationally intensive nature of the stability
calculations. The extensive beta limit database for tokamaks is consistent with ideal MHD stability limits, yielding agreement to within about 10% in beta for cases where the internal profiles of the
plasma are accurately measured. This good agreement provides confidence in ideal stability
calculations for other configurations and in the design of prototype fusion reactors.
Resistive Wall Modes.
Resistive wall modes (RWM) develop in plasmas that require the presence of a perfectly conducting wall for stability. RWM stability is a key issue for many magnetic configurations. Moderate beta values are possible without a nearby wall in the tokamak, stellarator, and other configurations, but a nearby conducting wall can significantly improve ideal kink mode stability in most configurations, including the tokamak, ST, reversed field pinch (RFP), spheromak, and possibly the FRC. In the advanced tokamak and ST, wall stabilization is critical for operation with a large bootstrap fraction. The spheromak requires wall stabilization to avoid the low-m,n tilt and shift modes, and possibly bending modes. However, in the presence of a non-ideal wall, the slowly growing RWM is unstable. The resistive wall mode has been a long-standing issue for the RFP, and has more recently been observed in tokamak experiments. Progress in understanding the physics of the RWM and developing the means to stabilize it could be directly applicable to all magnetic configurations. A closely related issue is to understand plasma rotation, its sources and sinks, and its role in stabilizing the RWM.
Resistive instabilities.
Resistive instabilities are an issue for all magnetic configurations, since the onset can occur at beta values well below the ideal limit. The stability of neoclassical tearing modes (NTM) is a key issue for magnetic configurations with a strong bootstrap current. The NTM is a metastable mode; in certain plasma configurations, a sufficiently large deformation of the bootstrap current produced by a “seed island” can contribute to the growth of the island. The NTM is already an important performance-limiting factor in many tokamak experiments, leading to degraded confinement or disruption. Although the basic mechanism is well established, the capability to predict the onset in present and future devices requires better understanding of the damping mechanisms which determine the threshold island size, and of the mode coupling by which other instabilities (such as sawteeth in tokamaks) can generate seed islands. Resistive Ballooning Mode, similar to ideal ballooning, but with finite resistivity taken into consideration, provides another example of a resistive instability.
Opportunities for Improving MHD Stability.
Configuration.
The configuration of the plasma and its confinement device represent an
opportunity to improve MHD stability in a robust way. The benefits of discharge shaping and low
aspect ratio for ideal MHD stability have been clearly demonstrated in tokamaks and STs, and will
continue to be investigated in experiments such as DIII-D, Alcator C-Mod, NSTX, and MAST. New
stellarator experiments such as NCSX (proposed) will test the prediction that addition of
appropriately designed helical coils can stabilize ideal kink modes at high beta, and lower-beta tests
of ballooning stability are possible in HSX. The new ST experiments provide an opportunity to
test predictions that a low aspect ratio yields improved stability to tearing modes, including
neoclassical, through a large stabilizing “Glasser effect” term associated with a large Pfirsch-Schlüter
current. Neoclassical tearing modes can be avoided by minimizing the bootstrap current in
quasi-helical and quasi-omnigenous stellarator configurations. Neoclassical tearing modes are also
stabilized with the appropriate relative signs of the bootstrap current and the magnetic shear; this
prediction is supported by the absence of NTMs in central negative shear regions of tokamaks.
Stellarator configurations such as the proposed NCSX, a quasi-axisymmetric stellarator design,
can be created with negative magnetic shear and positive bootstrap current to achieve stability to the
NTM. Kink mode stabilization by a resistive wall has been demonstrated in RFPs and tokamaks,
and will be investigated in other configurations including STs (NSTX) and spheromaks (SSPX).
A new proposal to stabilize resistive wall modes by a flowing liquid lithium wall needs further
evaluation.
Internal Structure.
Control of the internal structure of the plasma allows more active
avoidance of MHD instabilities. Maintaining the proper current density profile, for example, can
help to maintain stability to tearing modes. Open-loop optimization of the pressure and current
density profiles with external heating and current drive sources is routinely used in many devices.
Improved diagnostic measurements along with localized heating and current drive sources, now
becoming available, will allow active feedback control of the internal profiles in the near future.
Such work is beginning or planned in most of the large tokamaks (JET, JT–60U, DIII–D,
C–Mod, and ASDEX–U) using RF heating and current drive. Real-time analysis of profile data
such as MSE current profile measurements and real-time identification of stability boundaries are
essential components of profile control. Strong plasma rotation can stabilize resistive wall modes,
as demonstrated in tokamak experiments, and rotational shear is also predicted to stabilize resistive
modes. Opportunities to test these predictions are provided by configurations such as the ST,
spheromak, and FRC, which have a large natural diamagnetic rotation, as well as tokamaks with
rotation driven by neutral beam injection. The Electric Tokamak experiment is intended to have a
very large driven rotation, approaching Alfvénic regimes where ideal stability may also be
influenced. Maintaining sufficient plasma rotation, and the possible role of the RWM in damping
the rotation, are important issues that can be investigated in these experiments.
Feedback Control.
Active feedback control of MHD instabilities should allow operation
beyond the “passive” stability limits. Localized rf current drive at the rational surface is predicted
to reduce or eliminate neoclassical tearing mode islands. Experiments have begun in ASDEX–U
and COMPASS-D with promising results, and are planned for next year in DIII–D. Routine use
of such a technique in generalized plasma conditions will require real-time identification of the
unstable mode and its radial location. If the plasma rotation needed to stabilize the resistive wall
mode cannot be maintained, feedback stabilization with external coils will be required. Feedback
experiments have begun in DIII–D and HBT-EP, and feedback control should be explored for the
RFP and other configurations. Physics understanding of these active control techniques will be
directly applicable between configurations.
Disruption Mitigation.
The techniques discussed above for improving MHD stability are the
principal means of avoiding disruptions. However, in the event that these techniques do not
prevent an instability, the effects of a disruption can be mitigated by various techniques.
Experiments in
JT–60U have demonstrated reduction of electromagnetic stresses through operation at a neutral
point for vertical stability. Pre-emptive removal of the plasma energy by injection of a large gas
puff or an impurity pellet has been demonstrated in tokamak experiments, and ongoing
experiments in C–Mod, JT–60U, ASDEX–U, and DIII–D will improve the understanding and
predictive capability. Cryogenic liquid jets of helium are another proposed technique, which may
be required for larger devices. Mitigation techniques developed for tokamaks will be directly
applicable to other configurations.

</doc>
<doc id="40310" url="http://en.wikipedia.org/wiki?curid=40310" title="Magnetohydrodynamics">
Magnetohydrodynamics

Magnetohydrodynamics (MHD) ("magneto fluid dynamics" or "hydromagnetics") is the study of the magnetic properties of electrically conducting fluids. Examples of such fluids include plasmas, liquid metals, and salt water or electrolytes. The word "magnetohydrodynamics (MHD)" is derived from "magneto-" meaning magnetic field, "hydro-" meaning liquid, and "-dynamics" meaning movement. The field of MHD was initiated by Hannes Alfvén, for which he received the Nobel Prize in Physics in 1970.
The fundamental concept behind MHD is that magnetic fields can induce currents in a moving conductive fluid, which in turn polarizes the fluid and reciprocally changes the magnetic field itself. The set of equations that describe MHD are a combination of the Navier-Stokes equations of fluid dynamics and Maxwell's equations of electromagnetism. These differential equations must be solved simultaneously, either analytically or numerically.
History.
The first recorded use of the word "magnetohydrodynamics" is by Hannes Alfvén in 1942:
The ebbing salty water flowing past London's Waterloo Bridge interacts with the Earth's magnetic field to produce a potential difference between the two river-banks. Michael Faraday tried this experiment in 1832 but the current was too small to measure with the equipment at the time, and the river bed contributed to short-circuit the signal. However, by a similar process the voltage induced by the tide in the English Channel was measured in 1851.
Ideal and resistive MHD.
The simplest form of MHD, Ideal MHD, assumes that the fluid has so little resistivity that it can be treated as a perfect conductor. This is the limit of infinite magnetic Reynolds number. In ideal MHD, Lenz's law dictates that the fluid is in a sense "tied" to the magnetic field lines. To explain, in ideal MHD a small rope-like volume of fluid surrounding a field line will continue to lie along a magnetic field line,
even as it is twisted and distorted by fluid flows in the system. This is sometimes referred to as the magnetic field lines being "frozen" in the fluid.
The connection between magnetic field lines and fluid in ideal MHD fixes the topology of the magnetic field in the fluid—for example, if a set of magnetic field lines are tied into a knot, then they will remain so as long as the fluid/plasma has negligible resistivity. This difficulty in reconnecting magnetic field lines makes it possible to store energy by moving the fluid or the source of the magnetic field. The energy can then become available if the conditions for ideal MHD break down, allowing magnetic reconnection that releases the stored energy from the magnetic field.
Ideal MHD equations.
The ideal MHD equations consist of the continuity equation, the Cauchy momentum equation, Ampere's Law neglecting displacement current, and a temperature evolution equation. As with any fluid description to a kinetic system, a closure approximation must be applied to highest moment of the particle distribution equation. This is often accomplished with approximations to the heat flux through a condition of adiabaticity or isothermality.
In the following, formula_1 is the magnetic field, formula_2 is the electric field, formula_3 is the bulk plasma velocity, formula_4 is the current density, formula_5 is the mass density, formula_6 is the plasma pressure, and formula_7 is time. The continuity equation is
The Cauchy momentum equation is
The Lorentz force term formula_10 can be expanded using Ampere's law and the identity formula_11 to give
where the first term on the right hand side is the magnetic tension force and the second term is the magnetic pressure force.
The ideal Ohm's law for a plasma is given by
Faraday's law is
The low-frequency Ampere's law neglects displacement current and is given by
The magnetic divergence constraint is
The energy equation is given by
where formula_18 is the ratio of specific heats for an adiabatic equation of state. This energy equation is, of course, only applicable in the absence of shocks or heat conduction as it assumes that the entropy of a fluid element does not change.
Applicability of ideal MHD to plasmas.
Ideal MHD is only strictly applicable when:
Importance of resistivity.
In an imperfectly conducting fluid the magnetic field can generally move through the fluid following a diffusion law with the resistivity of the plasma serving as a diffusion constant. This means that solutions to the ideal MHD equations are only applicable for a limited time for a region of a given size before diffusion becomes too important to ignore. One can estimate the diffusion time across a solar active region (from collisional resistivity) to be hundreds to thousands of years, much longer than the actual lifetime of a sunspot—so it would seem reasonable to ignore the resistivity. By contrast, a meter-sized volume of seawater has a magnetic diffusion time measured in milliseconds.
Even in physical systems which are large and conductive enough that simple estimates of the Lundquist number suggest that we can ignore the resistivity, resistivity may still be important: many instabilities exist that can increase the effective resistivity of the plasma by factors of more than a billion. The enhanced resistivity is usually the result of the formation of small scale structure like current sheets or fine scale magnetic turbulence, introducing small spatial scales into the system over which ideal MHD is broken and magnetic diffusion can occur quickly. When this happens, magnetic reconnection may occur in the plasma to release stored magnetic energy as waves, bulk mechanical acceleration of material, particle acceleration, and heat.
Magnetic reconnection in highly conductive systems is important because it concentrates energy in time and space, so that gentle forces applied to a plasma for long periods of time can cause violent explosions and bursts of radiation.
When the fluid cannot be considered as completely conductive, but the other conditions for ideal MHD are satisfied, it is possible to use an extended model called resistive MHD. This includes an extra term in Ohm's Law which models the collisional resistivity. Generally MHD computer simulations are at least somewhat resistive because their computational grid introduces a numerical resistivity.
Importance of kinetic effects.
Another limitation of MHD (and fluid theories in general) is that they depend on the assumption that the plasma is strongly collisional (this is the first criterion listed above), so that the time scale of collisions is shorter than the other characteristic times in the system, and the particle distributions are Maxwellian. This is usually not the case in fusion, space and astrophysical plasmas. When this is not the case, or we are interested in smaller spatial scales, it may be necessary to use a kinetic model which properly accounts for the non-Maxwellian shape of the distribution function. However, because MHD is relatively simple and captures many of the important properties of plasma dynamics it is often qualitatively accurate and is almost invariably the first model tried.
Effects which are essentially kinetic and not captured by fluid models include double layers, Landau damping, a wide range of instabilities, chemical separation in space plasmas and electron runaway.
Structures in MHD systems.
In many MHD systems most of the electric current is compressed into thin nearly-two-dimensional ribbons termed current sheets. These can divide the fluid into magnetic domains, inside of which the currents are relatively weak. Current sheets in
the solar corona are thought to be between a few meters and a few kilometers in thickness, which is quite thin compared to the magnetic domains (which are thousands to hundreds of thousands of kilometers across). Another example is in the Earth's magnetosphere, where current sheets separate topologically distinct domains, isolating most of the Earth's ionosphere from the solar wind.
MHD waves.
The wave modes derived using MHD plasma theory are called magnetohydrodynamic waves or MHD waves. In general there are three MHD wave modes:
All these waves have constant phase velocities for all frequencies, and hence there is no dispersion. At the limits when the angle between the wave propagation vector k and magnetic field B is either 0 (180) or 90 degrees, the wave modes are called:
The MHD oscillations will be damped if the fluid is not perfectly conducting but has a finite conductivity, or if viscous effects are present.
MHD waves and oscillations are a popular tool for the remote diagnostics of laboratory and astrophysical plasmas, e.g. the corona of the Sun (Coronal seismology).
Extensions to magnetohydrodynamics.
Resistive MHD.
Resistive MHD describes magnetized fluids with finite electron diffusivity (formula_19). This diffusivity leads to a breaking in the magnetic topology; magnetic field lines can 'reconnect' when they collide. Usually this term is small and reconnections can be handled by thinking of them as not dissimilar to shocks; this process has been shown to be important in the Earth-Solar magnetic interactions.
Extended MHD.
Extended MHD describes a class of phenomena in plasmas that are higher order than resistive MHD, but which can adequately be treated with a single fluid description. These include the effects of Hall physics, electron pressure gradients, finite Larmor Radii in the particle gyromotion, and electron inertia.
Two-Fluid MHD.
Two-Fluid MHD describes plasmas that include a non-negligible Hall electric field. As a result, the electron and ion momenta must be treated separately. This description is more closely tied to Maxwell's equations as an evolution equation for the electric field exists.
Hall MHD.
In 1960, M. J. Lighthill criticized the applicability of ideal or resistive MHD theory for plasmas. It concerned the neglect of the "Hall current term", a frequent simplification made in magnetic fusion theory. Hall-magnetohydrodynamics (HMHD) takes into account this electric field description of magnetohydrodynamics. The most important difference is that in the absence of field line breaking, the magnetic field is tied to the electrons and not to the bulk fluid.
Collisionless MHD.
MHD is also often used for collisionless plasmas. In that case the MHD equations are derived from the Vlasov equation.
Reduced MHD.
By using a multiscale analysis the (resistive) MHD equations can be reduced to a set of four closed scalar equations. This allows e.g. for more efficient numerical calculations.
Applications.
Geophysics.
Beneath the Earth's mantle lies the core, which is made up of two parts: the solid inner core and liquid outer core. Both have significant quantities of iron. The liquid outer core moves in the presence of the magnetic field and eddies are set up into the same due to the Coriolis effect. These eddies develop a magnetic field which boosts Earth's original magnetic field—a process which is self-sustaining and is called the geomagnetic dynamo.
Based on the MHD equations, Glatzmaier and Paul Roberts have made a supercomputer model of the Earth's interior. After running the simulations for thousands of years in virtual time, the changes in Earth's magnetic field can be studied. The simulation results are in good agreement with the observations as the simulations have correctly predicted that the Earth's magnetic field flips every few hundred thousands of years. During the flips, the magnetic field does not vanish altogether—it just gets more complicated.
Earthquakes.
Some monitoring stations have reported that earthquakes are sometimes preceded by a spike in ULF activity. A remarkable example of this occurred before the 1989 Loma Prieta earthquake in California, although a subsequent study indicates that this was little more than a sensor malfunction. On December 9, 2010, geoscientists announced that the DEMETER satellite observed a dramatic increase in ULF radio waves over Haiti in the month before the magnitude 7.0 Mw 2010 earthquake. Researchers are attempting to learn more about this correlation to find out whether this method can be used as part of an early warning system for earthquakes.
Astrophysics and cosmology.
MHD applies quite well to astrophysics and cosmology since over 99% of baryonic matter content of the Universe is made up of plasma, including stars, the interplanetary medium (space between the planets), the interstellar medium (space between the stars), the intergalactic medium, nebulae and jets. Many astrophysical systems are not in local thermal equilibrium, and therefore require an additional kinematic treatment to describe all the phenomena within the system (see Astrophysical plasma).
Sunspots are caused by the Sun's magnetic fields, as Joseph Larmor theorized in 1919. The solar wind is also governed by MHD. The differential solar rotation may be the long-term effect of magnetic drag at the poles of the Sun, an MHD phenomenon due to the Parker spiral shape assumed by the extended magnetic field of the Sun.
Previously, theories describing the formation of the Sun and planets could not explain how the Sun has 99.87% of the mass, yet only 0.54% of the angular momentum in the solar system. In a closed system such as the cloud of gas and dust from which the Sun was formed, mass and angular momentum are both conserved. That conservation would imply that as the mass concentrated in the center of the cloud to form the Sun, it would spin up, much like a skater pulling their arms in. The high speed of rotation predicted by early theories would have flung the proto-Sun apart before it could have formed. However, magnetohydrodynamic effects transfer the Sun's angular momentum into the outer solar system, slowing its rotation.
Breakdown of ideal MHD (in the form of magnetic reconnection) is known to be the cause of solar flares, the largest explosions in the solar system. The magnetic field in a solar active region over a sunspot can become quite stressed over time, storing energy that is released suddenly as a burst of motion, X-rays, and radiation when the main current sheet collapses, reconnecting the field.
A paper by Kohli and Haslam includes a detailed summary of the work done in studying primordial magnetic fields in the cosmological context. A part of this summary is displayed below.
Magnetic fields have been thought to play a major role in the early universe, as it is well known that after inflation, the early universe was a good conductor, even though the number density of free electrons dropped dramatically during recombination, its residual value was enough to maintain high conductivity in baryonic matter. As a result, cosmic magnetic fields have remained frozen into the expanding baryonic fluid during most of their evolution. One can then analyze the magnetic effects on the dynamics of the early universe through the ideal magnetohydrodynamics (hereafter, referred to as MHD), in which the magnetic field source is considered to be a perfect conductor, such that the energy momentum tensor is that for an ordinary magnetic field.
Hughston and Jacobs showed that in the case of a pure magnetic field, only Bianchi Types I, II, VI(formula_20) (which is the same as Type III), and VII (formula_21) admit field components, whereas Types IV, V, VI (formula_22), VII (formula_23), VIII, and IX admit no field components. These results led to a number of papers of Bianchi models with a perfect-fluid magnetic field source.
With respect to the dynamical systems approach applied to the latter, LeBlanc studied Bianchi Type II magnetic cosmologies in which he provided an analysis on the future and past asymptotic states of the resulting dynamical system LeBlanc also studied the asymptotic states of magnetic perfect-fluid Bianchi Type I cosmologies. In this paper, a new solution to the Einstein field equations was discovered. Using phase plane analysis techniques, Collins studied the behaviour of a class of perfect-fluid anisotropic cosmological models, and established a correspondence between magnetic models of Bianchi Type I and perfect fluid models of Bianchi Type II. In addition, LeBlanc, Kerr, and Wainwright studied the asymptotic states of magnetic Bianchi Type VI cosmologies. In their work, they showed that there is a finite probability that an arbitrarily selected model will be close to isotropy during some time interval in its evolution. We should note that Barrow, Maartens, and Tsagas did significant work in the reformulation of a formula_24 covariant description of the magnetohydrodynamic equations that has provided further understanding and clarity on the role of large-vale electromagnetic fields in the perturbed Friedmann-Lemaitre-Robertson-Walker models.
Viscous MHD Bianchi models, which are of special interest with regards to early-universe cosmologies have been presented in the literature on a number of occasions. van Leeuwen and Salvati studied the dynamics of general Bianchi class A models containing a magneto-viscous fluid and a large-scale magnetic field. Banerjee and Sanyal presented some exact solutions of Bianchi Types I and III cosmological models consisting of a viscous fluid and axial magnetic field.
Benton and Tupper studied Bianchi Type I models with a "powers-of-t" metric under the influence of a viscous fluid with a magnetic field. Salvati, Schelling, and van Leeuwen numerically analyzed the evolution of the Bianchi type I universe with a viscous fluid and large-scale magnetic field. Ribeiro and Sanyal studied a Bianchi Type $VI_{0}$ viscous fluid cosmology with an axial magnetic field in which they obtained exact solutions to the Einstein field equations assuming linear relations among the square root of matter density and the shear and expansion scalars. The work of Kohli and Haslam also gives a detailed analysis based on dynamical systems theory, the evolution of a Bianchi type I model in the presence of a viscous fluid, in which they also discovered a new solution to the Einstein field equations.
Sensors.
Magnetohydrodynamic sensors are used for precision measurements of angular velocities in inertial navigation systems such as in aerospace engineering. Accuracy improves with the size of the sensor. The sensor is capable of surviving in harsh environments.
Engineering.
MHD is related to engineering problems such as plasma confinement, liquid-metal cooling of nuclear reactors, and electromagnetic casting (among others).
A magnetohydrodynamic drive or MHD propulsor is a method for propelling seagoing vessels using only electric and magnetic fields with no moving parts, using magnetohydrodynamics. The working principle involves electrification of the propellant (gas or water) which can then be directed by a magnetic field, pushing the vehicle in the opposite direction. Although some working prototypes exist, MHD drives remain impractical.
The first prototype of this kind of propulsion was built and tested in 1965 by Steward Way, a professor of mechanical engineering at the University of California, Santa Barbara. Way, on leave from his job at Westinghouse Electric, assigned his senior-year undergraduate students to develop a submarine with this new propulsion system. In the early 1990s, Mitsubishi built a boat, the 'Yamato,' which used a magnetohydrodynamic drive incorporating a superconductor cooled by liquid helium, and could travel at 15 km/h.
MHD power generation fueled by potassium-seeded coal combustion gas showed potential for more efficient energy conversion (the absence of solid moving parts allows operation at higher temperatures), but failed due to cost-prohibitive technical difficulties.
One major engineering problem was the failure of the wall of the primary-coal combustion chamber due to abrasion.
In microfluidics, MHD is studied as a fluid pump for producing a continuous, nonpulsating flow in a complex microchannel design.
Magnetic drug targeting.
An important task in cancer research is developing more precise methods for delivery of medicine to affected areas. One method involves the binding of medicine to biologically compatible magnetic particles (e.g. ferrofluids), which are guided to the target via careful placement of permanent magnets on the external body. Magnetohydrodynamic equations and finite element analysis are used to study the interaction between the magnetic fluid particles in the bloodstream and the external magnetic field.

</doc>
<doc id="40311" url="http://en.wikipedia.org/wiki?curid=40311" title="Great Chicago Fire">
Great Chicago Fire

The Great Chicago Fire was a conflagration that burned from Sunday, October 8, to early Tuesday, October 10, 1871. The fire killed up to 300 people, destroyed roughly 3.3 sqmi of Chicago, Illinois, and left more than 100,000 residents homeless. Though the fire was one of the largest U.S. disasters of the 19th century, and destroyed much of the city's central business district, Chicago was rebuilt and continued to grow as one of the most populous and economically important American cities.
The very night the fire broke out, an even deadlier fire annihilated Peshtigo, Wisconsin and other villages and towns north of the city Green Bay.
Origin.
The fire started at about 9:00 P.M, October 8, in or around a small barn that bordered the alley behind 137 DeKoven Street. The traditional account of the origin of the fire is that it was started by a cow kicking over a lantern in the barn owned by Patrick and Catherine O'Leary. In 1893, Michael Ahern, the "Chicago Republican" reporter who wrote the O'Leary account, admitted he had made it up as colorful copy. The shed next to the O'Learys' was the first building to be consumed by the fire, but the official report could not find the exact cause. There has, however, been some speculation that would suggest that the fire was caused by a person, instead of a cow. Some testimonies stated that a group of men were gambling inside the barn so they would not be seen by others. The lamp that they were using was accidentally knocked over and started the fire. Little evidence has been presented to prove whether or not this is true. There has been speculation as to whether the cause of the fire was related to other fires that began the same day. See Questions about the fire.
The fire's spread was aided by the city's use of wood as the predominant building material in a style called balloon frame, a drought before the fire, and strong southwest winds that carried flying embers toward the heart of the city. More than two thirds of the structures in Chicago at the time of the fire were made entirely of wood. Most houses and buildings were topped with highly flammable tar or shingle roofs. All the city's sidewalks and many roads were made of wood. Compounding this problem, Chicago had only received an inch of rain from July 4 to October 9 causing severe drought conditions.
In 1871, the Chicago Fire Department had 185 firefighters with just 17 horse-drawn steam engines to protect the entire city. The initial response by the fire department was quick, but due to an error by the watchman, Matthias Schaffer, the firefighters were sent to the wrong place, allowing the fire to grow unchecked. An alarm sent from the area near the fire also failed to register at the courthouse where the fire watchmen were. Also, the firefighters were tired from having fought numerous small fires and one large fire in the week before. These factors combined to turn a small barn fire into a conflagration.
Spread of the blaze.
When firefighters finally arrived at DeKoven Street, the fire had grown and spread to neighboring buildings and was progressing towards the central business district. Firefighters had hoped that the South Branch of the Chicago River and an area that had previously thoroughly burned would act as a natural firebreak. All along the river, however, were lumber yards, warehouses, and coal yards, and barges and numerous bridges across the river. As the fire grew, the southwest wind intensified and became superheated, causing structures to catch fire from the heat and from burning debris blown by the wind. Around 11:30 PM, flaming debris blew across the river and landed on roofs and the South Side Gas Works.
With the fire across the river and moving rapidly towards the heart of the city, panic set in. About this time, Mayor Roswell B. Mason sent message to nearby towns asking for help. When the courthouse caught fire, he ordered the building to be evacuated and the prisoners jailed in the basement to be released. At 2:20 AM on the 9th, the cupola of the courthouse collapsed sending the great bell crashing down. Some witnesses reported hearing the sound from a mile away.
As more buildings succumbed to the flames, a major contributing factor to the fire’s spread was a meteorological phenomenon known as a fire whirl. As overheated air rises, it comes into contact with cooler air and begins to spin creating a tornado-like effect. These fire whirls are the likely causes of driving flaming debris so high and so far. Such debris was blown across the main branch of the Chicago River to a railroad car carrying kerosene. The fire had jumped the river a second time and was now raging across the city’s north side.
Despite the fire spreading and growing rapidly, the city's firefighters continued to battle the blaze. A short time after the fire jumped the river, a burning piece of timber lodged on the roof of the city’s waterworks. Within minutes, the interior of the building was engulfed in flames and the building was destroyed. With it, the city’s water mains went dry and the city was helpless. The fire burned unchecked from building to building, block to block.
Finally, late into the evening of the 9th, it started to rain, but the fire had already started to burn itself out. The fire had spread to the sparsely populated areas of the north side, having consumed the densely populated areas thoroughly.
Aftermath.
Once the fire had ended, the smoldering remains were still too hot for a survey of the damage to be completed for many days. Eventually the city determined that the fire destroyed an area about four miles (6 km) long and averaging 3/4 mile (1 km) wide, encompassing more than 2000 acre. Destroyed were more than 73 mi of roads, 120 mi of sidewalk, 2,000 lampposts, 17,500 buildings, and $222 million in property—about a third of the city's valuation (more than $4 billion in 2015 dollars). Of the 300,000 inhabitants, 100,000 were left homeless. 120 bodies were recovered, but the death toll may have been as high as 300. The county coroner speculated that an accurate count was impossible as some victims may have drowned or had been incinerated leaving no remains.
In the days and weeks following the fire, monetary donations flowed into Chicago from around the country and foreign cities, along with donations of food, clothing, and other goods. These donations came from individuals, corporations, and cities. New York City gave $450,000 along with clothing and provisions, St. Louis gave $300,000, and the Common Council of London gave 1,000 Guineas as well as ₤7,000 from private donations. Cincinnati, Cleveland, and Buffalo, all commercial rivals, donated hundreds and thousands of dollars. Milwaukee, along with other nearby cities, helped by sending fire-fighting equipment. Additionally, food, clothing and books were brought by train from all over the continent. Mayor Mason placed the Chicago Relief and Aid Society in charge of the city’s relief efforts.
Operating from the First Congregational Church, city officials and the Aldermen began taking steps to preserve order in the city. Price fixing was a key concern. In one ordinance, the city set the price of bread at 8¢ for a 12-ounce loaf. Public buildings were opened as places of refuge, and saloons closed at 9 in the evening for the week following the fire.
The fire also led to questions about the developments in the United States. Due to Chicago’s rapid expansion at this time, the fire led to Americans reflecting on industrialization. The religious point of view said that Americans should return to a more old-fashioned way of life, and that the fire was caused by people ignoring morality. Many Americans on the other hand believed that a lesson that should be learned from the fire was that cities needed to improve their building techniques. Frederick Law Olmsted attributed this to Chicago’s style of building:
"Chicago had a weakness for “big things,” and liked to think that it was outbuilding New York. It did a great deal of commercial advertising in its house-tops. The faults of construction as well as of art in its great showy buildings must have been numerous. Their walls were thin, and were overweighted with gross and coarse misornamentation."
Olmsted also believes that with brick walls and disciplined firemen and police, the damage caused and deaths would have been much less.
Almost immediately, the city began to rewrite its fire standards, spurred by the efforts of leading insurance executives and fire prevention reformers such as Arthur C. Ducat and others. Chicago soon developed one of the country's leading fire fighting forces.
Land speculators, such as Gurdon Saltonstall Hubbard, and business owners quickly set about rebuilding the city. The first load of lumber for rebuilding was delivered the day the last burning building was extinguished. By the World's Columbian Exposition 22 years later, Chicago hosted more than 21 million visitors. The Palmer House hotel burned to the ground in the fire 13 days after its grand opening. Its developer Potter Palmer secured a loan and rebuilt the hotel to higher standards across the street from the original, proclaiming it to be "The World's First Fireproof Building".
In 1956, the remaining structures on the original O'Leary property at 558 W. DeKoven Street were torn down for construction of the Chicago Fire Academy, a training facility for Chicago firefighters. A bronze sculpture of stylized flames, entitled "Pillar of Fire" by sculptor Egon Weiner, was erected on the point of origin in 1961.
Questions about the fire.
Catherine O'Leary seemed the perfect scapegoat: she was a poor, Irish Catholic immigrant. During the latter half of the 19th century, anti-Irish sentiment was strong throughout the United States and in Chicago. This was intensified as a result of the growing political power of the city's Irish population. This story was circulating in Chicago even before the flames had died out, and it was noted in the "Chicago Tribune"'s first post-fire issue. In 1893 the reporter Michael Ahern retracted the "cow-and-lantern" story, admitting it was fabricated.
The cow and fire story is the story which puts the blame on Catherine O’Leary; it is explained by Richard F. Bales. A fire broke out in the barn of Patrick and Catherine O’Leary and began to spread through Chicago. As the fire was still burning the fingers began to be pointed at Mrs. O’Leary and her cow. The story states that the fire began as Mrs. O’Leary was milking a cow and the cow kicked over the lamp which began the fire by setting the straw on fire which set the barn on fire. This was denied by the O’Leary household stating that they were already in bed before the fire started, but stories of the cow began to spread across the city. O’Leary was later exonerated.
The amateur historian Richard Bales has also suggested the fire started when Daniel "Pegleg" Sullivan, who first reported the fire, ignited hay in the barn while trying to steal milk. Bales' account does not have consensus. The Chicago Public Library staff criticized his account in their web page on the fire.
Anthony DeBartolo reported evidence in the "Chicago Tribune" suggesting that Louis M. Cohn may have started the fire during a craps game. According to Cohn, on the night of the fire, he was gambling in the O'Learys' barn with one of their sons and some other neighborhood boys. When Mrs. O'Leary came out to the barn to chase the kids away at around 9:00, they knocked over a lantern in their flight, although Cohn states that he paused long enough to scoop up the money. Following his death in 1942, Cohn bequeathed $35,000 which was assigned by his executors to the Medill School of Journalism at Northwestern University. The bequest was given to the school on September 28, 1944, along with his confession.
An alternative theory, first suggested in 1882 by Ignatius L. Donnelly in "", is that the Great Chicago Fire was caused by a meteor shower. At a 2004 conference of the Aerospace Corporation and the American Institute of Aeronautics and Astronautics, engineer and physicist Robert Wood suggested that the fire began when Biela's Comet broke up over the Midwest. That four large fires took place, all on the same day, all on the shores of Lake Michigan (see Related Events), suggests a common root cause. Eyewitnesses reported sighting spontaneous ignitions, lack of smoke, "balls of fire" falling from the sky, and blue flames. According to Wood, these accounts suggest that the fires were caused by the methane that is commonly found in comets.
But as meteorites are not known to start or spread fires and are cool to the touch after reaching the ground, this theory has not found favor in the scientific community. A common cause for the fires in the Midwest can be found in the fact that the area had suffered through a tinder-dry summer, so that winds from the front that moved in that evening were capable of generating rapidly expanding blazes from available ignition sources, which were plentiful in the region. Methane-air mixtures become flammable only when the methane concentration exceeds 5%, at which point the mixtures also become explosive. Methane gas is lighter than air and thus does not accumulate near the ground; any localized pockets of methane in the open air would rapidly dissipate. Moreover, if a fragment of an icy comet were to strike the Earth, the most likely outcome, due to the low tensile strength of such bodies, would be for it to disintegrate in the upper atmosphere, leading to an air burst explosion analogous to that of the Tunguska event.
Surviving structures.
The following structures are the only structures from the burnt district still standing:
St. Michael's Church and the Pumping Station were both gutted in the fire, but their exteriors survived, and the buildings were rebuilt using the surviving walls. Additionally, though the inhabitable portions of the building were destroyed, the bell tower of St. James Cathedral survived the fire and was incorporated into the rebuilt church. The stones near the top of the tower are still blackened from the soot and smoke. A couple of wooden cottages on North Cleveland Avenue also survived the blaze.
Related events.
On that hot, dry, and windy autumn day, three other major fires occurred along the shores of Lake Michigan at the same time as the Great Chicago Fire. Some 250 mi to the north, the Peshtigo Fire consumed the town of Peshtigo, Wisconsin, along with a dozen other villages. It killed 1,200 to 2,500 people and charred approximately 1.5 million acres (6,000 km²). The Peshtigo Fire remains the deadliest in American history but the remoteness of the region meant it was little noticed at the time.
Across the lake to the east, the town of Holland, Michigan, and other nearby areas burned to the ground. Some 100 mi to the north of Holland, the lumbering community of Manistee also went up in flames in what became known as The Great Michigan Fire.
Farther east, along the shore of Lake Huron, the Port Huron Fire swept through Port Huron, Michigan and much of Michigan's "Thumb". On October 9, 1871, a fire swept through the city of Urbana, Illinois, 140 mi south of Chicago, destroying portions of its downtown area. Windsor, Ontario, likewise burned on October 12.
The city of Singapore, Michigan, provided a large portion of the lumber to rebuild Chicago. As a result, the area was so heavily deforested that the land deteriorated into barren sand dunes and the town had to be abandoned.
Panorama of Chicago after the 1871 Fire.
Image attributed to Nick Bernhard.

</doc>
<doc id="40313" url="http://en.wikipedia.org/wiki?curid=40313" title="Universal grammar">
Universal grammar

Universal grammar (UG) is a theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. It is sometimes known as 'mental grammar', and as opposed to other 'grammars', e.g. prescriptive, descriptive and pedagogical. The theory suggests that linguistic ability manifests itself without being taught (see the poverty of the stimulus argument), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.
Argument.
The theory of Universal Grammar proposes that if human beings are brought up under normal conditions (not conditions of extreme sensory deprivation), then they will always develop language with a certain property X (e.g., distinguishing nouns from verbs, or distinguishing function words from lexical words). As a result, property X is considered to be a property of universal grammar in the most general sense (here not capitalized).
There are theoretical senses of the term Universal Grammar as well (here capitalized). The most general of these would be that Universal Grammar is whatever properties of a normally developing human brain cause it to learn languages that conform to universal grammar (the non-capitalized, pretheoretical sense). Using the above examples, Universal Grammar would be the innate property of the human brain that causes it to posit a difference between nouns and verbs whenever presented with linguistic data.
As Chomsky puts it, "Evidently, development of language in the individual must involve three factors: (1) genetic endowment, which sets limits on the attainable languages, thereby making language acquisition possible; (2) external data, converted to the experience that selects one or another language within a narrow range; (3) principles not specific to FL." [FL is the faculty of language, whatever properties of the brain cause it to learn language.] So (1) is Universal Grammar in the first theoretical sense, (2) is the linguistic data to which the child is exposed.
Occasionally, aspects of Universal Grammar seem to be describable in terms of general details regarding cognition.
For example, if a predisposition to categorize events and objects as different classes of things is part of human cognition and directly results in nouns and verbs showing up in all languages, then it could be assumed that rather than this aspect of Universal Grammar being specific to language, it is more generally a part of human cognition. To distinguish properties of languages that can be traced to other facts regarding cognition from properties of languages that cannot, the abbreviation UG* can be used. UG is the term often used by Chomsky for those aspects of the human brain which cause language to be the way it is (i.e. are Universal Grammar in the sense used here) but here for discussion it is used for those aspects which are furthermore specific to language (thus UG, as Chomsky uses it, is just an abbreviation for Universal Grammar, but UG* as used here is a subset of Universal Grammar).
In the same article, Chomsky casts the theme of a larger research program in terms of the following question: "How little can be attributed to UG while still accounting for the variety of I-languages attained, relying on third factor principles?" (I-languages meaning internal languages, the brain states that correspond to knowing how to speak and understand a particular language, and third factor principles meaning (3) in the previous quote).
Chomsky has speculated that UG might be extremely simple and abstract, for example only a mechanism for combining symbols in a particular way, which he calls Merge. To see that Chomsky does not use the term "UG" in the narrow sense UG* suggested above, consider the following quote from the same article:
"The conclusion that Merge falls within UG holds whether such recursive generation is unique to FL or is
appropriated from other systems."
I.e. Merge is part of UG because it causes language to be the way it is, is universal, and is not part of (2) (the environment) or (3) (general properties independent of genetics and environment). Merge is part of Universal Grammar whether it is specific to language or whether, as Chomsky suggests, it is also used for example in mathematical thinking.
The distinction is important because there is a long history of argument about UG*, whereas most people working on language agree that there is Universal Grammar. Many people assume that Chomsky means UG* when he writes UG (and in some cases he might actually mean UG*, though not in the passage quoted above).
Some students of universal grammar study a variety of grammars to abstract generalizations called linguistic universals, often in the form of "If X holds true, then Y occurs." These have been extended to a variety of traits, such as the phonemes found in languages, what word orders languages choose, and why children exhibit certain linguistic behaviors.
Later linguists who have influenced this theory include Noam Chomsky and Richard Montague, developing their version of this theory as they considered issues of the Argument from poverty of the stimulus to arise from the constructivist approach to linguistic theory. The application of the idea of Universal Grammar to the area of second language acquisition (SLA) is represented mainly by the McGill linguist Lydia White.
Syntacticians generally hold that there are parametric points of variation between languages, although heated debate occurs over whether UG constraints are essentially universal due to being "hard-wired" (Chomsky's Principles and Parameters approach), a logical consequence of a specific syntactic architecture (the Generalized Phrase Structure approach) or the result of functional constraints on communication (the functionalist approach).
Relation to the evolution of language.
In an article titled, "The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?" Hauser, Chomsky, and Fitch present the three leading hypotheses for how language evolved and brought humans to the point where we have a Universal Grammar.
Hypothesis 1 states that FLB (the Faculty of Language in the broad sense) is strictly homologous to animal communication.
This means that homologous aspects of the Faculty of Language exist in non-human animals.
Hypothesis 2 states that FLB "is a derived, uniquely human adaptation for language". This hypothesis believes that individual traits were subject to natural selection and came to be very specialized for humans.
Hypothesis 3 states that only FLN (the Faculty of Language in the narrow sense) is unique to humans. It believes that while mechanisms of FLB are present in both humans and non-human animals, that the computational mechanism of recursion is recently evolved solely in humans. This is the hypothesis which most closely aligns to the typical theory of Universal Grammar championed by Chomsky.
History.
The idea of a universal grammar can be traced back to Roger Bacon's observation that all languages are built upon a common grammar, even though it may undergo accidental variations, and the 13th century speculative grammarians who, following Bacon, postulated universal rules underlying all grammars. The concept of a universal grammar or language was at the core of the 17th century projects for philosophical languages. There is a Scottish school of universal grammarians from the 18th century, to be distinguished from the philosophical language project, which includes authors such as James Beattie, Hugh Blair, James Burnett, James Harris, and Adam Smith. The article on "Grammar" in the first edition of the Encyclopædia Britannica (1771) contains an extensive section titled "Of Universal Grammar".
The idea rose to notability in modern linguistics with theorists such as Noam Chomsky and Richard Montague, developed in the 1950s to 1970s, as part of the "Linguistics Wars".
During the early 20th century, in contrast, language was usually understood from a behaviourist perspective, suggesting that language learning, like any other kind of learning, could be explained by a succession of trials, errors, and rewards for success. In other words, children learned their mother tongue by simple imitation, listening to and repeating what adults said. For example, when a child says "milk" and the mother will smile and give her some as a result, the child will find this outcome rewarding, enhancing the child's language development.
Chomsky's theory.
Chomsky argued that the human brain contains a limited set of rules for organizing language. This implies in turn that all languages have a common structural basis; the set of rules is what is known as "universal grammar".
Speakers proficient in a language know which expressions are acceptable in their language and which are unacceptable. The key puzzle is how speakers come to know these restrictions of their language, since expressions that violate those restrictions are not present in the input, indicated as such. Chomsky argued that this poverty of stimulus means Skinner's behaviorist perspective cannot explain language acquisition. The absence of negative evidence—evidence that an expression is part of a class of ungrammatical sentences in one's language—is the core of his argument. For example, in English one cannot relate a question word like "what" to a predicate within a relative clause:
Such expressions are not available to language learners: they are, by hypothesis, ungrammatical. Speakers of the local language do not use them, nor note them as unacceptable to language learners. Universal grammar offers a solution to the poverty of the stimulus problem by making certain restrictions universal characteristics of human languages. Language learners are consequently never tempted to generalize in an illicit fashion.
Presence of creole languages.
The presence of creole languages is sometimes cited as further support for this theory, especially by Bickerton's controversial language bioprogram theory. Creoles are languages that are developed and formed when different societies come together and are forced to devise their own system of communication. The system used by the original speakers is typically an inconsistent mix of vocabulary items known as a pidgin. As these speakers' children begin to acquire their first language, they use the pidgin input to effectively create their own original language, known as a creole. Unlike pidgins, creoles have native speakers and make use of a full grammar.
According to Bickerton, the idea of universal grammar is supported by creole languages because certain features are shared by virtually all of these languages. For example, their default point of reference in time (expressed by bare verb stems) is not the present moment, but the past. Using pre-verbal auxiliaries, they uniformly express tense, aspect, and mood. Negative concord occurs, but it affects the verbal subject (as opposed to the object, as it does in languages like Spanish). Another similarity among creoles is that questions are created simply by changing a declarative sentence's intonation, not its word order or content.
However, extensive work by Carla Hudson-Kam and Elissa Newport suggests that creole languages may not support a universal grammar, as has sometimes been supposed. In a series of experiments, Hudson-Kam and Newport looked at how children and adults learn artificial grammars. Notably, they found that children tend to ignore minor variations in the input when those variations are infrequent, and reproduce only the most frequent forms. In doing so, they tend to standardize the language that they hear around them. Hudson-Kam and Newport hypothesize that in a pidgin situation (and in the real life situation of a deaf child whose parents were disfluent signers), children are systematizing the language they hear based on the probability and frequency of forms, and not, as has been suggested on the basis of a universal grammar. Further, it seems unsurprising that creoles would share features with the languages they are derived from and thus look similar "grammatically".
Many adherents of Universal Grammar argue against a concept of Relexification, which says that a language replaces its lexicon almost entirely with that of another. This goes against universalist ideas of a Universal Grammar, which has an innate grammar.
Criticisms.
While the majority of linguistics accept universal grammar, there have been a few linguists who do not accept the theory.
Geoffrey Sampson maintains that universal grammar theories are not falsifiable and are therefore pseudoscientific theory. He argues that the grammatical "rules" linguists posit are simply post-hoc observations about existing languages, rather than predictions about what is possible in a language. Similarly, Jeffrey Elman argues that the unlearnability of languages assumed by Universal Grammar is based on a too-strict, "worst-case" model of grammar, that is not in keeping with any actual grammar. In keeping with these points, James Hurford argues that the postulate of a language acquisition device (LAD) essentially amounts to the trivial claim that languages are learnt by humans, and thus, that the LAD is less a theory than an explanandum looking for theories.
Morten Christiansen and Nick Chater have argued that the relatively fast-changing nature of language would prevent the slower-changing genetic structures from ever catching up, undermining the possibility of a genetically hard-wired universal grammar. Instead of an innate Universal Grammar, they claim, "apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics."
Hinzen summarizes the most common criticisms of Universal Grammar:
Other researchers have come to some of the same conclusions as Hinzen. Christensen and Chater note that there was not a stable environment across all populations, cultures, and languages, in which a language acquisition gene could have adapted. Instead, Christensen and Chater focus on the relationship between language and the learner, claiming that language has been shaped to fit the human brain. According to Christensen and Chater, "apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics." (489).
In addition, it has been suggested that people learn about probabilistic patterns of word distributions in their language, rather than hard and fast rules (see Distributional hypothesis). For example, children overgeneralize the past tense marker "ed" and mispronounce the irregular verbs, producing forms like "goed" and "eated "and correct these errors over time. It has also been proposed that the poverty of the stimulus problem can be largely avoided, if we assume that children employ "similarity-based generalization" strategies in language learning, generalizing about the usage of new words from similar words that they already know how to use.
Language acquisition researcher Michael Ramscar has suggested that when children erroneously expect an ungrammatical form that then never occurs, the repeated failure of expectation serves as a form of implicit negative feedback that allows them to correct their errors over time such as how children correct grammar generalizations like "goed" to "went" through repetitive failure. This implies that word learning is a probabilistic, error-driven process, rather than a process of fast mapping, as many nativists assume.
In the domain of field research, the Pirahã language is claimed to be a counterexample to the basic tenets of Universal Grammar. This research has been led by Daniel Everett. Among other things, this language is alleged to lack all evidence for recursion, including embedded clauses, as well as quantifiers and color terms. According to the writings of Dr. Everett, the Pirahã showed these linguistic shortcomings not because they were simple-minded, but because their culture — which emphasized concrete matters in the present and also lacked creation myths and traditions of art making — did not necessitate it. Some other linguists have argued, however, that some of these properties have been misanalyzed, and that others are actually expected under current theories of Universal Grammar. Other linguists have attempted to reassess the Pirahã to see if it did indeed use recursion. In a corpus analysis of the Pirahã language, linguists failed to disprove Everett's arguments against Universal Grammar and the lack of recursion in Pirahã, but they also stated that there was "no strong evidence for the lack of recursion either" and they stated that there may be "evidence of recursive structure".
Daniel Everett has gone as far as claiming that universal grammar does not exist. In his words, "universal grammar doesn't seem to work, there doesn't seem to be much evidence for [it]. And what can we put in its place? A complex interplay of factors, of which culture, the values human beings share, plays a major role in structuring the way that we talk and the things that we talk about." Michael Tomasello, a developmental psychologist, also supports this claim, arguing that "although many aspects of human linguistic competence have indeed evolved biologically, specific grammatical principles and constructions have not. And universals in the grammatical structure of different languages have come from more general processes and constraints of human cognition, communication, and vocal-auditory processing, operating during the conventionalization and transmission of the particular grammatical constructions of particular linguistic communities."

</doc>
<doc id="40314" url="http://en.wikipedia.org/wiki?curid=40314" title="Munro">
Munro

A Munro (  ) is a mountain in Scotland with a height over 3000 ft. Munros are named after Sir Hugh Munro, 4th Baronet (1856–1919), who produced the first list of such hills, known as "Munros Tables", in 1891. A Munro top is a summit that is not regarded as a separate mountain and which is over 3,000 feet. In the 2012 revision of the tables, published by the Scottish Mountaineering Club, there are 282 Munros and 227 further subsidiary tops. The best known Munro is Ben Nevis, the highest mountain in the British Isles.
The Munros of Scotland present challenging conditions to hikers, particularly in winter. Each year, people die on the mountains. Nevertheless, a popular practice amongst hillwalkers is "Munro bagging", the aim being to climb all of the listed Munros. As of 2009, more than 4,000 had reported completing their round. The first continuous round of the Munros was completed by Hamish Brown in 1974, whilst the current holder of the record for the fastest continuous round is Stephen Pyke, who completed his 2010 round in just under 40 days.
History.
Before the publication of "Munros Tables" in 1891, there was much uncertainty about the number of Scottish peaks over 3,000 feet. Estimates ranged from 31 (in M.J.B. Baddeley's guides) to 236 (listed in Robert Hall's third edition of "The Highland Sportsman and Tourist", published in 1884). When the Scottish Mountaineering Club was formed in 1889, one of its aims was to remedy this by accurately documenting all of Scotland's mountains over 3,000 feet. Sir Hugh Munro, a founding member of the Club, took on the task using his own experience as a mountaineer, as well as detailed study of the Ordnance Survey Six-inch to the mile (1:10,560) and One-inch to the mile (1:63,360) maps.
Munro researched and produced a set of tables that were published in the Scottish Mountaineering Club Journal in September 1891. The tables listed 538 summits over 3,000 feet, 282 of which were regarded as "separate mountains" The term "Munro" applies to separate mountains, while the lesser summits are known as "tops". Munro did not set any measure of topographic prominence by which a peak qualified as a separate mountain, so there has been much debate about how distinct two hills must be if they are to be counted as two separate Munros.
The Scottish Mountaineering Club has revised the tables, both in response to new height data on Ordnance Survey maps and to address the perceived inconsistency as to which peaks qualify for Munro status. In 1992, the publication of Alan Dawson's book "Relative Hills of Britain", showed that three tops not already considered summits, had a prominence of more than 500 ft. Given this they would have qualified as Corbett summits had they been under 3,000 feet. In the 1997 tables these three tops, on Beinn Alligin, Beinn Eighe and Buachaille Etive Beag, gained full Munro summit status. Dawson's book also highlighted a number of significant tops with as much as 60 m of prominence which were not listed as Munro subsidiary tops. The 1997 tables promoted five of these to full Munro status.
Other classification schemes in Scotland, such as the Corbetts 2500 to(-) and Grahams 2000 to(-), require a peak to have a prominence of at least 500 ft for inclusion. The Munros, however, lack a rigid set of criteria for inclusion, with many summits of lesser prominence listed, principally because their summits are hard to reach. The 1997 tables ironed out many anomalies, but despite it being the highest-profile hill list in UK, some still consider it not wholly satisfactory.
During May and July 2009 the Munro Society re-surveyed several mountains that are known to be close to the 3,000 ft figure to determine their height more accurately. On 10 September 2009 the society announced that the mountain Sgùrr nan Ceannaichean, south of Glen Carron, had a height of 913.43 m. Therefore, the Scottish Mountaineering Club removed the Munro status of Sgùrr nan Ceannaichean and this mountain is now a Corbett. In a Summer 2011 height survey by The Munro Society, Beinn a' Chlaidheimh was found to be 914 m and thus short of the Munro mark. In September 2012, the Scottish Mountaineering Club demoted it from Munro to Corbett status.
As of September 2012, the Scottish Mountaineering Club lists 282 Munros and 227 further subsidiary tops.
Munros.
Perhaps the most famous Munro is Ben Nevis in the Lochaber area. It is the highest peak in the British Isles, with an elevation of 1344 m.
Other well-known Munros include:
Bagging the Munros.
Compared with some continental ranges, Scottish mountains might be modest in height, but walking and climbing in them can be treacherous because of their latitude and exposure to Atlantic and Arctic weather systems. Even in summer, conditions can be atrocious; thick fog, strong winds, driving rain and freezing summit temperatures are not unusual.
Winter ascents of some Munros are serious undertakings due to the unpredictable weather, the likelihood of ice and snow, and poor visibility. Some walkers are unprepared for extreme weather on the exposed tops and fatalities are recorded every year, often resulting from slips on wet rock or ice.
Some hillwalkers aim to climb every Munro, known as "Munro bagging". Munro-bagging is a form of peak bagging. A walker who has climbed all Munros is entitled to be called a Munroist.
Notable completions.
By May 2013, more than 5,000 people had completed the Munros. The Scottish Mountaineering Club, who maintain a list of those Munroists who have reported completing the Munros, have attempted to popularise the archaic spelling of "compleation".
Hugh Munro never completed his own list, missing out on Càrn an Fhidhleir and Càrn Cloich-mhuillin (downgraded to a "top" in 1981). Sir Hugh is said to have missed the Inaccessible Pinnacle of Sgùrr Dearg, on the Isle of Skye, which he never climbed. However the "In Pinn", as it is known colloquially within Scottish mountaineering, was only listed as a subsidiary top on his list (despite being several metres higher than Sgùrr Dearg, which was listed as the main top).
The first "compleationist" was to be the Reverend A. E. Robertson, in 1901, later minister at Braes of Rannoch from 1907. However, research has cast doubt on this claim, and it is not certain that he reached the summit of Ben Wyvis. Also it is known that Robertson did not climb the Inaccessible Peak of Sgùrr Dearg. If Robertson is discounted, the first Munroist is Ronald Burn, who completed in 1923. Burn is also (indisputably) the first person to climb all the subsidiary "tops".
The person with the most rounds of Munros is Steven Fallon from Edinburgh, who has completed 13 rounds as of 2006.
Chris Smith became the first Member of Parliament to complete the Munros when he reached the summit of Sgùrr nan Coireachan on 27 May 1989.
Ben Fleetwood is probably the youngest person to have completed a round. He climbed the final Munro of his round – Ben More – on 30 August 2011 at the age of 10 years and 3 months. The youngest compleationist to have done the round without the presence of a parent or a guardian is probably Andy Nisbet, who finished his round in 1972 aged 18 years and 1 month.
Continuous rounds.
Hamish Brown did the first continuous self-propelled round of the Munros (except for the Skye and Mull ferries) between 4 April and 24 July 1974 with 449000 ft of ascent and mostly walking 1639 mi – just 150 mi were on a bicycle. The journey is fully documented in his book "Hamish's Mountain Walk", which is credited with kick-starting the popularity of Munro-bagging as a hobby. The average time taken to bag all the Munros is eight years.
The first reported completion of all the Munros plus the subsidiary tops in one continuous expedition was by Chris Townsend in 1996. His trip lasted between 18 May and 12 September (118 days), he covered a distance of 1770 mi (240 mi by bicycle) with 575000 ft of ascent. The round was broken twice for spells at the office, which could be regarded as stretching the meaning of "continuous".
The first person to complete a winter round (all the Munros in one winter season) was Martin Moran in 1984/85. His journey lasted between 21 December 1984 and 13 March 1985 (83 days), he walked 1028 mi with 412000 ft of ascent. He used motor transport (Campervan) to link his walk.
In the winter of 2005/06, Steve Perry completed a continuous unsupported round entirely on foot (and ferry). He is also the first person to have completed two continuous Munro rounds, having also walked Land's End to John O'Groats via every mainland 3,000 ft mountain between 18 February 2003 and 30 September 2003.
Fastest round.
Charlie Campbell, a former postman from Glasgow, held the record for the fastest round of the Munros between 2000 and 2010. He completed his round in 48 days 12 hours, finishing on 16 July 2000, on Ben Hope.He cycled and swam between Munros; no motorised transport was used. His record was broken by Stephen Pyke of Stone, Staffordshire, in 2010 who completed the round in 39 days, 9 hours. Pyke's round started on the island of Mull on 25 April 2010 and finished on Ben Hope in Sutherland on 3 June 2010. He cycled and kayaked between Munros; no motorised transport was used. He was backed by a support team in a motor home, but had to camp out in the more remote areas.
On 18 September 2011 Alex Robinson and Tom O'Connell finished their self-propelled continuous round on Ben Hope in a time of 48 days 6 hours and 56 minutes setting the second fastest time ever. At the age of just 21, Alex also became the youngest person to have completed a continuous round without the use of any motorised transport.
See also.
The SMC recognises six peaks in England, fifteen in Wales and thirteen in Ireland that would be Munros or Munro Tops if they were in Scotland. These are referred to as Furth Munros, i.e. the Munros furth of Scotland.
The first recorded Furthist is James Parker, who completed on Tryfan (Snowdonia) on 19 April 1929.

</doc>
<doc id="40316" url="http://en.wikipedia.org/wiki?curid=40316" title="Kurdish languages">
Kurdish languages

Kurdish (کوردی, "Kurdî") is a continuum of Northwestern Iranian languages spoken by the Kurds in Western Asia. They form three dialect groups known as "Kurmanji" or Northern Kurdish, "Sorani" or Central Kurdish, and "Pehlewani" or Southern Kurdish. "Gorani" and "Zazaki" are also spoken by Kurds, but are linguistically not Kurdish. Recent (as of 2009) estimates admit anywhere between 20 and 30 million native speakers of Kurdish in total.
The literary output in Kurdish was mostly confined to poetry until the early 20th century, when more general literature began to be developed. Today, there are two principal written Kurdish dialects, namely Kurmanji in the northern parts of the geographical region of Kurdistan, and Sorani further east and south. The Sorani form of Central Kurdish is, along with Arabic, one of the two official languages of Iraq and is in political documents simply referred to as "Kurdish". Kurmanji (Northern Kurdish) is a recognized minority language in Armenia, and is also spoken in Turkey, Syria, Iraq, and Iran.
A separate group of languages, Zaza–Gorani, is spoken by several million Kurds.
Classification and origin.
The Kurdish languages belong to the Iranian branch of the Indo-European family. They are generally classified as Northwestern Iranian languages, or by some scholars as intermediate between Northwestern and Southwestern Iranian. Martin van Bruinessen notes that "Kurdish has a strong south-western Iranian element", whereas "Zaza and Gurani [...] do belong to the north-west Iranian group". 
Ludwig Paul concludes that Kurdish seems to be a Northwestern Iranian language in origin, but acknowledges that it shares many traits with Southwestern Iranian languages like Persian, apparently due to longstanding and intense historical contacts.
Windfuhr identified Kurdish dialects as Parthian, albeit with a Median substratum.
The present state of knowledge about Kurdish allows, at least roughly, drawing the approximate borders of the areas where the main ethnic core of the speakers of the contemporary Kurdish dialects was formed. The most argued hypothesis on the localisation of the ethnic territory of the Kurds remains D.N. Mackenzie's theory, proposed in the early 1960s (Mackenzie 1961). Developing the ideas of P. Tedesco (1921: 255) and regarding the common phonetic isoglosses shared by Kurdish, Persian, and Baluchi, Mackenzie concluded that the speakers of these three languages may once have been in closer contact. 
He has tried to reconstruct the alleged Persian-Kurdish-Baluchi linguistic unity presumably in the central parts of Iran. According to Mackenzie's theory, the Persians (or Proto-Persians) occupied the province of Fars in the southwest (proceeding from the assumption that the Achaemenids spoke Persian), the Baluchis (Proto-Baluchis) inhabited the central areas of Western Iran, and the Kurds (Proto-Kurds), in the wording of G. Windfuhr (1975: 459), lived either in northwestern Luristan or in the province of Isfahan.
Subdivisions.
Kurdish is divided into three groups, where dialects from different groups are not mutually intelligible without acquired bilingualism.
In historical evolution terms, Kurmanji is less modified than Sorani and Kermanshahi in both phonetic and morphological structure. The Sorani group has been influenced by among other things its closer cultural proximity to the other languages spoken by Kurds in the region including the Gorani language in parts of Iranian Kurdistan and Iraqi Kurdistan. The Kermanshahi group has been influenced by among other things its closer cultural proximity to Persian.
Philip G. Kreyenbroek, an expert writing in 1992, says:
Since 1932 most Kurds have used the Roman script to write Kurmanji... Sorani is normally written in an adapted form of the Arabic script... Reasons for describing Kurmanji and Sorani as 'dialects' of one language are their common origin and the fact that this usage reflects the sense of ethnic identity and unity among the Kurds. From a linguistic or at least a grammatical point of view, however, Kurmanji and Sorani differ as much from each other as English and German, and it would seem appropriate to refer to them as languages. For example, Sorani has neither gender nor case-endings, whereas Kurmanji has both... Differences in vocabulary and pronunciation are not as great as between German and English, but they are still considerable.
According to "Encyclopaedia of Islam", although Kurdish is not a unified language, its many dialects are interrelated and at the same time distinguishable from other Western Iranian languages. The same source classifies different Kurdish dialects as two main groups, northern and central. The reality is that the average Kurmanji speaker does not find it easy to communicate with the inhabitants of Suleymania or Halabja.
Some linguistic scholars assert that the term "Kurdish" has been applied extrinsically in describing the language the Kurds speak, whereas ethnic Kurds have used the word term to simply describe their ethnic or national identity and refer to their language as "Kurmanji", "Sorani", "Hewrami", "Kermanshahi", "Kalhery" or whatever other dialect or language they speak. Some historians have noted that it is only recently that the Kurds who speak the Sorani dialect have begun referring to their language as "Kurdî", in addition to their identity, which is translated to simply mean Kurdish.
Gorani, Zazaki and Shabaki.
The Zaza–Gorani languages, spoken by communities in the wider area who identify as ethnic Kurds, are not linguistically classified as Kurdish. 
They are classified as adjunct to Kurdish within the Northwestern Iranian languages, although authorities differ in the details. 
Windfurh 2009 groups Kurdish with Zaza Gorani within a "Northwestern I" group, while Glottolog based on "Encyclopedia Iranica"
prefers an areal grouping of "Central dialects" (or "Kermanic") within Northwest Iranic, with Kurdish but not Zaza-Gorani grouped with "Kermanic".
Gorani appears to be distinct from Kurmanji and Sorani, yet shares vocabulary with both of them and some grammatical similarities with Sorani. Hewrami, a dialect of Gorani, was an important literary language since the fourteenth century but was replaced by Sorani in the twentieth.
European scholars have maintained that Gorani is separate from Kurdish and that Kurdish is synonymous with the Kurmanji-language group, whereas ethnic Kurds maintain that Kurdish encompasses any of the unique languages or dialects spoken by Kurds that are not spoken by neighboring ethnic groups.
Gorani is often classified as part of the Zaza–Gorani branch of Indo-Iranian languages. The Zazaki language, spoken in the northernmost parts of Kurdistan differs both grammatically and in vocabulary and is generally not understandable by Gorani speakers but it is considered related to Gorani. Almost all Zaza-speaking communities, as well as speakers of another closely related language spoken in parts of Iraqi Kurdistan called Shabaki, identify themselves as ethnic Kurds.
Geoffrey Haig and Ergin Öpengin in their recent study suggest grouping the Kurdish languages into Northern Kurdish, Central Kurdish, Southern Kurdish, Gorani and Zazaki, and avoid the subgrouping Zaza–Gorani.
History.
During his stay in Damascus, historian Ibn Wahshiyya came across two books on agriculture written in Kurdish, one on the culture of the vine and the palm tree, and the other on water and the means of finding it out in unknown ground. He translated both from Kurdish into Arabic in the early 9th century AD.
Among the earliest Kurdish religious texts is the "Yazidi Black Book", the sacred book of Yazidi faith. It is considered to have been authored sometime in the 13th century AD by "Hassan bin Adi" (b. 1195 AD), the great-grandnephew of Sheikh Adi ibn Musafir (d. 1162), the founder of the faith. It contains the Yazidi account of the creation of the world, the origin of man, the story of Adam and Eve and the major prohibitions of the faith. From the 15th to 17th centuries, classical Kurdish poets and writers developed a literary language. The most notable classical Kurdish poets from this period were Ali Hariri, Ahmad Khani, Malaye Jaziri and Faqi Tayran.
The Italian priest Maurizio Garzoni published the first Kurdish grammar titled "Grammatica e Vocabolario della Lingua Kurda" in Rome in 1787 after eighteen years of missionary work among the Kurds of Amadiya.
This work is very important in Kurdish history as it is the first acknowledgment of the widespread use of a distinctive Kurdish language. Garzoni was given the title "Father of Kurdology" by later scholars. The Kurdish language was banned in a large portion of Kurdistan for some time. After the 1980 Turkish coup d'état until 1991 the use of the Kurdish language was illegal in Turkey.
Current status.
Today, Central Kurdish is an official language in Iraq. In Syria, on the other hand, publishing materials in Kurdish is forbidden, though this prohibition is not enforced anymore due to the civil war.
Before August 2002, the Turkish government placed severe restrictions on the use of Kurdish, prohibiting the language in education and broadcast media. The Kurdish alphabet is not recognized in Turkey, and the use of Kurdish names containing the letters "X", "W", and "Q", which do not exist in the Turkish alphabet, is not allowed. In 2012, Kurdish-language lessons became an elective subject in public schools. Previously, Kurdish education had only been possible in private institutions.
In Iran, though it is used in some local media and newspapers, it is not used in public schools. In 2005, 80 Iranian Kurds took part in an experiment and gained scholarships to study in Kurdish in Iraqi Kurdistan.
In March 2006, Turkey allowed private television channels to begin airing programming in Kurdish. However, the Turkish government said that they must avoid showing children's cartoons, or educational programs that teach Kurdish, and could broadcast only for 45 minutes a day or four hours a week. However, most of these restrictions on private Kurdish television channels were relaxed in September 2009.
In 2010, Kurdish municipalities in the southeast decided to begin printing water bills, marriage certificates and construction and road signs, as well as emergency, social and cultural notices in Kurdish alongside Turkish. Friday sermons by Imams began to be delivered in the language, and Esnaf provided Kurdish price tags.
The state-run Turkish Radio and Television Corporation (TRT) started its 24-hour Kurdish television station on 1 January 2009 with the motto "we live under the same sky". The Turkish Prime Minister sent a video message in Kurdish to the opening ceremony, which was attended by Minister of Culture and other state officials. The channel uses the "X", "W", and "Q" letters during broadcasting.
In Kyrgyzstan, 96.4% of the Kurdish population still speak Kurdish as their native language. In Kazakhstan, its 88.7% of the Kurds.
Phonology.
According to the Kurdish Academy of Language, Kurdish has the following phonemes:
Vowels.
According to the Kurdish Academy of Language, vowel phonemes of Kurdish are as follows:
As in most modern Iranian languages, Kurdish vowels contrast in quality; they often carry a secondary length distinction that does not affect syllabic weight. This distinction appears in the writing systems developed for Kurdish. The five "short" vowels are /ɛ/, /æ/, /ɪ̈/, /o/, and /u/, and the four long vowels are /aː/, /iː/, /ʉː/, and /uː/.
Indo-European linguistic comparison.
Because Kurdish is an Indo-European language, there are many words that are cognates in Kurdish and other Indo-European languages such as Avestan, Persian, Sanskrit, German, English, Norwegian, Latin and Greek. (Source: "Altiranisches Wörterbuch (1904)" for the first two and last six.)
Vocabulary.
The bulk of the vocabulary in Kurdish is of Iranian origin, especially of northwestern Iranian. A considerable number of loanwords come from Semitic, mainly Arabic, which entered through Islam and historical relations with Arab tribes. Yet, a smaller group of loanwords which are of Armenian, Caucasian, and Turkic origins are used in Kurdish, besides some European words. There are also Kurdish words with no clear etymology.
Writing system.
The Kurdish language is written using four different writing systems. In Iraq and Iran it is written using an Arabic script, composed by Sa'id Kaban Sedqi. More recently, it is sometimes written with a Latin alphabet in Iraq. In Turkey, Syria, and Armenia, it is now written using a Latin script. Kurdish was also written in the Arabic script in Turkey and Syria until 1932. There is a proposal for a unified international recognized Kurdish alphabet based on ISO-8859-1 called "Yekgirtú". Kurdish in the former USSR is written with a Cyrillic alphabet. Kurdish has even been written in the Armenian alphabet in Soviet Armenia and in the Ottoman Empire (a translation of the Gospels in 1857 and of all New Testament in 1872).

</doc>
<doc id="40317" url="http://en.wikipedia.org/wiki?curid=40317" title="UTF-16">
UTF-16

UTF-16 (16-bit Unicode Transformation Format) is a character encoding capable of encoding all 1,112,064 possible characters in Unicode. The encoding is variable-length, as code points are encoded with one or two 16-bit "code units". (also see Comparison of Unicode encodings for a comparison of UTF-8, -16 & -32)
UTF-16 developed from an earlier fixed-width 16-bit encoding known as UCS-2 (for 2-byte Universal Character Set) once it became clear that a fixed-width 2-byte encoding could not encode enough characters to be truly universal.
History.
In the late 1980s work began on developing a uniform encoding for a "Universal Character Set" (UCS) that would replace earlier language-specific encodings with one coordinated system. The goal was to include all required characters from most of the world's languages, as well as symbols from technical domains such as science, mathematics, and music. The original idea was to expand the typical 256-character encodings requiring 1 byte per character with an encoding using 216 = 65,536 values requiring 2 bytes per character. Two groups worked on this in parallel, the IEEE and the Unicode Consortium, the latter representing mostly manufacturers of computing equipment. The two groups attempted to synchronize their character assignments, so that the developing encodings would be mutually compatible. The early 2-byte encoding was usually called "Unicode", but is now called "UCS-2". 
Early in this process, however, it became increasingly clear that 216 characters would not suffice, and IEEE introduced a larger 31-bit space with an encoding (UCS-4) that would require 4 bytes per character. This was resisted by the Unicode Consortium, both because 4 bytes per character wasted a lot of disk space and memory, and because some manufacturers were already heavily invested in 2-byte-per-character technology. The UTF-16 encoding scheme was developed as a compromise to resolve this impasse in version 2.0 of the Unicode standard in July 1996 and is fully specified in RFC 2781 published in 2000 by the IETF.
In UTF-16, code points greater or equal to 216 are encoded using "two" 16-bit code units. The standards organizations chose the largest block available of un-allocated code points to use as these code units (they apparently felt it unwise to use any of the Private Use Areas). They neglected to define a method of encoding these code points, thus leading to a "hole" in the set of possible code points, a source of some difficulty when dealing with Unicode. This mistake was not made with UTF-8.
UTF-16 is specified in the latest versions of both the international standard ISO/IEC 10646 and the Unicode Standard.
Description.
U+0000 to U+D7FF and U+E000 to U+FFFF.
Both UTF-16 and UCS-2 encode code points in this range as single 16-bit code units that are numerically equal to the corresponding code points. These code points in the BMP are the "only" code points that can be represented in UCS-2. Modern text almost exclusively consists of these code points.
U+10000 to U+10FFFF.
Code points from the other planes (called Supplementary Planes) are encoded in as two 16-bit code units called "surrogate pairs", by the following scheme:
There was an attempt to rename "high" and "low" surrogates to "leading" and "trailing" due to their numerical values not matching their names. This appears to have been abandoned in recent Unicode standards.
Since the ranges for the high surrogates, low surrogates, and valid BMP characters are disjoint, searches are simplified: it is not possible for part of one character to match a different part of another character. It also means that UTF-16 is "self-synchronizing" on 16-bit words: whether a code unit starts a character can be determined without examining earlier code units. UTF-8 shares these advantages, but many earlier multi-byte encoding schemes did not allow unambiguous searching and could only be synchronized by re-parsing from the start of the string (UTF-16 is not self-synchronizing if one byte is lost or if traversal starts at a random byte).
Because the most commonly used characters are all in the Basic Multilingual Plane, handling of surrogate pairs is often not thoroughly tested. This leads to persistent bugs and potential security holes, even in popular and well-reviewed application software (e.g. CVE-2008-2938, CVE-2012-2135).
U+D800 to U+DFFF.
The Unicode standard permanently reserves these code point values for UTF-16 encoding of the high and low surrogates, and they will never be assigned a character, so there should be no reason to encode them. The official Unicode standard says that no UTF forms, including UTF-16, can encode these code points.
However UCS-2, UTF-8, and UTF-32 can encode these code points in trivial and obvious ways, and large amounts of software does so even though the standard states that such arrangements should be treated as encoding errors. It is possible to unambiguously encode them in UTF-16 by using a code unit equal to the code point, as long as no sequence of two code units can be interpreted as a legal surrogate pair (that is, as long as a high surrogate is never followed by a low surrogate). The majority of UTF-16 encoder and decoder implementations translate between encodings as though this were the case.
Examples.
Consider the encoding of U+10437 (𐐷):
The following table summarizes this conversion, as well as others. The colors indicate how bits from the code point are distributed among the UTF-16 bytes. Additional bits added by the UTF-16 encoding process are shown in black.
Example code.
C functions to convert Unicode code points from/to UTF-16 streams, assuming there are read and write functions that handle the 16-bit code units. The decoder requires the ability to "push back" a 16-bit code unit so the next read gets it (this is a popular but not the only method of dealing with unpaired surrogates):
Byte order encoding schemes.
UTF-16 and UCS-2 produce a sequence of 16-bit code units. Since most communication and storage protocols are defined for bytes, and each unit thus takes two 8-bit bytes, and the order of the bytes may depend on the endianness (byte order) of the computer architecture.
To assist in recognizing the byte order of code units, UTF-16 allows a Byte Order Mark (BOM), a code unit with the value U+FEFF, to precede the first actual coded value. (U+FEFF is the invisible zero-width non-breaking space/ZWNBSP character.) If the endian architecture of the decoder matches that of the encoder, the decoder detects the 0xFEFF value, but an opposite-endian decoder interprets the BOM as the non-character value U+FFFE reserved for this purpose. This incorrect result provides a hint to perform byte-swapping for the remaining values. If the BOM is missing, RFC 2781 says that big-endian encoding should be assumed. (In practice, due to Windows using little-endian order by default, many applications also assume little-endian encoding by default.) If there is no BOM, one method of recognizing a UTF-16 encoding is searching for the space character (U+0020) which is very common in texts in most languages.
The standard also allows the byte order to be stated explicitly by specifying UTF-16BE or UTF-16LE as the encoding type. When the byte order is specified explicitly this way, a BOM is specifically "not" supposed to be prepended to the text, and a U+FEFF at the beginning should be handled as a ZWNBSP character. Many applications ignore the BOM code at the start of any Unicode encoding. Web browsers often use a BOM as a hint in determining the character encoding.
For Internet protocols, IANA has approved "UTF-16", "UTF-16BE", and "UTF-16LE" as the names for these encodings. (The names are case insensitive.) The aliases UTF_16 or UTF16 may be meaningful in some programming languages or software applications, but they are not standard names in Internet protocols.
UCS-2 encoding is defined to be big-endian only. In practice most software defaults to little-endian, and handles a leading BOM to define the byte order just as in UTF-16. Although the similar designations UCS-2BE and UCS-2LE imitate the UTF-16 labels, they do not represent official encoding schemes.
Usage.
UTF-16 is used for text in the OS API in Microsoft Windows 2000/XP/2003/Vista/7/8/CE. Older Windows NT systems (prior to Windows 2000) only support UCS-2. In Windows XP, no code point above U+FFFF is included in any font delivered with Windows for European languages. Files and network data tend to be a mix of UTF-16, UTF-8, and legacy byte encodings.
IBM iSeries systems designate code page CCSID 13488 for UCS-2 character encoding, CCSID 1200 for UTF-16 encoding, and CCSID 1208 for UTF-8 encoding.
UTF-16 is used by the Qualcomm BREW operating systems; the .NET environments; and the Qt cross-platform graphical widget toolkit.
Symbian OS used in Nokia S60 handsets and Sony Ericsson UIQ handsets uses UCS-2.
The Joliet file system, used in CD-ROM media, encodes file names using UCS-2BE (up to sixty-four Unicode characters per file name).
The Python language environment officially only uses UCS-2 internally since version 2.0, but the UTF-8 decoder to "Unicode" produces correct UTF-16. Since Python 2.2, "wide" builds of Unicode are supported which use UTF-32 instead; these are primarily used on Linux. Python 3.3 no longer ever uses UTF-16, instead strings are stored in one of ASCII/Latin-1, UCS-2, or UTF-32, depending on which code points are in the string, with a UTF-8 version also included so that repeated conversions to UTF-8 are fast.
Java originally used UCS-2, and added UTF-16 supplementary character support in J2SE 5.0.
In many languages quoted strings need a new syntax for quoting non-BMP characters, as the codice_1 syntax explicitly limits itself to 4 hex digits. The most common (used by C#, D and several other languages) is to use an upper-case 'U' with 8 hex digits such as codice_2 In Java 7 regular expressions and ICU and Perl, the syntax codice_3 must be used. In many other cases (such as Java outside of regular expressions) the only way to get non-BMP characters is to enter the surrogate halves individually, for example: codice_4 for U+1D11E.
These implementations all return the number of 16-bit code units rather than the number of Unicode code points when the equivalent of strlen() is used on their strings, and indexing into a string returns the indexed code unit, not the indexed code point, this leads some people to claim that UTF-16 is not supported. However the term "character" is defined and used in multiple ways within the Unicode terminology, so an unambiguous count is not possible and there is no reason for strlen to attempt to return any such value. Most of the confusion is due to obsolete ASCII-era documentation using the term "character" when a fixed-size "byte" or "octet" was intended.

</doc>
<doc id="40318" url="http://en.wikipedia.org/wiki?curid=40318" title="Portland cement">
Portland cement

 
Portland cement is the most common type of cement in general use around the world, used as a basic ingredient of concrete, mortar, stucco, and most non-speciality grout. It developed from other types of hydraulic lime in England in the mid 19th century and usually originates from limestone. It is a fine powder produced by heating materials in a kiln to form what is called clinker, grinding the clinker, and adding small amounts of other materials. Several types of Portland cement are available with the most common being called ordinary Portland cement (OPC) which is grey in color, but a white Portland cement is also available.
Portland cement is caustic so it can cause chemical burns, the powder can cause irritation or with severe exposure lung cancer, and contains some toxic ingredients such as silica and chromium. Environmental concerns are the high energy consumption required to mine, manufacture, and transport the cement and the related air pollution including the release of greenhouse gases (e.g., carbon dioxide), dioxin, NOx, SO2, and particulates.
The low cost and widespread availability of the limestone, shales, and other naturally occurring materials used in Portland cement make it one of the lowest-cost materials widely used over the last century throughout the world. Concrete is one of the most versatile construction materials available in the world.
History.
Portland cement was developed from natural cements made in Britain beginning in the middle of the 18th century. Its name is derived from its similarity to Portland stone, a type of building stone quarried on the Isle of Portland in Dorset, England.
The development of modern Portland cement (sometimes called ordinary or normal Portland cement) began in 1756 when John Smeaton experimented with combinations of different limestones and additives including trass and pozzolanas relating to the planned construction of a lighthouse now known as Smeaton's Tower. In the late 18th century, Roman cement was developed and patented in 1796 by James Parker; Roman cement quickly became popular, but was largely replaced by Portland cement in the 1850s. In 1811 James Frost produced a cement he called British cement. James Frost is reported to have erected a manufactory for making of an artificial cement in 1826. In 1843, Aspdin's son William improved their cement, which was initially called "Patent Portland cement", although he had no patent. In 1818, French engineer Louis Vicat invented an artificial hydraulic lime considered the "principal forerunner" of Portland cement and "...Edgar Dobbs of Southwark patented a cement of this kind in 1811." Portland cement was used by Joseph Aspdin in his cement patent in 1824 because of the cements' resemblance to Portland stone. The name "Portland cement" is also recorded in a directory published in 1823 being associated with a William Lockwood, Dave Stewart, and possibly others. However, Aspdins' cement was nothing like modern Portland cement, but was a first step in the development of modern Portland cement, called a 'proto-Portland cement'. William Aspdin had left his fathers company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. In 1848, William Aspdin further improved his cement; in 1853, he moved to Germany, where he was involved in cement making. William Aspdin made what could be called 'meso-Portland cement' (a mix of Portland cement and hydraulic lime). Isaac Charles Johnson further refined the production of 'meso-Portland cement' (middle stage of development) and claimed to be the real father of Portland cement. John Grant of the Metropolitan Board of Works in 1859 set out requirements for cement to be used in the London sewer project. This became a specification for Portland cement. The next development with the manufacture of Portland cement was the introduction of the rotary kiln patented by German Friedrich Hoffmann called a Hoffmann kiln for brick making in 1858 and then Frederick Ransome in 1885 (U.K.) and 1886 (U.S.) which allowed a stronger, more homogeneous mixture and a continuous manufacturing process. The Hoffman "endless" kiln which gave "perfect control over combustion" was tested in 1860 and showed the process produced a better grade of cement. This cement was made at the Portland Cementfabrik Stern at Stettin, which was the first to use a Hoffman kiln. It is thought that the first modern Portland cement was made there. The Association of German Cement Manufacturers issued a standard on Portland cement in 1878.
Portland cement had been imported into the United States from Germany and England and in the 1870s and 1880s it was being produced by Eagle Portland cement near Kalamazoo, Michigan, and in 1875, the first Portland cement was produced by Coplay Cement Company under the direction of David O. Saylor in Coplay, Pennsylvania. By the early 20th century American made Portland cement had displaced most of the imported Portland cement.
Manufacturing.
ASTM C150 defines Portland cement as "hydraulic cement (cement that not only hardens by reacting with water but also forms a water-resistant product) produced by pulverizing clinkers which consist essentially of hydraulic calcium silicates, usually containing one or more of the forms of calcium sulphate as an inter ground addition." The European Standard EN 197-1 uses the following definition:
Portland cement clinker is a hydraulic material which shall consist of at least two-thirds by mass of calcium silicates (3 CaO·SiO2 and 2 CaO·SiO2), the remainder consisting of aluminium- and iron-containing clinker phases and other compounds. The ratio of CaO to SiO2 shall not be less than 2.0. The magnesium oxide content (MgO) shall not exceed 5.0% by mass. 
(The last two requirements were already set out in the German Standard, issued in 1909).
Clinkers make up more than 90% of the cement along with a limited amount of calcium sulfate (which controls the set time) and up to 5% minor constituents (fillers) as allowed by various standards. Clinkers are nodules (diameters, 0.2–1.0 inch [5–25 mm]) of a sintered material that is produced when a raw mixture of predetermined composition is heated to high temperature. The key chemical reaction which defines Portland cement from other hydraulic limes occurs at these high temperatures (>1300 C and is when the belite (Ca2SiO4) combines with calcium oxide (CaO) to form alite (Ca3SiO5).
Portland cement clinker is made by heating, in a cement kiln, a mixture of raw materials to a calcining temperature of above 600 C and then a fusion temperature, which is about 1450 C for modern cements, to sinter the materials into clinker. The materials in cement clinker are alite, belite, tri-calcium aluminate, and tetra-calcium alumino ferrite. The aluminium, iron, and magnesium oxides are present as a flux allowing the calcium silicates to form at a lower temperature and contribute little to the strength. For special cements, such as Low Heat (LH) and Sulfate Resistant (SR) types, it is necessary to limit the amount of tricalcium aluminate (3 CaO·Al2O3) formed. The major raw material for the clinker-making is usually limestone (CaCO3) mixed with a second material containing clay as source of alumino-silicate. Normally, an impure limestone which contains clay or SiO2 is used. The CaCO3 content of these limestones can be as low as 80%. Secondary raw materials (materials in the rawmix other than limestone) depend on the purity of the limestone. Some of the materials used are clay, shale, sand, iron ore, bauxite, fly ash, and slag. When a cement kiln is fired by coal, the ash of the coal acts as a secondary raw material.
Cement grinding.
To achieve the desired setting qualities in the finished product, a quantity (2–8%, but typically 5%) of calcium sulfate (usually gypsum or anhydrite) is added to the clinker and the mixture is finely ground to form the finished cement powder. This is achieved in a cement mill. The grinding process is controlled to obtain a powder with a broad particle size range, in which typically 15% by mass consists of particles below 5 μm diameter, and 5% of particles above 45 μm. The measure of fineness usually used is the "specific surface area", which is the total particle surface area of a unit mass of cement. The rate of initial reaction (up to 24 hours) of the cement on addition of water is directly proportional to the specific surface area. Typical values are 320–380 m2·kg−1 for general purpose cements, and 450–650 m2·kg−1 for "rapid hardening" cements. The cement is conveyed by belt or powder pump to a silo for storage. Cement plants normally have sufficient silo space for one to 20 weeks of production, depending upon local demand cycles. The cement is delivered to end users either in bags or as bulk powder blown from a pressure vehicle into the customer's silo. In industrial countries, 80% or more of cement is delivered in bulk.
Setting and hardening.
Cement sets when mixed with water by way of a complex series of chemical reactions still only partly understood. The different constituents slowly crystallise and the interlocking of their crystals gives cement its strength. Carbon dioxide is slowly absorbed to convert the portlandite (Ca(OH)2) into insoluble calcium carbonate. After the initial setting, immersion in warm water will speed up setting. Gypsum is added as an inhibitor to prevent flash setting.
Use.
The most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load bearing) element. Concrete can be used in the construction of structural elements like panels, beams, road furniture, or may make cast-"in situ" concrete for building superstructures like roads and dams. These may be supplied with concrete mixed on site, or may be provided with "ready-mixed" concrete made at permanent mixing sites. Portland cement is also used in mortars (with sand and water only) for plasters and screeds, and in grouts (cement/water mixes squeezed into gaps to consolidate foundations, road-beds, etc.).
When water is mixed with Portland cement, the product sets in a few hours and hardens over a period of weeks. These processes can vary widely depending upon the mix used and the conditions of curing of the product, but a typical concrete sets in about 6 hours and develops a compressive strength of 8 MPa in 24 hours. The strength rises to 15 MPa at 3 days, 23 MPa at 1 week, 35 MPa at 4 weeks and 41 MPa at 3 months. In principle, the strength continues to rise slowly as long as water is available for continued hydration, but concrete is usually allowed to dry out after a few weeks and this causes strength growth to stop.
Types.
General.
Different standards are used for classification of Portland cement. The two major standards are the ASTM C150 used primarily in the USA and European EN 197. EN 197 cement types CEM I, II, III, IV, and V do not correspond to the similarly named cement types in ASTM C150.
ASTM C150.
The five types of Portland cements exist, with variations of the first three according to ASTM C150.
Type I Portland cement is known as common or general-purpose cement. It is generally assumed unless another type is specified. It is commonly used for general construction especially when making precast and precast-prestressed concrete that is not to be in contact with soils or ground water. The typical compound compositions of this type are:
55% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 2.8% MgO, 2.9% (SO3), 1.0% ignition loss, and 1.0% free CaO
A limitation on the composition is that the (C3A) shall not exceed 15%.
Type II gives off less heat during hydration. This type of cement costs about the same as type I. Its typical compound composition is:
51% (C3S), 24% (C2S), 6% (C3A), 11% (C4AF), 2.9% MgO, 2.5% (SO3), 0.8% ignition loss, and 1.0% free CaO
A limitation on the composition is that the (C3A) shall not exceed 8%, which reduces its vulnerability to sulfates. This type is for general construction exposed to moderate sulfate attack and is meant for use when concrete is in contact with soils and ground water, especially in the western United States due to the high sulfur content of the soils. Because of similar price to that of type I, type II is much used as a general purpose cement, and the majority of Portland cement sold in North America meets this specification.
Note: Cement meeting (among others) the specifications for types I and II has become commonly available on the world market.
Type III has relatively high early strength. Its typical compound composition is: 57% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 3.0% MgO, 3.1% (SO3), 0.9% Ignition loss, and 1.3% free CaO. This cement is similar to type I, but ground finer. Some manufacturers make a separate clinker with higher C3S and/or C3A content, but this is increasingly rare, and the general purpose clinker is usually used, ground to a specific surface area typically 50–80% higher. The gypsum level may also be increased a small amount. This gives the concrete using this type of cement a three-day compressive strength equal to the seven-day compressive strength of types I and II. Its seven-day compressive strength is almost equal to 28-day compressive strengths of types I and II. The only downside is that the six-month strength of type III is the same or slightly less than that of types I and II. Therefore, the long-term strength is sacrificed a little. It is usually used for precast concrete manufacture, where high one-day strength allows fast turnover of molds. It may also be used in emergency construction and repairs and construction of machine bases and gate installations.
Type IV Portland cement is generally known for its low heat of hydration. Its typical compound composition is: 28% (C3S), 49% (C2S), 4% (C3A), 12% (C4AF), 1.8% MgO, 1.9% (SO3), 0.9% Ignition loss, and 0.8% free CaO. The percentages of (C2S) and (C4AF) are relatively high and (C3S) and (C3A) are relatively low. A limitation on this type is that the maximum percentage of (C3A) is seven, and the maximum percentage of (C3S) is thirty-five. This causes the heat given off by the hydration reaction to develop at a slower rate. However, as a consequence the strength of the concrete develops slowly. After one or two years the strength is higher than the other types after full curing. This cement is used for very large concrete structures, such as dams, which have a low surface to volume ratio. This type of cement is generally not stocked by manufacturers but some might consider a large special order. This type of cement has not been made for many years, because Portland-pozzolan cements and ground granulated blast furnace slag addition offer a cheaper and more reliable alternative.
Type V is used where sulfate resistance is important. Its typical compound composition is: 38% (C3S), 43% (C2S), 4% (C3A), 9% (C4AF), 1.9% MgO, 1.8% (SO3), 0.9% Ignition loss, and 0.8% free CaO. This cement has a very low (C3A) composition which accounts for its high sulfate resistance. The maximum content of (C3A) allowed is 5% for type V Portland cement. Another limitation is that the (C4AF) + 2(C3A) composition cannot exceed 20%. This type is used in concrete to be exposed to alkali soil and ground water sulfates which react with (C3A) causing disruptive expansion. It is unavailable in many places, although its use is common in the western United States and Canada. As with type IV, type V Portland cement has mainly been supplanted by the use of ordinary cement with added ground granulated blast furnace slag or tertiary blended cements containing slag and fly ash.
Types Ia, IIa, and IIIa have the same composition as types I, II, and III. The only difference is that in Ia, IIa, and IIIa, an air-entraining agent is ground into the mix. The air-entrainment must meet the minimum and maximum optional specification found in the ASTM manual. These types are only available in the eastern United States and Canada, only on a limited basis. They are a poor approach to air-entrainment which improves resistance to freezing under low temperatures.
Types II(MH) and II(MH)a have a similar composition as types II and IIa, but with a mild heat.
EN 197.
EN 197-1 defines five classes of common cement that comprise Portland cement as a main constituent. These classes differ from the ASTM classes.
Constituents that are permitted in Portland-composite cements are artificial pozzolans (blastfurnace slag, silica fume, and fly ashes) or natural pozzolans (siliceous or siliceous aluminous materials such as volcanic ash glasses, calcined clays and shale).
White Portland cement.
White Portland cement or white ordinary Portland cement (WOPC) is similar to ordinary, grey Portland cement in all respects except for its high degree of whiteness. Obtaining this colour requires some modification to the method of manufacture; because of this, it is somewhat more expensive than the grey product. The main requirement is to have low iron content which should be less than 0.5% expressed as Fe2O3 for white cement and less than 0.9% for off-white cement. It helps to have the iron oxide as ferrous oxide (FeO) which is obtained via slight reducing conditions i.e. operating with zero excess oxygen at the kiln exit. This gives the clinker and cement a green tinge. Other metals such as Cr, Mn, Ti, etc. in trace content can also give color tinges, so for a project it is best to use cement from a single source.
Safety issues.
Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is also exothermic. As a result, wet cement is strongly caustic and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. The reaction of cement dust with moisture in the sinuses and lungs can also cause a chemical burn as well as headaches, fatigue, and lung cancer.
The production of comparatively low-alkalinity cements (pH<11) is an area of ongoing investigation.
In Scandinavia, France, and the UK, the level of chromium(VI), which is considered to be toxic and a major skin irritant, may not exceed 2 ppm.
Environmental effects.
Portland cement manufacture can cause environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust; gases; noise and vibration when operating machinery and during blasting in quarries; consumption of large quantities of fuel during manufacture; release of CO2 from the raw materials during manufacture, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.
Epidemiologic Notes and Reports Sulfur Dioxide Exposure in Portland Cement Plants, from the Centers for Disease Control, states "Workers at Portland cement facilities, particularly those burning fuel containing sulfur, should be aware of the acute and chronic effects of exposure to SO2 [sulfur dioxide], and peak and full-shift concentrations of SO2 should be periodically measured."—
"The Arizona Department of Environmental Quality was informed this week that the Arizona Portland Cement Co. failed a second round of testing for emissions of hazardous air pollutants at the company's Rillito plant near Tucson. The latest round of testing, performed in January 2003 by the company, is designed to ensure that the facility complies with federal standards governing the emissions of dioxins and furans, which are byproducts of the manufacturing process." Cement Reviews' "Environmental News" web page details case after case of environmental problems with cement manufacturing.
An independent research effort of AEA Technology to identify critical issues for the cement industry today concluded the most important environment, health and safety performance issues facing the cement industry are atmospheric releases (including greenhouse gas emissions, dioxin, NOx, SO2, and particulates), accidents, and worker exposure to dust.
The CO2 associated with Portland cement manufacture comes from 3 sources:
Source 1 is fairly constant: minimum around 0.47 kg CO2 per kg of cement, maximum 0.54, typical value around 0.50 worldwide. Source 2 varies with plant efficiency: efficient precalciner plant 0.24 kg CO2 per kg cement, low-efficiency wet process as high as 0.65, typical modern practices (e.g. UK) averaging around 0.30. Source 3 is almost insignificant at 0.002–0.005. So typical total CO2 is around 0.80 kg CO2 per kg finished cement. This omits the CO2 associated with electric power consumption because this varies according to the local generation type and efficiency. Typical electrical energy consumption is of the order of 90–150 kWh per tonne cement, equivalent to 0.09–0.15 kg CO2 per kg finished cement if the electricity is coal-generated.
Overall, with nuclear- or hydroelectric power and efficient manufacturing, CO2 generation can be reduced to 0.7 kg per kg cement, but can be twice as high. The thrust of innovation for the future is to reduce sources 1 and 2 by modification of the chemistry of cement, by the use of wastes, and by adopting more efficient processes. Although cement manufacturing is clearly a very large CO2 emitter, concrete (of which cement makes up about 15%) compares quite favourably with other building systems in this regard.
Cement plants used for waste disposal or processing.
Due to the high temperatures inside cement kilns, combined with the oxidizing (oxygen-rich) atmosphere and long residence times, cement kilns are used as a processing option for various types of waste streams: indeed, they efficiently destroy many hazardous organic compounds. The waste streams also often contain combustible materials which allow the substitution of part of the fossil fuel normally used in the process.
Waste materials used in cement kilns as a fuel supplement:
Portland cement manufacture also has the potential to benefit from using industrial byproducts from the waste stream. These include in particular:

</doc>
<doc id="40319" url="http://en.wikipedia.org/wiki?curid=40319" title="Sarah Hughes">
Sarah Hughes

Sarah Elizabeth Hughes (born May 2, 1985) is an American figure skater. She is the 2002 Olympic Champion and 2001 World bronze medalist in ladies' singles.
Personal life.
Hughes was born in Great Neck, New York. Her father, John Hughes, is a Canadian of Irish descent and was one of the captains of the undefeated and untied NCAA champion 1969–70 Cornell University ice hockey team. Her mother, Amy Pastarnack, is Jewish and is a breast cancer survivor. This led Sarah Hughes to become an advocate for breast cancer awareness. She appeared in a commercial for General Electric promoting breast cancer awareness and research. Hughes stated: "I always said that if I can get one person to get a mammogram, I've accomplished something." Among the other causes Hughes supports are Figure Skating in Harlem, which provides free ice skating lessons and academic tutoring for girls in the Harlem community in New York City. Hughes has supported this program for over ten years.
Hughes attended Great Neck North High School. In 2003, she began her studies at Yale University. On May 25, 2009, Hughes graduated from Yale and received a bachelor's degree in American studies with a concentration in U.S. politics and communities.
Sarah Hughes is the fourth of six children. One of her younger sisters, Emily, is also a figure skater and competed at the 2006 Winter Olympics. In December 2012, her older brother Matt, graduated from the police academy and is currently an NYPD officer. She is the cousin of Gregg "Opie" Hughes, from the Opie & Anthony show.
Career.
Sarah Hughes began skating at the age of three. Robin Wagner, who also choreographed for her from 1994, became her head coach in January 1998.
Hughes won the junior title at the 1998 U.S. Championships in the 1997–1998 season. The following season, she competed on the ISU Junior Grand Prix and won the silver medal at the 1998–1999 Junior Grand Prix Final. She also took silver at the 1999 World Junior Championships held in November 1998. At the 1999 U.S. Championships, Hughes won the pewter medal in her senior-level debut. As the fourth-place finisher, Hughes would not normally have received one of the three spots for U.S. ladies at the 1999 World Championships, however, Naomi Nari Nam, the silver medalist, was not age-eligible for the event according to ISU rules. Hughes was likewise not age-eligible but at the time a loophole existed for skaters who had medaled at Junior Worlds. Hughes was sent to senior Worlds and finished 7th in her debut.
In the 1999–2000 season, Hughes made her Grand Prix debut, winning the bronze medal at the 1999 Trophée Lalique. She won the bronze medal at the 2000 U.S. Championships and was credited with a triple-salchow-triple-loop combination. She placed 5th at the 2000 World Championships.
In the 2000–2001 season, she won three medals on the Grand Prix circuit and won the bronze medal at the 2000–2001 Grand Prix of Figure Skating Final. She won the silver medal at the 2001 U.S. Championships. At the 2001 Worlds, she won the bronze medal.
In the 2001–2002 season, Hughes competed again on the Grand Prix, winning the 2001 Skate Canada International and placing second at her other two events. She won her second consecutive bronze medal at the Grand Prix Final and then won the bronze medal at the 2002 U.S. Championships to qualify for the 2002 Winter Olympics.
The week before the opening of the 2002 Olympics, Hughes appeared on the cover of "Time" magazine.
At the 2002 Olympics, Hughes placed fourth in the short program after being penalized for underrotating her triple flip and lutz. In her long program, she landed seven triple jumps, including a triple toe loop-triple loop and a triple salchow-triple loop combination. She won the long program, as the three contenders ahead of her after the short program all made mistakes in their respective long programs. Figure Skating rules at the time dictated that if someone placed fourth in the short program, but won the free skate, they could not automatically win the event. Michelle Kwan, who was in first place after the short program would have to lose the free program to Hughes and one other skater as well. Hughes won the free skate, with Irina Slutskaya placing second in that portion, ahead of Kwan. Therefore, the final standings were Hughes in first, Slutskaya in second and Kwan in third. She is the only American woman to have won the Olympic title without ever having won either a World or U.S. senior national title.
After her Olympic win, Hughes was honored with a parade in her hometown of Great Neck. Senator Hillary Rodham Clinton spoke at the event and declared it Sarah Hughes Day. She received the James E. Sullivan Award as the top amateur athlete in the U.S.
Hughes did not compete at the 2002 Worlds. For the 2002–2003 season, she won the silver medal at the 2003 U.S. Championships and placed sixth at the 2003 World Championships.
Hughes took the 2004–2005 year off from college to skate professionally with the Smuckers Stars on Ice tour company. She was inducted into the International Jewish Sports Hall of Fame in 2005.
Hughes' biography, "Sudden Champion: The Sarah Hughes Story", was written by Richard Krawiec in 2002.
Skating technique.
Hughes had a variety of triple-triple jump combinations, including a triple loop-triple loop, triple salchow-triple loop, and a triple toe-triple loop. Her best jump was perhaps the triple loop which she often completed out of a back spiral. She was also known for her camel spin with change of edge, and her layback spin position.

</doc>
<doc id="40321" url="http://en.wikipedia.org/wiki?curid=40321" title="Soundgarden">
Soundgarden

Soundgarden is an American rock band formed in Seattle, Washington, in 1984 by singer and rhythm guitarist Chris Cornell, lead guitarist Kim Thayil, and bassist Hiro Yamamoto. Matt Cameron became the band's full-time drummer in 1986, while bassist Ben Shepherd became a permanent replacement for Yamamoto in 1990.
Soundgarden was one of the seminal bands in the creation of grunge, a style of alternative rock that developed in Seattle, and was one of a number of grunge bands signed to the record label Sub Pop. Soundgarden was the first grunge band to sign to a major label (A&M Records, in 1988), though the band did not achieve commercial success until they popularized the genre in the early 1990s with Seattle contemporaries Pearl Jam, Nirvana, and Alice in Chains.
Soundgarden achieved its biggest success with the 1994 album "Superunknown", which debuted at number one on the "Billboard" charts and yielded the Grammy Award-winning singles "Black Hole Sun" and "Spoonman". In 1997, the band broke up due to internal strife over its creative direction. After several years working on projects and other bands, Soundgarden reunited in 2010 and their sixth studio album, "King Animal", was released two years later.
As of 2012, Soundgarden had sold more than 10.5 million records in the United States, and an estimated 22.5 million worldwide. VH1 ranked Soundgarden at number 14 in their special "100 Greatest Artists of Hard Rock".
History.
Formation and early recordings (1984–1988).
Soundgarden's origins can be found in a band called The Shemps, which performed around Seattle in the early 1980s, and featured bassist Hiro Yamamoto and drummer and singer Chris Cornell. Following Yamamoto's departure, the band recruited guitarist Kim Thayil as its new bassist. Thayil had moved to Seattle from Park Forest, Illinois, with Yamamoto and Bruce Pavitt, who would later start the independent record label Sub Pop. Cornell and Yamamoto stayed in contact, and after The Shemps broke up Cornell and Yamamoto started jamming together, and were eventually joined by Thayil.
Soundgarden was formed in 1984 by Cornell (drums and vocals), Yamamoto (bass), and Thayil (guitar). The band named themselves after a wind-channeling pipe sculpture, "A Sound Garden", located on National Oceanic and Atmospheric Administration property at 7600 Sand Point Way next to Magnuson Park, Seattle. Cornell originally played drums while singing, but in 1985 the band enlisted Scott Sundquist to allow Cornell to concentrate on vocals. The band traveled around playing various concerts with this line-up for about a year. Their first recordings were three songs that appeared on a 1986 compilation album for C/Z Records called "Deep Six". It also featured songs by fellow grunge pioneers Green River, Skin Yard, Malfunkshun, The U-Men, and The Melvins. In 1986, Sundquist left the band to spend time with his family, and was replaced by Matt Cameron, the drummer from Skin Yard.
KCMU DJ Jonathan Poneman was impressed after seeing Soundgarden perform one night, later saying, "I saw this band that was everything rock music should be." Poneman offered to fund a release by the band, so Thayil told him to team up with Bruce Pavitt. Poneman offered to contribute $20,000 in funding for Sub Pop, effectively turning it into a full-fledged record label. Soundgarden signed to Sub Pop, and the label released "Hunted Down" in 1987 as the band's first single. The B-side of the "Hunted Down" single, "Nothing to Say", appeared on the KCMU compilation tape "Bands That Will Make Money", which was distributed to record companies, many of whom showed interest in Soundgarden. Through Sub Pop, the band released the "Screaming Life" EP in 1987, and the "Fopp" EP in 1988. A combination of the two was issued as "Screaming Life/Fopp" in 1990.
Debut album, major label signing, and rift with audience (1988–1990).
Though the band was being courted by major labels, in 1988 it signed to the smaller label SST Records for its debut album, "Ultramega OK", released on October 31, 1988. Cornell said that the band "made a huge mistake with "Ultramega OK"" since they used a producer suggested by SST who "didn't know what was happening in Seattle." On that album, Soundgarden demonstrates, according to Steve Huey of AllMusic, a "Stooges/MC5-meets-Zeppelin/Sabbath sound." The band's first music video, "Flower", was directed by Mark Miremont, and aired regularly on MTV's "120 Minutes". Soundgarden supported "Ultramega OK" with a tour in the United States in the spring of 1989 and a tour in Europe, which began in May 1989 and was the band's first overseas tour. "Ultramega OK" earned the band a Grammy Award nomination for Best Metal Performance in 1990.
After touring in support of "Ultramega OK" the band signed with A&M Records, which caused a rift between Soundgarden and its traditional audience. Thayil said, "In the beginning, our fans came from the punk rock crowd. They abandoned us when they thought we had sold out the punk tenets, getting on a major label and touring with Guns N' Roses. There were fashion issues and social issues, and people thought we no longer belonged to their scene, to their particular sub-culture." The band subsequently began work on its first album for a major label, and personnel difficulties caused a shift in the band's songwriting process, according to Cornell: "At the time Hiro [Yamamoto] had excommunicated himself from the band and there wasn't a free-flowing system as far as music went, so I ended up writing a lot of it." On September 5, 1989, the band released its second album, "Louder Than Love", which saw the band take "a step toward the metal mainstream," according to Steve Huey of Allmusic, describing "a slow, grinding, detuned mountain of Sabbath/Zeppelin riffs and Chris Cornell wailing." Because of some of the lyrics, most notably on "Hands All Over" and "Big Dumb Sex", the band faced various retail and distribution problems upon the album's release. "Louder Than Love" became the band's first album to chart on the "Billboard" 200, peaking at number 108 on the chart in 1990.
A month before touring for "Louder Than Love" commenced, bassist Hiro Yamamoto, who was becoming frustrated that he wasn't contributing much, left to go back to college. He was replaced by Jason Everman, formerly of Nirvana. The band toured North America from December 1989 to March 1990, opening for Voivod, which was supporting their album "Nothingface" tour, with Faith No More also serving as an opening act at the beginning and end of the tour. The band then went on to tour Europe. Bassist Jason Everman was fired immediately after Soundgarden completed its promotional tour for "Louder Than Love" in mid-1990; Thayil said that "Jason just "didn't" work out." "Louder Than Love" spawned the EP "Loudest Love" and the video compilation "Louder Than Live", both released in 1990.
Established lineup, censorship, and rise in popularity (1991–1993).
Bassist Ben Shepherd replaced previous bassist Jason Everman and the new line-up recorded Soundgarden's third album in 1991. Cornell said that Shepherd brought a "fresh and creative" approach to the recording sessions, and the band as a whole said that his knowledge of music and writing skills redefined the band. The resulting album, "Badmotorfinger", was released on October 8, 1991. Steve Huey of Allmusic said that the songwriting on "Badmotorfinger" "takes a quantum leap in focus and consistency." He added, "It's surprisingly cerebral and arty music for a band courting mainstream metal audiences." Thayil suggested that the album's lyrics are "like reading a novel [about] man's conflict with himself and society, or the government, or his family, or the economy, or anything." The first single from "Badmotorfinger", "Jesus Christ Pose", garnered attention when MTV decided to ban its corresponding music video in 1991. Many listeners were outraged by the song and its video, perceiving it as anti-Christian. The band received death threats while on tour in the United Kingdom in support of the album. Cornell explained that the lyrics criticize public figures who use religion (particularly the image of Jesus Christ) to portray themselves as being persecuted. Although overshadowed at the time of its release by the sudden popularity of Nirvana's "Nevermind", the focus of attention brought by "Nevermind" to the Seattle scene helped Soundgarden gain wider attention. The singles "Outshined" and "Rusty Cage" were able to find an audience at alternative rock radio and MTV. "Badmotorfinger" was nominated for a Grammy Award for Best Metal Performance in 1992. The album was among the 100 top selling albums of 1992.
Following the release of "Badmotorfinger", Soundgarden went on a tour in North America that ran from October 1991 to November 1991. Afterward, the band took a slot opening for Guns N' Roses in North America on the band's Use Your Illusion Tour. Soundgarden was personally selected by Guns N' Roses as its opening band. The band took a slot opening for Skid Row in North America in February 1992 on the band's "Slave to the Grind" tour, and then headed to Europe for a month-long headlining theater tour. The band returned for a tour in the United States and subsequently rejoined Guns N' Roses in the summer of 1992 in Europe as part of the Use Your Illusion Tour along with fellow opening act Faith No More. Regarding the time spent opening for Guns N' Roses, Cornell said, "It wasn't a whole lot of fun going out in front of 40,000 people for 35 minutes every day. Most of them hadn't heard our songs and didn't care about them. It was a bizarre thing." The band would go on to play the 1992 Lollapalooza tour with the Red Hot Chili Peppers, Pearl Jam, and Ministry,and the Jim Rose Circus among others. In anticipation of the band's appearance at Lollapalooza, a limited edition of "Badmotorfinger" was released in 1992 with a second disc containing the EP "Satanoscillatemymetallicsonatas" (a palindrome), featuring Soundgarden's cover of Black Sabbath's "Into the Void", titled "Into the Void (Sealth)", which was nominated for a Grammy Award for Best Metal Performance in 1993. The band later released the video compilation "Motorvision", which was filmed at the Paramount Theatre in 1992. The band also made an appearance in the movie "Singles" performing "Birth Ritual". The song appeared on the soundtrack, as did a Cornell solo song, "Seasons".
In 1993, the band contributed the track "Show Me" to the AIDS-Benefit Album No Alternative produced by the Red Hot Organization.
Breakthrough album and mainstream success (1994–1995).
Soundgarden began working on its fourth album after touring in support of "Badmotorfinger". Cornell said that while working on the album, the band members allowed each other more freedom than on past records, and Thayil observed that the band spent a lot more time working on the actual recording of the songs than on previous records. Released on March 8, 1994, "Superunknown" became the band's breakthrough album, driven by the singles "Spoonman", "The Day I Tried to Live", "Black Hole Sun", "My Wave", and "Fell on Black Days"; "Superunknown" debuted at number one on the "Billboard" 200 album chart.
The songs on "Superunknown" captured the creativity and heaviness of the band's earlier works, while showcasing the group's newly evolving style. Lyrically, the album was quite dark and mysterious, and it is often interpreted to be dealing with substance abuse, suicide, and depression. Cornell was inspired by the writings of Sylvia Plath at the time. The album was also more experimental than previous releases, with some songs incorporating Middle-Eastern or Indian music. J. D. Considine of "Rolling Stone" said "Superunknown" "demonstrates far greater range than many bands manage in an entire career." He also stated, "At its best, "Superunknown" offers a more harrowing depiction of alienation and despair than anything on "In Utero"." The music video for "Black Hole Sun" became a hit on MTV and received the award for Best Metal/Hard Rock Video at the 1994 MTV Video Music Awards and in 1995 it received the Clio Award for Alternative Music Video. Soundgarden won two Grammy Awards in 1995; "Black Hole Sun" received the award for Best Hard Rock Performance and "Spoonman" received the award for Best Metal Performance. "Superunknown" was nominated for a Grammy Award for Best Rock Album in 1995. "Superunknown" has been certified five times platinum in the United States and remains Soundgarden's most successful album.
The band began touring in January 1994 in Oceania and Japan, areas where the record came out early and where the band had never toured before. This round of touring ended in February 1994, and then in March 1994 the band moved on to Europe. They began a theater tour of the United States on May 27, 1994, with the opening acts Tad and Eleven. In late 1994, after touring in support of "Superunknown", doctors discovered that Cornell had severely strained his vocal cords, and Soundgarden canceled several shows to avoid causing any permanent damage. Cornell said, "I think we kinda overdid it! We were playing five or six nights a week and my voice pretty much took a beating. Towards the end of the American tour I felt like I could still kinda sing, but I wasn't really giving the band a fair shake. You don't buy a ticket to see some guy croak for two hours! That seemed like kind of a rip off." The band would make up the dates later in 1995. "Superunknown" spawned the EP "Songs from the Superunknown" and the CD-ROM "Alive in the Superunknown", both released in 1995.
"Down on the Upside", internal conflicts and breakup (1996–1997).
Following the worldwide tour in support of "Superunknown", the band members began working on what would become their last studio album for over 15 years. The band chose to produce the record themselves. However, tensions within the group reportedly arose during the sessions, with Thayil and Cornell allegedly clashing over Cornell's desire to shift away from the heavy guitar riffing that had become the band's trademark. Cornell said, "By the time we were finished, it felt like it had been kind of hard, like it was a long, hard haul. But there was stuff we were discovering." The band's fifth album, "Down on the Upside", was released on May 21, 1996. The album was notably less heavy than the group's preceding albums, and marked a further departure from the band's grunge roots; Soundgarden explained at the time that it wanted to experiment with other sounds, which included acoustic instrumentation: David Browne of "Entertainment Weekly" said, "Few bands since Led Zeppelin have so crisply mixed instruments both acoustic and electric." The overall mood of the album's lyrics is less dark than on previous Soundgarden albums, with Cornell describing some songs as "self-affirming." The album spawned several singles, including "Pretty Noose", "Burden in My Hand", and "Blow Up the Outside World". "Pretty Noose" was nominated for a Grammy Award for Best Hard Rock Performance in 1997. Despite favorable reviews and modest sales, the album did not match the sales or praise of "Superunknown".
The band took a slot on the 1996 Lollapalooza tour with Metallica, who had insisted on Soundgarden's appearance on the tour. After Lollapalooza, the band embarked on a worldwide tour, and already-existing tensions increased during that tour. When asked whether the band hated touring, Cornell replied "We really enjoy it to a point, and then it gets tedious, because it becomes repetitious. You feel like fans have paid their money and they expect you to come out and play them your songs like the first time you ever played them. That's the point where we hate touring." At the tour's final stop in Honolulu, Hawaii on February 9, 1997, Shepherd threw his bass into the air in frustration after suffering equipment failure, and subsequently stormed off the stage. The band retreated, with Cornell returning to conclude the show with a solo encore. On April 9, 1997, the band announced it was disbanding. Thayil said, "It was pretty obvious from everybody's general attitude over the course of the previous half year that there was some dissatisfaction." Cameron later said that Soundgarden was "eaten up by the business." Soundgarden released a greatest hits collection entitled "A-Sides" on November 4, 1997. The album was composed of 17 songs, including the previously-unreleased "Bleed Together", which had been recorded during the "Down on the Upside" recording sessions.
Post-breakup activities (1998–2009).
Cornell released a solo album in September 1999, entitled "Euphoria Morning", which featured Matt Cameron on the track "Disappearing One". Later, in 2001, Cornell formed the platinum-selling supergroup Audioslave with Tom Morello, Tim Commerford and Brad Wilk, then-former members of Rage Against the Machine, which recorded three albums (US:Triple-Platinum) "Audioslave" (2002), (US:Platinum) "Out of Exile" (2005), and (US:Gold) "Revelations" (2006)). Cornell left Audioslave in early 2007, resulting in the band's break-up. His second solo album, "Carry On", was released in June 2007 and his third solo album, "Scream", produced by Timbaland, was released in March 2009, both to mixed commercial and critical success. In 2009 Cornell also provided the vocals for "Promise" on Slash's debut solo album "Slash".
Thayil joined forces with former Dead Kennedys singer Jello Biafra and former Nirvana bassist Krist Novoselic and drummer Gina Mainwal for one show, performing as The No WTO Combo during the WTO ministerial conference in Seattle on December 1, 1999. Thayil later contributed guitar tracks to Steve Fisk's 2001 album, "999 Levels of Undo", as well as Dave Grohl's 2004 side-project album, "Probot". In 2006, Thayil played guitar on the album "Altar", the collaboration between the bands Sunn O))) and Boris.
Cameron initially turned his efforts to his side-project Wellwater Conspiracy, to which both Shepherd and Thayil have contributed. He then worked briefly with The Smashing Pumpkins on the band's 1998 album, "Adore". In 1998, he stepped in on drums for Pearl Jam's Yield Tour following Jack Irons's health problems, and subsequently joined Pearl Jam as an official member; he has recorded five albums as the band's drummer ("Binaural" (2000), "Riot Act" (2002), "Pearl Jam" (2006), "Backspacer" (2009) and "Lightning Bolt" (2013)). Cameron also played percussion on Geddy Lee's album "My Favourite Headache".
Shepherd was the singer on Wellwater Conspiracy's 1997 debut studio album, "Declaration of Conformity", but left the band in 1998. He has toured with Mark Lanegan and played bass on two of Lanegan's albums, "I'll Take Care of You" (1999) and "Field Songs" (2001). Shepherd and Cameron lent a hand with recording Tony Iommi's album "IOMMI" (2000); they were part of the side-project band Hater while they were members of Soundgarden and in 2005 Shepherd released the band's long-delayed second album, "The 2nd".
In a July 2009 interview with "Rolling Stone", Cornell shot down rumors of a reunion, saying that conversations between the band members had been limited to discussion about the release of a box set or B-sides album of Soundgarden rarities, and that there had been no discussion of a reunion at all.
On October 6, 2009, all the members of Soundgarden attended Night 3 of Pearl Jam's four-night stand at the Gibson Amphitheatre in Universal City, CA. During an encore, Temple of the Dog reunited for the first time since Pearl Jam's show at the Santa Barbara Bowl on October 28, 2003. Chris Cornell joined the band to sing "Hunger Strike". It was the first public appearance of Soundgarden together since their breakup in April 1997. Consequently, rumors of an impending reunion were circulating on the internet.
Reunion, "Telephantasm" and "King Animal" (2010–2013).
On January 1, 2010, Cornell alluded to a Soundgarden reunion via his Twitter, writing: "The 12-year break is over and school is back in session. Sign up now. Knights of the Soundtable ride again!" The message linked to a website that features a picture of the group performing live and a place for fans to enter their e-mail addresses to get updates on the reunion. Entering that information unlocks an archival video for the song "Get on the Snake", from Soundgarden's second studio album, 1989's "Louder Than Love". On March 1, 2010, Soundgarden announced to the people who signed their e-mail subscribers that they are re-releasing an old single "Hunted Down" with the song "Nothing to Say" on a 7-inch vinyl released on April 17 only at Record Store Day. Also, they released "Spoonman" live at the Del Mar Fairgrounds in San Diego, California from 1996. Soundgarden played its first show since 1997 on April 16 at the Showbox at the Market in the band's hometown of Seattle. The band headlined Lollapalooza on August 8.
"Telephantasm: A Retrospective", a new Soundgarden compilation album, was packaged with initial shipments of the "" video game and released on September 28, 2010. This is the first time a retail music CD has been packaged with a video game, and is one week before the same CD is available in stores on October 5, 2010. An expanded version of "Telephantasm" consisting of two CDs and one DVD is currently available for sale. A previously unreleased Soundgarden song—"Black Rain"—debuted on the "Guitar Hero" video game and appears on the compilation album. The compilation album achieved platinum certification status after its first day of retail availability. "Black Rain" hit rock radio stations on August 10, 2010. It became the band's first single since 1997. In November 2010, Soundgarden was the second musical guest on the show "Conan", making it their first television appearance in 13 years, and issued a 7-inch vinyl, "The Telephantasm", for Black Friday Record Store Day. In March 2011, Soundgarden released their first live album, "Live on I-5".
In February 2011 it was announced on Soundgarden's homepage that they had started recording a new album. On March 1, 2011, Chris Cornell confirmed that Adam Kasper would produce the new album. Four days later, the band stated it would consist of material that was "90 percent new" and the rest consisting of updated versions of older ideas. They also noted that they had 12 to 14 songs that were "kind of ready to go". Although Cameron claimed the album would be released in 2011, the recording was prolonged as Thayil said that "the more we enjoy it, the more our fans should end up enjoying it.". Thayil also reported that some songs sound "similar in a sense to "Down on the Upside"" and that the album would be "picking up where we left off. There are some heavy moments, and there are some fast songs." The next day, Cornell reported that the new album would not be released until the spring of 2012.
In April 2011, Soundgarden announced a summer tour consisting of four dates in July, and was also headliner for Voodoo Experience at City Park in New Orleans Halloween weekend 2011. It was announced in March 2012 via the band's official Facebook page that they would be including a new song on the soundtrack of the upcoming movie "The Avengers", based on the franchise by Marvel Comics. The song was titled "Live to Rise" and marked the first newly recorded song that the band have released since reforming in 2010. "Live to Rise" was released as a free download on iTunes April 17. Also in March it was announced that Soundgarden would headline the Friday night of the Hard Rock Calling Festival the following July. In April, Soundgarden announced the release of a box set titled "Classic Album Selection" for Europe, containing all of their studio albums (except for "Ultramega OK") and live album "Live on I-5". On May 5, just before The Offspring began playing their set, the band appeared as a special guest at the 20th annual KROQ Weenie Roast in Irvine, California. Later that month, Soundgarden told Rolling Stone they were eyeing an October release for their new album. That June, the band appeared at Download Festival in Donington, England. The band released "Been Away Too Long", the first single from their new album "King Animal" on September 27. "King Animal" was released on November 13, 2012. The band released a video for "By Crooked Steps", directed by Dave Grohl, in early 2013. "Halfway There" was the third single to be released from the album.
"Echo of Miles..." and next album (2014–present).
On November 15, 2013, drummer Matt Cameron announced that he would not be touring with Soundgarden in 2014, due to prior commitments promoting Pearl Jam's album "Lightning Bolt". On March 16, 2014, it was announced that Soundgarden and Nine Inch Nails, along with opening act Death Grips, were going to tour together in North America. On March 27, 2014, former Pearl Jam drummer Matt Chamberlain replaced Cameron for live shows in South America.
On October 28, 2014, Soundgarden announced that they would release the 3CD compilation box set, "", on November 24. The set is a collection of rarities, live tracks, and unreleased material spanning the group's history. It includes previously released songs, such as "Live to Rise", "Black Rain", "Birth Ritual", and others, as well as a newly recorded rendition of a song from the band's pre-Matt Cameron 1985 demo, "The Storm", now simply titled "Storm", which was produced by Jack Endino. One day prior to the announcement, on October 27, the band posted a copy of "Storm" to YouTube unannounced.
Kim Thayil has mentioned in several interviews that it seems highly likely that the band will start working on material for a new album in 2015.
Musical style and influences.
Soundgarden were pioneers of the grunge music genre, which mixed elements of punk rock and metal into a dirty, aggressive sound. "Soundgarden are quite good…" remarked Black Sabbath's Tony Iommi, "It's very much like the same sort of stuff that we would have done." Soundgarden's sound during the early years of the Seattle grunge scene has been described as consisting of "gnarled neo-Zeppelinisms." The influence of Led Zeppelin was evident, with "Q" magazine noting that Soundgarden were "in thrall to '70s rock, but contemptuous of the genre's overt sexism and machismo." According to Sub Pop, the band had "a hunky lead singer and fused Led Zeppelin and the Butthole Surfers." The Butthole Surfers' mix of punk, heavy metal and noise rock was a major influence on the early work of Soundgarden.
Black Sabbath also had a huge impact on the band's sound, especially on the guitar riffs and tunings. Joel McIver stated: "Soundgarden are one of the bands I've heard closest to the original Sabbath sound". Soundgarden, like other early grunge bands, were also influenced by British post-punk bands such as Gang of Four and Bauhaus which were popular in the early 1980s Seattle scene. Cornell himself said: "When Soundgarden formed we were post-punk - pretty quirky. Then somehow we found this neo-Sabbath psychedelic rock that fitted well with who we were". Thayil described the band's sound as a "Sabbath-influenced punk".
Soundgarden broadened its musical range with its later releases. By 1994’s "Superunknown", the band began to incorporate more psychedelic influences into its music. As a member of Soundgarden, Cornell became known for his wide vocal range and his dark, existentialist lyrics.
Soundgarden often uses alternative tunings in its songs. Many Soundgarden songs are performed in drop D tuning, including "Jesus Christ Pose", "Outshined", "Spoonman", "Black Hole Sun" and "Black Rain". The E strings of the instruments were at times tuned even lower, such as on "Rusty Cage", where the lower E is tuned all the way down to B. Some songs use more unorthodox tunings: "Been Away Too Long", "My Wave" and "The Day I Tried to Live" are all in a E–E–B–B–B–B tuning and "Burden in My Hand", "Head Down" and "Pretty Noose" in a tuning of C-G-C-G-G-E".
Soundgarden also uses unorthodox time signatures; while such songs as "Jesus Christ Pose", "4th of July", and "Blow Up the Outside World" are in typical 4/4 time, "Outshined" is in 7/4, "My Wave" is in 5/4 and 4/4, "He Didn't" is in 5/4 and 6/4, "Black Hole Sun" is in 4/4 and 9/8, "The Day I Tried to Live" is in 15/8 for its verses and switches to 4/4 for the second half of its choruses. "Fell on Black Days" and "Somewhere" are in 6/4, "Never the Machine Forever" and "Black Rain" are in 9/8, "Beyond the Wheel", "Get on the Snake" and "New Damage" are in 9/4, "Face Pollution" uses 9/8 and 6/4, "Rusty Cage" is in 4/4, 7/4, and 19/8, "Ugly Truth" is in 4/4 and 6/8, "Limo Wreck" alternates between 12/8, 15/8, 9/8, and 6/8, "Half" is in 5/8 with a measure of 11/16 before a 4/4 section, and "Spoonman" alternates between 7/4 verses and 4/4 choruses with a section in 6/4.
Thayil has said that Soundgarden usually did not consider the time signature of a song until after the band had written it, and said that the use of odd meters was "a total accident." Thayil also used the meters as an example of the band's anti-commercial stance, saying that if Soundgarden "were in the business of hit singles, we'd at least write songs in 4/4 so you could dance to them."
Legacy.
Soundgarden was one of the early bands of the 1980s Seattle music scene and is regarded as being one of the originators of the genre later known as grunge. The development of the Seattle independent record label Sub Pop is tied closely to Soundgarden, since Sub Pop co-founder Jonathan Poneman funded Soundgarden's early releases, and the band's success led to the expansion of Sub Pop as a serious record label. Nirvana frontman Kurt Cobain was a fan of Soundgarden's early music, and reportedly Soundgarden's involvement with Sub Pop influenced Cobain to sign Nirvana with the label. Soundgarden was the first grunge band to sign to a major label when the band joined the roster of A&M Records in 1989. Soundgarden, however, did not achieve initial success, and only with successive album releases did the band meet with increased sales and wider attention. Bassist Ben Shepherd has not been receptive to the grunge label, saying in a 2013 interview “That’s just marketing. It’s called rock and roll, or it’s called punk rock or whatever. We never were Grunge, we were just a band from Seattle." They were ranked No. 14 on VH1's "100 Greatest Artists of Hard Rock".
Regarding Soundgarden's legacy, in a 2007 interview Cornell said, 
I think, and this is now with some distance in listening to the records, but on the outside looking in with all earnestness I think Soundgarden made the best records out of that scene. I think we were the most daring and experimental and genre pushing really and I'm really proud of it. And I guess that's why I have trepidation about the idea of reforming. I don't know what it would mean. I guess I just have this image of who we were and I had probably a lot of anxiety during the period of being Soundgarden, as we all did, that it was a responsibility and it was an important band of music and we didn't want to mess it up and we managed to not, which I felt is a great achievement.
Soundgarden has been praised for its technical musical ability and the expansion of its sound as the band's career progressed. "Heavy yet ethereal, powerful yet always-in-control, Soundgarden's music was a study in contrasts," said Henry Wilson of "Hit Parader". Wilson proclaimed the band's music as "a brilliant display of technical proficiency tempered by heart-felt emotion." Soundgarden is one of the bands credited with the development of the genre of alternative metal, with Stephen Thomas Erlewine of AllMusic stating that "Soundgarden made a place for heavy metal in alternative rock." Ben Ratliff of "Rolling Stone" defined Soundgarden as the "standard-bearers of stoner rock" during the 1990s. The band inspired and influenced a number of bands, such as Between the Buried and Me and The Dillinger Escape Plan.
Awards and nominations.
Grammy Awards
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="40322" url="http://en.wikipedia.org/wiki?curid=40322" title="Theodore Beza">
Theodore Beza

Theodore Beza (Latin: "Theodorus Beza"; French: "Théodore de Bèze" or "de Besze"; June 24, 1519 – October 13, 1605) was a French Protestant Christian theologian and scholar who played an important role in the Reformation. A member of the monarchomaque movement who opposed absolute monarchy, he was a disciple of John Calvin and lived most of his life in Switzerland.
Biography.
Early life.
Theodore Beza was born at Vezelay, in Burgundy, France. His father, Pierre de Beze, royal governor of Vezelay, descended from a Burgundian family of distinction; his mother, Marie Bourdelot, was known for her generosity. Beza's father had two brothers; Nicholas, who was member of Parliament at Paris; and Claude, who was abbot of the Cistercian monastery Froimont in the diocese of Beauvais.
Nicholas, who was unmarried, during a visit to Vezelay was so pleased with Theodore that, with the permission of his parents, he took him to Paris to educate him there. From Paris, Theodore was sent to Orléans in December 1528 to receive instruction from the famous German teacher Melchior Wolmar. He was received into Wolmar's house, and the day on which this took place was afterward celebrated as a second birthday.
Young Beza soon followed his teacher to Bourges, where the latter was called by the duchess Margaret of Angoulême, sister of Francis I. At the time, Bourges was the focus of the Reformation movement in France. In 1534, after Francis I issued his edict against ecclesiastical innovations, Wolmar returned to Germany. Beza, in accordance with the wish of his father, went back to Orléans to study law, and spent four years there (1535–39). The pursuit of law had little attraction for him; he enjoyed more the reading of the ancient classics, especially Ovid, Catullus, and Tibullus.
He received the degree of licentiate in law August 11, 1539, and, as his father desired, went to Paris, where he began to practice. To support him, his relatives had obtained for him two benefices, the proceeds of which amounted to 700 golden crowns a year; and his uncle had promised to make him his successor.
Beza spent two years in Paris and gained a prominent position in literary circles. To escape the many temptations to which he was exposed, with the knowledge of two friends, he became engaged in the year 1544 to a young girl of humble descent, Claudine Denoese, promising to publicly marry her as soon as his circumstances would allow it.
In 1548 he published a collection of Latin poetry, "Juvenilia", which made him famous, and he was widely considered one of the best writers of Latin poetry of his time. Some cautioned against reading biographical details in his writings. Philip Schaff argued that it was a mistake to "read between his lines what he never intended to put there" or to imagine "offences of which he was not guilty even in thought."
Shortly after the publication of his book, he fell ill and his illness, it is reported, revealed to him his spiritual needs. Gradually he came to accept salvation in Christ, which lifted his spirits. He then resolved to sever his connections of the time, and went to Geneva, the French city of refuge for Evangelicals (adherents of the Reformation movement), where he arrived with Claudine on October 23, 1548.
Teacher at Lausanne.
He was received by John Calvin, who had met him already in Wolmar's house, and was married in the church. Beza was at a loss for immediate occupation, so he went to Tübingen to see his former teacher Wolmar. On his way home he visited Pierre Viret at Lausanne, who brought about his appointment as professor of Greek at the academy there, in November 1549.
Beza found time to write a Biblical drama, "Abraham Sacrifiant ", in which he contrasted Catholicism with Protestantism, and the work was well received. In June, 1551, he added a few psalms to the French version of the Psalms begun by Clément Marot, which was also very successful.
About the same time he published "Passavantius, ", a satire directed against Pierre Lizet, the former president of the Parliament of Paris, and principal originator of the "fiery chamber" ("chambre ardente"), who, at the time (1551) was abbot of St. Victor near Paris and publishing a number of polemical writings.
Of a more serious character were two controversies in which Beza was involved at this time. The first concerned the doctrine of predestination and the controversy of Calvin with Jerome Hermes Bolsec. The second referred to the burning of Michael Servetus at Geneva Oct. 27, 1553. In defense of Calvin and the Genevan magistrates, Beza published in 1554 the work "De haereticis a civili magistratu puniendis" (translated into French in 1560).
Journeys on behalf of the Protestants.
In 1557, Beza took a special interest in the Waldensians of Piedmont, who were being harassed by the French government. On their behalf, he went with William Farel to Bern, Zürich, Basel, and Schaffhausen, then to Strasburg, Mömpelgard, Baden, and Göppingen. In Baden and Göppingen, Beza and Farel made a declaration concerning the Waldensians' views on the sacrament on May 14, 1557. The written declaration clearly stated their position and was well received by the Lutheran theologians, but was strongly disapproved of in Bern and Zurich.
In the autumn of 1558, Beza undertook a second journey with Farel to Worms by way of Strasburg in the hopes of bringing about an intercession by the Evangelical princes of the empire in favor of the persecuted brethren at Paris. 
With Melanchthon and other theologians then assembled at the Colloquy of Worms, Beza proposed a union of all Protestant Christians, but the proposal was decidedly denied by Zurich and Bern.
False reports reached the German princes that the hostilities against the Huguenots in France had ceased and no embassy was sent to the court of France. 
As a result, Beza undertook another journey with Farel, Johannes Buddaeus, and Gaspard Carmel to Strasburg and Frankfort, where the sending of an embassy to Paris was resolved upon.
Settles in Geneva.
Upon his return to Lausanne, Beza was greatly disturbed. In union with many ministers and professors in city and country, Viret at last thought of establishing a consistory and of introducing a church discipline which should apply excommunication especially at the celebration of the communion. 
But the Bernese, then in control of Lausanne, would have no Calvinistic church government. This caused many difficulties, and Beza thought it best in 1558, to settle at Geneva.
Here he was given chair of Greek in the newly established academy, and after Calvin's death also that of theology. 
He was also obliged to preach.
He completed the revision of Pierre Olivetan's translation of the New Testament, begun some years before. 
In 1559, he undertook another journey in the interest of the Huguenots, this time to Heidelberg. At about the same time, he had to defend Calvin against Joachim Westphal in Hamburg and Tilemann Heshusius.
More important than this polemical activity was Beza's statement of his own confession. It was originally prepared for his father in justification of his actions and published in revised form to promote Evangelical knowledge among Beza's countrymen. It was printed in Latin in 1560 with a dedication to Wolmar. An English translation was published at London 1563, 1572, and 1585. Translations into German, Dutch, and Italian were also issued.
Events of 1560–63.
In the mean time, things took such shape in France that the happiest future for Protestantism seemed possible. King Anthony of Navarre, yielding to the urgent requests of Evangelical noblemen, declared his willingness to listen to a prominent teacher of the Church. Beza, a French nobleman and head of the academy in the metropolis of French Protestantism, was invited to Castle Nerac, but he could not plant the seed of Evangelical faith in the heart of the king.
In the following year, 1561, Beza represented the Evangelicals at the Colloquy of Poissy, and in an eloquent manner defended the principles of the Evangelical faith. 
The colloquy was without result, but Beza as the head and advocate of all Reformed congregations of France was revered and hated at the same time. 
The queen insisted upon another colloquy, which was opened at St. Germain Jan. 28, 1562, eleven days after the proclamation of the famous January edict, which granted important privileges to those of the Reformed faith. 
But the colloquy was broken off when it became evident that the Catholic party was preparing (after the Massacre of Vassy, on March 1) to overthrow Protestantism.
Beza hastily issued a circular letter (March 25) to all Reformed congregations of the empire, and went to Orléans with the Huguenot leader Conde and his troops. It was necessary to proceed quickly and energetically. But there were neither soldiers nor money. At the request of Conde, Beza visited all Huguenot cities to obtain both. He also wrote a manifesto in which he argued the justice of the Reformed cause. As one of the messengers to collect soldiers and money among his coreligionists, Beza was appointed to visit England, Germany, and Switzerland. He went to Strasburg and Basel, but met with failure. He then returned to Geneva, which he reached September 4. He had hardly been there fourteen days when he was called once more to Orléans by D'Andelot. The campaign was becoming more successful; but the publication of the unfortunate edict of pacification which Conde accepted (Mar. 12,1563) filled Beza and all Protestant France with horror.
Calvin's successor.
For twenty-two months Beza had been absent from Geneva, and the interests of school and Church there and especially the condition of Calvin made it necessary for him to return. For there was no one to take the place of Calvin, who was sick and unable longer to bear the burden resting on him. Calvin and Beza arranged to perform their duties jointly in alternate weeks, but the death of Calvin occurred soon afterward (May 27, 1564). As a matter of course Beza was his successor.
Until 1580, Beza was not only moderator of the Company of Pastors, but also the real soul of the great institution of learning at Geneva which Calvin had founded in 1559, consisting of a gymnasium and an academy. 
As long as he lived, Beza was interested in higher education. 
The Protestant youth for nearly forty years thronged his lecture-room to hear his theological lectures, in which he expounded the purest Calvinistic orthodoxy. 
As a counselor he was listened to by both magistrates and pastors. 
Geneva is indebted to him for the founding of a law school in which François Hotman, Jules Pacius, and Denys Godefroy, the most eminent jurists of the century, lectured in turn (cf. Charles Borgeaud, "L'Academie de Calvin, " Geneva, 1900).
Course of events after 1564.
As Calvin's successor, Beza was very successful, not only in carrying on his work but also in giving peace to the Church at Geneva. The magistrates had fully appropriated the ideas of Calvin, and the direction of spiritual affairs, the organs of which were the "ministers of the word" and "the consistory", was founded on a solid basis. No doctrinal controversy arose after 1564. The discussions concerned questions of a practical, social, or ecclesiastical nature, such as the supremacy of the magistrates over the pastors, freedom in preaching, and the obligation of the pastors to submit to the majority of the Company of Pastors.
Beza obtruded his will in no way upon his associates, and took no harsh measures against injudicious or hot-headed colleagues, though sometimes he took their cases in hand and acted as mediator; and yet he often experienced an opposition so extreme that he threatened to resign. Although he was inclined to take the part of the magistrates, he knew how to defend the rights and independence of the spiritual power when occasion arose, without, however, conceding to it such a preponderating influence as did Calvin.
Beza did not believe it wise for the Company of Pastors to have a permanent head. He convinced the Company to petition the Small Council to have limited terms for the position of moderator. In 1580 the Council agreed to a system of weekly rotating presidency.
His activity was great. He mediated between the "compagnie" and the magistracy; the latter continually asked his advice even in political questions. He corresponded with all the leaders of the Reformed party in Europe. After the St. Bartholomew's Day Massacre (1572), he used his influence to give to the refugees a hospitable reception at Geneva.
In 1574, he wrote his "De jure magistratuum" (Right of Magistrates), in which he emphatically protested against tyranny in religious matters, and affirmed that it is legitimate for a people to oppose an unworthy magistracy in a practical manner and if necessary to use weapons and depose them.
To sum up: Without being a great dogmatician like his master, nor a creative genius in the ecclesiastical realm, Beza had qualities which made him famous as humanist, exegete, orator, and leader in religious and political affairs, and qualified him to be the guide of the Calvinists in all Europe. In the various controversies into which he was drawn, Beza often showed an excess of irritation and intolerance, from which Bernardino Ochino, pastor of the Italian congregation at Zurich (on account of a treatise which contained some objectionable points on polygamy), and Sebastian Castellio at Basel (on account of his Latin and French translations of the Bible) had especially to suffer.
With Reformed France, Beza continued to maintain the closest relations. He was the moderator of the general synod which met in April, 1571, at La Rochelle and decided not to abolish church discipline or to acknowledge the civil government as head of the Church, as the Paris minister Jean Morel and the philosopher Pierre Ramus demanded; it also decided to confirm anew the Calvinistic doctrine of the Lord's Supper (by the expression: "substance of the body of Christ") against Zwinglianism, which caused a very unpleasant discussion between Beza and Ramus and Heinrich Bullinger.
In the following year (May, 1572) he took an important part in the national synod at Nîmes. He was also interested in the controversies which concerned the Augsburg Confession in Germany, especially after 1564, on the doctrine of the Person of Christ and the sacrament, and published several works against Westphal, Hesshusen, Selnecker, Johannes Brenz, and Jakob Andrea. This made him, especially after 1571, hated by all those who adhered to Lutheranism in opposition to Melanchthon.
The Colloquy of Mömpelgard.
The last polemical conflict of importance Beza encountered from the Lutherans was at the Colloquy of Mömpelgard, Mar. 14-27, 1586, to which he had been invited by the Lutheran Count Frederick of Württemberg at the wish of the French-speaking and Reformed residents as well as by French noblemen who had fled to Mömpelgard. As a matter of course the intended union which was the purpose of the colloquy was not brought about; nevertheless it called forth serious developments within the Reformed Church.
When the edition of the acts of the colloquy, as prepared by Jakob Andrea, was published, Samuel Huber, of Burg near Bern, who belonged to the Lutheranizing faction of the Swiss clergy, took so great offense at the supralapsarian doctrine of predestination propounded at Mömpelgard by Beza and Musculus that he felt it to be his duty to denounce Musculus to the magistrates of Bern as an innovator in doctrine. To adjust the matter, the magistrates arranged a colloquy between Huber and Musculus (September 2, 1587), in which the former represented the universalism, the latter the particularism, of grace.
As the colloquy was resultless, a debate was arranged at Bern, Apr. 15-18, 1588, at which the defense of the accepted system of doctrine was at the start put into Beza's hands. The three delegates of the Helvetic cantons who presided at the debate declared in the end that Beza had substantiated the teaching propounded at Mömpelgard as the orthodox one, and Huber was dismissed from his office.
Last days.
After that time Beza's activity was confined more and more to the affairs of his home. His faithful wife Claudine had died childless in 1588, a few days before he went to the Bern Disputation. Forty years they had lived happily together. He contracted, on the advice of his friends, a second marriage with Catharina del Piano, a Genoese widow, in order to have a helpmate in his declining years. Up to his sixty-fifth year he enjoyed excellent health, but after that a gradual sinking of his vitality became perceptible. He was active in teaching until January 1597.
The saddest experience in his old days was the conversion of King Henry IV to Catholicism, in spite of his most earnest exhortations (1593). Strange to say, in 1596 the report was spread by the Jesuits in Germany, France, England, and Italy that Beza and the Church of Geneva had returned into the bosom of Rome, and Beza replied in a satire that revealed the possession still of his old fire of thought and vigor of expression.
He died in Geneva. He was not buried, like Calvin, in the general cemetery, Plain-Palais (for the Savoyards had threatened to abduct his body to Rome), but at the direction of the magistrates, in the monastery of St. Pierre.
Literary works.
Humanistic and historical writings.
In Beza's literary activity as well as in his life, distinction must be made between the period of the humanist (which ended with the publication of his "Juvenilia") and that of the ecclesiastic. Combining his pastoral and literary gifts, Beza wrote the first drama produced in French, Abrahm Sacrifiant; a play that is an antecedent to the work of Racine and is still occasionally produced today. Later productions like the humanistic, biting, satirical "Passavantius" and his "Complainte de Messire Pierre Lizet..." prove that in later years he occasionally went back to his first love. In his old age he published his "Cato censorius " (1591), and revised his "Poemata", from which he purged juvenile eccentricities.
Of his historiographical works, aside from his "Icones" (1580), which have only an iconographical value, mention may be made of the famous "Histoire ecclesiastique des Eglises reformes au Royaume de France" (1580), and his biography of Calvin, with which must be named his edition of Calvin's "Epistolae et responsa" (1575).
Theological works.
But all these humanistic and historical studies are surpassed by his theological productions (contained in "Tractationes theologicae"). In these Beza appears the perfect pupil or the "alter ego " of Calvin. His view of life is deterministic and the basis of his religious thinking is the predestinate recognition of the necessity of all temporal existence as an effect of the absolute, eternal, and immutable will of God, so that even the fall of the human race appears to him essential to the divine plan of the world. Beza, in tabular form, thoroughly elucidates the religious views which emanated from a fundamental supralapsarian mode of thought. This he added to his highly instructive treatise "Summa totius Christianismi."
Beza's "De vera excommunicatione et Christiano presbyterio" (1590), written as a response to Thomas Erastus's "Explicatio gravissimae quaestionis utrum excommunicatio" (1589) contributed an important defense of the right of ecclesiastical authorities (rather than civil authorities) to excommunicate.
Beza's Greek New Testament.
Of no less importance are the contributions of Beza to Biblical scholarship. In 1565 he issued an edition of the Greek New Testament, accompanied in parallel columns by the text of the Vulgate and a translation of his own (already published as early as 1556). Annotations were added, also previously published, but now he greatly enriched and enlarged them.
In the preparation of this edition of the Greek text, but much more in the preparation of the second edition which he brought out in 1582, Beza may have availed himself of the help of two very valuable manuscripts. One is known as the "Codex Bezae" or "Cantabrigensis, " and was later presented by Beza to the University of Cambridge; the second is the "Codex Claromontanus", which Beza had found in Clermont (now in the National Library at Paris).
It was not, however, to these sources that Beza was chiefly indebted, but rather to the previous edition of the eminent Robert Estienne (1550), itself based in great measure upon one of the later editions of Erasmus. Beza's labors in this direction were exceedingly helpful to those who came after. The same thing may be asserted with equal truth of his Latin version and of the copious notes with which it was accompanied. The former is said to have been published over a hundred times.
Although some contend that Beza's view of the doctrine of predestination exercised an overly dominant influence upon his interpretation of the Scriptures, there is no question that he added much to a clear understanding of the New Testament.

</doc>
<doc id="40323" url="http://en.wikipedia.org/wiki?curid=40323" title="Inertial confinement fusion">
Inertial confinement fusion

Inertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.
To compress and heat the fuel, energy is delivered to the outer layer of the target using high-energy beams of laser light, electrons or ions, although for a variety of reasons, almost all ICF devices as of 2015[ [update]] have used lasers. The heated outer layer explodes outward, producing a reaction force against the remainder of the target, accelerating it inwards, compressing the target. This process is designed to create shock waves that travel inward through the target. A sufficiently powerful set of shock waves can compress and heat the fuel at the center so much that fusion reactions occur.
The energy released by these reactions will then heat the surrounding fuel, and if the heating is strong enough this could also begin to undergo fusion. The aim of ICF is to produce a condition known as "ignition", where this heating process causes a chain reaction that burns a significant portion of the fuel. Typical fuel pellets are about the size of a pinhead and contain around 10 milligrams of fuel: in practice, only a small proportion of this fuel will undergo fusion, but if all this fuel were consumed it would release the energy equivalent to burning a barrel of oil.
ICF is one of two major branches of fusion energy research, the other being magnetic confinement fusion. When it was first proposed in the early 1970s, ICF appeared to be a practical approach to fusion power production and the field flourished. Experiments during the 1970s and '80s demonstrated that the efficiency of these devices was much lower than expected, and reaching ignition would not be easy. Throughout the 1980s and '90s, many experiments were conducted in order to understand the complex interaction of high-intensity laser light and plasma. These led to the design of newer machines, much larger, that would finally reach ignition energies.
The largest operational ICF experiment is the National Ignition Facility (NIF) in the US, designed using all of the decades-long experience of earlier experiments. Like those earlier experiments, however, NIF has failed to reach ignition and is, as of 2013, generating about 1/3rd of the required energy levels. As of October 7, 2013, this facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is a major step forward. A similar large-scale device in France, Laser Mégajoule, has not begun operation.
Description.
Basic fusion.
Fusion reactions combine lighter atoms, such as hydrogen, together to form larger ones. Generally the reactions take place at such high temperatures that the atoms have been ionized, their electrons stripped off by the heat; thus, fusion is typically described in terms of "nuclei" instead of "atoms".
Nuclei are positively charged, and thus repel each other due to the electrostatic force. Overcoming this repulsion costs a considerable amount of energy, which is known as the "Coulomb barrier" or "fusion barrier energy". Generally, less energy will be needed to cause lighter nuclei to fuse, as they have less charge and thus a lower barrier energy, and when they do fuse, more energy will be released. As the mass of the nuclei increase, there is a point where the reaction no longer gives off net energy—the energy needed to overcome the energy barrier is greater than the energy released in the resulting fusion reaction. The crossover point is iron, Fe56.
The best fuel from an energy perspective is a one to one mix of deuterium and tritium; both are heavy isotopes of hydrogen. The D-T (deuterium & tritium) mix has a low barrier because of its high ratio of neutrons to protons. The presence of neutral neutrons in the nuclei helps pull them together via the nuclear force, while the presence of positively charged protons pushes the nuclei apart via electrostatic force. Tritium has one of the highest ratios of neutrons to protons of any stable or moderately unstable nuclide—two neutrons and one proton. Adding protons or removing neutrons increases the energy barrier.
A mix of D-T at standard conditions does not undergo fusion; the nuclei must be forced together before the nuclear force can pull them together into stable collections. Even in the hot, dense center of the sun, the average proton will exist for billions of years before it fuses. For practical fusion power systems, the rate must be dramatically increased; heated to tens of millions of degrees, and/or compressed to immense pressures. The temperature and pressure required for any particular fuel to fuse is known as the Lawson criterion. These conditions have been known since the 1950s when the first H-bombs were built. To meet the Lawson Criterion is extremely difficult on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.
ICF mechanism of action.
In a hydrogen bomb, the fusion fuel is compressed and heated with a separate fission bomb (see Teller-Ulam design). A variety of mechanisms transfers the energy of the fission "trigger"'s explosion into the fusion fuel. The requirement of a fission bomb makes the method impractical for power generation. Not only would the triggers be prohibitively expensive to produce, but there is a minimum size that such a bomb can be built, defined roughly by the critical mass of the plutonium fuel used. Generally it seems difficult to build nuclear devices smaller than about 1 kiloton in yield, which would make it a difficult engineering problem to extract power from the resulting explosions.
As the explosion size is scaled down, so too is the amount of energy needed to start the reaction off. Studies from the late 1950s and early 1960s suggested that scaling down into the megajoule energy range would require energy levels that could be delivered by any number of means. This led to the idea of using a device that would "beam" the energy at the fusion fuel, ensuring mechanical separation. By the mid-1960s, it appeared that the laser would develop to the point where the required energy levels would be available.
Generally ICF systems use a single laser, the "driver", whose beam is split up into a number of beams which are subsequently individually amplified by a trillion times or more. These are sent into the reaction chamber (called a target chamber) by a number of mirrors, positioned in order to illuminate the target evenly over its whole surface. The heat applied by the driver causes the outer layer of the target to explode, just as the outer layers of an H-bomb's fuel cylinder do when illuminated by the X-rays of the fission device.
The material exploding off the surface causes the remaining material on the inside to be driven inwards with great force, eventually collapsing into a tiny near-spherical ball. In modern ICF devices the density of the resulting fuel mixture is as much as one-hundred times the density of lead, around 1000 g/cm3. This density is not high enough to create any useful rate of fusion on its own. However, during the collapse of the fuel, shock waves also form and travel into the center of the fuel at high speed. When they meet their counterparts moving in from the other sides of the fuel in the center, the density of that spot is raised much further.
Given the correct conditions, the fusion rate in the region highly compressed by the shock wave can give off significant amounts of highly energetic alpha particles. Due to the high density of the surrounding fuel, they move only a short distance before being "thermalised", losing their energy to the fuel as heat. This additional energy will cause additional fusion reactions in the heated fuel, giving off more high-energy particles. This process spreads outward from the centre, leading to a kind of self-sustaining burn known as "ignition".
Issues with successful achievement.
The primary problems with increasing ICF performance since the early experiments in the 1970s have been of energy delivery to the target, controlling symmetry of the imploding fuel, preventing premature heating of the fuel (before maximum density is achieved), preventing premature mixing of hot and cool fuel by hydrodynamic instabilities and the formation of a 'tight' shockwave convergence at the compressed fuel center.
In order to focus the shock wave on the center of the target, the target must be made with extremely high precision and sphericity with aberrations of no more than a few micrometres over its surface (inner and outer). Likewise the aiming of the laser beams must be extremely precise and the beams must arrive at the same time at all points on the target. Beam timing is a relatively simple issue though and is solved by using delay lines in the beams' optical path to achieve picosecond levels of timing accuracy. The other major problem plaguing the achievement of high symmetry and high temperatures/densities of the imploding target are so called "beam-beam" imbalance and beam anisotropy. These problems are, respectively, where the energy delivered by one beam may be higher or lower than other beams impinging on the target and of "hot spots" within a beam diameter hitting a target which induces uneven compression on the target surface, thereby forming Rayleigh–Taylor instabilities in the fuel, prematurely mixing it and reducing heating efficacy at the time of maximum compression.
All of these problems have been substantially mitigated to varying degrees in the past two decades of research by using various beam smoothing techniques and beam energy diagnostics to balance beam to beam energy; however, RT instability remains a major issue. Target design has also improved tremendously over the years. Modern cryogenic hydrogen ice targets tend to freeze a thin layer of deuterium just on the inside of a plastic sphere while irradiating it with a low power IR laser to smooth its inner surface while monitoring it with a microscope equipped camera, thereby allowing the layer to be closely monitored ensuring its "smoothness". Cryogenic targets filled with a deuterium tritium (D-T) mixture are "self-smoothing" due to the small amount of heat created by the decay of the radioactive tritium isotope. This is often referred to as "beta-layering".
Certain targets are surrounded by a small metal cylinder which is irradiated by the laser beams instead of the target itself, an approach known as "indirect drive". In this approach the lasers are focused on the inner side of the cylinder, heating it to a superhot plasma which radiates mostly in X-rays. The X-rays from this plasma are then absorbed by the target surface, imploding it in the same way as if it had been hit with the lasers directly. The absorption of thermal x-rays by the target is more efficient than the direct absorption of laser light, however these "hohlraums" or "burning chambers" also take up considerable energy to heat on their own thus significantly reducing the overall efficiency of laser-to-target energy transfer. They are thus a debated feature even today; the equally numerous "direct-drive" design does not use them. Most often, indirect drive hohlraum targets are used to simulate thermonuclear weapons tests due to the fact that the fusion fuel in them is also imploded mainly by X-ray radiation.
A variety of ICF drivers are being explored. Lasers have improved dramatically since the 1970s, scaling up in energy and power from a few joules and kilowatts to megajoules (see NIF laser) and hundreds of terawatts, using mostly frequency doubled or tripled light from neodymium glass amplifiers.
Heavy ion beams are particularly interesting for commercial generation, as they are easy to create, control, and focus. On the downside, it is very difficult to achieve the very high energy densities required to implode a target efficiently, and most ion-beam systems require the use of a hohlraum surrounding the target to smooth out the irradiation, reducing the overall efficiency of the coupling of the ion beam's energy to that of the imploding target further.
History of ICF.
First conception.
In the US.
In the western world, ICF's history can be traced back to a seminal meeting called by Edward Teller in 1957 on the topic of peaceful uses of atomic explosions. Among the many topics covered during the event, some consideration was given to using a hydrogen bomb to heat a water-filled underground cavern. The resulting steam would then be used to power conventional generators, and thereby provide electrical power.
This meeting led to the Operation Plowshare efforts, given this name in 1961. Three primary concepts were studied as part of Plowshare; energy generation under Project PACER, the use of large nuclear explosions for excavation, and as a sort of nuclear fracking for the natural gas industry. PACER was directly tested in December 1961 when the 3 kt Project Gnome device was implanted in a salt dome in Nevada. In spite of all theorizing and attempts to stop it, radioactive steam was released from the drill shaft, some distance from the test site. Further studies as part of Project PACER led to a number of engineered cavities replacing natural ones, but through this period the entire Plowshare efforts turned from bad to worse, especially after the failure of 1962's Sedan which released huge quantities of fallout. PACER nevertheless continued to receive some funding until 1975, when a 3rd party study demonstrated that the cost of electricity from PACER would be the equivalent to conventional nuclear plants with fuel costs over ten times as great as they were.
Another outcome of the Teller meeting was to prompt John Nuckolls to start considering what happens when the fusion side of the bomb, the "secondary," was scaled down to very small size. His earliest work concerned the study of how small a fusion bomb could be made while still having a large "gain" to provide net energy output. This work suggested that at very small sizes, on the order of milligrams, very little energy would be needed to ignite it, much less than a fission "primary". He proposed building, in effect, tiny all-fusion explosives using a tiny drop of D-T fuel suspended in the center of a metal shell, today known as a hohlraum. The shell provided the same effect as the bomb casing in an H-bomb, trapping x-rays inside so they irradiated the fuel. The main difference is that the x-rays would not be supplied by a primary within the shell, but some sort of external device that heated the shell from the outside until it was glowing in the x-ray region (see thermal radiation). The power would be delivered by a then-unidentified pulsed power source he referred to using bomb terminology, the "primary".
The main advantage to this scheme is the efficiency of the fusion process at high densities. According to the Lawson criterion, the amount of energy needed to heat the D-T fuel to break-even conditions at ambient pressure is perhaps 100 times greater than the energy needed to compress it to a pressure that would deliver the same rate of fusion. So, in theory, the ICF approach would be dramatically more efficient in terms of gain. This can be understood by considering the energy losses in a conventional scenario where the fuel is slowly heated, as in the case of magnetic fusion energy; the rate of energy loss to the environment is based on the temperature difference between the fuel and its surroundings, which continues to increase as the fuel is heated. In the ICF case, the entire hohlraum is filled with high-temperature radiation, limiting losses.
In Germany.
Around the same time (in 1956) a meeting was organized at the Max Planck Institute in Germany by the fusion pioneer Carl Friedrich von Weizsäcker. At this meeting Friedwardt Winterberg proposed the non-fission ignition of a thermonuclear micro-explosion by a convergent shock wave driven with high explosives. Further reference about Winterberg's work in Germany on nuclear micro explosions (mininukes) is contained in a declassified report of the former East German Stasi (Staatsicherheitsdienst).
In 1964 Winterberg proposed that ignition could be achieved by an intense beam of microparticles accelerated to a velocity of 1000 km/s. And in 1968, he proposed to use intense electron and ion beams, generated by Marx generators, for the same purpose. The advantage of this proposal is that the generation of charged particle beams is not only less expensive than the generation of laser beams but also can entrap the charged fusion reaction products due to the strong self-magnetic beam field, drastically reducing the compression requirements for beam ignited cylindrican targets.
Early research.
Through the late 1950s, Nuckolls and collaborators at the Lawrence Livermore National Laboratory (LLNL) ran a number of computer simulations of the ICF concept. In early 1960 this produced a full simulation of the implosion of 1 mg of D-T fuel inside a dense shell. The simulation suggested that a 5 MJ power input to the hohlraum would produce 50 MJ of fusion output, a gain of 10. At the time the laser had not yet been invented, and a wide variety of possible drivers were considered, including pulsed power machines, charged particle accelerators, plasma guns, and hypervelocity pellet guns.
Through the year two key theoretical advances were made. New simulations considered the timing of the energy delivered in the pulse, known as "pulse shaping", leading to better implosion. Additionally, the shell was made much larger and thinner, forming a thin shell as opposed to an almost solid ball. These two changes dramatically increased the efficiency of the implosion, and thereby greatly lowered the energy required to compress it. Using these improvements, it was calculated that a driver of about 1 MJ would be needed, a five-fold improvement. Over the next two years several other theoretical advancements were proposed, notably Ray Kidder's development of an implosion system without a hohlraum, the so-called "direct drive" approach, and Stirling Colgate and Ron Zabawski's work on very small systems with as little as 1 μg of D-T fuel.
The introduction of the laser in 1960 at Hughes Research Laboratories in California appeared to present a perfect driver mechanism. Starting in 1962, Livermore's director John S. Foster, Jr. and Edward Teller began a small-scale laser study effort directed toward the ICF approach. Even at this early stage the suitability of the ICF system for weapons research was well understood, and the primary reason for its ability to gain funding. Over the next decade, LLNL made several small experimental devices for basic laser-plasma interaction studies.
Development begins.
In 1967 Kip Siegel started KMS Industries using the proceeds of the sale of his share of an earlier company, Conductron, a pioneer in holography. In the early 1970s he formed KMS Fusion to begin development of a laser-based ICF system. This development led to considerable opposition from the weapons labs, including LLNL, who put forth a variety of reasons that KMS should not be allowed to develop ICF in public. This opposition was funnelled through the Atomic Energy Commission, who demanded funding for their own efforts. Adding to the background noise were rumours of an aggressive Soviet ICF program, new higher-powered CO2 and glass lasers, the electron beam driver concept, and the 1970s energy crisis which added impetus to many energy projects.
In 1972 Nuckolls wrote an influential public paper in "Nature" introducing ICF and suggesting that testbed systems could be made to generate fusion with drivers in the kJ range, and high-gain systems with MJ drivers.
In spite of limited resources and numerous business problems, KMS Fusion successfully demonstrated fusion from the ICF process on 1 May 1974. However, this success was followed not long after by Siegel's death, and the end of KMS fusion about a year later, having run the company on Siegel's life insurance policy. By this point several weapons labs and universities had started their own programs, notably the solid-state lasers (Nd:glass lasers) at LLNL and the University of Rochester, and krypton fluoride excimer lasers systems at Los Alamos and the Naval Research Laboratory.
Although KMS's success led to a major development effort, the advances that followed were, and still are, hampered by the seemingly intractable problems that characterize fusion research in general.
High-Energy ICF.
High energy ICF experiments (multi-hundred joules per shot and greater experiments) began in earnest in the early-1970s, when lasers of the required energy and power were first designed. This was some time after the successful design of magnetic confinement fusion systems, and around the time of the particularly successful tokamak design that was introduced in the early '70s. Nevertheless, high funding for fusion research stimulated by the multiple energy crises during the mid to late 1970s produced rapid gains in performance, and inertial designs were soon reaching the same sort of "below break-even" conditions of the best magnetic systems.
LLNL was, in particular, very well funded and started a major laser fusion development program. Their Janus laser started operation in 1974, and validated the approach of using Nd:glass lasers to generate very high power devices. Focusing problems were explored in the Long path laser and Cyclops laser, which led to the larger Argus laser. None of these were intended to be practical ICF devices, but each one advanced the state of the art to the point where there was some confidence the basic approach was valid. At the time it was believed that making a much larger device of the Cyclops type could both compress and heat the ICF targets, leading to ignition in the "short term". This was a misconception based on extrapolation of the fusion yields seen from experiments utilizing the so-called "exploding pusher" type of fuel capsules. During the period spanning the years of the late '70s and early '80s the estimates for laser energy on target needed to achieve ignition doubled almost yearly as the various plasma instabilities and laser-plasma energy coupling loss modes were gradually understood. The realization that the simple exploding pusher target designs and mere few kilojoule (kJ) laser irradiation intensities would never scale to high gain fusion yields led to the effort to increase laser energies to the 100 kJ level in the UV and to the production of advanced ablator and cryogenic DT ice target designs.
Shiva and Nova.
One of the earliest serious and large scale attempts at an ICF driver design was the Shiva laser, a 20-beam neodymium doped glass laser system built at the Lawrence Livermore National Laboratory (LLNL) that started operation in 1978. Shiva was a "proof of concept" design intended to demonstrate compression of fusion fuel capsules to many times the liquid density of hydrogen. In this, Shiva succeeded and compressed its pellets to 100 times the liquid density of deuterium. However, due to the laser's strong coupling with hot electrons, premature heating of the dense plasma (ions) was problematic and fusion yields were low. This failure by Shiva to efficiently heat the compressed plasma pointed to the use of optical frequency multipliers as a solution which would frequency triple the infrared light from the laser into the ultraviolet at 351 nm. Newly discovered schemes to efficiently frequency triple high intensity laser light discovered at the Laboratory for Laser Energetics in 1980 enabled this method of target irradiation to be experimented with in the 24 beam OMEGA laser and the NOVETTE laser, which was followed by the Nova laser design with 10 times the energy of Shiva, the first design with the specific goal of reaching ignition conditions.
Nova also failed in its goal of achieving ignition, this time due to severe variation in laser intensity in its beams (and differences in intensity between beams) caused by filamentation which resulted in large non-uniformity in irradiation smoothness at the target and asymmetric implosion. The techniques pioneered earlier could not address these new issues. But again this failure led to a much greater understanding of the process of implosion, and the way forward again seemed clear, namely the increase in uniformity of irradiation, the reduction of hot-spots in the laser beams through beam smoothing techniques to reduce Rayleigh–Taylor instability imprinting on the target and increased laser energy on target by at least an order of magnitude. Funding for fusion research was severely constrained in the 80's, but Nova nevertheless successfully gathered enough information for a next generation machine.
National Ignition Facility.
The resulting design, now known as the National Ignition Facility, started construction at LLNL in 1997. NIF's main objective will be to operate as the flagship experimental device of the so-called nuclear stewardship program, supporting LLNLs traditional bomb-making role. Completed in March 2009, NIF has now conducted experiments using all 192 beams, including experiments that set new records for power delivery by a laser.
The first credible attempts at ignition were initially scheduled for 2010, but ignition was not achieved as of September 30, 2012. As of October 7, 2013, the facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is still a long way from satisfying the Lawson criterion, but is a major step forward.
Fast ignition.
A more recent development is the concept of "fast ignition," which may offer a way to directly heat the high density fuel after compression, thus decoupling the heating and compression phases of the implosion. In this approach the target is first compressed "normally" using a driver laser system, and then when the implosion reaches maximum density (at the stagnation point or "bang time"), a second ultra-short pulse ultra-high power petawatt (PW) laser delivers a single pulse focused on one side of the core, dramatically heating it and hopefully starting fusion ignition. The two types of fast ignition are the "plasma bore-through" method and the "cone-in-shell" method. In the first method the petawatt laser is simply expected to bore straight through the outer plasma of an imploding capsule and to impinge on and heat the dense core, whereas in the cone-in-shell method, the capsule is mounted on the end of a small high-z (high atomic number) cone such that the tip of the cone projects into the core of the capsule. In this second method, when the capsule is imploded, the petawatt has a clear view straight to the high density core and does not have to waste energy boring through a 'corona' plasma; however, the presence of the cone affects the implosion process in significant ways that are not fully understood. Several projects are currently underway to explore the fast ignition approach, including upgrades to the OMEGA laser at the University of Rochester, the GEKKO XII device in Japan, and an entirely new £500 million facility, known as HiPER, proposed for construction in the European Union. If successful, the fast ignition approach could dramatically lower the total amount of energy needed to be delivered to the target; whereas NIF uses UV beams of 2 MJ, HiPER's driver is 200 kJ and heater 70 kJ, yet the predicted fusion gains are nevertheless even higher than on NIF.
Other projects.
Laser Mégajoule, the French project, has seen its first experimental line achieved in 2002, and is due for completion in 2012.
Using a different approach entirely is the "z"-pinch device. "Z"-pinch uses massive amounts of electrical current which is switched into a cylinder comprising many of extremely fine wires. The wires vaporize to form an electrically conductive plasma that carries a very high current; the resulting circumferential magnetic field squeezes the plasma cylinder, imploding it and thereby generating a high-power x-ray pulse that can be used to drive the implosion of a fuel capsule. Challenges to this approach include relatively low drive temperatures, resulting in slow implosion velocities and potentially large instability growth, and preheat caused by high-energy x-rays.
Most recently, Winterberg has proposed the ignition of a deuterium microexplosion, with a gigavolt super-Marx generator, which is a Marx generator driven by up to 100 ordinary Marx generators.
As an energy source.
Practical power plants built using ICF have been studied since the late 1970s when ICF experiments were beginning to ramp up to higher powers; they are known as inertial fusion energy, or IFE plants. These devices would deliver a successive stream of targets to the reaction chamber, several a second typically, and capture the resulting heat and neutron radiation from their implosion and fusion to drive a conventional steam turbine.
Technical challenges.
IFE faces continued technical challenges in reaching the conditions needed for ignition. But even if these were all to be solved, there are a significant number of practical problems that seem just as difficult to overcome. Laser driven systems were initially believed to be able to generate commercially useful amounts of energy. However, as estimates of the energy required to reach ignition grew dramatically during the 1970s and '80s, these hopes were abandoned. Given the low efficiency of the laser amplification process (about 1 to 1.5%), and the losses in generation (steam-driven turbine systems are typically about 35% efficient), fusion gains would have to be on the order of 350 just to energetically break even. These sorts of gains appeared to be impossible to generate, and ICF work turned primarily to weapons research.
With the recent introduction of fast ignition and similar approaches, things have changed dramatically. In this approach gains of 100 are predicted in the first experimental device, HiPER. Given a gain of about 100 and a laser efficiency of about 1%, HiPER produces about the same amount of "fusion" energy as electrical energy was needed to create it. It also appears that an order of magnitude improvement in laser efficiency may be possible through the use of newer designs that replace the flash lamps with laser diodes that are tuned to produce most of their energy in a frequency range that is strongly absorbed. Initial experimental devices offer efficiencies of about 10%, and it is suggested that 20% is a real possibility with some additional development.
With "classical" devices like NIF about 330 MJ of electrical power are used to produce the driver beams, producing an expected yield of about 20 MJ, with the maximum credible yield of 45 MJ. Using the same sorts of numbers in a reactor combining fast ignition with newer lasers would offer dramatically improved performance. HiPER requires about 270 kJ of laser energy, so assuming a first-generation diode laser driver at 10% the reactor would require about 3 MJ of electrical power. This is expected to produce about 30 MJ of fusion power. Even a very poor conversion to electrical energy appears to offer real-world power output, and incremental improvements in yield and laser efficiency appear to be able to offer a commercially useful output.
Practical problems.
ICF systems face some of the same secondary power extraction problems as magnetic systems in generating useful power from their reactions. One of the primary concerns is how to successfully remove heat from the reaction chamber without interfering with the targets and driver beams. Another serious concern is that the huge number of neutrons released in the fusion reactions react with the plant, causing them to become intensely radioactive themselves, as well as mechanically weakening metals. Fusion plants built of conventional metals like steel would have a fairly short lifetime and the core containment vessels will have to be replaced frequently.
One current concept in dealing with both of these problems, as shown in the HYLIFE-II baseline design, is to use a "waterfall" of FLiBe, a molten mix of fluoride salts of lithium and beryllium, which both protect the chamber from neutrons and carry away heat. The FLiBe is then passed into a heat exchanger where it heats water for use in the turbines. Another, Sombrero, uses a reaction chamber built of Carbon-fiber-reinforced polymer which has a very low neutron cross section. Cooling is provided by a molten ceramic, chosen because of its ability to stop the neutrons from traveling any further, while at the same time being an efficient heat transfer agent.
Economic viability.
Even if these technical advances solve the considerable problems in IFE, another factor working against IFE is the cost of the fuel. Even as Nuckolls was developing his earliest detailed calculations on the idea, co-workers pointed this out: if an IFE machine produces 50 MJ of fusion energy, one might expect that a shot could produce perhaps 10 MJ of power for export. Converted to better known units, this is the equivalent of 2.8 kWh of electrical power. Wholesale rates for electrical power on the grid were about 0.3 cents/kWh at the time, which meant the monetary value of the shot was perhaps one cent. In the intervening 50 years the price of power has remained about even with the rate of inflation, and the rate in 2012 in Ontario, Canada was about 2.8 cents/kWh
Thus, in order for an IFE plant to be economically viable, fuel shots would have to cost considerably less than ten cents in year 2012 dollars. At the time this objection was first noted, Nuckolls suggested using liquid droplets sprayed into the hohlraum from an eye-dropper-like apparatus. Given the ever-increasing demands for higher uniformity of the targets, this approach does not appear practical, as even the inner ablator and fuel itself currently costs several orders of magnitude more than this. Moreover, Nuckolls' solution had the fuel dropped into a fixed hohlraum that would be re-used in a continual cycle, but at current energy levels the hohlraum is destroyed with every shot.
Direct-drive systems avoid the use of a hohlraum and thereby may be less expensive in fuel terms. However, these systems still require an ablator, and the accuracy and geometrical considerations are even more important. They are also far less developed than the indirect-drive systems, and face considerably more technical problems in terms of implosion physics. Currently there is no strong consensus whether a direct-drive system would actually be less expensive to operate.
Projected development.
The various phases of such a project are the following, the sequence of inertial confinement fusion development follows much the same outline:
At the moment, according to the available data, inertial confinement fusion experiments have not gone beyond the first phase, although Nova and others have repeatedly demonstrated operation within this realm.
In the short term a number of new systems are expected to reach the second stage.
For a true industrial demonstration, further work is required. In particular, the laser systems need to be able to run at high operating frequencies, perhaps one to ten times a second. Most of the laser systems mentioned in this article have trouble operating even as much as once a day. Parts of the HiPER budget are dedicated to research in this direction as well. Because they convert electricity into laser light with much higher efficiency, diode lasers also run cooler, which in turn allows them to be operated at much higher frequencies. HiPER is currently studying devices that operate at 1 MJ at 1 Hz, or alternately 100 kJ at 10 Hz.
Nuclear weapons program.
The very hot and dense conditions encountered during an Inertial Confinement Fusion experiment are similar to those created in a thermonuclear weapon, and have applications to the nuclear weapons program. ICF experiments might be used, for example, to help determine how warhead performance will degrade as it ages, or as part of a program of designing new weapons. Retaining knowledge and corporate expertise in the nuclear weapons program is another motivation for pursuing ICF. Funding for the NIF in the United States is sourced from the 'Nuclear Weapons Stockpile Stewardship' program, and the goals of the program are oriented accordingly. It has been argued that some aspects of ICF research may violate the Comprehensive Test Ban Treaty or the Nuclear Non-Proliferation Treaty. In the long term, despite the formidable technical hurdles, ICF research might potentially lead to the creation of a "pure fusion weapon".
Neutron source.
Inertial confinement fusion has the potential to produce orders of magnitude more neutrons than spallation. Neutrons are capable of locating hydrogen atoms in molecules, resolving atomic thermal motion and studying collective excitations of photons more effectively than X-rays. Neutron scattering studies of molecular structures could resolve problems associated with protein folding, diffusion through membranes, proton transfer mechanisms, dynamics of molecular motors, etc. by modulating thermal neutrons into beams of slow neutrons. In combination with fissionable materials, neutrons produced by ICF can potentially be used in Hybrid Nuclear Fusion designs to produce electric power.
Bibliography.
</dl>

</doc>
<doc id="40324" url="http://en.wikipedia.org/wiki?curid=40324" title="Quadratic programming">
Quadratic programming

Quadratic programming (QP) is a special type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables.
Problem formulation.
The quadratic programming problem can be formulated as follows.
Suppose formula_1 is a positive integer representing the number of variables and formula_2 is a positive integer representing the number of constraints. Suppose formula_3 is an formula_1-dimensional real vector, formula_5 is an formula_6 real symmetric matrix, formula_7 is an formula_8 real matrix, and formula_9 is an formula_2-dimensional real vector.
The quadratic programming problem is
where formula_11 denotes the vector transpose of formula_12. The notation formula_13 means that every entry of the vector formula_14 is less than or equal to the corresponding entry of the vector formula_15.
A related programming problem, quadratically constrained quadratic programming, can be posed by adding quadratic constraints on the variables.
Solution methods.
For general problems a variety of methods are commonly used, including
Convex quadratic programming is a special case of the more general field of convex optimization.
Equality constraints.
Quadratic programming is particularly simple when there are only equality constraints; specifically, the problem is linear. By using Lagrange multipliers and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem is given by the linear system:
where formula_17 is a set of Lagrange multipliers which come out of the solution alongside formula_18.
The easiest means of approaching this system is direct solution (for example, LU factorization), which for small problems is very practical. For large problems, the system poses some unusual difficulties, most notably that problem is never positive definite (even if formula_5 is), making it potentially very difficult to find a good numeric approach, and there are many approaches to choose from dependent on the problem.
If the constraints don't couple the variables too tightly, a relatively simple attack is to change the variables so that constraints are unconditionally satisfied. For example, suppose formula_20 (generalizing to nonzero is straightforward). Looking at the constraint equations:
introduce a new variable formula_22 defined by
where formula_22 has dimension of formula_18 minus the number of constraints. Then
and if formula_27 is chosen so that formula_28 the constraint equation will be always satisfied. Finding such formula_27 entails finding the null space of formula_30, which is more or less simple depending on the structure of formula_30. Substituting into the quadratic form gives an unconstrained minimization problem:
the solution of which is given by:
Under certain conditions on formula_5, the reduced matrix formula_35 will be positive definite. It's possible to write a variation on the conjugate gradient method which avoids the explicit calculation of formula_27.
Lagrangian duality.
The Lagrangian dual of a QP is also a QP. To see that let us focus on the case where formula_37 and Q is positive definite. We write the Lagrangian function as 
Defining the (Lagrangian) dual function formula_39, defined as formula_40, we find an infimum of formula_41, using formula_42
formula_43
hence the dual function is 
hence the Lagrangian dual of the QP is
maximize: formula_45
subject to: formula_46.
Besides the Lagrangian duality theory, there are other duality pairings (e.g. Wolfe, etc.).
Complexity.
For positive definite "Q", the ellipsoid method solves the problem in polynomial time. If, on the other hand, "Q" is indefinite, then the problem is NP-hard. In fact, even if "Q" has only one negative eigenvalue, the problem is NP-hard.

</doc>
<doc id="40325" url="http://en.wikipedia.org/wiki?curid=40325" title="Positive semidefinite">
Positive semidefinite

In mathematics, positive semidefinite may refer to:

</doc>
<doc id="40326" url="http://en.wikipedia.org/wiki?curid=40326" title="Positive-definite matrix">
Positive-definite matrix

In linear algebra, a symmetric "n" × "n" real matrix formula_1 is said to be positive definite if formula_2 is positive for every non-zero column vector formula_3 of formula_4 real numbers. Here formula_5 denotes the transpose of formula_3.
More generally, an "n" × "n" Hermitian matrix formula_1 is said to be positive definite if formula_8 is real and positive for all non-zero column vectors formula_3 of formula_4 complex numbers. Here formula_11 denotes the conjugate transpose of formula_3.
The negative definite, positive semi-definite, and negative semi-definite matrices are defined in the same way, except that the expression formula_2 or formula_8 is required to be always negative, non-negative, and non-positive, respectively.
Positive definite matrices are closely related to positive-definite symmetric bilinear forms (or sesquilinear forms in the complex case), and to inner products of vector spaces.
Some authors use more general definitions of "positive definite" that include some non-symmetric real matrices, or non-Hermitian complex ones.
Examples.
The examples "M" and "N" above show that a matrix in which some elements are negative may still be positive-definite, and conversely a matrix whose entries are all positive may not be positive definite.
Connections.
The general purely quadratic real function "f"("z") on "n" real variables "z"1, ..., "zn" can always be written as "z"T"Mz" where "z" is the column vector with those variables, and "M" is a symmetric real matrix. Therefore, the matrix being positive definite means that "f" has a unique minimum (zero) when "z" is zero, and is strictly positive for any other "z".
More generally, a twice-differentiable real function "f" on "n" real variables has an isolated local minimum at arguments "z"1, ..., "zn" if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive definite at that point. Similar statements can be made for negative definite and semi-definite matrices.
In statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear combination of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.
Characterizations.
Let "M" be an "n" × "n" Hermitian matrix. The following properties are equivalent to "M" being positive definite:
Quadratic forms.
The (purely) quadratic form associated with a real matrix "M" is the function "Q" : R"n" → R such that "Q"("x") = "x"T"Mx" for all "x". It turns out that the matrix "M" is positive definite if and only if it is symmetric and its quadratic form is a strictly convex function.
More generally, any quadratic function from R"n" to R can be written as "x"T"Mx" + "x"T"b" + "c" where "M" is a symmetric "n" × "n" matrix, "b" is a real "n"-vector, and "c" a real constant. This quadratic function is strictly convex when "M" is positive definite, and hence has a unique finite global minimum, if and only if "M" is positive definite. For this reason, positive definite matrices play an important role in optimization problems.
Simultaneous diagonalization.
A symmetric, and a symmetric and positive-definite matrix can be simultaneously diagonalized, although not necessarily via a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.
Let "M" be a symmetric and "N" a symmetric and positive-definite matrix. Write the generalized eigenvalue equation as ("M"−λ"N")"x" = 0 where we impose that "x" be normalized, i.e. "x"T"Nx" = 1. Now we use Cholesky decomposition to write the inverse of "N" as "Q"T"Q". Multiplying by "Q" and "Q"T, we get "Q"("M"−λ"N")"Q"T"x" = 0, which can be rewritten as ("QMQ"T)"y" = λ"y" where "y"T"y" = 1. Manipulation now yields "MX" = "NX"Λ where "X" is a matrix having as columns the generalized eigenvectors and Λ is a diagonal matrix with the generalized eigenvalues. Now premultiplication with "X"T gives the final result: "X"T"MX" = Λ and "X"T"NX" = "I", but note that this is no longer an orthogonal diagonalization.
Note that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other. For this result see Horn&Johnson, 1985, page 218 and following.
Negative-definite, semidefinite and indefinite matrices.
A Hermitian matrix is negative-definite, negative-semidefinite, or positive-semidefinite if and only if all of its eigenvalues are negative, non-positive, or non-negative, respectively.
Negative-definite.
The "n" × "n" Hermitian matrix "M" is said to be "negative-definite" if
for all non-zero "x" in C"n" (or, all non-zero "x" in R"n" for the real matrix), where "x*" is the conjugate transpose of "x".
A matrix is negative definite if its "k-"th order leading principal minor is negative when "k" is odd, and positive when "k" is even.
Positive-semidefinite.
M is called "positive-semidefinite" (or sometimes "nonnegative-definite") if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
A matrix "M" is positive-semidefinite if and only if it arises as the Gram matrix of some set of vectors. In contrast to the positive-definite case, these vectors need not be linearly independent.
For any matrix "A", the matrix "A*A" is positive semidefinite, and rank("A") = rank("A*A"). 
Conversely, any Hermitian positive semi-definite matrix "M" can be written as "M" = "LL*", where "L" is lower triangular; this is the Cholesky decomposition. If "M" is not positive definite, then some of the diagonal elements of "L" may be zero.
A Hermitian matrix is positive semidefinite if and only if all of its principal minors are nonnegative. It is however not enough to consider the leading principal minors only, as is checked on the diagonal matrix with entries 0 and -1.
Negative-semidefinite.
It is called "negative-semidefinite" if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
Indefinite.
A Hermitian matrix which is neither positive definite, negative definite, positive-semidefinite, nor negative-semidefinite is called "indefinite". Indefinite matrices are also characterized by having both positive and negative eigenvalues.
Further properties.
If "M" is a Hermitian positive-semidefinite matrix, one sometimes writes "M" ≥ 0 and if "M" is positive-definite one writes "M" > 0. The notion comes from functional analysis where positive-semidefinite matrices define positive operators.
For arbitrary square matrices "M", "N" we write "M" ≥ "N" if "M" − "N" ≥ 0; i.e., "M" − "N" is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering "M" > "N".
Block matrices.
A positive 2"n" × 2"n" matrix may also be defined by blocks:
where each block is "n" × "n". By applying the positivity condition, it immediately follows that "A" and "D" are hermitian, and "C" = "B*".
We have that "z*Mz" ≥ 0 for all complex "z", and in particular for "z" = ( "v", 0)T. Then
A similar argument can be applied to "D", and thus we conclude that both "A" and "D" must be positive definite matrices, as well.
Converse results can be proved with stronger conditions on the blocks, for instance using the Schur complement.
On the definition.
Consistency between real and complex definitions.
Since every real matrix is also a complex matrix, the definitions of "positive definite" for the two classes must agree.
For complex matrices, the most common definition says that ""M" is positive definite if and only if "z*Mz" is real and positive for all non-zero "complex" column vectors "z". This condition implies that "M" is Hermitian, that is, its transpose is equal to its conjugate. To see this, consider the matrices "A" = ("M"+"M*")/2 and "B" = ("M"−"M*")/(2"i"), so that "M" = "A"+"iB" and "z*Mz" = "z*Az" + "iz*Bz". The matrices "A" and "B" are Hermitian, therefore "z*Az" and "z*Bz" are individually real. If "z*Mz" is real, then "z*Bz" must be zero for all "z". Then "B" is the zero matrix and "M" = "A", proving that "M" is Hermitian.
By this definition, a positive definite "real" matrix "M" is Hermitian, hence symmetric; and "z"T"Mz" is positive for all non-zero "real" column vectors "z". However the last condition alone is not sufficient for "M" to be positive definite. For example, if
then for any real vector "z" with entries "a" and "b" we have "z"T"Mz" = ("a"−"b")"a" + ("a"+"b")"b" = "a"2 + "b"2, which is always positive if "z" is not zero. However, if "z" is the complex vector with entries 1 and "i", one gets
which is not real. Therefore, "M" is not positive definite.
On the other hand, for a "symmetric" real matrix "M", the condition "z"T"Mz" > 0 for all nonzero real vectors "z"" "does" imply that "M" is positive definite in the complex sense.
Extension for non symmetric matrices.
Some authors choose to say that a complex matrix "M" is positive definite if Re("z*Mz") > 0 for all non-zero complex vectors "z", where Re("c") denotes the real part of a complex number "c". This weaker definition encompasses some non-Hermitian complex matrices, including some non-symmetric real ones, such as formula_52.
Indeed, with this definition, a real matrix is positive definite if and only if "z"T"Mz" > 0 for all nonzero real vectors "z", even if "M" is not symmetric.
In general, we have Re("z*Mz") > 0 for all complex nonzero vectors "z" if and only if the Hermitian part ("M" + "M*")/2 of "M" is positive definite in the narrower sense. Similarly, we have "x"T"Mx" > 0 for all real nonzero vectors "x" if and only if the symmetric part ("M" + "M"T)/2 of "M" is positive definite in the narrower sense.
In summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.

</doc>
<doc id="40330" url="http://en.wikipedia.org/wiki?curid=40330" title="Magnoliaceae">
Magnoliaceae

The Magnoliaceae are a flowering plant family, the magnolia family, in the order Magnoliales. It consists of two subfamilies:
Magnolioideae, of which "Magnolia" is the most well-known genus, and Liriodendroidae, a monogeneric subfamily, of which "Liriodendron" (tulip trees) is the only genus.
Unlike most angiosperms, whose flower parts are in whorls (rings), the Magnoliaceae have their stamens and pistils in spirals on a conical receptacle. This arrangement is found in some fossil plants and is believed to be a basal or early condition for angiosperms. The flowers also have parts not distinctly differentiated into sepals and petals, while angiosperms that evolved later tend to have distinctly differentiated sepals and petals. The poorly differentiated perianth parts that occupy both positions are known as tepals.
The family has about 219 species in seven genera, although some classification systems include all of subfamily Magnoioideae in genus "Magnolia". The family ranges across subtropical eastern North America, Mexico and Central America, the West Indies, tropical South America, southern and eastern India, Sri Lanka, Indochina, Malesia, China, Japan, and Korea.
Genera.
The genera in the family include "Lirianthe, Liriodendron, Magnolia, Michelia", and Yulania.
Description.
The monophyly of Magnoliaceae is supported by a number of shared morphological characters among the various genera in the family. Most have bisexual flowers (with the exception of "Kmeria" and some species of "Magnolia" section "Gynopodium"), showy, fragrant, radial, and with an elongated receptacle. Leaves are alternate, simple, and sometimes lobed. The inflorescence is a solitary, showy flower with indistinguishable petals and sepals. Sepals range from six to many; stamens are numerous and feature short filaments which are poorly differentiated from the anthers. Carpels are usually numerous, distinct, and on an elongated receptacle or torus. The fruit is an etario of follicles which usually become closely appressed as they mature and open along the abaxial surface. Seeds have a fleshy coat and color that ranges from red to orange (except "Liriodendron"). Magnoliaceae flowers are beetle pollinated, except for "Liriodendron", which is bee pollinated. The carpels of" Magnolia" flowers are especially thick to avoid damage by beetles that land, crawl, and feast on them. The seeds of Magnolioideae are bird dispersed, while the seeds of "Liriodendron" are wind dispersed.
Biogeography.
Due to its great age, the geographical distribution of the Magnoliaceae has become disjunct or fragmented as a result of major geologic events such as ice ages, continental drift, and mountain formation. This distribution pattern has isolated some species, while keeping others in close contact.
Extant species of the Magnoliaceae are widely distributed in temperate and tropical Asia from the Himalayas to Japan and southwest through Malaysia and New Guinea. Asia is home to about two-thirds of the species in Magnoliaceae, with the remainder of the family spread across the Americas with temperate species extending into southern Canada and tropical elements extending into Brazil and the West Indies.
Systematics.
Due to the family-wide morphological similarity, no consensus has yet emerged on the number of genera in the family. The development of DNA sequencing at the end of the 20th century had a profound impact on the research of phylogenetic relationships within the family.
The employment of ndhF and cpDNA sequences has refuted many of the traditionally accepted phylogenetic relationships within the Magnoliaceae. For example, the genera "Magnolia" and "Michelia" were shown to be paraphyletic when the remaining four genera of the Magnolioideae are split out. In fact, even many of the subgenera ("Magnolia" subg. "Magnolia", "Magnolia" subg. "Talauma") have been found to be paraphyletic. Although no completely resolved phylogeny for the family has yet been determined, these technological advances have allowed systematists to broadly circumscribe major lineages.
Economic significance.
As a whole, the Magnoliaceae are not an economically significant family. With the exception of ornamental cultivation, the economic significance of magnolias is generally confined to the use of wood from certain timber species and the use of bark and flowers from several species believed to possess medicinal qualities. Magnolias have a rich cultural tradition in China, where references to their healing qualities go back thousands of years. The Chinese have long used the bark of "Magnolia officinalis", a magnolia native to the mountains of China with large leaves and fragrant white flowers, as a remedy for cramps, abdominal pain, nausea, diarrhea, and indigestion. Certain magnolia flowers, such as the buds of "Magnolia liliiflora", have been used to treat chronic respiratory and sinus infections and lung congestion. Recently, magnolia bark has become incorporated into alternative medicine in the west, where tablets made from the bark of "M. officinalis" have been marketed as an aid for anxiety, allergies, asthma, and weight loss. Compounds found in magnolia bark might have antibacterial and antifungal properties, but no large-scale study on the health effects of magnolia bark or flowers has yet been conducted.

</doc>
<doc id="40331" url="http://en.wikipedia.org/wiki?curid=40331" title="Cannoli">
Cannoli

Cannoli (Sicilian: Cannula) are Sicilian pastry desserts. The singular is "cannolo" (or in the Sicilian language "cannolu", plural "cannula"), meaning "little tube", with the etymology stemming from the Arabic "Qanawat". Cannoli originated in Sicily in Piana degli Albanesi and are a staple of Sicilian cuisine. They are also popular in Italian-American cuisine. In Italy, they are commonly known as "cannoli siciliani", "Sicilian cannoli".
Cannoli consist of tube-shaped shells of fried pastry dough, filled with a sweet, creamy filling usually containing ricotta. They range in size from "cannulicchi", no bigger than a finger, to the fist-sized proportions typically found south of Palermo, Sicily, in Piana degli Albanesi.
History.
Cannoli have been traced to the Arabs during the Emirate of Sicily, with a possible origin for the word and recipe deriving directly from "qanawāt". These were deep fried dough tubes filled with various sweets, which were a popular pastry across the Islamic world at the time, from Al-Andalus to Iraq and including Sicily. Cannoli come from the Palermo and Messina areas and were historically prepared as a treat during Carnevale season, possibly as a fertility symbol; one legend assigns their origin to the harem of Caltanissetta. The dessert eventually became a year-round staple throughout Italy.
Variants.
The versions with which Americans are most familiar tend to involve variations on the original concept. This is possibly due to adaptations made by Italians who immigrated to the United States in the early 1900s and discovered limited availability of certain ingredients. The cannoli sold in Italian-American bakeries today usually still contain ricotta, but mascarpone is a less common alternative. Rarely, the filling is a simple custard of sugar, milk, and cornstarch. In either case, the cream is often flavored with vanilla or orange flower water and a small amount of cinnamon. Chopped pistachios, semi-sweet chocolate pieces, and candied citrus peel or cherries are often still included, dotting the open ends of the pastries.
In some Italian-American families a variant using pizzelle cookies as a shell instead of fried dough has been popular since the 1930s. This variant comes from the regions of Lazio and Abruzzo close to the Monte Cassino which is the area attributed to the development of the pizzelle.
In this variant, the cookies while still hot from the press are wrapped around a dowel or other handy round item like a rolling pin, to form the tube and allowed to cool at which point the cookie hardens. This is then filled with a sweetened ricotta filling.

</doc>
<doc id="40333" url="http://en.wikipedia.org/wiki?curid=40333" title="Magnolia">
Magnolia

Magnolia is a large genus of about 210 flowering plant species in the subfamily Magnolioideae of the family Magnoliaceae. It is named after French botanist Pierre Magnol.
"Magnolia" is an ancient genus. Appearing before bees did, the flowers are theorized to have evolved to encourage pollination by beetles. To avoid damage from pollinating beetles, the carpels of "Magnolia" flowers are extremely tough. Fossilised specimens of "M. acuminata" have been found dating to 20 million years ago, and of plants identifiably belonging to the Magnoliaceae date to 95 million years ago. Another aspect of "Magnolia" considered to represent an ancestral state is the flower bud is enclosed in a bract rather than in sepals; the perianth parts are undifferentiated and called tepals rather than distinct sepals and petals. "Magnolia" shares the tepal characteristic with several other flowering plants near the base of the flowering plant lineage such as "Amborella" and "Nymphaea" (as well as with many more recently derived plants such as "Lilium").
The natural range of "Magnolia" species is a disjunct distribution, with a main centre in east and southeast Asia and a secondary centre in eastern North America, Central America, the West Indies, and some species in South America.
Description.
As with all Magnoliaceae the perianth is undifferentiated, with 9–15 tepals in 3 or more whorls. The flowers are bisexual with numerous adnate carpels and stamens are arranged in a spiral fashion on the elongated receptacle. The fruit dehisces along their dorsal sutures. The pollen is monocolpate, and the embryo development is of the Polygonium type.(Kapil 1964)(Xu and Rudall 2006)
Taxonomy.
History.
Early.
The name "Magnolia" first appeared in 1703 in the "Genera" of Charles Plumier (1646–1704), for a flowering tree from the island of Martinique ("talauma"). English botanist William Sherard, who studied botany in Paris under Joseph Pitton de Tournefort, a pupil of Magnol, was most probably the first after Plumier to adopt the genus name "Magnolia". He was at least responsible for the taxonomic part of Johann Jacob Dillenius's "Hortus Elthamensis" and of Mark Catesby's "Natural History of Carolina, Florida and the Bahama Islands". These were the first works after Plumier's "Genera" that used the name "Magnolia", this time for some species of flowering trees from temperate North America. The species that Plumier originally named "Magnolia" was later described as "Annona dodecapetala" by Lamarck, and has since been named "Magnolia plumieri" and "Talauma plumieri" (and still a number of other names) but is now known as "Magnolia dodecapetala".
Carolus Linnaeus, who was familiar with Plumier's "Genera", adopted the genus name "Magnolia" in 1735 in his first edition of "Systema Naturae", without a description, but with a reference to Plumier's work. In 1753, he took up Plumier's "Magnolia" in the first edition of "Species Plantarum". There he described a monotypic genus, with the sole species being "Magnolia virginiana". Since Linnaeus never saw a herbarium specimen (if there ever was one) of Plumier's "Magnolia" and had only his description and a rather poor picture at hand, he must have taken it for the same plant which was described by Catesby in his 1730 "Natural History of Carolina". He placed it in the synonymy of "Magnolia virginiana" var. "fœtida", the taxon now known as "Magnolia grandiflora". Under "Magnolia virginiana" Linnaeus described five varieties ("glauca", "fœtida", "grisea", "tripetala", and "acuminata"). In the tenth edition of "Systema Naturae" (1759), he merged "grisea" with "glauca", and raised the four remaining varieties to species status.
By the end of the 18th century, botanists and plant hunters exploring Asia began to name and describe the "Magnolia" species from China and Japan. The first Asiatic species to be described by western botanists were "Magnolia denudata" and "Magnolia liliiflora", and "Magnolia coco" and "Magnolia figo". Soon after that, in 1794, Carl Peter Thunberg collected and described "Magnolia obovata" from Japan and at roughly the same time "Magnolia kobus" was also first collected.
Recent.
With the number of species increasing, the genus was divided into the two subgenera "Magnolia" and "Yulania". "Magnolia" contains the American evergreen species "M. grandiflora", which is of horticultural importance, especially in the southeastern United States, and "M. virginiana", the type species. "Yulania" contains several deciduous Asiatic species, such as "M. denudata" and "M. kobus", which have become horticulturally important in their own right and as parents in hybrids. Classified in "Yulania", is also the American deciduous "M. acuminata" (cucumber tree), which has recently attained greater status as the parent responsible for the yellow flower colour in many new hybrids.
Relations in the family Magnoliaceae have been puzzling taxonomists for a long time. Because the family is quite old and has survived many geological events (such as ice ages, mountain formation, and continental drift), its distribution has become scattered. Some species or groups of species have been isolated for a long time, while others could stay in close contact. To create divisions in the family (or even within the genus "Magnolia"), solely based upon morphological characters, has proven to be a nearly impossible task.
Phylogenetic era.
By the end of the 20th century, DNA sequencing had become available as a method of large-scale research on phylogenetic relationships. Several studies, including studies on many species in the family Magnoliaceae, were carried out to investigate relationships. What these studies all revealed was that genus "Michelia" and "Magnolia" subgenus "Yulania" were far more closely allied to each other than either one of them was to "Magnolia" subgenus "Magnolia". These phylogenetic studies were supported by morphological data.
As nomenclature is supposed to reflect relationships, the situation with the species names in "Michelia" and "Magnolia" subgenus "Yulania" was undesirable. Taxonomically, three choices are available: 1 to join "Michelia" and "Yulania" species in a common genus, not being "Magnolia" (for which the name "Michelia" has priority), 2 to raise subgenus "Yulania" to generic rank, leaving "Michelia" names and subgenus "Magnolia" names untouched, or 3 to join "Michelia" with genus "Magnolia" into genus "Magnolia" s.l. (a big genus). "Magnolia" subgenus "Magnolia" cannot be renamed because it contains "M. virginiana", the type species of the genus and of the family.
Not many "Michelia" species have so far become horticulturally or economically important, apart for their wood. Both subgenus "Magnolia" and subgenus "Yulania" include species of major horticultural importance, and a change of name would be very undesirable for many people, especially in the horticultural branch. In Europe, "Magnolia" even is more or less a synonym for "Yulania", since most of the cultivated species on this continent have "Magnolia (Yulania) denudata" as one of their parents. Most taxonomists who acknowledge close relations between "Yulania" and "Michelia" therefore support the third option and join "Michelia" with "Magnolia".
The same goes, "mutatis mutandis", for the (former) genera "Talauma" and "Dugandiodendron", which are then placed in subgenus "Magnolia", and genus "Manglietia", which could be joined with subgenus "Magnolia" or may even earn the status of an extra subgenus. "Elmerrillia" seems to be closely related to "Michelia" and "Yulania", in which case it will most likely be treated in the same way as "Michelia" is now. The precise nomenclatural status of small or monospecific genera like "Kmeria", "Parakmeria", "Pachylarnax", "Manglietiastrum", "Aromadendron", "Woonyoungia", "Alcimandra", "Paramichelia" and "Tsoongiodendron" remains uncertain. Taxonomists who merge "Michelia" into "Magnolia" tend to merge these small genera into "Magnolia" s.l. as well. Botanists do not yet agree on whether to recognize a big "Magnolia" genus or the different small genera. For example, "Flora of China" offers two choices: a large "Magnolia" which includes about 300 species, everything in the Magnoliaceae except "Liriodendron" (tulip tree), or 16 different genera, some of them recently split out or re-recognized, each of which contains up to 50 species. The western co-author favors the big "Magnolia" genus, whereas the Chinese co-authors recognize the different small genera.
Subdivision.
Species of Magnolia are most commonly listed under three subgenera, 12 sections, and 13 subsections., such as that used here, following the classification of the Magnolia Society. It does not represent the last word on the subclassification of the genus "Magnolia" (see above), as a clear consensus has not yet been reached. Each species entry follows this pattern: "Botanical name" - common name(s), if any 
The subdivision structure is as follows:
Subgenus "Magnolia".
Anthers open by splitting at the front facing the centre of the flower, deciduous or evergreen, flowers produced after the leaves.
Subgenus "Yulania".
Anthers open by splitting at the sides, deciduous, flowers mostly produced before leaves (except "M. acuminata")
Etymology.
Charles Plumier (1646–1704) described a flowering tree from the island of Martinique in his "Genera", giving it the name "Magnolia", after the French botanist Pierre Magnol.
Uses.
Horticultural uses.
In general, the "Magnolia" genus has attracted horticultural interest. Some, such as the star magnolia ("M. stellata") and the saucer magnolia ("Magnolia × soulangeana") flower quite early in the spring, before the leaves open. Others flower in late spring or early summer, including the sweetbay magnolia ("M. virginiana") and the southern magnolia ("M. grandiflora").
Hybridisation has been immensely successful in combining the best aspects of different species to give plants which flower at an earlier age than the parent species, as well as having more impressive flowers. One of the most popular garden magnolias, saucer magnolia ("Magnolia × soulangeana"), is a hybrid of "M. liliiflora" and "M. denudata".
In the eastern United States, five native species are frequently in cultivation: "M. acuminata" (as a shade tree), "M. grandiflora", "M. virginiana", "M. tripetala", and "M. macrophylla". The last two species must be planted where high winds are not a frequent problem because of the size of their leaves.
Traditional medicine.
The bark and flower buds of "M. officinalis" have long been used in traditional Chinese medicine, where they are known as "hou po" (厚朴). In Japan, "kōboku", "M. obovata", has been used in a similar manner.
Timber.
The cucumbertree, "M. acuminata", grows to large size and is harvested as a timber tree in northeastern US forests. Its wood is sold as "yellow poplar" along with that of the tuliptree, "Liriodendron tulipifera". The Fraser magnolia, "M. fraseri", also attains enough size sometimes to be harvested, as well.
Other uses.
In parts of Japan, the leaves of "M. obovata" are used for wrapping food and as cooking dishes.
Magnolias are used as food plants by the larvae of some Lepidoptera species, including the giant leopard moth.
Chemical compounds and bioeffects.
The aromatic bark contains magnolol, honokiol, 4-O-methylhonokiol, and obovatol. Magnolol and honokiol activate the nuclear receptor peroxisome proliferator-activated receptor gamma.
Culture.
Arts.
Visual arts.
The Canadian artist, Sarah Maloney, has created a series of sculptures of Magnolia flowers in bronze and steel, entitled "First Flowers", in which she draws our attention to the dual symbols of beginnings in the flower, as both an evolutionary archetype and also one of the first trees to flower in spring.
See also.
List of AGM magnolias
References.
Bibliography.
</dl>

</doc>
<doc id="40335" url="http://en.wikipedia.org/wiki?curid=40335" title="Salzburg">
Salzburg

 - Unknown metadata keyword: 
}}, 2014-01-01 - Unknown metadata keyword: date
}} - Unknown metadata keyword: 
}}.</ref>
}}.</ref>
 (UTC+1)
 (UTC+2)
Salzburg (]; Bavarian: "Såizburg"; literally: "Salt Fortress") is the fourth-largest city in Austria and the capital of the federal state of Salzburg.
Salzburg's "Old Town" ("Altstadt") is internationally renowned for its baroque architecture and is one of the best-preserved city centers north of the Alps. It was listed as a UNESCO World Heritage Site in 1997. The city has three universities and a large population of students. Tourists also frequent the city to tour the city's historic center, many palaces, and the scenic Alpine surroundings.
Salzburg was the birthplace of 18th-century composer Wolfgang Amadeus Mozart. In the mid‑20th century, the city was the setting for the musical play and film "The Sound of Music".
History.
Antiquity to the High Middle Ages.
Traces of human settlements have been found in the area, dating to the Neolithic Age. The first settlements in Salzburg continuous with the present were apparently by the Celts around the 5th century BC.
Around 15 BC the separate settlements were merged into one city by the Roman Empire. At this time, the city was called "Juvavum" and was awarded the status of a Roman "municipium" in 45 AD. Juvavum developed into an important town of the Roman province of Noricum. After the collapse of the Norican frontier, Juvavum declined so sharply that by the late 7th century it nearly became a ruin.
The "Life of Saint Rupert" credits the 8th-century saint with the city's rebirth. When Theodo of Bavaria asked Rupert to become bishop c. 700, Rupert reconnoitered the river for the site of his basilica. Rupert chose Juvavum, ordained priests, and annexed the manor Piding. Rupert named the city "Salzburg". He traveled to evangelise among pagans.
The name Salzburg means "Salt Castle" (Latin:"Salis Burgium"). The name derives from the barges carrying salt on the Salzach River, which were subject to a toll in the 8th century and was customary for many communities and cities on European rivers. The Festung Hohensalzburg, the city's fortress, was built in 1077 by Archbishop Gebhard, who made it his residence. It was greatly expanded during the following centuries.
Independence of Salzburg.
Independence from Bavaria was secured in the late 14th century. Salzburg was the seat of the Archbishopric of Salzburg; a prince-bishopric of the Holy Roman Empire. As the reformation movement gained steam, riots broke out among peasants in the areas in and around Salzburg. The city was occupied during the German Peasants' War, and the archbishop had to flee to the safety of the fortress It was besieged for three months in 1525.
Eventually, tensions were quelled, and the independence of the city led to an increase in wealth and prosperity, culminating in the late 16th to 18th centuries under the Prince Archbishops Wolf Dietrich von Raitenau, Markus Sittikus, and Paris Lodron. It was in the 17th century that Italian architects (and Austrians who had studied the Baroque style) rebuilt the city center as it is today along with many palaces.
Modern era.
Religious conflict.
On 31 October 1731, the 214th anniversary of Martin Luther's reformation, Archbishop Count Leopold Anton von Firmian signed an Edict of Expulsion, the "Emigrationspatent", directing all Protestant citizens to recant their non‑Catholic beliefs. There were 21,475 citizens who publicly listed themselves as Protestant and refused to recant. They were all exiled, not being able to return until 1734.
The exodus began in November, and the Protestants were forced to walk through the winter, seeking refuge in Germany. A significant number died, and some children were kidnapped to be raised by Catholics. Stories of their plight spread, eventually even inspiring the German poet Johann Wolfgang Goethe to write the narrative poem "Hermann and Dorothea," which, though it frames the story in terms of the French Revolution, is a commentary on the Salzburg exile.
In early 1732 King Frederick William I of Prussia accepted 12,000 Salzburger Protestant emigrants, who settled in towns of East Prussia that had been devastated by the plague twenty years before. Other smaller groups made their way to Debrecen and the Banat regions of the Kingdom of Hungary, to what is now part of Romania and Serbia. The Kingdom of Hungary recruited Germans to repopulate areas along the Danube River decimated by the plague and the Ottoman invasion. The Salzburgers also migrated to Protestant areas near Berlin and Hanover in Germany and to the Netherlands.
The Protestant German refugees went to western Europe, the United States and other western nations. Those who settled in western Germany founded a community association to preserve their historic identity as Salzburgers.
Illuminism.
In 1772-1803, under archbishop Hieronymus Graf von Colloredo, Salzburg was a centre of late Illuminism.
Electorate of Salzburg.
In 1803, the archbishopric was secularised by Emperor Napoleon and handed over to Ferdinand III of Tuscany, former Grand Duke of Tuscany, as the Electorate of Salzburg.
Austrian annexation of Salzburg.
In 1805, Salzburg was annexed to the Austrian Empire, along with the Berchtesgaden Provostry.
Salzburg under Bavarian rule.
In 1809, the territory of Salzburg was transferred to the Kingdom of Bavaria after Austria's defeat at Wagram.
Division of Salzburg and annexation by Austria and Bavaria.
At the Congress of Vienna in 1815, it was definitively returned to Austria, but without Rupertigau and Berchtesgaden, which remained with Bavaria. Salzburg was integrated into the Salzach province and Salzburgerland was ruled from Linz. In 1850, Salzburg's status was once more restored as the capital of the Duchy of Salzburg, a crownland of the Austrian Empire. The city became part of Austria-Hungary in 1866 as the capital of a crownland into the Austrian Empire. The nostalgia of the Romantic Era led to increased tourism. In 1892, a funicular was installed to facilitate tourism to the fortress of Hohensalzburg
20th century.
First republic.
Following World War I and the dissolution of the Austro-Hungarian Empire; Salzburg, as the capital of one of the Austro-Hungarian territories, became part of the new German Austria. In 1918, it represented the residual German-speaking territories of the Austrian heartlands. This was replaced by the First Austrian Republic in 1919, after the Treaty of Versailles.
Annexation by German Third Reich.
The Anschluss (the occupation and annexation of Austria, including Salzburg, into German Third Reich) took place the 12th of March 1938, one day before a scheduled referendum about Austria's independence. German troops were moved to the city. Political opponents, Jewish citizens and other minorities were subsequently arrested and deported. The synagogue was destroyed and several POW camps for prisoners from the Soviet Union and other nations were organized in the city.
During the Nazi tenure a Roma camp was built in Salzburg-Maxglan. It was an Arbeitserziehungslager (work education camp), which provided slave labour to local industry, as well a Zwischenlager (transit camp) before the deportation to German extermination camps or ghettos in German occupied territories.
World War II.
Allied bombing destroyed 7,600 houses and killed 550 inhabitants. A total of 15 strikes destroyed 46 percent of the city's buildings especially around Salzburg train station. Although the town's bridges and the dome of the cathedral were destroyed, much of its Baroque architecture remained intact. As a result, it is one of the few remaining examples of a town of its style. American troops entered Salzburg on 5 May 1945.
In the city of Salzburg, there were several DP Camps following World War II. Among these were Riedenburg, Camp Herzl (Franz-Josefs-Kaserne), Camp Mülln, Bet Bialik, Bet Trumpeldor, and New Palestine. Salzburg was the centre of the American-occupied area in Austria.
Present day.
After World War II, Salzburg became the capital city of the State of Salzburg ("Land Salzburg"). On 27 January 2006, the 250th anniversary of the birth of Wolfgang Amadeus Mozart, all 35 churches of Salzburg rang their bells a little after 8:00 p.m. (local time) to celebrate the occasion. Major celebrations took place throughout the year.
Geography.
Salzburg is on the banks of the Salzach River, at the northern boundary of the Alps. The mountains to Salzburg's south contrast with the rolling plains to the north. The closest alpine peak, the 1,972‑metre-high Untersberg, is less than 16 km from the city centre. The "Altstadt", or "old town", is dominated by its baroque towers and churches and the massive "Festung Hohensalzburg". This area is surrounded by two smaller mountains, the "Mönchsberg" and "Kapuzinerberg", which offer green relief within the city. Salzburg is approximately 150 km east of Munich, 281 km northwest of Ljubljana, Slovenia, and 300 km west of Vienna.
Climate.
Salzburg is part of the temperate zone. The Köppen climate classification specifies the climate as continental (Dfb). Due to the location at the northern rim of the Alps, the amount of precipitation is comparatively high, mainly in the summer months. The specific drizzle is called "Schnürlregen" in the local dialect. In winter and spring, pronounced foehn winds regularly occur.
Population development.
Salzburg's official population significantly increased in 1935 when the city absorbed adjacent municipalities. After World War II, numerous refugees found a new home in the city. New residential space was constructed for American soldiers of the postwar occupation, and could be used for refugees when they left. Around 1950, Salzburg passed the mark of 100,000 citizens, and in 2006, it reached the mark of 150,000 citizens. In the agglomeration, there was a population of about 210,000 in 2007.
Architecture of Salzburg.
Romanesque and Gothic.
The Romanesque and Gothic churches, the monasteries and the early carcass houses dominated the medieval city for a long time. The Cathedral of Archbishop Conrad of Wittelsbach was the largest basilica in the north of the Alps. The choir of the , construction began by Hans von Burghausen and completed by , was one of the most prestigious religious gothic constructions of southern Germany. At the end of the Gothic era the collegiate church “Nonnberg”, Margaret Chapel in St. Peter's Cemetery, the St. George's Chapel and the stately halls of the “Hoher Stock” in the Hohensalzburg Castle were constructed.
Renaissance and baroque.
Inspired by Vincenzo Scamozzi, Prince Archbishop Wolf Dietrich von Raitenau began transforming a medieval town to the architectural ideals of the late Renaissance. Plans for a massive cathedral by Scamozzi failed to materialize upon the fall of the archbishop. A second cathedral planned by Santino Solari rose as the first early Baroque church in Salzburg. It served as an example for many other churches in Southern Germany and Austria. and continued the rebuilding of the city with major projects such as Hellbrunn Palace, the prince archbishop's residence, the university buildings, fortifications, and many other buildings. Giovanni Antonio Daria managed by order of Prince Archbishop Guido von Thun the construction of the residential well. Giovanni , by order of the same archbishop, created the Erhard and the Kajetan church in the south of the town. The redesign of the city was completed with buildings designed by Johann Bernhard Fischer von Erlach, donated by Prince Archbishop Johann Ernst von Thun. After the era of Ernst von Thun the expansion of the city came to a halt, which is the reason why there are no churches built in the rococo style. Sigismund von Schrattenbach continued with the construction of “Sigmundstor” and the statue of holy Maria on the cathedral square. With the fall and division of the former “Fürsterzbistums Salzburg” (Archbishopric) to Upper Austria, Bavaria (Rupertigau) and Tyrol (Zillertal Matrei) began a long period of urban stagnancy. This era didn't end before the period of promoterism ("Gründerzeit") brought new life into urban development. The builder dynasty and filled major positions in shaping the city in this era.
Classical modernism and post-war modernism.
Buildings of classical modernism and in particular the post-war modernism are frequently encountered in Salzburg. Examples are the Zahnwurzen house (a house in the Linzergasse 22 in the right center of the old town), the “Lepi” (a public baths in "Leopoldskron") (built 1964) and the original 1957 constructed congress center of Salzburg, which got replaced by a new building in 2001. An important and famous example of architecture of this era is the 1960 opening of the Großes Festspielhaus by Clemens Holzmeister.
Contemporary architecture.
Adding contemporary architecture to Salzburg's old town without risking its UNESCO World Heritage status is problematic. Yet some new structures have been added: the Mozarteum at the baroque Mirabell garden (Architecture Robert Rechenauer), the 2001 Congress house (Architecture: Freemasons), the 2011 Unipark Nonntal (Architecture: Storch Ehlers partners), the 2001 “Makartsteg” bridge (Architecture: HALLE1), and the “Residential and studio house” of the architects and in the middle of Salzburg's old town (winner of the ). Other examples of contemporary architecture exist outside the old town: the Faculty of Science building (Universität Salzburg – Architecture ) built on the edge of free green space, the blob architecture of Red Bull Hangar‑7 (Architecture: Volkmar Burgstaller) at Salzburg Airport, home to Dietrich Mateschitz's Flying Bulls and the Europark shopping mall. (Architecture: Massimiliano Fuksas)
Districts.
Salzburg has twenty-four urban districts and three extra-urban populations.
Urban districts ("Stadtteile"):
Extra-urban populations ("Landschaftsräume"):
Main sights.
Salzburg is a tourist favorite, with the number of tourists outnumbering locals by a large margin in peak times. In addition to Mozart's birthplace noted above, other notable places include:
Old Town
Outside the Old Town
Greater Salzburg area
Education.
Salzburg is a centre of education and home to three universities, as well as several professional colleges and gymnasiums (high schools).
Transport.
The city is served by comprehensive rail connections, with frequent east-west trains serving Vienna, Munich, Innsbruck, and Zürich, including daily high-speed ICE services. The city acts as a hub for south-bound trains through the Alps into Italy.
Salzburg Airport has scheduled flights to European cities such as Frankfurt, Vienna, London, Rotterdam, Amsterdam, Brussels, Düsseldorf, and Zürich, as well as Hamburg and Dublin. In addition to these, there are numerous charter flights.
In the main city, there is the Salzburg trolleybus system and bus system with a total of more than 20 lines, and service every 10 minutes. Salzburg has an S-Bahn system with four Lines (S1, S2, S3, S11), trains depart from the main station every 30 minutes, and they are part of the ÖBB network. Suburb line number S1 reaches the world famous Silent Night chapel in Oberndorf in about 25 minutes.
Popular culture.
In the 1960s, the movie "The Sound of Music" used some locations in and around Salzburg and the state of Salzburg. The movie was based on the true story of Maria von Trapp who took up with an aristocratic family and fled the German Anschluss. Although the film is not particularly popular nor well known among Austrians, the town draws many visitors who wish to visit the filming locations, alone or on tours.
Salzburg is the setting for the Austrian crime series Stockinger.
In the 2010 film "Knight & Day", Salzburg serves as the backdrop for a large portion of the film.
Language.
Austrian German is widely written. Austro-Bavarian is the German dialect of this territory and widely spoken.
Sports.
The former SV Austria Salzburg reached the UEFA Cup final in 1994. On 6 April 2005 Red Bull bought the club and changed the name into FC Red Bull Salzburg. The home stadium of Red Bull Salzburg is the Wals Siezenheim Stadium in a suburb in the agglomeration of Salzburg and was one of the venues for the 2008 European Football Championship.
Red Bull also sponsors the local ice hockey team, the EC Salzburg Red Bulls. They play in the Erste Bank Eishockey Liga, an Austria-headquartered crossborder league featuring the best teams from Austria, Hungary, Slovenia and Croatia, as well as one Czech team.
Salzburg was a candidate city for the 2010 & 2014 Winter Olympics, but lost to Vancouver and Sochi respectively.
International relations.
Twin towns—sister cities.
 Reims, France, since 1964
Gallery.
A night time long exposure of Salzburg
Salzburg old town with a typical narrow alleyway
Salzburg Altstadt Panorama
Salzburg panorama as seen from the Hohensalzburg Castle

</doc>
<doc id="40336" url="http://en.wikipedia.org/wiki?curid=40336" title="Rhododendron">
Rhododendron

Rhododendron (from Ancient Greek ῥόδον "rhódon" "rose" and δένδρον "déndron" "tree") is a genus of 1,024 species of woody plants in the heath family (Ericaceae), either evergreen or deciduous, and found mainly in Asia, although it is also widespread throughout the Southern Highlands of the Appalachian Mountains of North America. It is the national flower of Nepal. Most species have showy flowers. Azaleas make up two subgenera of "Rhododendron". They are distinguished from "true" rhododendrons by having only five anthers per flower.
Description.
"Rhododendron" is a genus characterised by shrubs and small to (rarely) large trees, the smallest species growing to 10 - tall, and the largest, "R. protistum var. giganteum", reported to 30 m tall. The leaves are spirally arranged; leaf size can range from 1 - to over 50 cm, exceptionally 100 cm in "R. sinogrande". They may be either evergreen or deciduous. In some species, the undersides of the leaves are covered with scales (lepidote) or hairs (indumentum). Some of the best known species are noted for their many clusters of large flowers. There are alpine species with small flowers and small leaves, and tropical species such as section "Vireya" that often grow as epiphytes. Species in this genus may be part of the heath complex in oak-heath forests in eastern North America. They have frequently been divided based on the presence or absence of scales on the abaxial (lower) leaf surface (lepidote or elepidote). These scales, unique to subgenus "Rhododendron", are modified hairs consisting of a polygonal scale attached by a stalk.
"Rhododendron" are characterised by having inflorescences with scarious (dry) perulae, a chromosome number of x=13, fruit that has a septicidal capsule, an ovary that is superior (or nearly so), stamens that have no appendages, and agglutinate (clumped) pollen.
Taxonomy.
The "Rhododendron" genus is the largest of the genera in the Ericaceae family, with 1,024 species, though estimates vary from 850-1000 depending on the authority used, (Fayaz 2012) and is morphologically diverse. Consequently the taxonomy has been historically complex.
Early history.
Although Rhododendrons had been known since the description of "Rhododendron hirsutum" by Charles de l'Écluse (Clusius) in the sixteenth century, and were known to classical writers (Magor 1990), and referred to as "Chamaerhododendron" (low-growing rose tree), the genus was first formally described by Linnaeus in his Species Plantarum in 1753. He listed five species under "Rhododendron" ("Rhododendron ferrugineum" (type species), "R. dauricum", "R. hirsutum", "R. chamaecistus" (now "Rhodothamnus chamaecistus" (L.) Rchb.) and "R. maximum"). At that time he considered the then known six species of "Azalea" that he had described earlier in 1735 in his Systema Naturae as a separate genus.
Linnaeus' six species of Azalea were "Azalea indica", "A. pontica", "A. lutea", "A. viscosa", "A. lapponica" and "A. procumbens" (now "Kalmia procumbens"), which he distinguished from "Rhododendron" by having five stamens, as opposed to ten. As new species of what are now considered "Rhododendron" were discovered, if they seemed to differ significantly from the type species they were assigned to separate genera. For instance "Rhodora" for "Rhododendron canadense" (Linnaeus 1763), "Vireya" (Blume 1826) and "Hymenanthes" for "Rhododendron metternichhii", now R. degronianum (1826). Meanwhile other botanists such as Salisbury (1796) and Tate (1831) began to question the distinction between "Azalea" and "Rhododendron", and finally in 1836, "Azalea" was incorporated into "Rhododendron" (Don 1834) and the genus divided into eight sections. Of these "Tsutsutsi" ("Tsutsusi"), "Pentanthera", "Pogonanthum", "Ponticum" and "Rhodora" are still used, the other sections being "Lepipherum", "Booram", and "Chamaecistus". This structure largely survived till recently (2004), following which the development of molecular phylogeny led to major re-examinations of traditional morphological classifications, although other authors such as Candolle (1838), who described six sections, used slightly different numeration.
As more species became available in the nineteenth century a better understanding of the characteristics necessary for the major divisions. Chief amongst these were Maximovicz's "Rhododendreae Asiae Orientali" (1870) and Planchon. Maximovicz used flower bud position and its relationship with leaf buds to create eight Sections. Bentham and Hooker (1876) used a similar scheme, but called the divisions Series. It was not until 1893 that Koehne appreciated the significance of scaling and hence the separation of lepidote and elepidote species. The large number of species that were available by the early twentieth century prompted a new approach when Balfour introduced the concept of grouping species into series, in "The Species of Rhododendron" (1930), referred to as the Balfourian system. That system continued up to modern times in Davidian's four volume "The Rhododendron Species" (1982-1995).
Modern era.
The next major attempt at classification was by Sleumer who from 1934 began incorporating the Balfourian series into the older hierarchical structure of subgenera and sections, according to the International Code of Botanical Nomenclature, culminating in his "Ein System der Gattung Rhododendron L." (1949), and subsequent refinements. Most of the Balfourian series are represented by Sleumer as subsections, though some appear as sections or even subgenera. Sleumer based his system on the relationship of the flower buds to the leaf buds, habitat, flower structure, and whether the leaves were lepidote or non-lepidote. While Sleumer's work was widely accepted, many in the United States and the United Kingdom continued to use the simpler Balfourian system of the Edinburgh group.
Sleumer's system underwent many revisions by others, predominantly the Edinburgh group in their continuing Royal Botanic Garden Edinburgh notes. Cullen (1980) in Edinburgh, placing more emphasis on the lepidote characteristics of the leaves united all of the lepidote species into subgenus "Rhodendron", including four of Sleumer's (1980) subgenera ("Rhododendron", "Pseudoazalea", "Pseudorhodorastrum", "Rhodorastrum"). Philipson & Philipson (1986) raised two sections of subgenus "Aleastrum" ("Mumeazalea", "Candidastrum") to subgenera, while reducing genus "Therorhodion" to a subgenus of "Rhododendron". In 1987 Spethmann, adding phytochemical features proposed a system with fifteen subgenera grouped into three 'chorus' subgenera.
A number of closely related genera had been included together with "Rhododendron" in a former tribe, Rhodoreae. These have been progressively incorporated into "Rhododendron". Chamberlain and Rae (1990) moved the monotypic section "Tsusiopsis" together with the monotypic genus "Tsusiophyllum" into section "Tsutsusi", while in the same year Kron & Judd reduced genus "Ledum" to a subsection of section "Rhododendron". Then Judd & Kron (1995) moved two species ("Rhododendron schlippenbachii", "R. quinquefolium") from section "Brachybachii" subgenus "Tsutsusi" and two from section "Rhodora" subgenus "Pentanthera" ("R. albrechti"i, "R. pentaphyllu"m) into section "Sciadorhodion" subgenus "Pentanthera". Finally Chamberlain brought the various systems together in 1996, with 1,025 species divided into eight subgenera. For a comparison of the Sleuner and Chamberlain schemata see Table 1 of Goetsch (2005).
Phylogenetic analyses.
The era of molecular analysis rather than descriptive features can be dated to the work of Kurashige (1988) and Kron (1997) who used matK sequencing, while Lian-Ming used ITS sequences to determine a cladistic analysis. They confirmed that the genus "Rhodendron" was monophyletic, with subgenus "Therorhodion" in the basal position, consistent with the "mat"K studies. Following publication of the studies of Goetsch "et al." with RPB2 (2005). there began an ongoing realignment of species and groups within the genus, based on evolutionary relationships. Their work was more supportive of Sleumer's original system than the later modifications introduced by Chamberlain "et al.".
The major finding of Goetsch and colleagues was that all species examined (except "R. camtschaticum", subgenus "Therorhodion") formed three major clades which they labelled A, B and C, with the subgenera "Rhododendron" and "Hymenanthes" nested within clades A and B as monophyletic groups respectively. By contrast subgenera "Azaleastrum" and "Pentanthera"
were polyphyletic, while "R. camtschaticum" appeared as a sister to all other rhododendrons. The small polyphyletic subgenera "Pentanthera" and "Azaleastrum" were divided between two clades. The four sections of "Pentanethra" between clades B and C, with two each, while "Azaleastrum" had one section in each of A and C.
Thus subgenera "Azaleastrum" and "Pentanethera" needed to be dissassembled, and "Rhododendron", "Hymenanthes" and "Tsutsusi" correspondingly expanded. In addition to the two separate genera included under "Rhododendron" by Chamberlain ("Ledum", "Tsusiophyllum"), Goetsch "et al". added "Menziesia" (Clade C). Despite a degree of paraphyly, the subgenus "Rhodendron" was otherwise untouched with regard to its three sections but four other subgenera were eliminated and one new subgenus created, leaving a total of five subgenera in all, from eight in Chamberlain's scheme. The discontinued subgenera are "Pentanethera", "Tsutsusi", "Candidastrum" and "Mumeazalea", while a new subgenus was created by elevating subgenus "Azaleastrum" section "Choniastrum" to subgenus rank.
Subgenus "Pentanethera" (deciduous azaleas) with its four sections was dismembered by eliminating two sections and redistributing the other two between the existing subgenera in clades B ("Hymenanthes") and C ("Azaleastrum"), although the name was retained in section "Pentanethera" (14 species) which was moved to subgenus "Hymenanthes". Of the remaining three sections, monotypic "Viscidula" was discontinued by moving "Rhododendron nipponicum" to "Tsutsusi" (C), while "Rhodora" (2 species) was itself polyphyletic and was broken up by moving "Rhododendron canadense" to section "Pentanethera" (B) and "Rhododendron vaseyi" to section "Sciadorhodion", which then became a new section of subgenus "Azaleastrum" (C).
Subgenus "Tsutsusi" (C) was reduced to section status retaining the name, and included in subgenus "Azaleastrum". Of the three minor subgenera, all in C, two were discontinued. The single species of monotypic subgenus "Candidastrum " ("Rhododendron albiflorum") was moved to subgenus "Azaleastrum", section "Sciadorhodion". Similarly the single species in monotypic subgenus "Mumeazalea" (Rhododendron semibarbatum) was placed in the new section "Tsutsusi", subgenus "Azaleastrum". Genus "Menziesa" (9 species) was also added to section "Sciadorhodion". The remaining small subgenus "Therorhodion" with its two species was left intact. Thus two subgenera, "Hymenanthes" and "Azaleastrum" were expanded at the expense of four subgenera that were eliminated, although "Azaleastrum" lost one section ("Choniastrum") as a new subgenus, since it was a distinct subclade in A. In all, "Hymenanthes" increased from one to two sections, while "Azaleastrum", by losing one section and gaining two increased from two to three sections. (See schemata under Subgenera). (Table 1.)
Subsequent research has supported the revision by Goetsch, although has largely concentrated on further defining the phylogeny within the subdivisions.(Craven 2008) In 2011 the two species of "Diplarche" were also added to "Rhododendron", "incertae sedis".(Craven 2011) Similar findings were reported independently the following year by Brown "et al".
Subdivision.
This genus has been progressively subdivided into a hierarchy of subgenus, section, subsection, and species.
Subgenera.
Terminology from the Sleumer (1949) system is frequently found in older literature, with five subgenera and is as follows;
In the later traditional classification, attributed to Chamberlain (1996), and as used by horticulturalists and the American Rhododendron Society, "Rhododendron" has eight subgenera based on morphology, namely the presence of scales (lepidote), deciduousness of leaves, and the floral and vegetative branching patterns, after Sleumer (1980). These consist of four large and four small subgenera. The first two subgenera ("Rhododendron" and "Hymenanthes") represent the species commonly considered as 'Rhododendrons'. The next two smaller subgenera ("Pentanthera" and "Tsutsusi") represent the 'Azaleas'. The remaining four subgenera contain very few species. The largest of these is subgenus "Rhododendron", containing nearly half of all known species and all of the lepidote species.
For a comparison of the Sleumer and Chamberlain systems, see Goetsch et al. (2005) Table 1.
This division was based on a number of what were thought to be key morphological characteristics. These included the position of the inflorescence buds (terminal or lateral), whether lepidote or elepidote, deciduousness of leaves, and whether new foliage was derived from axils from previous year's shoots or the lowest scaly leaves (Table 2.).
Following the cladistic analysis of Goetsch "et al." (2005) this scheme was simplified, based on the discovery of three major clades (A,B,C) as follows.
Clade A
Clade B
Clade C
Sister taxon
Sections and subsections.
The larger subgenera are further subdivided into sections and subsections Some subgenera contain only a single section, and some sections only a single subsection. Shown here is the traditional classification, with species number after Chamberlain (1996), but this scheme is undergoing constant revision. Revisions by Goetsch "et al." (2005) and by Craven et al. (2008) shown in ("parenthetical italics"). Older ranks such as Series (groups of species) are no longer used but may be found in the literature, but the American Rhododendron Society still uses a similar device, called Alliances
Distribution and habitat.
Species of the genus "Rhododendron" are widely distributed between latitudes 80°N and 20°S and are considered Alpine native plants from North America to Europe, Russia, and Asia, and from Greenland to Queensland, Australia and the Solomon Islands. The centres of diversification are in the Himalayas and Malaysia, with the greatest species diversity in the Sino-Himalayan region, Southwest China and northern Burma, from Uttarakhand, Nepal and Sikkim to northwestern Yunnan and western Sichuan and southeastern Tibet, and with other significant areas of diversity in the mountains of Korea, Japan and Taiwan. More than 90% of "Rhododendron" "sensu" Chamberlain belong to the Asian subgenera "Rhododendron", "Hymenanthes" and section "Tsutsusi". Of the first two of these, the species are predominantly found in the area of the Himalayas and Southwest China (Sino-Himalayan Region).
The 300 Tropical species within the "Vereya" section of subgenus "Rhododendron" occupy the Malay archipelago from their presumed Southeast Asian origin to Northern Australia, with 55 known species in Borneo and 164 in New Guinea. The species in New Guinea are native to subalpine moist grasslands at around 3,000 metres above sea level in the Central Highlands. Subgenera "Rhododendron" and "Hymenanthes", together with section "Pentanethera" of subgenus "Pentanethera" are also represented to a lesser degree in the Mountainous areas of North America and Western Eurasia. Subgenus "Tsutsusi" is found in the maritime regions of East Asia (Japan, Korea, Taiwan, East China), but not in North America or Eurasia.
Ecology.
Invasive species.
Some species (e.g. "Rhododendron ponticum" in Ireland and the United Kingdom) are invasive as introduced plants, spreading in woodland areas replacing the natural understory. "R. ponticum" is difficult to eradicate, as its roots can make new shoots.
Insects.
A number of insects either target rhododendrons or will opportunistically attack them. Rhododendron borers and various weevils are major pests of rhododendrons, and many caterpillars will preferentially devour them.
"Rhododendron" species are used as food plants by the larvae of some members of the order Lepidoptera (butterflies and moths) (See List of Lepidoptera that feed on rhododendrons).
Diseases.
Major diseases include "Phytophthora" root rot, stem and twig fungal dieback; Ohio State University Extension provides information on maintaining health of rhododendrons. Rhododendrons can easily be suffocated by other plants.
Cultivation.
Both species and hybrid rhododendrons (including azaleas) are used extensively as ornamental plants in landscaping in many parts of the world, including both temperate and subtemperate regions,(Craven 2008) while many species and cultivars are grown commercially for the nursery trade. Rhododendrons are often valued in landscaping for their structure, size, flowers, and the fact that many of them are evergreen. Azaleas are frequently used around foundations and occasionally as hedges, and many larger-leafed rhododendrons lend themselves well to more informal plantings and woodland gardens, or as specimen plants. In some areas, larger rhododendrons can be pruned to encourage more tree-like form, with some species such as "R. arboreum" and "R. falconeri" eventually growing to 10–15 m or more tall.
Commercial growing.
Rhododendrons are grown commercially in many areas for sale, and are occasionally collected in the wild, a practice now rare in most areas. Larger commercial growers often ship long distances; in the United States, most of them are located on the west coast (Oregon, Washington state and California). Large-scale commercial growing often selects for different characteristics than hobbyist growers might want, such as resistance to root rot when overwatered, ability to be forced into budding early, ease of rooting or other propagation, and saleability.
In the Indian state of Himachal Pradesh, rhododendron flowers have been used for some time to make popular fruit and flower wines. The industry is promoted by the state government with tax benefits, looking to promote this industry as a full-fledged subclass of its economy.
Horticultural divisions.
Horticulturally, rhododendrons may be divided into the following groups:-
Planting and care.
Like other ericaceous plants, most rhododendrons prefer acid soils with a pH of roughly 4.5-5.5; some tropical Vireyas and a few other rhododendron species grow as epiphytes and require a planting mix similar to orchids. Rhododendrons have fibrous roots and prefer well-drained soils high in organic material. In areas with poorly drained or alkaline soils, rhododendrons are often grown in raised beds using media such as composted pine bark. Mulching and careful watering are important, especially before the plant is established.
A new calcium-tolerant stock of rhododendrons (trademarked as 'Inkarho') has been exhibited at the RHS Chelsea Flower Show in London (2011). Individual hybrids of rhododendrons have been grafted on to a rootstock on a single rhododendron plant that was found growing in a chalk quarry. The rootstock is able to grow in calcium-rich soil up to a pH of 7.5.
Hybrids.
Rhododendrons are extensively hybridized in cultivation, and natural hybrids often occur in areas where species ranges overlap. There are over 28,000 cultivars of Rhododendron in the held by the Royal Horticultural Society. Most have been bred for their flowers, but a few are of garden interest because of ornamental leaves and some for ornamental bark or stems. Some hybrids have fragrant flowers—such as the Loderi hybrids, created by crossing "R. fortunei" and "R. griffithianum". Other examples include the PJM hybrids, formed from a cross between "Rhododendron carolinianum" and "Rhododendron dauricum", and named after Peter J. Mezitt of Weston Nurseries, Massachusetts.
Uses.
Pharmacology.
"Rhododendron" species have long been used in traditional medicine. Animal studies and "in vitro" research has identified possible anti-inflammatory and hepatoprotective activities which may be due to the antioxidant effects of flavonoids or other phenolic compounds and saponins the plant contains. Xiong "et al." have found that the root of the plant is able to reduce the activity of NF-κB in rats.
Toxicology.
Some species of rhododendron are poisonous to grazing animals because of a toxin called grayanotoxin in their pollen and nectar. People have been known to become ill from eating honey made by bees feeding on rhododendron and azalea flowers. Xenophon described the odd behaviour of Greek soldiers after having consumed honey in a village surrounded by "Rhododendron ponticum" during the march of the Ten Thousand in 401 BC. Pompey's soldiers reportedly suffered lethal casualties following the consumption of honey made from "Rhododendron" deliberately left behind by Pontic forces in 67 BC during the Third Mithridatic War. Later, it was recognized that honey resulting from these plants has a slightly hallucinogenic and laxative effect. The suspect rhododendrons are "Rhododendron ponticum" and "Rhododendron luteum" (formerly "Azalea pontica"), both found in northern Asia Minor.A brief documented video of this occurring in the modern day involves a group of men in Nepal foraging for this affected honey can be found here: http://eupterrafoundation.com/hallucinogenic-honey. Eleven similar cases have been documented in Istanbul, Turkey during the 1980s. Rhododendron is extremely toxic to horses, with some animals dying within a few hours of ingesting the plant, although most horses tend to avoid it if they have access to good forage. The effects of "R. ponticum" was mentioned in the 2009 film Sherlock Holmes as a proposed way to arrange a fake execution. It was also mentioned in the third episode of Season 2 of BBC's Sherlock (TV series), and has been speculated to have been a part of Sherlock's fake death scheme.
Culture.
Symbolism.
"Rhododendron arboreum" ("lali guransh") is the national flower of Nepal. "R. ponticum" is the state flower of Indian-administered Kashmir and Pakistan-controlled Kashmir. "Rhododendron niveum" is the state tree of Sikkim in India. Rhododendron is also the state tree of the state of Uttarakhand, India. Pink Rhododendron (Rhododendron campanulatum) is the State Flower of Himachal Pradesh, India.
"Rhododendron maximum", the most widespread rhododendron of the Appalachian Mountains, is the state flower of West Virginia, and is in the Flag of West Virginia. 
"Rhododendron macrophyllum", a widespread rhododendron of the Pacific Northwest, is the state flower of Washington.
Literature.
In Joyce's "Ulysses", rhododendrons play an important role in Leopold and Molly's early courtship: Molly remembers them in her soliloquy - "the sun shines for you he said the day we were lying among the rhododendrons on Howth head in the grey tweed suit and his straw hat the day I got him to propose to me". Jasper Fforde a British author, also uses rhododendron as a motif throughout many of his published books. See Thursday Next series, and .
Amongst the Zomi tribes in India and Myanmar, "Rhododendrons" called "Ngeisok" is used in a poetic manner to signify a lady.
Culinary.
The rhododendron is the national flower of Nepal, where the flower is considered edible and enjoyed for its sour taste. The pickled flower can last for months and the flower juice is also marketed. The flower, fresh or dried, is added to fish curry in the belief that it will soften the bones.
The juice of rhododendron flower is used to make a squash called burans (named after the flower) in the hilly regions of Uttarakhand. It is admired for its distinctive flavor and color. 
Labrador tea.
Labrador tea is an herbal tea (not a true tea) made from three closely related species: 

</doc>
<doc id="40337" url="http://en.wikipedia.org/wiki?curid=40337" title="Fetchmail">
Fetchmail

Fetchmail is an open source software utility for POSIX-compliant operating systems which is used to retrieve e-mail from a remote POP3, IMAP, ETRN or ODMR mail server to the user's local system. It was developed from the popclient program, written by Carl Harris.
Its chief significance is perhaps that its author, Eric S. Raymond, used it as a model to discuss his theories of open source software development in a widely read and influential essay on software development methodologies, "The Cathedral and the Bazaar".
Design.
By design Fetchmail's only means of delivering messages is by submitting them to the local MTA/Message transfer agent; delivering directly to mail folders such as maildir is not supported.
Dan Bernstein, getmail creator Charles Cazabon and FreeBSD developer Terry Lambert, have criticized Fetchmail's design, its number of security holes, and that it was prematurely put into "maintenance mode". In 2004, a new team of maintainers took over Fetchmail development, and laid out development plans that broke with design decisions that Eric Raymond had made in earlier versions.

</doc>
<doc id="40338" url="http://en.wikipedia.org/wiki?curid=40338" title="Mayor of London">
Mayor of London

The Mayor of London is an elected politician who, along with the London Assembly of 25 members, is accountable for the strategic government of Greater London. Conservative Boris Johnson has held the position since 4 May 2008. The position was previously held by Ken Livingstone from the creation of the role on 4 May 2000 until his succession by Johnson.
The role, created in 2000 after the London devolution referendum, was the first directly elected mayor in the United Kingdom.
The Mayor of London is the mayor of the entirety of Greater London, including the City of London, for which there is also the ceremonial Lord Mayor of the City of London. Each London Borough also has a ceremonial mayor or, in Hackney, Lewisham, Newham or Tower Hamlets, an elected mayor.
Background.
The Greater London Council, the elected governance for Greater London, was abolished in 1986 following the Local Government Act 1985. Strategic functions were split off to various joint arrangements. Londoners voted in a referendum in 1998 to create new governance structures for Greater London. The directly elected Mayor of London was created by the Greater London Authority Act 1999 in 2000 as part of the reforms.
Elections.
The Mayor is elected by the supplementary vote method for a fixed term of four years, with elections taking place in May. As with most elected posts in the UK, there is a deposit, in this case of £10,000 which is returnable on the candidate's winning at least 5% of the first-choice votes cast.
2000.
The 2000 campaign was incident-filled. The eventual winner, Ken Livingstone, went back on an earlier pledge not to run as an independent after losing the Labour nomination to Frank Dobson. The Conservative Party had to replace Lord Archer of Weston-super-Mare as their candidate when he was charged with perjury; Steve Norris was elected as his replacement.
2004.
In 2004, the second election was held. After being re-admitted to the Labour Party, Ken Livingstone was their official candidate. He won re-election after second preference votes were counted, with Steve Norris again coming second.
2008.
The incumbent Labour Mayor, Ken Livingstone was defeated by Conservative candidate Boris Johnson who became London's 2nd Mayor.
2012.
Conservative Mayor Boris Johnson was reelected to a second term in office, defeating former Labour mayor Ken Livingstone. Livingstone announced his retirement from politics in his concession speech.
2016.
The 2016 London mayoral election is scheduled for 5 May 2016.
Incumbent Mayor Boris Johnson is not running for re-election for a third term in office; as he is instead the Member of Parliament for the Conservative Party in Uxbridge and South Ruislip in the 2015 general election.
Powers and functions.
Most powers are derived from the Greater London Authority Act 1999 with additional functions coming from the Greater London Authority Act 2007, the Localism Act 2011 and Police Reform and Social Responsibility Act 2011.
The main functions are:
The remaining local government functions are performed by the London borough councils. There is some overlap, for example the borough councils are responsible for waste management, but the mayor is required to produce a waste management strategy.
Initiatives.
Ken Livingstone.
Initiatives taken by Ken Livingstone as Mayor of London included the London congestion charge on private vehicles using city centre London on weekdays, the creation of the London Climate Change Agency, the and the founding of the international Large Cities Climate Leadership Group, now known as C40 Cities Climate Leadership Group. The congestion charge led to many new buses being introduced across London. In 2003 Livingstone oversaw the introduction of the Oyster Card electronic ticketing system for Transport for London services.
They have also included the London Partnerships Register which was a voluntary scheme without legal force for same-sex couples to register their partnership, and paved the way for the introduction by the United Kingdom Parliament of civil partnerships. Unlike civil partnerships, the London Partnerships Register was open to heterosexual couples who favour a public commitment other than marriage.
As Mayor of London, Ken Livingstone was also a supporter of the London Olympics in 2012, and is known to encourage sport in London; especially when sport can be combined with helping UK charities-like The London Marathon and British 10K charity races. However, Livingstone, in a Mayoral election debate on the BBC's "Question Time" in April 2008 did state that the primary reason he supported the Olympic bid was to secure funding for the redevelopment of the East End of London. In the summer of 2007 he brought the Tour de France cycle race to London.
Boris Johnson.
In May 2008, Boris Johnson introduced a new transport safety initiative to put 440 high-visibility police officers on bus hubs and the immediate vicinity. A ban on alcohol on underground, bus, Docklands Light Railway, and tram services and stations across the capital was announced.
Also in May 2008, Boris Johnson announced the closure of "The Londoner" newspaper, saving approximately £2.9 million. A percentage of this saving will be spent on planting 10,000 new street trees.
In 2010 Boris Johnson extended the coverage of Oyster Card electronic ticketing to all National Rail overground train services 
Also in 2010 Boris Johnson opened the Santander Cycles formerly the (Barclays Cycle Hire) scheme with 5,000 bicycles available for hire across London. However, it should be noted that the plans for this were created under Ken Livingstone's administration.
In 2011 Boris Johnson set up the Outer London Fund, a money pot of up to £50 million designed to help facilitate better, more effective local high streets. Areas in London were given the chance to submit proposals for two separate pots of money, which would be granted to them if their bid was successful. Successful bids for Phase 1 included Enfield, Muswell Hill and Bexley Town Centre. The recipients of Phase 2 funding are still to be announced.
In January 2013 Boris Johnson appointed journalist Andrew Gilligan as the first Cycling Commissioner for London.
In March 2013 Boris Johnson announced £1 billion of investment in infrastructure to make cycling safer in London, including a 15 mi East-West segregated 'Crossrail for bikes'.
In the General Election of 7 May 2015, Boris Johnson was elected as MP for Uxbridge and Ruislip South with 50.2% of the vote on a turnout of 63.4%. For this he receives the basic MP's salary of £67,060 per annum plus expenses to cover staff, office expenses, accommodation if he does not have a London home and travel.
Salary.
The Mayor of London's salary is £143,911 per year, which is similar to that of a government Cabinet minister.

</doc>
<doc id="40339" url="http://en.wikipedia.org/wiki?curid=40339" title="Lord Mayor of London">
Lord Mayor of London

The Right Honourable the Lord Mayor of London is the formal title and style of the Mayor (and Leader) of the City of London Corporation.
The office of "Lord Mayor of London" differs from that of "Mayor of London"; the former being the governing officer solely for the City of London, while the Mayor of London has responsibility for the whole of Greater London, a much larger area. Within the City of London, the Lord Mayor is accorded precedence over all individuals except the Sovereign and retains various traditional powers, rights and privileges.
In 2006 the Corporation of London changed its name to the "City of London Corporation", when the title Lord Mayor of the City of London was reintroduced, partly to avoid confusion with the Mayor of London. However, the legal and commonly-used title remains Lord Mayor of London.
The Lord Mayor is elected at "Common Hall" each year on Michaelmas, and takes office on the Friday before the second Saturday in November, at "The Silent Ceremony".
The Lord Mayor's Show is held on the day after taking office; the Lord Mayor, preceded by a procession, travels to the Royal Courts of Justice at the Strand in Westminster to swear allegiance to the Sovereign before the Justices of the High Court.
One of the world's oldest continuously elected (and thereby successful) civic offices, the Lord Mayor's main role nowadays is to represent, support and promote the businesses and residents in the City of London. Today, these businesses are mostly in the financial sector and the Lord Mayor is regarded as the champion of the entire UK-based financial sector regardless of ownership or location throughout the country. As Leader of the Corporation of the City of London, the Lord Mayor serves as the key spokesman for the local authority and also has important ceremonial and social responsibilities. All Lord Mayors of London are apolitical, which gives them added credibility at home and abroad when representing the financial sector.
The Lord Mayor of London delivers over 800 speeches per year and spends over 100 days abroad usually in more than 20 countries. The Lord Mayor, also "ex-officio" Chancellor of London's City University, is assisted in the day-to-day running of his schedule by the Mansion House Staff who are senior administrative personnel in the Corporation of London and whose titles range from the Town Clerk and Chief Executive to Chamberlain and Remembrancer.
The present Lord Mayor is Alan Yarrow (for 2014-15).
Titles and honours.
Of the 69 cities in the United Kingdom, the City of London is among the 30 that have Lord Mayors (or, in Scotland, Lords Provost). The Lord Mayor is entitled to the style The Right Honourable; the same privilege extends only to the Lord Mayors of York, Cardiff and Belfast, and to the Lords Provost of Edinburgh and Glasgow. The style, however, is used when referring to the office as opposed to the holder thereof; thus, "The Rt Hon The Lord Mayor of the City of London" would be correct, while "The Rt Hon Alan Yarrow" would be incorrect. The latter prefix applies only to Privy Counsellors.
A woman who holds the office is also known as a Lord Mayor. The wife of a male Lord Mayor is styled as Lady Mayoress, but no equivalent title exists for the husband of a female Lord Mayor. A female Lord Mayor or an unmarried male Lord Mayor may appoint a female consort, usually a fellow member of the corporation, to the role of Lady Mayoress. In speech, a Lord Mayor is referred to as "My Lord Mayor", and a Lady Mayoress as "My Lady Mayoress".
It was once customary for Lord Mayors to be appointed knights upon taking office and baronets upon retirement, unless they already held such a title. This custom was followed with a few inconsistencies from the 16th until the 19th centuries; creations became more regular from 1889 onwards. However, from 1964 onwards, the regular creation of hereditary titles such as baronetcies was phased out, so subsequent Lord Mayors were offered knighthoods (and, until 1993, most often as Knight Grand Cross of the Order of the British Empire (GBE)). Since 1993, Lord Mayors have not automatically received any national honour upon appointment; instead, they have been made Knights Bachelors upon retirement, although Gordon Brown's Government broke with that tradition by making Ian Luder a CBE, after his term of office in 2009, and the following year Nick Anstee declined offers of a national honour. Furthermore, foreign Heads of State visiting the City of London on a UK State Visit, diplomatically bestow upon the Lord Mayor one of their suitable national honours. For example, in 2001, Sir David Howard was created a Grand Cordon (First Class) of the Order of Independence of Jordan by King Abdullah II. Recently Lord Mayors have been appointed at the beginning of their term of office Knights or Dames of St John, as a mark of respect, by HM The Queen, Sovereign Head of the Order of St John.
History.
The office of Lord Mayor was instituted in 1189, the first holder of the office being Henry Fitz-Ailwin de Londonestone. The Mayor of the City of London has been elected by the City, rather than appointed by the Sovereign, ever since a Royal Charter providing for a Mayor was issued by King John in 1215. The title "Lord Mayor" came to be used after 1354, when it was granted to Thomas Legge (then serving his second of two terms) by King Edward III.
Lord Mayors are elected for one-year terms; by custom, they do not now serve more than one consecutive term. Numerous individuals have served multiple terms in office, including:<br>
As Mayor:
As Lord Mayor:
The last individual to serve multiple terms was Sir Robert Fowler (elected in 1883 and in 1885).
Almost 700 people have served as Lord Mayor. Dame Mary Donaldson, elected in 1983, and Fiona Woolf, elected in 2013, are the only women so far to have held the office.
Some Lord Mayors in the Middle Ages, such as Sir Edward Dalyngrigge (1392), did not reside in London. Since 1435, the Lord Mayor has been chosen from amongst the Aldermen of the City of London.
Election.
The Lord Mayor is elected at Common Hall, comprising Liverymen from all of the City's Livery Companies. Common Hall is summoned by the sitting Lord Mayor; it meets at Guildhall on Michaelmas Day (29 September) or on the closest weekday. Voting is by show of hands; if, however, any liveryman so demands, balloting is held a fortnight later.
The qualification to stand for election is that one must have served as a City Sheriff and be a current Alderman. Since 1385, prior service as Sheriff has been mandatory for election to the Lord Mayoralty. Two Sheriffs are selected annually by Common Hall, which meets on Midsummer's Day for this purpose. By an ordinance of 1435, the Lord Mayor must be chosen from amongst the Aldermen of the City of London. Those on the electoral roll of each of the City's 25 Wards select one Alderman, who formerly held office for life or until resignation. Now each Alderman must submit for re-election at least once in every six years.
The Lord Mayor is then sworn in November, on the day before the Lord Mayor's Show ("see below"). The ceremony is known as the "Silent Ceremony" because, aside from a short declaration by the incoming Lord Mayor, no speeches are made. At Guildhall, the outgoing Lord Mayor transfers the mayoral insignia—the Seal, the Purse, the Sword and the Mace—to the incoming Lord Mayor.
Lord Mayor's Show.
The day after being sworn in to office, the Lord Mayor leads a procession from the City of London to the Royal Courts of Justice in the City of Westminster, where the Lord Mayor swears allegiance to the Crown. This pageantry has evolved into one of London's longest-running and most popular annual events, known as the "Lord Mayor's Show". The Lord Mayor travels in the City's State Coach that was built in 1757 at a cost of £1,065.0s.3d. (equivalent to £ in 2015). Nowadays, this festival combines traditional British pageantry with the element of carnival, and since 1959 it has been held on the second Saturday in November. Participants include the Livery Companies, bands and members of the military, charities and schools. In the evening, a fireworks display is held.
Role.
The Lord Mayor is a member of the City of London's governing body, the City of London Corporation (incorporated as "The Mayor and Commonalty and Citizens of the City of London"). The Corporation comprises the Court of Aldermen and the Court of Common Council; the former includes only the Aldermen, while the latter includes both Aldermen and Common Councilmen. The Lord Mayor belongs to and presides over both bodies.
As noted earlier, the main role of the Lord Mayor is to represent, support and promote all aspects of UK-financial service industries, including maritime. They undertake this as head of the City of London Corporation and, during the year, host visiting foreign Ministers, businessmen and dignitaries; furthermore, they conduct several foreign visits of their own so as to promote British financial sectors.
Banquets hosted by the Lord Mayor serve as opportunities for senior Government figures to deliver major speeches. At the Lord Mayor's Banquet (held on the Monday after the Lord Mayor's Show), the Prime Minister delivers the keynote address. At the Banker's Dinner in June, the Chancellor of the Exchequer delivers a speech known as the Mansion House Speech, which takes its name from the Lord Mayor's residence. At the Easter Banquet, also hosted each year at the Mansion House, the Secretary of State for Foreign and Commonwealth Affairs addresses an audience of international dignitaries.
In 2013, The Lord Mayor carried the Sword of Mourning at Baroness Thatcher's Funeral processing ahead of HM The Queen and HRH Prince Philip into St Paul's Cathedral.
The Lord Mayor performs numerous other functions, including serving as the Chief Magistrate of the City of London, Admiral of the Port of London, Chancellor of City University, President of Gresham College, President of City of London Reserve Forces and Cadets Association, and Trustee of St Paul's Cathedral. The Lord Mayor also heads the City's Commission of Lieutenancy, which represents the Sovereign in the City of London (other counties usually have Lord Lieutenants, as opposed to Commissions), and annually attends the Treloar Trust (named after Sir William Treloar, Lord Mayor in 1906), in Hampshire. The Treloar Trust runs two educational sites for disabled children, a school and college.
The Lord Mayor is ex officio the Admiral of the Port of London
Rights and privileges.
The residence of the Lord Mayor is known as Mansion House. The creation of the residence was considered after the Great Fire of London (1666), but construction did not commence until 1739. It was first occupied by a Lord Mayor in 1752, when Sir Crispin Gascoigne took up residence.
In each of the eighteen courtrooms of the Old Bailey, the centre of the judges' bench is reserved for the Lord Mayor, in his capacity of Chief Justice of the City of London. The presiding judge therefore sits to one side.
It is sometimes asserted that the Lord Mayor may exclude the Sovereign from the City of London. The legend is based on the misinterpretation of the ceremony observed each time the Sovereign enters the City. At Temple Bar the Lord Mayor presents the City's pearl-encrusted Sword of State to the Sovereign as a symbol of the latter's overlordship. The Sovereign does not, as is often purported, wait for the Lord Mayor's permission to enter the City. When the Sovereign enters the city, a short ceremony usually takes place where the Lord Mayor symbolically surrenders his or her authority to the Sovereign by presenting the sword to them. If the Sovereign is attending a service at St Paul's this ceremony would take place there rather than at the boundary of the City for matters of convenience.
The importance of the office is reflected by the composition of the Accession Council, a body which proclaims the accession of new Sovereigns. The Council includes the Lord Mayor and Aldermen of London, as well as the Lords Spiritual, Lords Temporal and Privy Counsellors. At the coronation banquet which followed, the Lord Mayor of the City of London had the right to assist the Royal Butler. The same privilege is held by the Lord Mayor of Oxford; the Mayor of Winchester may assist the Royal Cook. Such privileges have not been exercised since 1821, when the last coronation banquet (commemorating the coronation of George IV) was held.
Official dress.
The Lord Mayor still continues to wear a form of court dress long abandoned by many modern day officials on a regular, almost daily, basis. Their basic under dress is of the traditional black velvet court dress (old style) consisting of a coat, waistcoat and knee breeches with steel cut buttons. This is worn with black silk stockings, patent court shoes with steel buckles, white shirt with lace cuffs and a large jabot stock. This form of court dress is worn by all Lord Mayors regardless of gender.
Over his or her underdress for ceremonial occasions is worn a black silk damask robe trimmed with gold lace of a design exactly the same as that of the Lord Chancellor's. When outdoors, they wear a black beaver plush tricorne hat trimmed with white (or black in the event of memorials and funerals) ostrich feathers and a steel 'loop' for the cockade. This has been traditionally made by Patey's commissioned by the Worshipful Company of Feltmakers for each incumbent Lord Mayor.
For state occasions when the Sovereign is present, the Lord Mayor, instead of the gold-lace robe, wears a crimson velvet cape trimmed with an ermine cape and facings that have black sealskin spots on, very similar to a royal earl's coronation robe. It is tied with gold cordons.
Since 1545 the Lord Mayor of London has worn a Royal Livery Collar of Esses. However, the Collar's origins are not royal, Sir John Alen, thrice Lord Mayor, having bequeathed it to the next Lord Mayor and his successors "to use and occupie yerely at and uppon principall and festivall dayes." It was enlarged in 1567, and in its present shape has 28 Esses (the Lancastrian ‘S’), the Tudor rose, the tasselled knots of the Garter and also the Portcullis. The collar is worn over whatever the Lord Mayor maybe wearing, secured onto their underdress or state robes by means of black or white silk satin ribbons on the shoulders.
At coronations, the Lord Mayor wears a special coronation robe made especially for the incumbent by Ede & Ravenscroft. This is a cape-like mantle of scarlet superfine wool trimmed with bars of gold lace and ermine spotted with black sealskin. It is lined with white silk satin. They also carry a baton of office. After the coronation, the incumbent may personally keep their coronation robe as a token.

</doc>
<doc id="40343" url="http://en.wikipedia.org/wiki?curid=40343" title="Prehnite">
Prehnite

Prehnite is a inosilicate of calcium and aluminium with the formula: Ca2Al(AlSi3O10)(OH)2. Limited Fe3+ substitutes for aluminium in the structure. Prehnite crystallizes in the orthorhombic crystal system, and most oftens forms as stalactitic or botryoidal aggregates, with only just the crests of small crystals showing any faces, which are almost always curved or composite. Very rarely will it form distinct, well individualized crystals showing a square-like cross-section, like those found at the Jeffrey Mine in Asbestos, Quebec, Canada. It is brittle with an uneven fracture and a vitreous to pearly lustre. Its hardness is 6-6.5, its specific gravity is 2.80-2.90 and its color varies from light green to yellow, but also colorless, blue or white. In April 2000, a rare orange Prehnite was discovered at the famous Kalahari Manganese Fields in South Africa. It is mostly translucent, and rarely transparent.
Though not a zeolite, it is found associated with minerals such as datolite, calcite, apophyllite, stilbite, laumontite, heulandite etc. in veins and cavities of basaltic rocks, sometimes in granites, syenites, or gneisses. It is an indicator mineral of the prehnite-pumpellyite metamorphic facies. 
It was first described in 1788 for an occurrence in the Karoo dolerites of Cradock, Eastern Cape Province, South Africa. It was named for Colonel Hendrik Von Prehn (1733–1785), commander of the military forces of the Dutch colony at the Cape of Good Hope from 1768 to 1780.
Extensive deposits of gem quality prehnite occur in the basalt tableland surrounding Wave Hill Station in the central Northern Territory, of Australia.

</doc>
<doc id="40344" url="http://en.wikipedia.org/wiki?curid=40344" title="Semiconductor device">
Semiconductor device

Semiconductor devices are electronic components that exploit the electronic properties of semiconductor materials, principally silicon, germanium, and gallium arsenide, as well as organic semiconductors. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. They use electronic conduction in the solid state as opposed to the gaseous state or thermionic emission in a high vacuum.
Semiconductor devices are manufactured both as single discrete devices and as "integrated circuits" (ICs), which consist of a number—from a few (as low as two) to billions—of devices manufactured and interconnected on a single semiconductor substrate, or wafer.
Semiconductor materials are useful because their behavior can be easily manipulated by the addition of impurities, known as doping. Semiconductor conductivity can be controlled by introduction of an electric or magnetic field, by exposure to light or heat, or by mechanical deformation of a doped monocrystalline grid; thus, semiconductors can make excellent sensors. Current conduction in a semiconductor occurs via mobile or "free" "electrons" and "holes", collectively known as "charge carriers". Doping a semiconductor such as silicon with a small amount of impurity atoms, such as phosphorus or boron, greatly increases the number of free electrons or holes within the semiconductor. When a doped semiconductor contains excess holes it is called "p-type", and when it contains excess free electrons it is known as "n-type", where "p" (positive for holes) or "n" (negative for electrons) is the sign of the charge of the majority mobile charge carriers. The semiconductor material used in devices is doped under highly controlled conditions in a fabrication facility, or "fab", to control precisely the location and concentration of p- and n-type dopants. The junctions which form where n-type and p-type semiconductors join together are called p–n junctions.
Diode.
A semiconductor diode is a device typically made from a single p–n junction. At the junction of a p-type and an n-type semiconductor there forms a depletion region where current conduction is inhibited by the lack of mobile charge carriers. When the device is "forward biased" (connected with the p-side at higher electric potential than the n-side), this depletion region is diminished, allowing for significant conduction, while only very small current can be achieved when the diode is "reverse biased" and thus the depletion region expanded.
Exposing a semiconductor to light can generate electron–hole pairs, which increases the number of free carriers and thereby the conductivity. Diodes optimized to take advantage of this phenomenon are known as "photodiodes".
Compound semiconductor diodes can also be used to generate light, as in light-emitting diodes and laser diodes.
Transistor.
Bipolar junction transistors are formed from two p–n junctions, in either n–p–n or p–n–p configuration. The middle, or "base", region between the junctions is typically very narrow. The other regions, and their associated terminals, are known as the "emitter" and the "collector". A small current injected through the junction between the base and the emitter changes the properties of the base-collector junction so that it can conduct current even though it is reverse biased. This creates a much larger current between the collector and emitter, controlled by the base-emitter current.
Another type of transistor, the field-effect transistor, operates on the principle that semiconductor conductivity can be increased or decreased by the presence of an electric field. An electric field can increase the number of free electrons and holes in a semiconductor, thereby changing its conductivity. The field may be applied by a reverse-biased p–n junction, forming a "junction field-effect transistor" (JFET) or by an electrode isolated from the bulk material by an oxide layer, forming a "metal–oxide–semiconductor field-effect transistor" (MOSFET).
The MOSFET, a solid-state device, is the most used semiconductor device today. The "gate" electrode is charged to produce an electric field that controls the conductivity of a "channel" between two terminals, called the "source" and "drain". Depending on the type of carrier in the channel, the device may be an "n-channel" (for electrons) or a "p-channel" (for holes) MOSFET. Although the MOSFET is named in part for its "metal" gate, in modern devices polysilicon is typically used instead.
Semiconductor device materials.
By far, silicon (Si) is the most widely used material in semiconductor devices. Its combination of low raw material cost, relatively simple processing, and a useful temperature range make it currently the best compromise among the various competing materials. Silicon used in semiconductor device manufacturing is currently fabricated into boules that are large enough in diameter to allow the production of 300 mm (12 in.) wafers.
Germanium (Ge) was a widely used early semiconductor material but its thermal sensitivity makes it less useful than silicon. Today, germanium is often alloyed with silicon for use in very-high-speed SiGe devices; IBM is a major producer of such devices.
Gallium arsenide (GaAs) is also widely used in high-speed devices but so far, it has been difficult to form large-diameter boules of this material, limiting the wafer diameter to sizes significantly smaller than silicon wafers thus making mass production of GaAs devices significantly more expensive than silicon.
Other less common materials are also in use or under investigation.
Silicon carbide (SiC) has found some application as the raw material for blue light-emitting diodes (LEDs) and is being investigated for use in semiconductor devices that could withstand very high operating temperatures and environments with the presence of significant levels of ionizing radiation. IMPATT diodes have also been fabricated from SiC.
Various indium compounds (indium arsenide, indium antimonide, and indium phosphide) are also being used in LEDs and solid state laser diodes. Selenium sulfide is being studied in the manufacture of photovoltaic solar cells.
The most common use for organic semiconductors is Organic light-emitting diodes.
List of common semiconductor devices.
"Two-terminal devices:"
"Three-terminal devices:"
"Four-terminal devices:"
"Multi-terminal devices:"
"VLSI (Very-large-scale integration):"
Semiconductor device applications.
All transistor types can be used as the building blocks of logic gates, which are fundamental in the design of digital circuits. In digital circuits like microprocessors, transistors act as on-off switches; in the MOSFET, for instance, the voltage applied to the gate determines whether the switch is on or off.
Transistors used for analog circuits do not act as on-off switches; rather, they respond to a continuous range of inputs with a continuous range of outputs. Common analog circuits include amplifiers and oscillators.
Circuits that interface or translate between digital circuits and analog circuits are known as mixed-signal circuits.
Power semiconductor devices are discrete devices or integrated circuits intended for high current or high voltage applications. Power integrated circuits combine IC technology with power semiconductor technology, these are sometimes referred to as "smart" power devices. Several companies specialize in manufacturing power semiconductors.
Component identifiers.
The type designators of semiconductor devices are often manufacturer specific. Nevertheless, there have been attempts at creating standards for type codes, and a subset of devices follow those. For discrete devices, for example, there are three standards: JEDEC JESD370B in United States, Pro Electron in Europe and Japanese Industrial Standards (JIS) in Japan.
History of semiconductor device development.
Cat's-whisker detector.
Semiconductors had been used in the electronics field for some time before the invention of the transistor. Around the turn of the 20th century they were quite common as detectors in radios, used in a device called a "cat's whisker" developed by Jagadish Chandra Bose and others. These detectors were somewhat troublesome, however, requiring the operator to move a small tungsten filament (the whisker) around the surface of a galena (lead sulfide) or carborundum (silicon carbide) crystal until it suddenly started working. Then, over a period of a few hours or days, the cat's whisker would slowly stop working and the process would have to be repeated. At the time their operation was completely mysterious. After the introduction of the more reliable and amplified vacuum tube based radios, the cat's whisker systems quickly disappeared. The "cat's whisker" is a primitive example of a special type of diode still popular today, called a Schottky diode.
Metal rectifier.
Another early type of semiconductor device is the metal rectifier in which the semiconductor is copper oxide or selenium. Westinghouse Electric (1886) was a major manufacturer of these rectifiers.
World War II.
During World War II, radar research quickly pushed radar receivers to operate at ever higher frequencies and the traditional tube based radio receivers no longer worked well. The introduction of the cavity magnetron from Britain to the United States in 1940 during the Tizard Mission resulted in a pressing need for a practical high-frequency amplifier. 
On a whim, Russell Ohl of Bell Laboratories decided to try a cat's whisker. By this point they had not been in use for a number of years, and no one at the labs had one. After hunting one down at a used radio store in Manhattan, he found that it worked much better than tube-based systems.
Ohl investigated why the cat's whisker functioned so well. He spent most of 1939 trying to grow more pure versions of the crystals. He soon found that with higher quality crystals their finicky behaviour went away, but so did their ability to operate as a radio detector. One day he found one of his purest crystals nevertheless worked well, and interestingly, it had a clearly visible crack near the middle. However as he moved about the room trying to test it, the detector would mysteriously work, and then stop again. After some study he found that the behaviour was controlled by the light in the room–more light caused more conductance in the crystal. He invited several other people to see this crystal, and Walter Brattain immediately realized there was some sort of junction at the crack.
Further research cleared up the remaining mystery. The crystal had cracked because either side contained very slightly different amounts of the impurities Ohl could not remove–about 0.2%. One side of the crystal had impurities that added extra electrons (the carriers of electrical current) and made it a "conductor". The other had impurities that wanted to bind to these electrons, making it (what he called) an "insulator". Because the two parts of the crystal were in contact with each other, the electrons could be pushed out of the conductive side which had extra electrons (soon to be known as the "emitter") and replaced by new ones being provided (from a battery, for instance) where they would flow into the insulating portion and be collected by the whisker filament (named the "collector"). However, when the voltage was reversed the electrons being pushed into the collector would quickly fill up the "holes" (the electron-needy impurities), and conduction would stop almost instantly. This junction of the two crystals (or parts of one crystal) created a solid-state diode, and the concept soon became known as semiconduction. The mechanism of action when the diode is off has to do with the separation of charge carriers around the junction. This is called a "depletion region".
Development of the diode.
Armed with the knowledge of how these new diodes worked, a vigorous effort began to learn how to build them on demand. Teams at Purdue University, Bell Labs, MIT, and the University of Chicago all joined forces to build better crystals. Within a year germanium production had been perfected to the point where military-grade diodes were being used in most radar sets.
Development of the transistor.
After the war, William Shockley decided to attempt the building of a triode-like semiconductor device. He secured funding and lab space, and went to work on the problem with Brattain and John Bardeen.
The key to the development of the transistor was the further understanding of the process of the electron mobility in a semiconductor. It was realized that if there were some way to control the flow of the electrons from the emitter to the collector of this newly discovered diode, an amplifier could be built. For instance, if contacts are placed on both sides of a single type of crystal, current will not flow between them through the crystal. However if a third contact could then "inject" electrons or holes into the material, current would flow.
Actually doing this appeared to be very difficult. If the crystal were of any reasonable size, the number of electrons (or holes) required to be injected would have to be very large, making it less than useful as an amplifier because it would require a large injection current to start with. That said, the whole idea of the crystal diode was that the crystal itself could provide the electrons over a very small distance, the depletion region. The key appeared to be to place the input and output contacts very close together on the surface of the crystal on either side of this region.
Brattain started working on building such a device, and tantalizing hints of amplification continued to appear as the team worked on the problem. Sometimes the system would work but then stop working unexpectedly. In one instance a non-working system started working when placed in water. Ohl and Brattain eventually developed a new branch of quantum mechanics, which became known as surface physics, to account for the behaviour. The electrons in any one piece of the crystal would migrate about due to nearby charges. Electrons in the emitters, or the "holes" in the collectors, would cluster at the surface of the crystal where they could find their opposite charge "floating around" in the air (or water). Yet they could be pushed away from the surface with the application of a small amount of charge from any other location on the crystal. Instead of needing a large supply of injected electrons, a very small number in the right place on the crystal would accomplish the same thing.
Their understanding solved the problem of needing a very small control area to some degree. Instead of needing two separate semiconductors connected by a common, but tiny, region, a single larger surface would serve. The electron-emitting and collecting leads would both be placed very close together on the top, with the control lead placed on the base of the crystal. When current flowed through this "base" lead, the electrons or holes would be pushed out, across the block of semiconductor, and collect on the far surface. As long as the emitter and collector were very close together, this should allow enough electrons or holes between them to allow conduction to start.
The first transistor.
The Bell team made many attempts to build such a system with various tools, but generally failed. Setups where the contacts were close enough were invariably as fragile as the original cat's whisker detectors had been, and would work briefly, if at all. Eventually they had a practical breakthrough. A piece of gold foil was glued to the edge of a plastic wedge, and then the foil was sliced with a razor at the tip of the triangle. The result was two very closely spaced contacts of gold. When the wedge was pushed down onto the surface of a crystal and voltage applied to the other side (on the base of the crystal), current started to flow from one contact to the other as the base voltage pushed the electrons away from the base towards the other side near the contacts. The point-contact transistor had been invented.
While the device was constructed a week earlier, Brattain's notes describe the first demonstration to higher-ups at Bell Labs on the afternoon of 23 December 1947, often given as the birthdate of the transistor. what is now known as the "p–n–p point-contact germanium transistor" operated as a speech amplifier with a power gain of 18 in that trial. John Bardeen, Walter Houser Brattain, and William Bradford Shockley were awarded the 1956 Nobel Prize in physics for their work.
Origin of the term "transistor".
Bell Telephone Laboratories needed a generic name for their new invention: "Semiconductor Triode", "Solid Triode", "Surface States Triode" ["sic"], "Crystal Triode" and "Iotatron" were all considered, but "transistor", coined by John R. Pierce, won an internal ballot. The rationale for the name is described in the following extract from the company's Technical Memoranda (May 28, 1948) [26] calling for votes:
Transistor. This is an abbreviated combination of the words "transconductance" or "transfer", and "varistor". The device logically belongs in the varistor family, and has the transconductance or transfer impedance of a device having gain, so that this combination is descriptive.
Improvements in transistor design.
Shockley was upset about the device being credited to Brattain and Bardeen, who he felt had built it "behind his back" to take the glory. Matters became worse when Bell Labs lawyers found that some of Shockley's own writings on the transistor were close enough to those of an earlier 1925 patent by Julius Edgar Lilienfeld that they thought it best that his name be left off the patent application.
Shockley was incensed, and decided to demonstrate who was the real brains of the operation. A few months later he invented an entirely new, considerably more robust, type of transistor with a layer or 'sandwich' structure. This structure went on to be used for the vast majority of all transistors into the 1960s, and evolved into the bipolar junction transistor.
With the fragility problems solved, a remaining problem was purity. Making germanium of the required purity was proving to be a serious problem, and limited the yield of transistors that actually worked from a given batch of material. Germanium's sensitivity to temperature also limited its usefulness. Scientists theorized that silicon would be easier to fabricate, but few investigated this possibility. Gordon K. Teal was the first to develop a working silicon transistor, and his company, the nascent Texas Instruments, profited from its technological edge. From the late 1960s most transistors were silicon-based. Within a few years transistor-based products, most notably easily portable radios, were appearing on the market. 
A major improvement in manufacturing yield came when a chemist advised the companies fabricating semiconductors to use distilled rather than tap water: calcium ions present in tap water were the cause of the poor yields. "Zone melting", a technique using a band of molten material moving through the crystal, further increased crystal purity.

</doc>
<doc id="40345" url="http://en.wikipedia.org/wiki?curid=40345" title="MOSFET">
MOSFET

The metal–oxide–semiconductor field-effect transistor (MOSFET, MOS-FET, or MOS FET) is a type of transistor used for amplifying or switching electronic signals.
Although the MOSFET is a four-terminal device with source (S), gate (G), drain (D), and body (B) terminals, the body (or substrate) of the MOSFET is often connected to the source terminal, making it a three-terminal device like other field-effect transistors. Because these two terminals are normally connected to each other (short-circuited) internally, only three terminals appear in electrical diagrams. The MOSFET is by far the most common transistor in both digital and analog circuits, though the bipolar junction transistor was at one time much more common.
The main advantage of a MOSFET over a regular transistor is that it requires very little current to turn on (less than 1mA), while delivering a much higher current to a load (10 to 50A or more). However, the MOSFET requires a higher gate voltage (3-4V) to turn on.
In "enhancement mode" MOSFETs, a voltage drop across the oxide induces a conducting channel between the source and drain contacts "via" the field effect. The term "enhancement mode" refers to the increase of conductivity with increase in oxide field that adds carriers to the channel, also referred to as the "inversion layer". The channel can contain electrons (called an nMOSFET or nMOS), or holes (called a pMOSFET or pMOS), opposite in type to the substrate, so nMOS is made with a p-type substrate, and pMOS with an n-type substrate (see article on semiconductor devices). In the less common "depletion mode" MOSFET, detailed later on, the channel consists of carriers in a surface impurity layer of opposite type to the substrate, and conductivity is decreased by application of a field that depletes carriers from this surface layer.
The "metal" in the name MOSFET is now often a misnomer because the previously metal gate material is now often a layer of polysilicon (polycrystalline silicon). Aluminium had been the gate material until the mid-1970s, when polysilicon became dominant, due to its capability to form self-aligned gates. Metallic gates are regaining popularity, since it is difficult to increase the speed of operation of transistors without metal gates.
Likewise, the "oxide" in the name can be a misnomer, as different dielectric materials are used with the aim of obtaining strong channels with smaller applied voltages.
An insulated-gate field-effect transistor or IGFET is a related term almost synonymous with MOSFET. The term may be more inclusive, since many "MOSFETs" use a gate that is not metal, and a gate insulator that is not oxide. Another synonym is MISFET for metal–insulator–semiconductor FET.
The basic principle of the field-effect transistor was first patented by Julius Edgar Lilienfeld in 1925.
Composition.
Usually the semiconductor of choice is silicon, but some chip manufacturers, most notably IBM and Intel, recently started using a chemical compound of silicon and germanium (SiGe) in MOSFET channels. Unfortunately, many semiconductors with better electrical properties than silicon, such as gallium arsenide, do not form good semiconductor-to-insulator interfaces, and thus are not suitable for MOSFETs. Research continues on creating insulators with acceptable electrical characteristics on other semiconductor material.
In order to overcome the increase in power consumption due to gate current leakage, a high-κ dielectric is used instead of silicon dioxide for the gate insulator, while polysilicon is replaced by metal gates (see Intel announcement).
The gate is separated from the channel by a thin insulating layer, traditionally of silicon dioxide and later of silicon oxynitride. Some companies have started to introduce a high-κ dielectric + metal gate combination in the 45 nanometer node.
When a voltage is applied between the gate and body terminals, the electric field generated penetrates through the oxide and creates an "inversion layer" or "channel" at the semiconductor-insulator interface. The inversion channel is of the same type, p-type or n-type, as the source and drain, and thus it provides a channel through which current can pass. Varying the voltage between the gate and body modulates the conductivity of this layer and thereby controls the current flow between drain and source. This is known as enhancement mode.
Circuit symbols.
A variety of symbols are used for the MOSFET. The basic design is generally a line for the channel with the source and drain leaving it at right angles and then bending back at right angles into the same direction as the channel. Sometimes three line segments are used for enhancement mode and a solid line for depletion mode. (see Depletion and enhancement modes) Another line is drawn parallel to the channel for the gate.
The "bulk" or "body" connection, if shown, is shown connected to the back of the channel with an arrow indicating PMOS or NMOS. Arrows always point from P to N, so an NMOS (N-channel in P-well or P-substrate) has the arrow pointing in (from the bulk to the channel). If the bulk is connected to the source (as is generally the case with discrete devices) it is sometimes angled to meet up with the source leaving the transistor. If the bulk is not shown (as is often the case in IC design as they are generally common bulk) an inversion symbol is sometimes used to indicate PMOS, alternatively an arrow on the source may be used in the same way as for bipolar transistors (out for nMOS, in for pMOS).
Comparison of enhancement-mode and depletion-mode MOSFET symbols, along with JFET symbols. The orientation of the symbols, (most significantly the position of source relative to drain) is such that more positive voltages appear higher on the page than less positive voltages, implying current flowing "down" the page:
In schematics where G, S, D are not labeled, the detailed features of the symbol indicate which terminal is source and which is drain. For enhancement-mode and depletion-mode MOSFET symbols (in columns two and five), the source terminal is the one connected to the triangle. Additionally, in this diagram, the gate is shown as an "L" shape, whose input leg is closer to S than D, also indicating which is which. However, these symbols are often drawn with a "T" shaped gate (as elsewhere on this page), so it is the triangle which must be relied upon to indicate the source terminal.
For the symbols in which the bulk, or body, terminal is shown, it is here shown internally connected to the source (i.e., the black triangles in the diagrams in columns 2 and 5). This is a typical configuration, but by no means the only important configuration. In general, the MOSFET is a four-terminal device, and in integrated circuits many of the MOSFETs share a body connection, not necessarily connected to the source terminals of all the transistors.
MOSFET operation.
Metal–oxide–semiconductor structure.
The traditional metal–oxide–semiconductor (MOS) structure is obtained by growing a layer of silicon dioxide (SiO2) on top of a silicon substrate and depositing a layer of metal or polycrystalline silicon (the latter is commonly used). As the silicon dioxide is a dielectric material, its structure is equivalent to a planar capacitor, with one of the electrodes replaced by a semiconductor.
When a voltage is applied across a MOS structure, it modifies the distribution of charges in the semiconductor. If we consider a p-type semiconductor (with formula_1 the density of acceptors, "p" the density of holes; "p = NA" in neutral bulk), a positive voltage, formula_2, from gate to body (see figure) creates a depletion layer by forcing the positively charged holes away from the gate-insulator/semiconductor interface, leaving exposed a carrier-free region of immobile, negatively charged acceptor ions (see doping (semiconductor)). If formula_2 is high enough, a high concentration of negative charge carriers forms in an inversion layer located in a thin layer next to the interface between the semiconductor and the insulator. Unlike the MOSFET, where the inversion layer electrons are supplied rapidly from the source/drain electrodes, in the MOS capacitor they are produced much more slowly by thermal generation through carrier generation and recombination centers in the depletion region. Conventionally, the gate voltage at which the volume density of electrons in the inversion layer is the same as the volume density of holes in the body is called the threshold voltage. When the voltage between transistor gate and source (VGS) exceeds the threshold voltage (Vth), it is known as overdrive voltage.
This structure with p-type body is the basis of the n-type MOSFET, which requires the addition of an n-type source and drain regions.
MOSFET structure and channel formation.
A metal–oxide–semiconductor field-effect transistor (MOSFET) is based on the modulation of charge concentration by a MOS capacitance between a body electrode and a gate electrode located above the body and insulated from all other device regions by a gate dielectric layer which in the case of a MOSFET is an oxide, such as silicon dioxide. If dielectrics other than an oxide such as silicon dioxide (often referred to as oxide) are employed the device may be referred to as a metal–insulator–semiconductor FET (MISFET). Compared to the MOS capacitor, the MOSFET includes two additional terminals (source and drain), each connected to individual highly doped regions that are separated by the body region. These regions can be either p or n type, but they must both be of the same type, and of opposite type to the body region. The source and drain (unlike the body) are highly doped as signified by a "+" sign after the type of doping.
If the MOSFET is an n-channel or nMOS FET, then the source and drain are "n+" regions and the body is a "p" region. If the MOSFET is a p-channel or pMOS FET, then the source and drain are "p+" regions and the body is a "n" region. The source is so named because it is the source of the charge carriers (electrons for n-channel, holes for p-channel) that flow through the channel; similarly, the drain is where the charge carriers leave the channel.
The occupancy of the energy bands in a semiconductor is set by the position of the Fermi level relative to the semiconductor energy-band edges. As described above, and shown in the figure, with sufficient gate voltage, the valence band edge is driven far from the Fermi level, and holes from the body are driven away from the gate. At larger gate bias still, near the semiconductor surface the conduction band edge is brought close to the Fermi level, populating the surface with electrons in an "inversion layer" or "n-channel" at the interface between the p region and the oxide. This conducting channel extends between the source and the drain, and current is conducted through it when a voltage is applied between the two electrodes. Increasing the voltage on the gate leads to a higher electron density in the inversion layer and therefore increases the current flow between the source and drain.
For gate voltages below the threshold value, the channel is lightly populated, and only a very small subthreshold leakage current can flow between the source and the drain.
When a negative gate-source voltage (positive source-gate) is applied, it creates a "p-channel" at the surface of the n region, analogous to the n-channel case, but with opposite polarities of charges and voltages. When a voltage less negative than the threshold value (a negative voltage for p-channel) is applied between gate and source, the channel disappears and only a very small subthreshold current can flow between the source and the drain.
The device may comprise a Silicon On Insulator (SOI) device in which a buried oxide (BOX) is formed below a thin semiconductor layer. If the channel region between the gate dielectric and a BOX region is very thin, the very thin channel region is referred to as an ultrathin channel (UTC) region with the source and drain regions formed on either side thereof in and/or above the thin semiconductor layer. Alternatively, the device may comprise a semiconductor on insulator (SEMOI) device in which semiconductors other than silicon are employed. Many alternative semiconductor materials may be employed.
When the source and drain regions are formed above the channel in whole or in part, they are referred to as raised source/drain (RSD) regions.
Modes of operation.
The operation of a MOSFET can be separated into three different modes, depending on the voltages at the terminals. In the following discussion, a simplified algebraic model is used. Modern MOSFET characteristics are more complex than the algebraic model presented here.
For an enhancement-mode, n-channel MOSFET, the three operational modes are:
Body effect.
The occupancy of the energy bands in a semiconductor is set by the position of the Fermi level relative to the semiconductor energy-band edges. Application of a source-to-substrate reverse bias of the source-body pn-junction introduces a split between the Fermi levels for electrons and holes, moving the Fermi level for the channel further from the band edge, lowering the occupancy of the channel. The effect is to increase the gate voltage necessary to establish the channel, as seen in the figure. This change in channel strength by application of reverse bias is called the 'body effect'.
Simply put, using an nMOS example, the gate-to-body bias VGB positions the conduction-band energy levels, while the source-to-body bias VSB positions the electron Fermi level near the interface, deciding occupancy of these levels near the interface, and hence the strength of the inversion layer or channel.
The body effect upon the channel can be described using a modification of the threshold voltage, approximated by the following equation:
where "VTB" is the threshold voltage with substrate bias present, and "VT0" is the zero-"VSB" value of threshold voltage, formula_32 is the body effect parameter, and 2"φB" is the approximate potential drop between surface and bulk across the depletion layer when "VSB" = 0 and gate bias is sufficient to insure that a channel is present. As this equation shows, a reverse bias "VSB" > 0 causes an increase in threshold voltage "VTB" and therefore demands a larger gate voltage before the channel populates.
The body can be operated as a second gate, and is sometimes referred to as the "back gate"; the body effect is sometimes called the "back-gate effect".
Applications.
Digital integrated circuits such as microprocessors and memory devices contain thousands to millions of integrated MOSFET transistors on each device, providing the basic switching functions required to implement logic gates and data storage. Discrete devices are widely used in applications such as switch mode power supplies, variable frequency drives and other power electronics applications where each device may be switching hundreds or thousands of watts. Radio-frequency amplifiers up to the UHF spectrum use MOSFET transistors as analog signal and power amplifiers. Radio systems also use MOSFETs as oscillators, or mixers to convert frequencies. MOSFET devices are also applied in audio-frequency power amplifiers for public address systems, sound reinforcement and home and automobile sound systems.
History.
The basic principle of this kind of transistor was first patented by Julius Edgar Lilienfeld in 1925. Twenty five years later, when Bell Telephone attempted to patent the junction transistor, they found Lilienfeld already holding a patent, worded in a way that would include all types of transistors. Bell Labs was able to work out an agreement with Lilienfeld, who was still alive at that time (it is not known if they paid him money or not). It was at that time the Bell Labs version was given the name bipolar junction transistor, or simply junction transistor, and Lilienfeld's design took the name field effect transistor.
In 1959, Dawon Kahng and Martin M. (John) Atalla at Bell Labs invented the metal–oxide–semiconductor field-effect transistor (MOSFET) as an offshoot to the patented FET design.
Operationally and structurally different from the bipolar junction transistor,
the MOSFET was made by putting an insulating layer on the surface of the semiconductor and then placing a metallic gate electrode on that. It used crystalline silicon for the semiconductor and a thermally oxidized layer of silicon dioxide for the insulator. The silicon MOSFET did not generate localized electron traps at the interface between the silicon and its native oxide layer, and thus was inherently free from the trapping and scattering of carriers that had impeded the performance of earlier field-effect transistors. Following the development of clean rooms to reduce contamination to levels never before thought necessary, and of photolithography and the planar process to allow circuits to be made in very few steps, the Si–SiO2 system possessed such technical attractions as low cost of production (on a per circuit basis) and ease of integration. Largely because of these two factors, the MOSFET has become the most widely used type of transistor in integrated circuits.
Additionally, the method of coupling two complementary MOSFETS (P-channel and N-channel) into one high/low switch, known as CMOS, means that digital circuits dissipate very little power except when actually switched.
The earliest microprocessors starting in 1970 were all "MOS microprocessors"—i.e., fabricated entirely from PMOS logic or fabricated entirely from NMOS logic.
In the 1970s, "MOS microprocessors" were often contrasted with "CMOS microprocessors" and "bipolar bit-slice processors".
CMOS circuits.
The MOSFET is used in digital complementary metal–oxide–semiconductor (CMOS) logic, which uses p- and n-channel MOSFETs as building blocks. Overheating is a major concern in integrated circuits since ever more transistors are packed into ever smaller chips. CMOS logic reduces power consumption because no current flows (ideally), and thus no power is consumed, except when the inputs to logic gates are being switched. CMOS accomplishes this current reduction by complementing every nMOSFET with a pMOSFET and connecting both gates and both drains together. A high voltage on the gates will cause the nMOSFET to conduct and the pMOSFET not to conduct and a low voltage on the gates causes the reverse. During the switching time as the voltage goes from one state to another, both MOSFETs will conduct briefly. This arrangement greatly reduces power consumption and heat generation. Digital and analog CMOS applications are described below.
Digital.
The growth of digital technologies like the microprocessor has provided the motivation to advance MOSFET technology faster than any other type of silicon-based transistor. A big advantage of MOSFETs for digital switching is that the oxide layer between the gate and the channel prevents DC current from flowing through the gate, further reducing power consumption and giving a very large input impedance. The insulating oxide between the gate and channel effectively isolates a MOSFET in one logic stage from earlier and later stages, which allows a single MOSFET output to drive a considerable number of MOSFET inputs. Bipolar transistor-based logic (such as TTL) does not have such a high fanout capacity. This isolation also makes it easier for the designers to ignore to some extent loading effects between logic stages independently. That extent is defined by the operating frequency: as frequencies increase, the input impedance of the MOSFETs decreases.
Analog.
The MOSFET's advantages in digital circuits do not translate into supremacy in all analog circuits. The two types of circuit draw upon different features of transistor behavior. Digital circuits switch, spending most of their time outside the switching region, while analog circuits depend on the linearity of response when the MOSFET is held precisely in the switching region. The bipolar junction transistor (BJT) has traditionally been the analog designer's transistor of choice, due largely to its higher transconductance and its lower output impedance (drain-voltage independence) in the switching region.
Nevertheless, MOSFETs are widely used in many types of analog circuits because of certain advantages. The characteristics and performance of many analog circuits can be scaled up or down by changing the sizes (length and width) of the MOSFETs used. By comparison, in most bipolar transistors the size of the device does not significantly affect its performance. MOSFETs' ideal characteristics regarding gate current (zero) and drain-source offset voltage (zero) also make them nearly ideal switch elements, and also make switched capacitor analog circuits practical. In their linear region, MOSFETs can be used as precision resistors, which can have a much higher controlled resistance than BJTs. In high power circuits, MOSFETs sometimes have the advantage of not suffering from thermal runaway as BJTs do. Also, MOSFETs can be configured to perform as capacitors and gyrator circuits which allow op-amps made from them to appear as inductors, thereby allowing all of the normal analog devices on a chip (except for diodes, which can be made smaller than a MOSFET anyway) to be built entirely out of MOSFETs. This means that complete analog circuits can be made on a silicon chip in a much smaller space and with simpler fabrication techniques.
MOSFETS are ideally suited to switch inductive loads because of tolerance to inductive kickback.
Some ICs combine analog and digital MOSFET circuitry on a single mixed-signal integrated circuit, making the needed board space even smaller. This creates a need to isolate the analog circuits from the digital circuits on a chip level, leading to the use of isolation rings and Silicon-On-Insulator (SOI). Since MOSFETs require more space to handle a given amount of power than a BJT, fabrication processes can incorporate BJTs and MOSFETs into a single device. Mixed-transistor devices are called Bi-FETs (bipolar FETs) if they contain just one BJT-FET and BiCMOS (bipolar-CMOS) if they contain complementary BJT-FETs. Such devices have the advantages of both insulated gates and higher current density.
MOSFET scaling.
Over the past decades, the MOSFET has continually been scaled down in size; typical MOSFET channel lengths were once several micrometres, but modern integrated circuits are incorporating MOSFETs with channel lengths of tens of nanometers. Robert Dennard's work on scaling theory was pivotal in recognising that this ongoing reduction was possible. Intel began production of a process featuring a 32 nm feature size (with the channel being even shorter) in late 2009. The semiconductor industry maintains a "roadmap", the ITRS, which sets the pace for MOSFET development. Historically, the difficulties with decreasing the size of the MOSFET have been associated with the semiconductor device fabrication process, the need to use very low voltages, and with poorer electrical performance necessitating circuit redesign and innovation (small MOSFETs exhibit higher leakage currents, and lower output resistance, discussed below).
Reasons for MOSFET scaling.
Smaller MOSFETs are desirable for several reasons. The main reason to make transistors smaller is to pack more and more devices in a given chip area. This results in a chip with the same functionality in a smaller area, or chips with more functionality in the same area. Since fabrication costs for a semiconductor wafer are relatively fixed, the cost per integrated circuits is mainly related to the number of chips that can be produced per wafer. Hence, smaller ICs allow more chips per wafer, reducing the price per chip. In fact, over the past 30 years the number of transistors per chip has been doubled every 2–3 years once a new technology node is introduced. For example the number of MOSFETs in a microprocessor fabricated in a 45 nm technology can well be twice as many as in a 65 nm chip. This doubling of transistor density was first observed by Gordon Moore in 1965 and is commonly referred to as Moore's law.
It is also expected that smaller transistors switch faster. For example, one approach to size reduction is a scaling of the MOSFET that requires all device dimensions to reduce proportionally. The main device dimensions are the channel length, channel width, and oxide thickness. When they are scaled down by equal factors, the transistor channel resistance does not change, while gate capacitance is cut by that factor. Hence, the RC delay of the transistor scales with a similar factor.
While this has been traditionally the case for the older technologies, for the state-of-the-art MOSFETs reduction of the transistor dimensions does not necessarily translate to higher chip speed because the delay due to interconnections is more significant.
Difficulties arising due to MOSFET size reduction.
Producing MOSFETs with channel lengths much smaller than a micrometre is a challenge, and the difficulties of semiconductor device fabrication are always a limiting factor in advancing integrated circuit technology. Though processes such as ALD have improved fabrication for small components, the small size of the MOSFET (less than a few tens of nanometers) has created operational problems.
Higher subthreshold conduction.
As MOSFET geometries shrink, the voltage that can be applied to the gate must be reduced to maintain reliability. To maintain performance, the threshold voltage of the MOSFET has to be reduced as well. As threshold voltage is reduced, the transistor cannot be switched from complete turn-off to complete turn-on with the limited voltage swing available; the circuit design is a compromise between strong current in the "on" case and low current in the "off" case, and the application determines whether to favor one over the other. Subthreshold leakage (including subthreshold conduction, gate-oxide leakage and reverse-biased junction leakage), which was ignored in the past, now can consume upwards of half of the total power consumption of modern high-performance VLSI chips.
Increased gate-oxide leakage.
The gate oxide, which serves as insulator between the gate and channel, should be made as thin as possible to increase the channel conductivity and performance when the transistor is on and to reduce subthreshold leakage when the transistor is off. However, with current gate oxides with a thickness of around 1.2 nm (which in silicon is ~5 atoms thick) the quantum mechanical phenomenon of electron tunneling occurs between the gate and channel, leading to increased power consumption.
Silicon dioxide has traditionally been used as the gate insulator. Silicon dioxide however has a modest dielectric constant. Increasing the dielectric constant of the gate dielectric allows a thicker layer while maintaining a high capacitance (capacitance is proportional to dielectric constant and inversely proportional to dielectric thickness). All else equal, a higher dielectric thickness reduces the quantum tunneling current through the dielectric between the gate and the channel.
Insulators that have a larger dielectric constant than silicon dioxide (referred to as high-k dielectrics), such as group IVb metal silicates e.g. hafnium and zirconium silicates and oxides are being used to reduce the gate leakage from the 45 nanometer technology node onwards.
On the other hand, the barrier height of the new gate insulator is an important consideration; the difference in conduction band energy between the semiconductor and the dielectric (and the corresponding difference in valence band energy) also affects leakage current level. For the traditional gate oxide, silicon dioxide, the former barrier is approximately 8 eV. For many alternative dielectrics the value is significantly lower, tending to increase the tunneling current, somewhat negating the advantage of higher dielectric constant.
The maximum gate-source voltage is determined by the strength of the electric field able to be sustained by the gate dielectric before significant leakage occurs. As the insulating dielectric is made thinner, the electric field strength within it goes up for a fixed voltage. This necessitates using lower voltages with the thinner dielectric.
Increased junction leakage.
To make devices smaller, junction design has become more complex, leading to higher doping levels, shallower junctions, "halo" doping and so forth, all to decrease drain-induced barrier lowering (see the section on junction design). To keep these complex junctions in place, the annealing steps formerly used to remove damage and electrically active defects must be curtailed increasing junction leakage. Heavier doping is also associated with thinner depletion layers and more recombination centers that result in increased leakage current, even without lattice damage.
DIBL and VT roll off.
Because of the short channel, channel formation is not entirely done by the gate, but now the drain and source also affect the channel formation. As the channel length decreases, the depletion regions of the source and drain come closer together and make the threshold voltage (VT) a function of the length of the channel. This is called VT roll-off. VT also becomes function of drain to source voltage VDS. As we increase the VDS, the depletion regions increase in size, and a considerable amount of charge is depleted by the VDS. The gate voltage required to form the channel is then lowered, and thus, the VT decreases with an increase in VDS. This effect is called drain induced barrier lowering (DIBL).
Lower output resistance.
For analog operation, good gain requires a high MOSFET output impedance, which is to say, the MOSFET current should vary only slightly with the applied drain-to-source voltage. As devices are made smaller, the influence of the drain competes more successfully with that of the gate due to the growing proximity of these two electrodes, increasing the sensitivity of the MOSFET current to the drain voltage. To counteract the resulting decrease in output resistance, circuits are made more complex, either by requiring more devices, for example the cascode and cascade amplifiers, or by feedback circuitry using operational amplifiers, for example a circuit like that in the adjacent figure.
Lower transconductance.
The transconductance of the MOSFET decides its gain and is proportional to hole or electron mobility (depending on device type), at least for low drain voltages. As MOSFET size is reduced, the fields in the channel increase and the dopant impurity levels increase. Both changes reduce the carrier mobility, and hence the transconductance. As channel lengths are reduced without proportional reduction in drain voltage, raising the electric field in the channel, the result is velocity saturation of the carriers, limiting the current and the transconductance.
Interconnect capacitance.
Traditionally, switching time was roughly proportional to the gate capacitance of gates. However, with transistors becoming smaller and more transistors being placed on the chip, interconnect capacitance (the capacitance of the metal-layer connections between different parts of the chip) is becoming a large percentage of capacitance. Signals have to travel through the interconnect, which leads to increased delay and lower performance.
Heat production.
The ever-increasing density of MOSFETs on an integrated circuit creates problems of substantial localized heat generation that can impair circuit operation. Circuits operate more slowly at high temperatures, and have reduced reliability and shorter lifetimes. Heat sinks and other cooling devices and methods are now required for many integrated circuits including microprocessors.
Power MOSFETs are at risk of thermal runaway. As their on-state resistance rises with temperature, if the load is approximately a constant-current load then the power loss rises correspondingly, generating further heat. When the heatsink is not able to keep the temperature low enough, the junction temperature may rise quickly and uncontrollably, resulting in destruction of the device.
Process variations.
With MOSFETS becoming smaller, the number of atoms in the silicon that produce many of the transistor's properties is becoming fewer, with the result that control of dopant numbers and placement is more erratic. During chip manufacturing, random process variations affect all transistor dimensions: length, width, junction depths, oxide thickness "etc.", and become a greater percentage of overall transistor size as the transistor shrinks. The transistor characteristics become less certain, more statistical. The random nature of manufacture means we do not know which particular example MOSFETs actually will end up in a particular instance of the circuit. This uncertainty forces a less optimal design because the design must work for a great variety of possible component MOSFETs. See process variation, design for manufacturability, reliability engineering, and statistical process control.
Modeling challenges.
Modern ICs are computer-simulated with the goal of obtaining working circuits from the very first manufactured lot. As devices are miniaturized, the complexity of the processing makes it difficult to predict exactly what the final devices look like, and modeling of physical processes becomes more challenging as well. In addition, microscopic variations in structure due simply to the probabilistic nature of atomic processes require statistical (not just deterministic) predictions. These factors combine to make adequate simulation and "right the first time" manufacture difficult.
MOSFET construction.
Gate material.
The primary criterion for the gate material is that it is a good conductor. Highly doped polycrystalline silicon is an acceptable but certainly not ideal conductor, and also suffers from some more technical deficiencies in its role as the standard gate material. Nevertheless, there are several reasons favoring use of polysilicon:
While polysilicon gates have been the de facto standard for the last twenty years, they do have some disadvantages which have led to their likely future replacement by metal gates. These disadvantages include:
Present high performance CPUs use metal gate technology, together with high-k dielectrics, a combination known as HKMG (High-K, Metal Gate). The disadvantages of metal gates are overcome by a few techniques:
Insulator.
As devices are made smaller, insulating layers are made thinner, and at some point tunneling of carriers through the insulator from the channel to the gate electrode takes place. To reduce the resulting leakage current, the insulator can be made thicker by choosing a material with a higher dielectric constant. To see how thickness and dielectric constant are related, note that Gauss's law connects field to charge as:
with "Q" = charge density, κ = dielectric constant, ε0 = permittivity of empty space and "E" = electric field. From this law it appears the same charge can be maintained in the channel at a lower field provided κ is increased. The voltage on the gate is given by:
with "VG" = gate voltage, "Vch" = voltage at channel side of insulator, and "tins" = insulator thickness. This equation shows the gate voltage will not increase when the insulator thickness increases, provided κ increases to keep "tins /κ = constant" (see the article on high-κ dielectrics for more detail, and the section in this article on gate-oxide leakage).
The insulator in a MOSFET is a dielectric which can in any event be silicon oxide, but many other dielectric materials are employed. The generic term for the dielectric is gate dielectric since the dielectric lies directly below the gate electrode and above the channel of the MOSFET.
Junction design.
The source-to-body and drain-to-body junctions are the object of much attention because of three major factors: their design affects the current-voltage ("I-V") characteristics of the device, lowering output resistance, and also the speed of the device through the loading effect of the junction capacitances, and finally, the component of stand-by power dissipation due to junction leakage.
The drain induced barrier lowering of the threshold voltage and channel length modulation effects upon "I-V" curves are reduced by using shallow junction extensions. In addition, "halo" doping can be used, that is, the addition of very thin heavily doped regions of the same doping type as the body tight against the junction walls to limit the extent of depletion regions.
The capacitive effects are limited by using raised source and drain geometries that make most of the contact area border thick dielectric instead of silicon.
These various features of junction design are shown (with artistic license) in the figure.
Junction leakage is discussed further in the section increased junction leakage.
Other MOSFET types.
Dual-gate MOSFET.
The dual-gate MOSFET has a tetrode configuration, where both gates control the current in the device. It is commonly used for small-signal devices in radio frequency applications where biasing the drain-side gate at constant potential reduces the gain loss caused by Miller effect, replacing two separate transistors in cascode configuration. Other common uses in RF circuits include gain control and mixing (frequency conversion). The "tetrode" description, though accurate, does not replicate the vacuum-tube tetrode. Vacuum-tube tetrodes, using a screen grid, exhibit much lower grid-plate capacitance and much higher output impedance and voltage gains than triode vacuum tubes. These improvements are commonly an order of magnitude (10 times) or considerably more. Tetrode transistors (whether bipolar junction or field-effect) do not exhibit improvements of such a great degree.
FinFET.
The FinFET, see figure to right, is a double-gate silicon-on-insulator device, one of a number of geometries being introduced to mitigate the effects of short channels and reduce drain-induced barrier lowering. The "fin" refers to the narrow channel between source and drain. A thin insulating oxide layer on either side of the fin separates it from the gate. SOI FinFETs with a thick oxide on top of the fin are called "double-gate" and those with a thin oxide on top as well as on the sides are called "triple-gate" FinFETs.
Depletion-mode MOSFETs.
There are "depletion-mode" MOSFET devices, which are less commonly used than the standard "enhancement-mode" devices already described. These are MOSFET devices that are doped so that a channel exists even with zero voltage from gate to source. To control the channel, a negative voltage is applied to the gate (for an n-channel device), depleting the channel, which reduces the current flow through the device. In essence, the depletion-mode device is equivalent to a normally closed (on) switch, while the enhancement-mode device is equivalent to a normally open (off) switch.
Due to their low noise figure in the RF region, and better gain, these devices are often preferred to bipolars in RF front-ends such as in TV sets. Depletion-mode MOSFET families include BF 960 by Siemens and BF 980 by Philips (dated 1980s), whose derivatives are still used in AGC and RF mixer front-ends.
NMOS logic.
For devices of equal current driving capability, n-channel MOSFETs can be made smaller than p-channel MOSFETs (due to p-channel charge carriers (holes) having lower mobility than do n-channel charge carriers, electrons) and producing only one type of MOSFET on a silicon substrate is cheaper and technically simpler. These were the driving principles in the design of NMOS logic which uses n-channel MOSFETs exclusively. However, neglecting leakage current, unlike CMOS logic, NMOS logic consumes power even when no switching is taking place. With advances in technology, CMOS logic displaced NMOS logic in the mid-1980s to become the preferred process for digital chips.
Power MOSFET.
Power MOSFETs have a different structure than the one presented above. As with most power devices, the structure is vertical and not planar. Using a vertical structure, it is possible for the transistor to sustain both high blocking voltage and high current. The voltage rating of the transistor is a function of the doping and thickness of the N-epitaxial layer (see cross section), while the current rating is a function of the channel width (the wider the channel, the higher the current). In a planar structure, the current and breakdown voltage ratings are both a function of the channel dimensions (respectively width and length of the channel), resulting in inefficient use of the "silicon estate". With the vertical structure, the component area is roughly proportional to the current it can sustain, and the component thickness (actually the N-epitaxial layer thickness) is proportional to the breakdown voltage.
Power MOSFETs with lateral structure are mainly used in high-end audio amplifiers and high-power PA systems. Their advantage is a better behaviour in the saturated region (corresponding to the linear region of a bipolar transistor) than the vertical MOSFETs. Vertical MOSFETs are designed for switching applications.
DMOS.
DMOS stands for double-diffused metal–oxide–semiconductor. Most power MOSFETs are made using this technology.
RHBD MOSFETs.
Semiconductor sub-micrometer and nanometer electronic circuits are the primary concern for operating within the normal tolerance in harsh radiation environments like outer space. One of the design approaches for making a radiation-hardened-by-design (RHBD) device is Enclosed-Layout-Transistor (ELT). Normally, the gate of the MOSFET surrounds the drain, which is placed in the center of the ELT. The source of the MOSFET surrounds the gate. Another RHBD MOSFET is called H-Gate. Both of these transistors have very low leakage current with respect to radiation. However, they are large in size and take more space on silicon than a standard MOSFET.
Newer technologies are emerging for smaller devices for cost saving, low power and increased operating speed. The standard MOSFET is also becoming extremely sensitive to radiation for the newer technologies. A lot more research works should be completed before space electronics can safely use RHBD MOSFET circuits of nanotechnology.
In older STI (shallow trench isolation) designs, radiation strikes near the silicon oxide region cause the channel inversion at the corners of the standard MOSFET due to accumulation of radiation induced trapped charges. If the charges are large enough, the accumulated charges affect STI surface edges along the channel near the channel interface (gate) of the standard MOSFET. Thus the device channel inversion occurs along the channel edges and the device creates off-state leakage path, causing device to turn on. So the reliability of circuits degrades severely. The ELT offers many advantages. These advantages include improvement of reliability by reducing unwanted surface inversion at the gate edges that occurs in the standard MOSFET. Since the gate edges are enclosed in ELT, there is no gate oxide edge (STI at gate interface), and thus the transistor off-state leakage is reduced very much.
Low-power microelectronic circuits including computers, communication devices and monitoring systems in space shuttle and satellites are very different from what we use on earth. They are radiation (high-speed atomic particles like proton and neutron, solar flare magnetic energy dissipation in Earth's space, energetic cosmic rays like X-ray, gamma ray etc.) tolerant circuits. These special electronics are designed by applying very different techniques using RHBD MOSFETs to ensure the safe space journey and also space-walk of astronauts.
MOSFET analog switch.
MOSFET analog switches use the MOSFET to pass analog signals when on, and as a high impedance when off. Signals flow in both directions across a MOSFET switch. In this application, the drain and source of a MOSFET exchange places depending on the relative voltages of the source/drain electrodes. The source is the more negative side for an N-MOS or the more positive side for a P-MOS. All of these switches are limited on what signals they can pass or stop by their gate–source, gate–drain and source–drain voltages; exceeding the voltage, current, or power limits will potentially damage the switch.
Single-type MOSFET switch.
This analog switch uses a four-terminal simple MOSFET of either P or N type.
In the case of an n-type switch, the body is connected to the most negative supply (usually GND) and the gate is used as the switch control. Whenever the gate voltage exceeds the source voltage by at least a threshold voltage, the MOSFET conducts. The higher the voltage, the more the MOSFET can conduct. An N-MOS switch passes all voltages less than Vgate–Vtn. When the switch is conducting, it typically operates in the linear (or ohmic) mode of operation, since the source and drain voltages will typically be nearly equal.
In the case of a P-MOS, the body is connected to the most positive voltage, and the gate is brought to a lower potential to turn the switch on. The P-MOS switch passes all voltages higher than Vgate–Vtp (threshold voltage Vtp is negative in the case of enhancement-mode P-MOS).
A P-MOS switch will have about three times the resistance of an N-MOS device of equal dimensions because electrons have about three times the mobility of holes in silicon.
Dual-type (CMOS) MOSFET switch.
This "complementary" or CMOS type of switch uses one P-MOS and one N-MOS FET to counteract the limitations of the single-type switch. The FETs have their drains and sources connected in parallel, the body of the P-MOS is connected to the high potential (VDD) and the body of the N-MOS is connected to the low potential (Gnd). To turn the switch on, the gate of the P-MOS is driven to the low potential and the gate of the N-MOS is driven to the high potential. For voltages between VDD–Vtn and Gnd–Vtp, both FETs conduct the signal; for voltages less than Gnd–Vtp, the N-MOS conducts alone; and for voltages greater than VDD–Vtn, the P-MOS conducts alone.
The voltage limits for this switch are the gate–source, gate–drain and source–drain voltage limits for both FETs. Also, the P-MOS is typically two to three times wider than the N-MOS, so the switch will be balanced for speed in the two directions.
Tri-state circuitry sometimes incorporates a CMOS MOSFET switch on its output to provide for a low-ohmic, full-range output when on, and a high-ohmic, mid-level signal when off.

</doc>
<doc id="40346" url="http://en.wikipedia.org/wiki?curid=40346" title="JFET">
JFET

The junction gate field-effect transistor (JFET or JUGFET) is the simplest type of field-effect transistor. They are three-terminal semiconductor devices that can be used as electronically-controlled switches, amplifiers, or voltage-controlled resistors. 
Unlike bipolar transistors, JFETs are exclusively voltage-controlled in that they do not need a biasing current. Electric charge flows through a semiconducting channel between "source" and "drain" terminals. By applying a reverse bias voltage to a "gate" terminal, the channel is "pinched", so that the electric current is impeded or switched off completely. A JFET is usually on when there is no potential difference between its gate and source terminals. If a potential difference of the proper polarity is applied between its gate and source terminals, the JFET will be more resistive to current flow, which means less current would flow in the channel between the source and drain terminals. Thus, JFETs are sometimes referred to as depletion-mode devices. 
JFETs can have an n-type or p-type channel. In the n-type, if the voltage applied to the gate is less than that applied to the source, the current will be reduced (similarly in the p-type, if the voltage applied to the gate is "greater" than that applied to the source). A JFET has a large input impedance (sometimes on the order of 1010 ohms), which means that it has a negligible effect on external components or circuits connected to its gate. 
Structure.
The JFET is a long channel of semiconductor material, doped to contain an abundance of positive charge carriers or holes ("p-type"), or of negative carriers or electrons ("n-type"). Ohmic contacts at each end form the source (S) and drain (D). A pn-junction is formed on one or both sides of the channel, or surrounding it, using a region with doping opposite to that of the channel, and biased using an ohmic gate contact (G).
Function.
JFET operation is like that of a garden hose. The flow of water through a hose can be controlled by squeezing it to reduce the cross section; the flow of electric charge through a JFET is controlled by constricting the current-carrying channel. The current also depends on the electric field between source and drain (analogous to the difference in pressure on either end of the hose).
Constriction of the conducting channel is accomplished using the field effect: a voltage between the gate and source is applied to reverse bias the gate-source pn-junction, thereby widening the depletion layer of this junction (see top figure), encroaching upon the conducting channel and restricting its cross-sectional area. The depletion layer is so-called because it is depleted of mobile carriers and so is electrically non-conducting for practical purposes.
When the depletion layer spans the width of the conduction channel, "pinch-off" is achieved and drain to source conduction stops. Pinch-off occurs at a particular reverse bias (VGS) of the gate-source junction. The pinch-off voltage (Vp) varies considerably, even among devices of the same type. For example, VGS(off) for the Temic J202 device varies from −0.8 V to −4 V. Typical values vary from −0.3 V to −10 V.
To switch off an n-channel device requires a negative gate-source voltage (VGS). Conversely, to switch off a p-channel device requires positive VGS.
In normal operation, the electric field developed by the gate blocks source-drain conduction to some extent.
Some JFET devices are symmetrical with respect to the source and drain.
Schematic symbols.
The JFET gate is sometimes drawn in the middle of the channel (instead of at the drain or source electrode as in these examples). This symmetry suggests that "drain" and "source" are interchangeable, so the symbol should be used only for those JFETs where they are indeed interchangeable.
Officially, the style of the symbol should show the component inside a circle (representing the envelope of a discrete device). This is true in both the US and Europe. The symbol is usually drawn without the circle when drawing schematics of integrated circuits. More recently, the symbol is often drawn without its circle even for discrete devices.
In every case the arrow head shows the polarity of the P-N junction formed between the channel and gate. As with an ordinary diode, the arrow points from P to N, the direction of conventional current when forward-biased. An English mnemonic is that the arrow of an N-channel device "points in".
Comparison with other transistors.
At room temperature, JFET gate current (the reverse leakage of the gate-to-channel junction) is comparable to that of a MOSFET (which has insulating oxide between gate and channel), but much less than the base current of a bipolar junction transistor. The JFET has higher transconductance than the MOSFET, as well as lower flicker noise, and is therefore used in some low-noise, high input-impedance op-amps.
History of the JFET.
A succession of FET-like devices were patented by Julius Lilienfeld in the 1920s and 1930s. Materials science and fabrication technology would require decades of advances before FETs could actually be made, however. In 1947, researchers John Bardeen, Walter Houser Brattain, and William Shockley failed in their repeated attempts to make a FET. They discovered the point-contact transistor in the course of trying to diagnose the reasons for their failures. The first practical JFETs were made a decade later.
Mathematical model.
The current in N-JFET due to a small voltage VDS (that is, in the linear ohmic region) is given by treating the channel as a rectangular bar of material of electrical conductivity formula_1: 
where
The drain current in the "saturation region" is often approximated in terms of gate bias as:
where
In the "saturation region", the JFET drain current is most significantly affected by the gate–source voltage and barely affected by the drain–source voltage. 
If the channel doping is uniform, such that the depletion region thickness will grow in proportion to the square root of (the absolute value of) the gate–source voltage, then the channel thickness "b" can be expressed in terms of the zero-bias channel thickness "a" as:
where 
Then the drain current in the linear ohmic region can be expressed as:
or (in terms of formula_6):

</doc>
<doc id="40347" url="http://en.wikipedia.org/wiki?curid=40347" title="House of Babenberg">
House of Babenberg

The House of Babenberg was the ruling noble family of Austria from 976 to 1246. Originally from Bamberg in Franconia in present-day Bavaria, the Babenbergs were counts, margraves, and dukes in the Danube region of present-day Upper Austria, Lower Austria, and Styria.
One or two families.
The Babenberg family can be broken down into two distinct groups: 1) The "Franconian Babenbergs", the so-called "Elder House of Babenberg", or "Popponids" out of which came the Hennebergs and the Counts of Schweinfurt. 2) "Austrian Babenbergs" which ruled Austria. The second group claimed to have originated from the first but scholars have not been able to verify that claim.
Popponids.
Like the French royal Capetian dynasty, the Elder Babenbergs descended from the Robertians. The earliest known Babenberg was one Poppo, maybe a descendant of the Frankish count Cancor. In the early 9th century he appeared as a count in the Grabfeld, a historic region in northeastern Franconia bordering on Thuringia. One of his sons, Henry, sometimes called margrave and duke in Franconia under King Charles the Fat of East Francia, fell fighting against the Normans in 886; another, Poppo, was margrave in Thuringia from 880 to 892, when he was deposed by King Charles successor Arnulf of Carinthia. The Popponids had been favoured by Charles the Fat, but Arnulf reversed this policy in favour of the rival family of the Conradines from the Lahngau in Rhenish Franconia.
The leaders of the Babenbergs were the three sons of Duke Henry, who called themselves after their castle of Babenberg on the upper Main, around which their possessions centred. The city of Bamberg was built around the ancestral castle of the family.
The Babenberg feud.
The rivalry between the Babenberg and Conradine families was intensified by their efforts to extend their authority in the region of the middle Main, and this quarrel, known as the "Babenberg feud", came to a head at the beginning of the 10th century during the troubled reign of the German king Louis the Child. In the battle of Fritzlar in 906, the Conradines won a decisive victory, although count Conrad the Elder fell in the battle. Two of the Babenberg brothers were also killed. The third, Adalbert of Prague, was summoned before the imperial court by the regent Hatto I, Archbishop of Mainz, a partisan of the Conradines. He refused to appear, held his own for a time in his castle at Theres against the king's forces, but surrendered in 906, and in spite of a promise of safe-conduct by Hatto was beheaded.
The Conradines became dukes of Franconia, while the Babenbergs lost their influence in Franconia.
Margraves of Austria.
In 976 Leopold I, a member of the Babenberg family, who was a count in the Donnegau, is described as count of the Eastern March, a district not more than 60 miles in breadth on the eastern frontier of Bavaria which grew into the duchy of Austria. Leopold, who received the mark as a reward for his fidelity to the emperor Otto II during the Bavarian rising in 976, extended its area at the expense of the Hungarians, and was succeeded in 994 by his son Henry I. Henry, who continued his father's policy, was followed in 1018 by his brother Adalbert and in 1055 by his nephew, Ernest, whose marked loyalty to the emperors Henry II and Henry III was rewarded by many tokens of favour.
The succeeding margrave, Leopold II, quarrelled with Henry III, who was unable to oust him from the mark or to prevent the succession of his son Leopold III in 1096. Leopold supported Henry, the son of Henry IV, in his rising against his father, but was soon drawn over to the emperor's side, and in 1106 married the daughter of emperor Henry IV, Agnes, widow of Frederick I of Swabia. He declined the imperial crown in 1125. His zeal in founding monasteries earned for him his surname "the Pious", and canonization by Pope Innocent VIII in 1485. He is regarded as the patron saint of Austria.
Elevation to dukes.
One of Leopold's sons was Otto, bishop of Freising. His eldest son, Leopold IV, became margrave in 1136, and in 1139 received from the German king Conrad III the duchy of Bavaria, which had been forfeited by Henry the Proud. Leopold's brother Henry (surnamed Jasomirgott, allegedly from his favourite oath, "So help me God!") was made count palatine of the Rhine in 1140, and became margrave of Austria on Leopold's death in 1141. Having married Gertrude, the widow of Henry the Proud, he was invested in 1143 with the duchy of Bavaria, and resigned his office as count palatine. In 1147 he went on crusade, and after his return, renounced Bavaria at the instance of the new king Frederick I who gave the duchy of Bavaria to Henry the Lion of Saxony. As compensation for this, Austria, the capital of which had been transferred to Vienna in 1156, was elevated into a duchy in the Privilegium Minus.
The rise of Babenberg power.
The second duke was Henry's son Leopold V, who succeeded him in 1177 and took part in the crusades of 1182 and 1190. In Palestine he quarrelled with Richard I of England, captured him on his homeward journey and handed him over to the emperor Henry VI. Leopold increased the territories of the Babenbergs by acquiring Styria under the will of his kinsman Duke Ottokar IV. He died in 1194, and Austria fell to one son, Frederick, and Styria to another, Leopold; but on Frederick's death in 1198 they were again united by Duke Leopold VI, surnamed "the Glorious".
The new duke fought against the infidels in Spain, Egypt and Palestine, but is more celebrated as a lawgiver, a patron of letters and a founder of towns. Under him Vienna became the centre of culture in Germany and the great school of Minnesingers. His later years were spent in strife with his son Frederick, and he died in 1230 at San Germano, now renamed Cassino, whither he had gone to arrange the peace between the emperor Frederick II and Pope Gregory IX.
The Last of the Babenbergs.
Frederick II, Leopold VI's son by Theodora Angelina, succeeded his father as duke upon the elder man's death in 1230. Frederick II soon earned the epithet "the Quarrelsome" as a result of his ongoing disputes with the kings of Hungary and Bohemia and with the Holy Roman Emperor, also named Frederick II. The Austrian Frederick II deprived his mother and sisters of their possessions, was hated by his subjects on account of his oppressive rule, and, in 1236, was placed under the imperial ban and driven from Austria. However, he was later restored to his duchy when the Emperor Frederick II was excommunicated. Subsequently, the Austrian Frederick II treated with the Emperor Frederick II in vain to make Austria a kingdom. 
The male line of the Babenbergs became extinct in 1246, when Frederick II "the Quarrelsome" was killed in battle (the Henneberg branch of the Franconian Babenbergs lived on until 1583 when its lands where divided among the two branches of the Wettin family).
His heir general was Gertrude of Austria, the only child of his late elder brother, Henry of Austria by that man's wife, Agnes of Thuringia. However, neither her husbands nor her son succeeded in settling the Babenberg inheritance under their power.
After some years of struggle known as the "Interregnum", the Duchies of Austria and Styria fell to Otakar II of Bohemia, and subsequently to Rudolph I of Habsburg, whose descendants were to rule Austria until 1918.
Genetic Legacy.
Byzantine Blood.
All the Babenberg dukes from Leopold V onward were descended from Byzantine emperors - Leopold's mother, Theodora Komnene, being a granddaughter of the Emperor, John II Komnenos. Subsequently, Leopold V's younger son, Leopold VI, also married a Byzantine princess (Theodora Angelina), as did his youngest son (by Theodora), Frederick II, who married "Sophia Laskarina".
The Babenbergs and the Habsburgs.
The next dynasty in Austria - the Habsburgs - were originally not descendants of the Babenbergs. It was not until the children of Albert I of Germany that the Babenberg blood was brought into the Habsburg line, though this blood was from the pre-ducal Babenbergs. A side effect of this marriage was the use of the Babenberg name "Leopold" by the Habsburgs for one of their sons. 
The Habsburgs did eventually gain descent from the Babenberg dukes, though at different times. The first Habsburg line to be descended from the Babenbergs was the "Albertine" line. This was achieved through the marriage of Albert III, Duke of Austria to Beatrix of Nuremberg. As such, their son, Albert IV, Duke of Austria, was the first Habsburg duke who was descended from the Babenberg dukes. However, the male line of that branch of the Habsburgs died out in 1457 with Ladislas V Posthumus of Bohemia.
The next Habsburg line to gain Babenberg blood was the "Styrian" line, which occurred with the children of Ferdinand I, Holy Roman Emperor and Anna of Bohemia and Hungary, the latter of whom descended from Babenberg dukes. It was actually from Elizabeth of Austria, the sister of Ladislas V Posthumus of Bohemia, that the Styrian line gained their Babenberg blood.
The "Spanish" line was the last Habsburg line to gain Babenberg blood. Again it was via the previous Habsburg line to gain Babenberg blood (i.e. the Styrian) that the Spanish Habsburg gained their descent from the Babenbergs - Anna of Austria, the wife of Philip II of Spain and mother of Philip (from whom all subsequent Spanish Habsburgs were descended), was a male-line granddaughter of Ferdinand and Anna. As a result, after 1598, all Habsburg scions descended from the Babenberg Dukes.
References.
</dl>

</doc>
<doc id="40348" url="http://en.wikipedia.org/wiki?curid=40348" title="Sacrosanctum Concilium">
Sacrosanctum Concilium

Sacrosanctum Concilium, the Constitution on the Sacred Liturgy, is one of the constitutions of the Second Vatican Council. It was approved by the assembled bishops by a vote of 2,147 to 4 and promulgated by Pope Paul VI on December 4, 1963. The main aim was to achieve greater lay participation in the Catholic Church's liturgy.
Contents.
"The numbers given correspond to section numbers within the text."
Title and purpose.
As is customary with Catholic documents, the name of this Constitution, "Sacred Council" in Latin, is taken from the first line of the document:1. This sacred Council has several aims in view: it desires to impart an ever increasing vigor to the Christian life of the faithful; to adapt more suitably to the needs of our own times those institutions which are subject to change; to foster whatever can promote union among all who believe in Christ; to strengthen whatever can help to call the whole of mankind into the household of the Church. The Council therefore sees particularly cogent reasons for undertaking the reform and promotion of the liturgy. --—"Sacrosanctum Concilium"
Participation of the laity.
One of the first issues considered by the council, and the matter that had the most immediate effect on the lives of individual Catholics, was the revision of the liturgy. The central idea was that there ought to be greater lay participation in the liturgy.
 Mother Church earnestly desires that all the faithful should be led to that fully conscious and active participation in liturgical celebrations which is demanded by the very nature of the liturgy. Such participation by the Christian people as a chosen race, a royal priesthood, a holy nation, a redeemed people (1 Peter 2:9; cf. 2:4–5), is their right and duty by reason of their baptism.—"Sacrosanctum Concilium" 14
Many have claimed that Vatican II went much further in encouraging "active participation" than previous Popes had allowed or recommended. Popes Pius X, Pius XI, and Pius XII consistently asked that the people be taught how to chant the responses at Mass and that they learn the prayers of the Mass in order to participate intelligently. For its part, Vatican II never asked for the involvement of the laity in the sanctuary that is typical of post-conciliar practice. The council fathers established guidelines to govern the revision of the liturgy, which included allowed and encouraged greater use of the vernacular (native language) in addition to Latin, particularly for the biblical readings and other prayers. As bishops determined, local or national customs could be cautiously incorporated into the liturgy.
Consilium.
Implementation of the Council's directives on the liturgy was carried out under the authority of Pope Paul VI by a special papal commission, later incorporated in the Congregation for Divine Worship and the Discipline of the Sacraments, and, in the areas entrusted to them, by national conferences of bishops, which, if they had a shared language, were expected to collaborate in producing a common translation.

</doc>
<doc id="40351" url="http://en.wikipedia.org/wiki?curid=40351" title="The Cathedral and the Bazaar">
The Cathedral and the Bazaar

The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (abbreviated CatB) is an essay, and later a book, by Eric S. Raymond on software engineering methods, based on his observations of the Linux kernel development process and his experiences managing an open source project, fetchmail. It examines the struggle between top-down and bottom-up design. The essay was first presented by the author at the Linux Kongress on May 27, 1997 in Würzburg and was published as part of the book in 1999.
The illustration on the cover of the book is a 1913 painting by titled "Composition with Figures" and belongs to the collection of the State Tretyakov Gallery.
"The Cathedral and the Bazaar".
The essay contrasts two different free software development models:
The essay's central thesis is Raymond's proposition that "given enough eyeballs, all bugs are shallow" (which he terms Linus's Law): the more widely available the source code is for public testing, scrutiny, and experimentation, the more rapidly all forms of bugs will be discovered. In contrast, Raymond claims that an inordinate amount of time and energy must be spent hunting for bugs in the Cathedral model, since the working version of the code is available only to a few developers.
Lessons for creating good open source software.
Raymond points to 19 "lessons" learned from various software development efforts, each describing attributes associated with good practice in open source software development:
Legacy.
In 1998, the essay helped the final push for Netscape Communications Corporation to release the source code for Netscape Communicator and start the Mozilla project; it was cited by Frank Hecker and other employees as an outside independent validation of his arguments. Netscape's public recognition of this influence brought Raymond renown in hacker culture.
When O'Reilly Media published the book in 1999, it became the first complete and commercially distributed book published under the Open Publication License.
Marshall Poe, in his essay "The Hive", likens Wikipedia to the Bazaar model that Raymond defines. Jimmy Wales himself was in fact inspired by the work (as well as arguments put forward in works prior to the Internet age, such as Friedrich Hayek's "The Use of Knowledge in Society"), arguing that "It opened my eyes to the possibility of mass collaboration".
References.
</dl>

</doc>
<doc id="40352" url="http://en.wikipedia.org/wiki?curid=40352" title="Gaudium et spes">
Gaudium et spes

Gaudium et spes (], "Joy and Hope"), the Pastoral Constitution on the Church in the Modern World, was one of the four Apostolic Constitutions resulting from the Second Vatican Council. The document is an overview of the Catholic Church's teachings about humanity's relationship to society, especially in reference to economics, poverty, social justice, culture, science, technology and ecumenism.
Approved by a vote of 2,307 to 75 of the bishops assembled at the council, it was promulgated by Pope Paul VI on 7 December 1965, the day the council ended. As is customary with Catholic documents, the title is taken from its incipit in Latin:
Overview.
The document was not drafted before the council met, but arose from the floor of the council and was one of the last to be promulgated. Pope John XXIII, being deathly ill at the time, was forced to watch the proceedings on closed circuit television. He was too sick to attend, and died within months.
The previous Vatican Council in 1869-70 had tried to defend the role of the church in an increasingly secular world. Those who interpret the purpose of the Second Council as one of embracing this world use "Gaudium et spes" as the primary hermeneutic for all its documents. One of the cardinals, Leo Joseph Suenens of Belgium, urged the council to take on social responsibility for Third World suffering, International peace and war, and the poor.
Marie-Dominique Chenu, famed professor of the Pontifical University of Saint Thomas Aquinas, "Angelicum" was influential in the composition of "Gaudium et spes".
Contents.
"The numbers given correspond to section numbers within the text."
Ecumenical impact.
The document has had a huge influence on the social teachings of the wider Christian churches and communities, especially the churches that belong to the World Council of Churches.

</doc>
<doc id="40353" url="http://en.wikipedia.org/wiki?curid=40353" title="London, Ontario">
London, Ontario

London is a Canadian city located in Southwestern Ontario along the Quebec City–Windsor Corridor. The city has a population of 366,151 according to the 2011 Canadian census. London is at the forks of the non-navigable Thames River, approximately halfway between Toronto, Ontario and Detroit, Michigan. The City of London is a separated municipality, politically separate from Middlesex County, though it remains the county seat.
London and the Thames were named in 1793 by Lord Simcoe, who proposed the site for the capital of Upper Canada. The first European settlement was between 1801 and 1804 by Peter Hagerman. The village was founded in 1826 and incorporated in 1855. Since then, London has grown to be the largest Southwestern Ontario municipality, and Canada's 11th largest municipality, having annexed many of the smaller communities that surrounded it.
London is a regional centre of health care and education, being home to the University of Western Ontario, Fanshawe College, and several hospitals. The city hosts a number of musical and artistic exhibits and festivals, which contribute to its tourism industry, but its economic activity is centred on education, medical research, insurance, and information technology. London's university and hospitals are among its top ten employers. London lies at the junction of Highway 401 and 402, connecting it to Toronto, Windsor, and Sarnia. It also has an international airport, train and bus station.
History.
Founding.
Prior to European contact in the 18th century, the present site of London was occupied by several Neutral and Odawa/Ojibwa villages. Archaeological investigations in the region indicate that aboriginal people have resided in the area for at least the past 10,000 years.
The current location of London was selected as the site of the future capital of Upper Canada in 1793 by Lieutenant-Governor John Graves Simcoe. Simcoe intended to name the settlement "Georgina", in honour of King George III, and renamed the river. However, the choice of a capital site in the midst of extensive hardwood forests was rejected by Guy Carleton (Governor Dorchester). In 1814, there was a skirmish during the War of 1812 in what is now southwest London at Reservoir Hill, formerly Hungerford Hill.
The village of London, named after the English capital of London, was not founded until 1826, and not as the capital Simcoe envisioned. Rather, it was an administrative seat for the area west of the actual capital, York (now Toronto). Locally, it was part of the Talbot Settlement, named for Colonel Thomas Talbot, the chief coloniser of the area, who oversaw the land surveying and built the first government buildings for the administration of the Western Ontario peninsular region. Together with the rest of Southwestern Ontario, the village benefited from Talbot's provisions, not only for building and maintaining roads, but also for assignment of access priorities to main routes to productive land. At the time, Crown and clergy reserves were receiving preference in the rest of Ontario.
In 1832, the new settlement suffered an outbreak of cholera. London proved a centre of strong Tory support during the Upper Canada Rebellion of 1837, notwithstanding a brief rebellion led by Dr. Charles Duncombe. Consequently, the British government located its Ontario peninsular garrison there in 1838, increasing its population with soldiers and their dependents, and the business support populations they required. London was incorporated as a town in 1840.
On 13 April 1845, fire destroyed much of London, which was at the time largely constructed of wooden buildings. One of the first casualties was the town's only fire engine. This fire burned nearly 30 acres of land destroying 150 buildings before burning itself out later the same day. One-fifth of London was destroyed and this was the province's first million dollar fire.
On 1 January 1855, London was incorporated as a "city" (10,000 or more residents). In the 1860s, a sulphur spring was discovered at the forks of the Thames River while industrialists were drilling for oil. The springs became a popular destination for wealthy Ontarians, until the turn of the 20th century when a textile factory was built at the site, replacing the spa.
Development.
Sir John Carling, Tory MP for London, gave three events to explain the development of London in a 1901 speech. They were: the location of the court and administration in London in 1826; the arrival of the military garrison in 1838; and the arrival of the railway in 1853.
In 1875, London's first iron bridge, the Blackfriars Street Bridge, was constructed. It replaced a succession of flood-failed wooden structures that had provided the city's only northern road crossing of the river. A rare example of a bowstring truss bridge, the Blackfriars remains open to pedestrian and bicycle traffic, though it is currently closed indefinitely to vehicular traffic due to various structural problems. The Blackfriars, amidst the river-distance between the Carling Brewery and the historic Tecumseh Park (including a major mill), linked London with its western suburb of Petersville, named for Squire Peters of Grosvenor Lodge. That community joined with the southern subdivision of Kensington in 1874, formally incorporating as the municipality of Petersville. Although it changed its name in 1880 to the more inclusive "London West", it remained a separate municipality until ratepayers voted for amalgamation with London in 1897, largely due to repeated flooding. The most serious flood was that of July 1883, which resulted in serious loss of life and property devaluation. This area retains much original and attractively maintained 19th-century tradespeople's and workers' housing, including Georgian cottages as well as larger houses, and a distinct sense of place.
London's eastern suburb, London East, was (and remains) an industrial centre, which also incorporated in 1874. Attaining the status of town in 1881, it continued as a separate municipality until concerns over expensive waterworks and other fiscal problems led to amalgamation in 1885. The southern suburb of London, including Wortley Village, was collectively known as "London South". Never incorporated, the South was annexed to the city in 1890, although Wortley Village still retains a distinct sense of place. By contrast, the settlement at Broughdale on the city's north end had a clear identity, adjoined the university, and was not annexed until 1961.
On 24 May 1881, the ferry "SS Victoria" capsized in the Thames River, drowning approximately 200 passengers, the worst disaster in London's history. Two years later, on 12 July 1883, the first of the two most devastating floods in London's history killed 17 people. The second major flood, on 26 April 1937, destroyed more than a thousand houses and caused over $50 million in damages, particularly in West London. After repeated floods the Upper Thames River Conservation Authority in 1953 built Fanshawe Dam on the North Thames to control the downstream rivers. Financing for this project came from the federal, provincial, and municipal governments. Other natural disasters include a 1984 tornado that led to damage on several streets in the White Oaks area of South London.
London's role as a military centre continued into the 20th century during the two World Wars, serving as the administrative centre for the Western Ontario district. In 1905, the London Armoury was built and housed the First Hussars until 1975. A private investor purchased the historic site and built a new hotel (Delta London Armouries, 1996)in its place preserving the shell of the historic building. In the 1950s, two reserve battalions amalgamated and became London and Oxford Rifles (3rd Battalion), The Royal Canadian Regiment. This unit continues to serve today as 4th Battalion, The Royal Canadian Regiment. The Regimental Headquarters of The Royal Canadian Regiment remains in London at Wolseley Barracks on Oxford Street. The barracks are home to the First Hussars militia regiment as well.
Annexation to present.
London annexed many of the surrounding communities in 1961, including Byron and Masonville, adding 60,000 people and more than doubling its area. After this amalgamation, suburban growth accelerated as London grew outward in all directions, creating expansive new subdivisions such as Westmount, Oakridge, Whitehills, Pond Mills, White Oaks and Stoneybrook.
In 1992, London annexed nearly the entire township of Westminster, a large, primarily rural municipality directly south of the city, including the police village of Lambeth. With this massive annexation, London almost doubled in area again, adding several thousand more residents. London now stretches south to the boundary with Elgin County.
The 1993 annexation made London one of the largest urban municipalities in Ontario. Intense commercial and residential development is presently occurring in the southwest and northwest areas of the city. Opponents of this development cite urban sprawl, destruction of rare Carolinian zone forest and farm lands, replacement of distinctive regions by generic malls, and standard transportation and pollution concerns as major issues facing London. The City of London is currently the eleventh-largest urban area in Canada, eleventh-largest census metropolitan area in Canada, and the sixth-largest city in Ontario.
Geography.
The area was formed during the retreat of the glaciers during the last ice age, which produced areas of marshland, notably the Sifton Bog (which is actually a fen), as well as some of the most agriculturally productive areas of farmland in Ontario.
The Thames River dominates London's geography. The North and South branches of the Thames River meet at the centre of the city, a location known as "The Forks" or "The Fork of the Thames." The North Thames runs through the man-made Fanshawe Lake, located in northeast London. Fanshawe Lake was created by Fanshawe Dam, constructed to protect the downriver areas from the catastrophic flooding which affected the city in 1883 and 1937.
Climate.
London has a humid continental climate (Köppen "Dfb"), though due to its windward location relative to Lake Huron and elevation changes across the city, it is virtually on the Dfa/Dfb (hot summer) boundary favouring the former climate zone to the southwest of the confluence of the South and North Thames Rivers, and the latter zone to the northeast (including the airport). In 2012, London satisfied the requirements for the Dfa climate type, with a July average of 22 C. Because of its location in the continent, London experiences large seasonal contrast, tempered to a point by the surrounding Great Lakes. The summers are usually warm to hot and humid, with a July average of 20.8 C, and temperatures above 30 C occur on average 10 days per year. In 2012, however, temperatures at or above 30 C occurred a total of 27 times. The city is affected by frequent thunderstorms due to hot, humid summer weather, as well as the convergence of breezes originating from Lake Huron and Lake Erie. The same convergence zone is responsible for spawning funnel clouds and the occasional tornado. London is located in Canada's Tornado Alley. Spring and autumn in between are not long, and winters are cold but witness frequent thaws. Annual precipitation averages nearly 1011 mm. Its winter snowfall totals are heavy, averaging slightly over 2 m per year. The majority of it comes from lake effect snow and snow squalls originating from Lake Huron, some 60 km to the northwest, which occurs when strong, cold winds blow from that direction. From 5 December 2010, to 9 December 2010, London experienced record snowfall when up to 2 m of snow fell in parts of the city. Schools and businesses were closed for three days and bus service was cancelled after the second day of snow.
Parks.
London has a number of parks. Victoria Park in downtown London is a major centre of community events, attracting an estimated 1 million visitors per year. Other major parks include Harris Park, Gibbons Park, Fanshawe Conservation Area (Fanshawe Pioneer Village), Springbank Park, and Westminster Ponds. The city also maintains a number of gardens and conservatories.
Demographics.
According to the 2011 census, the city of London had a population of 366,151 people, a 3.9% increase from the 2006 population. Children under five accounted for approximately 5.2 percent of the resident population of London. The percentage of the resident population in London of retirement age (65 and over) is 13.7, also the percentage for Canada as a whole. The average age is 38.2 years of age, compared to 39.9 years of age for all of Canada.
Between 2006 and 2011, the population of metropolitan London grew by 3.7 percent, compared with an increase of 5.7 percent for Ontario as a whole.
According to the 2011 census, the majority of Londoners profess a Christian faith, which accounts for 62.8 percent of the population (Roman Catholic: 27.0%, Protestant: 25.0%, other Christian: 9.0%). Other religions include Islam (4.4%), Buddhism (0.8%), Hinduism (0.8%), and Judaism (0.5%), with 29.9 percent of the population reporting no religious affiliation.
According to the 2011 census, 82.0 percent of the population of London are White, 2.7 percent are Latin American, 2.6 percent are Arab, 2.4 percent are Black, 2.2 percent are South Asian, 2.0 percent are Chinese, 1.9 percent are Aboriginal, 1.0 percent are Southeast Asian, 0.8 percent are West Asian, 0.8 percent are Korean, 0.6 percent are Filipino, and 0.7 percent belong to other groups. In the 2011 census, the predominant ethnic origins of Londoners were English (30.5%), Canadian (26.0%), Scottish (20.8%), Irish (20.3%), German (11.5%), French (10.1%), Dutch (6.2%), Italian (4.7%), Polish (4.4%), Portuguese (2.8%), and Ukrainian (2.5%).
Economy.
London's economy is dominated by medical research, insurance, manufacturing, and information technology. Much of the life sciences and biotechnology-related research is conducted or supported by the University of Western Ontario, which adds about C$1.5 billion to the London economy annually.
The headquarters of the Canadian division of 3M are located in London. The London Life Insurance Company was founded there, as was Imperial Oil (in 1880) and both the Labatt and Carling breweries. The Libro Financial Group was founded in London 1951 and is the second largest credit union in Ontario and employs over 550 people.Canada Trust was also founded in London in 1864. The TD-Canada Trust tower is still one of the tallest buildings in London, and has been home to two nesting peregrine falcons for more than a decade.
General Dynamics Land Systems builds armoured personnel carriers in the city. A $223 million expansion project in 1984 temporarily made Kellogg's Canada's 106000 m2 London plant one of the most technologically advanced manufacturing facilities in the Kellogg Company. In late 2013, Kellogg's announced the closure of this plant by end of 2014, resulting in 500 jobs lost (production to move to Belleville and Michigan plants).
A portion of the city's population work in factories outside of the city limits, including the General Motors automotive plant CAMI, and a Toyota plant in Woodstock. A Ford plant in Talbotville became one of the casualties of the economic crisis in 2011.
In 1999, the Western Fair Association introduced slot machines. Currently, 750 slot machines operate at the fair grounds year-round. McCormick Canada, formerly Club House Foods, was founded in 1883 and currently employs more than 500 Londoners.
London's city centre mall, Galleria, renamed Citi Plaza in 2009, has suffered since the 2000 collapse of Eaton's and the loss of its Hudson's Bay Company store. The large space left empty by the departure of the Bay has since been filled by London's central library. Other sections of Galleria/Citi Plaza have also lost businesses, which have been replaced by campuses for London's major post-secondary education schools, Fanshawe College and the University of Western Ontario. London Mews, another downtown mall, was demolished in 2001 and replaced by parking lots.
11 December 2009, Minister of State| Gary Goodyear announced a new $11-million cargo terminal at the London International Airport.
Culture.
The city is home to many festivals, funded by the London Arts Council, including Sunfest, the Home County Folk Festival, the London Fringe Theatre Festival, the Expressions in Chalk Street Painting Festival, Rock the Park, Western Fair, the London Ontario Live Arts Festival (LOLA) and The International Food Festival]]. The London Rib-Fest, where barbecue ribs are cooked and served, is the second largest barbecue rib festival in North America. Pride London Festival is the 11th largest Pride festival in Ontario. Sunfest, a World music festival, is the second biggest in Canada after Caribana in Toronto, and is among the top 100 summer destinations in North America.
Musically, London is home to Orchestra London, the London Youth Symphony, noise music pioneers the Nihilist Spasm Band, and the Amabile Choirs of London, Canada.
London is home to several museums, including Museum London, which is located at the Forks of the Thames. Museum London exhibits art by a wide variety of local, regional and national artists. London is also home to the Museum of Ontario Archaeology, owned by the University of Western Ontario (UWO). Its main feature is Canada's only on-going excavation and partial reconstruction of a prehistoric village of the Neutral Nation (Lawson Site). Other museums include the London Regional Children’s Museum, the Royal Canadian Regiment Museum, and the Secrets of Radar Museum. The Guy Lombardo museum closed to the public in 2007 but its collection remains in London.
London is also home to the McIntosh Gallery, an art gallery on the UWO campus, and the Grand Theatre, a professional theatre. The Open House Arts Collective is involved in promoting cultural activities in London. The London Public Library also hosts art exhibitions and author readings. The Writers Resource Center is the home of the Canadian Poetry Association London Chapter. The Forest City Gallery is one of Canada's first artist run centres.
Eldon House is the former residence of the prominent Harris Family and oldest surviving such building in London. The entire property was donated to the city of London in 1959 and is now a heritage site. An Ontario Historical Plaque was erected by the province to commemorate The Eldon House's role in Ontario's heritage. The Banting House National Historic Site of Canada is the house where Sir Frederick Banting thought of the idea that led to the discovery of insulin. Banting lived and practiced in London for ten months, from July 1920 to May 1921. London is also the site of the Flame of Hope, which is intended to burn until a cure for diabetes is discovered.
For famous persons born in London, Ontario see List of people from London, Ontario.
Sports.
London is currently the home of the London Knights of the Ontario Hockey League, who play at the Budweiser Gardens (previously known as the John Labatt Centre). The Knights were both 2004-2005 OHL and Memorial Cup Champions. During the summer months, the London Majors of the Intercounty Baseball League play at Labatt Park. London City of the Canadian Soccer League, the second tier of professional Canadian Association Football, is the highest level of soccer in London. The club was founded in 1973; it is the oldest active professional soccer franchise in North America. The squad plays at Cove Road Stadium at the German Canadian Club. Other sports teams include the London Silver Dolphins Swim Team, the Forest City Volleyball Club, , the London St. George's Rugby Club, the London Aquatics Club, the London Rhythmic Gymnastics Club, London City Soccer Club and Forest City London.
Football teams include the London Junior Mustangs (Ontario Varsity Football League) London Lords (WMLF), the London Beefeaters (Ontario Football Conference), Forest City Thunderbirds (Ontario Football Conference).
London's basketball team, the London Lightning plays at Budweiser Gardens as members of the National Basketball League of Canada. Finishing their inaugural regular season at 28-8, the Lightning would go on to win the 2011-12 NBL Canada championship, defeating the Halifax Rainmen in the finals three games to two.
There are also a number of former sports teams that have now either moved or folded. London's four former baseball teams are the London Monarchs (Canadian Baseball League), the London Werewolves (Frontier League), the London Tecumsehs (International Association) and the London Tigers (AA Eastern League). Other former sports teams include the London Lasers (Canadian Soccer League) and the London Nationals (Western Ontario Hockey League).
The University of Western Ontario teams play under the name "Mustangs". The university's football team plays at TD Waterhouse Stadium. Western's Rowing Team rows out of one of two National Training Centres at Fanshawe Lake. Fanshawe College teams play under the name "Falcons". The Women's Cross Country team has won 3 consecutive Canadian Collegiate Athletic Association (CCAA) National Championships. In 2010, the program cemented itself as the first CCAA program to win both Men's and Women's National team titles, as well as CCAA Coach of the Year.
The Western Fair Raceway, about 85 acres harness racing track and simulcast centre, operates year-round. The grounds include a coin slot casino, a former IMAX theatre, and Sports and Agri-complex. Labatt Memorial Park the world's oldest continuously used baseball grounds, was established as Tecumseh Park in 1877; it was renamed in 1937, because the London field has been flooded and rebuilt twice (1883 and 1937), including a re-orientation of the bases (after the 1883 flood). The Forest City Velodrome, located at the former London Ice House, is the only indoor cycling track in Ontario and the third to be built in North America, opened in 2005.
Law and government.
London's municipal government is divided among fourteen councillors (one representing each of London's fourteen wards) and the mayor. Matt Brown was elected mayor in the 2014 municipal election, officially taking office on 1 December 2014[ [update]]. Prior to Brown's election, London's most recent elected mayor was Joe Fontana; following Fontana's resignation on 19 June 2014, city councillor Joe Swan served as acting mayor until councillor Joni Baechler was selected as interim mayor 24 June. Until the elections in 2010, there was a Board of Control, consisting of four controllers and the mayor, all elected city-wide.
The composition of London City Council was challenged by two ballot questions during the civic election of 2003. A proposal to restructure the municipal government would have seen the council reduced to ten wards and the Board of Control eliminated. The council could not come to a determination and as a result decided to put two questions on the ballot for the fall 2003 election: whether city council should be reduced in size and whether the Board of Control should be eliminated. While the "yes" votes prevailed in both instances, the voter turnout failed to exceed 50 per cent and was therefore insufficient to make the decisions binding under the "Municipal Act". When the council voted to retain the status quo, Imagine London, a citizens group, petitioned the Ontario Municipal Board (OMB) to change the ward composition of the city from seven wards in a roughly radial pattern from the downtown core, to 14 wards defined by communities of interest.
The OMB ruled for the petitioners in December 2005 and, while the city sought leave to appeal the OMB decision via the courts, leave was denied on 28 February 2006, in a decision of Superior Court's Justice McDermid. In response, the city conceded change, but asked for special legislation from the province to ensure that there will only be one councillor in each of the 14 new wards, not two. 1 June 2006, the Ontario bill received royal assent, which guarantees that London will have one councillor per ward.
Although London has many ties to Middlesex County, it is now "separated" and the two have no jurisdictional overlap. The exception is the Middlesex County courthouse and former jail, as the judiciary is administered directly by the province.
In the provincial government, London is represented by Liberal Deb Matthews (London North Centre); Progressive Conservative Jeff Yurek (Elgin—Middlesex—London), and NDPs: Teresa Armstrong (London—Fanshawe) and Peggy Sattler (London West). In the federal government, London is represented by Conservatives Ed Holder (London West), Joe Preston (Elgin—Middlesex—London) and Susan Truppe (London North Centre), and NDP Irene Mathyssen (London—Fanshawe).
Civic initiatives.
The City of London initiatives in Old East London are helping to create a renewed sense of vigour in the East London Business District. Specific initiatives include the creation of the Old East Heritage Conservation District under Part V of the "Ontario Heritage Act", special Building Code policies and Facade Restoration Programs.
London is home to heritage properties representing a variety of architectural styles, including Queen Anne, Art Deco, Modern, and Brutalist
Londoners have become protective of the trees in the city, protesting "unnecessary" removal of trees. The City Council and tourist industry have created projects to replant trees throughout the city. As well, they have begun to erect metal trees of various colours in the downtown area, causing some controversy.
Transportation.
Road transportation.
London is at the junction of Highway 401 that connects the city to Toronto and Detroit, and Highway 402 to Sarnia. Also, Highway 403, which diverges from the 401 at nearby Woodstock, Ontario, provides ready access to Brantford, Hamilton, the Golden Horseshoe area, and the Niagara Peninsula. Many smaller two-lane highways also pass through or near London, including Kings Highways 2, 3, 4, 7 and 22. Many of these are "historical" names, as provincial downloading in the 1980s and 1990s put responsibility for most provincial highways on municipal governments. Nevertheless, these roads continue to provide access from London to nearby communities and locations in much of Western Ontario, including Goderich, Port Stanley and Owen Sound.
Since the 1970s, London has improved urban road alignments that eliminated "jogs" in established traffic patterns over 19th-century street mis-alignments. The lack of a municipal freeway (either through or around the city) as well as the presence of two significant railways (each with attendant switching yards and few over/under-passes) are the primary causes of rush hour congestion, along with construction and heavy snow. Thus, traffic times can be significantly variable, although major traffic jams are rare. Wellington Road between Commissioners Road E and Southdale Road E is London's busiest section of roadway, with more than 46,000 vehicles using the span on an average day City council rejected early plans for the construction of a freeway, and instead accepted the Veterans Memorial Parkway to serve the east end. Some Londoners have expressed concern that the absence of a local freeway may hinder London's economic and population growth, while others have voiced concern that such a freeway would destroy environmentally sensitive areas and further contribute to London's already uncontrolled suburban sprawl. Road capacity improvements have been made to Veterans Memorial Parkway (formerly named Airport Road and Highway 100) in the industrialized east end. However, the Parkway has received criticism for not being built as a proper highway; a recent city-run study suggested upgrading it by replacing the intersections with interchanges.
London's public transit system is run by the London Transit Commission, which has 38 bus routes throughout the city. The Transit Commission has been improving bus service over the years, but not enough to cope with the city's growing number of riders during peak periods. Bus service is currently the only mode of public transit available to the public in London, with no ground light rail or rapid transit networks like those used in other Canadian cities. London does have several taxi and for-hire limousine services. Recently, London has constructed cycleways along some of its major arteries in order to encourage a reduction in automobile use.
Intercity transport.
London is on the Canadian National Railway main line between Toronto and Chicago (with a secondary main line to Windsor) and the Canadian Pacific Railway main line between Toronto and Detroit. Via Rail operates regional passenger service through London station as part of the Quebec City–Windsor Corridor, with connections to the United States. Via Rail's London terminal is the fourth-busiest passenger terminal in Canada.
London is also a destination for inter-city bus travellers. London is the seventh-busiest Greyhound Canada terminal in terms of passengers, and connecting services radiate from London throughout Southwestern Ontario and through to the American cities of Detroit, Michigan and Chicago, Illinois.
Aboutown Transportation is a diversified transportation company based in the city that operates the "North Link", intercity bus service from Owen Sound, and six transit bus routes between Kings and Brescia Colleges, and the main campus at the University of Western Ontario.
London International Airport (YXU) is the 12th busiest passenger airport in Canada and the 11th busiest airport in Canada by take-offs and landings. It is served by airlines including Air Canada Jazz, United Airlines and WestJet, and provides direct flights to both domestic and international destinations, including Toronto, Chicago, Las Vegas, Orlando, Ottawa, Winnipeg, Calgary and Cancún.
Plans.
The city of London is considering bus rapid transit (BRT) and/or high-occupancy vehicle lanes (HOV) to help it achieve its long-term transportation plan. Additional cycleways are planned for integration in road-widening projects, where there is need and sufficient space along routes. An expressway/freeway network is possible along the eastern and western ends of the city, from Highway 401 (and Highway 402 for the western route) past Oxford Street, potentially with another highway, joining the two in the city's north end.
A parclo interchange between Highway 401 and Wonderland Road has been planned to move traffic more efficiently through the city's southwest end. It will probably be built when the Ontario Ministry of Transportation widens Highway 401 from four to six lanes between Highway 4 and Highway 402 and reconstructs the outdated cloverleaf interchange with nearby Colonel Talbot Road. Construction will begin in 2013.
The City of London has assessed the entire length of the Veterans Memorial Parkway, identifying areas where interchanges can be constructed, grade separations can occur, and where cul-de-sacs can be placed. Upon completion, the Veterans Memorial Parkway would no longer be an expressway, but a freeway, for the majority of its length.
A high-speed rail station has been proposed for London, connecting it to a future high-speed rail line along the Quebec City-Windsor corridor. It would run along the Canadian National rail right of way through the city.
Education.
London public elementary and secondary schools are governed by four school boards – the Thames Valley District School Board, the London District Catholic School Board and the French first language school boards (the "Conseil scolaire Viamonde" and the "Conseil scolaire catholique Providence" or CSC). The CSC has a satellite office in London.
There are also more than twenty private schools in the city, including Nancy Campbell Collegiate Institute, Canada's only remaining Bahá'í school.
London is home to London Central Secondary School, the highest ranking academic school in Ontario.
The city is home to two post-secondary institutions: the University of Western Ontario (UWO) and Fanshawe College, a college of applied arts and technology. UWO, founded in 1878, has about 3500 full-time faculty and staff members and almost 30,000 undergraduate and graduate students. It placed tenth in the 2008 "Maclean's" magazine rankings of Canadian universities. The Richard Ivey School of Business, part of UWO, was formed in 1922 and ranked among the best business schools in the country by the Financial Times in 2009. UWO has three affiliated colleges: Brescia University College, founded in 1919 (Canada's only university-level women's college); Huron University College, founded in 1863 (also the founding college of UWO) and King's University College, founded in 1954. All three are liberal arts colleges with religious affiliations: Huron with the Anglican Church of Canada, King's and Brescia with the Roman Catholic Church. London is also home to Lester B. Pearson School for the Arts one of few of its kind.
Fanshawe College has an enrollment of approximately 15,000 students, including 3,500 apprentices and over 500 international students from over 30 countries. It also has almost 40,000 students in part-time continuing education courses. Fanshawe's Key Performance Indicators (KPI) have been over the provincial average for many years now, with increasing percentages year by year.
The Ontario Institute of Audio Recording Technology (OIART) is also in London. Founded in 1983, it offers recording studio experience for audio engineering students.
Westervelt College is also located in London. This private career college was founded in 1885 and offers several diploma programs.
Sister cities.
London currently has one sister city:

</doc>
<doc id="40355" url="http://en.wikipedia.org/wiki?curid=40355" title="Optimizing compiler">
Optimizing compiler

In computing, an optimizing compiler is a compiler that tries to minimize or maximize some attributes of an executable computer program. The most common requirement is to minimize the time taken to execute a program; a less common one is to minimize the amount of memory occupied. The growth of portable computers has created a market for minimizing the power consumed by a program. Compiler optimization is generally implemented using a sequence of "optimizing transformations", algorithms which take a program and transform it to produce a semantically equivalent output program that uses fewer resources.
It has been shown that some code optimization problems are NP-complete, or even undecidable. In practice, factors such as the programmer's willingness to wait for the compiler to complete its task place upper limits on the optimizations that a compiler implementor might provide. (Optimization is generally a very CPU- and memory-intensive process.) In the past, computer memory limitations were also a major factor in limiting which optimizations could be performed. Because of all these factors, optimization rarely produces "optimal" output in any sense, and in fact an "optimization" may impede performance in some cases; rather, they are heuristic methods for improving resource usage in typical programs.
Types of optimization.
Techniques used in optimization can be broken up among various "scopes" which can affect anything from a single statement to the entire program. Generally speaking, locally scoped techniques are easier to implement than global ones but result in smaller gains. Some examples of scopes include:
In addition to scoped optimizations there are two further general categories of optimization:
The following is an instance of a local machine dependent optimization. To set a register to 0, the obvious way is to use the constant '0' in an instruction that sets a register value to a constant. A less obvious way is to XOR a register with itself. It is up to the compiler to know which instruction variant to use. On many RISC machines, both instructions would be equally appropriate, since they would both be the same length and take the same time. On many other microprocessors such as the Intel x86 family, it turns out that the XOR variant is shorter and probably faster, as there will be no need to decode an immediate operand, nor use the internal "immediate operand register". (A potential problem with this is that XOR may introduce a data dependency on the previous value of the register, causing a pipeline stall. However, processors often have XOR of a register with itself as a special case that doesn't cause stalls.)
Common themes.
To a large extent, compiler optimization techniques have the following themes, which sometimes conflict.
Specific techniques.
Loop optimizations.
Some optimization techniques primarily designed to operate on loops include
Data-flow optimizations.
Data-flow optimizations, based on data-flow analysis, primarily depend on how certain properties of data are propagated by control edges in the control flow graph. Some of these include:
SSA-based optimizations.
These optimizations are intended to be done after transforming the program into a special form called static single assignment (see SSA form), in which every variable is assigned in only one place. Although some function without SSA, they are most effective with SSA. Many optimizations listed in other sections also benefit with no special changes, such as register allocation.
Functional language optimizations.
Although many of these also apply to non-functional languages, they either originate in, are most easily implemented in, or are particularly critical in functional languages such as Lisp and ML.
Other optimizations.
"Please help separate and categorize these further and create detailed pages for them, especially the more complex ones, or link to one where one exists."
Interprocedural optimizations.
Interprocedural optimization works on the entire program, across procedure and file boundaries. It works tightly with intraprocedural counterparts, carried out with the cooperation of a local part and global part. Typical interprocedural optimizations are: procedure inlining, interprocedural dead code elimination, interprocedural constant propagation, and procedure reordering. As usual, the compiler needs to perform interprocedural analysis before its actual optimizations. Interprocedural analyses include alias analysis, array access analysis, and the construction of a call graph.
Interprocedural optimization is common in modern commercial compilers from SGI, Intel, Microsoft, and Sun Microsystems. For a long time the open source GCC was criticized for a lack of powerful interprocedural analysis and optimizations, though this is now improving. Another open source compiler with full analysis and optimization infrastructure is Open64.
Due to the extra time and space required by interprocedural analysis, most compilers do not perform it by default. Users must use compiler options explicitly to tell the compiler to enable interprocedural analysis and other expensive optimizations.
Problems with optimization.
Early in the history of compilers, compiler optimizations were not as good as hand-written ones. As compiler technologies have improved, good compilers can often generate better code than human programmers, and good post pass optimizers can improve highly hand-optimized code even further. For RISC CPU architectures, and even more so for VLIW hardware, compiler optimization is the key for obtaining efficient code, because RISC instruction sets are so compact that it is hard for a human to manually schedule or combine small instructions to get efficient results. Indeed, these architectures were designed to rely on compiler writers for adequate performance.
However, optimizing compilers are by no means perfect. There is no way that a compiler can guarantee that, for all program source code, the fastest (or smallest) possible equivalent compiled program is output; such a compiler is fundamentally impossible because it would solve the halting problem (assuming Turing completeness).
This may be proven by considering a call to a function, foo(). This function returns nothing and does not have side effects (no I/O, does not modify global variables and "live" data structures, etc.). The fastest possible equivalent program would be simply to eliminate the function call. However, if the function foo() in fact does "not" return, then the program with the call to foo() would be different from the program without the call; the optimizing compiler will then have to determine this by solving the halting problem.
Additionally, there are a number of other more practical issues with optimizing compiler technology:
Work to improve optimization technology continues. One approach is the use of so-called post-pass optimizers (some commercial versions of which date back to mainframe software of the late 1970s). These tools take the executable output by an "optimizing" compiler and optimize it even further. Post pass optimizers usually work on the assembly language or machine code level (contrast with compilers that optimize intermediate representations of programs). The performance of post pass compilers are limited by the fact that much of the information available in the original source code is not always available to them.
As processor performance continues to improve at a rapid pace, while memory bandwidth improves more slowly, optimizations that reduce memory bandwidth requirements (even at the cost of making the processor execute relatively more instructions) will become more useful. Examples of this, already mentioned above, include loop nest optimization and rematerialization.
History.
Early compilers of the 1960s were often primarily concerned with simply compiling code correctly or efficiently – compile times were a major concern. One of the earliest notable optimizing compilers was that for BLISS (1970), which was described in "The Design of an Optimizing Compiler" (1975). By the 1980s optimizing compilers were sufficiently effective that programming in assembly language declined, and by the late 1990s for even performance sensitive code, optimizing compilers exceeded the performance of human experts. This co-evolved with the development of RISC chips and advanced processor features such as instruction scheduling and speculative execution which were designed to be targeted by optimizing compilers, rather than by human-written assembly code.

</doc>
<doc id="40359" url="http://en.wikipedia.org/wiki?curid=40359" title="Due process">
Due process

Due process is the legal requirement that the state must respect all legal rights that are owed to a person. Due process balances the power of law of the land and protects the individual person from it. When a government harms a person without following the exact course of the law, this constitutes a due process violation, which offends the rule of law.
Due process has also been frequently interpreted as limiting laws and legal proceedings (see substantive due process), so that judges—instead of legislators—may define and guarantee fundamental fairness, justice, and liberty. This interpretation has proven controversial, and is analogous to the concepts of natural justice, and procedural justice used in various other jurisdictions. This interpretation of due process is sometimes expressed as a command that the government must not be unfair to the people or abuse them physically.
Due process is not used in contemporary English law, though two similar concepts are natural justice (which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions) and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American theory of due process, which, as explained below, presently contains many implied rights not found in the ancient or modern concepts of due process in England.
Due process developed from clause 39 of the Magna Carta in England. When English and American law gradually diverged, due process was not upheld in England, but did become incorporated in the Constitution of the United States.
By jurisdiction.
Magna Carta.
In clause 39 of Magna Carta, issued in 1215, John, King of England promised: "No free man shall be seized or imprisoned, or stripped of his rights or possessions, or outlawed or exiled, or deprived of his standing in any other way, nor will we proceed with force against him, or send others to do so, except by the lawful judgment of his equals or by the law of the land." Magna Carta itself immediately became part of the "law of the land", and Clause 61 of that charter authorized an elected body of twenty-five barons to determine by majority vote what redress the King must provide when the King offends "in any respect against any man." Thus, Magna Carta established the rule of law in England by not only requiring the monarchy to obey the law of the land, but also limiting how the monarchy could change the law of the land. However, in the thirteenth century these provisions may have been referring only to the rights of landowners, and not to ordinary peasantry or villagers.
Shorter versions of Magna Carta were subsequently issued by British monarchs, and Clause 39 of Magna Carta was renumbered "29." The phrase "due process of law" first appeared in a statutory rendition of Magna Carta in A.D. 1354 during the reign of Edward III of England, as follows: "No man of what state or condition he be, shall be put out of his lands or tenements nor taken, nor disinherited, nor put to death, without he be brought to answer by due process of law."
In 1608, the English jurist Edward Coke wrote a treatise in which he discussed the meaning of Magna Carta. Coke explained that no man shall be deprived but by "legem terrae", the law of the land, "that is, by the common law, statute law, or custom of England... (that is, to speak it once and for all) by the due course, and process of law.."
Both the clause in Magna Carta and the later statute of 1354 were again explained in 1704 (during the reign of Queen Anne) by the Queen's Bench, in the case of "Regina v. Paty". In that case, the House of Commons had deprived John Paty and certain other citizens of the right to vote in an election, and had committed them to Newgate Prison merely for the offense of pursuing a legal action in the courts. The Queen's Bench, in an opinion by Justice Powys, explained the meaning of "due process of law" as follows:
[I]t is objected, that by Mag. Chart. c. 29, no man ought to be taken or imprisoned, but by the law of the land. But to this I answer, that lex terrae is not confined to the common law, but takes in all the other laws, which are in force in this realm; as the civil and canon law... By the 28 Ed. 3, c. 3, there the words lex terrae, which are used in Mag. Char. are explained by the words, due process of law; and the meaning of the statute is, that all commitments must be by a legal authority; and the law of Parliament is as much a law as any, nay, if there be any superiority this is a superior law.
Chief Justice Holt dissented in this case, because he believed that the commitment had not in fact been by a legal authority. The House of Commons had purported to legislate unilaterally, without approval of the House of Lords, ostensibly to regulate the election of its members. Although the Queen's Bench held that the House of Commons had not infringed or overturned due process, John Paty was ultimately freed by Queen Anne when she prorogued Parliament.
English law and American law diverge.
Throughout centuries of British history, many laws and treatises asserted various requirements as being part of "due process" or included in the "law of the land". This view usually held in regards to what was required by existing law, rather than what was intrinsically required by due process itself. As the U.S. Supreme Court has explained, a due process requirement in Britain was not "essential to the idea of due process of law in the prosecution and punishment of crimes, but was only mentioned as an example and illustration of due process of law as it actually existed in cases in which it was customarily used."
Ultimately, the scattered references to "due process of law" in English law did not limit the power of the government; about this, American law professor John Orth wrote that "the great phrases failed to retain their vitality." Orth points out that this is generally attributed to the rise of the doctrine of parliamentary supremacy in the United Kingdom, which was accompanied by hostility towards judicial review as an undemocratic foreign invention.
Scholars have occasionally interpreted Lord Coke's ruling in "Dr. Bonham's Case" as implying the possibility of judicial review, but by the 1870s, Lord Campbell was dismissing judicial review as "a foolish doctrine alleged to have been laid down extra-judicially in Dr. Bonham's Case..., a conundrum [that] ought to have been laughed at." Lacking the power of judicial review, English courts possessed no means by which to declare government statutes or acts invalid as a violation of due process. As a consequence, English law and American law diverged, with American legislators possessing no means by which to declare judicial invalidation of statutes incorrect (with the sole exception of proposing a constitutional amendment, which is rarely successful). In 1977, an English political science professor explained the present situation in England for the benefit of American lawyers:
Two similar concepts in contemporary English law are natural justice (which generally applies only to decisions of administrative agencies and some types of private bodies like trade unions) and the British constitutional concept of the rule of law as articulated by A. V. Dicey and others. However, neither concept lines up perfectly with the American conception of due process, which presently contains many implied rights not found in the ancient or modern concepts of due process in England.
United States.
The Fifth and Fourteenth Amendments to the United States Constitution each contain a Due Process Clause. Due process deals with the administration of justice and thus the Due Process Clause acts as a safeguard from arbitrary denial of life, liberty, or property by the Government outside the sanction of law. The Supreme Court of the United States interprets the Clauses as providing four protections: procedural due process (in civil and criminal proceedings), substantive due process, a prohibition against vague laws, and as the vehicle for the incorporation of the Bill of Rights.
Others.
Various countries recognize some form of due process under customary international law. Although the specifics are often unclear, most nations agree that they should guarantee foreign visitors a basic minimum level of justice and fairness. Some nations have argued that they are bound to grant no more rights to aliens than they do to their own citizens—the doctrine of national treatment—which also means that both would be vulnerable to the same deprivations by the government. With the growth of international human rights law and the frequent use of treaties to govern treatment of foreign nationals abroad, the distinction in practice between these two perspectives may be disappearing.

</doc>
<doc id="40363" url="http://en.wikipedia.org/wiki?curid=40363" title="Dosimeter">
Dosimeter

A radiation dosimeter is a device that measures exposure to ionizing radiation. It has two main uses: for human radiation protection and for measurement of dose in both medical and industrial processes.
Personal dosimeters.
The personal ionising radiation dosimeter is of fundamental importance in the disciplines of radiation dosimetry and radiation health physics and is primarily used to estimate the radiation dose deposited in an individual wearing the device.
Ionising radiation damage to the human body is cumulative, and is related to the total dose received, for which the SI unit is the sievert. Workers exposed to radiation, such as radiographers, nuclear power plant workers, doctors using radiotherapy, those in laboratories using radionuclides, and HAZMAT teams are required to wear dosimeters so a record of occupational exposure can be made. Such devices are known as "legal dosimeters" if they have been approved for use in recording personnel dose for regulatory purposes.
Dosimeters can be worn to obtain a whole body dose and there are also specialist types that can be worn on the fingers or clipped to headgear, to measure the localised body irradiation for specific activities.
Types.
Common types of personal dosimeters for ionizing radiation include:
Electronic personal dosimeter (EPD).
The electronic personal dosimeter (EPD) is an electronic device that has a number of sophisticated functions, such as continual monitoring which allows alarm warnings at preset levels and live readout of dose accumulated. These are especially useful in high dose areas where residence time of the wearer is limited due to dose constraints. The dosimeter can be reset, usually after taking a reading for record purposes, and thereby re-used multiple times.
MOSFET Dosimeter
MOSFET dosimeters are now used as clinical dosimeters for radiotherapy radiation beams. The main advantages of MOSFET devices are:
1. The MOSFET dosimeter is direct reading with a very thin active area (less than 2 μm).
2. The physical size of the MOSFET when packaged is less than 4 mm.
3. The post radiation signal is permanently stored and is dose rate independent.
Gate oxide of MOSFET which is conventionally silicon dioxide is a active sensing material in MOSFET dosimeters. Radiation creates defects (acts like electron-hole pairs) in oxide, which in turn affects the threshold voltage of the MOSFET. This change in threshold voltage is proportional to radiation dose. Alternate high-k gate dielectrics like Hafnium dioxide and Aluminum oxides are also proposed as a radiation dosimeters. 
Film badge dosimeter.
Film badge dosimeters are for one-time use only. The level of radiation absorption is indicated by a change to the film emulsion, which is shown when the film is developed.
Quartz fiber dosimeter.
Quartz fiber dosimeters are charged to a high voltage. As the gas in the dosimeter chamber becomes ionized by radiation the charge leaks away, causing the fiber indicator to change against a graduated scale.
Thermoluminescent dosimeter (TLD).
A thermoluminescent dosimeter measures ionizing radiation exposure by measuring the intensity of visible light emitted from a crystal in the detector when heated. The intensity of light emitted is dependent upon the radiation exposure.
Both the quartz fiber and film badge types are being superseded by the TLD and the EPD.
Radiation protection dose quantities.
The dosimeter plays an important role within the international radiation protection system developed by the International Commission on Radiological Protection (ICRP) and the International Commission on Radiation Units and Measurements (ICRU). This is shown in the accompanying diagram.
Protection quantities.
The protection quantities are used as "limiting quantities" to specify exposure limits to ensure that the occurrence of stochastic health effects is kept below unacceptable levels and that tissue reactions are avoided. These quantities cannot be practically measured and are a calculated value of irradiation of organs of the human body, which is arrived at by using an anthropomorphic phantom. This is a 3D computational model of the human body which attempts to take into account a number of complex effects such as body self-shielding and internal scattering of radiation.
As protection quantities cannot practically be measured, operational quantities are used to relate them to practical radiation instrument and dosimeter responses.
Operational quantities.
Operational quantities are aimed at providing an estimate or upper limit for the value of the protection quantities related to an exposure. They are used in practical regulations or guidance. These relate real-life operational instrument measurements and responses to the calculated protection quantities.
Personal dose equivalent, Hp(d), is defined by the ICRP as the dose equivalent in soft tissue at an appropriate depth, d, below a specified point on the human body. The specified point is usually given by the position where the individual’s dosimeter is worn.
Instrument and dosimeter response.
This is an actual reading obtained from such as an ambient dose gamma monitor, or a personal dosimeter. The dosimeter is calibrated in a known radiation field to ensure display of accurate operational quantities and allow a relationship to known health effect. The personal dose equivalent is used to assess dose uptake, and allow regulatory limits to be met. It is the figure usually entered into the records of external dose for occupational radiation workers.
Dosimeter calibration.
The "slab" phantom is used to represent the human torso for calibration of whole body dosimeters. The IAEA states "The slab phantom is 300 mm × 300 mm × 150 mm depth to represent the human torso".
Process irradiation verification.
Manufacturing processes that treat products with ionizing radiation, such as food irradiation, use dosimeters to calibrate doses deposited in the matter being irradiated. These usually must have a greater dose range than personal dosimeters, and doses are normally measured in the unit of absorbed dose: the gray (Gy). The dosimeter is located on or adjacent to the items being irradiated during the process as a validation of dose levels received.

</doc>
<doc id="40364" url="http://en.wikipedia.org/wiki?curid=40364" title="Electrometer">
Electrometer

An electrometer is an electrical instrument for measuring electric charge or electrical potential difference. There are many different types, ranging from historical handmade mechanical instruments to high-precision electronic devices. Modern electrometers based on vacuum tube or solid-state technology can be used to make voltage and charge measurements with very low leakage currents, down to 1 femtoampere. A simpler but related instrument, the electroscope, works on similar principles but only indicates the relative magnitudes of voltages or charges.
Older electrometers.
Gold-leaf electrometer.
The gold-leaf electroscope was one of the first sensitive instruments used to indicate electric charge. It is still used for science demonstrations but has been superseded in most applications by electronic measuring instruments. The instrument consists of two thin leaves of gold foil suspended from an electrode. When the electrode is charged by induction or by contact, the leaves acquire similar electric charges and repel each other due to the Coulomb force. Their separation is a direct indication of the net charge stored on them. On the glass opposite the leaves, pieces of tin foil may be pasted, so that when the leaves diverge fully they may discharge into the ground. The leaves may be enclosed in a glass envelope to protect them from drafts, and the envelope may be evacuated to minimize charge leakage. A further cause of charge leakage is ionizing radiation, so to prevent this, the electrometer must be surrounded by lead shielding. This principle has been used to detect ionizing radiation, as seen in the quartz fibre electrometer and Kearny fallout meter.
This type of electroscope usually acts as an indicator and not a measuring device, although it can be calibrated. The Braun electroscope replaced the gold-leaf electroscope for more accurate measurements.
The instrument was developed in the 18th century by several researchers, among them Abraham Bennet and Alessandro Volta.
Early quadrant electrometer.
While the term "quadrant electrometer" eventually referred to Kelvin's version, this term was first used to describe a simpler device. It consists of an upright stem of wood, to which is affixed to a semicircle of ivory. From the center there hangs a light cork ball upon a pivot. When the instrument is placed upon a charged body, the stem participates and repels the cork ball. The amount of repulsion may be read off the graduated semicircle, though it is obvious that the measured angle is not in direct proportion to the charge.
Coulomb's electrometer.
This design uses torsion to give a measurement more sensitive than repulsion of gold leaves or cork-balls. It consists of a glass cylinder with a glass tube on top. In the axes of the tube is a glass thread, the lower end of this holds a bar of gum lac, with a gilt pith ball at each extremity. Through another aperture on the cylinder, another gum lac rod with gilt balls may be introduced. This is called the carrier rod.
If the lower ball of the carrier rod is charged when it is entered into the aperture, this will repel one of the movable balls inside. An index and scale (not pictured) is attached to the top of the twistable glass rod. The number of degrees twisted to bring the balls back together is in exact proportion of the amount of charge of the ball of the carrier rod.
Peltier electrometer.
Developed by Peltier, this uses a form of magnetic compass to measure deflection by balancing the electrostatic force with a magnetic needle.
Bohnenberger electrometer.
The Bohnenberger electrometer, developed by J.G.F. von Bohnenberger, consists of a single gold leaf suspended vertically between the anode and cathode of a dry pile. Any charge imparted to the gold leaf causes it to move toward one or the other pole; thus, the sign of the charge as well as its approximate magnitude may be gauged.
Attraction electrometer.
Also known as Attracted Disk Electrometers, attraction electrometers are sensitive balances measuring the attraction between charged disks. William Snow Harris is credited with the invention of this instrument, which was further improved by Lord Kelvin.
Kelvin's quadrant electrometer.
Developed by Lord Kelvin, this is the most sensitive and accurate of all the mechanical electrometers. The original design uses a light aluminum sector suspended inside a drum cut into four segments. The segments are insulated and connected diagonally in pairs. The charged aluminum sector is attracted to one pair of segments and repelled from the other. The deflection is observed by a beam of light reflected from a small mirror attached to the sector, just as in a galvanometer. The engraving on the right shows a slightly different form of this electrometer, using four flat plates rather than closed segments. The plates can be connected externally in the conventional diagonal way (as shown), or in a different order for specific applications.
A more sensitive form of quadrant electrometer was developed by Frederick Lindemann. It employs a metal-coated quartz fiber instead of an aluminum sector. The deflection is measured by observing the movement of the fiber under a microscope. Initially used for measuring star light, it was employed for the infrared detection of airplanes in the early stages of World War II.
Some mechanic electrometers were housed inside a cage often referred to as a “bird cage”. This is a form of Faraday Cage that protected the instrument from external electrostatic charges.
Modern electrometers.
A modern electrometer is a highly sensitive electronic voltmeter whose input impedance is so high that the current flowing into it can be considered, for most practical purposes, to be zero. The actual value of input resistance for modern electronic electrometers is around 1014Ω, compared to around 1010Ω for nanovoltmeters. Due to the extremely high input impedance, special design considerations must be applied to avoid leakage current such as driven shields and special insulation materials.
Among other applications, electrometers are used in nuclear physics experiments as they are able to measure the tiny charges left in matter by the passage of ionizing radiation. The most common use for modern electrometers is the measurement of radiation with ionization chambers, in instruments such as geiger counters.
Vibrating reed electrometers.
Vibrating reed electrometers use a variable capacitor formed between a moving electrode (in the form of a vibrating reed) and a fixed input electrode. As the distance between the two electrodes varies, the capacitance also varies and electric charge is forced in and out of the capacitor. The alternating current signal produced by the flow of this charge is amplified and used as an analogue for the DC voltage applied to the capacitor. The DC input resistance of the electrometer is determined solely by the leakage resistance of the capacitor, and is typically extremely high, (although its AC input impedance is lower).
For convenience of use, the vibrating reed assembly is often attached by a cable to the rest of the electrometer. This allows for a relatively small unit to be located near the charge to be measured while the much larger reed-driver and amplifier unit can be located wherever it is convenient for the operator.
Valve electrometers.
Valve electrometers use a specialized vacuum tube (thermionic valve) with a very high gain (transconductance) and input resistance. The input current is allowed to flow into the high impedance grid, and the voltage so generated is vastly amplified in the anode (plate) circuit. Valves designed for electrometer use have leakage currents as low as a few femtoamperes (10−15 amperes). Such valves must be handled with gloved hands as the salts left on the glass envelope can provide leakage paths for these tiny currents.
Solid-state electrometers.
The most modern electrometers consist of a solid state amplifier using one or more field-effect transistors, connections for external measurement devices, and usually a display and/or data-logging connections. The amplifier amplifies small currents so that they are more easily measured. The external connections are usually of a co-axial or tri-axial design, and allow attachment of diodes or ionization chambers for ionising radiation measurement. The display or data-logging connections allow the user to see the data or record it for later analysis. Electrometers designed for use with ionization chambers may include a high-voltage power supply, which is used to power the ionization chamber.
Solid-state electrometers are often multipurpose devices that can measure voltage, charge, resistance and current. They measure voltage by means of "voltage balancing", in which the input voltage is compared with an internal reference voltage source using an electronic circuit with a very high input impedance (of the order of 1014 ohms). A similar circuit modified to act as a current-to-voltage converter enables the instrument to measure currents as small as a few femtoamperes. Combined with an internal voltage source, the current measuring mode can be adapted to measure very high resistances, of the order of 1017 ohms. Finally, by calculation from the known capacitance of the electrometer's input terminal, the instrument can measure very small electric charges, down to a small fraction of a picocoulomb. 

</doc>
<doc id="40365" url="http://en.wikipedia.org/wiki?curid=40365" title="Galvanometer">
Galvanometer

A galvanometer is a type of sensitive ammeter: an instrument for detecting electric current. It is an analog electromechanical actuator that produces a rotary deflection of some type of pointer in response to electric current through its coil in a magnetic field.
Galvanometers were the first instruments used to detect and measure electric currents. Sensitive galvanometers were used to detect signals from long submarine cables, and to discover the electrical activity of the heart and brain. Some galvanometers use a solid pointer on a scale to show measurements; other very sensitive types use a miniature mirror and a beam of light to provide mechanical amplification of low-level signals. Initially a laboratory instrument relying on the Earth's own magnetic field to provide restoring force for the pointer, galvanometers were developed into compact, rugged, sensitive portable instruments essential to the development of electrotechnology. A type of galvanometer that records measurements permanently is the chart recorder. The term has expanded to include use of the same mechanism in recording, positioning, and servomechanism equipment.
History.
The deflection of a magnetic compass needle by current in a wire was first described by Hans Oersted in 1820. The phenomenon was studied both for its own sake and as a means of measuring electrical current. The earliest galvanometer was reported by Johann Schweigger at the University of Halle on 16 September 1820. André-Marie Ampère also contributed to its development. Early designs increased the effect of the magnetic field generated by the current by using multiple turns of wire. The instruments were at first called "multipliers" due to this common design feature. The term "galvanometer," in common use by 1836, was derived from the surname of Italian electricity researcher Luigi Galvani, who in 1791 discovered that electric current would make a dead frog's leg jerk.
Originally, the instruments relied on the Earth's magnetic field to provide the restoring force for the compass needle. These were called "tangent" galvanometers and had to be oriented before use. Later instruments of the "astatic" type used opposing magnets to become independent of the Earth's field and would operate in any orientation. The most sensitive form, the Thomson or mirror galvanometer, was improved by William Thomson (Lord Kelvin) from the early design invented in 1826 by Johann Christian Poggendorff. Thomson's design, which he patented in 1858, was able to detect very rapid current changes. Instead of a compass needle, it used small magnets attached to a lightweight mirror, suspended by a thread. The deflection of a light beam greatly magnified the deflection induced by small currents. Alternatively, the deflection of the suspended magnets could be observed directly through a microscope.
The ability to measure quantitatively voltage and current allowed Georg Ohm to formulate Ohm's Law, which states the voltage across a conductor is directly proportional to the current through it.
The early moving-magnet form of galvanometer had the disadvantage that it was affected by any magnets or iron masses near it, and its deflection was not linearly proportional to the current. In 1882 Jacques-Arsène d'Arsonval and Marcel Deprez developed a form with a stationary permanent magnet and a moving coil of wire, suspended by fine wires which provided both an electrical connection to the coil and the restoring torque to return to the zero position. An iron tube between the magnet's pole pieces defined a circular gap through which the coil rotated. This gap produced a consistent, radial magnetic field across the coil, giving a linear response throughout the instrument's range. A mirror attached to the coil deflected a beam of light to indicate the coil position. The concentrated magnetic field and delicate suspension made these instruments sensitive; d'Arsonval's initial instrument could detect ten microamperes.
Edward Weston extensively improved the design. He replaced the fine wire suspension with a pivot, and provided restoring torque and electrical connections through spiral springs rather like those of a wristwatch balance wheel hairspring. He developed a method of stabilizing the magnetic field of the permanent magnet, so the instrument would have consistent accuracy over time. He replaced the light beam and mirror with a knife-edge pointer that could be read directly. A mirror under the pointer, in the same plane as the scale, eliminated parallax observation error. To maintain the field strength, Weston's design used a very narrow circumferential slot through which the coil moved, with a minimal air-gap. This improved linearity of pointer deflection with respect to coil current. Finally, the coil was wound on a light-weight form made of conductive metal, which acted as a damper. By 1888, Edward Weston had patented and brought out a commercial form of this instrument, which became a standard electrical equipment component. It was known as a "portable" instrument because it was affected very little by mounting position or by transporting it from place to place. This design is almost universally used in moving-coil meters today.
Operation.
The most familiar use is as an analog measuring instrument, often called an ammeter. It is used to measure the direct current (flow of electric charge) through an electric circuit. The D'Arsonval/Weston form used today is constructed with a small pivoting coil of wire in the field of a permanent magnet. The coil is attached to a thin pointer that traverses a calibrated scale. A tiny torsion spring pulls the coil and pointer to the zero position.
When a direct current (DC) flows through the coil, the coil generates a magnetic field. This field acts against the permanent magnet. The coil twists, pushing against the spring, and moves the pointer. The hand points at a scale indicating the electric current. Careful design of the pole pieces ensures that the magnetic field is uniform, so that the angular deflection of the pointer is proportional to the current. A useful meter generally contains provision for damping the mechanical resonance of the moving coil and pointer, so that the pointer settles quickly to its position without oscillation.
The basic sensitivity of a meter might be, for instance, 100 microamperes full scale (with a voltage drop of, say, 50 millivolts at full current). Such meters are often calibrated to read some other quantity that can be converted to a current of that magnitude. The use of current dividers, often called shunts, allows a meter to be calibrated to measure larger currents. A meter can be calibrated as a DC voltmeter if the resistance of the coil is known by calculating the voltage required to generate a full scale current. A meter can be configured to read other voltages by putting it in a voltage divider circuit. This is generally done by placing a resistor in series with the meter coil. A meter can be used to read resistance by placing it in series with a known voltage (a battery) and an adjustable resistor. In a preparatory step, the circuit is completed and the resistor adjusted to produce full scale deflection. When an unknown resistor is placed in series in the circuit the current will be less than full scale and an appropriately calibrated scale can display the value of the previously unknown resistor.
Because the pointer of the meter is usually a small distance above the scale of the meter, parallax error can occur when the operator attempts to read the scale line that "lines up" with the pointer. To counter this, some meters include a mirror along the markings of the principal scale. The accuracy of the reading from a mirrored scale is improved by positioning one's head while reading the scale so that the pointer and the reflection of the pointer are aligned; at this point, the operator's eye must be directly above the pointer and any parallax error has been minimized.
Types.
Today the main type of galvanometer mechanism still used is the moving coil D'Arsonval/Weston mechanism, which is used in traditional analog meters.
Tangent galvanometer.
A tangent galvanometer is an early measuring instrument used for the measurement of electric current. It works by using a compass needle to compare a magnetic field generated by the unknown current to the magnetic field of the Earth. It gets its name from its operating principle, the tangent law of magnetism, which states that the tangent of the angle a compass needle makes is proportional to the ratio of the strengths of the two perpendicular magnetic fields. It was first described by Claude Pouillet in 1837.
A tangent galvanometer consists of a coil of insulated copper wire wound on a circular non-magnetic frame. The frame is mounted vertically on a horizontal base provided with levelling screws. The coil can be rotated on a vertical axis passing through its centre. A compass box is mounted horizontally at the centre of a circular scale. It consists of a tiny, powerful magnetic needle pivoted at the centre of the coil. The magnetic needle is free to rotate in the horizontal plane. The circular scale is divided into four quadrants. Each quadrant is graduated from 0° to 90°. A long thin aluminium pointer is attached to the needle at its centre and at right angle to it. To avoid errors due to parallax, a plane mirror is mounted below the compass needle.
In operation, the instrument is first rotated until the magnetic field of the Earth, indicated by the compass needle, is parallel with the plane of the coil. Then the unknown current is applied to the coil. This creates a second magnetic field on the axis of the coil, perpendicular to the Earth's magnetic field. The compass needle responds to the vector sum of the two fields, and deflects to an angle equal to the tangent of the ratio of the two fields. From the angle read from the compass's scale, the current could be found from a table. The current supply wires have to be wound in a small helix, like a pig's tail, otherwise the field due to the wire will affect the compass needle and an incorrect reading will be obtained.
Theory.
The galvanometer is oriented so that the plane of the coil is vertical and aligned along parallel to the horizontal component "BH" of the Earth's magnetic field (i.e. parallel to the local "magnetic meridian"). When an electrical current flows through the galvanometer coil, a second magnetic field "B" is created. At the center of the coil, where the compass needle is located, the coil's field is perpendicular to the plane of the coil. The magnitude of the coil's field is:
where "I" is the current in amperes, "n" is the number of turns of the coil and "r" is the radius of the coil. These two perpendicular magnetic fields add vectorially, and the compass needle points along the direction of their resultant "BH+B". The current in the coil causes the compass needle to rotate by an angle "θ":
From tangent law, , i.e.
or
or , where "K" is called the Reduction Factor of the tangent galvanometer.
One problem with the tangent galvanometer is that its resolution degrades at both high currents and low currents. The maximum resolution is obtained when the value of "θ" is 45°. When the value of "θ" is close to 0° or 90°, a large percentage change in the current will only move the needle a few degrees.
Geomagnetic field measurement.
A tangent galvanometer can also be used to measure the magnitude of the horizontal component of the geomagnetic field. When used in this way, a low-voltage power source, such as a battery, is connected in series with a rheostat, the galvanometer, and an ammeter. The galvanometer is first aligned so that the coil is parallel to the geomagnetic field, whose direction is indicated by the compass when there is no current through the coils. The battery is then connected and the rheostat is adjusted until the compass needle deflects 45 degrees from the geomagnetic field, indicating that the magnitude of the magnetic field at the center of the coil is the same as that of the horizontal component of the geomagnetic field. This field strength can be calculated from the current as measured by the ammeter, the number of turns of the coil, and the radius of the coils.
Astatic galvanometer.
The "astatic galvanometer" was developed by Leopoldo Nobili in 1825.
Unlike a compass-needle galvanometer, the astatic galvanometer has two magnetic needles parallel to each other, but with the magnetic poles reversed. The needle assembly is suspended by a silk thread, and has no net magnetic dipole moment. It is not affected by the earth's magnetic field. The lower needle is inside the current sensing coils and is deflected by the magnetic field created by the passing current.
Mirror galvanometer.
Extremely sensitive measuring equipment once used mirror galvanometers that substituted a mirror for the pointer. A beam of light reflected from the mirror acted as a long, massless pointer. Such instruments were used as receivers for early trans-Atlantic telegraph systems, for instance. The moving beam of light could also be used to make a record on a moving photographic film, producing a graph of current versus time, in a device called an oscillograph. The string galvanometer was a type of mirror galvanometer so sensitive that it was used to make the first electrocardiogram of the electrical activity of the human heart.
Ballistic galvanometer.
A ballistic galvanometer is an instrument with a high moment of inertia, arranged so that its deflection is proportional to the total charge sent through the meter's coil.
Uses.
Past uses.
A major early use for galvanometers was for finding faults in telecommunications cables. They were superseded in this application late in the 20th century by time-domain reflectometers.
Probably the largest use of galvanometers was the D'Arsonval/Weston type movement used in analog meters in electronic equipment. Since the 1980s, galvanometer-type analog meter movements have been displaced by analog to digital converters (ADCs) for some uses. A digital panel meter (DPM) contains an analog to digital converter and numeric display. The advantages of a digital instrument are higher precision and accuracy, but factors such as power consumption or cost may still favor application of analog meter movements.
Galvanometer mechanisms were also used to position the "pens" in analog strip chart recorders such as used in electrocardiographs, electroencephalographs and polygraphs. Strip chart recorders with galvanometer driven pens may have a full scale frequency response of 100 Hz and several centimeters deflection. The writing mechanism may be a heated tip on the needle writing on heat-sensitive paper, or a hollow ink-fed pen. In some types the pen is continuously pressed against the paper, so the galvanometer must be strong enough to move the pen against the friction of the paper. In other types, such as the Rustrak recorders, the needle is only intermittently pressed against the writing medium; at that moment, an impression is made and then the pressure is removed, allowing the needle to move to a new position and the cycle repeats. In this case, the galvanometer need not be especially strong.
Galvanometer mechanisms were also used in exposure mechanisms in film cameras.
Modern uses.
Most modern uses for the galvanometer mechanism are in positioning and control systems. Galvanometer mechanisms are divided into moving magnet and moving coil galvanometers; in addition, they are divided into "closed-loop" and "open-loop" - or "resonant" - types. 
"Mirror" galvanometer systems are used as beam positioning or beam steering elements in laser scanning systems. For example, 
for material processing with high-power lasers, mirror galvanometer are typically high power galvanometer mechanisms used with closed loop servo control systems. The newest galvanometers designed for beam steering applications can have frequency responses over 10 kHz with appropriate servo technology. Closed-loop mirror galvanometers are also used in stereolithography, in laser sintering, in laser engraving, in laser beam welding, in laser TV, in laser displays, and in imaging applications such as Optical Coherence Tomography (OCT) retinal scanning. Almost all of these galvanometers are of the moving magnet type.
Open loop, or resonant mirror galvanometers, are mainly used in laser-based barcode scanners, in some printing machines, in some imaging applications, in military applications, and in space systems. Their non-lubricated bearings are especially of interest in applications that require a high vacuum.
A galvanometer mechanism is used for the "head positioning" servos in hard disk drives and CD and DVD players. These are all of the moving coil type, in order to keep mass, and thus access times, as low as possible.

</doc>
<doc id="40366" url="http://en.wikipedia.org/wiki?curid=40366" title="ADC">
ADC

ADC may refer to:
In aviation:
In biology and medicine:
In entertainment and the arts:
In government and politics:
In technology:
In other fields:

</doc>
<doc id="40367" url="http://en.wikipedia.org/wiki?curid=40367" title="Analog-to-digital converter">
Analog-to-digital converter

An analog-to-digital converter (ADC, A/D, or A to D) is a device that converts a continuous physical quantity (usually voltage) to a digital number that represents the quantity's amplitude.
The conversion involves quantization of the input, so it necessarily introduces a small amount of error. Instead of doing a single conversion, an ADC often performs the conversions ("samples" the input) periodically. The result is a sequence of digital values that have been converted from a continuous-time and continuous-amplitude analog signal to a discrete-time and discrete-amplitude digital signal.
An ADC is defined by its bandwidth (the range of frequencies it can measure) and its signal to noise ratio (how accurately it can measure a signal relative to the noise it introduces). The actual bandwidth of an ADC is characterized primarily by its sampling rate, and to a lesser extent by how it handles errors such as aliasing. The dynamic range of an ADC is influenced by many factors, including the resolution (the number of output levels it can quantize a signal to), linearity and accuracy (how well the quantization levels match the true analog signal) and jitter (small timing errors that introduce additional noise). The dynamic range of an ADC is often summarized in terms of its effective number of bits (ENOB), the number of bits of each measure it returns that are on average not noise. An ideal ADC has an ENOB equal to its resolution. ADCs are chosen to match the bandwidth and required signal to noise ratio of the signal to be quantized. If an ADC operates at a sampling rate greater than twice the bandwidth of the signal, then perfect reconstruction is possible given an ideal ADC and neglecting quantization error. The presence of quantization error limits the dynamic range of even an ideal ADC, however, if the dynamic range of the ADC exceeds that of the input signal, its effects may be neglected resulting in an essentially perfect digital representation of the input signal.
An ADC may also provide an isolated measurement such as an electronic device that converts an input analog voltage or current to a digital number proportional to the magnitude of the voltage or current. However, some non-electronic or only partially electronic devices, such as rotary encoders, can also be considered ADCs. The digital output may use different coding schemes. Typically the digital output will be a two's complement binary number that is proportional to the input, but there are other possibilities. An encoder, for example, might output a Gray code.
The inverse operation is performed by a digital-to-analog converter (DAC).
Concepts.
Resolution.
The resolution of the converter indicates the number of discrete values it can produce over the range of analog values. The resolution determines the magnitude of the quantization error and therefore determines the maximum possible average signal to noise ratio for an ideal ADC without the use of oversampling. The values are usually stored electronically in binary form, so the resolution is usually expressed in bits. In consequence, the number of discrete values available, or "levels", is assumed to be a power of two. For example, an ADC with a resolution of 8 bits can encode an analog input to one in 256 different levels, since 28 = 256. The values can represent the ranges from 0 to 255 (i.e. unsigned integer) or from −128 to 127 (i.e. signed integer), depending on the application.
Resolution can also be defined electrically, and expressed in volts. The minimum change in voltage required to guarantee a change in the output code level is called the least significant bit (LSB) voltage. The resolution "Q" of the ADC is equal to the LSB voltage. The voltage resolution of an ADC is equal to its overall voltage measurement range divided by the number of discrete values:
where "M" is the ADC's resolution in bits and "E"FSR is the full scale voltage range (also called 'span'). "E"FSR is given by
where "V"RefHi and "V"RefLow are the upper and lower extremes, respectively, of the voltages that can be coded.
Normally, the number of voltage intervals is given by
where "M" is the ADC's resolution in bits.
That is, one voltage interval is assigned in between two consecutive code levels.
Example:
In practice, the useful resolution of a converter is limited by the best signal-to-noise ratio (SNR) that can be achieved for a digitized signal. An ADC can resolve a signal to only a certain number of bits of resolution, called the effective number of bits (ENOB). One effective bit of resolution changes the signal-to-noise ratio of the digitized signal by 6 dB, if the resolution is limited by the ADC. If a preamplifier has been used prior to A/D conversion, the noise introduced by the amplifier can be an important contributing factor towards the overall SNR.
Quantization error.
Quantization error is the noise introduced by quantization in an ideal ADC. It is a rounding error between the analog input voltage to the ADC and the output digitized value. The noise is non-linear and signal-dependent.
In an ideal analog-to-digital converter, where the quantization error is uniformly distributed between −1/2 LSB and +1/2 LSB, and the signal has a uniform distribution covering all quantization levels, the Signal-to-quantization-noise ratio (SQNR) can be calculated from
Where Q is the number of quantization bits. For example, a 16-bit ADC has a maximum signal-to-noise ratio of 6.02 × 16 = 96.3 dB, and therefore the quantization error is 96.3 dB below the maximum level. Quantization error is distributed from DC to the Nyquist frequency, consequently if part of the ADC's bandwidth is not used (as in oversampling), some of the quantization error will fall out of band, effectively improving the SQNR. In an oversampled system, noise shaping can be used to further increase SQNR by forcing more quantization error out of the band.
Dither.
In ADCs, performance can usually be improved using dither. This is a very small amount of random noise (white noise), which is added to the input before conversion.
Its effect is to cause the state of the LSB to randomly oscillate between 0 and 1 in the presence of very low levels of input, rather than sticking at a fixed value. Rather than the signal simply getting cut off altogether at this low level (which is only being quantized to a resolution of 1 bit), it extends the effective range of signals that the ADC can convert, at the expense of a slight increase in noise – effectively the quantization error is diffused across a series of noise values which is far less objectionable than a hard cutoff. The result is an accurate representation of the signal over time. A suitable filter at the output of the system can thus recover this small signal variation.
An audio signal of very low level (with respect to the bit depth of the ADC) sampled without dither sounds extremely distorted and unpleasant. Without dither the low level may cause the least significant bit to "stick" at 0 or 1. With dithering, the true level of the audio may be calculated by averaging the actual quantized sample with a series of other samples [the dither] that are recorded over time.
A virtually identical process, also called dither or dithering, is often used when quantizing photographic images to a fewer number of bits per pixel—the image becomes noisier but to the eye looks far more realistic than the quantized image, which otherwise becomes banded. This analogous process may help to visualize the effect of dither on an analogue audio signal that is converted to digital.
Dithering is also used in integrating systems such as electricity meters. Since the values are added together, the dithering produces results that are more exact than the LSB of the analog-to-digital converter.
Note that dither can only increase the resolution of a sampler, it cannot improve the linearity, and thus accuracy does not necessarily improve.
Accuracy.
An ADC has several sources of errors. Quantization error and (assuming the ADC is intended to be linear) non-linearity are intrinsic to any analog-to-digital conversion.
These errors are measured in a unit called the least significant bit (LSB). In the above example of an eight-bit ADC, an error of one LSB is 1/256 of the full signal range, or about 0.4%.
Non-linearity.
All ADCs suffer from non-linearity errors caused by their physical imperfections, causing their output to deviate from a linear function (or some other function, in the case of a deliberately non-linear ADC) of their input. These errors can sometimes be mitigated by calibration, or prevented by testing.
Important parameters for linearity are integral non-linearity (INL) and differential non-linearity (DNL). These non-linearities reduce the dynamic range of the signals that can be digitized by the ADC, also reducing the effective resolution of the ADC.
Jitter.
When digitizing a sine wave formula_5, the use of a non-ideal sampling clock will result in some uncertainty in when samples are recorded. Provided that the actual sampling time "uncertainty" due to the "clock jitter" is formula_6, the error caused by this phenomenon can be estimated as formula_7. This will result in additional recorded noise that will reduce the effective number of bits (ENOB) below that predicted by quantization error alone.
The error is zero for DC, small at low frequencies, but significant when high frequencies have high amplitudes. This effect can be ignored if it is drowned out by the "quantizing error". Jitter requirements can be calculated using the following formula: formula_8, where q is the number of ADC bits.
Clock jitter is caused by phase noise.
The resolution of ADCs with a digitization bandwidth between 1 MHz and 1 GHz is limited by jitter.
When sampling audio signals at 44.1 kHz, the anti-aliasing filter should have eliminated all frequencies above 22 kHz.
The input frequency (in this case, < 22 kHz), not the ADC clock frequency, is the determining factor with respect to jitter performance.
Sampling rate.
The analog signal is continuous in time and it is necessary to convert this to a flow of digital values. It is therefore required to define the rate at which new digital values are sampled from the analog signal. The rate of new values is called the "sampling rate" or "sampling frequency" of the converter.
A continuously varying bandlimited signal can be sampled (that is, the signal values at intervals of time T, the sampling time, are measured and stored) and then the original signal can be "exactly" reproduced from the discrete-time values by an interpolation formula. The accuracy is limited by quantization error. However, this faithful reproduction is only possible if the sampling rate is higher than twice the highest frequency of the signal. This is essentially what is embodied in the Shannon-Nyquist sampling theorem.
Since a practical ADC cannot make an instantaneous conversion, the input value must necessarily be held constant during the time that the converter performs a conversion (called the "conversion time"). An input circuit called a sample and hold performs this task—in most cases by using a capacitor to store the analog voltage at the input, and using an electronic switch or gate to disconnect the capacitor from the input. Many ADC integrated circuits include the sample and hold subsystem internally.
Aliasing.
An ADC works by sampling the value of the input at discrete intervals in time. Provided that the input is sampled above the Nyquist rate, defined as twice the highest frequency of interest, then all frequencies in the signal can be reconstructed. If frequencies above half the Nyquist rate are sampled, they are incorrectly detected as lower frequencies, a process referred to as aliasing. Aliasing occurs because instantaneously sampling a function at two or fewer times per cycle results in missed cycles, and therefore the appearance of an incorrectly lower frequency. For example, a 2 kHz sine wave being sampled at 1.5 kHz would be reconstructed as a 500 Hz sine wave.
To avoid aliasing, the input to an ADC must be low-pass filtered to remove frequencies above half the sampling rate. This filter is called an "anti-aliasing filter", and is essential for a practical ADC system that is applied to analog signals with higher frequency content. In applications where protection against aliasing is essential, oversampling may be used to greatly reduce or even eliminate it.
Although aliasing in most systems is unwanted, it should also be noted that it can be exploited to provide simultaneous down-mixing of a band-limited high frequency signal (see undersampling and frequency mixer). The alias is effectively the lower heterodyne of the signal frequency and sampling frequency.
Oversampling.
Signals are often sampled at the minimum rate required, for economy, with the result that the quantization noise introduced is white noise spread over the whole pass band of the converter. If a signal is sampled at a rate much higher than the Nyquist rate and then digitally filtered to limit it to the signal bandwidth there are the following advantages:
Oversampling is typically used in audio frequency ADCs where the required sampling rate (typically 44.1 or 48 kHz) is very low compared to the clock speed of typical transistor circuits (>1 MHz). In this case, by using the extra bandwidth to distribute quantization error onto out of band frequencies, the accuracy of the ADC can be greatly increased at no cost. Furthermore, as any aliased signals are also typically out of band, aliasing can often be completely eliminated using very low cost filters.
Relative speed and precision.
The speed of an ADC varies by type. The Wilkinson ADC is limited by the clock rate which is processable by current digital circuits. Currently, frequencies up to 300 MHz are possible. For a successive-approximation ADC, the conversion time scales with the logarithm of the resolution, e.g. the number of bits. Thus for high resolution, it is possible that the successive-approximation ADC is faster than the Wilkinson. However, the time consuming steps in the Wilkinson are digital, while those in the successive-approximation are analog. Since analog is inherently slower than digital, as the resolution increases, the time required also increases. Thus there are competing processes at work. Flash ADCs are certainly the fastest type of the three. The conversion is basically performed in a single parallel step. For an 8-bit unit, conversion takes place in a few tens of nanoseconds.
There is, as expected, somewhat of a tradeoff between speed and precision. Flash ADCs have drifts and uncertainties associated with the comparator levels. This results in poor linearity. For successive-approximation ADCs, poor linearity is also present, but less so than for flash ADCs. Here, non-linearity arises from accumulating errors from the subtraction processes. Wilkinson ADCs have the highest linearity of the three. These have the best differential non-linearity. The other types require channel smoothing to achieve the level of the Wilkinson.
The sliding scale principle.
The sliding scale or randomizing method can be employed to greatly improve the linearity of any type of ADC, but especially flash and successive approximation types. For any ADC the mapping from input voltage to digital output value is not exactly a floor or ceiling function as it should be. Under normal conditions, a pulse of a particular amplitude is always converted to a digital value. The problem lies in that the ranges of analog values for the digitized values are not all of the same width, and the differential linearity decreases proportionally with the divergence from the average width. The sliding scale principle uses an averaging effect to overcome this phenomenon. A random, but known analog voltage is added to the sampled input voltage. It is then converted to digital form, and the equivalent digital amount is subtracted, thus restoring it to its original value. The advantage is that the conversion has taken place at a random point. The statistical distribution of the final levels is decided by a weighted average over a region of the range of the ADC. This in turn desensitizes it to the width of any specific level.
ADC types.
These are the most common ways of implementing an electronic ADC:
There can be other ADCs that use a combination of electronics and other technologies:
Commercial analog-to-digital converters.
Commercial ADCs are usually implemented as integrated circuits.
Most converters sample with 6 to 24 bits of resolution, and produce fewer than 1 megasample per second. Thermal noise generated by passive components such as resistors masks the measurement when higher resolution is desired. For audio applications and in room temperatures, such noise is usually a little less than 1 μV (microvolt) of white noise. If the MSB corresponds to a standard 2 V of output signal, this translates to a noise-limited performance that is less than 20~21 bits, and obviates the need for any dithering. As of February 2002, Mega- and giga-sample per second converters are available. Mega-sample converters are required in digital video cameras, video capture cards, and TV tuner cards to convert full-speed analog video to digital video files.
Commercial converters usually have ±0.5 to ±1.5 LSB error in their output.
In many cases, the most expensive part of an integrated circuit is the pins, because they make the package larger, and each pin has to be connected to the integrated circuit's silicon. To save pins, it is common for slow ADCs to send their data one bit at a time over a serial interface to the computer, with the next bit coming out when a clock signal changes state, say from 0 to 5 V. This saves quite a few pins on the ADC package, and in many cases, does not make the overall design any more complex (even microprocessors which use memory-mapped I/O only need a few bits of a port to implement a serial bus to an ADC).
Commercial ADCs often have several inputs that feed the same converter, usually through an analog multiplexer. Different models of ADC may include sample and hold circuits, instrumentation amplifiers or differential inputs, where the quantity measured is the difference between two voltages.
Applications.
Music recording.
Analog-to-digital converters are integral to current music reproduction technology. People produce much music on computers using an analog recording and therefore need analog-to-digital converters to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files.
The current crop of analog-to-digital converters utilized in music can sample at rates up to 192 kilohertz. Considerable literature exists on these matters, but commercial considerations often play a significant role. Most high-profile recording studios record in 24-bit/192-176.4 kHz pulse-code modulation (PCM) or in Direct Stream Digital (DSD) formats, and then downsample or decimate the signal for Red-Book CD production (44.1 kHz) or to 48 kHz for commonly used radio and television broadcast applications.
Digital signal processing.
People must use ADCs to process, store, or transport virtually any analog signal in digital form. TV tuner cards, for example, use fast video analog-to-digital converters. Slow on-chip 8, 10, 12, or 16 bit analog-to-digital converters are common in microcontrollers. Digital storage oscilloscopes need very fast analog-to-digital converters, also crucial for software defined radio and their new applications.
Scientific instruments.
Digital imaging systems commonly use analog-to-digital converters in digitizing pixels.
Some radar systems commonly use analog-to-digital converters to convert signal strength to digital values for subsequent signal processing. Many other in situ and remote sensing systems commonly use analogous technology.
The number of binary bits in the resulting digitized numeric values reflects the resolution, the number of unique discrete levels of quantization (signal processing). The correspondence between the analog signal and the digital signal depends on the quantization error. The quantization process must occur at an adequate speed, a constraint that may limit the resolution of the digital signal.
Many sensors produce an analog signal; temperature, pressure, pH, light intensity etc. All these signals can be amplified and fed to an ADC to produce a digital number proportional to the input signal.
Testing.
Testing an Analog to Digital Converter requires an analog input source, hardware to send control signals and capture digital data output. Some ADCs also require an accurate source of reference signal.
The key parameters to test a SAR ADC are the following:

</doc>
<doc id="40372" url="http://en.wikipedia.org/wiki?curid=40372" title="John Thaw">
John Thaw

John Edward Thaw, CBE (3 January 1942 – 21 February 2002) was an English actor. He appeared in a range of television, stage, and cinema roles, his most popular being television series such as "Inspector Morse", "Redcap", "The Sweeney", "Home to Roost", and "Kavanagh QC".
Early life.
Thaw was born in Longsight, Manchester, to working class parents Dorothy (née Ablott) and John, a long-distance lorry driver. Thaw had a difficult childhood as his mother left when he was seven years old and he did not see her again for 12 years. His younger brother, Raymond Stuart "Ray" emigrated to Australia in the mid-1960s. Thaw grew up in Gorton and Burnage, attending the Ducie Technical High School for Boys. He entered the Royal Academy of Dramatic Art (RADA) at the age of 16, where he was a contemporary of Tom Courtenay.
Career.
Soon after leaving RADA Thaw made his formal stage début in "A Shred of Evidence" at the Liverpool Playhouse and was awarded a contract with the theatre. His first film role was a bit part in the 1962 adaptation of "The Loneliness of the Long Distance Runner" starring Tom Courtenay and he also acted on-stage opposite Sir Laurence Olivier in "Semi-Detached" (1962) by David Turner. He appeared in several episodes of the BBC police series "Z-Cars" in 1963–64 as a detective constable who left the force because of an unusual drink problem: he could not take the alcohol so often part of the policeman's work. Between 1964 and 1966, he starred in two series of the ABC Weekend Television/ITV production "Redcap", playing the hard-nosed military policeman Sergeant John Mann. He was also a guest star in an early episode of "The Avengers". In 1967 he appeared in the Granada TV/ITV series, "Inheritance", alongside James Bolam and Michael Goodliffe, as well as appearing in TV plays such as "The Talking Head" and episodes of series such as "Budgie", where he played against type (opposite Adam Faith) as an effeminate failed playwright with a full beard and a Welsh accent.
Thaw will perhaps be best remembered for two roles: the hard-bitten, tough talking Flying Squad detective Jack Regan in the Thames Television/ITV series (and two films) "The Sweeney" (1975–1978), which established him as a major star in the United Kingdom, and as the quietly spoken, introspective, well-educated and bitter detective "Inspector Morse" (1987–93, with specials from 1995–98 and 2000). 
Thaw was only 32 when he was cast in The Sweeney, although many viewers thought he was older. 
Thaw was the subject of "This Is Your Life" in 1981 when he was surprised by Eamonn Andrews in the foyer of the National Theatre, London.
Alongside his put-upon Detective Sergeant Lewis (Kevin Whately), Morse became a cult character—"a cognitive curmudgeon with his love of classical music, his classic Jaguar and spates of melancholy". Thaw was the definitive Morse, grumpy, crossword-fixated, drunk, slightly misogynistic, and pedantic about grammar. Inspector Morse became one of the UK's most loved TV series; the final three episodes, shown in 2000, were seen by 18 million people, about one third of the British population. He won "Most Popular Actor" at the 1999 National Television Awards and won two BAFTA awards for his role as Morse.
He subsequently played liberal working-class Lancastrian barrister James Kavanagh in "Kavanagh QC" (1995–99, and a special in 2001). Thaw also tried his hand at comedy with two sitcoms—"Thick as Thieves" (London Weekend/ITV, 1974) with Bob Hoskins and "Home to Roost" (Yorkshire/ITV, 1985–90). Thaw is best known in America for the Morse series, as well as the BBC series "A Year in Provence" (1993) with Lindsay Duncan.
During the 1970s and '80s, Thaw frequently appeared in productions with the Royal Shakespeare Company and Royal National Theatre. He appeared in a number of films for director Richard Attenborough, including "Cry Freedom", where he portrayed the conservative South African justice minister Jimmy Kruger (for which he received a BAFTA nomination for Best Supporting Actor), and "Chaplin" alongside Robert Downey Jr..
Thaw also appeared in the TV adaptation of the Michelle Magorian book "Goodnight Mister Tom" (Carlton Television/ITV). It won "Most Popular Drama" at the National Television Awards, 1999. In September 2006, Thaw was voted by the general public as number 3 in a poll of "TV's Greatest Stars".
Personal life.
On 27 June 1964, Thaw married Sally Alexander, a feminist activist and theatre stage manager, and now professor of history at Goldsmiths, University of London. They divorced four years later. He met actress Sheila Hancock in 1969 on the set of a London comedy "So What About Love?" She was married to fellow actor Alexander "Alec" Ross, and after Thaw professed his love to Hancock, she told him that she would not have an affair. After the death of her husband (from oesophageal cancer) in 1971, Thaw and Hancock married on 24 December 1973 in Cirencester, and he remained with her until his death in 2002 (also from oesophageal cancer).
He had three daughters (all of whom are actresses): Abigail from his first marriage to Sally Alexander, Joanna from his second marriage to Sheila Hancock, and he also adopted Sheila Hancock's daughter Melanie Jane, from Hancock's first marriage to Alec Ross. Melanie Jane legally changed her surname from Ross to Thaw.
Thaw was a committed socialist and lifelong supporter of the Labour Party. He was appointed a Commander of the Most Excellent Order of the British Empire (CBE) in March 1993 by Queen Elizabeth II. In September 2006, Thaw was voted by the general public as number 3, after David Jason and Morecambe and Wise, in a poll of TV's 50 Greatest Stars for the past 50 years.
Illness and death.
A heavy drinker until going teetotal in 1995, and a heavy smoker from the age of 12, Thaw was diagnosed with cancer of the oesophagus in June 2001. He underwent chemotherapy in hope of overcoming the illness, and at first seemed to be responding well to the treatment, but just before Christmas 2001 he was told that the cancer had spread.
He died on 21 February 2002, seven weeks after his 60th birthday, the day after he signed a new contract with ITV, and the day before his wife's birthday. At the time of his death he was living at his country home, near the villages of Luckington and Sherston in Wiltshire, and was cremated in Westerleigh, near Yate in South Gloucestershire, in a private service. A memorial service was held on 4 September 2002 at St Martin-in-the-Fields church in Trafalgar Square, attended by 800 people including Prince Charles, Lord Attenborough, Sir Tom Courtenay and Cherie Blair.
Honours and awards.
Won
Nominated

</doc>
<doc id="40375" url="http://en.wikipedia.org/wiki?curid=40375" title="Outline of space technology">
Outline of space technology

Space technology – technology developed by space science or the aerospace industry for use in spaceflight, satellites, or space exploration. Space technology includes spacecraft, satellites, space stations, and support infrastructure, equipment, and procedures. Space is such a novel environment that attempting to work in it requires new tools and techniques. Many common everyday services such as weather forecasting, remote sensing, GPS systems, satellite television, and some long distance communications systems critically rely on space infrastructure. Of the sciences, astronomy and Earth science (via remote sensing) benefit from space technology. New technologies originating with or accelerated by space-related endeavors are often subsequently exploited in other economic activities.

</doc>
<doc id="40377" url="http://en.wikipedia.org/wiki?curid=40377" title="Ole Einar Bjørndalen">
Ole Einar Bjørndalen

Ole Einar Bjørndalen (born 27 January 1974) is a Norwegian professional biathlete, often referred to by the nickname "The King of Biathlon". He is the most medaled Olympian in the history of the Winter Olympic Games, with 13 medals. He is also the most successful biathlete of all time at the Biathlon World Championships, having won 40 medals, double that of any other biathlete. With 94 World Cup wins, Bjørndalen is ranked first all-time for career victories on the Biathlon World Cup tour, more than twice that of anyone else. He has won the Overall World Cup title six times, in 1997–98, in 2002–03, in 2004–05, in 2005–06, in 2007–08 and in 2008–09, more than any other male biathlete and the same as female record holder Magdalena Forsberg.
In 1992, he won his first career medal at the junior world championships. A year later in 1993, after winning three junior world championship titles, a medal haul only previously achieved by Sergei Tchepikov, Bjørndalen made his Biathlon World Cup debut. His breakthrough came in 1994 when he featured on his first World Cup podium in a sprint race held in Bad Gastein, Austria. Bjørndalen first competed in the Olympic Games at the Lillehammer 1994 Winter Olympics, held in his home country of Norway. He obtained his first major victory on 11 January 1996 in an individual competition held in Antholz-Anterselva, Italy. On 20 February 2014, Bjørndalen was elected to an eight-year term at the International Olympic Committee's athlete commission.
Career.
In 1993, at the age of 19, Bjørndalen first came into focus by winning 3 out of 4 possible gold medals at the Junior Biathlon World Championships, which among other things led to him being chosen to represent Norway in the 1994 Olympics, at the cost of highly merited biathlete Eirik Kvalfoss. As of 19 February 2014, Bjørndalen has won eight Olympic gold medals, four Olympic silver, one Olympic bronze, nineteen World Championship gold medals, eleven silver, and nine bronze, and a record high of 94 individual Biathlon World Cup wins, the most of any biathlete to date.
He has won the World Cup six times (1997–98, 2002–03, 2004–05, 2005–06, 2007–08, and 2008–09), finished second six times (1996–97, 1998–99, 1999–2000, 2000–01, 2003–04, and 2006–07), and third once (2001–02). In his first season (1992–93) he finished 62nd, the season after, 30th and the season after that, 4th. When winning the overall world cup in 1998, at the age of 24, he won every event in biathlon in one season – world championships gold medal, Olympic gold medal and the overall World Cup title. His World Cup podium record is 171 podium finishes, 94 1st places, 50 2nd places, and 27 3rd places in the individual events. Bjørndalen has 1 World Cup victory in the team event. In relay Bjørndalen has won 33 races, he has also 21 2nd places and 12 3rd.places. In total he has 66 podium finishes in the world cup, relay event. Bjørndalen has 238 World cup podium finishes, individual, team and relay races combined.
Bjørndalen has won the Sprint world cup nine times in the seasons: 1994-1995, 1996-1997, 1997-1998, 1999-1900, 2000–01, 2002–03, 2004–05, 2007–08 and 2008-09. Ole Einar Bjoerndalen also came 2nd in the Sprint world cup in the seasons: 2003-04 and 2005-06. Ole Einar has won Pursuit world cup five times from 1999-00, 2002–03, 2005–06, 2007–08 and 2008-09. He has 2nd place in the seasons 2000-01, 2003–04, 2004–05, 2006-07 and 3rd places in 1996-97, 1998–99 and 2001-02. Bjoerndalen has been winner of the Mass start world cup five times in: 2002-03, 2004–05, 2005–06, 2006–07 and 2007-08. He came 2nd in 2000-01, 2003–04 and 2008-09.
Ole Einar Bjoerndalen was number 3 in the Mass start world cup in the season 1998-99. He has also once won the Normal distance world cup. It was in 2004-05. Bjoerndalen has also finished number 2 in the 1998-99, 2000–01, 2001–02 and 2005-06 seasons.
Ole Einar also came 3rd in 1997-98 season. He has won a total of 20 times, 13 times finished in second place and five times came in 3rd place. Overall, he has been on the podium 38 times.
Bjørndalen has won the relay world cup nine times in the seasons: 1997-98, 1999-00, 2000–01, 2001–02, 2003–04, 2004–05, 2007–08, 2009–10 and 2010-11. He has 6 times finished second in the world cup relay in: 1996-97, 2006–07, 2008–09, 2011–12, 2012–13 and 2014-15.
Bjoerndalen also came in third place in: 1998-99 and 2002-03 . All together he has been on the podium 17 seasons in the world cup relay. Bjørndalen has won the mixed relay world cup 3 times. It happened in the seasons: 2012-13, 2013–14 and 2014-15.
OEB has won (together with the Norwegian biathlon team) the nations cup nine times. It happened in the: 1998/99, 2002/03, 2003/04, 2004/05, 2007/08, 2008/09, 2010/11, 2013/14 and 2014/15 season. Bjørndalen has also achieved five second places in the nationscup in the years: 1999/00, 2000/01, 2001/02, 2005/06 and 2012/13. He has finished in third place in the nations cup 3 times, in the: 1996/97, 1997/98 and 2006/07 season. In total he has finished 17 times at the podium in the nations cup for men.
He is the only biathlete ever to win all biathlon events in a single Winter Olympics (2002 Salt Lake City Games). This encompassed the sprint, pursuit, individual, and relay events, the latter together with three other participants. Bjørndalen's 94 biathlon World Cup victories and one cross-country victory is four behind of Gunda Niemann-Stirnemann's record of 98 World Cup victories for a winter sport athlete.
Bjørndalen occasionally competes in cross-country skiing, and in 2006 he won an FIS Cross-Country World Cup race in Gällivare, Sweden, to become the first male biathlete to win a cross-country competition. He is also the only biathlete who has won every event during the same Winter Olympic Games (four gold medals). He achieved this feat at the Salt Lake City 2002 Winter Olympics, becoming the most successful athlete there. This makes him one of only three Olympians to win four gold medals during the same Winter Games. He repeated this medal haul at the Biathlon World Championships 2005 in Hochfilzen, Austria and at the Biathlon World Championships 2009 in Pyeongchang, South Korea.
At the Vancouver 2010 Winter Olympics, Bjørndalen became the most successful biathlete in Winter Olympic history by surpassing the previous record of nine career Olympic medals, which he shared with Uschi Disl of Germany. He then anchored Norway to gold in the 4 × 7.5 km relay. This was the second time that Norway had won a title in this event, with the other being at the 2002 Winter Olympics (also anchored by Bjørndalen). With this victory he became the second most decorated Winter Olympian of all time and one of only two athletes to win 11 medals at the Winter Olympics. With his gold medal in 10 km sprint at the Sochi 2014 Winter Olympics, he tied fellow Norwegian Bjørn Dæhlie for most Winter Olympic medals, with 12 in total.
As of February 19, 2014, Bjørndalen has won eight Olympic gold medals, four silver and a bronze. He has also won 19 World Championship gold medals, 12 silver and nine bronze (more than anybody in biathlon history), along with a record 94 World Cup victories in Biathlon an 1 World Cup victory in Cross Country skiing, 171 podium finishes in Biathlon and 3 in Cross Country skiing. He also finished in the top three of the Overall World Cup rankings for a record thirteen successive seasons between the 1996–97 and 2008–09 seasons. In total Ole Einar Bjørndalen has won 44 Norwegian Championship gold medals. He has won 30 gold medals in the Norwegian Championship, biathlon, winter event: 20 individual gold medals: individual (4), sprint (6), pursuit (6), mass start (4) and 10 gold medals in relay and the team event: relay (8) and team (2). Bjørndalen has also achieved 14 individual gold medals in the Norwegian Championship, biathlon, summer event: sprint (7) and pursuit (7).
2005–06 World Cup season.
Bjørndalen finished the 2005–06 International Biathlon Union World Cup season in first place, with Frenchman Raphaël Poirée in second place and German Sven Fischer in third. Bjørndalen lay in third place in the standings going into the last three races of the season in Holmenkollen, with Poirée in first, and Fischer in second. However, Bjørndalen won all three races, giving him six victories in the last eight races, and clinching the crystal globe. He also won the pursuit, and the mass start title, and came second in the individual and the sprint. In the pursuit he finished ahead of Fischer by 54 points, and 29 points ahead of Poirée in the mass start. In the individual he finished 41 points behind Michael Greis, and in the sprint he was 5 points behind Tomasz Sikora. Norway finished fourth in the relay.
Bjørndalen closed out the season by winning all three events (sprint, pursuit, and mass start) at the Holmenkollen ski festival biathlon competition. This put his career victories at the ski events to five, having won once both in 2003 (pursuit) and in 2004 (sprint).
2006–07 World Cup season.
Bjørndalen made a perfect start to the season, winning all of the first five races in Östersund and Hochfilzen. In the fifth race of the season, the pursuit race in Hochfilzen, he won with one of his largest margins ever, more than 2 minutes. On 30 December 2006 Bjørndalen took part in the Biathlon World Team Challenge in Gelsenkirchen in the Veltins Arena. In front of about 51,000 people he won it for fourth time in a row. His partner for second consecutive time was Linda Grubben. They both left their rivals, the Robert family, more than one minute behind.
In Oberhof, coming down from training in the heights, Bjørndalen performed below standard for the season, and finishing only 30th and 5th in the individual competitions.
In Ruhpolding he led his team-mates to victory in the relay event. He won the two following individual competitions. After competing in the FIS Nordic World Ski Championships Sapporo 2007, he missed several Biathlon World Cup events; after missing eight competitions altogether Bjørndalen finished second in the overall standings, after German Michael Greis.
2008–09 World Cup season.
Bjørndalen started off the season suffering from the effects of long-term illness, but still placed second in both of the pursuit events. He missed the Biathlon World Team Challenge in Gelsenkirchen, focusing on training instead. After the break, he returned with victories in both the sprint and pursuit events in Ruhpolding and a third place in the mass start in Oberhof.
At the Biathlon World Championships 2009 in Pyeongchang, during the men's 12.5 km pursuit, Bjørndalen with at least 15 other competitors accidentally skied the wrong way at the start of the first lap due to the bad marking. Just after leaving the start, the athletes skied over a bridge instead of skiing beside it, which was the right way. A jury meeting decided to give all these athletes a one-minute time penalty, following a complaint from the Russian team. However, another complaint by seven other member states led to the Appeal Jury reverting to the original result. Along with Bjørndalen's first ever 20 km individual World Championship title, he won four out of six possible gold medals (10 km sprint, 12.5 km pursuit, 20 km individual and the 4 × 7.5 km relay).
After the World Championships Bjørndalen came second in the sprint in Vancouver, he took over the world cup overall lead. He followed up with a second place, and two victories at the events in Granåsen, Trondheim (the latter being a mass start where he shot clean). He secured his sixth overall win in the last sprint of the season, in Khanty-Mansiysk where he placed second. In the following event (a pursuit), he was beaten at the finish line by teammate Emil Hegle Svendsen, but won the pursuit cup.
Personal life.
Bjørndalen resides in the village of Obertilliach, Austria. He also used to live in Toblach, Italy, with Italian-Belgian biathlete Nathalie Santer. They started dating in 1998 and married on 27 May 2006. On 4 October 2012 they filed for divorce on mutual agreement.
Awards and honors.
Ole Einar Bjørndalen won the Aftenposten's gold medal in 1998. He was named the Norwegian Sportsperson of the Year in 2002 and 2014. For his accomplishments in biathlon and cross-country skiing, Bjørndalen received the Egebergs Ærespris in 2002. Bjørndalen was also awarded with the Fearnleys olympic honorary award in 2002. Ole Einar Bjørndalen was nominated Laureus World Sportsman of the year in 2003 He came second, only lost to Lance Armstrong this year, who is later rescinded. In 2008, a nearly three meter tall bronze statue of Bjørndalen, created by sculptor Kirsten Kokkin, was erected in his hometown of Simostranda, Norway. In March 2011, he, Michael Greis and Andrea Henkel were awarded the Holmenkollen Medal, the first biathletes to receive the medal. In November 2014 Bjørndalen was awarded, the best male athlete at the Olympic Winter Games 2014 in Sochi by the Association des Comités Nationaux Olympiques 
Worldcup Cross Country.
Bjørndalen first participated in the worldcup cross country in Finland at the 10 kilometer skating event in a small town called Muonio in November 1998. His position was number 23 in this race. His first podium place in the world cup cross country came in Kuopio 25 November 2001. Bjørndalen came in 2nd place in the 10 km skating event. One month later he once again came in 2nd place, this time he lost to Per Elofsson in the 30 km skating mass-start event in Ramsau, Austria. 
18 November 2006 Bjørndalen made history by becoming the first biathlist to win a world cup event in cross country in the Swedish town Gällivare. Bjørndalen won the 15 km skating event. In 2007 his fellow country man, and fellow biathlist Lars Berger won the 15 km, cross country event at the World Championship in 2007.
Bjørndalen has been on the podium twice in relay in the world cup in cross country for his country, Norway. First time was in Beitostølen in 2003, he became 3rd. Second time he came on the podium was in La Clusaz in France in 2006, he came in 2nd place.
FIS events, Olympic Winter Games, cross country.
Ole Einar Bjørndalen has won FIS events in cross country twice. First time in 1997 on the 30 kilometer skating event in Valdres, Norway. The second time he won was in the 10 km skating event in Beitostølen, Norway in 2006. He has also two 2nd places in a FIS-event. First time he came in second place was at the 15 km skating event in Misurina, Italia, 1998. Second time he came in second place was at the 10 km skating event at Beitostølen in 2004. In addition to this, Bjørndalen has one third place in the FIS event at Beitostølen in 2001, 10 km skating. Ole Einar Bjørndalen became 5th in the Olympic Winter Games in the cross country event, 30 km skating in Salt Lake City, the 9th of February 2002. He has won Skarverennet in 2006 and 2007, and came in 2nd place after Petter Northug in 2008.
Other victories.
Ole Einar Bjørndalen won the Beach Volley Ball Championship at Laguna Beach in 2001 Bjørndalen has won 4 times at the show event in Gelsenkirchen (Schalke 04 arena), in 2003, 2004, 2005 and 2006. Ole Einar Bjørndalen has achieved 2nd place in the 2003 Dobbiaco-Cortina, a longdistance cross country skiing event, (42 km) in Italy in the town Cortina. He won his 2nd place in the 26th edition of this prestigious event. Bjørndalen was only beaten by Costantin Pierluigi from Italy. Pierluigi winner time was 1 hour 43 minutes and 16,5 seconds. Ole Einar came halv a second shy of the winner time. Bjørndalen has also won the biathlon show-event in Püttlingen in 2008 together with Kati Wilhelm. Bjørndalen has achieved these podium places in the biathlon show-event in Püttlingen: 1st in 2008 (together with Kati Wilhelm), 2nd in 2011 (together with Magdalena Neuner) and 3rd place in 2005 (together with Nathalie Santer, his former wife.) and 2010 (together with Sabrina Buchholz).
Results.
All results are sourced from the International Biathlon Union.
Olympic Games.
"13 medals (8 gold, 4 silver, 1 bronze)"
World Championships.
"40 medals (19 gold, 12 silver, 9 bronze)"
Individual victories.
"94 victories (36 Sp, 37 Pu, 7 In, 14 MS)" 
Shooting.
Bjørndalen is a solid shooter, but is generally outside the top twenty marksmen. Bjørndalen finished the 2005–06 season with a shooting percentage of 84%, hitting 292 out of 345 possible targets, that placed him in 36th position for shooting accuracy. His shooting record for both prone and standing were practically identical, 146/172 in the prone and 146/173 in the standing position. In the individual disciplines, he shot 92% in the individual, 89% in the sprint, 96% in the pursuit, 93% in the mass start and 96% in the relay.
In the 2004–05 season Bjørndalen was the 16th best shot with an 85% success rate, the second best Norwegian behind Egil Gjelland. He hit 331 targets out of a possible 364. His prone like most biathletes was much better than his standing shoot, he hit 169/180 (92%) in the prone and 163/184 (81%) in the standing. He had an average of 88% in the individual, sprint and relay, a 91% hit rate in the mass start but only 79% in the pursuit. During his career in 1999/00 he averaged 82%, in 2000–01 78%, 2001–02 74%, 2002–03 86% and in 2003–04 he hit 80% of the targets, however in those five years his standing shoot was the same or better than his prone shoot. In comparison his greatest rival Raphaël Poirée averaged 87% in 2004–05 and 86% in 2005–06. Nikolay Kruglov was the best shot in 2004–05 with a 91% success rate, with Ricco Groß in second with 89%, and in 2005 Julien Robert was best with a 93% average and Groß again second with 91%.
Shooting statistics.
Statistics sourced from the International Biathlon Union.
and 
Equipment.
Bjørndalen uses Madshus skis, boots and poles.
He uses Rottefella NNN bindings.
His gloves and base layer are from Odlo, and he uses Casco glasses.
During the off-season in 2006 Bjørndalen was testing a new ski boot that had a high heel in the Torsby ski tunnel with boot manufacturers Madshus. The theory is that it forces the knee more forward for better position and it incorporates the large gluteal muscles.
References.
General
Specific

</doc>
<doc id="40381" url="http://en.wikipedia.org/wiki?curid=40381" title="Janne Lahtela">
Janne Lahtela

Janne Lahtela (born February 28, 1974 in Kemijärvi) is a Finnish former athlete, who established himself as one of the most dominant persons in the history of moguls skiing. He is currently the head coach of Japan's freestyle skiing team. He also is a key founder and sponsor for the IDOne ski company based out of Japan.
Lahtela won a gold medal in the moguls final of 2002 Winter Olympic Games. Four years earlier he had taken a silver medal in front of his cousin Sami Mustonen, who took bronze. He has also won the moguls skiing World Cup five times and became a World Champion in 1999.

</doc>
<doc id="40384" url="http://en.wikipedia.org/wiki?curid=40384" title="Tristan Gale">
Tristan Gale

Tristan Gale (born August 10, 1980) is an American skeleton racer who competed from 2001 to 2006. At the 2002 Winter Olympics, she became the inaugural women's skeleton champion. Gale dyed her hair with streaks of red, white and blue for the 2002 Olympics. During the 2002–2003 season, Tristan won a second gold medal on her home track in Salt Lake during a World Cup stop. She remains undefeated at the track in Utah since the Olympics.
Gale also won a bronze medal in the women's skeleton event at the 2003 FIBT World Championships in Nagano. She retired before the 2006 Winter Olympics in Turin. Gale's best overall seasonal finish in the Skeleton World Cup was third in 2002-3.
A native of Ruidoso, New Mexico, Gale lives in Salt Lake City.

</doc>
<doc id="40385" url="http://en.wikipedia.org/wiki?curid=40385" title="Ko Gi-hyun">
Ko Gi-hyun

Ko Gi-Hyun (Hangul: 고기현, Hanja: 高基鉉) (born May 11, 1986) is a South Korean short track speed skater. Ko remains the second youngest individual gold medalist after Tara Lipinski in the history of the Olympic Winter Games, winning gold in women's 1500 m event at the 2002 Winter Olympics in Salt lake city, United States, at 15 years and 277 days old.

</doc>
<doc id="40386" url="http://en.wikipedia.org/wiki?curid=40386" title="EFF">
EFF

EFF or Eff may refer to:

</doc>
<doc id="40387" url="http://en.wikipedia.org/wiki?curid=40387" title="SVG (disambiguation)">
SVG (disambiguation)

SVG is an acronym that can stand for:

</doc>
<doc id="40389" url="http://en.wikipedia.org/wiki?curid=40389" title="Upper Canada">
Upper Canada

The Province of Upper Canada (French: "province du Haut-Canada") was a part of British Canada established in 1791 by the United Kingdom to govern the central third of the lands in British North America and to accommodate Loyalist refugees of the United States of America after the American Revolution. The new province remained, for the next fifty years of growth and settlement, the colonial government of the territory.
Upper Canada existed from 26 December 1791 to 10 February 1841 and generally comprised present-day Southern Ontario. The "upper" prefix in the name reflects its geographic position being closer to the headwaters of the Saint Lawrence River than that of Lower Canada (or present-day Quebec) to the northeast.
Upper Canada included all of modern-day Southern Ontario and all those areas of Northern Ontario in the "Pays d'en Haut" which had formed part of New France, essentially the watersheds of the Ottawa River or Lakes Huron and Superior (excluding any lands within the watershed of Hudson Bay).
Establishment.
Control of all of Canada passed from France to Great Britain in 1763 when the Treaty of Paris ended the Seven Years' War in America. The territories of modern southern Ontario and southern Quebec were initially maintained as the single Province of Quebec, as it had been under the French. From 1763 to 1791, the Province of Quebec maintained its French language, cultural behavioural expectations, practices and laws. This status was renewed and reinforced by the Quebec Act of 1774, which expanded Quebec's territory to include part of the Indian Reserve to the west (i.e., parts of southern Ontario), and other western territories south of the Great Lakes including much of what would become the United States' Northwest Territory, including the modern states of Illinois, Indiana, Michigan, Ohio, Wisconsin and parts of Minnesota.
The part of the province west of Montreal and Quebec in the upper river basin soon began receiving many English-speaking Protestant United Empire Loyalists who arrived in the area as refugees from the American Revolution. This region quickly became culturally distinct. While the act addressed some religious issues, it did not appease those used to English law.
"Upper Canada" became a political entity on December 26, 1791 with the Parliament of Great Britain's passage of the Constitutional Act of 1791. The act divided the Province of Quebec into Upper and Lower Canada. The division was effected so that Loyalist American settlers and British immigrants in Upper Canada could have English laws and institutions, and the French-speaking population of Lower Canada could maintain French civil law and the Catholic religion.
The first lieutenant-governor was John Graves Simcoe. On February 1, 1796, the capital of Upper Canada was moved from Newark (now Niagara-on-the-Lake) to York (now Toronto), which was judged to be less vulnerable to attack by the Americans.
Government.
Provincial administration.
Upper Canada's constitution was said to be "the very image and transcript" of the British constitution, and based on the principal of "mixed monarchy" - a balance of monarchy, aristocracy and democracy.
The Executive arm of government in the colony consisted of a lieutenant-governor, his executive council, and the Officers of the Crown: the Adjutant General of the Militia, the Attorney General, the Auditor General of Land Patents, the Auditor General (only one appointment ever made), Crown Lands Office, Indian Office, Inspector General, Kings' Printer, Provincial Secretary & Registrar's Office, Receiver General, Solicitor General, & Surveyor General.
The Executive Council of Upper Canada had a similar function to the Cabinet in England but was not responsible to the Legislative Assembly. They held a consultative position, however, and did not serve in administrative offices as cabinet ministers do. Members of the Executive Council were not necessarily members of the Legislative Assembly but were usually members of the Legislative Council.
Parliament.
The Legislative branch of the government consisted of the parliament comprising legislative council and legislative assembly. When the capital was first moved to Toronto from Newark (present-day Niagara-on-the-Lake) in 1796, the Parliament Buildings of Upper Canada were located at the corner of Parliament and Front Streets, in buildings that were burned by U.S. forces in the War of 1812, rebuilt, then burned again by accident. The site was eventually abandoned for another, to the west.
The Legislative Council of Upper Canada was the upper house governing the province of Upper Canada. Although modelled after the British House of Lords, Upper Canada had no aristocracy. Members of the Legislative council, appointed for life, formed the core of the oligarchic group, the Family Compact, that came to dominate government and economy in the province.
The Legislative Assembly of Upper Canada functioned as the lower house in the Parliament of Upper Canada. Its legislative power was subject to veto by the appointed Lieutenant Governor, Executive Council, and Legislative Council.
Local government.
Local government in the Province of Upper Canada was based on districts. In 1788, four districts were created:
The name changes all took place in 1792.
Justices of the Peace were appointed by the Lt. Governor. Any two justices meeting together could form the lowest level of the justice system, the Courts of Request. A Court of Quarter Sessions was held four times a year in each district composed of all the resident justices. The Quarter Sessions met to oversee the administration of the district and deal with legal cases. They formed, in effect, the municipal government until an area was incorporated as either a Police Board or a City after 1834.
Additional districts were created from the existing districts as the population grew until 1849, when local government mainly based on counties came into effect. At that time, there were 20 districts; legislation to create a new Kent District was never completed. Up until 1841, the district officials were appointed by the lieutenant-governor, although usually with local input.
Politics.
The Family Compact.
The Family Compact is the epithet applied to an oligarchic group of men who exercised most of the political and judicial power in Upper Canada from the 1810s to the 1840s. It was noted for its conservatism and opposition to democracy. The uniting factors amongst the Compact were its loyalist tradition, hierarchical class structure and adherence to the established Anglican Church. Leaders such as John Beverley Robinson and John Strachan proclaimed it an ideal government, especially as contrasted with the rowdy democracy in the nearby United States. The Family Compact emerged from the War of 1812 and collapsed in the aftermath of the Rebellions of 1837.
The Reform Movement.
There were many outstanding individual reform politicians in Upper Canada, including Robert Randal, Peter Perry, Marshall Spring Bidwell, William Ketchum and Dr. William Warren Baldwin. However, organized collective reform activity began with Robert Fleming Gourlay. Gourlay was a well-connected Scottish emigrant who arrived in 1817, hoping to encourage "assisted emigration" of the poor from Britain. He solicited information on the colony through township questionnaires, and soon became a critic of government mismanagement. When the local legislature ignored his call for an inquiry, he called for a petition to the British Parliament. He organized township meetings, and a provincial convention — which the government considered dangerous and seditious. Gourlay was tried in December 1818 under the 1804 Sedition Act and jailed for 8 months. He was banished from the province in August 1819. His expulsion made him a martyr in the reform community.
The next wave of organized Reform activity emerged in the 1830s through the work of William Lyon Mackenzie, James Lesslie, John Rolph, William John O'Grady and Dr Thomas Morrison, all of Toronto. They were critical to introducing the British Political Unions to Upper Canada. Political Unions were not parties. The unions organized petitions to Parliament.
The Upper Canada Central Political Union was organized in 1832-3 by Dr Thomas David Morrison (mayor of Toronto in 1836) while William Lyon Mackenzie was in England. This union collected 19,930 signatures on a petition protesting Mackenzie's unjust expulsion from the House of Assembly by the Family Compact.
This union was reorganized as the Canadian Alliance Society (1835). It shared a large meeting space in the market buildings with the Mechanics Institute and the Children of Peace. The Canadian Alliance Society adopted much of the platform (such as secret ballot & universal suffrage) of the Owenite National Union of the Working Classes in London, England, that were to be integrated into the Chartist movement in England.
The Canadian Alliance Society was reborn as the Constitutional Reform Society (1836), when it was led by the more moderate reformer, Dr William W. Baldwin. After the disastrous 1836 elections, it took the final form as the Toronto Political Union in 1837. It was the Toronto Political Union that called for a Constitutional Convention in July 1837, and began organizing local "Vigilance Committees" to elect delegates. This became the organizational structure for the Rebellion of 1837.
The Upper Canada Rebellion of 1837.
The Upper Canada Rebellion was an insurrection against the oligarchic government of the Family Compact by W.L. Mackenzie in December 1837. Long term grievances included antagonism between Later Loyalists and British Loyalists, political corruption, the collapse of the international financial system and the resultant economic distress, and a growing republican sentiment. While public grievances had existed for years, it was the Rebellion in Lower Canada (present day Quebec) that emboldened rebels in Upper Canada to openly revolt soon after. The Upper Canada Rebellion was largely defeated shortly after it began, although resistance lingered until 1838 (and became more violent) - mainly through the support of the Hunters' Lodges, a secret anti-British, American militia that emerged in states around the Great Lakes. They launched the Patriot War in 1838-39.
John Lambton, Lord Durham's support for "responsible government" undercut the Tories and gradually led the public to reject what it viewed as poor administration, unfair land and education policies, and inadequate attention to urgent transportation needs. Durham's report led to the administrative unification of Upper and Lower Canada as the Province of Canada in 1841. Responsible government did not occur until the late 1840s under Robert Baldwin and Louis-Hippolyte Lafontaine.
Sydenham and the Union of the Canadas.
After the Rebellions, the new governor, Charles Poulett Thomson, 1st Baron Sydenham, proved an exemplary Utilitarian, despite his aristocratic pretensions. This combination of free trade and aristocratic pretensions needs to be underscored; although a liberal capitalist, Sydenham was no radical democrat. Sydenham approached the task of implementing those aspects of Durham’s report that the colonial office approved of, municipal reform, and the union of the Canadas, with a “campaign of state violence and coercive institutional innovation... empowered not just by the British state but also by his Benthamite certainties.” Like governors Bond Head before him, and Metcalfe after, he was to turn to the Orange Order for often violent support. It was Sydenham who played a critical role in transforming Compact Tories into Conservatives.
Sydenham introduced a vast expansion of the state apparatus through the introduction of municipal government. Areas not already governed through civic corporations or police boards would be governed through centrally controlled District Councils with authority over roads, schools, and local policing. A strengthened Executive Council would further usurp much of the elected assembly’s legislative role, leaving elected politician’s to simply review the administration’s legislative program and budgets.
Settlement.
First Nations Dispossession and Reserves.
The First Nations occupying the territory that was to become Upper Canada were:
Prior to the creation of Upper Canada in 1791 much land had already been ceded by the First Nations to the Crown in accordance with the Royal Proclamation of 1763. During the American Revolutionary War most of the First Nations supported the British. After the Americans launched a campaign that burned the villages of the Iroquois in New York State in 1779 the refugees fled to Fort Niagara and other British posts, and remained permanently in Canada.
Land was granted to these allied Six Nations who had served on the British side during the American Revolution by the Haldimand Proclamation (1784). Haldimand had purchased a tract of land from the Mississaugas. The nature of the grant has been under dispute.
Loyalists, Later Loyalists & the Land Grant System.
Crown land policy to 1825 was multi-fold in the use of a "free" resource that had value to people who themselves may have little or no money for its purchase and for the price of settling upon it to support themselves and a create a new society. First, the cash-strapped Crown government in Canada could pay and reward the services and loyalty of the “United Empire Loyalists” who, originated outside of Canada, with out encumbrance of debt by being awarded with small portions of land (under 200 acre) with the proviso that it be settled by those to which it was granted; Second, portions would be reserved for the future use of the Crown and the Clergy that did not require settlement by which to gain control. Lt. Governor Simcoe saw this as the mechanism by which an aristocracy might be created, and that compact settlement could be avoided with the grants of large tracts of land to those Loyalists not required to settle on it as the means of gaining control.
Assisted Immigration.
The Calton weavers were a community of handweavers established in the community of Calton, then in Lanarkshire just outside Glasgow, Scotland in the 18th century. In the early 19th century, many of the weavers emigrated to Canada, settling in Carleton Place and other communities in eastern Ontario, where they continued their trade.
In 1825, 1,878 Irish immigrants from the city of Cork arrived in the community of Scott's Plains. The British Parliament had approved an experimental emigration plan to transport poor Irish families to Upper Canada in 1822. The scheme was managed by Peter Robinson, a member of the Family Compact and brother of the Attorney General. Scott's Plains was renamed Peterborough in his honour.
The Talbot Settlement.
Thomas Talbot emigrated in 1791, where he became personal secretary to John Graves Simcoe, Lieutenant-Governor of Upper Canada. Talbot convinced the government to allow him to implement a land settlement scheme of 5000 acre in Elgin County in the townships of Dunwich and Aldborough in 1803. According to his government agreement, he was entitled to 200 acre for every settler who received 50 acre; in this way he gained an estate of 20000 acre. Talbot's administration was regarded as despotic. He was infamous for registering settlers' names on the local settlement map in pencil and if displeased, erasing their entry. Talbot's abuse of power was a contributing factor in the Upper Canada Rebellion of 1837.
Crown & Clergy Reserves.
The Crown reserves, one seventh of all lands granted, were to provide the provincial executive with an independent source of revenue not under the control of the elected Assembly. The Clergy Reserves, also one seventh of all lands granted in the province, were created “for the support and maintenance of a Protestant clergy” in lieu of tithes. The revenue from the lease of these lands was claimed by the Rev. John Strachan on behalf of the Church of England. These reserves were directly administered by the Crown; which, in turn, came under increasing political pressure from other Protestant bodies. The Reserve lands were to be a focal point of dissent within the Legislative Assembly.
Land Sale System.
The land grant policy changed after 1825 as the Upper Canadian administration faced a financial crisis that would otherwise require raising local taxes, thereby making it more dependent on a local elected legislature. The Upper Canadian state ended its policy of granting land to “unofficial” settlers and implemented a broad plan of revenue-generating sales. The Crown replaced its old policy of land grants to ordinary settlers in newly opened districts with land sales by auction. It also passed legislation that allowed the auctioning of previously granted land for payment of back-taxes.
The Canada Company.
Few chose to lease the Crown reserves as long as free grants of land were still available. The Lieutenant Governor increasingly found himself depending upon the customs duties shared with, but collected in Lower Canada for revenue; after a dispute with the lower province on the relative proportions to be allocated to each, these duties were withheld, forcing the Lt. Governor of Upper Canada to search for new sources of revenue. It is important to note, then, that the Canada Company was created as a means of generating government revenue that was not under the control of the elected Assembly, thereby granting the Lt. Governor greater independence from local voters.
The plan for the Canada Company was promoted by the province’s Attorney General, John Beverly Robinson, then studying law at Lincoln’s Inn in London. The Lt. Governor’s financial crisis led to a quick adoption of Robinson’s scheme to sell the Crown reserves to a new land company which would provide the provincial government with annual payments of between £15,000 to £20,000. The Canada Company was chartered in London in 1826; after three years of mismanagement by John Galt, the company hired William Allan and Thomas Mercer Jones to manage the company’s Upper Canadian business. Jones was to manage the “Huron Tract,” and Allan to sell the Crown reserves already surveyed in other districts.
According to the Canada Company, “the poorest individual can here procure for himself and family a valuable tract; which, with a little labour, he can soon convert into a comfortable home, such as he could probably never attain in any other country — all his own!” However, recent studies have suggested that a minimum of £100 to £200 plus the cost of land was required to start a new farm in the bush. As a result, few of these poor settlers had any hope of starting their own farm, although many tried.
The Huron Tract.
The Huron Tract lies in the counties of Huron, Perth, Middlesex and present day Lambton County, Ontario bordering on Lake Huron to the west and Lake Erie to the east. The tract was purchased by the Canada Company for resale to settlers. Influenced by William "Tiger" Dunlop, John Galt and other businessmen formed the Canada Company. The Canada Company was the administrative agent for the Huron Tract.
The Clergy Corporation.
The Clergy Corporation was incorporated in 1819 to manage the Clergy Reserves. After the Rev. John Strachan was appointed to the Executive Council, the advisory body to the Lieutenant Governor, in 1815, he began to push for the Church of England's autonomous control of the clergy reserves on the model of the Clergy Corporation created in Lower Canada in 1817. Although all clergymen in the Church of England were members of the body corporate, the act prepared in 1819 by Strachan’s former student, Attorney General John Beverly Robinson, also appointed the Inspector General and the Surveyor General to the board, and made a quorum of three for meetings; these two public officers also sat on the Legislative Council with Strachan. These three were usually members of the Family Compact.
List of major cities and towns of Upper Canada.
In Upper Canada, major cities with key forts were an essential for survival, defending the area from armed attacks and composing the local militia.
Population.
Ethnic Groups.
Since the province is frequently referred to as "English Canada" after the Union of the Canadas, and its ethnic homogeneity said to be a factor in the Upper Canada Rebellion of 1837, it is interesting to note the range of ethnic groups in Upper Canada. However, due to the lack of a detailed breakdown, it is difficult to count each group, and this may be considered abuse of statistics. An idea of the ethnic breakdown can be had if one considers the religious census of 1842, which is helpfully provided below: Roman Catholics were 15% of the population, and adherents to this religion were, at the time, mainly drawn from the Irish and the French settlers. The Roman Catholic faith also numbered some votaries from amongst the Scottish settlers. The category of "other" religious adherents, somewhat under 5% of the population, included the Aboriginal and Metis culture.
First Nations.
See above: Land Settlement
Metis.
Many British and French-Canadian fur traders married First Nations and Inuit women from the Cree, Ojibwa, or Saulteaux First Nations. The majority of these fur traders were Scottish and French and were Catholic.
Canadiens/French-Canadians.
Early settlements in the region include the Mission of Sainte-Marie among the Hurons at Midland in 1649, Sault Ste. Marie in 1668, and Fort Pontchartrain du Détroit in 1701. Southern Ontario was part of the "Pays d'en-haut" (Upper Country) of New France, and later part of the province of Quebec until Quebec was split into Upper and Lower Canada in 1791. The first wave of settlement in the Detroit/Windsor area came in the 18th century during the French regime. A second wave came in the 19th and early 20th centuries to the areas of Eastern Ontario and Northeastern Ontario. In the Ottawa Valley, in particular, some families have moved back and forth across the Ottawa River for generations (the river is the border between Ontario and Quebec). In the city of Ottawa some areas such as Vanier and Orleans have a rich Franco-heritage where families often have members on both sides of the Ottawa River.
Loyalists/Later Loyalists.
After an initial group of about 7,000 United Empire Loyalists were thinly settled across the province in the mid-1780s, a far larger number of "late-Loyalists" arrived in the late 1790s and were required to take an oath of allegiance to the Crown in order to obtain land if they came from the US. Their fundamental political allegiances were always considered dubious. By 1812, this had become acutely problematic since the American settlers outnumbered the original Loyalists by more than ten to one. Following the War of 1812, the colonial government under Lt. Governor Gore took active steps to prevent Americans from swearing allegiance, thereby making them ineligible to obtain land grants. The tensions between the Loyalists and late Loyalists erupted in the "Alien Question" crisis in 1820-21 when the Bidwells (Barnabas and his son Marshall) sought election to the provincial assembly. They faced opponents who claimed they could not hold elective office because of their American citizenship. If the Bidwells were aliens so were the majority of the province. The issue was not resolved until 1828 when the Colonial government retroactively granted them citizenship.
Freed Slaves.
The Act Against Slavery passed in Upper Canada on July 9, 1793. The 1793 "Act against Slavery" forbade the importation of any additional slaves and freed children. It did not grant freedom to adult slaves—they were finally freed by the British Parliament in 1833. As a consequence, many Canadian slaves fled south to New England and New York, where slavery was no longer legal. Many American slaves who had escaped from the South via the Underground Railroad or fleeing from the Black Codes in the Ohio Valley came north to Ontario, a good portion settling on land lots and began farming. It is estimated that thousands of escaped slaves entered Upper Canada from the United States.
The British.
The Great Migration from Britain from 1815-1850 has been numbered at 800,000. The population of Upper Canada in 1837 is documented at 409,000. Given the lack of detailed census data it is difficult to assess the relative size of the American & Canadian born "British" and the foreign born "British." By the time of the first census in 1841, only half of the population of Upper Canada were foreign born British. References to "English Canada" can thus be confusing, and indicate little about individual ethnic identity.
Religion.
Church of England.
The first Lt. Governor, Sir John Graves Simcoe, sought to make the Church of England the Established Church of the province. To that end, he created the Clergy Reserves, the revenues of which were to support the church. The Clergy Reserves proved to be a long-term political issue, as other denominations, particularly the Church of Scotland (Presbyterians) sought a proportional share of the revenues. The Church of England was never numerically dominant in the province, as it was in England, especially in the early years when most of the American born Later Loyalists arrived. The growth of the Church of England depended largely on later British emigration for growth.
The Church was led by the Rev. John Strachan (1778-1867), a pillar of the Family Compact. Strachan was part of the oligarchic ruling class of the province, and besides leading the Church of England, also sat on the Executive Council, the Legislative Council, helped found the Bank of Upper Canada, Upper Canada College, and the University of Toronto.
Catholic Church.
Father Alexander Macdonell was a Scottish Catholic priest who formed his evicted clan into regiment, of which he served as chaplain. He was the first Catholic chaplain in the British Army since the Reformation. When the regiment was disbanded, Rev. Macdonell appealed to the government to grant its members a tract of land in Canada, and, in 1804, 160,000 acres (650 km²) were provided in what is now Glengarry County, Canada.
In 1815, he began his service as the first Roman Catholic Bishop at St. Raphael's Church in the Highlands of Ontario. In 1819 he was appointed Vicar Apostolic of Upper Canada, which in 1826 was erected into a suffrigan bishopric of the Archdiocese of Quebec. In 1826, he was appointed to the legislative council.
Macdonell's role on the Legislative Council was one of the tensions with the Toronto congregation, led by Father William O'Grady. O'Grady, like Macdonell, had served as an army chaplain (to Connell James Baldwin's soldiers in Brazil). O'Grady followed Baldwin to Toronto Gore Township in 1828. From January 1829 he was pastor of St. Paul's church in York. Tensions between the Scottish and Irish came to a head when O'Grady was defrocked, in part for his activities in the Reform movement. He went on to edit a Reform newspaper in Toronto, the "Canadian Correspondent".
Ryerson and the Methodists.
The undisputed leader of the highly fractious Methodists in Upper Canada was Egerton Ryerson, editor of their newspaper, "The Christian Guardian". Ryerson (1803-1882) was an itinerant minister — or circuit rider — in the Niagara area for the Methodist Episcopal Church — an American branch of Methodism. As British immigration increased, Methodism in Upper Canada was torn between those with ties to the Methodist Episcopal Church and the British Wesleyan Methodists. Ryerson used the "Christian Guardian" to argue for the rights of Methodists in the province and, later, to help convince rank-and-file Methodists that a merger with British Wesleyans (effected in 1833) was in their best interest.
Presbyterians.
The United Presbytery of the Canadas was formed in 1818. By 1839 this United Synod (at one time there were three presbyteries) was absorbed by The Presbyterian Church of Canada in Connection with the Established Church of Scotland. It was erected into a synod by the parent church in 1831, bolstered with missionaries supplied from the Glasgow Missionary Society. The United Associate Synod in Scotland (after 1847, the United Presbyterian Church of Scotland) agreed to send missionaries to the Canadas; three were appointed, and arrived in 1832. In 1834, this group also began to receive a number of United Synod clergy and congregations, which led to the aforementioned union with the Auld Kirk by 1840. On Christmas Day 1834, a Canadian Synod was erected in the newly incorporated city of Toronto, which also included congregations and at least one minister from the United Synod of the Canadas. They later started their own Toronto congregation in 1838. In Toronto, the United Synod of Canada congregation (formed in the Town of York in 1820), and their minister Rev. James Harris withdrew in 1834, remaining independent until 1844, when they joined with Free Church dissenters from the Church of Scotland's St. Andrew's Toronto (formed in 1830) to create Knox Presbyterian Church, Toronto.
Mennonites, Tunkers, Quakers and Children of Peace.
These groups of later Loyalists were proportionately larger in the early decades of the province's settlement. The Mennonites, Tunkers, Quakers and Children of Peace are the traditional Peace churches. The Mennonites and Tunkers were generally German speaking, and immigrated as Later Loyalists from Pennsylvania. Many of their descendants continue to speak a form of German called Pennsylvania German. The Quakers (Society of Friends) immigrated from New York, the New England States and Pennsylvania. The Children of Peace were founded during the War of 1812 after a schism in the Society of Friends in York County. A further schism occurred in 1828, leaving two branches, "Orthodox" Quakers and "Hicksite" Quakers.
Poverty.
In the decade ending in 1837, the population of Upper Canada doubled, to 397,489, fed in large part by erratic spurts of displaced paupers, the “surplus population” of the British Isles. Historian Rainer Baehre estimated that between 1831 and 1835 a bare minimum of one fifth of all emigrants to the province arrived totally destitute, forwarded by their parishes in the United Kingdom. The pauper immigrants arriving in Toronto were the excess agricultural workers and artisans whose growing ranks sent the cost of parish-based poor relief in England spiraling; a financial crisis that generated frenetic public debate and the overhaul of the Poor Laws in 1834. “Assisted emigration,” a second solution to the problem touted by the Parliamentary Under-Secretary in the Colonial Office, Robert Wilmot Horton, would remove them permanently from the parish poor rolls.
The roots of Wilmot-Horton’s “assisted emigration” policies began in April, 1820, in the middle of an insurrection in Glasgow, where a young, already twice bankrupted William Lyon Mackenzie was setting sail for Canada on a ship called Psyche. After the week-long violence, the rebellion was easily crushed; the participants were driven less by treason than distress. In a city of 147,000 people without a regular parish system of poor relief, between ten and fifteen thousand were destitute. The Prime Minister agreed to provide free transportation from Quebec to Upper Canada, a 100 acre land grant, and a year’s supply of provisions to any of the rebellious weavers who could pay their own way to Quebec. In all, in 1820 and 1821, a private charity helped 2,716 Lanarkshire and Glasgow emigrants to Upper Canada to take up their free grants, primarily in the Peterborough area. A second project was the Petworth Emigration Committee organized by the Reverend Thomas Sockett, who chartered ships and sent emigrants from England to Canada in each of the six years between 1832 and 1837. This area in the south of England was terrorized by the Captain Swing Riots, a series of clandestine attacks on large farmers who refused relief to unemployed agricultural workers. The area hardest hit — Kent — was the area where Sir Francis Bond Head, later Lt. Governor of Upper Canada in 1836, was the Assistant Poor Law Commissioner. One of his jobs was to force the unemployed into "Houses of Industry."
Trade, monetary policy and financial institutions.
Corporations.
There were two types of corporate actors at work in the Upper Canadian economy: the legislatively chartered companies and the unregulated joint stock companies. The joint stock company was popular in building public works, since it should be for general public benefit, as the benefit would otherwise be sacrificed to legislated monopolies with exclusive privileges. or lie dormant. An example of the legislated monopoly is found in the Bank of Upper Canada. However, it should be noted that the benefit of the joint-stock shareholders, as the risk takers, was whole and entire; and the general public benefitted only indirectly. As late as 1849, even the moderate reform politician Robert Baldwin was to complain that “unless a stop were made to it, there would be nothing but corporations from one end of the country to the other.” Radical reformers, like William Lyon Mackenzie, who opposed all “legislated monopolies,” saw joint stock associations as the only protection against “the whole property of the country... being tied up as an irredeemable appendage to incorporated institutions, and put beyond the reach of individual possession.” As a result, most of the joint stock companies formed in this period were created by political reformers who objected to the legislated monopolies granted to members of the Family Compact.
Currency & Banking.
Currency.
See Coins of Upper Canada.
The government of Upper Canada never issued a provincial currency. A variety of coins, mainly of French, Spanish, English and American origin circulated. The government used the Halifax standard, where one pound Halifax equalled four Spanish dollars. One pound sterling equalled 1.111 Hailfax pounds (until 1820), and 1.127 Halifax pounds after 1820.
Paper currency was issued primarily by the Bank of Upper Canada, although with the diversification of the banking system, each bank would issue its own distinctive notes.
Bank of Upper Canada.
The Bank of Upper Canada was “captured” from Kingston merchants by the York elite at the instigation of John Strachan in 1821, with the assistance of William Allan, a Toronto merchant and Executive Councillor. York was too small to warrant such an institution as indicated by the inability of its promoters to raise even the minimal 10% of the £200,000 authorized capital required for start-up. It succeeded where the Bank of Kingston had failed only because it had the political influence to have this minimum reduced by half, and because the provincial government subscribed for two thousand of its eight thousand shares. The administration appointed four of the bank’s fifteen directors that, as with the Clergy Corporation, made for a tight bond between the nominally private company and the state. Forty-four men served as bank directors during the 1830s; eleven of them were executive councilors, fifteen of them were legislative councilors, and thirteen were magistrates in Toronto. More importantly, all 11 men who had ever sat on the Executive Council also sat on the board of the Bank at one time or another. 10 of these men also sat on the Legislative Council. The overlapping membership on the boards of the Bank of Upper Canada and on the Executive and Legislative Councils served to integrate the economic and political activities of church, state, and the “financial sector.” These overlapping memberships reinforced the oligarchic nature of power in the colony and allowed the administration to operate without any effective elective check. The Bank of Upper Canada was a political sore point for the Reformers throughout the 1830s.
Bank wars: the Scottish joint stock banks.
The difference between the chartered banks and the joint stock banks lay almost entirely on the issue of liability and its implications for the issuance of bank notes. The joint stock banks lacked limited liability, hence every partner in the bank was responsible for the bank’s debts to the full extent of their personal property. The formation of new joint stock banks blossomed in 1835 in the aftermath of a parliamentary report by Dr Charles Duncombe, which established their legality here. Duncombe’s report drew in large part on an increasingly dominant banking orthodoxy in the United Kingdom which challenged the English system of chartered banks. Duncombe’s Select Committee on Currency offered a template for the creation of joint stock banks based on several successful British banks. Within weeks two Devonshire businessmen, Capt. George Truscott and John Cleveland Green, established the “Farmer’s Bank” in Toronto. The only other successful bank established under this law was "The Bank of the People" which was set up by Toronto's Reformers. The Bank of the People provided the loan that allowed William Lyon Mackenzie to establish the newspaper The Constitution in 1836 in the lead up to the Rebellion of 1837. Mackenzie wrote at the time: “Archdeacon Strachan’s bank (the old one)... serve the double purpose of keeping the merchants in chains of debt and bonds to the bank manager, and the Farmer’s acres under the harrow of the storekeeper. You will be shewn how to break this degraded yoke of mortgages, ejectments, judgments and bonds. Money bound you - money shall loose you”. During the financial panic of 1836, the Family Compact sought to protect its interests in the nearly bankrupt Bank of Upper Canada by making joint stock banks illegal.
Trade.
After the Napoleonic Wars, as industrial production in Britain took off, English manufacturers began dumping cheap goods in Montreal; this allowed an increasing number of shopkeepers in York to obtain their goods competitively from Montreal wholesalers. It was during this period that the three largest pre-war merchants who imported directly from Britain retired from business as a result; Quetton St. George in 1815, Alexander Wood in 1821, and William Allan in 1822. Toronto and Kingston then underwent a boom in the number of increasingly specialized shops and wholesalers. The Toronto wholesale firm of Isaac Buchanan and Company were one of the largest of the new wholesalers. Isaac Buchanan was a Scots merchant in Toronto, in partnership with his brother Peter, who remained in Glasgow to manage the British end of the firm. They established their business in Toronto in 1835, having bought out Isaac’s previous partners, William Guild and Co., who had established themselves in Toronto in 1832. As a wholesale firm, the Buchanan’s had invested more than £10,000 in their business.
Another of those new wholesale businesses was the Farmers' Storehouse Company. The Farmers Storehouse Company was formed in the Home District and is probably Canada's first Farmers' Cooperative. The Storehouse expedited the sale of farmer's wheat to Montreal, and provided them with cheaper consumer goods.
Wheat and grains.
Upper Canada was in the unenviable position of having few exports with which to pay for all its imported manufactured needs. For the vast majority of those who settled in rural areas, debt could be paid off only through the sale of wheat and flour; yet, throughout much of the 1820s, the price of wheat went through periodic cycles of boom and bust depending upon the British markets that ultimately provided the credit upon which the farmer lived.
In the decade 1830-9, exports of wheat averaged less than £1 per person a year (less than £6 per household), and in the 1820s just half that.
Given the small amounts of saleable wheat and flour, and the rarity of cash, some have questioned how market oriented these early farmers were. Instead of depending on the market to meet their needs, many of these farmers depended on networks of shared resources and cooperative marketing. For example, rather than hire labour, they met their labour needs through "work bees." such farmers are said to be 'subsistence oriented' and not to respond to market cues; rather, they engage in a moral economy seeking 'subsistence insurance' and a 'just price'. The Children of Peace in the village of Hope (now Sharon) are a well documented example. They were the most prosperous agricultural community in Canada West by 1851.
Timber.
The Ottawa River timber trade resulted from Napoleaon's 1806 Continental Blockade in Europe. The United Kingdom required a new source of timber for its navy and shipbuilding. Later the U.K.'s application of gradually increasing preferential tariffs increased Canadian imports. The trade in squared timber lasted until the 1850s. The transportation of raw timber by means of floating down the Ottawa River was proved possible in 1806 by Philemon Wright. Squared timber would be assembled into large rafts which held living quarters for men on their six-week journey to Quebec City, which had large exporting facilities and easy access to the Atlantic Ocean.
The timber trade was Upper and Lower Canada's major industry in terms of employment and value of the product. The largest supplier of square red and white pine to the British market was the Ottawa River and the Ottawa Valley. They had "rich red and white pine forests." Bytown (later called Ottawa), was a major lumber and sawmill centre of Canada.
Transportation and communications.
Canal system.
The early nineteenth century was the age of canals. The Erie Canal, stretching from Buffalo to Albany, New York, threatened to divert all of the grain and other trade on the upper Great Lakes through the Hudson River to New York city after its completion in 1825. Upper Canadians sought to build a similar system that would tie this trade to the St Lawrence River and Montreal.
The Rideau Canal.
The Rideau Canal's purpose was military and hence was paid for by the British and not the local treasury. It was intended to provide a secure supply and communications route between Montreal and the British naval base in Kingston. The objective was to bypass the St. Lawrence River bordering New York; a route which would have left British supply ships vulnerable to an attack. Westward from Montreal, travel would proceed along the Ottawa River to Bytown (now Ottawa), then southwest via the canal to Kingston and out into Lake Ontario. Because the Rideau Canal was easier to navigate than the St. Lawrence River due to the series of rapids between Montreal and Kingston, it became a busy commercial artery from Montreal to the Great Lakes. The construction of the canal was supervised by Lieutenant-Colonel John By of the Royal Engineers. The work started in 1826, and was completed 6 years latter in 1832 at a cost of £822,000.
The Welland Canal.
The Welland Canal was created to directly link Lake Erie with Lake Ontario, bypassing Niagara Falls and the Erie Canal. It was the idea of William Hamilton Merritt who owned a sawmill, grist mill and store on the Twelve Mile Creek. The Legislature authorized the joint-stock Welland Canal Company on January 19, 1824, with a capitalization of $150,000, and Merritt as the agent. The canal was officially opened exactly five years later on November 30, 1829. However, the original route to Lake Erie followed the Welland and Niagara Rivers and was difficult and slow to navigate. The Welland Canal Company obtained a loan of 50,000 pounds from the Province of Upper Canada in March 1831 in order to cut a canal directly to Gravelly Bay (now Port Colborne) as the new Lake Erie terminus for the canal.
By the time the canal was finished in 1837, it had cost the province £425,000 in loans and stock subscriptions. The Company was supposed to have been a private one using private capital; but the province had little private capital available, hence most of the original funds came from New York. To keep the canal in Upper Canadian hands, the province had passed a law barring Americans from the company's directorate. The company was thus controlled by the Family Compact, even though they had few shares. By 1834, it was clear the canal would never make money and that the province would be on the hook for the large loans; the canal and the canal company thus became a political issue, as local farmers argued the huge expense would ultimately only benefit American farmers in the west and the merchants who transported their grain.
Desjardins Canal.
The Desjardins Canal, named after its promoter Pierre Desjardins, was built to give Dundas, Ontario, easier access to Burlington Bay and Lake Ontario. Access to Lake Ontario from Dundas was made difficult by the topography of the area, which included a natural sand and gravel barrier, across Burlington Bay which allowed only boats with a shallow draft through. In 1823 a canal was dug through the sandbar. In 1826 the passage was completed, allowing schooners to sail to neighbouring Hamilton. Hamilton then became a major port and quickly expanded as a center of trade and commerce. In 1826 a group of Dundas businessmen incorporated in order to compete with Hamilton and increase the value of their real estate holdings. The project to build Desjardins Canal continued for ten years, from 1827 to 1837, and required constant infusions of money from the province. In 1837, the year it opened, the company's £6000 income was, of which £5,000 was from a government loan and £166 was received from canal tolls.
Lake traffic: steamships.
There is disagreement as to whether the Canadian built "Frontenac" (170 feet), launched on September 7, 1816, at Ernestown, Ontario or the U.S. built "Ontario" (110 feet), launched in the spring of 1817 at Sacketts Harbor, New York, was the first steamboat on the Great Lakes. While the "Frontenac" was launched first, the "Ontario" began active service first. The first steamboat on the upper Great Lakes was the passenger carrying "Walk-In-The-Water", built in 1818 to navigate Lake Erie.
In the years between 1809 and 1837 just over 100 steamboats were launched by Upper and Lower Canadians for the St. Lawrence River and Great Lakes trade, of which ten operated on Lake Ontario. The single largest engine foundry in British North America before 1838 was the Eagle Foundry of Montreal, founded by John Dod Ward in the fall of 1819 which manufactured 33 of the steam engines. The largest Upper Canadian engine manufacturer was Sheldon & Dutcher of Toronto, who made three engines in the 1830s before being driven to Bankruptcy by the Bank of Upper Canada in 1837.
The major owner-operators of steamships on Lake Ontario were Donald Bethune, John Hamilton, Hugh Richardson, and Henry Gildersleeve, each of whom would have invested a substantial fortune.
US relations.
War of 1812 (1812–1815).
During the War of 1812 with the United States, Upper Canada was the chief target of the Americans, since it was weakly defended and populated largely by American immigrants. However, division in the United States over the war, a lackluster American militia, the incompetence of American military commanders, and swift and decisive action by the British commander, Sir Isaac Brock, kept Upper Canada part of British North America.
Detroit was captured by the British on August 6, 1812. The Michigan Territory was held under British control until it was abandoned in 1813. The Americans won the decisive Battle of Lake Erie (September 10, 1813) and forced the British to retreat from the western areas. On the retreat they were intercepted at the Battle of the Thames (October 5, 1813) and destroyed in a major American victory that killed Tecumseh and broke the power of Britain's Indian allies.
Major battles fought on territory in Upper Canada included;
Many other battles were fought in American territory bordering Upper Canada, including the Northwest Territory (most in modern day Michigan), upstate New York and naval battles in the Great Lakes.
The Treaty of Ghent (ratified in 1815) ended the war and restored the status quo ante bellum between the combatants.
1837 Rebellion and Patriot War.
Mackenzie, Duncombe, John Rolph and 200 supporters fled to Navy Island in the Niagara River, where they declared themselves the Republic of Canada on December 13. They obtained supplies from supporters in the United States, resulting in British reprisals (see Caroline affair). This incident has been used to establish the principle of "anticipatory self-defense" in international politics, which holds that it may be justified only in cases in which the "necessity of that self-defense is instant, overwhelming, and leaving no choice of means, and no moment for deliberation". This formulation is part of the "Caroline" test. The "Caroline" affair is also now invoked frequently in the course of the dispute around preemptive strike (or preemption doctrine).
On January 13, 1838, under attack by British armaments, the rebels fled. Mackenzie went to the United States where he was arrested and charged under the Neutrality Act. The Neutrality Act of 1794 made it illegal for an American to wage war against any country at peace with the United States. Application of the Neutrality Act during the Patriot War led to the largest use of US government military force against its own citizens since the Whiskey Rebellion.
The extended series of incidents comprising the Patriot War were finally settled by U.S. Secretary of State Daniel Webster and Alexander Baring, 1st Baron Ashburton, in the course of their negotiations leading to the Webster–Ashburton Treaty of 1842.
Education.
In 1807 the Grammar School Act allowed the government to take over various grammar schools across the province and incorporating them into a network of eight new, public grammar schools (secondary schools), one for each of the eight districts (Eastern, Johnstown, Midland, Newcastle, Home, Niagara, London, and Western).
Of those, these were the better known ones:
Canada West.
Canada West was the western portion of the United Province of Canada from February 10, 1841, to July 1, 1867. Its boundaries were identical to those of the former Province of Upper Canada. Lower Canada would also become Canada East.
The area was named the Province of Ontario under the British North America Act of 1867.
Further reading.
</dl>

</doc>
<doc id="40390" url="http://en.wikipedia.org/wiki?curid=40390" title="Chat">
Chat

Chat or chats may refer to:

</doc>
<doc id="40394" url="http://en.wikipedia.org/wiki?curid=40394" title="Design Patterns">
Design Patterns

Design Patterns: Elements of Reusable Object-Oriented Software is a software engineering book describing recurring solutions to common problems in software design. The book's authors are Erich Gamma, Richard Helm, Ralph Johnson and John Vlissides with a foreword by Grady Booch. The book is divided into two parts, with the first two chapters exploring the capabilities and pitfalls of object-oriented programming, and the remaining chapters describing 23 classic software design patterns. The book includes examples in C++ and Smalltalk.
It has been highly influential to the field of software engineering and is regarded as an important source for object-oriented design theory and practice. More than 500,000 copies have been sold in English and in 13 other languages. The authors are often referred to as the Gang of Four (GoF).
History.
The book started at a birds of a feather (BoF) session at OOPSLA '90, "Towards an Architecture Handbook", run by Bruce Anderson, where Erich Gamma and Richard Helm met and discovered their common interest. They were later joined by Ralph Johnson and John Vlissides. The original publication date of the book was October 21, 1994 with a 1995 copyright, hence it is often cited with a 1995-year, despite being published in 1994. The book was first made available to the public at OOPSLA meeting held in Portland, Oregon, in October 1994.
In 2005 the ACM SIGPLAN awarded that year's Programming Languages Achievement Award to the authors, in recognition of the impact of their work "on programming practice and programming language design". As of March 2012, the book was in its 40th printing.
Introduction, Chapter 1.
Chapter 1 is a discussion of object-oriented design techniques, based on the authors' experience, which they believe would lead to good object-oriented software design, including:
The authors claim the following as advantages of interfaces over implementation:
Use of an interface also leads to dynamic binding and polymorphism, which are central features of object-oriented programming.
The authors refer to inheritance as "white-box reuse", with
white-box referring to visibility, because the internals of parent classes are often visible to subclasses. In contrast, the authors refer to object composition (in which objects with well-defined interfaces are used dynamically at runtime by objects obtaining references to
other objects) as "black-box reuse" because no internal details of composed objects need be visible in the code using them.
The authors discuss the tension between inheritance and encapsulation at length and state that in their experience, designers overuse inheritance (Gang of Four 1995:20). The danger is stated as follows:
They warn that the implementation of a subclass can become so bound up with the implementation of its parent class that any change in the parent's implementation will force the subclass to change. Furthermore, they claim that a way to avoid this is to inherit only from abstract classes—but then, they point out that there is minimal code reuse.
Using inheritance is recommended mainly when adding to the functionality of existing components, reusing most of the old code and adding relatively small amounts of new code.
To the authors, 'delegation' is an extreme form of object composition that can always be used to replace inheritance. Delegation involves two objects: a 'sender' passes itself to a 'delegate' to let the delegate refer to the sender. Thus the link between two parts of a system are established only at runtime, not at compile-time. The Callback article has more information about delegation.
The authors also discuss so-called parameterized types, which are also known as generics (Ada, Eiffel, Java, C#, VB.NET, and Delphi) or templates (C++). These allow any type to be defined without specifying all the other types it uses—the unspecified types are supplied as 'parameters' at the point of use.
The authors admit that delegation and parameterization are very powerful but add a warning:
The authors further distinguish between 'Aggregation', where one object 'has' or 'is part of' another object (implying that an aggregate object and its owner have identical lifetimes) and acquaintance, where one object merely 'knows of' another object. Sometimes acquaintance is called 'association' or the 'using' relationship. Acquaintance objects may request operations of each other, but they aren't responsible for each other. Acquaintance is a weaker relationship than aggregation and suggests much looser coupling between objects, which can often be desirable for maximum maintainability in a design.
The authors employ the term 'toolkit' where others might today use 'class library', as in C# or Java. In their parlance, toolkits are the object-oriented equivalent of subroutine libraries, whereas a 'framework' is a set of cooperating classes that make up a reusable design for a specific class of software. They state that applications are hard to design, toolkits are harder, and frameworks are the hardest to design.
Case study, Chapter 2.
Chapter 2 is a step-by-step case study on "the design of a 'What-You-See-Is-What-You-Get' (or 'WYSIWYG') document editor called Lexi." (pp33)
The chapter goes through seven problems that must be addressed in order to properly design Lexi, including any constraints that must be followed. Each problem is analyzed in depth, and solutions are proposed. Each solution is explained in full, including pseudo-code and a slightly modified version of Object Modeling Technique where appropriate.
Finally, each solution is associated directly with one or more design patterns. It is shown how the solution is a direct implementation of that design pattern.
The seven problems (including their constraints) and their solutions (including the pattern(s) referenced), are as follows:
Document Structure.
The document is "an arrangement of basic graphical elements" such as characters, lines, other shapes, etc., that "capture the total information content of the document"(pp35). The structure of the document contains a collection of these elements, and each element can in turn be a substructure of other elements.
Problems and Constraints
Solution and Pattern
A "recursive composition" is a hierarchical structure of elements, that builds "increasingly complex elements out of simpler ones" (pp36). Each node in the structure knows of its own children and its parent. If an operation is to be performed on the whole structure, each node calls the operation on its children (recursively).
This is an implementation of the composite pattern, which is a collection of nodes. The node is an abstract base class, and derivatives can either be leaves (singular), or collections of other nodes (which in turn can contain leaves or collection-nodes). When an operation is performed on the parent, that operation is recursively passed down the hierarchy.
Formatting.
Formatting differs from structure. Formatting is a method of constructing a particular instance of the document's physical structure. This includes breaking text into lines, using hyphens, adjusting for margin widths, etc.
Problems and Constraints
Solution and Pattern
A "Compositor" class will encapsulate the algorithm used to format a composition. Compositor is a subclass of the primitive object of the document's structure. A Compositor has an associated instance of a Composition object. When a Compositor runs its codice_1, it iterates through each element of its associated Composition, and rearranges the structure by inserting Row and Column objects as needed.
The Compositor itself is an abstract class, allowing for derivative classes to use different formatting algorithms (such as double-spacing, wider margins, etc.)
The Strategy Pattern is used to accomplish this goal. A Strategy is a method of encapsulating multiple algorithms to be used based on a changing context. In this case, formatting should be different, depending on whether text, graphics, simple elements, etc., are being formatted.
Embellishing the User Interface.
The ability to change the graphical interface that the user uses to interact with the document.
Problems and Constraints
Solution and Pattern
The use of a "transparent enclosure" allows elements that augment the behaviour of composition to be added to a composition. These elements, such as Border and Scroller, are special subclasses of the singular element itself. This allows the composition to be augmented, effectively adding state-like elements. Since these augmentations are part of the structure, their appropriate codice_2 will be called when the structure's codice_2 is called. This means that the client does not need any special knowledge or interface with the structure in order to use the embellishments.
This is a Decorator pattern, one that adds responsibilities to an object without modifying the object itself.
Supporting Multiple Look-And-Feel Standards.
Look-and-feel refers to platform-specific UI standards. These standards "define guidelines for how applications appear and react to the user" (pp47).
Problems and Constraints
Solution and Pattern
Since object creation of different concrete objects cannot be done at runtime, the object creation process must be abstracted. This is done with an abstract guiFactory, which takes on the responsibility of creating UI elements. The abstract guiFactory has concrete implementations, such as MotifFactory, which creates concrete elements of the appropriate type (MotifScrollBar). In this way, the program need only ask for a ScrollBar and, at run-time, it will be given the correct concrete element.
This is an Abstract Factory. A regular factory creates concrete objects of one type. An abstract factory creates concrete objects of varying types, depending on the concrete implementation of the factory itself. Its ability to focus on not just concrete objects, but entire "families" of concrete objects "distinguishes it from other creational patterns, which involve only one kind of product object" (pp51).
Supporting Multiple Window Systems.
Just as look-and-feel is different across platforms, so is the method of handling windows. Each platform displays, lays out, handles input to and output from, and layers windows differently.
Problems and Constraints
Solution and Pattern
It is possible to develop "our own abstract and concrete product classes", because "all window systems do generally the same thing" (p. 52). Each window system provides operations for drawing primitive shapes, iconifying/de-iconifying, resizing, and refreshing window contents.
An abstract base codice_4 class can be derived to the different types of existing windows, such as application, iconified, dialog. These classes will contain operations that are associated with windows, such as reshaping, graphically refreshing, etc. Each window contains elements, whose codice_5 functions are called upon by the codice_4's own draw-related functions.
In order to avoid having to create platform-specific Window subclasses for every possible platform, an interface will be used. The codice_4 class will implement a codice_4 implementation (codice_9) abstract class. This class will then in turn be derived into multiple platform-specific implementations, each with platform-specific operations. Hence, only one set of codice_4 classes are needed for each type of codice_4, and only one set of codice_9 classes are needed for each platform (rather than the Cartesian product of all available types and platforms). In addition, adding a new window type does not require any modification of platform implementation, or vice versa.
This is a Bridge pattern. codice_4 and codice_9 are different, but related. codice_4 deals with windowing in the program, and codice_9 deals with windowing on a platform. One of them can change without ever having to modify the other. The Bridge pattern allows these two "separate class hierarchies to work together even as they evolve independently" (p. 54).
User Operations.
All actions the user can take with the document, ranging from entering text, changing formatting, quitting, saving, etc.
Problems and Constraints
Solution and Pattern
Each menu item, rather than being instantiated with a list of parameters, is instead done with a "Command" object.
Command is an abstract object that only has a single abstract codice_17 method. Derivative objects extend the codice_17 method appropriately (i.e., the codice_19 would utilize the content's clipboard buffer). These objects can be used by widgets or buttons just as easily as they can be used by menu items.
To support undo and redo, codice_20 is also given codice_21 and codice_22. In derivative classes, the former contains code that will undo that command, and the latter returns a boolean value that defines if the command is undoable. codice_22 allows some commands to be non-undoable, such as a Save command.
All executed codice_24 are kept in a list with a method of keeping a "present" marker directly after the most recently executed command. A request to undo will call the codice_25 directly before "present", then move "present" back one command. Conversely, a codice_26 request will call codice_27 after "present", and move "present" forward one.
This codice_20 approach is an implementation of the Command pattern. It encapsulates requests in objects, and uses a common interface to access those requests. Thus, the client can handle different requests, and commands can be scattered throughout the application.
Spell Check and Hyphenation.
This is the document editor's ability to textually analyze the contents of a document. Although there are many analyses that can be performed, spell check and hyphenation-formatting are the focus.
Problems and Constraints
Solution and Pattern
Removing the integer-based index from the basic element allows for a different iteration interface to be implemented. This will require extra methods for traversal and object retrieval. These methods are put into an abstract codice_29 interface. Each element then implements a derivation of the codice_29, depending on how that element keeps its list (codice_31, codice_32, etc.).
Functions for traversal and retrieval are put into the abstract Iterator interface. Future Iterators can be derived based on the type of list they will be iterating through, such as Arrays or Linked Lists. Thus, no matter what type of indexing method any implementation of the element uses, it will have the appropriate Iterator.
This is an implementation of the Iterator pattern. It allows the client to traverse through any object collection, without needing to access the contents of the collection directly, or be concerned about the type of list the collection's structure uses.
Now that traversal has been handled, it is possible to analyze the elements of a structure. It is not feasible to build each type of analysis into the element structure themselves; every element would need to be coded, and much of the code would be the same for similar elements.
Instead, a generic codice_33 method is built into the element's abstract class. Each Iterator is given a reference to a specific algorithm (such as spell check, grammar check, etc.). When that Iterator iterates through its collection, it calls each element's codice_34, passing the specified algorithm. codice_34 then passes a reference to its element back to said algorithm for analysis.
Thus, to perform a spell check, a front-to-end iterator would be given a reference to a codice_36 object. The iterator would then access each element, executing its codice_33 method with the codice_36 parameter. Each codice_34 would then call the codice_36, passing a reference to the appropriate element.
In this manner, any algorithm can be used with any traversal method, without hard-code coupling one with the other. For example, Find can be used as "find next" or "find previous", depending on if a "forward" iterator was used, or a "backwards" iterator.
In addition, the algorithm themselves can be responsible for dealing with different elements. For example, a codice_36 algorithm would ignore a codice_42 element, rather than having to program every codice_42-derived element to not send themselves to a codice_36.
Patterns by Type.
Creational.
Creational patterns are ones that create objects for you, rather than having you instantiate objects directly. This gives your program more flexibility in deciding which objects need to be created for a given case.
Structural.
These concern class and object composition. They use inheritance to compose interfaces and define ways to compose objects to obtain new functionality.
Behavioral.
Most of these design patterns are specifically concerned with communication between objects.
Criticism.
Significant criticism has been directed at the concept of software design patterns generally, and at "Design Patterns" specifically.
A primary criticism of "Design Patterns" is that its patterns are simply workarounds for missing features in C++, replacing elegant abstract features with lengthy concrete patterns, essentially becoming a "human compiler" or "generating by hand the expansions of some macro". Peter Norvig demonstrates that 16 out of the 23 patterns in "Design Patterns" are simplified or eliminated (via direct language support) in Lisp or Dylan. Related observations were made by Hannemann and Kiczales who implemented several of the 23 design patterns using an aspect-oriented programming language (AspectJ) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns.
There has also been humorous criticism, such as a show trial at OOPSLA '99 on 3 November 1999, and a parody of the format, by Jim Coplien, entitled "".

</doc>
<doc id="40396" url="http://en.wikipedia.org/wiki?curid=40396" title="Montana-class battleship">
Montana-class battleship

The "Montana"-class battleships of the United States Navy were planned as successors to the "Iowa" class, being slower but larger, better armored, and having superior firepower. Five were approved for construction during World War II, but changes in wartime building priorities resulted in their cancellation in favor of the "Essex"-class aircraft carriers and "Iowa" class before any "Montana"-class keels were laid. With beams of 121 feet, they would have been the first U.S. battleships to be too wide to transit the 110 foot wide locks of the Panama Canal.
Intended armament would have been 12 16 in Mark 7 guns in four triple turrets, up from the "Iowas' " three triple 16s. With an increased anti-aircraft capability and thicker armor belt, the "Montana"-class would have been the largest, best-protected, and most heavily armed U.S. battleships ever, the only class to come close to rivaling the Empire of Japan's immense "Yamato"-class battleships.
Preliminary design work for the "Montana"-class began before the US entry into World War II. The first two vessels were approved by Congress in 1939 following the passage of the Second Vinson Act. The Japanese attack on Pearl Harbor delayed construction of the "Montana" class. The success of carrier combat at the Battle of the Coral Sea and, to a greater extent, the Battle of Midway, diminished the value of the battleship. Consequently, the US Navy chose to cancel the "Montana"-class in favor of more urgently needed aircraft carriers, amphibious and anti-submarine vessels.
Because the "Iowa"s were fast enough to escort the new "Essex"-class aircraft carriers, their orders were retained, making them the last US Navy battleships to be commissioned.
History.
As the political situation in Europe and Asia worsened in the prelude to World War II, Carl Vinson, the chairman of the House Committee on Naval Affairs, instituted the Vinson Naval Plan, which aimed to get the Navy into fighting shape after the cutbacks imposed by the Great Depression and pair of London Naval Treaties of the 1930s. As part of the overall plan Congress passed the Second Vinson Act in 1938, which cleared the way for construction of the four "South Dakota"-class fast battleships and the first two "Iowa" class fast battleships (hull numbers BB-61 and BB-62). Four additional battleships (with hull numbers BB-63, BB-64, BB-65, and BB-66) were approved for construction in 1940, with the last two intended to be the first ships of the "Montana" class. By 1942, it was apparent to the US Navy high command that they needed as many fast battleships as possible, and hull numbers BB-65 and BB-66 were allocated to planned fast battleships "Illinois" and "Kentucky".
The Navy, mindful of the ongoing construction of Japan's "Yamato"-class battleships, had been working on a 58,000-ton "super battleship" concept since 1938. This new class, with twelve 16 in guns, was assigned the name "Montana" and cleared for construction by the United States Congress under the Two-Ocean Navy Act in 1940; funding for the new ships was approved in 1941. These ships, the last battleships to be ordered by the Navy, were originally to be designated BB-65 through BB-69; however, BB-65 and BB−66 were subsequently re-ordered as "Iowa"-class ships, "Illinois" and "Kentucky", and the "Montana"s were redesignated BB-67 through BB-71.
Completion of the "Montana"-class, and the last two "Iowa"-class battleships, was intended to give the US Navy a considerable advantage over any other nation, or probable combination of nations, with a total of 17 new battleships by the late 1940s. The "Montana"s also would have been the only American ships to come close to equaling Japan's massive "Yamato" and her sister "Musashi" in terms of size and firepower.
Design.
Preliminary planning for the "Montana"-class battleships took place in 1939, when the aircraft carrier was still considered inferior to the battleship. The Navy began designing a 65,000-ton battleship to counter the threat posed by the "Yamato"-class battleships of the Imperial Japanese Navy. Although the Navy knew little about the "Yamato"-class, the new Japanese battleships were rumored to have a main gun battery of 18 in. Initially, plans were drawn for a 45,000-ton (46,000-metric-ton) US battleship, but after evaluation, the Battleship Design Advisory Board increased the displacement of the planned ship to 58,000 tons (59,000 metric tons).
At the time, the design board issued a basic outline for the "Montana" class that called for it to be free of beam restrictions imposed by the Panama Canal, be 25% stronger offensively and defensively than any other battleship completed or under construction, and be capable of withstanding the "super heavy" 2700 lb shells used by US battleships equipped with either the 16-inch/45 caliber guns or 16 in/50 cal Mark 7 guns. Although freed of the restriction of fitting through the Panama Canal, the length and height of the "Montana"-class were limited by one of the shipyards at which they were to be built: the New York Navy Yard could not handle the construction of a 58,000-ton (59,000-metric-ton) ship, and vessels built there had to be low enough to clear the Brooklyn Bridge at low tide.
After debate at the design board about whether the "Montana"-class should be fast, achieving the high 33 kn speed of the "Iowa" class, or up-gunned and up-armored, firepower was selected over speed. By returning the "Montana" class to the slower 28 kn maximum speed of the "North Carolina"- and "South Dakota"- class ships, naval architects were able to increase armor protection for the "Montana"s, enabling the ships to withstand enemy fire equivalent to their own guns' ammunition. This limited the "Montana"s‍ '​ ability to escort and defend the Pacific-based Allied aircraft carrier fleet, as the class was to be powered by eight Babcock & Wilcox boilers which would have enabled them to steam at approximately 27 to.
Fate.
By January 1941, the design limit for the 58,000-ton (59,000-metric-ton) battleship plan had been reached, and consensus among those designing the battleship class was to increase the displacement to support the armor and weaponry on the ships. At the same time, planners decided to adopt a slightly greater length and reduce power for a better machinery arrangement, as well as improving internal subdivisions, and selecting as the secondary armament several dual-mounted 5 in/54 cal guns instead of the 5 in/38 cal guns used on the "Iowa"s. At this point, the net design for the "Montana" class somewhat resembled the "Iowa" class since they would be equipped with the same caliber main guns and similar caliber secondary guns; however, "Montana" and her sisters had more armor, mounted three more main guns, and were 22 ft longer and 13 ft wider than the "Iowa" class.
By April 1942, the "Montana"-class design had been approved; construction was authorized by the United States Congress and the projected date of completion was estimated to be somewhere between 1 July and 1 November 1945. The Navy ordered the ships in May 1942, but the "Montana" class was placed on hold because the "Iowa"-class battleships and the "Essex"-class aircraft carriers were under construction in the shipyards intended to build the "Montana"s. Unfortunately for the "Montana" class, both the "Iowa" and "Essex" classes had been given higher priorities: the "Iowas" as they were fast enough to keep up with and defend the "Essex"-class carriers with 20 mm and 40 mm guns, and the "Essex"es because of their ability to launch aircraft to gain and maintain air supremacy over the islands in the Pacific and intercept warships of the Imperial Japanese Navy. Because of this, the entire "Montana" class was suspended in May 1942, before any of their keels had been laid. In July 1942, the construction of the "Montana" class was canceled following the Battle of Midway, and the corresponding shift in naval warfare from surface engagements to air supremacy, and, thus, a shift from battleships to aircraft carriers. The design for the ship's hull did find life in the Midway-class aircraft carrier. Using the Montana-class's hull design gave the ship enhanced maneuverability over previous air craft carriers. As was expected with the Montana-class if completed, Midway-class aircraft carriers were the first US Navy ships too big to transit the Panama canal.
Ships.
Five ships of the "Montana" class were authorized on 19 July 1940, but they were suspended indefinitely until being canceled on 21 July 1943. The ships were to be built at the New York Navy Yard, Philadelphia Navy Yard, and Norfolk Navy Yard.
USS "Montana" (BB-67).
"Montana" was planned to be the lead ship of the class. She was the third ship to be named in honor of the 41st state, and was assigned to the Philadelphia Navy Yard. Both the earlier battleship, BB-51, and BB-67 were canceled, so Montana is the only one of the (48 at the time) US states never to have had a battleship with a "BB" hull classification completed in its honor.
USS "Ohio" (BB-68).
"Ohio" was to be the second "Montana"-class battleship. She was to be named in honor of the 17th state, and was assigned to the Philadelphia Navy Yard for construction. "Ohio" would have been the fourth ship to bear that name had she been commissioned.
USS "Maine" (BB-69).
"Maine" was to be the third "Montana"-class battleship. She was to be named in honor of the 23rd state, and was assigned to the New York Navy Yard. "Maine" would have been the third ship to bear that name had she been commissioned.
USS "New Hampshire" (BB-70).
"New Hampshire" was to be the fourth "Montana"-class battleship, and was to be named in honor of the ninth state. She was assigned to the New York Navy Yard, and would have been the third ship to bear that name had she been commissioned.
USS "Louisiana" (BB-71).
"Louisiana" was to be the fifth and final "Montana"-class battleship. She was to be named in honor of the 18th state and assigned to the Norfolk Navy Yard, Portsmouth, Virginia. "Louisiana" would have been the third ship to bear that name had she been commissioned. By hull number, "Louisiana" was the last American battleship authorized for construction.
Armament.
The armament of the "Montana"-class battleships would have been similar to the preceding "Iowa"-class battleships, but with an increase in the number of primary and secondary guns for use against enemy surface ships and aircraft. Had they been completed, the "Montana"s would have been gun-for-gun the most powerful battleships the United States had constructed, and the only US battleship class that would have come close to equaling the Imperial Japanese Navy battleships "Yamato" and "Musashi" in armament, armor, and displacement.
Main battery.
The primary armament of a "Montana"-class battleship would have been 12 16"/50 caliber Mark 7 gun, which were to be housed in four three-gun turrets: two forward and two aft. The guns, the same used to arm the "Iowa"-class battleships, were 66 ft long – 50 times their 16 in bore, or 50 calibers, from breechface to muzzle. Each gun weighed about 239000 lb without the breech, or 267900 lb with the breech. They fired projectiles weighing up to 2700 lb at a maximum speed of 2690 ft/s with a range of up to 24 nmi. At maximum range the projectile would have spent almost 1½ minutes in flight. The addition of the No. 4 turret would have allowed "Montana" to overtake the "Yamato" as the battleship having heaviest broadside overall; "Montana" and her sisters would have had a broadside of 32400 lb vs. 28800 lb for "Yamato". Each gun would have rested within an armored barbette, but only the top of the barbette would have protruded above the main deck. The barbettes would have extended either four decks (turrets 1 and 4) or five decks (turrets 2 and 3) down. The lower spaces would have contained rooms for handling the projectiles and storing the powder bags used to fire them. Each turret would have required a crew of 94 men to operate. The turrets would not have been attached to the ship, but would have rested on rollers, which meant that had any of the "Montana"-class ships capsized, the turrets would have fallen out. Each turret would have cost US$1.4 million, but this figure did not take into account the cost of the guns themselves.
The turrets would have been "three-gun", not "triple", because each barrel would have elevated and fired independently. The ships could fire any combination of their guns, including a broadside of all 12. Contrary to popular belief, the ships would not have moved sideways noticeably when a broadside was fired.
The guns would have been elevated from −5° to +45°, moving at up to 12° per second. The turrets would have rotated about 300° at about 4° per second and could even be fired back beyond the beam, which is sometimes called "over the shoulder". Within each turret, a red stripe on the wall of the turret, just inches from the railing, would have marked the boundary of the gun's recoil, providing the crew of each gun turret with a visual reference for the minimum safe distance range.
Like most battleships in World War II, the "Montana" class would have been equipped with a fire control computer, in this case the Ford Mk 1A Ballistic Computer, a 3150 lb rangekeeper designed to direct gunfire on land, sea, and in the air. This analog computer would have been used to direct the fire from the battleship's big guns, taking into account several factors such as the speed of the targeted ship, the time it takes for a projectile to travel, and air resistance to the shells fired at a target. At the time the "Montana" class was set to begin construction, the rangekeepers had gained the ability to use radar data to help target enemy ships and land-based targets. The results of this advance were telling: the rangekeeper was able to track and fire at targets at a greater range and with increased accuracy, as was demonstrated in November 1942 when the battleship "Washington" engaged the Imperial Japanese Navy battleship "Kirishima" at a range of 18500 yd at night; the "Washington" scored at least nine heavy caliber hits that critically damaged the "Kirishima" and led to her loss. This gave the US Navy a major advantage in World War II, as the Japanese did not develop radar or automated fire control to the level of the US Navy.
"When you're penetrating armor, there is a thing called frontal density – it's not just the weight of the shell, it's the weight of the shell trying to punch a hole through [the armor]. Well, the 16"/50 heavy shell was almost as good an armor penetrator as the Japanese 18.1" shell."
The large caliber guns were designed to fire two different 16 in shells: an armor piercing round for anti-ship and anti-structure work, and a high explosive round designed for use against unarmored targets and shore bombardment.
The Mk. 8 APC (Armor-Piercing, Capped) shell weighed in at 2700 lb, and was designed to penetrate the hardened steel armor carried by foreign battleships. At 20000 yd, the Mk. 8 could penetrate 20 in of steel armor plate. At the same range, the Mk. 8 could penetrate 21 ft of reinforced concrete. For unarmored targets and shore bombardment, the 1900 lb Mk. 13 HC (High-Capacity—referring to the large bursting charge) shell was available. The Mk. 13 shell could create a crater 50 ft wide and 20 ft deep upon impact and detonation, and could defoliate trees 400 yd from the point of impact.
The final type of ammunition developed for the 16-inch guns, well after the "Montanas" had been cancelled, were W23 "Katie" shells. These were born from the nuclear deterrence that had begun to shape the US armed forces at the start of the Cold War. To compete with the Air Force and the Army, which had developed nuclear bombs and nuclear shells for use on the battlefield, the Navy began a top-secret program to develop Mk. 23 nuclear naval shells with an estimated yield of 15 to 20 kilotons. The shells entered development around 1953, and were reportedly ready by 1956; however, only the "Iowa"-class battleships could have fired them.
Secondary battery.
The secondary armament for "Montana" and her sisters was to be 20 5 in/54 cal guns housed in 10 turrets along the island of the battleship; five on the starboard side and five on the port. These guns, designed specifically for the "Montana"s, were to be the replacement for the 5 in/38 cal secondary gun batteries then in widespread use with the US Navy.
The 5 in/54 cal gun turrets were similar to the 5 in/38 cal gun mounts in that they were equally adept in an anti-aircraft role and for damaging smaller ships, but differed in that they weighed more, fired heavier rounds of ammunition, and resulted in faster crew fatigue than the 5 in/38 cal guns. The ammunition storage for the 5 in/54 cal gun was 500 rounds per turret, and the guns could fire at targets nearly 26000 yd away at a 45° angle. At an 85° angle, the guns could hit an aerial target at over 50000 ft.
The cancellation of the "Montana"-class battleships in 1943 pushed back the combat debut of the 5 in/54 cal guns to 1945, when they were used aboard the US Navy's "Midway"-class aircraft carriers. The guns proved adequate for the carrier's air defense, but were gradually phased out of use by the carrier fleet because of their weight. (Rather than having the carrier defend itself by gunnery this would be assigned to other surrounding ships within a carrier battle group.)
Anti-aircraft batteries.
For the first time since the construction of the "Iowa"-class, the US Navy was not building a fast battleship class solely for the purpose of escorting Pacific-based aircraft carriers, and thus the "Montana"-class would not be designed principally for escorting the fast carrier task forces; nonetheless they would have been equipped with a wide array of anti-aircraft guns to protect themselves and other ships (principally the US aircraft carriers) from Japanese fighters and dive bombers.
Oerlikon 20 mm anti-aircraft guns.
The Oerlikon 20 mm anti-aircraft cannon was one of the most heavily produced anti-aircraft guns of World War II; the US alone manufactured a total of 124,735 of these guns. When activated in 1941, these guns replaced the .50 in (12.7 mm)/90 cal M2 Browning MG on a one-for-one basis. The Oerlikon 20 mm AA gun remained the primary anti-aircraft weapon of the United States Navy until the introduction of the 40 mm Bofors AA gun in 1943.
These guns are air-cooled and use a gas blow-back recoil system. Unlike other automatic guns employed during World War II, the barrel of the 20 mm Oerlikon gun does not recoil; the breechblock is never locked against the breech and is actually moving forward when the gun fires. This weapon lacks a counter-recoil brake, as the force of the counter-recoil is checked by recoil from the firing of the next round of ammunition.
Between December 1941 and September 1944, 32% of all Japanese aircraft downed were credited to this weapon, with the high point being 48.3% for the second half of 1942. In 1943, the revolutionary Mark 14 gunsight was introduced, which made these guns even more effective. The 20 mm guns, however, were found to be ineffective against the Japanese kamikaze attacks used during the latter half of World War II. They were subsequently phased out in favor of the heavier 40 mm Bofors AA guns.
Bofors 40 mm anti-aircraft guns.
Arguably the best light anti-aircraft weapon of World War II, the Bofors 40 mm anti-aircraft gun was used on almost every major warship in the US and UK fleet from about 1943 to 1945. Although a descendant of German, Dutch, and Swedish designs, the Bofors mounts used by the US Navy during World War II had been heavily "Americanized" to bring the guns up to the standards placed on them by the Navy. This resulted in a guns system set to British standards (now known as the Standard System) with interchangeable ammunition, which simplified the logistics situation for World War II. When coupled with hydraulic couple drives to reduce salt contamination and the Mark 51 director for improved accuracy, the Bofors 40 mm gun became a fearsome adversary, accounting for roughly half of all Japanese aircraft shot down between 1 October 1944 and 1 February 1945.
Armor.
Aside from its firepower, a battleship's defining feature is its armor. The exact design and placement of the armor, inextricably linked with the ship's stability and performance, is a complex science honed over decades.
A battleship is usually armored to withstand an attack from guns the size of its own, but the armor scheme of the preceding "North Carolina" class was only proof against 14 in shells (which they had originally been intended to carry), while the "South Dakota"-class battleship (1939) and "Iowa"-class battleship classes were designed only to resist their original complement of Mark V 2240 lb shells, not the new "super-heavy" 2700 lb APC (Armor Piercing, Capped) Mark7 VIII shells they actually used. The "Montana"s were the only US battleships designed to resist the Mark VIII.
Until the authorization of the "Montana" class all US battleships were built within the size limits for the Panama Canal. The main reason for this was logistical: the largest US shipyards were located on the East Coast of the United States, while the United States had territorial interests in both oceans. Requiring the battleships to fit within the Panama Canal took days off the transition time from the Atlantic Ocean to the Pacific Ocean by allowing ships to move through the canal instead of sailing around South America. By the time of the Two Ocean Navy bill, the Navy realized that ship designs could no longer be limited by the Panama Canal and thus approved the "Montana" class knowing that the ships would be unable to clear the locks. This shift in policy meant that the "Montana" class would have been the only World War II–era US battleships to be adequately armored against guns of the same power as their own.
Aircraft.
The "Montana"-class would have used aircraft for reconnaissance and for gunnery spotting. The type of aircraft used would have depended on when exactly the battleships would have been commissioned, but in all probability they would have used either the Kingfisher or the Seahawk. The aircraft would have been floatplanes launched from catapults on the ship's fantail. They would have landed on the water and taxied to the stern of the ship to be lifted by a crane back to the catapult.
Kingfisher.
The Vought OS2U Kingfisher was a lightly armed two-man aircraft designed in 1937. The Kingfisher's high operating ceiling made it well-suited for its primary mission: to observe the fall of shot from a battleship's guns and radio corrections back to the ship. The floatplanes used in World War II also performed search and rescue for naval aviators who were shot down or forced to ditch in the ocean.
Seahawk.
In June 1942, the US Navy Bureau of Aeronautics requested industry proposals for a new seaplane to replace the Kingfisher and Curtiss SO3C Seamew. The new aircraft was required to be able to use landing gear as well as floats. Curtiss submitted a design on 1 August, and received a contract for two prototypes and five service-test aircraft on 25 August. The first flight of a prototype XSC-1 took place on 16 February 1944 at the Columbus, Ohio Curtiss plant. The first production aircraft were delivered in October 1944, and by the beginning of 1945 the single-seat Curtiss SC Seahawk floatplane began replacing the Kingfisher. Had the "Montana"-class been completed, they would have arrived around the time of this replacement, and would likely have been equipped with the Seahawk for use in combat operations and seaborne search and rescue.
References.
</dl>
Further reading.
</dl>

</doc>
<doc id="40400" url="http://en.wikipedia.org/wiki?curid=40400" title="James A. Garfield">
James A. Garfield

James Abram Garfield (November 19, 1831 – September 19, 1881) was the 20th President of the United States, serving from March 4, 1881, until his assassination later that year. Garfield had served nine terms in the House of Representatives, and had been elected to the Senate before his candidacy for the White House, though he declined the senatorship once he was president-elect.
Garfield was raised in humble circumstances on an Ohio farm by his widowed mother. He worked at various jobs, including on a canal boat, in his youth. Beginning at age 17, he attended several Ohio schools, then studied at Williams College in Williamstown, Massachusetts, from which he graduated in 1856. A year later, Garfield entered politics as a Republican. He married Lucretia Rudolph in 1858, and served as a member of the Ohio State Senate (1859–1861). Garfield opposed Confederate secession, served as a major general in the Union Army during the American Civil War, and fought in the battles of Middle Creek, Shiloh, and Chickamauga. He was first elected to Congress in 1862 to represent Ohio's 19th District. Throughout Garfield's extended congressional service after the Civil War, he firmly supported the gold standard and gained a reputation as a skilled orator. Garfield initially agreed with Radical Republican views regarding Reconstruction, but later favored a moderate approach for civil rights enforcement for freedmen.
At the 1880 Republican National Convention, Senator-elect Garfield attended as campaign manager for Secretary of the Treasury John Sherman, and gave the presidential nomination speech for him. When neither Sherman nor his rivals – Ulysses S. Grant and James G. Blaine – could get enough votes to secure the nomination, delegates chose Garfield as a compromise on the 36th ballot. In the 1880 presidential election, Garfield conducted a low-key front porch campaign, and narrowly defeated Democrat Winfield Scott Hancock.
Garfield's accomplishments as president included a resurgence of presidential authority against senatorial courtesy in executive appointments, energizing American naval power, and purging corruption in the Post Office, all during his extremely short time in office. Garfield made notable diplomatic and judiciary appointments, including a U.S. Supreme Court justice. He enhanced the powers of the presidency when he defied the powerful New York senator Roscoe Conkling by appointing William H. Robertson to the lucrative post of Collector of the Port of New York, starting a fracas that ended with Robertson's confirmation and Conkling's resignation from the Senate. Garfield advocated agricultural technology, an educated electorate, and civil rights for African-Americans. He proposed substantial civil service reform, eventually passed by Congress in 1883 and signed into law by his successor, Chester A. Arthur, as the Pendleton Civil Service Reform Act. Despite his strong performance, Garfield is little-remembered but for his assassination.
Childhood.
James Garfield was born the youngest of five children on November 19, 1831, in a log cabin in Orange Township, now Moreland Hills, Ohio. Orange Township was located in the Western Reserve, and like many who settled there, Garfield's ancestors were from New England. James' father Abram had been born in Worcester, New York and came to Ohio to woo his childhood sweetheart, Mehitabel Ballou, only to find her married. He instead wed her sister Eliza, who had been born in New Hampshire. James was named for an older brother, dead in infancy.
In early 1833, Abram and Eliza Garfield joined the Disciples of Christ, a decision that would help shape their youngest son's life. Abram Garfield died later that year; his son was raised in poverty in a household led by the strong-willed Eliza. James was her favorite child, and the two remained close for the rest of her life. Eliza Garfield remarried in 1842, but soon left her second husband, Warren Belden (possibly Alfred Belden), and a then-scandalous divorce was awarded against her in 1850. James took his mother's side, and when Belden died in 1880 noted the fact in his diary with satisfaction. Garfield enjoyed his mother's stories about his ancestry, especially his Welsh great-great-grandfathers and his ancestor who served as a knight of Gaerfili Castle.
Poor and fatherless, Garfield was mocked by his fellow boys, and throughout his life was very sensitive to slights. He escaped through reading, devouring all the books he could find. He left home at age 16 in 1847. Rejected by the only ship in port in Cleveland, Garfield instead found work on a canal boat, responsible for managing the mules that pulled it. This labor would be used to good effect by Horatio Alger, who penned Garfield's campaign biography in 1880.
After six weeks, illness forced Garfield to return home and, during his recuperation, his mother and a local education official got him to promise to postpone his return to the canals for a year and go to school. Accordingly, in 1848, he began at Geauga Seminary, in nearby Chester Township. Garfield later said of his childhood, "I lament that I was born to poverty, and in this chaos of childhood, seventeen years passed before I caught any inspiration ... a precious 17 years when a boy with a father and some wealth might have become fixed in manly ways."
Education, marriage and early career.
At Geauga Academy, which he attended from 1848 to 1850, Garfield learned academic subjects he had not previously had time for. He shone as a student, and was especially interested in languages and elocution. He began to appreciate the power a speaker had over an audience, writing that the speaker's platform "creates some excitement. I love agitation and investigation and glory in defending unpopular truth against popular error." Geauga was co-educational, and Garfield was attracted to one of his fellow students, Lucretia Rudolph, whom he later married. To support himself at Geauga, he worked as a carpenter's assistant and as a teacher. The need to go from town to town to find a place as a teacher disgusted Garfield, and he thereafter developed a dislike of what he called "place-seeking", which became, he said, "the law of my life". In later years, he would astound his friends by letting positions pass that could have been his with a little politicking. Garfield had attended church more to please his mother than to worship God, but in his late teens underwent a religious awakening, and attended many camp meetings, at one of which he was born again. The next day, March 4, 1850, he was baptized into the Disciples by being submerged in the icy waters of the Chagrin River.
After leaving Geauga, Garfield worked for a year at various jobs, including teaching. Finding that some New Englanders worked their way through college, Garfield determined to do the same, and first sought a school that could prepare him for the entrance examinations. From 1851 to 1854, he attended the Western Reserve Eclectic Institute (later named Hiram College) in Hiram, Ohio, a school run by the Disciples. While there, he was most interested in the study of Greek and Latin, but was inclined to learn about and discuss any new thing he encountered. Securing a position on entry as janitor, he was hired to teach while still a student. Lucretia Rudolph had also enrolled at the Institute, and Garfield wooed her while teaching her Greek. He developed a regular preaching circuit at neighboring churches, in some cases earning a gold dollar per service. By 1854, Garfield had learned all the Institute could teach him and was a full-time teacher. Garfield then enrolled at Williams College in Williamstown, Massachusetts, as a third-year student, given credit for two year's study at the Institute after passing a cursory examination. Garfield was impressed with the college president, Mark Hopkins, who had responded warmly to Garfield's letter inquiring about admission. He said of Hopkins, "The ideal college is Mark Hopkins on one end of a log with a student on the other." Hopkins later stated about Garfield in his student days, "There was a large general capacity applicable to any subject. There was no pretense of genius, or alternation of spasmodic effort, but a satisfactory accomplishment in all directions." After his first term, Garfield was hired to teach penmanship to the students of nearby Pownal, Vermont—a post whose previous incumbent was Chester A. Arthur.
Garfield graduated from Williams in August 1856 as salutatorian, giving an address at the commencement. Garfield biographer Ira Rutkow pointed out that the future president's years at Williams gave Garfield the opportunity to know and respect those of different social backgrounds, and despite his origin as an unsophisticated Westerner, he was liked and respected by socially conscious New Englanders. "In short," as Rutkow later wrote, "Garfield had an extensive and positive first experience with the world outside the Western Reserve of Ohio."
On his return to Ohio, the degree from a prestigious Eastern school made Garfield a man of distinction. He returned to Hiram to teach at the Institute, and in 1857 was made its president. He did not see education as a field in which he could realize his full potential. At Williams, he had become more politically aware in the intensely anti-slavery atmosphere of the Massachusetts school, and began to consider politics as a career. In 1858, he married Lucretia; they would have seven children, five of whom survived infancy. Soon after the wedding, he formally entered his name to read law at a Cleveland firm, although he did his studying in Hiram. He was admitted to the bar in 1861.
Local Republican Party leaders invited Garfield to enter politics upon the death of Cyrus Prentiss, the presumptive nominee for the local state senate seat. He was nominated by the party convention on the sixth ballot, and was elected, serving until 1861. Garfield's major effort in the state senate was a bill providing for Ohio's first geological survey to measure its mineral resources, though it failed.
Civil War.
After Abraham Lincoln's election as president, several Southern states announced their secession from the Union to form a new government, the Confederate States of America. Garfield read military texts while anxiously awaiting the war effort, which he regarded as a holy crusade against the Slave Power. In April 1861, the rebels bombarded Fort Sumter, one of the last federal outposts in the South, beginning the Civil War. Although he had no military training, Garfield knew that his place was in the Union Army.
At Governor William Dennison's request, Garfield deferred his military ambitions to remain in the legislature, where he helped appropriate the funds to raise and equip Ohio's volunteer regiments. Afterward, the legislature adjourned and Garfield spent the spring and early summer on a speaking tour of northeastern Ohio, encouraging enlistment in the new regiments. Following a trip to Illinois to purchase muskets, Garfield returned to Ohio and, in August 1861, received a commission as a colonel in the 42nd Ohio Infantry regiment. The 42nd Ohio existed only on paper, so Garfield's first task was to fill its ranks. He did so quickly, recruiting many of his neighbors and former students. The regiment traveled to Camp Chase, outside Columbus, Ohio, to complete training. In December, Garfield was ordered to bring the 42nd to Kentucky, where they joined the Army of the Ohio under Brigadier General Don Carlos Buell.
Buell's command.
Buell quickly assigned Garfield the task of driving Confederate forces out of eastern Kentucky, giving him the 18th Brigade for the campaign which, besides his own 42nd, included the 40th Ohio Infantry, two Kentucky infantry regiments and two cavalry units. They departed Catlettsburg, Kentucky, in mid-December, advancing through the valley of the Big Sandy River. The march was uneventful until Union forces reached Paintsville, Kentucky, on January 6, 1862, where Garfield's cavalry engaged the rebels at Jenny's Creek. Confederate troops under Brigadier General Humphrey Marshall held the town in numbers roughly equal to Garfield's own, but Garfield positioned his troops so as to deceive Marshall into believing that rebel forces were outnumbered. Marshall ordered his troops to withdraw to the forks of Middle Creek, on the road to Virginia; Garfield ordered his troops to pursue the Confederates. They attacked the rebel positions on January 9, 1862, in the Battle of Middle Creek, the only pitched battle Garfield personally commanded. At the end of the fighting, the Confederates withdrew from the field, and Garfield sent his troops to Prestonsburg to reprovision.
In recognition of his success, Garfield was promoted to brigadier general. After Marshall's retreat, Garfield's was the sole remaining army in eastern Kentucky, and he announced that any men who had fought for the Confederacy would be granted amnesty if they returned to their homes and lived peaceably and remained loyal to the Union. The proclamation was surprisingly lenient, as Garfield now believed the war was a crusade for eradication of slavery. Following a brief skirmish at Pound Gap, the last rebel units in the area were outflanked, and they retreated to Virginia.
Garfield's promotion gave him command of the 20th Brigade of the Army of the Ohio, which was ordered in early 1862 to join Major General Ulysses S. Grant's forces as they advanced on Corinth, Mississippi. Before the 20th Brigade arrived, however, Confederate forces under General Albert Sidney Johnston surprised Grant's men in their camps, driving them back. Garfield's troops got word of the battle and advanced quickly, joining the rest of the army on the second day to drive the Confederates back across the field and into retreat. The action, later known as the Battle of Shiloh, was the bloodiest of the war to date; Garfield was exposed to fire for much of the day, but emerged uninjured. Major General Henry W. Halleck, Grant's superior, took charge of the combined armies and advanced ponderously toward Corinth; when they arrived, the Confederates had fled.
That summer Garfield suffered from jaundice and significant weight loss. He was forced to return home, where his wife nursed him back to health. While he was home, Garfield's friends worked to gain him the Republican nomination for Congress, although he refused to politick with the delegates. He returned to duty that autumn and went to Washington to await his next assignment. During this period of idleness, a rumor of an extra-marital affair caused friction in the Garfield marriage until Lucretia eventually chose to overlook it. Garfield repeatedly received tentative assignments that were quickly withdrawn, to his frustration. In the meantime, he served on the court-martial of Fitz John Porter, a sensational trial in which Porter was tried for his tardiness at the Second Battle of Bull Run. He was convinced of Porter's guilt, and voted with his fellow generals to convict. The trial lasted almost two months, from November 1862 to January 1863, and by the end of it, Garfield had at last procured an assignment as Chief of Staff to Major General William S. Rosecrans.
Chief of staff for Rosecrans.
The position of Chief of Staff for a general was usually held by a more junior officer, but Garfield's influence with Rosecrans was greater than usual, with duties extending beyond mere communication of orders to actual management of his Army of the Cumberland. Rosecrans had a voracious appetite for conversation, especially when he was unable to sleep; in Garfield, he found "the first well read person in the Army" and the ideal candidate for discussions that ran deep into the night. The two became close, and their talks covered all topics, especially religion; Rosecrans, who had converted from Methodism to Roman Catholicism, succeeded in softening Garfield's view of his faith. Garfield recommended that Rosecrans replace wing commanders Alexander McCook and Thomas Crittenden, whom he believed ineffective; Rosecrans ignored the suggestions. With Rosecrans, Garfield devised the Tullahoma Campaign to pursue and trap Confederate General Braxton Bragg in Tullahoma. After initial Union success, Bragg retreated toward Chattanooga, where Rosecrans stalled and requested more troops and supplies. Garfield argued for an immediate advance, in line with demands from Halleck and Lincoln. After a council of war and lengthy deliberations, Rosecrans agreed to attack.
At the ensuing Battle of Chickamauga on September 19 and 20, 1863, confusion among the wing commanders over Rosecrans's orders created a gap in the lines, resulting in a rout of the right flank. Rosecrans concluded that the battle was lost and fell back on Chattanooga to establish a defensive line. Garfield, however, thought that part of the army had held and, with Rosecrans's approval, headed across Missionary Ridge to survey the scene. Garfield's hunch was correct. His ride became legendary, while Rosecrans' error reignited criticism about his leadership. While Rosecrans's army had avoided disaster, they were stranded in Chattanooga, surrounded by Bragg's army. Garfield sent a telegram to Secretary of War Edwin M. Stanton alerting Washington to the need for reinforcements to avoid annihilation, and Lincoln and Halleck delivered 20,000 troops by rail within nine days. In the meantime, Grant was promoted to command of the western armies, and quickly replaced Rosecrans with George H. Thomas. Garfield was ordered to report to Washington, where he was promoted to major general, a commission he would resign before taking a seat in the House of Representatives. According to historian Jean Edward Smith, Grant and Garfield had a "guarded relationship", since Grant promoted Thomas to command of the Army of the Cumberland, rather than Garfield, after Rosecrans was dismissed.
Congressional career.
Election in 1862; Civil War years.
While serving in the army in early 1862, Garfield was approached by friends about running for Congress from Ohio's newly redrawn, heavily Republican 19th district. He was worried that he and other state-appointed generals would get obscure assignments, and running for Congress would allow him to resume his political career. The fact that the new Congress would not hold its first regular session until December 1863 would allow him to continue his war service for a time. Home on medical leave, he refused to campaign for the nomination, leaving that to political managers who secured it at the local convention in September 1862, on the eighth ballot. In October, he defeated D.B. Woods by a two-to-one margin in the general election for a seat in the 38th Congress.
Soon after the nomination, Garfield was ordered to report to War Secretary Edwin Stanton in Washington to discuss his military future. There, Garfield met Treasury Secretary Salmon P. Chase, who befriended him, seeing him as a younger version of himself. The two men agreed politically, and both were part of the Radical wing of the Republican Party. Once he took his seat in December 1863, Garfield was frustrated that Lincoln seemed reluctant to press the South hard. Many radicals, led in the House by Pennsylvania's Thaddeus Stevens, wanted lands owned by rebels to be confiscated, but Lincoln threatened to veto any bill that would do that on a widespread basis. Garfield, in debate on the House floor, supported such legislation and, discussing England's Glorious Revolution, hinted that Lincoln might be thrown out of office for resisting the bills. Although Garfield had supported Lincoln's Emancipation Proclamation, the congressman marveled that it was a "strange phenomenon in the world's history, when a second-rate Illinois lawyer is the instrument to utter words which shall form an epoch memorable in all future ages".
Garfield not only favored abolition, but believed that the leaders of the rebellion had forfeited their constitutional rights. He supported the confiscation of southern plantations and even exile or execution of rebellion leaders as a means to ensure the permanent destruction of slavery. He felt Congress was obliged "to determine what legislation is necessary to secure equal justice to all loyal persons, without regard to color." Garfield was more supportive when Lincoln took action against slavery. Early in his tenure, he differed from his party on several issues; his was the solitary Republican vote to terminate the use of bounties in recruiting. Some financially able recruits had used the bounty system to buy their way out of service (called commutation), which Garfield considered reprehensible. Garfield gave a speech pointing out the flaws in the existing conscription law: that of 300,000 called upon to enlist, barely 10,000 had, the remainder claiming exemption or providing money or a substitute. Lincoln appeared before the Military Affairs committee on which Garfield served, demanding a more effective bill; even if it cost him re-election, Lincoln was confident he could win the war before his term expired. After many false starts, Garfield, with the support of Lincoln, procured the passage of a conscription bill which excluded commutation.
Under Chase's influence, Garfield became a staunch proponent of a dollar backed by a gold standard, and was therefore a strong opponent of the "greenback"; he regretted very much, but understood, the necessity for suspension of payment in gold or silver during the emergency presented by the Civil War. Garfield voted with the Radical Republicans in passing the Wade–Davis Bill, designed to give Congress more authority over Reconstruction, but it was defeated by Lincoln's pocket veto.
Garfield did not consider Lincoln particularly worthy of re-election, but no viable alternative seemed available. "He will probably be the man, though I think we could do better." The Ohioan attended the party convention and promoted Rosecrans as Lincoln's running mate, but delegates chose Military Governor of Tennessee Andrew Johnson. Both Lincoln and Garfield were re-elected. By then, Chase had left the Cabinet and had been appointed Chief Justice, and his relations with Garfield became more distant.
Garfield took up the practice of law in 1865 as a means to improve his personal finances. His efforts took him to Wall Street where, the day after Lincoln's assassination, a riotous crowd led him into an impromptu speech to calm it: "Fellow citizens! Clouds and darkness are round about Him! His pavilion is dark waters and thick clouds of the skies! Justice and judgment are the establishment of His throne! Mercy and truth shall go before His face! Fellow citizens! God reigns, and the Government at Washington still lives!" The speech, with no mention or praise of Lincoln, was according to Garfield biographer Robert G. Caldwell "quite as significant for what it did not contain as for what it did". In the following years, Garfield had more praise for Lincoln; a year after the Illinoisan's death Garfield stated that "greatest among all these developments were the character and fame of Abraham Lincoln", and in 1878 called Lincoln "one of the few great rulers whose wisdom increased with his power".
Reconstruction.
After the war, Garfield became a proponent of black suffrage, though he admitted that the idea of African Americans as political equals with whites gave him "a strong feeling of repugnance". The new president, Johnson, sought the rapid restoration of the Southern states during the months between his accession and the meeting of Congress in December 1865; Garfield hesitantly supported this policy as an experiment. Johnson, an old friend, sought Garfield's backing, and their conversations led Garfield to assume that differences between president and Congress were not large. When Congress assembled in December (to Johnson's chagrin without the elected representatives of the Southern states, who were excluded), Garfield urged conciliation on his colleagues, although he feared that Johnson, a former Democrat, might combine with other Democrats to gain political control if he rejoined the party. Garfield foresaw conflict even before February 1866 when Johnson vetoed a bill to extend the life of the Freedmen's Bureau, charged with aiding the former slaves. By April, Garfield had concluded that Johnson was either "crazy or drunk with opium".
The conflict between the branches of government was the major issue of the 1866 campaign, with Johnson taking to the campaign trail in a Swing Around the Circle and Garfield facing opposition within his party in his home district. With the South still disenfranchised and Northern public opinion behind them, the Republicans gained a two-thirds majority in both houses of Congress. Garfield, having overcome his challengers at his district nominating convention, was easily re-elected.
Garfield opposed the initial talk of impeaching President Johnson when Congress convened in December 1866. However, he supported legislation to limit Johnson's powers, such as the Tenure of Office Act, which restricted Johnson in removing presidential appointees. Distracted by committee duties, he rarely spoke in connection with these bills, but was a loyal Republican vote against Johnson. Due to a court case, he was absent on the day in April 1868 when the House impeached Johnson, but soon gave a speech aligning himself with Thaddeus Stevens and others who sought Johnson's removal. When the president was acquitted in trial before the Senate, Garfield was shocked, and blamed the outcome of the trial on its presiding officer, Chief Justice Chase, his onetime mentor.
By the time Ulysses S. Grant succeeded Johnson in 1869, Garfield had moved away from the remaining radicals (Stevens, their leader, had died in 1868). He hailed the ratification of the 15th Amendment in 1870 as a triumph, and he favored the re-admission of Georgia to the Union as a matter of right, not politics. In 1871, Garfield opposed passage of the Ku Klux Klan Act, saying "I have never been more perplexed by a piece of legislation". He was torn between his indignation at "these terrorists" and his concern for the freedoms endangered by the power the bill gave to the president to enforce the act through suspension of habeas corpus.
Tariffs and finance.
Throughout his political career, Garfield favored the gold standard and decried attempts to increase the money supply through the issuance of paper money not backed by gold, and later, through the free and unlimited coinage of silver. In 1865, Garfield was placed on the House Ways and Means Committee, a long-awaited opportunity to focus on financial and economic issues. He reprised his opposition to the greenback, saying, "any party which commits itself to paper money will go down amid the general disaster, covered with the curses of a ruined people." In 1868 Garfield gave a two-hour speech on currency in the House, which was widely applauded as his best oratory to that point; in it he advocated a gradual resumption of specie payments, that is, the government paying out silver and gold, rather than paper money that could not be redeemed.
Tariffs had been raised to high levels during the Civil War. Afterwards, Garfield, who made a close study of financial affairs, advocated moving towards free trade, though the standard Republican position was a protective tariff that would allow American industries to grow. This break with his party likely cost him his place on the Ways and Means Committee in 1867, and though Republicans held the majority in the House until 1875, Garfield remained off that committee during that time. Garfield came to chair the powerful House Appropriations Committee, but it was Ways and Means, with its influence over fiscal policy, that he really wanted to lead. Part of the reason Garfield was denied a place on Ways and Means was the opposition of the influential Republican editor, Horace Greeley.
In September 1870, Garfield, who was then chairman of the House Banking Committee, led an investigation into the Black Friday Gold Panic scandal. The committee investigation into corruption was thorough, but found no indictable offenses. Garfield blamed the easy availability of fiat money greenbacks for financing the speculation that led to the scandal.
Garfield was not at all enthused about the re-election of President Grant in 1872—until Horace Greeley, who emerged as the candidate of the Democrats and Liberal Republicans, became the only serious alternative. Garfield opined, "I would say Grant was not fit to be nominated and Greeley is not fit to be elected." Both Grant and Garfield won overwhelming re-election victories.
Crédit Mobilier scandal; Salary Grab.
The Crédit Mobilier of America scandal involved corruption in the financing of the Union Pacific Railroad, part of the transcontinental railroad that was completed in 1869. Union Pacific officers and directors secretly purchased control of the Crédit Mobilier of America company, then contracted with the firm to have it undertake the construction of the railroad. The grossly inflated invoices submitted by the company were paid by the railroad, using federal funds appropriated to subsidize the project, and the company was allowed to purchase Union Pacific securities at par value, well below the market rate. Crédit Mobilier showed large profits and stock gains, and distributed substantial dividends. The high expenses meant that Congress was called upon to appropriate more funds. One of the railroad officials that controlled Crédit Mobilier was also a congressman, Oakes Ames of Massachusetts. He offered some of his colleagues the opportunity to buy Crédit Mobilier stock at par value, well below what it sold for on the market, and the railroad got its additional appropriations.
The story broke in July 1872, in the middle of the presidential campaign. Among those named were Vice President (and former House Speaker) Schuyler Colfax, Grant's second-term running mate (Massachusetts Senator Henry Wilson), Speaker James G. Blaine of Maine, and Garfield. Greeley had little luck taking advantage of the scandal. When Congress reconvened after the election, Blaine, seeking to clear his name, demanded a House investigation. Evidence before the special committee exonerated Blaine. Garfield had stated, in September 1872, that Ames had offered him stock, but he had repeatedly refused it. Testifying before the committee in January, Ames alleged that he had offered Garfield ten shares of stock at par value, but that Garfield had never taken the shares, or paid for them. A year had passed, from 1867 to 1868, before Garfield had finally refused it. Garfield, appearing before the committee on January 14, 1873, confirmed much of this. Ames testified several weeks later that Garfield agreed to take the stock on credit, and that it was paid for by the company's huge dividends. The two men differed over a sum of some $300 that Garfield received and later paid back, with Garfield deeming it a loan and Ames a dividend.
Garfield's biographers were unwilling to exonerate him in Crédit Mobilier, with Allan Peskin writing "Did Garfield lie? Not exactly. Did he tell the truth? Not completely. Was he corrupted? Not really. Even Garfield's enemies never claimed that his involvement ... influenced his behavior". Rutkow wrote that "Garfield's real offense was that he knowingly denied to the House investigating committee that he had agreed to accept the stock and that he had also received a dividend of $329." Caldwell suggested that Garfield "while he told the truth [before the committee], certainly failed to tell the whole truth, clearly evading an answer to certain vital questions and thus giving the impression of worse faults than those of which he was guilty". That Crédit Mobilier was a corrupt organization had been a secret badly kept, even mentioned on the floor of Congress, and editor Sam Bowles wrote at the time that Garfield, in his positions on committees dealing with finance, "had no more right to be ignorant in a matter of such grave importance as this, than the sentinel has to snore on his post".
Another issue that caused Garfield trouble in his 1874 re-election bid was the so-called "Salary Grab" of 1873, which increased the compensation for members of Congress by 50 percent, retroactive to 1871. Garfield was responsible, as Appropriations Committee chairman, for shepherding the legislative appropriations bill through the House; during the debate in February 1873, Massachusetts Representative Benjamin Butler offered the increase as an amendment, and despite Garfield's opposition, it passed the House and eventually became law. The law was very popular in the House, as almost half the members were lame ducks, but the public was outraged, and many of Garfield's constituents blamed him, though he refused to accept the increase. In what was a bad year for Republicans, who lost control of the House for the first time since the Civil War, Garfield had his closest congressional election, winning with only 57 percent of the vote.
Minority leader; Hayes presidency.
With the Democratic takeover of the House of Representatives in 1875, Garfield lost his chairmanship of the Appropriations Committee. The Democratic leadership in the House appointed Garfield as a Republican member of Ways and Means. With many of his leadership rivals defeated in the 1874 Democratic landslide, and Blaine elected to the Senate, Garfield was seen as the Republican floor leader and the likely Speaker should the party regain control of the chamber.
As the 1876 presidential election approached, Garfield was loyal to the candidacy of Senator Blaine, and fought for the former Speaker's nomination at the 1876 Republican National Convention in Cincinnati. When it became clear, after six ballots, that Blaine could not prevail, the convention nominated Ohio Governor Rutherford B. Hayes. Although Garfield had supported Blaine, he had kept good relations with Hayes, and wholeheartedly supported the governor. Garfield had hoped to retire from politics after his term expired to devote himself full-time to the practice of law, but to help his party, he sought re-election, and won it easily that October. Any celebration was short lived, as Garfield's youngest son, Neddie, fell ill with whooping cough shortly after the congressional election, and soon died.
When Hayes appeared to have lost the presidential election the following month to Democrat Samuel Tilden, the Republicans launched efforts to reverse the result in Southern states where they held the governorship: South Carolina, Louisiana, and Florida. If Hayes won all three states, he would take the election by a single electoral vote. Grant asked Garfield to serve as a "neutral observer" in the recount in Louisiana. The observers soon recommended to the state electoral commissions that Hayes be declared the winner—Garfield recommended that the entire vote of West Feliciana Parish, which had given Tilden a sizable majority, be thrown out. The Republican governors of the three states certified that Hayes had won their states, to the outrage of Democrats, who had the state legislatures submit rival returns, and threatened to prevent the counting of the electoral vote—under the Constitution, Congress is the final arbiter of the election. Congress then passed a bill establishing the Electoral Commission, to determine the winner. Although he opposed the Commission, feeling that Congress should count the vote and proclaim Hayes victorious, Garfield was appointed to it over the objections of Democrats that he was too partisan. Hayes emerged the victor by a Commission vote of 8 to 7, with all eight votes being cast by Republican politicians or appointees of that party to the Supreme Court. As part of the deal whereby they recognized Hayes as president, Southern Democrats secured the removal of federal troops from the South, ending Reconstruction.
Although a Senate seat would be disposed of by the Ohio General Assembly after the resignation of John Sherman to become Treasury Secretary, Hayes needed Garfield's expertise to protect him from the agenda of a hostile Congress, and asked him not to seek it. Garfield, as the president's key legislator, gained considerable prestige and respect from his role. When Congress debated what became the Bland-Allison Act, to have the government purchase large quantities of silver and strike it into fully legal tender dollar coins, Garfield fought against this deviation from the gold standard, but it was enacted over Hayes's veto in February 1878.
Garfield during this time purchased the property in Mentor that reporters later dubbed Lawnfield, and from which he would conduct the first successful front porch campaign for the presidency. Hayes suggested that Garfield run for governor in 1879, seeing that as a road that would likely put Garfield in the White House. Garfield preferred to seek election as senator, and devoted his efforts to seeing that Republicans won the 1879 election campaign for the General Assembly, with the likely Democratic candidate the incumbent Allen G. Thurman. The Republicans swept the legislative elections. Rivals were spoken of for the seat, such as Secretary Sherman, but he had presidential ambitions (for which he sought Garfield's support), and other candidates fell by the wayside. Garfield was elected to the Senate by the General Assembly in January 1880, though his term was not to begin until March 4, 1881.
Legal career and other activities.
Garfield was one of three attorneys who argued for the petitioners in the landmark Supreme Court case "Ex parte Milligan" in 1866. The petitioners were pro-Confederate northern men who had been found guilty and sentenced to death by a military court for treasonous activities. The case turned on whether the defendants should instead have been tried by a civilian court, and resulted in a ruling that civilians could not be tried before military tribunals while the civil courts were operating. The oral argument was Garfield's first court appearance. Jeremiah Black had taken him in as a junior partner a year before, and assigned the case to him in light of his highly regarded oratory skills. With the result, Garfield instantly achieved a reputation as a preeminent appellate lawyer.
During Grant's first term, discontented with public service, Garfield pursued opportunities in the law, but declined a partnership offer when told his prospective partner was of "intemperate and licentious" reputation. In 1873, after the death of Chase, Garfield appealed to Grant to appoint Justice Noah H. Swayne as Chief Justice. Grant, however, appointed Morrison R. Waite.
Garfield thought the land grants given to expanding railroads to be an unjust practice; as well, he opposed some monopolistic practices by corporations, as well as the power sought by the workers' unions. Garfield supported the proposed establishment of the United States civil service as a means of ridding officials of the annoyance of aggressive office seekers. He especially wished to eliminate the common practice whereby government workers, in exchange for their positions, were forced to kick back a percentage of their wages as political contributions.
In 1876, Garfield displayed his mathematical talent when he developed a trapezoid proof of the Pythagorean theorem. His finding was placed in the "New England Journal of Education". Mathematics historian William Dunham stated that Garfield's trapezoid work was "really a very clever proof".
Presidential election of 1880.
Republican nomination.
Having just been elected to the Senate with Sherman's support, Garfield entered the 1880 campaign season committed to Sherman as his choice for the Republican presidential nominee. Even before the convention began, however, a few Republicans, including Wharton Barker of Philadelphia, thought Garfield the best choice for the nomination. Garfield denied any interest in the position, but the attention was enough to make Sherman suspicious of his lieutenant's ambitions. Besides Sherman, the early favorites for the nomination were Blaine and former president Grant, but several other candidates attracted delegates as well.
As the convention began, Senator Roscoe Conkling of New York, the floor leader for the Grant forces (known as the Stalwart faction), proposed that the delegates pledge to support the eventual nominee in the general election. When three West Virginia delegates declined to be so bound, Conkling sought to expel them from the convention. Garfield rose to defend the men, giving a passionate speech in defense of their right to reserve judgement. The crowd turned against Conkling, and he withdrew the motion. The performance delighted Garfield's boosters, who now believed more than ever that he was the only man who could attract a majority of the delegates' votes.
After speeches in favor of the other front-runners, Garfield rose to place Sherman's name in nomination; his nominating speech was well-received, but the delegates mustered little excitement for the idea of Sherman as the next president. The first ballot showed Grant leading with 304 votes and Blaine in second with 284; Sherman's 93 placed him in a distant third. Subsequent ballots quickly demonstrated a deadlock between the Grant and Blaine forces, with neither having the 379 votes needed for nomination. Jeremiah McLain Rusk, a member of the Wisconsin delegation, and Benjamin Harrison, an Indiana delegate, sought to break the deadlock by shifting a few of the anti-Grant votes to a dark horse candidate—Garfield. Garfield gained 50 votes on the 35th ballot, and the stampede began. Garfield protested to the other members of his Ohio delegation that he had not sought the nomination and had never intended to betray Sherman, but they overruled his objections and cast their ballots for him. In the next round of voting, nearly all of the Sherman and Blaine delegates shifted their support to Garfield, giving him 399 votes and the Republican nomination. Most of the Grant forces backed the former president to the end, creating a disgruntled Stalwart minority in the party. To obtain that faction's support for the ticket, former New York customs collector Chester A. Arthur, a member of Conkling's political machine, was chosen as the vice-presidential nominee and Garfield's running mate.
Campaign against Hancock.
Despite including a Stalwart on the ticket, animosity between the Republican factions carried over from the convention, and Garfield traveled to New York to meet with party leaders there. After convincing the Stalwart crowd to put aside their differences and unite for the coming campaign, Garfield returned to Ohio, leaving the active campaigning to others, as was traditional at the time. Meanwhile, the Democrats settled on their nominee, Major General Winfield Scott Hancock of Pennsylvania, a career military officer. Hancock and the Democrats expected to carry the Solid South, while much of the North was considered safe territory for Garfield and the Republicans; most of the campaign would involve a few close states, including New York and Indiana.
Practical differences between the candidates were few, and Republicans began the campaign with the familiar theme of waving the bloody shirt: reminding Northern voters that the Democratic Party was responsible for secession and four years of civil war, and that if they held power they would reverse the gains of that war, dishonor Union veterans, and pay Confederate veterans pensions out of the federal treasury. With fifteen years having passed since the end of the war, and Union generals at the head of both tickets, the bloody shirt was of diminishing value in exciting the voters. With a few months to go before the election, the Republicans switched tactics to emphasize the tariff. Seizing on the Democratic platform's call for a "tariff for revenue only", Republicans told Northern workers that a Hancock presidency would weaken the tariff protection that kept them in good jobs. Hancock made the situation worse when, attempting to strike a moderate stance, he said "the tariff question is a local question". The ploy proved effective in uniting the North behind Garfield. In the end, fewer than ten thousand votes separated the two candidates, but in the Electoral College, Garfield had an easy victory over Hancock, 214 to 155.
Presidency, 1881.
Cabinet and inauguration.
Between his election and his inauguration, Garfield was occupied with assembling a cabinet that would establish peace between Conkling's and Blaine's warring factions. Blaine's delegates had provided much of the support for Garfield's nomination, and the Maine senator received the place of honor: Secretary of State. Blaine was not only the president's closest advisor, he was obsessed with knowing all that took place in the White House, and was even said to have spies posted there in his absence. Garfield nominated William Windom of Minnesota as Secretary of the Treasury, William H. Hunt of Louisiana as Secretary of the Navy, Robert Todd Lincoln as Secretary of War, and Samuel J. Kirkwood of Iowa as Secretary of the Interior. New York was represented by Thomas Lemuel James as Postmaster General. Garfield appointed Pennsylvania's Wayne MacVeagh, an adversary of Blaine's, as Attorney General. Blaine tried to sabotage the appointment by convincing Garfield to name an opponent of MacVeagh, William E. Chandler, as Solicitor General under MacVeagh. Only Chandler's rejection by the Senate forestalled MacVeagh's resignation over the matter.
Distracted by cabinet maneuvering, Garfield's inaugural address was not up to his typical oratorical standards. In one high point, Garfield emphasized the civil rights of African-Americans, saying "Freedom can never yield its fullness of blessings so long as the law or its administration places the smallest obstacle in the pathway of any virtuous citizen." After discussing the gold standard, the need for education, and an unexpected denunciation of Mormon polygamy, the speech ended. The crowd applauded, but the speech, according to Peskin, "however sincerely intended, betrayed its hasty composition by the flatness of its tone and the conventionality of its subject matter."
Garfield's appointment of James infuriated Conkling, a factional opponent of the Postmaster General, who demanded a compensatory appointment for his faction, such as the position of Secretary of the Treasury. The resulting squabble occupied much of Garfield's brief presidency. The feud with Conkling reached a climax when the president, at Blaine's instigation, nominated Conkling's enemy, Judge William H. Robertson, to be Collector of the Port of New York. This was one of the prize patronage positions below cabinet level, and was then held by Edwin A. Merritt. Conkling raised the time-honored principle of senatorial courtesy in an attempt to defeat the nomination, to no avail. Garfield, who believed the practice to be corrupt, would not back down and threatened to withdraw all nominations unless Robertson was confirmed, intending to "settle the question whether the President is registering clerk of the Senate or the Executive of the United States." Ultimately, Conkling and his New York colleague, Senator Thomas C. Platt, resigned their Senate seats to seek vindication, but found only further humiliation when the New York legislature elected others in their places. Robertson was confirmed as Collector and Garfield's victory was clear. To Blaine's chagrin, the victorious Garfield returned to his goal of balancing the interests of party factions, and nominated a number of Conkling's Stalwart friends to offices.
Reforms.
Grant and Hayes had both advocated civil service reform, and by 1881, civil service reform associations had organized with renewed energy across the nation. Garfield sympathized with them, believing that the spoils system damaged the presidency and distracted from more important concerns. Some reformers were disappointed that Garfield had advocated limited tenure only to minor office seekers and had given appointments to his old friends, but many remained loyal and supported Garfield.
Corruption in the post office also cried out for reform. In April 1880, there had been a congressional investigation into corruption in the Post Office Department, in which profiteering rings allegedly stole millions of dollars, securing bogus mail contracts on star routes. After obtaining contracts with the lowest bid, costs to run the mail routes would be escalated and profits would be divided among ring members. That year, Hayes stopped the implementation of any new star route contracts. Shortly after taking office, Garfield received information from Attorney General MacVeagh and Postmaster General James of postal corruption by an alleged star route ringleader, Second Assistant Postmaster-General Thomas J. Brady. Garfield demanded Brady's resignation and ordered prosecutions that would end in trials for conspiracy. When told that his party, including his own campaign manager, Stephen W. Dorsey, was involved, Garfield directed MacVeagh and James to root out the corruption in the Post Office Department "to the bone", regardless of where it might lead. Brady resigned and was eventually indicted for conspiracy. After two "star route" ring trials in 1882 and 1883, the jury found Brady not guilty.
Civil rights and education.
Garfield believed that the key to improving the state of African American civil rights would be found in education aided by the federal government. During Reconstruction, freedmen had gained citizenship and suffrage that enabled them to participate in government, but Garfield believed their rights were being eroded by Southern white resistance and illiteracy, and was concerned that blacks would become America's permanent "peasantry". His answer was to propose a "universal" education system funded by the federal government. Congress and the northern white public, however, had lost interest in African-American rights, and federal funding for universal education did not find support in Congress during Garfield's term. Garfield also worked to appoint several African Americans to prominent positions: Frederick Douglass, recorder of deeds in Washington; Robert Elliot, special agent to the Treasury; John M. Langston, Haitian minister; and Blanche K. Bruce, register to the Treasury. Garfield believed that Southern support for the Republican party could be gained by "commercial and industrial" interests rather than race issues and began to reverse Hayes's policy of conciliating Southern Democrats. He appointed William H. Hunt, a carpetbagger Republican from Louisiana, as Secretary of the Navy. To break the hold of the resurgent Democratic Party in the Solid South, Garfield took patronage advice from Virginia Senator William Mahone of the biracial independent Readjuster Party, hoping to add the independents' strength to the Republicans' there.
Foreign policy and naval reform.
Entering the presidency, Garfield had little foreign policy experience, so he leaned heavily on Blaine. Blaine, a former protectionist, now agreed with Garfield on the need to promote freer trade, especially within the Western Hemisphere. Their reasons were twofold: firstly, Garfield and Blaine believed that increasing trade with Latin America would be the best way to keep Great Britain from dominating the region. Secondly, by encouraging exports, they believed they could increase American prosperity, and by doing so position the Republican party as the author of that prosperity, ensuring continued electoral success. Garfield authorized Blaine to call for a Pan-American conference in 1882 to mediate disputes among the Latin American nations and to serve as a forum for talks on increasing trade. At the same time, they hoped to negotiate a peace in the War of the Pacific then being fought by Bolivia, Chile, and Peru. Blaine favored a resolution that would not result in Peru yielding any territory, but Chile, which had by 1881 occupied the Peruvian capital, Lima, rejected any settlement that restored the previous "status quo". Garfield sought to expand American influence in other areas, calling for renegotiation of the Clayton-Bulwer Treaty to allow the United States to construct a canal through Panama without British involvement, as well as attempting to reduce British influence in the strategically located Kingdom of Hawaii. Garfield's and Blaine's plans for the United States' involvement in the world stretched even beyond the Western Hemisphere, as he sought commercial treaties with Korea and Madagascar. Garfield also considered enhancing the United States' military strength abroad, asking Navy Secretary Hunt to investigate the condition of the navy with an eye toward expansion and modernization. In the end, these ambitious plans came to nothing after Garfield was assassinated. Nine countries had accepted invitations to the Pan-American conference, but the invitations were withdrawn in April 1882 after Blaine resigned from the cabinet and Arthur, Garfield's successor, cancelled the conference. Naval reform continued under Arthur, if on a more modest scale than Garfield and Hunt had envisioned, ultimately ending in the construction of the Squadron of Evolution.
Assassination.
Guiteau and shooting.
Charles J. Guiteau had followed various professions in his life, but in 1880 had determined to gain federal office by supporting what he expected to be the winning Republican ticket. He composed a speech, "Garfield vs. Hancock", and got it printed by the Republican National Committee. One means of persuading the voter in that era was through orators expounding on the candidate's merits, but with the Republicans seeking more famous men, Guiteau received few opportunities to speak. On one occasion, according to Kenneth D. Ackerman in his book about Garfield's candidacy and assassination, Guiteau was unable to finish his speech due to nerves. Guiteau, who considered himself a Stalwart, deemed his contribution to Garfield's victory sufficient to justify the position of consul in Paris, despite the fact he spoke no French, nor any foreign language.
One of President Garfield's more wearying duties was seeing office seekers, and he saw Guiteau at least once. White House officials suggested that Guiteau approach Blaine, as the consulship was within the Department of State. Blaine also saw the public regularly, and Guiteau became a regular at these sessions; Blaine, who had no intention of giving Guiteau a position he was not qualified for and had not earned, simply stated that the deadlock in the Senate over Robertson's nomination made it impossible to consider the Paris consulship, which required Senate confirmation. Once the New York senators had resigned, and Robertson had been confirmed as Collector, Guiteau pressed his claim, and Blaine told him he would not receive the position.
Guiteau came to believe he had lost the position because he was a Stalwart. The office-seeker decided that the only way to end the internecine warfare in the Republican Party was for Garfield to die—though he had nothing personal against the president. Arthur's succession would restore peace, he felt, and lead to rewards for fellow Stalwarts, including Guiteau.
The assassination of Abraham Lincoln was deemed a fluke due to the Civil War, and Garfield, like most people, saw no reason why the president should be guarded; Garfield's movements and plans were often printed in the newspapers. Guiteau knew the president would leave Washington for cooler climes on July 2, and made plans to kill him before then. He purchased a gun he thought would look good in a museum, and followed Garfield several times, but each time his plans were frustrated, or he lost his nerve. His opportunities dwindled to one—Garfield's departure by train for Massachusetts on the morning of July 2, 1881.
Guiteau concealed himself by the ladies' waiting room at the Sixth Street Station of the Baltimore and Potomac Railroad, from where Garfield was scheduled to depart. Most of Garfield's cabinet planned to accompany him at least part of the way; Blaine, who was to remain in Washington, came to the station to see him off. The two men were deep in conversation and did not notice Guiteau before he took out his revolver and shot Garfield twice, once in the back and once in the arm. The time was 9:30 am. The assassin attempted to leave the station, but was quickly captured. As Blaine recognized him and Guiteau made no secret of why he had shot Garfield, the assassin's motivation to benefit the Stalwarts reached many with the early news of the shooting, causing rage against that faction.
Treatment and death.
Garfield was hit by two shots; one glanced off his arm while the other pierced his back, shattering a rib and embedding itself in his abdomen. "My God, what is this?" he exclaimed. Guiteau, as he was led away, stated, "I did it. I will go to jail for it. I am a Stalwart and Arthur will be President."
Among those at the station was Robert Lincoln, who sixteen years before had watched his father die from an assassin's bullet. Garfield was taken on a mattress upstairs to a private office, where several doctors examined him, probing the wound with unwashed fingers. At his request, Garfield was taken back to the White House, and his wife, then in New Jersey, was sent for. Blaine sent word to Vice President Arthur in New York City, who received threats against his life because of his animosity toward Garfield and Guiteau's statements.
Although Joseph Lister's pioneering work in antisepsis was known to American doctors (Lister himself had visited America in 1876), few of them had confidence in it, and none of his advocates were among Garfield's treating physicians. The physician who took charge at the depot and then at the White House was Doctor Willard Bliss. A noted physician and surgeon, Bliss was an old friend of Garfield, and about a dozen doctors, led by Bliss, were soon probing the wound with unsterilized fingers and instruments. Garfield was given morphine for the pain, and asked Bliss to frankly tell him his chances, which Bliss put at one in a hundred. "Well, Doctor, we'll take that chance."
Over the next few days, Garfield made some improvement, as the nation viewed the news from the capital and prayed. Although he never stood again, he was able to sit up and write several times, and his recovery was viewed so positively that a steamer was fitted out as a seagoing hospital to aid with his convalescence. He was nourished on oatmeal (which he detested) and milk from a cow on the White House lawn. When told that Indian chief Sitting Bull, a prisoner of the army, was starving, Garfield said, "Let him starve", then, "Oh no, send him my oatmeal." Alexander Graham Bell tried to locate the bullet with a primitive metal detector; he was not successful. One means of keeping the president comfortable in Washington's summer heat was one of the first successful air conditioning units: air propelled by fans over ice and then dried reduced the temperature in the sickroom by 20 degrees Fahrenheit (11 degrees Celsius).
Beginning on July 23, Garfield took a turn for the worse. His temperature increased to 104 F; doctors, concerned by a pus sac that had developed by the wound, operated and inserted a drainage tube. This initially seemed to help, and Garfield, in his bed, was able to hold a brief cabinet meeting on July 29, though members were under orders from Bliss to discuss nothing that might excite Garfield. Doctors probed the sac, which went into Garfield's body, hoping to find the bullet; they most likely only made the infections worse. Garfield performed only one state act in August, signing an extradition paper. By the end of the month, the president was much more feeble than he had been, and his weight had decreased to 130 lb.
Garfield had long been anxious to escape hot, unhealthy Washington, and in early September the doctors agreed to move him to Elberon, where his wife had recovered earlier in the summer. He left the White House for the last time on September 5, traveling in a specially cushioned railway car; a spur line to the Franklyn Cottage, a seaside mansion given over to his use, was built in a night by volunteers. There, Garfield could see the ocean as officials and reporters maintained what became (after an initial rally) a death watch. Garfield's personal secretary, Joe Stanley Brown, wrote 40 years later, "to this day I cannot hear the sound of the low slow roll of the Atlantic on the shore, the sound which filled my ears as I walked from my cottage to his bedside, without recalling again that ghastly tragedy."
On September 18, Garfield asked A. F. Rockwell, a friend, if he would have a place in history. Rockwell assured him he would, and told Garfield he had much work still before him. "No, my work is done." The following day, Garfield, by then also suffering from pneumonia and heart pains, marveled that he could not pick up a glass despite feeling well, and went to sleep without discomfort. He awoke that evening around 10:15 pm with great pain in his chest. The attendant watching him sent for Bliss, who found him unconscious. Despite efforts to revive him, Garfield never awoke, dying at 10:35 pm that evening.
According to some historians and medical experts Garfield might have survived his wounds, had the doctors attending him had at their disposal today's medical research, techniques, and equipment. Standard medical practice at the time dictated that priority be given to locating the path of the bullet. Several of his doctors inserted their unsterilized fingers into the wound to probe for the bullet, a common practice in the 1880s. Historians agree that massive infection was a significant factor in President Garfield's demise. Biographer Peskin stated that medical malpractice did not contribute to Garfield's death; the inevitable infection and blood poisoning that would ensue from a deep bullet wound resulted in damage to multiple organs and spinal bone fragmentation. Rutkow, a professor of surgery at the University of Medicine and Dentistry of New Jersey, has argued that starvation also played a role. Rutkow suggests that "Garfield had such a nonlethal wound. In today's world, he would have gone home in a matter of two or three days."
Guiteau was indicted on October 14, 1881, for the murder of the president. In a chaotic trial in which Guiteau often interrupted and argued, and his counsel used the insanity defense, the jury found him guilty on January 5, 1882, and he was sentenced to death. Guiteau may have had syphilis, a disease that causes physiological mental impairment. He was executed on June 30, 1882.
Funeral, memorials and commemorations.
Garfield's funeral train left Long Branch on the same special track that brought him there, traveling over tracks blanketed with flowers and past houses adorned with flags. His body was transported to the Capitol and then continued on to Cleveland for burial. More than 70,000 citizens, some waiting over three hours, passed by Garfield's coffin as his body lay in state in Washington; later, on September 25, 1881, in Cleveland, more than 150,000—a number equal to the entire population of that city—likewise paid their respects. His body was temporarily interred in a vault in Cleveland's Lake View Cemetery until his permanent memorial was built.
Memorials to Garfield were erected across the country. On April 10, 1882, seven months after Garfield's death, the U.S. Post Office issued a postage stamp in his honor, the second stamp issued by the U.S. to honor an assassinated president. In 1884, sculptor Frank Happersberger completed a monument on the grounds of the San Francisco Conservatory of Flowers. In 1887, the James A. Garfield Monument was dedicated in Washington. Another monument, in Philadelphia's Fairmount Park, was erected in 1896.
On May 19, 1890, Garfield's body was permanently interred, with great solemnity and fanfare, in a mausoleum in Lake View Cemetery in Cleveland. Attending the dedication ceremonies were former president Hayes, President Benjamin Harrison, and future president William McKinley. Garfield's Treasury Secretary, William Windom, also attended. Harrison said that Garfield was always a "student and instructor" and that his life works and death would "continue to be instructive and inspiring incidents in American history". Three panels on the monument display Garfield as a teacher, Union major general, and orator; another shows him taking the Presidential oath, and a fifth shows his body lying in state at the Capitol rotunda in Washington D.C.
Garfield's murder by a deranged office seeker awakened public awareness of the need for civil service reform legislation. Senator George H. Pendleton, a Democrat from Ohio, launched a reform effort that resulted in the Pendleton Act in January 1883. This act reversed the "spoils system" where office seekers paid up or gave political service in order to obtain or keep federally appointed positions. Under the act, appointments were awarded on merit and competitive examination. To ensure the reform was implemented, Congress and Arthur established and funded the Civil Service Commission. The Pendleton Act, however, only covered 10% of federal government workers. For Arthur, previously known for having been a "veteran spoilsman", civil service reform became his most noteworthy achievement.
Legacy and historical view.
For a few years after his assassination, Garfield's life story was seen as an exemplar of the American success story: that even the poorest boy might someday become President of the United States. Peskin noted that "in mourning Garfield, Americans were not only honoring a president; they were paying tribute to a man whose life story embodied their own most cherished aspirations". As the rivalry between Stalwarts and Half-Breeds faded from the scene in the late 1880s and after, so too did memories of Garfield. In the 1890s, Americans became disillusioned with politicians, and looked elsewhere for inspiration, focusing on industrialists, labor leaders, scientists, and others as their heroes. Increasingly, Garfield's short time as president was forgotten.
The 20th century saw no revival for Garfield. Thomas Wolfe deemed the presidents of the Gilded Age, including Garfield, "lost Americans" whose "gravely vacant and bewhiskered faces mixed, melted, swam together". The politicians of the Gilded Age faded from the public eye, their luster eclipsed by those who had influenced America outside of political office during that time: the robber barons, the inventors, those who had sought social reform, and others who had lived as America rapidly changed. Current events and more recent figures occupied America's attention: according to Ackerman, "the busy Twentieth Century has made Garfield's era seem remote and irrelevant, its leaders ridiculed for their very obscurity."
Garfield's biographers, and those who have studied his presidency, tend to think well of him, and that his presidency saw a promising start before its untimely end. Historian Justus D. Doenecke, while deeming Garfield a bit of an enigma, chronicles his achievements, "by winning a victory over the Stalwarts, he enhanced both the power and prestige of his office. As a man, he was intelligent, sensitive, and alert, and his knowledge of how government worked was unmatched." Yet, Doenecke criticizes Garfield's dismissal of Merritt in Robertson's favor, and wonders if the president was truly in command of the situation even after the latter's confirmation. According to Caldwell, writing in 1931, "If Garfield lives in history, it will be partly on account of the charm of his personality—but also because in life and in death, he struck the first shrewd blows against a dangerous system of boss rule which seemed for a time about to engulf the politics of the nation. Perhaps if he had lived he could have done no more." Rutkow writes, "James Abram Garfield's presidency is reduced to a tantalizing 'what if.'"
Peskin believes Garfield deserves more credit for his political career than he has received:
True, his accomplishments were neither bold nor heroic, but his was not an age that called for heroism. His stormy presidency was brief and, and in some respects, unfortunate, but he did leave the office stronger than he found it. As a public man he had a hand in almost every issue of national importance for almost two decades, while as a party leader he, along with Blaine, forged the Republican Party into the instrument that would lead the United States into the twentieth century.
Works cited.
Books
Periodicals
Online
External links.
class="wikitable succession-box" style="margin:0.5em auto; font-size:95%;clear:both;"

</doc>
<doc id="40402" url="http://en.wikipedia.org/wiki?curid=40402" title="United States presidential election, 1876">
United States presidential election, 1876

 Ulysses S. Grant
Rutherford B. Hayes
The United States presidential election of 1876 was the 23rd quadrennial presidential election, held on Tuesday, November 7, 1876. It was one of the most contentious and controversial presidential elections in American history. The results of the election remain among the most disputed ever, although there is no question that Samuel J. Tilden of New York outpolled Ohio's Rutherford B. Hayes in the popular vote. After a first count of votes, Tilden won 184 electoral votes to Hayes's 165, with 20 votes unresolved. These 20 electoral votes were in dispute in four states: in the case of Florida, Louisiana, and South Carolina, each party reported its candidate had won the state, while in Oregon one elector was declared illegal (as an "elected or appointed official") and replaced. The question of who should have been awarded these electoral votes is the source of the continued controversy concerning the results of this election.
An informal deal was struck to resolve the dispute: the Compromise of 1877, which awarded all 20 electoral votes to Hayes. In return for the Democrats' acquiescence in Hayes's election, the Republicans agreed to withdraw federal troops from the South, ending Reconstruction. The Compromise effectively ceded power in the Southern states to the Democratic Redeemers, who went on to pursue their agenda of returning the South to a political economy resembling that of its pre-war condition, including the disenfranchisement of black voters.
This was the first presidential election in 20 years in which the Democratic candidate won a majority of the popular vote. This is also the only election in which a candidate for president received more than 50 percent of the popular vote but was not elected president by the Electoral College, and one of four elections in which the person winning the plurality of the popular vote did not win the election. It is to date the smallest electoral vote victory and the election with the highest voter turnout of the voting age population in American history.
Nominations.
Republican Party nomination.
Republican candidates:
During 1875 it was widely assumed that despite the poor economy and numerous political scandals he had endured, incumbent President Ulysses S. Grant would run for a third term as President, as, despite the long-standing tradition set by George Washington that Presidents only served two terms, Grant was considerably younger than any of the other two-term Presidents (Grant would be 54 years old when his term ended; the second-youngest two-term President, Washington himself, had been 65). In the autumn of that year however, Grant's advisers told him that while he still had enough support within the party that he could probably have secured the nomination without too much difficulty, they doubted his ability to win the election. The sudden death of Vice-President Henry Wilson in November seemed to sap any remaining desire Grant had to hold onto his office, and he announced later in the month that he would not seek re-nomination.
When the 6th Republican National Convention assembled on June 14, 1876, it appeared that James G. Blaine would be the nominee. On the first ballot, Blaine was just 100 votes short of a majority. His vote began to slide after the second ballot, as many Republicans feared that Blaine could not win the general election. Anti-Blaine delegates could not agree on a candidate until Blaine's total rose to 41% on the sixth ballot. Leaders of the reform Republicans met privately and considered alternatives. The choice was Ohio's reform Governor, Rutherford B. Hayes. On the seventh ballot, Hayes was nominated with 384 votes to 351 for Blaine and 21 for Benjamin Bristow. William A. Wheeler was nominated for vice-president by a much larger margin (366–89), and then amidst the balloting through acclamation, over his chief rival, Frederick Theodore Frelinghuysen, who later served as a member of the electoral commission that awarded the election to Hayes.
Republican Presidential Nomination Vote by State Delegation By Ballot 
Popular culture.
The Presidential Election of 1876 is a major theme of Gore Vidal's novel 1876.

</doc>
<doc id="40405" url="http://en.wikipedia.org/wiki?curid=40405" title="George Whitefield">
George Whitefield

George Whitefield (December 27 [O.S. December 16] 1714 – September 30, 1770), also known as George Whitfield, was an English Anglican cleric who helped spread the Great Awakening in Britain and, especially, in the American colonies.
Born in Gloucester, England, he attended Pembroke College, Oxford University, where he met the Wesley brothers. He was one of the founders of Methodism and of the evangelical movement generally. In 1740, Whitefield traveled to America, where he preached a series of revivals that came to be known as the "Great Awakening." He became perhaps the best-known preacher in Great Britain and North America during the 18th century. Because he traveled throughout the American colonies and drew great crowds and news coverage, he was one of the most widely-recognized public figures in colonial America.
Early life.
Whitefield was born at the Bell Inn, Southgate Street, Gloucester in England. Whitefield was the fifth son (seventh child) of Thomas Whitefield and Elizabeth Edwards who kept an inn at Gloucester. At an early age, he found that he had a passion and talent for acting in the theatre, a passion that he would carry on with the very theatrical re-enactments of Bible stories he told during his sermons. He was educated at the Crypt School, Gloucester, and Pembroke College, Oxford.
Because business at the inn had become poor, Whitefield did not have the means to pay for his tuition. He therefore entered Oxford as a servitor, the lowest rank of students at Oxford. In return for free tuition, he was assigned as a minister to a number of higher ranked students. His duties included teaching them in the morning, helping them bathe, taking out their garbage, carrying their books and even assisting with required written assignments. He was a part of the "Holy Club" at the University of Oxford with the Wesley brothers, John and Charles. An illness, as well as Henry Scougal's "The Life of God in the Soul of Man", influenced him to cry out to God for salvation. Following a religious conversion, he became very passionate for preaching his new-found faith. The Bishop of Gloucester ordained him a deacon.
Evangelism.
Whitefield preached his first sermon at St Mary de Crypt Church in his home town of Gloucester a week after his ordination. He had earlier become the leader of the Holy Club at Oxford when the Wesley brothers departed for Georgia.
In 1738 he went to Savannah, Georgia, in the American colonies, as parish priest. While there he decided that one of the great needs of the area was an orphan house. He decided this would be his life's work. He returned to England to raise funds, as well as to receive priest's orders. While preparing for his return he preached to large congregations. At the suggestion of friends he preached to the miners of Kingswood, outside Bristol, in the open air. Because he was returning to Georgia he invited John Wesley to take over his Bristol congregations, and to preach in the open air for the first time at Kingswood and then Blackheath, London.
Whitefield accepted the Church of England's doctrine of predestination and disagreed with the Wesley brothers' views on the doctrine of the Atonement, Arminianism. As a result Whitefield did what his friends hoped he would not do—hand over the entire ministry to John Wesley. Whitefield formed and was the president of the first Methodist conference. But he soon relinquished the position to concentrate on evangelical work.
Three churches were established in England in his name – one in Penn Street, Bristol, and two in London, in Moorfields and in Tottenham Court Road – all three of which became known by the name of "Whitefield's Tabernacle". The society meeting at the second Kingswood School at Kingswood, a town on the eastern edge of Bristol, was eventually also named Whitefield's Tabernacle. Whitefield acted as chaplain to Selina, Countess of Huntingdon, and some of his followers joined the Countess of Huntingdon's Connexion, whose chapels were built by Selina, where a form of Calvinistic Methodism similar to Whitefield's was taught. Many of Selina's chapels were built in the English and Welsh counties, and one was erected in London—Spa Fields Chapel.
In 1739, Whitefield returned to England to raise funds to establish the Bethesda Orphanage, which is the oldest extant charity in North America. On returning to North America in 1740, he preached a series of revivals that came to be known as the Great Awakening of 1740. In 1740 he engaged Moravian Brethren from Georgia to build an orphanage for Negro children on land he had bought in the Lehigh Valley of Pennsylvania. Following a theological disagreement, he dismissed them but was unable to complete the building, which the Moravians subsequently bought and completed. This now is the Whitefield House in the center of the Moravian settlement of Nazareth. He preached nearly every day for months to large crowds of sometimes several thousand people as he traveled throughout the colonies, especially New England. His journey on horseback from New York City to Charleston was the longest then undertaken in North America by a white man.
Like his contemporary and acquaintance, Jonathan Edwards, Whitefield preached staunchly Calvinist theology that was in line with the "moderate Calvinism" of the Thirty-nine Articles. While explicitly affirming God's sole agency in salvation, Whitefield freely offered the Gospel, saying at the end of his sermons: "Come poor, lost, undone sinner, come just as you are to Christ."
Revival meetings.
The Anglican Church did not assign him a pulpit, so he began preaching in parks and fields in England on his own, reaching out to people who normally did not attend church. Like Jonathan Edwards, he developed a style of preaching that elicited emotional responses from his audiences. But Whitefield had charisma, and his voice (which according to many accounts, could be heard over five hundred feet), his small stature, and even his cross-eyed appearance (which some people took as a mark of divine favour) all served to help make him one of the first celebrities in the American colonies.
Thanks to widespread dissemination of print media, perhaps half of all colonists eventually heard about, read about, or read something written by Whitefield. He employed print systematically, sending advance men to put up broadsides and distribute handbills announcing his sermons. He also arranged to have his sermons published. A crowd Whitefield estimated at 30,000 met him in Cambuslang in 1742.
Benjamin Franklin and Whitefield.
Benjamin Franklin attended a revival meeting in Philadelphia, Pennsylvania and was greatly impressed with Whitefield's ability to deliver a message to such a large group. Franklin had previously dismissed, as an exaggeration, reports of Whitefield preaching to crowds of the order of tens of thousands in England. When listening to Whitefield preaching from the Philadelphia court house, Franklin walked away towards his shop in Market Street until he could no longer hear Whitefield distinctly. He then estimated his distance from Whitefield and calculated the area of a semicircle centred on Whitefield. Allowing two square feet per person he computed that Whitefield could be heard by over thirty thousand people in the open air.
Franklin admired Whitefield as a fellow intellectual but thought Whitefield's plan to run an orphanage in Georgia would lose money. He published several of Whitefield's tracts and was impressed by Whitefield's ability to preach and speak with clarity and enthusiasm to crowds. Franklin was an ecumenist and approved of Whitefield's appeal to members of many denominations, but was not, like Whitefield, an evangelical. In his autobiography, Franklin famously wrote that he was a "thoroughgoing Deist," which precludes the idea that God is personal, though some suggest that Franklin was more traditional in his views, e.g., his speech at the Constitutional Convention where he recited the verse that not a single sparrow falls to the ground without God's notice; how then could the Constitution convention hope to succeed without God's careful oversight? After one of Whitefield's sermons, Franklin noted the:
wonderful… change soon made in the manners of our inhabitants. From being thoughtless or indifferent about religion, it seem'd as if all the world were growing religious, so that one could not walk thro' the town in an evening without hearing psalms sung in different families of every street." 
A lifelong close friendship developed between the revivalist preacher and the worldly Franklin. Looking beyond their public images, one finds a common charity, humility, and ethical sense embedded in the character of each man. True loyalty based on genuine affection, coupled with a high value placed on friendship, helped their association grow stronger over time.
Letters exchanged between Franklin and Whitefield can be found at the American Philosophical Society in Philadelphia. These letters document the creation of an orphanage for boys named the Charity School. 
And in 1749, Franklin chose the Whitefield meeting house, with its Charity School, to be purchased as the site of the newly formed Academy of Philadelphia which opened in 1751, followed in 1755 with the College of Philadelphia, both the predecessors of the University of Pennsylvania. A statue of George Whitefield is located in the Dormitory Quadrangle, standing in front of the Morris and Bodine sections of the present Ware College House on the University of Pennsylvania campus.
Travels.
Whitefield is remembered as one of the first to preach to the enslaved. Phillis Wheatley wrote a poem in his memory after he died.
In an age when crossing the Atlantic Ocean was a long and hazardous adventure, he visited America seven times, making thirteen ocean crossings in total. It is estimated that throughout his life, he preached more than 18,000 formal sermons, of which seventy-eight have been published In addition to his work in North America and England, he made fifteen journeys to Scotland—most famously to the "Preaching Braes" of Cambuslang in 1742—two journeys to Ireland, and one each to Bermuda, Gibraltar, and the Netherlands.
He went to the Georgia Colony in 1738 following John Wesley's departure, to serve as a colonial chaplain at Savannah.
Death.
Whitefield died in the parsonage of Old South Presbyterian Church, Newburyport, Massachusetts, on September 30, 1770, and was buried, according to his wishes, in a crypt under the pulpit of this church. A bust of Whitefield is in the collection of the Gloucester City Museum & Art Gallery.
It was John Wesley who preached his funeral sermon in London, at Whitefield's request.
Relation to other Methodist leaders.
In terms of theology, Whitefield, unlike John Wesley, was a supporter of Calvinism. The two differed on eternal election, final perseverance, and sanctification, but were reconciled as friends and co-workers, each going his own way. It is a prevailing misconception that Whitefield was not primarily an organizer like Wesley. However, as Wesleyan historian Rev. Luke Tyerman states, "It is notable that the first Calvinistic Methodist Association was held eighteen months before Wesley held his first Methodist Conference." He was a man of profound experience, which he communicated to audiences with clarity and passion. His patronization by the Countess of Huntingdon reflected this emphasis on practice.
Religious innovation.
In the First Great Awakening, rather than listening demurely to preachers, people groaned and roared in enthusiastic emotion; new divinity schools opened to challenge the hegemony of Yale and Harvard; personal experience became more important than formal education for preachers. Such concepts and habits formed a necessary foundation for the American Revolution.
Advocacy of slavery.
In 1735, slavery had been outlawed in the young Georgia colony. In 1749, George Whitefield campaigned for its legalization, claiming that the territory would never be prosperous unless farms were able to use slave labor. He began his fourth visit to America in 1751 advocating slavery, viewing its re-legalization in Georgia as necessary to make his plantation profitable. Partly through his campaigns and written pleas to the Georgia Trustees, it was re-legalized in 1751. Whitefield purchased slaves, who then worked at his Bethesda Orphanage. To help raise money for the orphanage, he also employed slaves at "Providence Plantation". Whitefield was known to treat his slaves well; they were reputed to be devoted to him, and he was critical of the abuse of slaves by other owners. When Whitefield died, he bequeathed his slaves to the Countess of Huntingdon. His attitude towards slavery is expressed in a letter to Mr B. written from Bristol 22 March 1751:
As for the lawfulness of keeping slaves, I have no doubt, since I hear of some that were bought with Abraham's money, and some that were born in his house.—And I cannot help thinking, that some of those servants mentioned by the Apostles in their epistles, were or had been slaves. It is plain, that the Gibeonites were doomed to perpetual slavery, and though liberty is a sweet thing to such as are born free, yet to those who never knew the sweets of it, slavery perhaps may not be so irksome. However this be, it is plain to a demonstration, that hot countries cannot be cultivated without negroes. What a flourishing country might Georgia have been, had the use of them been permitted years ago? How many white people have been destroyed for want of them, and how many thousands of pounds spent to no purpose at all? Had Mr Henry been in America, I believe he would have seen the lawfulness and necessity of having negroes there. And though it is true, that they are brought in a wrong way from their own country, and it is a trade not to be approved of, yet as it will be carried on whether we will or not; I should think myself highly favoured if I could purchase a good number of them, in order to make their lives comfortable, and lay a foundation for breeding up their posterity in the nurture and admonition of the Lord. You know, dear Sir, that I had no hand in bringing them into Georgia; though my judgement was for it, and so much money was yearly spent to no purpose, and I was strongly importuned thereto, yet I would not have a negro upon my plantation, till the use of them was publicly allowed in the colony. Now this is done, dear Sir, let us reason no more about it, but diligently improve the present opportunity for their instruction. The trustees favour it, and we may never have a like prospect. It rejoiced my soul, to hear that one of my poor negroes in Carolina was made a brother in Christ. How know we but we may have many such instances in Georgia ere it be long?
Works.
Whitefield's sermons were widely reputed to capture his audience's enthusiasm, and many of them as well as his letters and journals were published during his lifetime. He was an excellent orator as well, strong in voice and adept at extemporaneity. His voice was so expressive that people are said to have wept just hearing him allude to "Mesopotamia". His journals, originally intended only for private circulation, were surreptitiously published by Thomas Cooper. This led James Hutton to publish a version with Whitefield's approval. Exuberant and "too apostolical" language resulted in great criticism and his journals ceased publication after 1741. Although Whitefield prepared a new installment in 1744–45, it wasn't published until 1938, and nineteenth century biographies refer to an earlier manuscript. Whitefield published "A Short Account of God's Dealings with the Reverend George Whitefield" in 1740, which covered his life up to his ordination. In 1747, he published "A Further Account of God's Dealings with the Reverend George Whitefield", covering the period from his ordination to his first voyage to Georgia. In 1756, a heavily edited version of his journals and autobiographical accounts was released.
After his death, John Gillies, a Glasgow friend, published a memoir and six volumes of works, comprising three volumes of letters, a volume of tracts, and two volumes of sermons. Another collection of sermons was published just before he left London for the last time in 1769. These were disowned by Whitefield and Gillies, who tried to buy all copies and pulp them. They had been taken down in shorthand, but Whitefield said that they made him say nonsense on occasion. These sermons were included in a nineteenth-century volume, "Sermons on Important Subjects", along with the "approved" sermons from the "Works". An edition of the journals, in one volume, was edited by William Wale in 1905. This was reprinted with additional material in 1960 by the Banner of Truth Trust. It lacks the Bermuda journal entries found in Gillies biography and the quotes from manuscript journals found in nineteenth century biographies. A comparison of this edition with the original 18th century publications shows numerous omissions—some minor and a few major.
Whitefield also wrote several hymns. In 1739, Charles Wesley composed a hymn, "Hark, how all the welkin rings”. In 1758, Whitefield revised the opening couplet to Hark, the Herald Angels Sing.
Veneration.
Whitefield is honored together with Francis Asbury with a feast day on the liturgical calendar of the Episcopal Church (USA) on November 15.
Whitfield County, Georgia, is named after the Reverend. When the act by the Georgia General Assembly was written to create the county, the "e" was omitted from the spelling of the name to reflect the pronunciation of the name. 

</doc>
<doc id="40406" url="http://en.wikipedia.org/wiki?curid=40406" title="Sweyn Forkbeard">
Sweyn Forkbeard

Sweyn Forkbeard (Old Norse: "Sveinn Tjúguskegg" Danish "Sven Tveskæg"; 960 - 3 February 1014) was king of Denmark and England, as well as parts of Norway. His name appears as Swegen in the "Anglo-Saxon Chronicle". He was the son of King Harald Bluetooth of Denmark. He was the father of Cnut the Great.
In the mid 980s, Sweyn revolted against his father and seized the throne. Harald was driven into exile and died shortly afterwards in November 986 or 987. In 1000, with allegiance of the Trondejarl, Eric of Lade, Sweyn was ruler over most of Norway. After a long effort at conquest and shortly before his death, he became, in 1013, the first of the Danish Kings of England.
The Church and currency.
On the northern edges of the relatively recent Holy Roman Empire, with its roots in Charlemagne's conquests about two hundred years prior to Sweyn's time, Sweyn Forkbeard had coins made with an image in his likeness. The Latin inscription on the coins read, "ZVEN REX DÆNOR[UM]", which translates as "Sven, king of the Danes".
Sweyn's father, Harald Bluetooth, was the first of the Scandinavian kings to accept Christianity officially, in the early or mid-960s. According to Adam of Bremen, an 11th-century historian, Harald's son Sweyn was baptised "Otto", in tribute to the German king Otto I, who was the first Holy Roman Emperor. Forkbeard is never known to have officially made use of this Christian name.
Life and legacy.
Many details about Sweyn's life are contested. Scholars disagree about the various, too often contradictory, accounts of his life given in sources from this era of history, such as the "Anglo-Saxon Chronicle", Adam of Bremen's "Deeds of the Bishops of Hamburg", and the "Heimskringla", a 13th-century work by Icelandic author Snorri Sturluson. Conflicting accounts of Sweyn's later life also appear in the "Encomium Emmae Reginae", an 11th-century Latin "encomium" in honour of his son king Cnut's queen Emma of Normandy, along with "Chronicon ex chronicis" by Florence of Worcester, another 11th-century author.
The "Dictionary of National Biography" states that his mother's name is unknown, but the Danish encyclopedia "Den Store Danske" identifies her as Tove from the Western Wendland. "Den Store Danske" identifies Sweyn's wife as Gunhild, widow of Erik, king of Sweden, but the "Dictionary of National Biography", while agreeing that she was Erik's widow, describes her as an unnamed sister of Boleslav, ruler of Poland.
Many negative accounts build on Adam of Bremen's writings; Adam is said to have watched Sweyn and Scandinavia in general with an "unsympathetic and intolerant eye", according to some scholars. Adam accused Forkbeard of being a rebellious pagan who persecuted Christians, betrayed his father and expelled German bishops from Scania and Zealand. According to Adam, Sweyn was sent into exile by his father's German friends and deposed in favour of king Eric the Victorious of Sweden, whom Adam wrote ruled Denmark until his death in 994 or 995.
Historians generally have found problems with Adam's claims, such as that Sweyn was driven into exile in Scotland for a period as long as fourteen years. As many scholars point out, he built churches in Denmark throughout this period, such as Lund and Roskilde, while he led Danish raids against England.
Ruler of England.
The "Chronicle of John of Wallingford" (ca. 1225–1250) records Sweyn's involvement in raids against England during 1002–1005, 1006–1007, and 1009–1012 to revenge the St. Brice's Day massacre of England's Danish inhabitants in November 1002. Sweyn was believed to have had a personal interest in the atrocities, with his sister Gunhilde and her husband possibly amongst the victims.
Sweyn campaigned in Wessex and East Anglia in 1003–1004, but a famine forced him to return to Denmark in 1005. Further raids took place in 1006-1007, and in 1009-1012 Thorkell the Tall led a Viking invasion into England. Simon Keynes regards it as uncertain whether Sweyn supported these invasions, but "whatever the case, he was quick to exploit the disruption caused by the activities of Thorkell's army".
Some scholars have argued that Sweyn's participation may have been prompted by his state of impoverishment after having been forced to pay a hefty ransom. He needed revenue from the raids. He acquired massive sums of "Danegeld" through the raids. In 1013, he is reported to have personally led his forces in a full-scale invasion of England.
The contemporary "Peterborough Chronicle" (also called the "Laud Manuscript"), one of the "Anglo-Saxon Chronicles", states:
before the month of August came king Sweyn with his fleet to Sandwich. He went very quickly about East Anglia into the Humber's mouth, and so upward along the Trent till he came to Gainsborough. Earl Uchtred and all Northumbria quickly bowed to him, as did all the people of the Kingdom of Lindsey, then the people of the Five Boroughs. He was given hostages from each shire. When he understood that all the people had submitted to him, he bade that his force should be provisioned and horsed; he went south with the main part of the invasion force, while some of the invasion force, as well as the hostages, were with his son Cnut. After he came over Watling Street, they went to Oxford, and the town-dwellers soon bowed to him, and gave hostages. From there they went to Winchester, and the people did the same, then eastward to London.
But the Londoners put up a strong resistance, because King Æthelred and Thorkell the Tall, a Viking leader who had defected to Æthelred, personally held their ground against him in London itself. Sweyn then went west to Bath, where the western thanes submitted to him and gave hostages. The Londoners then followed suit, fearing Sweyn's revenge if they resisted any longer. King Æthelred sent his sons Edward and Alfred to Normandy, and himself retreated to the Isle of Wight, and then followed them into exile. On Christmas Day 1013 Sweyn was declared King of England.
Based in Gainsborough, Lincolnshire, Sweyn began to organise his vast new kingdom, but he died there on 3 February 1014, having ruled England for only five weeks. His embalmed body was returned to Denmark for burial in the church he had built himself. (Tradition locates this church in Roskilde, but it is more plausible that it was actually located in Lund in Scania (now part of Sweden).) Sweyn's elder son, Harald II, succeeded him as King of Denmark, but the Danish fleet in England proclaimed his younger son Cnut king. In England, the councillors had sent for Æthelred, who upon his return from exile in Normandy in the spring of 1014 managed to drive Cnut out of England. But Cnut returned and became King of England in 1016, eventually also ruling Denmark, Norway, parts of Sweden, Pomerania, and Schleswig.
Sweyn's son Cnut and Cnut's own sons Harold Harefoot and Harthacnut ruled England for 26 years. After Harthacnut's death, the English throne reverted to the House of Wessex in the person of King Edward the Confessor (reigned 1042-1066).
Sweyn's descendants through his daughter Estrid continue to reign in Denmark to this day. One of his descendants, Margaret of Denmark, married James III of Scotland in 1469, introducing Sweyn's bloodline into the Scottish royal house. After James VI of Scotland inherited the English throne in 1603, Sweyn's descendants became monarchs of England again.
Religion.
Adam of Bremen's writings about Sweyn and his father may have been influenced by Adam's desire to emphasise Sweyn's father Harald as a candidate for sainthood. He claimed that Sweyn, who was baptised along with his father, was a heathen. This may have been true, as much of Scandinavia was pagan at the time, but there are no data to corroborate the assertion. German and French records support that Harald Bluetooth was baptised.
According to Adam, Sweyn was punished by God for leading the uprising which led to king Harald's death, and had to spend "fourteen years" abroad - perhaps a Biblical reference from an ecclesiastical writer, as it refers to the symbolic number seven. Adam purports that Sweyn was shunned by all those with whom he sought refuge, but was finally allowed to live for a while in Scotland. Adam's intention appeared to be to show that Sweyn belonged with heathens and was not fit to rule a Christian country. According to Adam, Sweyn only achieved success as a ruler after accepting Christianity.
Sweyn was tolerant of paganism while favouring Christianity, at least politically. By allowing English ecclesiastical influence in his kingdom, he was spurning the Hamburg-Bremen archbishop. Since German bishops were an integral part of the secular state, Sweyn's preference for the English church may have been a political move. He sought to pre-empt any threat against his independence posed by the German kings. Contrary to Adam's writings, Sweyn did not appear to have re-established paganism. There is no evidence of reversion to pagan burial practices during Sweyn's reign. Whether King Sweyn was a heathen or not, he enlisted priests and bishops from England rather than from Hamburg. This may have been another reason for Adam of Bremen's apparent hostility in his accounts. Numerous converted priests of a Danish origin from the Danelaw lived in England, while Sweyn had few connections to Germany or its priests.
Sweyn must have known that once the Archbishop of Hamburg-Bremen gained influence in Denmark, the German Emperor Otto II would not be far behind. His Slavic neighbours to the south-east had been all but annexed by Germany once Otto's father Otto I divided their lands into bishoprics and put them under the "care" of the Holy Roman Emperor. Sweyn may have envisaged the same happening to his own territory.
Issue.
By Świętosława of Poland:

</doc>
<doc id="40407" url="http://en.wikipedia.org/wiki?curid=40407" title="Harold Furth">
Harold Furth

Harold Paul Furth (January 13, 1930, Vienna - February 21, 2002, Philadelphia) was an Austrian-American physicist.
Furth emigrated to the United States in 1941. He graduated from Harvard University with a Bachelor's degree in 1951 and received his Ph.D. from Harvard in 1960. Furth worked at Lawrence Livermore National Laboratory for several years before going to Princeton Plasma Physics Laboratory (PPPL) where he would spend the rest of his career working in plasma physics and nuclear fusion. He was also a professor of astrophysics at Princeton University.
In the late 1960s Furth contributed some important theoretical work on resistive magnetohydrodynamics instabilities in a slightly resistive plasma.
In 1981 Furth became the director at PPPL and led the laboratory until 1990 during record setting magnetic fusion energy experiments on the largest tokamak in the country, the Tokamak Fusion Test Reactor (TFTR).
He was awarded the Maxwell Prize in 1983 and the Delmer S. Fahrney Medal in 1992.
Furth was a member of the National Academy of Sciences and died of a heart ailment.

</doc>
<doc id="40410" url="http://en.wikipedia.org/wiki?curid=40410" title="Outer Hebrides">
Outer Hebrides

The Outer Hebrides, also known as the Western Isles (Scottish Gaelic: "Na h-Eileanan Siar" ]), "Innse Gall" ("islands of the strangers") or the Long Island, is an island chain off the west coast of mainland Scotland. The islands are geographically coextensive with Comhairle nan Eilean Siar, one of the 32 unitary council areas of Scotland. They form part of the Hebrides, separated from the Scottish mainland and from the Inner Hebrides by the waters of the Minch, the Little Minch and the Sea of the Hebrides. Scottish Gaelic is the predominant spoken language, although in a few areas English speakers form a majority.
Most of the islands have a bedrock formed from ancient metamorphic rocks and the climate is mild and oceanic. The 15 inhabited islands have a total population of 27,400 and there are more than 50 substantial uninhabited islands. From Barra Head to the Butt of Lewis is roughly 210 km.
There are various important prehistoric structures, many of which pre-date the first written references to the islands by Roman and Greek authors. The Western Isles became part of the Norse kingdom of the "Suðreyjar", which lasted for over 400 years until sovereignty was transferred to Scotland by the Treaty of Perth in 1266. Control of the islands was then held by clan chiefs, principal of whom were the MacLeods, MacDonalds, Mackenzies and MacNeils. The Highland Clearances of the 19th century had a devastating effect on many communities and it is only in recent years that population levels have ceased to decline. Much of the land is now under local control and commercial activity is based on tourism, crofting, fishing, and weaving.
Sea transport is crucial and a variety of ferry services operate between the islands and to mainland Scotland. Modern navigation systems now minimise the dangers but in the past the stormy seas have claimed many ships. Religion, music and sport are important aspects of local culture, and there are numerous designated conservation areas to protect the natural environment.
Geography.
The main islands form an archipelago of which the major islands include Lewis and Harris, North Uist, Benbecula, South Uist, and Barra. Lewis and Harris has an area of 217898 ha and is the largest island in Scotland and the third largest in the British Isles, after Great Britain and Ireland. It incorporates Lewis in the north and Harris in the south, both of which are frequently referred to as individual islands, although they are joined by a land border. The island does not have a common name in either English or Gaelic and is referred to as "Lewis and Harris", "Lewis with Harris", "Harris with Lewis" etc.
The largest islands are deeply indented by arms of the sea such as Loch Ròg, Loch Seaforth and Loch nam Madadh. There are also more than 7,500 freshwater lochs in the Outer Hebrides, about 24% of the total for the whole of Scotland. North and South Uist and Lewis in particular have landscapes with a high percentage of freshwater and a maze and complexity of loch shapes. Harris has fewer large bodies of water but innumerable small lochans. Loch Langavat on Lewis is 11 km long, and has several large islands in its midst, including Eilean Mòr. Although Loch Suaineabhal has only 25% of the Langavat's surface area it has a mean depth of 33 m and is the most voluminous on the island. Of Loch Sgadabhagh on North Uist it has been said that "there is probably no other loch in Britain which approaches Loch Scadavay in irregularity and complexity of outline." Loch Bì is South Uist's largest loch and at 8 km long it all but cuts the island in two.
Much of the western coastline of the islands is machair, a fertile low-lying dune pastureland. Lewis is comparatively flat, and largely consists of treeless moors of blanket peat. The highest eminence is Mealisval at 574 m in the south west. Most of Harris is mountainous, with large areas of exposed rock and Clisham, the archipelago's only Corbett, reaches 799 m in height. North and South Uist and Benbecula, (sometimes collectively referred to as The Uists) have sandy beaches and wide cultivated areas of machair to the west and virtually uninhabited mountainous areas to the east. The highest peak here is Beinn Mhòr at 620 m. The Uists and their immediate outliers have a combined area of 74540 ha. This includes the Uists themselves and the islands that link to them by causeways and bridges. Barra is 5875 ha in extent and has a rugged interior, surrounded by machair and extensive beaches.
Flora and fauna.
Much of the archipelago is a protected habitat including both the islands and the surrounding waters. There are 53 Sites of Special Scientific Interest of which the largest are Loch an Duin, North Uist at 15100 ha and North Harris, which is 12700 ha in extent.
Loch Druidibeg on South Uist is a national nature reserve owned and managed by Scottish Natural Heritage. The reserve covers 1,677 hectares across the whole range of local habitats. Over 200 species of flowering plants have been recorded on the reserve, some of which are nationally scarce. South Uist is considered the best place in the UK for the aquatic plant Slender Naiad, which is a European Protected Species.
There has been considerable controversy over hedgehogs on the Uists. The animals are not native to the islands, having been introduced in the 1970s to reduce garden pests and their spread posed a threat to the eggs of ground nesting wading birds. In 2003 Scottish Natural Heritage undertook culls of hedgehogs in the area although they were halted in 2007, with trapped animals now being relocated to the mainland.
Nationally important populations of breeding waders are present in the Outer Hebrides including common redshank, dunlin, lapwing and ringed plover. The islands also provide a habitat for other important species such as corncrake, hen harrier, golden eagle and otter. Offshore, basking shark and various species of whale and dolphin can be seen regularly, and the remoter islands' seabird populations are of international significance. St Kilda has 60,000 northern gannets, amounting to 24% of the world population, 49,000 breeding pairs of Leach's petrel, up to 90% of the European population and 136,000 pairs of puffin and 67,000 northern fulmar pairs, about 30% and 13% of the respective UK totals. Mingulay is an important breeding ground for razorbills, with 9,514 pairs, 6.3% of the European population.
The bumblebee "Bombus jonellus" var. "hebridensis" is endemic to the Hebrides and there are local variants of the dark green fritillary and green-veined white butterflies. The St Kilda wren is a subspecies of wren whose range is confined to the islands whose name it bears.
Population.
The inhabited islands had a total population of 26,502 at the time of the 2001 census and the 2011 figure was 27,684. During the same period Scottish island populations as a whole grew by 4% to 103,702. The largest settlement in the Outer Hebrides is Stornoway on Lewis, which has a population of about 8,100.
In addition to the major North Ford ("Oitir Mhòr") and South Ford causeways that connect North Uist to Benbecula via the northern of the Grimsay's, and Benbecula to South Uist, several other islands are linked by smaller causeways or bridges. Great Bernera and Scalpay have bridge connections to Lewis and Harris respectively, with causeways linking Baleshare and Berneray to North Uist, Eriskay to South Uist, Flodaigh, Fraoch-eilean and the southern Grimsay to Benbecula, and Vatersay to Barra. This means that all the inhabited islands are now connected to at least one other island by a land transport route.
Uninhabited islands.
There are more than fifty uninhabited islands greater in size than 40 ha in the Outer Hebrides, including the Barra Isles, Flannan Isles, Monach Islands, the Shiant Isles and the islands of Loch Ròg. In common with the other main island chains of Scotland, many of the more remote islands were abandoned during the 19th and 20th centuries, in some cases after continuous habitation since the prehistoric period. More than 35 such islands have been identified in the Outer Hebrides alone. On Barra Head, for example, Historic Scotland have identified eighty-three archaeological sites on the island, the majority being of a pre-medieval date. In the 18th century the population was over fifty, but the last native islanders had left by 1931. The island became completely uninhabited by 1980 with the automation of the lighthouse.
Some of the smaller islands continue to contribute to modern culture. The "Mingulay Boat Song", although evocative of island life, was written after the abandonment of the island in 1938 and Taransay hosted the BBC television series "Castaway 2000". Others have played a part in Scottish history. On 4 May 1746, the "Young Pretender" Charles Edward Stuart hid on Eilean Liubhaird with some of his men for four days whilst Royal Navy vessels patrolled the Minch.
Smaller isles and skerries and other island groups pepper the North Atlantic surrounding the main islands. Some are not geologically part of the Outer Hebrides, but are administratively and in most cases culturally, part of Comhairle nan Eilean Siar. 73 km to the west of Lewis lies St Kilda, now uninhabited except for a small military base. A similar distance to the north of Lewis are North Rona and Sula Sgeir, two small and remote islands. While Rona used to support a small population who grew grain and raised cattle, Sula Sgeir is an inhospitable rock. Thousands of northern gannets nest here, and by special arrangement some of their young, known as "gugas" are harvested annually by the men of Ness. The status of Rockall, which is 367 km to the west of North Uist and which the Island of Rockall Act 1972 decreed to be a part of the Western Isles, remains a matter of international dispute.
Geology.
Most of the islands have a bedrock formed from Lewisian gneiss. These are amongst the oldest rocks in Europe, having been formed in the Precambrian period up to three billion years ago. In addition to the Outer Hebrides, they form basement deposits on the Scottish mainland west of the Moine Thrust and on the islands of Coll and Tiree. These rocks are largely igneous in origin, mixed with metamorphosed marble, quartzite and mica schist and intruded by later basaltic dykes and granite magma. The gneiss's delicate pink colours are exposed throughout the islands and it is sometimes referred to by geologists as "The Old Boy".
Granite intrusions are found in the parish of Barvas in west Lewis, and another forms the summit plateau of the mountain Roineabhal in Harris. The granite here is anorthosite, and is similar in composition to rocks found in the mountains of the Moon. There are relatively small outcrops of Triassic sandstone at Broad Bay near Stornoway. The Shiant Isles and St Kilda are formed from much later tertiary basalt and basalt and gabbros respectively. The sandstone at Broad Bay was once thought to be Torridonian or Old Red Sandstone.
Climate.
The Outer Hebrides have a cool temperate climate that is remarkably mild and steady for such a northerly latitude, due to the influence of the North Atlantic Current. The average temperature for the year is 6 °C (44 °F) in January and 14 °C (57 °F) in summer. The average annual rainfall in Lewis is 1100 mm and sunshine hours range from 1,100–1,200 per year. The summer days are relatively long and May to August is the driest period. Winds are a key feature of the climate and even in summer there are almost constant breezes. According to the writer W. H. Murray if a visitor asks an islander for a weather forecast "he will not, like a mainlander answer dry, wet or sunny, but quote you a figure from the Beaufort Scale." There are gales one day in six at the Butt of Lewis and small fish are blown onto the grass on top of 190 metre (620 ft) high cliffs at Barra Head during winter storms.
Prehistory.
The Hebrides were originally settled in the Mesolithic era and have a diversity of important prehistoric sites. Eilean Dòmhnuill in Loch Olabhat on North Uist was constructed circa 3200–2800 BC and may be Scotland's earliest crannog (a type of artificial island). The Callanish Stones dating from about 2900 BC are the finest example of a stone circle in Scotland, the 13 primary monoliths of between one and five metres high creating a circumference about 13 m in diameter. Cladh Hallan on South Uist, the only site in the UK where prehistoric mummies have been found and the impressive ruins of Dun Carloway broch on Lewis both date from the Iron Age.
Etymology.
The earliest written references that have survived relating to the islands were made by Pliny the Elder in his "Natural History", where he states that there are 30 "Hebudes", and makes a separate reference to "Dumna", which Watson (1926) concludes is unequivocally the Outer Hebrides. Writing about 80 years later, in 140-150 AD, Ptolemy, drawing on the earlier naval expeditions of Agricola, also distinguished between the "Ebudes", of which he writes there were only five (and thus possibly meaning the Inner Hebrides) and Dumna. Dumna is cognate with the Early Celtic "dumnos" and means the "deep-sea isle". Pliny probably took his information from Pytheas of Massilia who visited Britain sometime between 322 and 285 BC. It is possible that Ptolemy did as well, as Agricola's information about the west coast of Scotland was of poor quality. Breeze also suggests that Dumna might be Lewis and Harris, the largest island of the Outer Hebrides although he conflates this single island with the name "Long Island". Watson (1926) states that the meaning of Ptolemy's "Eboudai" is unknown and that the root may be pre-Celtic. Murray (1966) claims that Ptolemy's "Ebudae" was originally derived from the Old Norse "Havbredey", meaning "isles on the edge of the sea". This idea is often repeated but no firm evidence of this derivation has emerged.
Other early written references include the flight of the Nemed people from Ireland to "Domon", which is mentioned in the 12th-century "Lebor Gabála Érenn" and a 13th-century poem concerning Raghnall mac Gofraidh, then the heir to the throne of Mann and the Isles, who is said to have "broken the gate of "Magh Domhna"". "Magh Domhna" means "the plain of Domhna (or Domon)", but the precise meaning of the text is not clear.
In Irish mythology the islands were the home of the Fomorians, described as "huge and ugly" and "ship men of the sea". They were pirates, extracting tribute from the coasts of Ireland and one of their kings was Indech mac Dé Domnand (i.e. Indech, son of the goddess Domnu, who ruled over the deep seas).
In modern Gaelic the islands are sometimes referred to collectively as "An t-Eilean Fada" (the Long Island) or "Na h-Eileanan a-Muigh" (the Outer Isles). "Innse Gall" (islands of the foreigners or strangers) is also heard occasionally, a name that was originally used by mainland Highlanders when the islands were ruled by the Norse.
The individual island and place names in the Outer Hebrides have mixed Gaelic and Norse origins. Various Gaelic terms are used repeatedly:
There are also several islands called "Orasaigh" from the Norse "Örfirirsey" meaning "tidal" or "ebb island".
History.
In Scotland, the Celtic Iron Age way of life, often troubled but never extinguished by Rome, re-asserted itself when the legions abandoned any permanent occupation in 211 AD. Hanson (2003) writes: "For many years it has been almost axiomatic in studies of the period that the Roman conquest must have had some major medium or long-term impact on Scotland. On present evidence that cannot be substantiated either in terms of environment, economy, or, indeed, society. The impact appears to have been very limited. The general picture remains one of broad continuity, not of disruption ... The Roman presence in Scotland was little more than a series of brief interludes within a longer continuum of indigenous development." The Romans' direct impact on the Highlands and Islands was scant and there is no evidence that they ever actually landed in the Outer Hebrides.
The later Iron Age inhabitants of the northern and western Hebrides were probably Pictish, although the historical record is sparse. Hunter (2000) states that in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence." The island of Pabbay is the site of the Pabbay Stone, the only extant Pictish symbol stone in the Outer Hebrides. This 6th century stele shows a flower, V-rod and lunar crescent to which has been added a later and somewhat crude cross.
Norse control.
Viking raids began on Scottish shores towards the end of the 8th century AD and the Hebrides came under Norse control and settlement during the ensuing decades, especially following the success of Harald Fairhair at the Battle of Hafrsfjord in 872. In the Western Isles Ketill Flatnose was the dominant figure of the mid 9th century, by which time he had amassed a substantial island realm and made a variety of alliances with other Norse leaders. These princelings nominally owed allegiance to the Norwegian crown, although in practice the latter's control was fairly limited. Norse control of the Hebrides was formalised in 1098 when Edgar of Scotland formally signed the islands over to Magnus III of Norway. The Scottish acceptance of Magnus III as King of the Isles came after the Norwegian king had conquered Orkney, the Hebrides and the Isle of Man in a swift campaign earlier the same year, directed against the local Norwegian leaders of the various islands petty kingdoms. By capturing the islands Magnus imposed a more direct royal control, although at a price. His skald Bjorn Cripplehand recorded that in Lewis "fire played high in the heaven" as "flame spouted from the houses" and that in the Uists "the king dyed his sword red in blood". Thompson (1968) provides a more literal translation: "Fire played in the fig-trees of Liodhus; it mounted up to heaven. Far and wide the people were driven to flight. The fire gushed out of the houses".
The Hebrides were now part of Kingdom of the Isles, whose rulers were themselves vassals of the Kings of Norway. The Kingdom had two parts: the "Suðr-eyjar" or South Isles encompassing the Hebrides and the Isle of Man; and the "Norðr-eyjar" or North Isles of Orkney and Shetland. This situation lasted until the partitioning of the Western Isles in 1156, at which time the Outer Hebrides remained under Norwegian control while the Inner Hebrides broke out under Somerled, the Norse-Celtic kinsman of the Manx royal house.
Following the ill-fated 1263 expedition of Haakon IV of Norway, the Outer Hebrides along with the Isle of Man, were yielded to the Kingdom of Scotland a result of the 1266 Treaty of Perth. Although their contribution to the islands can still be found in personal and placenames, the archaeological record of the Norse period is very limited. The best known find from this time is the Lewis chessmen, which date from the mid 12th century.
Scots rule.
As the Norse era drew to a close the Norse-speaking princes were gradually replaced by Gaelic-speaking clan chiefs including the MacLeods of Lewis and Harris, the MacDonalds of the Uists and MacNeil of Barra. This transition did little to relieve the islands of internecine strife although by the early 14th century the MacDonald Lords of the Isles, based on Islay, were in theory these chiefs' feudal superiors and managed to exert some control.
The growing threat that Clan Donald posed to the Scottish crown led to the forcible dissolution of the Lordship of the Isles by James IV in 1493, but although the king had the power to subdue the organised military might of the Hebrides, he and his immediate successors lacked the will or ability to provide an alternative form of governance. The House of Stuart's attempts to control the Outer Hebrides were then at first desultory and little more than punitive expeditions. In 1506 the Earl of Huntly besieged and captured Stornoway Castle using cannon. In 1540 James V himself conducted a royal tour, forcing the clan chiefs to accompany him. There followed a period of peace, but all too soon the clans were at loggerheads again.
In 1598 King James VI authorised some "Gentleman Adventurers" from Fife to civilise the "most barbarous Isle of Lewis". Initially successful, the colonists were driven out by local forces commanded by Murdoch and Neil MacLeod, who based their forces on Bearasaigh in Loch Ròg. The colonists tried again in 1605 with the same result but a third attempt in 1607 was more successful, and in due course Stornoway became a Burgh of Barony. By this time Lewis was held by the Mackenzies of Kintail, (later the Earls of Seaforth), who pursued a more enlightened approach, investing in fishing in particular. The historian W. C. MacKenzie was moved to write:
At the end of the seventeenth century, the picture we have of Lewis that of a people pursuing their avocation in peace, but not in plenty. The Seaforths ... , besides establishing orderly Government in the island.. had done a great deal to rescue the people from the slough of ignorance and incivility in which they found themselves immersed. But in the sphere of economics their policy apparently was of little service to the community.
The Seaforth's royalist inclinations led to Lewis becoming garrisoned during the Wars of the Three Kingdoms by Cromwell's troops, who destroyed the old castle in Stornoway and in 1645 Lewismen fought at the Battle of Auldearn. A new era of Hebridean involvement in the affairs of the wider world was about to commence.
British era.
With the implementation of the Treaty of Union in 1707 the Hebrides became part of the new Kingdom of Great Britain, but the clan's loyalties to a distant monarch were not strong. A considerable number of islandmen "came out" in support of the Jacobite Earl of Mar in the "15" although the response to the 1745 rising was muted. Nonetheless the aftermath of the decisive Battle of Culloden, which effectively ended Jacobite hopes of a Stuart restoration, was widely felt. The British government's strategy was to estrange the clan chiefs from their kinsmen and turn their descendants into English-speaking landlords whose main concern was the revenues their estates brought rather than the welfare of those who lived on them. This may have brought peace to the islands, but in the following century it came at a terrible price.
The Highland Clearances of the 19th century destroyed communities throughout the Highlands and Islands as the human populations were evicted and replaced with sheep farms. For example, Colonel Gordon of Cluny, owner of Barra, South Uist and Benbecula evicted thousands of islanders using trickery and cruelty and even offered to sell Barra to the government as a penal colony. Islands such as Fuaigh Mòr were completely cleared of their populations and even today the subject is recalled with bitterness and resentment in some areas. The position was exacerbated by the failure of the islands' kelp industry, which thrived from the 18th century until the end of the Napoleonic Wars in 1815 and large scale emigration became endemic. For example, hundreds left North Uist for Cape Breton, Nova Scotia. The pre-clearance population of the island had been almost 5,000, although by 1841 it had fallen to 3,870 and was only 2,349 by 1931.
For those who remained new economic opportunities emerged through the export of cattle, commercial fishing and tourism. During the summer season in the 1860s and 1870s five thousand inhabitants of Lewis could be found in Wick on the mainland of Scotland, employed on the fishing boats and at the quaysides. Nonetheless emigration and military service became the choice of many and the archipelago's populations continued to dwindle throughout the late 19th and 20th centuries. By 2001 the population of North Uist was only 1,271.
The work of the Napier Commission and the Congested Districts Board, and the passing of the Crofting Act of 1886 helped, but social unrest continued. In July 1906 grazing land on Vatersay was raided by landless men from Barra and its isles. Lady Gordon Cathcart took legal action against the "raiders" but the visiting judge took the view that she had neglected her duties as a landowner and that "long indifference to the necessities of the cottars had gone far to drive them to exasperation". Millennia of continuous occupation notwithstanding, many of the remoter islands were abandoned - Mingulay in 1912, Hirta in 1930, and Ceann Iar in 1942 among them. This process involved a transition from these places being perceived as relatively self-sufficient agricultural economies to a view becoming held by both island residents and outsiders alike that they lacked the essential services of a modern industrial economy.
There were gradual economic improvements, among the most visible of which was the replacement of the traditional thatched blackhouse with accommodation of a more modern design. The creation of the Highlands and Islands Development Board and the discovery of substantial deposits of North Sea oil in 1965, the establishment of a unitary local authority for the islands in 1975 and more recently the renewables sector have all contributed to a degree of economic stability in recent decades. The Arnish yard has had a chequered history but has been a significant employer in both the oil and renewables industries. Comhairle nan Eilean Siar, the local authority, employs 2,000 people, making it the largest employer in the Outer Hebrides. See also the "Innse Gall area plan 2010" and the Comhairle's "Factfile - Economy".
Economy.
Modern commercial activities centre on tourism, crofting, fishing, and weaving including the manufacture of Harris tweed. Some of the larger islands have development trusts that support the local economy and, in striking contrast to the 19th and 20th century domination by absentee landlords, more than two thirds of the Western Isles population now lives on community-owned estates. However the economic position of the islands remains relatively precarious. The Western Isles, including Stornoway, are defined by Highlands and Islands Enterprise as an economically "Fragile Area" and they have an estimated trade deficit of some £163.4 million. Overall, the area is relatively reliant on primary industries and the public sector, and fishing and fish farming in particular are vulnerable to environmental impacts, changing market pressures and European legislation.
There is some optimism about the possibility of future developments in for example, renewable energy generation, tourism, and education, and after declines in the 20th century the population has stabilised since 2003, although it is ageing.
Politics and local government.
From the passing of the Local Government (Scotland) Act 1889 to 1975 Lewis formed part of the county of Ross and Cromarty and the rest of the archipelago, including Harris, was part of Inverness-shire.
The Outer Hebrides became a unitary council area in 1975, although in most of the rest of Scotland similar unitary councils were not established until 1996. Since then, the islands have formed one of the 32 unitary council areas that now cover the whole country, with the council officially known by its Gaelic name, "Comhairle nan Eilean Siar" under the terms of the Local Government (Gaelic Names) (Scotland) Act 1997. The council has its base in Stornoway on Lewis and is often known locally simply as "the Comhairle" or "a' Chomhairle". The Comhairle is one of only three Councils in Scotland with a majority of elected members who are independents. The other independent run Councils are Shetland and Orkney. Moray is run by a Conservative/Independent coalition.
The name for the British Parliament constituency covering this area is Na h-Eileanan an Iar, the seat being held by Angus MacNeil MP since 2005, while the Scottish Parliament constituency for the area is Western Isles, the incumbent being Alasdair Allan MSP.
The Scottish independence referendum has led some islanders to call for a debate on the constitutional position of the Western Isles, linked with similar initiatives in Orkney and Shetland.
Gaelic language.
The Outer Hebrides have historically been a very strong Scottish Gaelic ("Gàidhlig") speaking area. Both the 1901 and 1921 census reported that all parishes were over 75% Gaelic speaking, including areas of high population density such as Stornoway. However, the Education (Scotland) Act 1872 led to generations of Gaels being forbidden to speak their native language in the classroom, and is now recognised as having dealt a major blow to the language. People still living can recall being beaten for speaking Gaelic in school. Nonetheless, by 1971 most areas were still more than 75% Gaelic speaking – with the exception of Stornoway, Benbecula and South Uist at 50-74%.
In the 2001 census, each island overall was over 50% Gaelic speaking – South Uist (71%), Harris (69%), Barra (68%), North Uist (67%), Lewis (56%) and Benbecula (56%). With 59.3% of Gaelic speakers or a total of 15,723 speakers, this made the Outer Hebrides the most strongly coherent Gaelic speaking area in Scotland.
Most areas were between 60-74% Gaelic speaking and the areas with the highest density of over 80% are Scalpay near Harris, Newtonferry and Kildonan, whilst Daliburgh, Linshader, Eriskay, Brue, Boisdale, West Harris, Ardveenish, Soval, Ness, and Bragar all have more than 75%. The areas with the lowest density of speakers are Stornoway (44%), Braigh (41%), Melbost (41%), and Balivanich (37%).
The Gaelic Language (Scotland) Act was enacted by the Scottish Parliament in 2005 to provide continuing support for the language. However, by 2011 the overall percentage of Gaelic speakers in the Outer Hebrides had fallen to 52%.
Transport.
Scheduled ferry services between the Outer Hebrides and the Scottish Mainland and Inner Hebrides operate on the following routes:
Other ferries operate between some of the islands.
National Rail services are available for onward journeys, from stations at Oban, which has direct services to Glasgow. However, parliamentary approval notwithstanding, plans in the 1890s to lay a railway connection to Ullapool were unable to obtain sufficient funding.
There are scheduled flights from Stornoway, Benbecula and Barra airports both inter-island and to the mainland. Barra's airport is the only one in the world to have scheduled flights landing on a beach. At high water the runways are under the sea so flight times vary with the tide.
Bus services are operated by Bus na Comhairle, based in Stornoway, and by numerous other operators, which run the bulk of services.
Shipwrecks.
The archipelago is exposed to wind and tide, and lighthouses are sited as an aid to navigation at locations from Barra Head in the south to the Butt of Lewis in the north. There are numerous sites of wrecked ships, and the Flannan Isles are the location of an enduring mystery that occurred in December 1900, when all three lighthouse keepers vanished without trace.
The "Annie Jane", a three-masted immigrant ship out of Liverpool bound for Montreal, Canada, struck rocks off the West Beach of Vatersay during a storm on Tuesday 28 September 1853. Within ten minutes the ship began to founder and break up casting 450 people into the raging sea. In spite of the conditions, islanders tried to rescue the passengers and crew. The remains of 350 men, women and children were buried in the dunes behind the beach and a small cairn and monument marks the site.
The tiny Beasts of Holm of the east coast of Lewis were the site of the sinking of the during the first few hours of 1919, one of the worst maritime disasters in United Kingdom waters during the 20th century. Calvay in the Sound of Barra provided the inspiration for Compton MacKenzie's novel "Whisky Galore" after the ran aground there with a cargo of single malt in 1941.
Religion, culture and sport.
Christianity has deep roots in the Western Isles, but owing mainly to the different allegiances of the clans in the past, the people in the northern islands (Lewis, Harris, North Uist) have historically been predominantly Presbyterian, and those of the southern islands (Benbecula, South Uist, Barra) predominantly Roman Catholic.
At the time of the 2001 Census, 42% of the population identified themselves as being affiliated with the Church of Scotland, with 13% Roman Catholic and 28% with other Christian churches. Many of this last group belong to the Free Church of Scotland, known for its strict observance of the Sabbath. 11% stated that they had no religion. There are also small Episcopalian congregations in Lewis and Harris and the Outer Hebrides are part of the Diocese of Argyll and The Isles in both the Episcopalian and Catholic traditions.
Gaelic music is popular in the islands and the Lewis and Harris Traditional Music Society plays an active role in promoting the genre. Fèis Bharraigh began in 1981 with the aim of developing the practice and study of the Gaelic language, literature, music, drama and culture on the islands of Barra and Vatersay. A two-week festival, it has inspired 43 other "feisean" throughout Scotland. The Lewis Pipe Band was founded in 1904 and the Lewis and Harris Piping Society in 1977.
Outdoor activities including rugby, football, golf, shinty, fishing, riding, canoeing, athletics, and multi-sports are popular in the Western Isles. The Hebridean Challenge is an adventure race run in five daily stages, which takes place along the length of the islands and includes hill and road running, road and mountain biking, short sea swims and demanding sea kayaking sections. There are four main sports centres: "Ionad Spors Leodhais" in Stornoway, which has a 25 m swimming pool; Harris Sports Centre; Lionacleit Sports Centre on Benbecula; and Castlebay Sports Centre on Barra. The Western Isles is a member of the International Island Games Association.
South Uist is home to the Askernish Golf Course. The oldest links in the Outer Hebrides, it was designed by Old Tom Morris. Although it was in use until the 1930s, its existence was largely forgotten until 2005 and it is now being restored to Morris's original design.
Further reading.
</dl>

</doc>
<doc id="40411" url="http://en.wikipedia.org/wiki?curid=40411" title="Mnemonic">
Mnemonic

A mnemonic (: , : the first "m" is silent), or mnemonic device, is any learning technique that aids information retention. Mnemonics aim to translate information into a form that the brain can retain better than its original form. Even the process of merely learning this conversion might already aid in the transfer of information to long-term memory. Commonly encountered mnemonics are often used for lists and in auditory form, such as short poems, acronyms, or memorable phrases, but mnemonics can also be used for other types of information and in visual or kinesthetic forms. Their use is based on the observation that the human mind more easily remembers spatial, personal, surprising, physical, sexual, humorous, or otherwise "relatable" information, rather than more abstract or impersonal forms of information.
The word "mnemonic" is derived from the Ancient Greek word μνημονικός ("mnēmonikos"), meaning "of memory, or relating to memory" and is related to Mnemosyne ("remembrance"), the name of the goddess of memory in Greek mythology. Both of these words are derived from μνήμη ("mnēmē"), "remembrance, memory". Mnemonics in antiquity were most often considered in the context of what is today known as the art of memory.
Ancient Greeks and Romans distinguished between two types of memory: the "natural" memory and the "artificial" memory. The former is inborn, and is the one that everyone uses automatically and without thinking. The artificial memory in contrast has to be trained and developed through the learning and practicing of a variety of mnemonic techniques.
Mnemonic systems are special techniques or strategies consciously used to improve memory. They help employ information already stored in long-term memory to make memorisation an easier task.
"Memory Needs Every Method Of Nurturing Its Capacity" is a mnemonic for how to spell mnemonic.
History.
The general name of mnemonics, or "memoria technica", was the name applied to devices for aiding the memory, enabling the mind to reproduce a relatively unfamiliar idea, and especially a series of dissociated ideas, by connecting it, or them, in some artificial whole, the parts of which are mutually suggestive. Mnemonic devices were much cultivated by Greek sophists and philosophers and are repeatedly referred to by Plato and Aristotle. In later times the invention was ascribed to the poet Simonides, perhaps for no other reason than that the strength of his memory was famous. Cicero, who attaches considerable importance to the art, but more to the principle of order as the best help to memory, speaks of Carneades (or perhaps Charmades) of Athens and Metrodorus of Scepsis as distinguished examples of the use of well-ordered images to aid the memory. The Romans valued such helps as giving facility in public speaking.
The Greek and the Roman system of mnemonics was founded on the use of mental places and signs or pictures, known as "topical" mnemonics. The most usual method was to choose a large house, of which the apartments, walls, windows, statues, furniture, etc., were severally associated with certain names, phrases, events or ideas, by means of symbolic pictures; and to recall these it was only necessary to search over the apartments of the house till the particular place was discovered where they had been deposited by the imagination.
In accordance with said system, if it were desired to fix an historic date in memory, it was localised in an imaginary town divided into a certain number of districts, each of with ten houses, each house with ten rooms, and each room with a hundred quadrates or memory-places, partly on the floor, partly on the four walls, partly on the roof. Therefore, if it were desired to fix in the memory the date of the invention of printing (1436), an imaginary book, or some other symbol of printing, would be placed in the thirty-sixth quadrate or memory-place of the fourth room of the first house of the historic district of the town. Except that the rules of mnemonics are referred to by Martianus Capella, nothing further is known regarding the practice until the 13th century.
Among the voluminous writings of Roger Bacon is a tractate "De arte memorativa". Ramon Llull devoted special attention to mnemonics in connection with his "ars generalis". The first important modification of the method of the Romans was that invented by the German poet Konrad Celtes, who, in his "Epitoma in utramque Ciceronis rhetoricam cum arte memorativa nova" (1492), instead of places made use of the letters of the alphabet. About the end of the 15th century Petrus de Ravenna (b. 1448) created such an astonishment in Italy by his mnemonic feats that he was believed by many to be a necromancer. His "Phoenix artis memoriae" (Venice, 1491, 4 vols.) went through as many as nine editions, the seventh appearing at Cologne in 1608.
An impression equally great was produced about the end of the 16th century by Lambert Schenkel ("Gazophylacium", 1610), who taught mnemonics in France, Italy and Germany, and, although he was denounced as a sorcerer by the University of Louvain, published in 1593 his tractate "De memoria" at Douai with the sanction of that celebrated theological faculty. The most complete account of his system is given in two works by his pupil Martin Sommer, published in Venice in 1619. In 1618 John Willis (d. 1628?) published "Mnemonica; sive ars reminiscendi", containing a clear statement of the principles of topical or local mnemonics. Giordano Bruno, in connection with his exposition of the "ars generalis" of Llull, included a "memoria technica" in his treatise "De umbris idearum". Other writers of this period are the Florentine Publicius (1482); Johannes Romberch (1533); Hieronimo Morafiot, "Ars memoriae" (1602); B. Porta, "Ars reminiscendi" (1602).
In 1648 Stanislaus Mink von Wennsshein made known what he called the "most fertile secret" in mnemonics — namely the use of consonants for figures, so as to express numbers by words (vowels being added as required); and the philosopher Gottfried Wilhelm Leibniz adopted an alphabet very similar to that of Wennsshein in connection with his scheme for a form of writing common to all languages. Wennsshein's method, which in fact is adopted with slight changes by the majority of subsequent "original" systems, was modified and supplemented in regard to many details by Richard Grey (1694-1771), who published a "Memoria technica" in 1730. The principal part of Grey's method (which may be compared with the Jewish system by which letters also stand for numerals, and therefore words for dates) is briefly this:
To remember anything in history, chronology, geography, etc., a word is formed, the beginning whereof, being the first syllable or syllables of the thing sought, does, by frequent repetition, of course draw after it the latter part, which is so contrived as to give the answer. Thus, in history, the Deluge happened in the year before Christ two thousand three hundred forty-eight; this is signified by the word Del-"etok", Del standing for Deluge and "etok" for 2348.
To assist in retaining the mnemonical words in the memory, they were formed into memorial lines, which, however, being composed of strange words in difficult hexameter scansion, are by no means easy to memorise. The vowel or consonant, which Grey connected with a particular figure, was chosen arbitrarily; but in 1806 Gregor von Feinaigle, a German monk from Salem near Constance, began in Paris to expound a system of mnemonics, one feature (based on Wennsshein's system) of which was to represent the numerical figures by letters chosen on account of some similarity to the figure to be represented or some accidental connection with it. This alphabet was supplemented by a complicated system of localities and signs. Feinaigle, who apparently published nothing himself, came to England in 1811, and in the following year one of his pupils published "The New Art of Memory", which, beside giving Feinaigle's system, contains valuable historical material about previous systems.
Simplified forms were published later by other mnemonists, as the more complicated ones fell almost into complete disuse; but methods founded chiefly on the so-called laws of association (cf. Mental association) were taught with some success in Germany.
Applications.
A wide range of mnemonics are used for several purposes. The most commonly used mnemonics are those for lists, numerical sequences, foreign-language acquisition, and medical treatment for patients with memory deficits.
For lists.
A common mnemonic for remembering lists is to create an easily remembered acronym, or, taking each of the initial letters of the list members, create a memorable phrase in which the words with the same acronym as the material. Anyone can create their own mnemonics to aid the memorisation of novel material.
Some common examples for first letter mnemonics:
For numerical sequences.
Mnemonic phrases or poems can be used to encode numeric sequences by various methods, one common one is to create a new phrase in which the number of letters in each word represents the according digit of pi. For example, the first 15 digits of the mathematical constant pi (3.14159265358979) can be encoded as "Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics"; "Now", having 3 letters, represents the first number, 3. Piphilology is the practice dedicated to creating mnemonics for pi.
Another is used for "calculating" the multiples of 9 up to 9 × 10 using one's fingers. Begin by holding out both hands with all fingers stretched out. Now count left to right the number of fingers that indicates the multiple. For example to figure 9 × 4, count four fingers from the left, ending at your left-hand index finger. Bend this finger down and count the remaining fingers. Fingers to the left of the bent finger represent tens, fingers to the right are ones. There are three fingers to the left and six to the right, which indicates 9 × 4 = 36. This works for 9 × 1 up through 9 × 10.
For foreign-language acquisition.
Mnemonics may be helpful in learning foreign languages, for example by transposing difficult foreign words with words in a language the learner knows already, also called "cognates" which are very common in the Spanish language. A useful such technique is to find linkwords, words that have the same pronunciation in a known language as the target word, and associate them visually or auditorially with the target word.
For example, in trying to assist the learner to remember ohel, the Hebrew word for "tent", the memorable sentence "Oh hell, there's a raccoon in my "tent"" can be used. Also in Hebrew, a way to remember the word, bayit (bahy- it), meaning house, one can use the sentence "that's a lovely house, I'd like to bayit." The linguist Michel Thomas taught students to remember that "estar" is the Spanish word for "to be" by using the phrase "to be a star".
Another Spanish example is by using the mnemonic "Vin Diesel Has Ten Weapons" to teach irregular command verbs in the you form. Spanish verb forms and tenses are regularly seen as the hardest part of learning the language. With a high number of verb tenses, and many verb forms that are not found in English, Spanish verbs can be hard to remember and then conjugate. The use of mnemonics has been proven to help students better learn foreign languages, and this holds true for Spanish verbs. A particularly hard verb tense to remember is command verbs. Command verbs in Spanish are conjugated differently depending on who the command is being given to. The phrase, when pronounced with a Spanish accent, is used to remember "Ven Di Sal Haz Ten Ve Pon Se", all of the irregular Spanish command verbs in the you form. This mnemonic helps students attempting to memorize different verb tenses.
Another technique is for learners of gendered languages to associate their mental images of words with a colour that matches the gender in the target language. An example here is to remember the Spanish word for "foot," "pie," [pee-ay] with the image of a foot stepping on a pie, which then spills blue filling (blue representing the male gender of the noun in this example).
For patients with memory deficits.
Mnemonics can be used in aiding patients with memory deficits that could be caused by head injuries, strokes, epilepsy, multiple sclerosis and other neurological conditions.
In a study conducted by Doornhein and De Haan, the patients were treated with six different memory strategies including the mnemonics technique. The results concluded that there were significant improvements on the immediate and delayed subtest of the RBMT, delayed recall on the Appointments test, and relatives rating on the MAC from the patients that received mnemonics treatment. However, in the case of stroke patients, the results did not reach statistical significance. 
Effectiveness.
Academic study of the use of mnemonics has shown their effectiveness. In one such experiment, subjects of different ages who applied mnemonic techniques to learn novel vocabulary outperformed control groups that applied contextual learning and free-learning styles.
Mnemonics vary in effectiveness for several groups ranging from young children to the elderly. Mnemonic learning strategies require time and resources by educators to develop creative and effective devices. The most simple and creative mnemonic devices usually are the most effective for teaching. In the classroom, mnemonic devices must be used at the appropriate time in the instructional sequence to achieve their maximum effectiveness.
Mnemonics were seen to be more effective for groups of people who struggled with or had weak long-term memory, like the elderly. Five years after a mnemonic training study, a research team followed-up 112 community-dwelling older adults, 60 years of age and over. Delayed recall of a word list was assessed prior to, and immediately following mnemonic training, and at the 5-year follow-up. Overall, there was no significant difference between word recall prior to training and that exhibited at follow-up. However, pre-training performance gains scores in performance immediately post-training and use of the mnemonic predicted performance at follow-up. Individuals who self-reported using the mnemonic exhibited the highest performance overall, with scores significantly higher than at pre-training. The findings suggest that mnemonic training has long-term benefits for some older adults, particularly those who continue to employ the mnemonic. 
This greatly contrasts with a study where the results showed from surveys of medical students that approximately only 20% frequently used mnemonic acronyms. Although the majority of a certain age group can benefit from the use of mnemonics, not everyone can learn best using them.
In humans, the process of aging particularly affects the medial temporal lobe and hippocampus, in which the episodic memory is synthesized. The episodic memory stores information about items, objects, or features with spatiotemporal contexts. Since mnemonics aid better in remembering spatial or physical information rather than more abstract forms, its effect may vary according to the subjects' age and how well do their medial temporal lobe and hippocampus function.
This could be further explained by one recent study which indicates a general deficit in the memory for spatial locations in aged adults (Mean age 69.7 with standard deviation of 7.4 years) compared to young adults (Mean age 21.7 with standard deviation of 4.2 years). Interestingly, at first, the difference in target recognition was not significant.
The researchers then divided the aged adults into two groups, aged unimpaired and aged impaired, according to a neuropsychological testing. With the aged groups split, there was an apparent deficit in target recognition in aged impaired adults compared to both young adults and aged unimpaired adults. This further supports the varying effectiveness of mnemonics in different age groups. 
Moreover, a different research was done previously with the same notion, which presented with similar results to that of Reagh et al. in verbal mnemonics discrimination task
Studies (notably "The Magical Number Seven, Plus or Minus Two") have suggested that the short-term memory of adult humans can hold only a limited number of items; grouping items into larger chunks such as in a mnemonic might be part of what permits the brain to hold a larger total amount of information in short-term memory, which in turn can aid the creation of long-term memories.

</doc>
<doc id="40412" url="http://en.wikipedia.org/wiki?curid=40412" title="License">
License

The verb license or grant license means to give permission. The noun license (American English) or licence (British, Indian, Canadian, Australian, New Zealand, Irish, or South African English) refers to that permission as well as to the document recording that permission.
A license may be granted by a party ("licensor") to another party ("licensee") as an element of an agreement between those parties. A shorthand definition of a license is "an authorization (by the licensor) to use the licensed material (by the licensee)."
In particular, a license may be issued by authorities, to allow an activity that would otherwise be forbidden. It may require paying a fee and/or proving a capability. The requirement may also serve to keep the authorities informed on a type of activity, and to give them the opportunity to set conditions and limitations.
A licensor may grant a license under intellectual property laws to authorize a use (such as copying software or using a (patented) invention) to a licensee, sparing the licensee from a claim of infringement brought by the licensor. A license under intellectual property commonly has several components beyond the grant itself, including a term, territory, renewal provisions, and other limitations deemed vital to the licensor.
Term: many licenses are valid for a particular length of time. This protects the licensor should the value of the license increase, or market conditions change. It also preserves enforceability by ensuring that no license extends beyond the term of the agreement.
Territory: a license may stipulate what territory the rights pertain to. For example, a license with a territory limited to "North America" (Mexico/United States/Canada) would not permit a licensee any protection from actions for use in Japan.
A shorthand definition of license is "a promise by the licensor not to sue the licensee." That means without a license any use or exploitation of intellectual property by a third party would amount to copying or infringement. Such copying would be improper and could, by using the legal system, be stopped if the intellectual property owner wanted to do so.
Intellectual property licensing plays a major role in business, academia and broadcasting. Business practices such as franchising, technology transfer, publication and character merchandising entirely depend on the licensing of intellectual property. Land licensing (proprietary licensing) and IP licensing form sub-branches of law born out of the interplay of general laws of contract and specific principles and statutory laws relating to these respective assets.
Mass licensing of software.
Mass distributed software is used by individuals on personal computers under license from the developer of that software. Such license is typically included in a more extensive end-user license agreement (EULA) entered into upon the installation of that software on a computer.
Typically, a license is associated with a unique code, that when approved grants the end user access to the software in question.
Under a typical end-user license agreement, the user may install the software on a limited number of computers.
The enforceability of end-user license agreements is sometimes questioned.
Trademark and brand licensing.
A licensor may grant permission to a licensee to distribute products under a trademark. With such a license, the licensee may use the trademark without fear of a claim of trademark infringement by the licensor.
The assignment of a license often depends on specific contractual terms. The most common terms are, that a license is only applicable for a particular geographic region, just for a certain period of time or merely for a stage in the value chain.
Moreover there are different types of fees within the trademark and brand licensing. The first form demands a fee independent of sales and profits, the second type of license fee is dependent on the productivity of the licensee.
Artwork and character licensing.
A licensor may grant a permission to a licensee to copy and distribute copyrighted works such as "art" (e.g., Thomas Kinkade's painting "Dawn in Los Gatos") and characters (e.g., Mickey Mouse). With such license, a licensee need not fear a claim of copyright infringement brought by the copyright owner.
Artistic license is, however, not related to the aforementioned license. It is a euphemism that denotes freedom of expression, the ability to make the subject appear more engaging or attractive, by fictionalising part of the subject.
Academia.
A license is an academic degree. Originally, in order to teach at a university, one needed this degree which, according to its title, gave the bearer a license to teach. The name survived despite the fact that nowadays a doctorate is typically needed in order to teach at a university. A person who holds a license is called a licentiate.
In Sweden, Finland, and in some other European university systems, a license or 'Licentiate' is a postgraduate degree between the master's degree and the doctorate. The Licentiate is a popular choice in those countries where a full doctoral degree would take five or more years to achieve.
In some other major countries, such as France, Belgium or Poland, a license is achieved before the master's degree (it takes 3 years of studies to become licentiate and 2 additional years to become Master) in France, while in Belgium the license takes 4 years while the master itself takes 2 more years. In Switzerland, a license is a 4-year degree then there is a DEA degree which is equivalent to the Master's degree. In Portugal, before the Bologna process, students would become licentiates after 5 years of studies (4 years in particular cases like Marketing, Management, etc.; and 6 years for Medicine). However, since the adoption of the Bologna Process engineering degrees in Portugal were changed from a 5-year license to a 3-year license followed by 2 years for the MSc: Not having the MSc does not confer accreditation by the Ordem dos Engenheiros.
See also.
Intellectual property-related:
Other:

</doc>
<doc id="40413" url="http://en.wikipedia.org/wiki?curid=40413" title="Perfectae Caritatis">
Perfectae Caritatis

Perfectæ Caritatis, the Decree on the Adaptation and Renewal of Religious Life, is the document issued by the Second Vatican Council which deals specifically with institutes of consecrated life in the Roman Catholic Church. One of the shorter documents of the Council, the decree was approved by a vote of 2,321 to 4 of the assembled bishops, and promulgated by Pope Paul VI on 28 October 1965. As is customary for Church documents, the title is taken from the first words (incipit) of the decree: "of Perfect Charity" in Latin.
Content.
The Second Vatican Council had already given an exposition of the nature of religious life in chapter 6 of the Constitution "Lumen gentium". This chapter described the essential form of religious life as a life "consecrated by the profession of the evangelical counsels" (n. 44). The Decree "Perfectae Caritatis" was published in order to, "treat of the life and discipline of those institutes whose members make profession of chastity, poverty and obedience and to provide for their needs in our time" ("Perfectae Caritatis" n. 1).
Containing 25 numbered paragraphs, the Decree established five general principles to guide the renewal of these institutes. Because of the broad variety of religious communities with their different histories, characteristics, customs, and missions, the Vatican Council did not give specific indications, and left to each individual community the authority to determine what needed to be changed in accord with the spirit of their founders, the needs of modern life, and the situations where they lived and worked.
Controversy.
The period that followed the promulgation of "Perfectae Caritatis" was marked by a huge amount of experimentation in religious life. Many institutes replaced their traditional habits with more modern attire, experimented with different forms of prayer and community life, and adapted obedience to a superior to a form of consultation and discussion. A great number of religious left religious life entirely, and in subsequent decades there was a large drop the number of religious vocations in the Western World. It is not clear how much of this change was due to the documents of the Second Vatican Council. Historians note that western society as a whole was going through social turmoil caused by the sexual revolution.

</doc>
<doc id="40414" url="http://en.wikipedia.org/wiki?curid=40414" title="Optatam Totius">
Optatam Totius

Optatam Totius, the Decree on Priestly Training, was a document produced by the Second Vatican Council. Approved by a vote of 2,318 to 3 of the bishops assembled at the council, the decree was promulgated by Pope Paul VI on October 28, 1965. (The full text in English is available from the .)
Contents.
"The numbers given correspond to the section numbers within the text."
"Optatam Totius" influenced German Roman Catholic theologian Karl Rahner to write his work "The Foundation of Christian Faith".
Controversy.
The period that followed the promulgation Optatam Totius was marked by a severe drop in the number of priestly vocations in the Western World. Church leaders had argued that age-old secularization was to blame and that it was not directly related to the documents of the Council. Historians have also pointed to the damage caused by the sexual revolution in 1968 and the strong backlash over "Humanae vitae". Yet other authors have asserted that the drop in vocations was at least partly deliberate and was part of an attempt to de-clericalize the Church and allow for a more pluralistic clergy.

</doc>
<doc id="40415" url="http://en.wikipedia.org/wiki?curid=40415" title="Jean Baptiste Point du Sable">
Jean Baptiste Point du Sable

Jean Baptiste Point du Sable (or Point de Sable, Point au Sable, Point Sable, Pointe DuSable) (before 1750 – August 28, 1818) is regarded as the first permanent resident of what became Chicago, Illinois. Little is known of his life prior to the 1770s. In 1779, he was living on the site of present-day Michigan City, Indiana, when he was arrested by the British military on suspicion of being an American sympathizer in the American Revolutionary War. In the early 1780s he worked for the British lieutenant-governor of Michilimackinac on an estate at what is now the city of St. Clair, Michigan, before moving to settle at the mouth of the Chicago River. He is first recorded living in Chicago in early 1790, having apparently become established sometime earlier. He sold his property in Chicago in 1800 and moved to St. Charles, Missouri, where he died on August 28, 1818.
Point du Sable has become known as the "Founder of Chicago". In Chicago, a school, museum, harbor, park and bridge have been named, or renamed, in his honor; and the place where he settled at the mouth of the Chicago River in the 1780s is recognized as a National Historic Landmark, now located in Pioneer Court.
Biography.
There is no known record of Point du Sable's life prior to the 1770s; his birth year, place of birth, and parents are unknown, though he is known from contemporary sources to have been of African descent. Juliette Kinzie, another early pioneer of Chicago, Illinois, never met Point du Sable but stated in her 1856 memoir that he was "a native of St. Domingo" (the island of Hispaniola). This became generally accepted by scholars as his place of birth. Historian Milo Milton Quaife, however, regarded Kinzie's account of Point du Sable as "largely fictitious and wholly unauthenticated". Quaife later put forward a theory that he was of French-Canadian origin. A historical novel published in 1953 (see below) helped to popularize the commonly recited claim that he was born in 1745 in Saint-Marc in Saint-Domingue (now Haiti). Point du Sable married a Potawatomi woman named Kitihawa (Christianized to Catherine) on October 27, 1788 in a Catholic ceremony – it is likely they were earlier married in the 1770s in the Native American tradition – they had a son named Jean and a daughter named Susanne.
In a footnote to a poem titled "Speech to the Western Indians", Arent DePeyster, British commandant at Fort Michilimackinac from 1774 to 1779 (a former French fort in what was by then the British Quebec Territory), noted that "Baptist Point de Saible" was "handsome", "well educated", and "settled in Eschecagou". When he published this poem in 1813, DePeyster presented it as a speech that he had made at the Indian village of Abercroche (now Harbor Springs, Michigan) on July 4, 1779. This footnote has led many scholars to assume that Point du Sable had settled in Chicago by 1779, however letters written by traders in the late 1770s suggest that Point du Sable was at this time settled at the mouth of Trail Creek ("Rivière du Chemin") at what is now Michigan City, Indiana. In August 1779, Point du Sable was arrested at Trail Creek by British troops and imprisoned briefly at Fort Michilimackinac. From the summer of 1780 until May 1784, Point du Sable managed the Pinery, a tract of woodlands claimed by British Lt. Patrick Sinclair on the St. Clair River in eastern Michigan. Point du Sable and his family lived at a cabin at the mouth of the Pine River in what is now the city of St. Clair.
Point du Sable settled on the north bank of the Chicago River close to its mouth at some time in the 1780s. The earliest known record of Point du Sable living in Chicago is an entry that Hugh Heward made in his journal on May 10, 1790 during a journey from Detroit across Michigan and through Illinois. Heward's party stopped at Pointe du Sable's house en route to the Chicago portage; they swapped their canoe for a pirogue that belonged to Point du Sable, and they bought bread, flour and pork from him. Perrish Grignon, who visited Chicago in about 1794, described Point du Sable as a large man who was a wealthy trader. In 1800 he sold his farm to John Kinzie's frontman, Jean La Lime, for 6,000 livres; the bill of sale, which was rediscovered in 1913 in an archive in Detroit, outlined all of the property Point du Sable owned as well as many of his personal artifacts. This included a house, two barns, a horse drawn mill, a bakehouse, a poultry house, a dairy and a smokehouse. The house was a 22 x log cabin filled with fine furniture and paintings.
After Point du Sable sold his property in Chicago he moved to St. Charles, Missouri, where he was commissioned by the colonial governor to operate a ferry across the Missouri River. He died in 1818, and was buried in St. Charles, in an unmarked grave in St. Charles Borromeo Cemetery. His entry in the parish burial register does not mention his origins, parents, or relatives, it simply describes him as "negre" (French for black). The St. Charles Borromeo Cemetery was moved twice in the 19th century, and oral tradition and records of the Archdiocese of St. Louis suggested that Point du Sable's remains were also moved. On October 12, 1968, the Illinois Sesquicentennial Commission erected a granite marker at the site believed to be Point du Sable's grave in the third St. Charles Borromeo Cemetery. In 2002 an archaeological investigation of the grave site was initiated by the African Scientific Research Institute at the University of Illinois at Chicago. Researchers using a combination of ground penetrating radar surveys and excavation of a 9 x area did not find any evidence of any burials at the supposed grave site, leading the archaeologists to conclude that Point du Sable's remains may not have been moved from one of the two previous cemeteries.
Theories and legends.
Early life.
Though there is little historical evidence regarding Point du Sable's life before the 1770s, there are a number of theories and legends that give accounts of his early life. Writing in 1933, Milo Milton Quaife identified a French immigrant to Canada, Pierre Dandonneau, who acquired the title "Sieur de Sable" and whose descendants were known by both the names "Dandonneau" and "Du Sable". Quaife was unable to find a direct link to Point du Sable, but identified descendants of Pierre Dandonneau living around the Great Lakes region in Detroit, Mackinac, and St Joseph, leading him to speculate that Point du Sable's father was a member of this family, whilst his mother was a slave. In 1951 a pamphlet by Joseph Jeremie, a native of Haiti, was published in which he claimed to be the great grandson of Point du Sable. Based on family recollections and tombstone inscriptions he claimed that Point du Sable was born in Saint-Marc in Haiti, studied in France, returned to Haiti to deal coffee before traveling to French Louisiana. Historian and Point du Sable biographer John F. Swenson has called these claims "elaborate, undocumented assertions ... in a fanciful biography". In 1953 Shirley Graham built on the work of Quaife and Jeremie in a historical novel of Point du Sable that she described as "not accurate history nor pure fiction", but rather "an imaginative interpretation of all the known facts". This book presented Point du Sable as the son of the mate on a pirate ship, the "Black Sea Gull", and a freed slave called Suzanne. Despite lack of evidence, and the continued debate about Point du Sable's early life, parentage, and birthplace, this popular story is widely presented as being definitive.
Peoria.
In 1815 a land claim that had been submitted by Nicholas Jarrot to the land commissioners at Kaskaskia, Illinois Territory was approved. In the claim Jarrot asserted that a "Jean Baptiste Poinstable" had been "head of a family at Peoria in the year 1783, and before and after that year", and that he "had a house built and cultivated land between the Old Fort and the new settlement in the year 1780". This document has been taken by Quaife and other historians as evidence that Point du Sable lived at Peoria prior to his arrival at Chicago, however, records show that Point du Sable was living at the Pinery in Michigan in the early 1780s. In addition, the Kaskaskia land commissioners identified many fraudulent land claims, including two previously submitted in the name of Point du Sable. Nicholas Jarrot, the claimant, was involved in many fraudulent land claims, and Swenson suggests that this claim was also fraudulent, made without the knowledge of Point du Sable.
Departure from Chicago.
Point du Sable left Chicago in 1800. Point du Sable sold his property to John Kinzie and moved to Missouri, at that time part of French Louisiana. The reason for his departure is unknown. In her memoir, Juliette Kinzie suggested that "perhaps he was disgusted at not being elected to a similar dignity [great chief] by the Pottowattamies." In 1874 Nehemiah Matson elaborated on this story, claiming that Point du Sable was a slave from Virginia who had moved with his master to Lexington, Kentucky in 1790. According to Matson, Point du Sable became a zealous Catholic in order to convince a Jesuit missionary to declare him chief of the local Native Americans, and left Chicago when the natives refused to accept him as their chief. Quaife dismisses both these stories as being fictional.
In her 1953 novel Graham suggests that Point du Sable left Chicago because he was angered with the United States government, which wanted him to buy the land on which he had lived and called his own for the previous two decades. The 1795 Treaty of Greenville and the subsequent westward migration of Indians away from the Chicago area might also have influenced his decision.
Legacy and honors.
Founder of Chicago.
Point du Sable is the earliest recorded resident of the settlement close to the mouth of the Chicago River that grew to become the city of Chicago. He is therefore widely regarded as the first permanent resident of Chicago and given the appellation "Founder of Chicago".
The expedition headed by Louis Jolliet and Jacques Marquette in 1673, though probably not the first Europeans to visit the area, are the first recorded to have crossed the Chicago Portage and travelled along the Chicago River. Marquette returned in 1674, camped a few days near the mouth of the river, then moved on to the portage, where he stayed through the winter of 1674–75. Joliet and Marquette did not report any Indians living near the Chicago River area at this time, though archaeologists have since discovered numerous Indian village sites elsewhere in the greater Chicago area. Two of La Salle's men built a stockade at the portage in the winter of 1682/1683.
However in 1697 Henri Tonti, Michel Accault, and François de La Forêt received permission from Governor Frontenac to establish a fortified trading post at Chicagou managed by Pierre de Liette, Tonti’s cousin, a Franco-Italian, which lasted until c.1705. De Liette kept a journal of his experiences living with the Illinois natives for those years he lived with them at the Chicago trading post. De Liette, describes in his writings the game of lacrosse played by the Indians on the extensive meadow behind these villages. In Chicago De Liette ran the trading post in partnership with François Daupin de La Forêt, Michel Accault, and Henri de Tonti [located probably near today`s Tribune Tower] which he had to close, leaving in 1705 after the king revoked his trading license.
The Mission of the Guardian Angel was somewhere in the vicinity of Chicago from 1696 until it was abandoned in around 1700. The Fox Wars effectively closed the Chicago area to Europeans in the first part of the 18th century. The first non-native to re-settle in the area may have been a trader named Guillory, who might have had a trading-post near Wolf Point on the Chicago River in around 1778. After Point du Sable, Antoine Ouilmette is the next recorded resident of Chicago; he claimed to have settled at the mouth of the Chicago River in July 1790, a few months after Hugh Heward visited Point du Sable.
Memorials.
[Point du Sable] is not yet honored in his own house (which Chicagoans call the "Kinzie House") or on his own land. No street bears his name and, save for the high school, he has no monument. Cadillac is honored in Detroit, Pitt in Pittsburgh, Cleveland in Cleveland—but the father of Chicago has no street or statue of stone to call his own.
 "Ebony", December 1963.
By the 1850s, historians of Chicago recognized Point du Sable as the city's earliest non-native permanent settler. For a long time, however, the city did not honor him in the same manner as other pioneers. A plaque was erected by the city in 1913 at the corner of Kinzie and Pine Streets to commemorate his homestead. In the planning stages of the 1933–1934 Century of Progress International Exposition a number of African-American groups campaigned for Point du Sable to be honored at the fair. At this time, few Chicagoans had even heard of Point du Sable and the fair's organizers presented the 1803 construction of Fort Dearborn as the city's historical beginning. The campaign was successful however, and a replica of Point du Sable's cabin was presented as part of the "background of the history of Chicago."
In 1965 a plaza called Pioneer Court was built on the site of Point du Sable's homestead as part of the construction of the Equitable Life Assurance Society of America building. The Jean Baptiste Point Du Sable Homesite was designated as a National Historic Landmark on May 11, 1976, as a site deemed to have "exceptional value to the nation." Pioneer Court is located at what is now 401 N. Michigan Avenue in the Near North Side of Chicago. In 2009, the City of Chicago and a private donor erected there a large bronze bust of Point du Sable by Chicago-born sculptor Erik Blome. In October 2010, the Michigan Avenue Bridge was renamed "DuSable Bridge" in honor of Point du Sable. Previously a small street named "De Saible Street" had been named after him.
A number of Chicago institutions have been named in honor of Point du Sable. DuSable High School opened in Bronzeville in 1934. Today it is a building for three schools: Daniel Hale Williams Prep School of Medicine, the Bronzeville Scholastic Institute, and the DuSable Leadership Academy. Dr. Margaret Taylor-Burroughs, a prominent African-American artist and writer taught at the school for twenty-three years. She and her husband co-founded the DuSable Museum of African American History, located on Chicago's South Side, which was renamed in honor of Point du Sable in 1968. DuSable Harbor is located in the heart of downtown Chicago at the foot of Randolph Street, and DuSable Park is an urban park (3.24 acre) in Chicago currently awaiting redevelopment. It was originally announced in 1987 by then Mayor Harold Washington. The US Postal Service has also honored Point du Sable with the issue of a Black Heritage Series, 22-cent postage stamp on February 20, 1987.
Notes and references.
</dl>

</doc>
<doc id="40416" url="http://en.wikipedia.org/wiki?curid=40416" title="Battle of Fort Dearborn">
Battle of Fort Dearborn

The Battle of Fort Dearborn (also known as the Fort Dearborn Massacre) was an engagement between United States troops and Potawatomi Indians that occurred on August 15, 1812, near Fort Dearborn in what is now Chicago, Illinois, but was then part of the Illinois Territory. The battle, which occurred during the War of 1812, followed the evacuation of the fort as ordered by William Hull, commander of the United States Army of the Northwest.
The battle lasted about 15 minutes and resulted in a complete victory for the Indians. Fort Dearborn was burned down and those soldiers and settlers that survived were taken captive. Some were later ransomed. After the battle, however, settlers continued to seek to enter the area, the fort was rebuilt in 1816, and settlers and the government were now convinced that all Indians had to be removed from the territory, far away from the settlement.
Background.
Fort Dearborn was constructed by United States troops under the command of Captain John Whistler in 1803. It was located on the south bank of the main stem of the Chicago River in what is now the Loop community area of downtown Chicago. At the time, the area was seen as wilderness; in the view of later commander, Heald, "so remote from the civilized part of the world." The fort was named in honor of Henry Dearborn, then United States Secretary of War. It had been commissioned following the Northwest Indian War of 1785–1795, and the signing of the Treaty of Greenville at Fort Greenville (now Greenville, Ohio), on August 3, 1795. As part of the terms of this treaty, a coalition of Native Americans and frontiersmen, known as the Western Confederacy, turned over to the United States large parts of modern-day Ohio, and various other parcels of land including 6 sqmi centered at the mouth of the Chicago River.
The British Empire had ceded the Northwest Territory—comprising the modern states of Ohio, Indiana, Illinois, Michigan, and Wisconsin—to the United States in the Treaty of Paris in 1783. However the area had been the subject of dispute between the Indian nations and the United States since the passage of the Northwest Ordinance in 1787. The Indian Nations followed Tenskwatawa, the Shawnee prophet and the brother of Tecumseh. Tenskwatawa had a vision of purifying his society by expelling the "children of the Evil Spirit", the American settlers. Tenskwatawa and Tecumseh formed a confederation of numerous tribes to block American expansion. The British saw the Indian nations as valuable allies and a buffer to its Canadian colonies and provided them arms. Attacks on American settlers in the Northwest further aggravated tensions between Britain and the United States. The Confederation's raids hindered American expansion into potentially valuable farmlands, mineral deposits and fur trade areas in the Northwest Territory.
In 1810, as a result of a long running feud, Captain Whistler and other senior officers at Fort Dearborn were removed. Whistler was replaced by Captain Nathan Heald, who had been stationed at Fort Wayne, Indiana. Heald was dissatisfied with his new posting and immediately applied for a leave of absence to spend the winter in Massachusetts. On his return journey to Chicago, he visited Kentucky, where he married Rebekah Wells, the daughter of Samuel Wells, and they traveled together to Chicago in June 1811.
As the United States and Britain moved towards war, antipathy between the settlers and Indians in the Chicago area increased. In the summer of 1811, British emissaries tried to enlist the support of Indians in the region, telling them that the British would help them to resist the encroaching American settlement. On April 6, 1812, a band of Winnebago Indians murdered Liberty White, an American, and John B. Cardin, a French Canadian, at a farm called Hardscrabble that was located on the south branch of the Chicago River, in the area now called "Bridgeport". News of the murder was carried to Fort Dearborn by a soldier of the garrison named John Kelso and a small boy who had managed to escape from the farm. Following the murder, some residents of Chicago moved into the fort while the rest fortified themselves in a house that had belonged to Charles Jouett, an Indian agent. Fifteen men from the civilian population were organized into a militia by Captain Heald, and armed with guns and ammunition from the fort.
Battle.
On June 18, 1812, the United States declared war on the British Empire, and on July 17, British forces captured Fort Mackinac. On July 29, General William Hull received news of the fall of Fort Mackinac and immediately sent orders to Captain Heald to evacuate Fort Dearborn, fearing that it could no longer be adequately supplied with provisions. In his letter to Captain Heald, which arrived at Fort Dearborn on August 9, General Hull ordered Heald to destroy all the arms and ammunition and give the remaining goods to friendly Indians in the hope of attaining an escort to Fort Wayne. Hull also sent a copy of these orders to Fort Wayne with additional instructions to provide Heald with all the information, advice and assistance within their power. In the following days, the sub-Indian agent at Fort Wayne, Captain William Wells, who was the uncle of Heald's wife, Rebekah, assembled a group of about 30 Miami Indians. Wells, Corporal Walter K. Jordan, and the Miamis traveled to Fort Dearborn to provide an escort for the evacuees.
Wells arrived at Fort Dearborn on August 12 or 13 (sources differ), and on August 14, Captain Heald held a council with the Potawatomi leaders to inform them of his intention to evacuate the fort. The Indians believed that Heald told them that he would distribute the fire-arms, ammunition, provisions and whiskey amongst them, and that, if they would send a band of Potawatomis to escort them safely to Fort Wayne, he would pay them a large sum of money. However, Heald ordered all the surplus arms, ammunition and liquor destroyed "fearing that [the Indians] would make bad use of it if put in their possession." On August 14, a Potawatomi chief called, Black Partridge, warned Heald that the young men of the tribe intended to attack, and that he could no longer restrain them.
At 9:00 am on August 15, the garrison—comprising, according to Heald's report, 54 U.S. regulars, 12 militia, nine women and 18 children—left Fort Dearborn with the intention of marching to Fort Wayne. Captain Wells led the group with some of the Miami escorts, while the rest of the Miamis were positioned at the rear. About 1+1/2 mi south of Fort Dearborn, a band of Potawatomi warriors ambushed the garrison. Heald reported that, upon discovering that the Indians were preparing to ambush from behind a dune, the company marched to the top of the dune, fired off a round and charged at the Indians. This maneuver separated the cavalry from the wagons, allowing the overwhelming Indian force to charge into the gap, divide and surround both groups. During the ensuing battle, some of the Indians charged at the wagon train that contained the women and children as well as the provisions. The wagons were defended by the militia, as well as Ensign and the fort physician Van Voorhis. The officers and militia were killed, along with two of the women and most of the children. Wells disengaged from the main battle and attempted to ride to the aid of those at the wagons. In doing so, he was brought down; according to eyewitness accounts he fought off many Indians before being killed, and a group of Indians immediately cut out his heart and ate it to absorb his courage. The battle lasted about 15 minutes, after which Heald and the surviving soldiers withdrew to an area of elevated ground on the prairie. They then surrendered to the Indians who took them as prisoners to their camp near Fort Dearborn. In his report, Heald detailed the American loss at 26 regulars, all 12 of the militia, two women and twelve children killed, with the other 28 regulars, seven women and six children taken prisoner.
Survivors' accounts differed on the role of the Miami warriors. Some said they fought for the Americans, while others said they did not fight at all. Regardless, William Henry Harrison claimed the Miami fought against the Americans, and used the Fort Dearborn massacre as a pretext to attack the Miami villages. Miami Chief, Pacanne, and his nephew, Jean Baptiste Richardville, accordingly ended their neutrality in the War of 1812, and allied with the British.
Accounts of the battle.
The recollections of a number of the survivors of the battle have been published. Heald's story was recorded on September 22, 1812, by Charles Askin in his diary, Heald also wrote brief accounts of events in his journal and in an official report of the battle. Walter Jordan recorded his version of events in a letter to his wife dated October 12, 1812. Helm wrote a detailed narrative of events; but, because of his fear of being court marshalled due to his criticism of Heald, delayed publication until 1814. John Kinzie's recollections of the battle were recorded by Henry Schoolcraft in August 1820.
These accounts of details of the conflict are discrepant, particularly in their attribution of blame for the battle. Juliette Magill Kinzie's "Wau-Bun: The Early Day in the Northwest", which was first published in 1856, provides the traditional account of the conflict. However it is based on family stories and is regarded as historically inaccurate. Nonetheless, its popular acceptance was surprisingly strong.
The Battle of Fort Dearborn has also been titled “The Fort Dearborn Massacre.” The battle has been historically described as a massacre because of the number of Americans killed, and is also argued to be self-defense on the part of the Indians defending their land.
Aftermath.
Following the battle the Indians took their prisoners to their camp near Fort Dearborn and the fort was burned to the ground. The region remained empty of U.S. citizens until after the war had ended. Some of the prisoners died in captivity, while others were later ransomed. The fort, however, was reestablished and rebuilt in 1816.
Seen from the perspective of the War of 1812, and the larger conflict between Britain and France which precipitated it, this was a very small and brief battle, but it ultimately had larger consequences in the territory. Arguably, for the Indians, it was an example of "winning the battle but losing the war": the U.S. later pursued a policy of removing the tribes from the region, resulting in the Treaty of Chicago, which was marked at its culmination in 1835 by the last great Indian war dance in the then nascent city. Thereafter, the Potowatomie and other tribes were moved further west.
Location of the battle.
Eye-witness accounts place the battle on the lake shore somewhere between 1 and south of Fort Dearborn. Heald's official report said the battle occurred 1+1/2 mi south of the fort, placing the battle at what is now the intersection of Roosevelt Road (previously known as, 12th Street) and Michigan Avenue. Juliette Kinzie, shortly before her death in 1870, stated that the battle had started by a large cottonwood tree, which at that time still stood on 18th Street between Prairie Avenue and the lake. The tree was supposed to have been the last remaining of a grove of trees that had been saplings at the time of the battle.
The tree was blown down in a storm on May 16, 1894 and a portion of its trunk was preserved at the Chicago Historical Society. Historian Harry A. Musham points out that the testimony relating to this tree is all second hand and came from people who settled in Chicago more than 20 years after the battle. Moreover, based on the diameter of the preserved section of trunk (about 3 ft) he estimated the age of the tree at the time that it was blown over at no more than 80 years, and therefore asserts that it could not have been growing at the time of the battle. Nevertheless, the site at 18th Street and Prairie Avenue has become the location traditionally associated with the battle, and on the battle's 197th anniversary in 2009, the Chicago Park District, the Prairie District Neighborhood Alliance and other community partners dedicated "Battle of Fort Dearborn Park" near the site at 18th Street and Calumet Avenue.
Monuments.
In 1893, George Pullman had a sculpture he had commissioned from Carl Rohl-Smith erected near his house. It portrays the rescue of Margaret Helm, the stepdaughter of Chicago resident John Kinzie and wife of Lt. Linai Thomas Helm, by Potawatomi chief Black Partridge, who led her and some others to Lake Michigan and helped her escape by boat. The monument was moved to the lobby of the Chicago Historical Society in 1931. In the 1970s, however, American Indian groups protested the display of the monument, and it was removed. In the 1990s, the statue was reinstalled near 18th Street and Prairie Avenue, close to its original site, at the time of the revival of the Prairie Avenue Historic District. It was later removed for conservation reasons by the Office of Public Art of the Chicago Department of Cultural Affairs. There are some efforts to reinstall the monument, but it is meeting resistance from the Chicago American Indian Center.
The battle is also memorialized with a sculpture by Henry Hering called "Defense" that is located on the south western tender's house of the Michigan Avenue Bridge (which partially covers the site of Fort Dearborn). There are also memorials in Chicago to individuals who fought in the battle. William Wells is commemorated in the naming of Wells Street, a north-south street and part of the original 1830 58-block plat of Chicago, while Nathan Heald is commemorated in the naming of Heald Square. Ronan Park on the city's Far North Side honors Ensign George Ronan, who was the first West Point graduate to die in battle.
Notes and references.
</dl>

</doc>
<doc id="40417" url="http://en.wikipedia.org/wiki?curid=40417" title="Fort Dearborn">
Fort Dearborn

Fort Dearborn was an United States fort built in 1803 beside the Chicago River in what is now Chicago, Illinois. It was constructed by troops under Captain John Whistler and named in honor of Henry Dearborn, then United States Secretary of War. The original fort was destroyed following the Battle of Fort Dearborn in 1812, and a new fort was constructed on the same site in 1816. The fort was de-commissioned by 1837, and parts of the fort were lost to the widening of the Chicago River in 1855 and a fire in 1857; the last vestiges being destroyed in the Great Chicago Fire of 1871. The site of the fort is now a Chicago Landmark, part of the Michigan–Wacker Historic District.
Background.
The history of human activity in the Chicago area prior to the arrival of European explorers is mostly unknown. In 1673, an expedition headed by Louis Jolliet and Jacques Marquette, though possibly not the first Europeans to visit the area, was the first recorded to have crossed the Chicago Portage and travelled along the Chicago River. Marquette returned in 1674, camped a few days near the mouth of the river, then moved on to the portage, where he stayed through the winter of 1674–75. Joliet and Marquette did not report any Indians living near the Chicago River area at this time, though archaeologists have since discovered numerous Indian village sites elsewhere in the greater Chicago area. Two of La Salle's men built a stockade at the portage in the winter of 1682/1683. A Jesuit mission, Mission of the Guardian Angel, was founded somewhere in the vicinity of Chicago in 1696, but was abandoned in around 1700. The Fox Wars effectively closed the Chicago area to Europeans in the first part of the 18th century. The first non-native to re-settle in the area may have been a trader named Guillory, who might have had a trading-post near Wolf Point on the Chicago River in around 1778. Jean Baptiste Point du Sable built a farm and trading post near the mouth of the Chicago River in the 1780s, and he is widely regarded as the Founder of Chicago. Antoine Ouilmette is the next recorded resident of Chicago; he claimed to have settled at the mouth of the Chicago River in July 1790.
In 1682, René Robert Cavelier, Sieur de La Salle had claimed a large territory, including the Chicago area, for France. In 1763 the French ceded this area to Great Britain's Province of Quebec following the French and Indian War. Great Britain then ceded the area to the United States at the end of the American Revolutionary War, though the Northwest Territory remained under "de facto" British control until 1796. Following the Northwest Indian War of 1785–1795, the Treaty of Greenville was signed at Fort Greenville (now Greenville, Ohio), on August 3, 1795. As part of the terms of this treaty, a coalition of Native Americans and Frontiers men, known as the Western Confederacy, turned over to the United States large parts of modern-day Ohio, and various other parcels of land including six square miles centered at the mouth of the Chicago River.
The first fort.
On March 9, 1803, Henry Dearborn, the Secretary of War wrote to Colonel Jean Hamtramck, the commandant of Detroit, instructing him to have an officer and six men survey the route from Detroit to Chicago, and to make a preliminary investigation of the situation at Chicago. Captain John Whistler was selected as commandant of the new post, and set out with six men to complete the survey. The survey completed, on July 14, 1803, a company of troops set out to make the overland journey from Detroit to Chicago. Whistler and his family made their way to Chicago on a schooner called "Tracy". The troops reached Chicago on August 17; the "Tracy" was anchored about half a mile offshore, unable to enter the Chicago River due to a sandbar at its mouth. Julia Whistler, the wife of Captain Whistler's son, Lieutenant William Whistler, later related that 2000 Indians gathered to see the "Tracy". The troops had completed the construction of the fort by the summer of 1804; it was a log-built fort enclosed in a double stockade, with two blockhouses. The fort was named "Fort Dearborn" after Henry Dearborn, the Secretary of War, who had commissioned its construction.
The fur trader John Kinzie arrived in Chicago in 1804, and rapidly became the civilian leader of the small settlement that grew around the fort. In 1810 Kinzie and Whistler became embroiled in a dispute over Kinzie supplying alcohol to the Indians. In April, Whistler and other senior officers at the fort were removed; Whistler was replaced as commandant of the fort by Captain Nathan Heald.
Fort Dearborn Massacre.
During the War of 1812, General William Hull ordered the evacuation of Fort Dearborn in August 1812. Heald oversaw the evacuation, but on August 15 the evacuees were ambushed by about 500 Potawatomi Indians in the Fort Dearborn Massacre. The Potawatomi captured Heald and his wife, Rebekah, and ransomed them to the British. Of the 148 soldiers, women and children who evacuated the fort, 86 were killed in the ambush. The Potawatomi burned the fort to the ground the next day.
The second fort.
Following the war, a second Fort Dearborn was built in 1816. This fort consisted of a double wall of wooden palisade, officer and enlisted barracks, a garden, and other buildings. The American forces garrisoned the fort until 1823, when peace with the Indians led the garrison to be deemed redundant. This temporary abandonment lasted until 1828, when it was re-garrisoned following the outbreak of war with the Winnebago Indians. In her 1856 memoir "Wau Bun", Juliette Kinzie described the fort as it appeared on her arrival in Chicago in 1831:
The fort was inclosed by high pickets, with bastions at the alternate angles. Large gates opened to the north and south, and there were small portions here and there for the accommodation of the inmates. ... Beyond the parade-ground which extended south of the pickets, were the company gardens, well filled with currant-bushes and young fruit-trees. The fort stood at what might naturally be supposed to be the mouth of the river, yet it was not so, for in these days the latter took a turn, sweeping round the promontory on which the fort was built, towards the south, and joined the lake about half a mile below
The fort was closed briefly before the Black Hawk War of 1832 and by 1837, the fort was being used by the Superintendent of Harbor Works. In 1837, the fort and its reserve, including part of the land that became Grant Park, was deeded to the city by the Federal Government. In 1855 part of the fort was demolished so that the south bank of the Chicago River could be dredged, straightening the bend in the river and widening it at this point by about 150 ft; and in 1857, a fire destroyed nearly all the remaining buildings in the fort. The fort's tower bell was rescued from the remains by Police Constable Jacob Schoenewald and donated for use in the bell tower of St. Joseph's Catholic Church during its construction in 1864. The blockhouse and the few surviving outbuildings were destroyed in the Great Chicago Fire of 1871.
Legacy and monuments.
Fort Dearborn was located at what is now the intersection of Wacker Drive and Michigan Avenue in the Loop community area of Chicago at the foot of the Magnificent Mile. Part of the fort outline is marked by plaques and a line embedded in the sidewalk and road near the Michigan Avenue Bridge and Wacker Drive. A few boards from the old fort were retained and are now in the Chicago History Museum in Lincoln Park.
On March 5, 1899, the "Chicago Tribune" publicized a Chicago Historical Society replica of the original fort.
Also in 1933, at the Century of Progress Exhibition, a detailed replica of Fort Dearborn was erected as a fair exhibit. As part of the celebration both a United States postage stamp and souvenir sheet (containing 25 of the stamps) were issued showing the fort.
In 1939, the Chicago City Council added a fourth star to the city flag to represent Fort Dearborn. This star is depicted as the left-most, or first, star of the flag.
The site of the fort was designated a Chicago Landmark on September 15, 1971.
References.
</dl>

</doc>
<doc id="40418" url="http://en.wikipedia.org/wiki?curid=40418" title="Carl Sandburg">
Carl Sandburg

Carl August Sandburg (January 6, 1878 – July 22, 1967) was an American poet, writer, and editor who won three Pulitzer Prizes, two for his poetry and one for his biography of Abraham Lincoln. During his lifetime, Sandburg was widely regarded as "a major figure in contemporary literature", especially for volumes of his collected verse, including "Chicago Poems" (1916), "Cornhuskers" (1918), and "Smoke and Steel" (1920). He enjoyed "unrivaled appeal as a poet in his day, perhaps because the breadth of his experiences connected him with so many strands of American life", and at his death in 1967, President Lyndon B. Johnson observed that "Carl Sandburg was more than the voice of America, more than the poet of its strength and genius. He was America."
Biography.
Sandburg was born in the three-room cottage at 313 East Third Street in Galesburg, Illinois, to Clara Mathilda (née Anderson) and August Sandberg, both of Swedish ancestry. He adopted the nickname "Charles" or "Charlie" in elementary school, at about the same time, he and his two oldest siblings changed the spelling of their last name to "Sandburg".
At the age of thirteen he left school and began driving a milk wagon. From the age of about fourteen until he was seventeen or eighteen, he worked as a porter at the Union Hotel barbershop in Galesburg. After that he was on the milk route again for eighteen months. He then became a bricklayer and a farm laborer on the wheat plains of Kansas. After an interval spent at Lombard College in Galesburg, he became a hotel servant in Denver, then a coal-heaver in Omaha. He began his writing career as a journalist for the "Chicago Daily News". Later he wrote poetry, history, biographies, novels, children's literature, and film reviews. Sandburg also collected and edited books of ballads and folklore. He spent most of his life in the Midwest before moving to North Carolina.
Sandburg volunteered to go to the military and was stationed in Puerto Rico with the 6th Illinois Infantry during the Spanish–American War, disembarking at Guánica, Puerto Rico on July 25, 1898. Sandburg was never actually called to battle. He attended West Point for just two weeks, before failing a mathematics and grammar exam. Sandburg returned to Galesburg and entered Lombard College, but left without a degree in 1903.
He moved to Milwaukee, Wisconsin, and joined the Social Democratic Party, the name by which the Socialist Party of America was known in the state. Sandburg served as a secretary to Emil Seidel, socialist mayor of Milwaukee from 1910 to 1912.
Sandburg met Lilian Steichen at the Social Democratic Party office in 1907, and they married the next year. Lilian's brother was the photographer Edward Steichen. Sandburg with his wife, whom he called Paula, raised three daughters.
The Sandburgs moved to Harbert, Michigan, and then to suburban Chicago, Illinois. They lived in Evanston, Illinois, before settling at 331 S. York Street in Elmhurst, Illinois, from 1919 to 1930. Sandburg wrote three children's books in Elmhurst, "Rootabaga Stories", in 1922, followed by "Rootabaga Pigeons" (1923), and "Potato Face" (1930). Sandburg also wrote "Abraham Lincoln: The Prairie Years", a two-volume biography, in 1926, "The American Songbag" (1927), and a book of poems called "Good Morning, America" (1928) in Elmhurst. The family moved to Michigan in 1930. The Sandburg house at 331 W. York Street, Elmhurst was demolished and the site is now a parking lot.
In 1919 Sandburg won a Pulitzer Prize "made possible by a special grant from The Poetry Society" for his collection "Corn Huskers". He won the 1940 Pulitzer Prize for History for "The War Years", the second volume of his "Abraham Lincoln", and a second Poetry Pulitzer in 1951 for "Complete Poems".
In 1945 he moved to Connemara, a 246 acre rural estate in Flat Rock, North Carolina. Here he produced a little over a third of his total published work, and lived with his wife, daughters, and two grandchildren.
On February 12, 1959, in commemorations of the 150th anniversary of Abraham Lincoln's birth, Congress met in joint session to hear actor Fredric March give a dramatic reading of the Gettysburg Address, followed by an address by Sandburg. s of 2013[ [update]], Sandburg remains the only American poet ever invited to address a joint session of Congress.
Sandburg supported the Civil Rights Movement, and was the first white man to be honored by the NAACP with their Silver Plaque Award, proclaiming him to be a "major prophet of civil rights in our time."
Sandburg died of natural causes in 1967; his ashes were interred under "Remembrance Rock", a granite boulder located behind his birth house.
Works.
Much of Carl Sandburg's poetry, such as "Chicago", focused on Chicago, Illinois, where he spent time as a reporter for the "Chicago Daily News" and the "Day Book". His most famous description of the city is as "Hog Butcher for the World/Tool Maker, Stacker of Wheat/Player with Railroads and the Nation's Freight Handler,/Stormy, Husky, Brawling, City of the Big Shoulders."
Sandburg earned Pulitzer Prizes for his collection "The Complete Poems of Carl Sandburg", "Corn Huskers", and for his biography of Abraham Lincoln ("Abraham Lincoln: The War Years"). He recorded excerpts from the biography and some of Lincoln's speeches for Caedmon Records in New York City in May 1957. He was awarded a Grammy Award in 1959 for Best Performance – Documentary Or Spoken Word (Other Than Comedy) for his recording of Aaron Copland's "Lincoln Portrait" with the New York Philharmonic.
Sandburg is also remembered by generations of children for his "Rootabaga Stories" and "Rootabaga Pigeons", a series of whimsical, sometimes melancholy stories he originally created for his own daughters. "The Rootabaga Stories" were born of Sandburg's desire for "American fairy tales" to match American childhood. He felt that the European stories involving royalty and knights were inappropriate, and so populated his stories with skyscrapers, trains, corn fairies and the "Five Marvelous Pretzels".
Folk music.
Sandburg's 1927 anthology, the "American Songbag", enjoyed enormous popularity, going through many editions; and Sandburg himself was perhaps the first American urban folk singer, accompanying himself on solo guitar at lectures and poetry recitals, and in recordings, long before the first or the second folk revival movements (of the 1940s and 1960s, respectively). According to the musicologist Judith Tick:
As a populist poet, Sandburg bestowed a powerful dignity on what the '20s called the "American scene" in a book he called a "ragbag of stripes and streaks of color from nearly all ends of the earth ... rich with the diversity of the United States." Reviewed widely in journals ranging from the "New Masses" to "Modern Music", the "American Songbag" influenced a number of musicians. Pete Seeger, who calls it a "landmark", saw it "almost as soon as it came out." The composer Elie Siegmeister took it to Paris with him in 1927, and he and his wife Hannah "were always singing these songs. That was home. That was where we belonged."
Legacy.
Carl Sandburg's boyhood home in Galesburg is now operated by the Illinois Historic Preservation Agency as the Carl Sandburg State Historic Site. The site contains the cottage Sandburg was born in, a modern visitor's center, and small garden with a large stone called Remembrance Rock, under which his and his wife's ashes are buried. Sandburg's home of 22 years in Flat Rock, Henderson County, North Carolina, is preserved by the National Park Service as the Carl Sandburg Home National Historic Site. Carl Sandburg College is located in Sandburg's birthplace of Galesburg, Illinois.
Carl Sandburg Village was a Chicago urban renewal project of the 1960s located in the Near North Side, Chicago. Financed by the city, it is located between Clark and LaSalle St. between Division Street and North Ave. Solomon & Cordwell, architects. In 1979, Carl Sandburg Village was converted to condominium ownership.
In 1960, Elmhurst, Illinois renamed the former Elmhurst Junior High School as "Carl Sandburg Middle School." Sandburg spoke at the dedication ceremony. He resided at 331 S. York Street in Elmhurst from 1919 to 1930. The house was demolished and the site is a parking lot. In 1954, Carl Sandburg High School was dedicated in Orland Park, Illinois. Sandburg was in attendance, and stretched what was supposed to be a one-hour event into several hours, regaling students with songs and stories. Years later, he returned to the school with no identification and, appearing to be a hobo, was thrown out by the principal. When he later returned with I.D., the embarrassed principal canceled the rest of the school day and held an assembly to honor the visit. In 1959, Carl Sandburg Junior High School was opened in Golden Valley, Minnesota. Carl Sandburg attended the dedication of the school. In 1988 the name was changed to Sandburg Middle School servicing grades 6, 7, and 8. Originally built with a capacity for 1,800 students the school now has 1,100 students enrolled. Sandburg Middle school was one of the first schools in the state of Minnesota to offer accelerated learning programs for gifted students. In December 1961, Carl Sandburg Elementary School was dedicated in San Bruno, California. Again, Sandburg came for the ceremonies and was clearly impressed with the faces of the young children, who gathered around him. The school was closed in the 1980s, due to falling enrollments in the San Bruno Park School District.
In Neshaminy School District of lower Bucks County resides the secondary institution Carl Sandburg Middle School. Located in the lobby is a finished split tree trunk with the quote engraved lengthwise horizontally: "Man is born with rainbows in his heart and you'll never read him unless you consider rainbows". Another secondary school by the same name is located south of Alexandria, Virginia, and is part of the Fairfax County Public Schools School District. Sandburg Halls is a student residence hall at the University of Wisconsin–Milwaukee. The building consists of four high-rise towers with a total housing capacity of 2,700 students. It has an exterior plaque on Sandburg's roles as an organizer for the Social Democratic Party and as personal secretary to Emil Seidel, Milwaukee's first Socialist mayor. There are several other schools named after Sandburg in Illinois, including those in Wheaton, Orland Park, Springfield, Mundelein, Freeport and Joliet.
On January 6, 1978, the 100th anniversary of his birth, the United States Postal Service issued a commemorative stamp honoring Sandburg. The spare design consists of a profile originally drawn by his friend William A. Smith in 1952, along with Sandburg's own distinctive autograph.
The Rare Book & Manuscript Library (University of Illinois at Urbana-Champaign) (RBML) houses the Carl Sandburg Papers. The bulk of the collection was purchased directly from Carl Sandburg and his family. In total, the RBML owns over 600 cubic feet of Sandburg's papers, including photographs, correspondence, and manuscripts.
Carl Sandburg Library first opened in Livonia, Michigan, on December 10, 1961. The name was recommended by the Library Commission as an example of an American author representing the best of literature of the Midwest. Carl Sandburg had taught at the University of Michigan for a time.
Funded by the State of Illinois, Amtrak in October 2006 added a second train on the Chicago–Quincy (via Galesburg and Macomb) route. Called the "Carl Sandburg", this new train joined the "Illinois Zephyr" on the Chicago–Quincy route.
Galesburg opened Sandburg Mall in 1974, named in honor of Sandburg. The Chicago Public Library installed the Carl Sandburg Award, annually awarded for contributions to literature.

</doc>
<doc id="40419" url="http://en.wikipedia.org/wiki?curid=40419" title="Dover">
Dover

Dover (; French: "Douvres") is a town and major ferry port in the home county of Kent, in South East England. It faces France across the strait of Dover, the narrowest part of the English Channel, and lies south-east of Canterbury; east of Kent's county town Maidstone; and north-east along the coastline from Dungeness and Hastings. The town is the administrative centre of the Dover District and home of the Dover Calais ferry through the Port of Dover. The surrounding chalk cliffs have become known as the White Cliffs of Dover.
Its strategic position has been evident throughout its history: archaeological finds have revealed that the area has always been a focus for peoples entering and leaving Britain. The name of the town derives from the name of the river that flows through it, the River Dour. The town has been inhabited since the Stone Age according to archaeological finds, and Dover is one of only a few places in Britain – London, Edinburgh, and Cornwall being other examples – to have a corresponding name in the French language, "Douvres".
There was a military barracks in Dover, which was closed in 2007. Although many of the former ferry services have declined, services related to the Port of Dover provide a great deal of the town’s employment, as does tourism. The prospect of privatising the sale of the Port of Dover to create increased cash flow for the government was given a recent ironic twist due to the rejection of a possible bid from the town of Calais in France after opposition in Dover against any sale forced the government to withdraw the Port from the market. Local residents had clubbed together to propose buying it for the community, more than 12,000 people have bought a £10 share in the People's Port Trust.
History.
The name Dover was first recorded in Latinised form as "Portus Dubris", which was for many years explained as derived from the Brythonic "Dubrās" ("the waters") referring to its river Dour. However, a recent detailed study showed that the name is far more likely to come from an ancient word for 'double bank' referring to the shingle spit(s) that formed across the harbour entrance, for which a word dover is still used in the Isle of Wight. Subsequent name forms included Doverre; the modern name was in use at least by the time Shakespeare wrote "King Lear" (between 1603 and 1606), in which the town and its cliffs play a prominent role. The cliffs may have given England its ancient name of "Albion" ("white").
Dover’s history, because of its proximity to France, has always been of great strategic importance to Britain. Archaeological finds have shown that there were Stone Age people in the area; and that by the Bronze Age the maritime influence was already strong. Some Iron Age finds exist also, but the coming of the Romans made Dover part of their communications network. Like "Lemanis" (Lympne) and "Rutupiae" (Richborough) Dover was connected by road to Canterbury and Watling Street; and it became Portus Dubris, a fortified port. Forts were built above the port; lighthouses were constructed to guide passing ships; and one of the best-preserved Roman villas in Britain is here.
Dover figured largely in the "Domesday Book" as an important borough. It also served as a bastion against various attackers: notably the French during the Napoleonic Wars; and against Germany during World War II. It was one of the Cinque Ports during medieval times.
Geography.
Dover is near the extreme south-east corner of Britain. At South Foreland, the nearest point to the continent, Cap Gris Nez near Calais is 34 km away, across the Strait of Dover.
The site of its original settlement lies in the valley of the River Dour, making it an ideal place for a port, sheltered from the prevailing south-westerly winds. This led to the silting up of the river mouth by the action of longshore drift; the town was then forced into making artificial breakwaters to keep the port in being. These breakwaters have been extended and adapted so that the port lies almost entirely on reclaimed land.
The higher land on either side of the valley – the Western Heights and the eastern high point on which Dover Castle stands – has been adapted to perform the function of protection against invaders. The town has gradually extended up the river valley, encompassing several villages in doing so. Little growth is possible along the coast, since the cliffs are on the sea’s edge. The railway, being tunnelled and embanked, skirts the foot of the cliffs.
Climate.
Dover has an oceanic climate (Koppen classification "Cfb") similar to the rest of England and the United Kingdom with moderate temperatures year-round and light precipitation each month.
Demography.
In 1800 Edward Hasted (1732–1812) reported that the town had a population of almost 10,000 people.
At the 2001 census, the town of Dover had 28,156 inhabitants, while the population of the whole urban area of Dover, as calculated by the Office for National Statistics, was 39,078 inhabitants.
With the expansion of Dover, many of the outlying ancient villages have been incorporated into the town. Originally the parishes of Dover St. Mary's and Dover St. James, since 1836 Buckland and Charlton have become part Dover, and Maxton (a hamlet to the west), River, Kearsney, Temple Ewell, and Whitfield, all to the north of the town centre, are within its conurbation.
Economy.
Shipping.
The Dover Harbour Board is the responsible authority for the running of the Port of Dover. The English Channel, here at its narrowest point in the Straits of Dover, is the busiest shipping lane in the world. Ferries crossing between here and the Continent have to negotiate their way through the constant stream of shipping crossing their path. The "Dover Strait Traffic Separation Scheme" allots ships separate lanes when passing through the Strait. The Scheme is controlled by the Channel Navigation Information Service based at Maritime Rescue Co-ordination Centre Dover. MRCC Dover is also charged with co-ordination of civil maritime search and rescue within these waters.
The Port of Dover is also used by cruise ships. The old Dover Marine railway station building houses one passenger terminal, together with a car park. A second, purpose built, terminal is located further out along the pier.
The ferry lines using the port are (number of daily sailings in parentheses):
These services have been cut in recent years:
The Dover lifeboat is a Severn class lifeboat based in the Western Docks.
Transport.
Dover’s main communications artery, the A2 road replicates two former routes, connecting the town with Canterbury. The Roman road was followed for centuries until, in the late 18th century, it became a toll road. Stagecoaches were operating: one description stated that the journey took all day to reach London, from 4am to being "in time for supper".
The other main roads, travelling west and east, are the A20 to Folkestone and thence to London and the A258 through Deal to Sandwich.
The railway reached Dover from two directions: the South Eastern Railway's main line connected with Folkestone in 1844, and the London, Chatham and Dover Railway opened its line from Canterbury in 1861. Trains run from Dover Priory to either London Charing Cross, London Victoria or London St Pancras International railway stations in London, and Ramsgate or Sandwich in Kent in terms of final destinations. Trains from Dover Priory are run by Southeastern (train operating company).
A tram system operated in the town from 1897 to 1936.
Dover has two long distance footpaths: the Saxon Shore Way and the North Downs Way. Two National Cycle Network routes begin their journey at the town.
Also, the ferry to Calais is a popular transport. From Dover to Calais it usually takes one and a half hours to cross the Strait of Dover. The Strait of Dover is the stretch of water between Dover and Calais, and also the stretch of water the Dover-Calais and Calais-Dover ferries cross.
More recently, ferries on the Dover to Dunkirk route have also become popular. This service was originally operated by ferry operator Norfolkline. This company was later acquired by the pan European operator DFDS Seaways in July 2010
. The crossing time is approximately two hours. Due to this route not being as well known as Dover to Calais, prices are often cheaper. The location of Dunkirk is also more convenient for those travelling on to countries in Northern Europe including Belgium, the Netherlands, Germany and further afield.
Stagecoach in East Kent provide local bus services. Dover is on the Stagecoach Diamond network providing links to Canterbury 15, 15A, 15B every 15 minutes and Deal 15, 15A every 30 minutes. The Diamond 15A route serves the Western Docks at the port of Dover from the Town Centre as well as Canterbury and Deal. Dover is also the start of The Wave network running every 15 minutes to New Romney 101 and 102 via Folkestone, Hythe and Dymchurch. 2 buses per hour are 101 services to Lydd via Lydd Airport, with one continuing from Lydd on to Hastings via Camber and Rye. Two buses per hour are 102 service, with one trminating at New Romney and the other to Lydd-on-Sea via Greatstone. Routes 87 and 88 provide a link to Sandwich and Ramsgate. Route 89 runs from Dover to Canterbury via Aylesham. Stagecoach also provide Dover with The Heart network 60, 61, 62 and 63. Just like nearby Folkestone, the Dover local routes run from the Town Centre to the outskirts of the town and to Tesco and B&Q.
National Express runs coaches from Dover to other towns in Kent including Canterbury, Folkestone, Ashford, Kent, Maidstone, Gillingham at Hempsted Valley shopping centre and Greenhithe at Bluewater Shopping Centre for Dartford to London including Bexleyheath, Eltham, Walworth, Canary Wharf, Elephant & Castle, The City (The City of London) and to Victoria Coach Station
All buses serve Pencester Road except route 68 to Maxton operated by Regent Coaches.
Retail.
The town's main shopping streets are the High Street and Biggin Street. There is also the Castleton Retail Park to the north-west of the town centre.
The town has a number of retail chains, including Marks and Spencer, Boots, WH Smith, Costa Coffee, a large Peacocks store in the former Woolworths store, and branches of Dorothy Perkins, and Burton Menswear as well as Argos.
There are a number of empty shops in Dover, as is also the case nationally but Dover Town Centre has fallen behind other comparable towns since the 1980s and increasing number of retailers are no longer represented in the town including H. Sameul, Evans, Internaccionale, Thomas Cook / Co-operative Travel, Millets and Barratts, Bon Marche, Johnsons Drycleaners, and Vodafone.
There are long awaited plans to redevelop the St James area of the town centre but these are taking a while for fruition. Due to demographics and income, Dover like many places has been successful in attracting Poundland, Morrisons, Greggs and Brighthouse as retail stores to the town centre and is seeing a re-emergence of independent stores in what is prime retail area of Biggin Street and High Street. There are more charity shops as is common across the country in the current economic environment.
There are plans to open a 6 screen Cineworld Cinema and leisure element ( Restaurants) at St James but not until 2017. It has been recently announced that Marks and Spencer will relocate to St James Development and that the current M and S general store will close. The new 16,000 sq feet store at St James will be an M and S Simply Food with café only and will not sell clothing or homeware unlike the current store which will shut in 2016. It's thought the new M and S will cater for passing Belgium and French tourist trade rather than to the benefit of Dover people and Dover Town Centre. Simmonds Jeweller's will also close their Dover branch after 40 years in January 2014. The M and S general store and Simmonds branch in nearby Deal will remain open.
Independent stores continue to grow in Dover, but the main town centre of Dover remains in decline compared to other towns like Folkestone, Canterbury, Westwood Cross and Ashford who continue to take trade away from Dover and Dover District.
Marks and Spencer Simply food is the only anchor store at the new St James and as of December 2013, the only retailer confirmed for the development. However, there is a big possibility that there will be an exodus of existing chains from Biggin Street and Cannon Street to the new development leaving Dover Town Centre increasingly empty as will be the case with the large M and S store which has been in Dover for over a 100 years.
RNLI.
Dover Lifeboat station is based at crosswall quay in Dover Harbour. There is a Severn-class lifeboat, which is the biggest in the fleet. It belongs to the RNLI which covers all of Great Britain. The lifeboat number is 17-09 and has a lot of emergencies in the Channel. The Severn class is designed to lay afloat. Built from fibre reinforced composite (FRC) the boat is lightweight yet very strong and is designed to right itself in the event of a capsize.
Education.
There are nine secondary level schools, 16 primary schools and two schools for special education.
Non-selective secondary schools include Astor College, St Edmund's Catholic School and Dover Christ Church Academy. Dover Grammar School for Boys and Dover Grammar School for Girls are the main grammar schools for the town.
The Duke of York's Royal Military School, England's only military boarding school for children of service personnel (co-ed ages 11–18), is also located in Dover, next to the former site of Connaught Barracks.
Dover College, a public school was founded in 1871 by a group of local business men.
Public services.
Dover has one hospital, Buckland Hospital located in a former Victorian workhouse on Coombe Valley Road. The town once had four hospitals, Buckland, Royal Victoria, Isolation and the Eye Hospitals located at various points across the town. Buckland Hospital is currently (2008) threatened with closure and various local organisations are trying to stop the cuts facing the hospital.
Astor College for the Arts federated with St Radigunds Primary School (then renamed White Cliffs Primary College for the Arts) to form the Dover Federation for the Arts (DFA). Subsequently Barton Junior School and Shatterlocks Nursery and Infant School joined the DFA. It has been a very successful Federation recognised by OFSTED with two schools rated as Outstanding and two being Good with outstanding features. Currently (November 2011) the DFA have applied to become a Federation of Academies.
As of January 2013, Buckland Hospital is to be rebuilt as a new community hospital on the same site, which will mean the loss of consultant clinics in Deal and likely total closure of the Deal Hospital site which has angered many people in Deal, who will be faced with a 50-minute round trip or more to the new hospital at Buckland once this is completed by 2015.
Local media.
Television.
Dover was the home to television studios and production offices of Southern Television Ltd, the company which operated the ITV franchise for South and South East England from 1958-1981. The studios were located on Russell Street and were home to programmes like 'Scene South East', 'Scene Midweek', 'Southern News', 'Farm Progress' and the nightly epilogue, 'Guideline'. The studios were operated by TVS in 1982 and home to 'Coast to Coast', however they closed a year later when the company moved their operations to the newly complete Television Centre in Maidstone.
Newspapers.
Dover has two paid for newspapers, the "Dover Express" (published by Kent Regional News and Media) and the "Dover Mercury" (published by the KM Group). Free newspapers for the town include the "Dover and Deal Extra", part of the KM Group; and "yourdover", part of KOS Media.
Radio.
Dover has one local commercial radio station, KMFM Shepway and White Cliffs Country, broadcasting to Dover on 106.8FM. The station was founded in Dover as Neptune Radio in September 1997 but moved to Folkestone in 2003 and was consequently rebranded after a takeover by the KM Group. Dover is also served by the county-wide stations Heart, Gold and BBC Radio Kent.
The Gateway Hospital Broadcasting Service, in Buckland Hospital radio, closed at the end of 2006. It was the oldest hospital radio station in East Kent being founded in 1968.
Dover Community Radio (DCR) currently offer internet programming and podcasts on local events and organisations on their website. The online station of the same name launched on 30 July 2011 offering local programmes, music and news for Dover and district.
SeaView Radio is a local community radio for Dover, currently broadcasting on the internet. playing a variety of music content from the past, present and future, be it 60's, 70's, 80's, 90's, Pop, Rock, Dance, R&B, Funk, Jazz or Soul, stretching over the last 4 decades.
Culture.
There are three museums: the main Dover Museum, the Dover Transport Museum, and the Roman Painted House.
International relations.
Twin towns / Sister cities.
Dover has three twin towns:
 Calais, France
Sports.
Dover Leisure Centre, is operated by , a not for profit charitable trust, caters for a wide range of sports and activities including a 25m swimming pool, teaching pool, sports hall, gym, fitness classes, sauna and steam room, is located on Townwall Street.
There are sports clubs catering for the usual range of sports, among them football (Dover Athletic F.C.) who play in the conference Premier league; rugby; rowing; swimming; water polo and netball (Dover and District Netball League).
One event which gets media attention is that of swimming the English Channel.
Sea fishing, from the beach, pier or out at sea, is carried out here. The so-called Dover sole ("solea solea") is found all over European waters.
Gallery.
 Dover Harbour, from the cliffs above.
Related Links.
Strait of Dover

</doc>
<doc id="40420" url="http://en.wikipedia.org/wiki?curid=40420" title="John Hancock Center">
John Hancock Center

The John Hancock Center is a 100-story, 1,127-foot (344 m) tall skyscraper, located at 875 North Michigan Avenue in the Streeterville area of Chicago, Illinois, United States. It was constructed under the supervision of Skidmore, Owings and Merrill, with chief designer Bruce Graham and structural engineer Fazlur Khan. When the building topped out on May 6, 1968, it was the tallest building in the world outside New York City. It is currently the fourth-tallest building in Chicago and the seventh-tallest in the United States (after One World Trade Center, the Willis Tower, the Trump Tower Chicago, the Empire State Building, the Bank of America Tower, and the Aon Center). When measured to the top of its antenna masts, it stands at 1506 ft. The building is home to offices and restaurants, as well as about 700 condominiums, and contains the third highest residence (above adjacent ground level) in the world, after the Trump Tower (also in Chicago), and the Burj Khalifa (in Dubai). The building was named for John Hancock Mutual Life Insurance Company, a developer and original tenant of the building, and has the nickname "Big John".
The 95th floor has long been home to a restaurant, the latest tenant being "The Signature Room on the 95th Floor". Diners can look out at Chicago and Lake Michigan. The Observatory attraction (called "360 Chicago" since March 2014) competes with the Willis Tower's Skydeck across town. John Hancock Center is in the heart of Michigan Avenue, a prime tourist hotspot in Chicago, while the Willis Tower is in the financial district. John Hancock Observatory allows a 360° view of the city, up to four states, and a distance of over 80 mi. The Observatory has Chicago's only open-air SkyWalk and also features a free multimedia tour in six languages, narrated by actor David Schwimmer. The 44th-floor sky lobby features America's highest indoor swimming pool.
History.
The project, which would at that time become the world's second tallest building, was originally conceived of and owned by Jerry Wolman in late 1964, the project being financed by John Hancock Mutual Life Insurance Co. Construction of the tower was interrupted in 1967 due to a flaw in an innovative engineering method used to pour concrete in stages that was discovered when the building was 20 stories high. The engineers were getting the same soil settlements for the 20 stories that had been built as what they had expected for the "entire" 99 stories. This forced the owner to stop development until the engineering problem could be resolved, and resulted in a credit crunch. This situation is similar to the one currently being experienced with the construction of Waterview Tower. The owner went bankrupt, which resulted in John Hancock taking over the project, which retained the original design, architect, engineer, and main contractor.
The building's first resident was Ray Heckla, the original building engineer, responsible for the residential floors from 44 to 92. Heckla moved his family in April 1969, before the building was completed.
On November 11, 1981, Veterans Day, high-rise firefighting and rescue advocate Dan Goodwin, for the purpose of calling attention to the inability to rescue people trapped in the upper floors of skyscrapers, successfully climbed the building's exterior wall. Wearing a wetsuit and using a climbing device that enabled him to ascend the I-beams on the building's side, Goodwin battled repeated attempts by the Chicago Fire Department to knock him off. Fire Commissioner William Blair ordered Chicago firemen to stop Goodwin by directing a fully engaged fire hose at him and by blasting fire axes through nearby glass from the inside. Fearing for Goodwin's life, Mayor Jane Byrne intervened and allowed him to continue to the top.
The John Hancock Center was featured in the 1988 movie "Poltergeist III".
On December 18, 1997, comedian Chris Farley was found dead in his apartment on the 60th floor of the John Hancock Center.
On March 9, 2002, part of a scaffold fell 43 stories after being torn loose by wind gusts around 60 mph crushing several cars, killing three people in two of them. The remaining part of the stage swung back-and-forth in the gusts repeatedly slamming against the building, damaging cladding panels, breaking windows, and sending pieces onto the street below.
On December 10, 2006, the non-residential portion of the building was sold by San Francisco based Shorenstein Properties LLC for $385 million and was purchased by a joint venture of Chicago-based Golub & Company and the Whitehall Street Real Estate Funds. Shorenstein had bought the building in 1998 for $220 million.
In June 2013, a venture of Chicago-based real estate investment firm Hearn Co., New York-based investment firm Mount Kellett Capital Management L.P. and San Antonio-based developer Lynd Co. closed on the expected acquisition of the Hancock's 856,000 square feet of office space and 710-car parking deck. The Chicago firm did not disclose a price, but sources said it was about $145 million.
The previous owners fetched about $410 million through an unusual process in which it sold off the tower at 875 N. Michigan Ave. in four separate pieces to widen the pool of potential buyers. The office and parking portion was the last step in that piecemeal sale process. The venture of Deutsche Bank AG and New York-based NorthStar Realty Finance Corp. paid an estimated $325 million for debt on the Hancock in 2012 after its previous owners defaulted on $400 million in loans. The NorthStar-Deutsche Bank venture already had sold the retail and restaurant space, the observatory and the broadcast antennas for a combined $256 million in three previous deals.
An annual stair climb race up the 94 floors from the Michigan Avenue level to the observation deck called 'Hustle up the Hancock' is held on the last Sunday of February. The climb benefits the Respiratory Health Association of Metropolitan Chicago. The record time as of 2007 is 9 minutes 30 seconds.
On April 16, 2009 at 6:00AM CDT, WYCC-TV transmitting off the John Hancock switched to all-digital broadcasting, becoming Chicago's first television station to stop broadcasting in an analog signal. WYCC-TV is one of only two Chicago market full-power television stations which broadcast from the top of the John Hancock Center. The other is WGBO-DT, while all of the other area stations broadcast from the top of the Willis Tower.
Design.
One of the most famous buildings of the structural expressionist style, the skyscraper's distinctive X-braced exterior shows that the structure's skin is part of its 'tubular system'. This is one of the engineering techniques which the designers used to achieve a record height (the tubular system is the structure that keeps the building upright during wind and earthquake loads). This X-bracing allows for both higher performance from tall structures and the ability to open up the inside floorplan. Such original features have made the John Hancock Center an architectural icon. It was pioneered by Bangladeshi-American structural civil engineer Fazlur Khan and chief architect Bruce Graham.
The building is protected by a fire sprinkler system.
The interior was remodeled in 1995, adding to the lobby travertine, black granite, and textured limestone surfaces. The elliptical-shaped plaza outside the building serves as a public oasis with seasonal plantings and a 12-foot (3.7 m) waterfall. A band of white lights at the top of the building is visible all over Chicago at night, and changes colors for different events. For example, at Christmas time the colors are green and red. When a Chicago-area sports team goes far in the playoffs, the colors are changed to match the team's colors.
The building is a member of the World Federation of Great Towers. It has won various awards for its distinctive style, including the Distinguished Architects Twenty-five Year Award from the American Institute of Architects in May 1999.
Height.
Including two antennas, the John Hancock Center has a height of 1,499 feet (457.2 m), making it the thirty-third tallest building in the world when measured to pinnacle height.
The Observatory elevators of the John Hancock center, manufactured by Otis, travel 96 floors at a top speed of 1800 ft/min.
360° Chicago.
Located on the 94th floor, 360° Chicago is the John Hancock Center's observatory. The floor of the observatory is 1,030 feet off of street-level below.The entrance can be found on the concourse level of the John Hancock Center; mainly accessible by the Michigan Avenue side of the building. The observatory, previously called the John Hancock Observatory, has been independently owned and operated since 2014 by the Montparnasse 56 Group out of Paris, France. The elevators are credited to be the fastest in the Western Hemisphere, at a top speed of 1,800 ft/min (20.5 mph). The observatory boasts larger floor space than its direct competitor, Skydeck at the Willis Tower. In addition, 360° Chicago has a cafe by Lavazza Coffee which stocks alcoholic beverages as well. The cafe is scheduled to be closed between January 15, 2015 until approximately May 1, 2015, for renovation. In the summer of 2014, 360° Chicago added it's TILT attraction. The TILT platform is an additional fee, and is a series of floor to ceiling windows that slowly tilt outside the building to 30°. The platform is on the observatory level, and faces south over the city. This observatory sees less attendance than the Skydeck at the Willis Tower, leading to a quieter and quicker experience.

</doc>
<doc id="40429" url="http://en.wikipedia.org/wiki?curid=40429" title="Waltzing Matilda">
Waltzing Matilda

"Waltzing Matilda" is Australia's most widely known bush ballad, and has been referred to as "the unofficial national anthem of Australia".
The title is Australian slang for travelling on foot (waltzing, derived from the German "auf der Walz") with one's belongings in a "Matilda" (swag) slung over one's back. The song narrates the story of an itinerant worker, or "swagman", making a drink of tea at a bush camp and capturing a jumbuck (sheep) to eat. When the sheep's owner arrives with three police officers to arrest the worker for the theft, the worker commits suicide by drowning himself in the nearby watering hole, after which his ghost haunts the site.
The original lyrics were written in 1895 by poet and nationalist Banjo Paterson. It was first published as sheet music in 1903. Extensive folklore surrounds the song and the process of its creation, to the extent that the song has its own museum, the Waltzing Matilda Centre in Winton, Queensland. In 2012, to remind Australians of the song's significance, Winton organised the inaugural Waltzing Matilda Day to be held on 6 April, the anniversary of its first performance.
The song was first recorded in 1926 as performed by John Collinson and Russell Callow. In 2008, this recording of "Waltzing Matilda" was added to the Sounds of Australia registry in the National Film and Sound Archive which says that there are more recordings of "Waltzing Matilda" than any other Australian song.
History.
Writing of the song.
The Australian poet Banjo Paterson wrote the words to "Waltzing Matilda" in January 1895 while staying at Dagworth Homestead, a livestock station near Winton in western Queensland owned by the Macpherson family. The words were written to a tune played on a zither or autoharp by 31 year-old Christina Macpherson, one of the family members at the station. Macpherson had heard the tune "The Craigielee March" played by a military band while attending the Warrnambool steeplechase horse racing in Victoria in April 1894, and played it back by ear at Dagworth. Paterson decided that the music would be a good piece to set lyrics to, and produced the original version during the rest of his stay at the station and in Winton.
The march itself was based on the Scottish Celtic folk tune "Thou Bonnie Wood of Craigielea", written by Robert Tannahill and first published in 1806, with James Barr composing the music in 1818. In the early 1890s it was arranged as the "The Craigielee" march music for brass band by Thomas Bulch. This tune itself was possibly based on the old melody of "Go to the Devil and Shake Yourself", composed by John Field (1782–1837) sometime before 1812. It is sometimes also called "When Sick Is It Tea You Want?" (London 1798) or "The Penniless Traveller" (O'Neill's 1850 collection).
It has been widely accepted that "Waltzing Matilda" is probably based on the following story:
In Queensland in 1891 the Great Shearers' Strike brought the colony close to civil war and was broken only after the Premier of Queensland, Samuel Griffith, called in the military. In September 1894, on a station called Dagworth (north of Winton), some shearers were again on strike. It turned violent with the strikers firing their rifles and pistols in the air and setting fire to the woolshed at the Dagworth Homestead, killing dozens of sheep. The owner of Dagworth Homestead and three policemen gave chase to a man named Samuel Hoffmeister – also known as "French(y)". Rather than be captured, Hoffmeister shot and killed himself at the Combo Waterhole.
Bob Macpherson (the brother of Christina) and Paterson are said to have taken rides together at Dagworth. Here they would probably have passed the Combo Waterhole, where Macpherson is purported to have told this story to Paterson. Although not remaining in close contact, Paterson and Christina Macpherson both maintained this version of events until their deaths. Amongst Macpherson's belongings, found after her death in 1936, was an unopened letter to a music researcher that read "... one day I played (from ear) a tune, which I had heard played by a band at the Races in Warrnambool ... he [Paterson] then said he thought he could write some words to it. He then and there wrote the first verse. We tried it and thought it went well, so he then wrote the other verses." Similarly, in the early 1930s on ABC radio Paterson said "The shearers staged a strike and Macpherson's woolshed at Dagworth was burnt down and a man was picked up dead ... Miss Macpherson used to play a little Scottish tune on a zither and I put words to it and called it "Waltzing Matilda"."
The song itself was first performed on 6 April 1895 by Sir Herbert Ramsay at the North Gregory Hotel in Winton, Queensland. The occasion was a banquet for the Premier of Queensland.
In February 2010 ABC News reported an investigation by barrister Trevor Monti that the death of Hoffmeister was more akin to a gangland assassination than to suicide. The same report asserts "Writer Matthew Richardson says the song was most likely written as a carefully worded political allegory to record and comment on the events of the shearers' strike."
Alternative theories.
There have been a number of alternative theories proposed for the origins or meaning of "Waltzing Matilda" since the time it was written, however most experts now essentially agree on the details outlined above. Some oral stories collected during the twentieth century claimed that Paterson had merely modified a pre-existing bush song, but there is no evidence for this. In 1905 Paterson himself published a book of bush ballads he had collected from around Australia entitled "Old Bush Songs", with nothing resembling "Waltzing Matilda" in it. Nor do any other publications or recordings of bush ballads include anything to suggest it pre-dated Paterson. Meanwhile handwritten manuscripts from the time the song originated indicate the song's origins with Paterson and Christina Macpherson, as do their own recollections and other pieces of evidence.
There has been speculation about the relationship "Waltzing Matilda" bears to an English song "The Bold Fusilier" (a.k.a. "Marching through Rochester", referring to Rochester in Kent, and the Duke of Marlborough), a song sung to the same tune and dated by some back to the 18th century but first printed in 1900. There is however no documentary proof that the "The Bold Fusilier" existed before 1900, and evidence suggests that this song was in fact written as a parody of "Waltzing Matilda" by English soldiers during the Boer War where Australian soldiers are known to have sung "Waltzing Matilda" as a theme. The first verse of "The Bold Fusilier" is:
<poem>A bold fusilier came marching back through Rochester
Off from the wars in the north country,
And he sang as he marched
Through the crowded streets of Rochester,
Who'll be a soldier for Marlboro and me?</poem>
In 2008 amateur Australian historian Peter Forrest claimed that the widespread belief that Paterson had penned the ballad as a socialist anthem, inspired by the Great Shearers' Strike, was false and a "misappropriation" by political groups. Forrest asserted that Paterson had in fact written the self-described "ditty" as part of his flirtation with Macpherson, despite his engagement to someone else. This theory was not shared by other historians like the Emeritus Professor in history and politics at Griffith University, Ross Fitzgerald, who argued that the defeat of the strike in the area he was visiting only several months before the song's creation would have been in Paterson's mind most likely consciously, but at least "unconsciously", and thus was likely an inspiration for the song. Fitzgerald stated "...the two things aren't mutually exclusive", a view shared by others who, while not denying the significance of Paterson's relationship with Macpherson, nonetheless recognise the underlying story of the shearer's strike and Hoffmeister's death in the lyrics of the song.
Ownership.
In 1903 Marie Cowan was hired to alter the song lyrics for use as an advertising jingle for Billy Tea, making it nationally famous. A third variation on the song, with a slightly different chorus, was published in 1907. Paterson sold the rights to "Waltzing Matilda" and "some other pieces" to Angus & Robertson Publishers for five pounds (the currency of the time).
The song was copyrighted by an American publisher, Carl Fischer Music, in 1941 as an original composition. Although no copyright applied to the song in Australia and many other countries, the Australian Olympic organisers had to pay royalties to Carl Fischer Music following the song being played at the 1996 Summer Olympics held in Atlanta. Arrangements such as those claimed by Richard D. Magoffin remain in copyright in America. 
Lyrics.
Typical lyrics.
There are no "official" lyrics to "Waltzing Matilda" and slight variations can be found in different sources. This version incorporates the famous "You'll never catch me alive said he" variation introduced by the Billy Tea company. Paterson's original lyrics referred to "drowning himself 'neath the coolibah tree".
<poem>
Once a jolly swagman camped by a billabong
Under the shade of a coolibah tree,
And he sang as he watched and waited till his billy boiled:
"Who'll come a-waltzing Matilda, with me?"
"Chorus:"
Waltzing Matilda, waltzing Matilda
You'll come a-waltzing Matilda, with me
And he sang as he watched and waited till his billy boiled:
"You'll come a-waltzing Matilda, with me."
Down came a jumbuck to drink at that billabong.
Up jumped the swagman and grabbed him with glee.
And he sang as he shoved that jumbuck in his tucker bag:
"You'll come a-waltzing Matilda, with me."
"(Chorus)"
Up rode the squatter, mounted on his thoroughbred.
Down came the troopers, one, two, and three.
"Whose is that jumbuck you've got in your tucker bag?
You'll come a-waltzing Matilda, with me."
"(Chorus)"
Up jumped the swagman and sprang into the billabong.
"You'll never take me alive!" said he
And his ghost may be heard as you pass by that billabong:
"Who'll come a-waltzing Matilda, with me?"
"(Chorus)"
</poem>
Glossary.
The lyrics contain many distinctively Australian English words, some now rarely used outside of the song. These include:
Variations.
The lyrics of "Waltzing Matilda" have been changed since it was written.
In a facsimile of the first part of the original manuscript, included in "Singer of the Bush", a collection of Paterson's works published by Lansdowne Press in 1983, the first two verses appear as follows:
Some corrections in the manuscript are evident; the verses originally read (differences in italics):
It has been suggested that these changes were from an even earlier version, and that Paterson was talked out of using this text, but the manuscript does not bear this out. In particular, the first line of the chorus was corrected before it had been finished, so the original version is incomplete.
The first published version, in 1903, differs slightly from this text:
By contrast with the original, and also with subsequent versions, the chorus of all the verses was the same in this version. This is also apparently the only version that that uses "billabongs" instead of "billabong".
Current variations of the third line of the first verse are "And he sang as he sat and waited by the billabong" or "And he sang as he watched and waited till his billy boiled". Another variation is that the third line of each chorus is kept unchanged from the first chorus, or is changed to the third line of the preceding verse.
There is also the very popular so-called Queensland version that has a different chorus, one very similar to that used by Paterson:
Status.
Official use.
The song has never been the officially recognised national anthem in Australia. Unofficially, however, it is often used in similar circumstances. The song was one of four included in a national plebiscite to choose Australia's national song held on 21 May 1977 by the Fraser Government to determine which song was preferred as Australia's national anthem. "Waltzing Matilda" received 28% of the vote compared with 43% for "Advance Australia Fair", 19% for "God Save the Queen" and 10% for "Song of Australia".
The lyrics are hidden on the final pages of Australian passports, such as above and below the words "notice" on some passports.
Sports.
"Waltzing Matilda" was used at the 1974 World Cup and at the Montreal Olympic Games in 1976 and, as a response to the New Zealand All Blacks haka, it has gained popularity as a sporting anthem for the Australia national rugby union team. It is also performed, along with "Advance Australia Fair", at the annual AFL Grand Final.
Matilda the Kangaroo was the mascot at the 1982 Commonwealth Games held in Brisbane, Queensland. Matilda was a cartoon kangaroo, who appeared as a 13-metre high (42 feet 8 inches) mechanical kangaroo at the opening ceremony, accompanied by Rolf Harris singing "Waltzing Matilda".
The Australian women's national soccer team is nicknamed the Matildas after this song.
Military units.
It is used as the quick march of the 1st Battalion, Royal Australian Regiment and as the official song of the U.S. 1st Marine Division, commemorating the time the unit spent in Australia during the Second World War. Partly also used in the British Royal Tank Regiment's slow march of "Royal Tank Regiment", because an early British tank model was called "Matilda".
Covers and derivative works.
"Waltzing Matilda" has been recorded by many Australian musicians and singers, including John Williamson, Peter Dawson, John Schumann, The Seekers, Tenor Australis, Thomas Edmonds, Rolf Harris, The Wiggles and Lazy Harry. Bands and artists from other nations, including Helmut Lotti, Wilf Carter (Montana Slim), The Irish Rovers, The Swingle Singers, and the Red Army Choir, have also recorded the song. According to the National Film and Sound Archive it has been recorded over 600 times. In 1983 the late country-and-western singer Slim Dusty's rendition became the first song to be broadcast to Earth by astronauts.
Film.
Versions of the song have been featured in a number of mainly Australian movies and television programs, which have included:
Sport.
"Waltzing Matilda" is a fixture at many Australian sporting events. These uses have included:
Stage.
On the occasion of Queensland's 150-year celebrations in 2009, Opera Queensland produced the revue "Waltzing Our Matilda", staged at the Conservatorium Theatre and subsequently touring twelve regional centres in Queensland. The show was created by Jason and Leisa Barry-Smith and Narelle French. The story line used the fictional process of Banjo Paterson writing the poem when he visited Queensland in 1895 to present episodes of four famous Australians: bass-baritone Peter Dawson (1882–1961), soprano Dame Nellie Melba (1861–1931), Bundaberg-born tenor Donald Smith (1922–1998), and soprano Gladys Moncrieff, also from Bundaberg. The performers were Jason Barry-Smith as Banjo Paterson, Guy Booth as Dawson, David Kidd as Smith, Emily Burke as Melba, Zoe Traylor as Moncrieff, and Donna Balson (piano, voice).

</doc>
<doc id="40433" url="http://en.wikipedia.org/wiki?curid=40433" title="Molde">
Molde

Molde (]) is a city and municipality in Romsdal in Møre og Romsdal county, Norway. The municipality is located on the Romsdal Peninsula, surrounding the Fannefjord and Moldefjord. The city is located on the northern shore of the Romsdalsfjord.
The city of Molde is the administrative center of Møre og Romsdal county, the administrative center of Municipality of Molde, the commercial hub of the Romsdal region, and the seat of the Diocese of Møre. Other main population centers in the municipality include Hjelset, Kleive, and Nesjestranda.
Molde has a maritime, temperate climate, with cool-to-warm summers, and relatively mild winters. The city is nicknamed "The Town of Roses".
It is an old settlement which emerged as a trading post in the late Middle Ages. Formal trading rights were introduced in 1614, and the city was incorporated through a royal charter in 1742. Molde was established as a municipality on 1 January 1838 (see formannskapsdistrikt)
The city continued to grow throughout the 18th and 19th centuries, becoming a centre for Norwegian textile and garment industry, as well as the administrative center for the region, and a major tourist destination. After World War II, Molde experienced accelerated growth, merging with Bolsøy Municipality and parts of Veøy Municipality on 1 January 1964, and has become a center for not only administrative and public services, but also academic resources and industrial output.
History.
The city's current location dates from the late medieval period, but is preceded by the early medieval township on Veøya, an island to the south of present day Molde. The settlement at Veøya probably dates from the Migration Period, but is first mentioned in the sagas by Snorri Sturluson as the location of the "Battle of Sekken" in 1162, where king Håkon the Broad-shouldered was killed fighting the aristocrat Erling Skakke, during the Norwegian civil wars.
However, settlement in the area can be traced much further back in time—evidence given by two rock slabs carved with petroglyphs found at Bjørset, west of the city center.
At the eve of the 15th century, the influence of Veøy waned, and the island was eventually deserted. However, commercial life in the region was not dead, and originating from the two settlements at Reknes and Molde (later "Moldegård"), a minor port called "Molde Fjære" ("Molde Landing") emerged, based on trade with timber and herring to foreign merchants. The town gained formal trading rights in 1614. During the Swedish occupation of Middle Norway, 1658–1660, after Denmark-Norway's devastating defeat in the Northern Wars, the town became a hub of resistance to the Swedes. After the rebellion and liberation in 1660, Molde became the administrative center of Romsdalen Amt and was incorporated through a royal charter in 1742. Molde continued to grow throughout the 18th and 19th Centuries, becoming a center for Norwegian textile and garment industry. Tourism later became a major industry, and Molde saw notabilities such as the German emperor Wilhelm II of Germany and the Prince of Wales as regular summer visitors. Molde consisted of luxurious hotels surrounding an idyllic township with quaint, wooden houses, lush gardens and parks, esplanades and pavilions, earning it the nickname "the Town of Roses". This was interrupted when one third of the city was destroyed in a fire on 21 January 1916. However, Molde recovered and continued to grow in the economically difficult interbellum period.
A second fire, or series of fires, struck from the German air-raids in April and May 1940, which destroyed about two thirds of the town. Molde was in effect the capital of Norway for a week after King Haakon, Crown Prince Olav, and members of the government and parliament arrived at Molde on April 23, after a dramatic flight from Oslo. They were put up at Glomstua, then at the western outskirt of the town, and experienced the bombing raids personally. The Norwegian gold reserve was also conveyed to Molde, and was hidden in a clothing factory.
However, German intelligence was well aware of this, and on April 25 the Luftwaffe initiated a series of air-raids. For a week the air-raid siren on the chimney of the dairy building announced the repeated attacks. April 29 turned out to be the worst day in the history of Molde, as the city was transformed into a sea of flames by incendiary bombs. Until then the church had escaped undamaged, but in the final sortie a firebomb got stuck high up in the tower, and beautiful wooden church was obliterated by fire.
After World War II, Molde experienced tremendous growth. As the modernization of the Norwegian society accelerated in the post-reconstruction years, Molde became a center for not only administrative and public services, but also academic resources and industrial output. After the consolidation of the town itself and its adjacent communities in 1964, Molde became a modern city, encompassing most branches of employment, from farming and fisheries, through industrial production, to banking, higher education, tourism, commerce, health care, and civil administration.
Municipality.
The city of Molde was established as an urban municipality on 1 January 1838 (see formannskapsdistrikt). It was surrounded by the rural municipality of Bolsøy. On 1 July 1915, a part of Bolsøy (population: 183) was transferred to the city of Molde. On 1 January 1952, another part of Bolsøy (population: 1,913) was transferred to Molde. On 1 January 1964, Molde (population: 8,289) merged with the Sekken, Veøya, and Nesjestranda parts of municipality of Veøy (population: 756), all of Bolsøy (population: 7,996), and the Mordal area of Nord-Aukra (population: 77) to form the present day municipality.
Name.
The city is named after the original settlement on the farmstead of Molde (Old Norse: "Moldar"). The name is the plural form of either "mold" which means "fertile soil" or "moldr" which means "skull" or "mold" (thus in reference to the rounded peaks in Moldemarka).
Pronunciation varies between the standard "Molde" and the rural "Molle". A person from Molde will refer to him/herself as a "Moldenser".
Coat-of-arms.
The coat-of-arms was granted on 29 June 1742. It shows a whale chasing herring into a barrel, portraying the city's founding industry of rich herring fisheries, which also alleviated the city during a major famine of the early 1740s. The sighting of whales, usually pods of orca following the schools of fish into the fjords to feed, were commonly held to be the start of the spring herring fisheries.
Moldesangen ("The Song of Molde") is the city's semi-official anthem. It was written by Palle Godtfred Olaus Dørum (1818–1886) and composed by Karl Groos (1789–1861), supposedly in 1818.()
Geography.
Molde proper consists of a 10 km long and 1 to wide strip of urban land running east-west along the north shore of the Moldefjord, an arm of the Romsdalsfjord, on the Romsdal peninsula. The city is sheltered by Bolsøya and the Molde archipelago, a chain of low-lying islands and islets, to the south, and the wood-clad hills of Moldemarka to the north. The city center is located just west of the river Moldeelva, which runs into the city from the north, originating in the Moldevatnet lake, through the valley Moldedalen. Despite the river being minor and seasonal, it supported several sawmills in the 16th and 17th centuries. This gave rise to the original town itself through a combination of a good harbor, proximity to the sea routes, vast timber resources, and a river capable of supporting mills. In 1909, the river housed the first hydro electric power plant capable of providing sufficient electricity for the city, and the upper reaches of the river still provides drinking water for most of the city.
Its panoramic view of some 222 partly snow-clad peaks, usually referred to as the Molde panorama, is one of the its main attractions, and have drawn tourists to the city since the 19th century. Molde is nicknamed the "Town of Roses", a name which originated during Molde's era as a tourist destination of international fame in the late 19th century.
Neighboring municipalities are Aukra, Gjemnes, and Fræna (to the north); Midsund (to the west); Vestnes and Rauma (to the south); and Nesset (to the east). Other centers of population in the area include Åndalsnes, Vestnes, and Elnesvågen.
Climate.
Molde has a maritime, temperate climate, with cool-to-warm summers, and relatively mild winters. The annual precipitation is medium high, with an average of 1640 mm per year. The warmest season is late summer. Molde holds the national high for the month of October, with 25.6 C (on 11 October 2005). The driest season is May–June. Due to its geographic location, Molde experiences frequent snowfalls in winter, but this snow is usually wet as the winters are usually mild. Due to the effects of Gulf Stream, the city rarely experiences lasting cold spells, and the average temperature is well above the average for its latitude.
A natural phenomenon occurring in Molde and the adjacent district, are frequent winter days with temperatures above 10 C, sometimes even above 15 C. This is due to foehn wind from south and south-east. Combined with a steady influx of warm, moist south-westerly winds from the Atlantic Ocean, warmed by the North Atlantic Current, it gives Molde a climate much warmer than its latitude would indicate. The sheltered location of the city, facing south with hills to the north, mountains to the east and mountainous islands to the west, contributes to Molde's climate and unusually rich plant life, especially among species naturally growing on far lower latitudes, like maple, chestnut, oak, tilia ("lime" or "linden"), beech, yew, and others.
Points of interest.
Salmon, sea trout and sea char are found in the rivers of the area, especially the Rauma, Driva, and Eira, already legendary among the British gentry in the mid-19th century. Trout is abundant in most lakes. Cod, pollock, saithe, mackerel and other species of saltwater fish are commonly caught in the Romsdalsfjord, both from land and from boat. Skiing is a popular activity among the inhabitants of Molde in the winter, on groomed tracks, in resorts or by own trail. There are several popular rock climbing, ice climbing, bouldering, glacier and basejumping areas in the immediate surroundings of Molde.
The "Atlantic road" was voted the Norwegian Construction of the Century in 2005. It is built on bridges and landfills across small islands and skerries, and spans from the small communities of Vikan and Vevang to Averøy, an island with several historic landmarks, such as the Bremsnes cave with Mesolithic findings from the Fosna culture, the medieval Kvernes stave church, and Langøysund, now a remote fishing community, but once a bustling port along the main coastal route. Langøysund was the site of the compromise between King Magnus I and the farmers along the coast in 1040. The compromise is regarded as Norway's Magna Carta, and is commemorated though the "Pilespisser" (English: Arrowheads) monument.
"Trollkirka" (English: lit. "Troll Church") is a marble grotto leading up to an underground waterfall. The grotto is situated 30 minutes outside Molde, followed by a 1-hour hike up a steep trail. "Trollveggen" is Europe’s tallest vertical, overhanging mountain face, with several very difficult climbing routes. "Trollstigen" is the most visited tourist road in Norway. The road twists and turns its way up an almost vertical mountainside through 11 hairpin bends to an altitude of 858 m. "Mardalsfossen" is the highest waterfall in Northern Europe and the fourth highest waterfall in the world, cascading 297 metres down into the valley. The total height of the waterfall is 655 m.
Bud is a fishing village on the very tip of the Romsdal peninsula. It gained importance during the Middle Ages as a trading post, and hosted the last free Privy Council of Norway in 1533, a desperate attempt to save the country's independence and stave off the Protestant Reformation, led by Olav Engelbrektsson, archbishop of Nidaros (today "Trondheim"). The massive Ergan coastal defences, a restored German coastal fort from World War II, and a part of the Atlantic Wall, is situated in Bud. The fishing communities of Ona, Bjørnsund and Håholmen are located on remote islands off the coast, only accessible by boat or ferry.
Moldemarka.
Moldemarka, the hilly woodland area north of the city, is public land. The area has an extensive network of paths, walking trails and skiing tracks. Forest roads enter the area from several directions. Bulletin boards and maps provide information regarding local plants and wildlife, as well as signposts along the trails. Marked trails lead to a number of peaks, sites and fishing lakes and rivers. A national fishing license is required to fish in the lakes and streams.
Varden, 407 m above sea level is a viewpoint directly above Molde, with a good view of the city, the fjord with the Molde archipelago and the Molde panorama.
 Molde Panorama. The Rica Seilet Hotel can be seen towards the west, besides the Aker Stadion. 
Transportation.
Hurtigruta calls on Molde every day, on its journey between Bergen and Kirkenes. The nearest railway station is Åndalsnes, the terminus for Raumabanen.
The city's airport at Årø has several daily flights to Oslo, Bergen, and Trondheim, as well as weekly flights to other domestic and international destinations.
The European route E39 and Norwegian County Road 64 both pass through the municipality. The city of Molde is connected to Fræna Municipality (to the north) by the Tussen Tunnel. The city is connected to the Røvika and Nesjestranda part of the municipality by the Fannefjord Tunnel and Bolsøy Bridge, significantly shortening the drive by avoiding driving all the way around the Fannefjorden. The proposed Langfjord Tunnel would connect Molde Municipality to Rauma Municipality via a tunnel under the Langfjorden.
Culture.
Three of the "four great" Norwegian authors is connected to Molde. Bjørnstjerne Bjørnson spent his childhood years at Nesset outside Molde, and attended school in the city. Henrik Ibsen frequently spent his vacations at the mansion "Moldegård" visiting the family Møller; and Alexander Kielland resided in the city as the governor of Romsdals amt. Ibsen's play "Rosmersholm" is generally thought to be inspired by life at the mansion Moldegård, and "The Lady from the Sea" is also believed to be set in the city of Molde, although never actually mentioned. Other authors from or with ties to Molde include Edvard Hoem, Jo Nesbø, Knut Ødegård, and Nini Roll Anker, a friend of Sigrid Undset.
The Romsdal Museum, one of Norway's largest folk museums, was established in 1912. Buildings originating from all over the region have been moved here to form a typical cluster of farm buildings including "open hearth" houses, sheds, outhouses, smokehouses and a small chapel. The "town street" with Mali's Café shows typical Molde town houses from the pre-World War I period. The Museum of the Fisheries is an open air museum located on the island of Hjertøya, 10 minutes from the center of Molde. A small fishing village with authentic buildings, boats and fishing equipment, the museum shows local coastal culture from 1850 onwards.
The local newspaper is Romsdals Budstikke.
Churches.
The Church of Norway has five parishes "(sokn)" within the municipality of Molde. It is part of the Molde arch-deanery in the Diocese of Møre.
Festivals.
The Moldejazz jazz festival is held in Molde every July. Moldejazz is the one of the largest and oldest jazz festival in Europe, and one of the most important. An estimated 40,000 tickets are sold for the more than a hundred events during the festival. Between 80,000 and 100,000 visitors visit the city during the one-week-long festival.
Every August, Molde and Nesset are hosts to the Bjørnson Festival, an international literature festival. Established by the poet Knut Ødegård in connection with the 250-year anniversary of Molde, the festival is named in honour of the Nobel Prize in Literature laureate Bjørnstjerne Bjørnson (1832–1910). It is the oldest and the most internationally acclaimed literature festival in Norway.
In addition to the two major events, a number of minor festivals are held annually. Byfest, the city's celebration of incorporation, is an arrangement by local artists, coinciding with the anniversary of the royal charter of 29 June 1742.
Education.
Molde University College offers a wide range of academic opportunities, from nursing and health related studies, to economics and administrative courses. The school is Norway's leading college in logistics, and well established as a center for research and academic programs in information technology, with degrees up to and including PhD.
Molde University College is also one of the country's leading institutions in international student exchange and programs conducted in English.
Sports.
Molde hosts a variety of sports teams, most notably the football team, Molde FK, which is playing in the Norwegian Premier League. Home matches are played at Aker stadion, inaugurated in 1998, which holds a record attendance of 13,308. The team is the reigning league champions (2011, 2012 and 2014), four-time Norwegian Cup winners (1994, 2005, 2013 and 2014), and has numerous appearances in European tournaments, including the UEFA Champions League. The club was founded in 1911, during Molde's period of great British and Continental influx, and was first named "International", since it predominantly played teams made up from crews of foreign vessels visiting the city.
In addition to a number of international players, the city has also produced several ski jumpers, cross-country and alpine skiers of international merit.
Other sports include the accomplished team handball clubs (SK Træff, SK Rival), athletics teams (IL Molde-Olymp), skiing clubs, basketball and volleyball teams.
International relations.
Twin towns — Sister cities.
Molde has five sister cities. They are: 

</doc>
