<doc id="10312" url="http://en.wikipedia.org/wiki?curid=10312" title="East Pakistan">
East Pakistan

East Pakistan (Bengali: পূর্ব পাকিস্তান "Pūrbô Pākistān"; Urdu: مشرقی پاکستان‎ "Mas̱ẖriqī Pākistān" ]), present-day Bangladesh, was a provincial state of Pakistan that existed in the Bengal region of the northeast of South Asia from 1955 until 1971, following the One Unit programme that laid the existence of East Pakistan.
In 1947, the region of Bengal under the British Empire was divided into East and West Bengal that separated the eastern areas with a Muslim majority from the western areas with a Hindu majority. The partition of Bengal saw the mainstream revival of Hindu–Muslim riots that drove both Bengali Muslims and Hindus further apart, leading to more unrest in Bengal. In 1947, districts of Bengal with a Muslim majority favoured the division after approving the 3 June Plan presented by the Viceroy of India Lord Mountbatten, and merged with the new province of East Bengal of the Dominion of Pakistan. From 1947 until 1954, East Bengal was an independent administrative unit which was governed by the Pakistan Muslim League led by Nurul Amin. In 1955, the Bengali Prime minister Muhammad Ali Bogra devolved the province of East Bengal and established the state as East Pakistan with Dhaka its state capital. During this time, the 1954 elections were held which saw the complete defeat of Pakistan Muslim League led by the United Front coalition of the Awami League, the Krishak Praja Party, the Democratic Party and Nizam-e-Islam. The Awami League gained the control of East Pakistan after appointing Huseyn Suhrawardy for the office of Prime minister. This authoritarian period that existed from 1958 until 1971, is often regarded as period of mass repression, resentment, and political neglect and ignorance. Allying with the population of West, the East's population unanimously voted for Fatima Jinnah during the 1965 presidential elections against Ayub Khan. The elections were widely believed to be heavily rigged in the favour of Ayub Khan using state patronage and intimidation to influence the indirectly elected electoral college. The economic disparity, impression that West Pakistan despite being less populated than East Pakistan was ruling and prospering at its cost further popularize the Bengali nationalism. The support for state autonomy grew when Awami League introduced the Six point movement in 1966, and participated with full force in the 1970 general elections in which the Awami League had won and secured the exclusive mandate of East-Pakistan.
After the general elections, President General Yahya Khan attempted to negotiate with both Pakistan Peoples Party and Awami League to share power in the central government but talks failed when President Yahya Khan authorised an armed operation (codename "Searchlight") to attack the Awami League. As response to this operation, the Awami League announced the declaration of independence of East Pakistan on 26 March 1971 and began an armed struggle against the Pakistan, with India staunchly supporting Awami League by the means of providing arm ammunition to its guerrilla forces.
East Pakistan had an area of 147,570 km2 (56,977 mi2), bordering India on three sides (East, North, and West) and the Bay of Bengal to the South. East Pakistan was one of the largest provincial states of Pakistan, with the largest population, largest political representation, and sharing the largest economic share. A nine-month-long war ended on 16 December 1971, when the Pakistan Armed Forces were overrun in Dhaka, ultimately signing the instrument of surrender which resulted in the largest number of prisoners of war since World War II. Finally on 16 December 1971, East Pakistan was officially disestablished and was succeeded as the independent state of Bangladesh.
Geographical history.
Many notable Muslim Bengali figures were among the of present date, State of Pakistan. The country was born in bloodshed and came into existence on 14 August 1947 confronted by seemingly insurmountable problems. As many as 12 million people Muslims leaving India for Pakistan, and Hindus and Sikhs opting to move to India from the new state of Pakistan which had been involved in the mass transfer of population between the two countries, and perhaps two million refugees had died in the violence that had accompanied the migrations in the borders of West Pakistan. Pakistan's boundaries were established hastily without adequate regard for the new nation's economic viability. Even the minimal requirements of a central government, skilled personnel and officers, equipment, and a capital city with government buildings were missing. Until 1947, the East Wing of Pakistan, separated from the West Wing by 1,600 km of Indian territory, had been heavily dependent on Hindu management. Many Bengali Hindus left for Calcutta after independence, and their place, particularly in commerce, was taken mostly by Muslims who had migrated from the Indian state of Bihar or by West Pakistanis from different provinces.
Political history.
Bengal was divided into two provinces on 3 July 1946 in preparation for the independence, the Hindu majority of West Bengal and the Muslim majority of East Bengal. The two provinces each had their own Chief Ministers and Governors. In August 1947, the West Bengal became part of India and East Bengal became part of Pakistan. Throughout this time, the tensions between East Bengal and the West Pakistan led to the One-Unit policy by Bengali Prime Minister Muhammad Ali Bogra. In 1955, most of the western wing was combined to form a new West Pakistan province (which contained four provinces and four territories) while East Bengal became the new province of East Pakistan (a single provisional state). In 1955, Bogra appointed communist leader Abu Hussain Sarkar as Chief Minister and Amiruddin Ahmad as Governor.
Following the promulgation of 1956 Constitution, Prime minister Bogra appointed Bengali bureaucrat and retired Major-General Iskander Mirza was as Interior minister and the Army Commander of army General Ayub Khan as the Defence minister whilst Muhammad Ali remained Economic minister. The main objective of the new government was to end disruptive provincial politics and to provide the country with a new constitution. After a revision, the Supreme Court of Pakistan declared that the Pakistan Constituent Assembly must be called. Governor-General Ghulam Mohammad was unable to circumvent the order, and the new Constituent Assembly, elected by the provincial assemblies, met for the first time in July 1955. Bogra, who had little support in the new assembly, fell in August and was replaced by Choudhry. Ghulam Mohammad, plagued by poor health, was succeeded as governor general in September 1955 by Mirza.
Language Movement and 1954 Provincial Elections.
Previously in 1952, then Chief Minister Nurul Amin who was firmly against the agitation, stated that the communists had played an integral and major role in staging the massive protests, mass demonstration, and strikes for the Bengali Language Movement. All over the country, the political parties had favored the general elections in Pakistan with the exception of Muslim League. The military, bureaucracy and the United States was nervous with good reasons where the support for the Soviet Union began to rise in both East and West. Finally in 1954, the legislative elections were to be held for the Parliament. Unlike in West, not all of the Hindu population migrated to India, instead a large number of Hindu population was in fact present in the state. The communist influence deepened and was finally realised in the elections. The United Front, Communist Party of Pakistan and the Awami League returned to power, inflicting severe defeat to the Muslim League. Out of 309, the Muslim League only won 10 seats, whereas the communist party had 4 seats of the ten contested. The communists working with other parties had secured 22 additional seats, totalling 26 seats. The right-wing Jamaat-e-Islami had completely failed in the elections.
In 1955, the United Front named Abu Hussain Sarkar as the Chief Minister of the State who ruled the state in two non-consecutive terms until 1958 when the martial law was imposed.
Martial law.
In East Pakistan, the political impasse culminated in 1958 in a violent scuffle in the East-Pakistan parliament between the members of the Pakistan Muslim League and the East-Pakistan police, in which the deputy speaker was fatally injured and two ministers badly wounded. Uncomfortable with the workings of democratic system, unruliness in the East Pakistan parliamentary elections and the threat of Baloch separatism in West-Pakistan, Bengali President Iskandar Ali Mirza issued a proclamation that abolished all political parties in both West and East Pakistan, abrogated the two-year-old constitution, and imposed the first martial law in the country on 7 October 1958.
President Iskander Mirza announced that "the martial law would be a temporary measure, lasting only until a new constitution was to be drafted. On 27 October, President Mirza swore in a twelve-member cabinet that included Army Commander General Ayub Khan as Defence Minister as well as chief martial law administrator of the country, along with three other senior military officers in ministerial positions. The cabinet included among the eight civilians, one of them being Zulfikar Ali Bhutto, a former Karachi University lecturer.
Roughly after two weeks, President Mirza's relations with Pakistan Armed Forces deteriorated leading Army Commander General Ayub Khan relieving the president from his presidency and forcefully exiling President Mirza to United Kingdom. General Ayub Khan justified his actions after appearing on national radio declaring that: "the armed forces and the people demanded a clean break with the past...". Until 1962, the martial law continued while Field Marshal Ayub Khan purged a number of politicians and civil servants from the government and replaced them with military officers. Ayub called his regime a "revolution to clean up the mess of black marketing, (sic), and corruption.".
Presidential republic and economy.
The martial law continued until 1962 when the government of Field Marshal Ayub Khan commissioned a constitutional bench under Chief Justice of Pakistan, Muhammad Shahabuddin, containing ten senior justices, each five from East Pakistan and five from West Pakistan. On 6 May 1961, the commission sent its draft to President Ayub Khan who thoroughly examined the draft with consulting with his cabinet. In January 1962, the cabinet finally approved the text of the new constitution, promulgated by President Ayub Khan on 1 March 1962 and finally came into effect on 8 June 1962. With the success of 1962 constitution, East Pakistan became a Presidential republic and abolished all parliamentary institutions in East Pakistan. The 1962 constitution had provided the presidential system for both provincial states (West Pakistan and East Pakistan) that each states were given autonomy to run their separate presidential provincial governments. The responsibilities and authority of the center and the provinces were clearly listed in the Constitution.
During the years between 1960 and 1965, the annual rate of growth of the gross domestic product per capita was 4.4% in the West Pakistan versus 2.6% in East Pakistan. Furthermore, Bengali politicians pushing for more autonomy stated that much of Pakistan's export earnings were generated in East Pakistan by the export of Bengali jute and tea. As late as 1960, approximately 70% of Pakistan's export earnings originated in the East Wing, although this percentage declined as international demand for jute dwindled. By the mid-1960s, East Pakistan was accounting for less than 60% of the nation's export earnings, and by the time of Bangladesh's independence in 1971, this percentage had dipped below 50%. This reality did not dissuade Mujib from demanding in 1966 that separate foreign exchange accounts be kept and that separate trade offices be opened overseas. By the mid-1960s, West Pakistan was benefiting from Ayub's "Decade of Progress," with its successful "green revolution" in wheat, and from the expansion of markets for West Pakistani textiles, while East Pakistan's standard of living remained at an abysmally low level. The Bengalis were also upset that West Pakistan, because it was the seat of government, was the major beneficiary of foreign aid.
Military government.
With Ayub Khan ousted from office in 1969, Commander of the Pakistani Army, General Yahya Khan became the country's second ruling Chief Martial Law Administrator. Both Bhutto and Mujib strongly disliked General Khan, but patiently endured him and his government as he had promised to hold an election in 1970. During this time, strong nationalistic sentiments in East Pakistan were perceived by the Pakistani Armed Forces and the central military government. Therefore, Khan and his military government wanted to divert the nationalistic threats and violence against non-East Pakistanis. The Eastern Military High Command was under constant pressure from the Awami League, and requested an active duty officer to control the command under such extreme pressure. The high flag rank officers, junior officers and many high command officers from the Pakistan's Armed Forces were highly cautious about their appointment in East-Pakistan, and the assignment of governing East Pakistan and appointment of an officer was considered highly difficult for the Pakistan High Military Command.
Civil disobedience.
East Pakistan's Armed Forces, under the military administrations of Major-General Muzaffaruddin and Lieutenant-General Sahabzada Yaqub Khan, used an excessive amount of show of military force to curb the uprising in the province. With such action, the situation became highly critical and civil control over the province slipped away from the government. On 24 March, dissatisfied with the performance of his generals, Yahya Khan removed General Muzaffaruddin and General Yaqub Khan from office on 1 September 1969. The appointment of a military administrator was considered quite difficult and challenging with the crisis continually deteriorating. Vice-Admiral Syed Mohammad Ahsan, Chief of Naval Staff of Pakistan Navy, had previously served as political and military adviser of East Pakistan to former President Ayub Khan. Having such a strong background in administration, and being an expert on East Pakistan affairs, General Yahya Khan appointed Vice-Admiral Syed Mohammad Ahsan as Martial Law Administrator, with absolute authority in his command. He was relieved as Chief of Naval Staff, and received extension from the government. On 1 September Admiral Ahsan assumed the command of the Eastern Military High Command, and became a unified commander of Pakistan Armed Forces in East-Pakistan. Under his command, the Pakistani Armed Forces were removed from the cities and deployed along the border. The rate of violence in East Pakistan dropped, nearly coming to an end. Civil rule improved and stabilised in East Pakistan under Martial Law Administrator Admiral Ahsan's era. The next year, in 1970, it was in this charged atmosphere that parliamentary elections were held in the country in December 1970.
Position towards West Pakistan.
The tense diplomatic relations between East and West Pakistan reached a climax in 1970 when the Awami League, the largest East Pakistani political party, led by Sheikh Mujibur Rahman, (Mujib), won a landslide victory in the national elections in East Pakistan. The party won 160 of the 162 seats allotted to East Pakistan, and thus a majority of the 300 seats in the Parliament. This gave the Awami League the constitutional right to form an absolute government. Khan invited Mujib to Rawalpindi to take the charge of the office, and negotiations took place between the military government and the Awami Party. Bhutto was shocked with the results, and threatened his Peoples Party's members if they attended the inaugural session at the National Assembly. Bhutto was famously heard saying "break the legs" of any member of his party who dared enter and attend the session. However, fearing East Pakistani separatism, Bhutto demanded Mujib to form a coalition government. After a secret meeting held in Larkana, Mujib agreed to give Bhutto the office of Presidency with Mujib as Prime Minister. General Yahya Khan and his military government were kept unaware of these developments and under pressure from his own military government, refused to allow Rahman to become the Prime Minister of Pakistan. This increased agitation for greater autonomy in East Pakistan. The Military Police arrested Mujib and Bhutto and placed them in Adiala Jail in Rawalpindi. The news spread like a fire in both East and West Pakistan, and the struggle for independence began in East Pakistan.
The senior high command officers in Pakistan Armed Forces, and Zulfikar Ali Bhutto, began to pressure General Yahya Khan to take armed action against Mujib and his party. Bhutto later distanced himself from Yahya Khan after he was arrested by Military Police along with Mujib. Soon after the arrests, a high level meeting was chaired by Yahya Khan. During the meeting, high commanders of Pakistan Armed Forces unanimously recommended an armed and violent military action. East Pakistan's Martial Law Administrator Admiral Ahsan, unified commander of Eastern Military High Command (EMHC), and Air Marshal Mitty Masud, Commander of Eastern Air Force Command (EAFC), were the only officers to object to the plans. When it became obvious that a military action in East Pakistan was inevitable, Admiral Ahsan resigned from his position as Martial Law Administrator in protest, and immediately flew back to Karachi, West Pakistan. Disheartened and isolated, Admiral Ahsan took early retirement from the Navy and quietly settled in Karachi. Once Operation Searchlight and Operation Barisal commenced, Air Marshal Masud flew to West Pakistan, and unlike Admiral Ahsan, tried to stop the violence in East Pakistan. When he failed in his attempts to meet General Yahya Khan, Masud too resigned from his position as Commander of Eastern Air Command, and took retirement from Air Force.
Final years and war.
Lieutenant-General Sahabzada Yaqub Khan was sent into East Pakistan in emergency, following a major blow of the resignation of Vice Admiral Ahsan. General Yaqub temporarily assumed the control of the province, as he was made the unified commander of Pakistan Armed Forces. General Yaqub mobilised the entire major forces in East Pakistan, and were re-deployed in East Pakistan.
Bangabandhu Sheikh Mujibur Rahman, the undisputed leader of the 75 million people of Bangladesh, in due fulfillment of the legitimate right of self-determination of the people of Bangladesh, duly made a declaration of independence at Dacca on March 26, 1971. All major Awami League leaders including elected leaders of National Assembly and Provincial Assembly fled to neighbouring India and an exile government was formed headed by Mujibur Rahman. While he was in Pakistan Prison, Syed Nazrul Islam was the acting President with Tazuddin Ahmed as the Prime Minister. The exile government took oath on 17 April 1971 at Mujib Nagar, within East Pakistan territory of Kustia district and formally formed the government. Colonel MOG Osmani was appointed the Commander in Chief of Liberation Forces and whole East Pakistan was divided into eleven sectors headed by eleven sector commanders. All sector commanders were Bengali officers from defected Pakistan Army. This started the Bangladesh Liberation War in which the freedom fighters, joined in December 1971 by 400,000 Indian soldiers, faced the Pakistani Armed Forces of 365,000 plus Paramilitary and collaborationist forces. An additional approximately 25,000 ill-equipped civilian volunteers and police forces also sided with the Pakistan Armed Forces. Bloody guerrilla warfare ensued in East Pakistan.
Dissolution of East Pakistan.
The Pakistan Armed Forces were unable to counter such threats. Poorly trained and inexperienced in guerrilla tactics, Pakistan Armed Forces and their assets were successfully sabotaged by the Bangladesh Liberation Forces. On April 1971, Lieutenant-General Tikka Khan succeeded General Yaqub Khan as Commander of unified forces. General Tikka Khan led the massive violent and massacre campaigns in the region. He is held responsible for killing hundreds of thousands of Bengali people in East Pakistan, mostly civilians and unarmed peoples. For his role, General Tikka Khan gained the title as "Butcher of Bengal". General Khan faced an international reaction against Pakistan, and therefore, General Tikka was removed as Commander of Eastern front. He installed a civilian administration under Abdul Motaleb Malik on 31 August 1971, which proved to be ineffective. However, during the meeting, with no high officers willing to assume the command of East Pakistan, Lieutenant-General Amir Abdullah Khan Niazi volunteered for the command of East Pakistan. Inexperienced and the large magnitude of this assignment, the government sent Vice-Admiral Mohammad Shariff as second-in-command of General Niazi. Admiral Shariff served as the deputy unified commander of Pakistan Armed Forces in East Pakistan. However, General Niazi proved to be a failure and ineffective ruler. Therefore, General Niazi and Air Marshal Enamul Haque, Commander of Eastern Air Force Command (EAFC), failed to launch any operation in East Pakistan against Indian or its allies. Except Admiral Shariff who continued to press pressure on Indian Navy iuntil the end of the conflict. Admiral Shariff made it nearly impossible for Indian Navy to land its naval forces on the shores with his well effective plans . The Indian Navy was unable to access East Pakistan and the Pakistan Navy was still offering resistance. The Indian Army, therefore, from all three directions of the province, entered East Pakistan. The Indian Navy then decided to wait near the Bay of Bengal until the Army reached the shore.
The Indian Air Force dismantled the capability of Pakistan Air Force in East Pakistan. Air Marshal Enamul Haque, Commander of Eastern Air Force Command (EAFC), failed to offer any serious resistance to the actions of the Indian Air Force. For most part of the war, the IAF enjoyed complete dominance in the skies over East Pakistan.
Surrender of the Pakistan Armed Forces.
On 16 December 1971 , the Pakistan Armed Forces surrendered to the joint liberation forces of Mukti Bahini and the Indian army, headed by Lieutenant-General Jagjit Singh Arora, the General Officer Commanding-in-Chief (GOC-in-C) of the Eastern Command of the Indian Army. Lieutenant General AAK Niazi, the last unified commander of Pakistan Armed Forces's Eastern Military High Command, signed the Instrument of Surrender at about 4:31 pm . Over 93,000 personnel, including Lt. General Niazi and Admiral Shariff, were taken as Prisoner of War.
On 16 December 1971, East Pakistan was liberated from Pakistan as the newly independent state of Bangladesh. The Eastern Military High Command, civilian institutions and paramilitary forces were disbanded. Bangladesh quickly gained recognition from most countries after the signing of the Shimla Agreement between India and Pakistan. Bangladesh joined the United Nations in 1974.
Military.
Since its unification with Pakistan, the East Pakistan Army had consisted of only one infantry brigade, which was made up of two battalions, the 1st East Bengal Regiment and the 1/14 or 3/8 Punjab Regiment in 1948. These two battalions boasted only five rifle companies between them (an infantry battalion normally had 5 companies). This weak brigade, under the command of Brigadier-General Ayub Khan (local rank Major-General – GOC of 14th Army Division), together with the East Pakistan Rifles which was tasked with defending East Pakistan during the Kashmir War of 1947. The PAF, Marines, and the Navy had little presence in the region. Only one PAF combatant squadron, No. 14 Squadron "Tail Choppers", was active in East Pakistan. This combatant squadron was commanded by an air force Major PQ Mehdi (later four-star general). The East Pakistan military personnel were trained in combat diving, demolitions, and guerrilla/anti-guerrilla tactics by the advisers from the Special Service Group (Navy) who were also charged with intelligence data collection and management cycle.
The East Pakistan Navy had only one active-duty combatant destroyer, the PNS "Sylhet"; one submarine "Ghazi" (which was repeatedly deployed in West); four gunboats, inadequate to function in deep water. The joint special operations were managed and undertaken by the Naval Special Service Group (SSG(N)) who were assisted by the army, air force and marines unit. The entire service, the Marines were deployed in East Pakistan, initially tasked with conducting exercises and combat operations in riverine areas and at near shoreline. The small directorate of Naval Intelligence (while the headquarters and personnel, facilities, and directions were coordinated by West) had vital role in directing special and reconnaissance missions, and intelligence gathering, also was charged with taking reasonable actions to slow down the Indian threat. The armed forces of East Pakistan also consisted the paramilitary organisation, the "Volunteers" from the intelligence unit of the "ISI's" Covert Action Division (CAD). All of these armed forces were commanded by the unified command structure, the Eastern Military High Command, led by an officer of three-star rank equivalent.
Provincial Symbols.
The main four provincial icons were the Oriental Magpie-Robin, Royal Bengal Tiger, Banyan tree and the Water Lily, some of these were nationalized by Bangladesh in 1972.
Memorials and Legacy.
The trauma was extremely severe in Pakistan when the news of secession of East Pakistan as Bangladesh arrived — a psychological setback, complete and humiliating defeat that shattered the prestige of Pakistan Armed Forces. The governor and martial law administrator Lieutenant-General Amir Abdullah Khan Niazi was defamed, his image was maligned and he was stripped of his honors. The people of Pakistan could not come to terms with the magnitude of defeat, and spontaneous demonstrations and mass protests erupted on the streets of major cities in (West) Pakistan. General Yahya Khan surrendered powers to Nurul Amin of Pakistan Muslim League, the first and last Vice-President and Prime minister of Pakistan.
Prime minister Amin invited Zulfikar Ali Bhutto (sworn as President, later Prime minister) and the Pakistan Peoples Party to take control of Pakistan, and in a color ceremony where Bhutto addressed his daring speech to his nation via national television. At this ceremony, Zulfikar Ali Bhutto waved his fist in the air and pledged to his nation to never again allow the surrender of his country like it happened with East Pakistan; therefore, he launched and orchestrated the large-scale atomic bomb project in 1972. In memorial of East Pakistan, the East-Pakistan diaspora in Pakistan established the East-Pakistan colony in Karachi, Sindh. In accordance, the East-Pakistani diaspora also composed patriotic tributes to Pakistan after the war; songs such as "Sohni Dharti" (lit. Beautiful land) and ""Jeevay, Jeevay Pakistan" (lit. long-live, long-live Pakistan), were composed by Bengali singer Shahnaz Rahmatullah in the 1970s and 1980s.
To Western observers, the loss of East Pakistan was a blessing— but it was a trauma that was not seen as such; even today it is still not seen that way. In a book, "Scoop! Inside Stories from the Partition to the Present", written by Pakistan-born Indian politician Kuldip Nayar, it is noted that "Losing East Pakistan and Bhutto's releasing of Mujib did not mean anything to Pakistan's policy - as if there was no liberation war. Bhutto's policy, and even today, the policy of Pakistan continues to state that "she will continue to fight for the honor and integrity of Pakistan. East Pakistan is an inseparable and inseverable part of Pakistan".

</doc>
<doc id="10313" url="http://en.wikipedia.org/wiki?curid=10313" title="E. O. Wilson">
E. O. Wilson

Edward Osborne "E. O." Wilson FMLS (born June 10, 1929) is an American biologist, researcher (sociobiology, biodiversity, island biogeography), theorist (consilience, biophilia), naturalist (conservationist) and author. His biological specialty is myrmecology, the study of ants, on which he is considered to be the world's leading expert.
Wilson is known for his scientific career, his role as "the father of sociobiology" and "the father of biodiversity", his environmental advocacy, and his secular-humanist and deist ideas pertaining to religious and ethical matters. Among his greatest contributions to ecological theory is the theory of island biogeography, which he developed in collaboration with the mathematical ecologist Robert MacArthur, and which is seen as the foundation of the development of conservation area design, as well as the unified neutral theory of biodiversity of Stephen Hubbell.
Wilson is (2014) the Pellegrino University Research Professor, Emeritus in Entomology for the Department of Organismic and Evolutionary Biology at Harvard University, a lecturer at Duke University, and a Fellow of the Committee for Skeptical Inquiry. He is a Humanist Laureate of the International Academy of Humanism. He is a two-time winner of the Pulitzer Prize for General Non-Fiction and a New York Times bestseller for "The Social Conquest of Earth", "Letters to a Young Scientist", and "The Meaning of Human Existence".
Early life.
Wilson was born in Birmingham, Alabama. According to his autobiography "Naturalist", he grew up mostly around Washington, D.C. and in the countryside around Mobile, Alabama. From an early age, he was interested in natural history. His parents, Edward and Inez Wilson, divorced when he was seven. The young naturalist grew up in several cities and towns, moving around with his father and his stepmother.
In the same year that his parents divorced, Wilson blinded himself in one eye in a fishing accident. He suffered for hours, but he continued fishing. He did not complain because he was anxious to stay outdoors. He never went in for medical treatment. Several months later, his right pupil clouded over with a cataract. He was admitted to Pensacola Hospital to have the lens removed. Wilson writes, in his autobiography, that the "surgery was a terrifying [19th] century ordeal". Wilson was left with full sight in his left eye, with a vision of 20/10. The 20/10 vision prompted him to focus on "little things": "I noticed butterflies and ants more than other kids did, and took an interest in them automatically."
Although he had lost his stereoscopy, he could see fine print and the hairs on the bodies of small insects. His reduced ability to observe mammals and birds led him to concentrate on insects.
At nine, Wilson undertook his first expeditions at the Rock Creek Park in Washington, DC. He began to collect insects and he gained a passion for butterflies. He would capture them using nets made with brooms, coat hangers, and cheesecloth bags. Going on these expeditions led to Wilson's fascination with ants. He describes in his autobiography how one day he pulled the bark of a rotting tree away and discovered citronella ants underneath. The worker ants he found were "short, fat, brilliant yellow, and emitted a strong lemony odor". Wilson said the event left a "vivid and lasting impression on [him]". He also earned the Eagle Scout award and served as Nature Director of his Boy Scout summer camp. At the age of 18, intent on becoming an entomologist, he began by collecting flies, but the shortage of insect pins caused by World War II caused him to switch to ants, which could be stored in vials. With the encouragement of Marion R. Smith, a myrmecologist from the National Museum of Natural History in Washington, Wilson began a survey of all the ants of Alabama. This study led him to report the first colony of fire ants in the US, near the port of Mobile.
Concerned that he might not be able to afford to go to a university, Wilson attempted to enlist in the United States Army. His plan was to earn U.S. government financial support for his education, but he failed his Army medical examination due to his impaired eyesight. Wilson was able to afford to enroll in the University of Alabama after all. There, he earned his B.S. and M.S. degrees in biology. He later earned his Ph.D. degree in biology from Harvard University.
Retirement.
In 1996, Wilson officially retired from teaching at Harvard University, where he continues to hold the positions of Professor Emeritus and Honorary Curator in Entomology. He and his wife Irene now reside in Lexington, Massachusetts. His daughter, Catherine, and her husband Jonathan, reside in nearby Stow, Massachusetts.
In December 2013, it was announced that, starting in 2014, Wilson's foundation, the E.O. Wilson Biodiversity Foundation, would be based as an independent foundation at the Nicholas School of the Environment at Duke University. As part of the agreement, Wilson would become a special lecturer at Duke University.
Theories and beliefs.
Epic of evolution.
"The evolutionary epic", Wilson wrote in his book "On Human Nature", "is probably the best myth we will ever have." Wilson's intended usage of the word "myth" does not denote falsehood—rather, a grand narrative that provides people with placement in time—a meaningful placement that celebrates extraordinary moments of shared heritage. Wilson was not the first to use the term, but his fame prompted its usage as the morphed phrase epic of evolution.
Wilson explained the need for the epic of evolution:
Human beings must have an epic, a sublime account of how the world was created and how humanity became part of it... Religious epics satisfy another primal need. They confirm we are part of something greater than ourselves... The way to achieve our epic that unites human spirituality, instead of cleave it, is to compose it from the best empirical knowledge that science and history can provide.
The worth of the epic, he said, is that "[t]he true evolutionary epic retold as poetry, is as intrinsically ennobling as any religious epic."
Cosmologist Brian Swimme concludes in a 1997 interview:
I think that what E. O. Wilson is trying to suggest is that to be fully human, a person has to see that life has a heroic dimension... I think for the scientist, and for other people, it's a question of, "Is the universe valuable? Is it sacred? Is it holy? Or is the human agenda all that matters?" I just don't think we're that stupid to continue in a way that continues to destroy. I'm hopeful that the Epic of Evolution will be yet another strategy in our culture that will lead our consciousness out of a very tight, human-centered materialism.
Naturalistic and liberal religious writers have picked up on Wilson's term and have used it in a number of texts. These authors however have at times used other terms to refer to the idea: "Universe Story" (Brian Swimme, John F. Haught), "Great Story" (Connie Barlow, Michael Dowd), "Everybody's Story" (Loyal Rue), "New Story" (Thomas Berry, Al Gore, Brian Swimme) and "Cosmic Evolution" (Eric Chaisson).
Sociobiology.
Michael McGoodwin paraphrasing and quoting Wilson (pp. 16 and 222) on sociobiology:
Sociobiology is defined as the scientific or systematic study of the biological basis of all forms of social behavior, in all kinds of organisms including man, and incorporating knowledge from ethology, ecology, and genetics, in order to derive general principles concerning the biological properties of entire societies. "If humankind evolved by Darwinian natural selection, [then] genetic chance and environmental necessity, not God, made the species." "The brain [and the mind] exists because it promotes the survival and multiplication of the genes that direct its assembly." The two apparent dilemmas we face therefore are: (1) We lack any goal external to our biological nature (for even religions evolve to enhance the persistence and influence of their practitioners). Will the transcendental goals of societies dissolve, and will our post-ideological societies regress steadily toward self-indulgence? (2) Morality evolved as instinct. "Which of the censors and motivators should be obeyed and which ones might better be curtailed or sublimated?"
Although much human diversity in behavior is culturally influenced, some has been shown to be genetic - rapid acquisition of language, human unpredictability, hypertrophy (extreme growth of pre-existing social structures), altruism and religions. "Religious practices that consistently enhance survival and procreation of the practitioners will propagate the physiological controls that favor the acquisition of the practices during single lifetimes." Unthinking submission to the communal will promotes the fitness of the members of the tribe. Even submission to secular religions and cults involve willing subordination of the individual to the group. Religious practices confer biological advantages.
Wilson used sociobiology and evolutionary principles to explain the behavior of the social insects and then to understand the social behavior of other animals, including humans, thus established sociobiology as a new scientific field. He argued that all animal behavior, including that of humans, is the product of heredity, environmental stimuli, and past experiences, and that free will is an illusion. He has referred to the biological basis of behaviour as the "genetic leash." The sociobiological view is that all animal social behavior is governed by epigenetic rules worked out by the laws of evolution. This theory and research proved to be seminal, controversial, and influential.
The controversy of sociobiological research lies in how it applies to humans. The theory established a scientific argument for rejecting the common doctrine of tabula rasa, which holds that human beings are born without any innate mental content and that culture functions to increase human knowledge and aid in survival and success. In the final chapter of the book "Sociobiology" and in the full text of his Pulitzer Prize-winning "On Human Nature", Wilson argues that the human mind is shaped as much by genetic inheritance as it is by culture (if not more). There are limits on just how much influence social and environmental factors can have in altering human behavior.
Ants and social insects.
Wilson, along with Bert Hölldobler, carried out a systematic study of ants and ant behavior, culminating in the encyclopedic work "The Ants" (1990). Because much self-sacrificing behavior on the part of individual ants can be explained on the basis of their genetic interests in the survival of the sisters, with whom they share 75% of their genes (though the actual case is some species' queens mate with multiple males and therefore some workers in a colony would only be 25% related), Wilson was led to argue for a sociobiological explanation for all social behavior on the model of the behavior of the social insects. In his more recent work, he has sought to defend his views against the criticism of younger scientists such as Deborah Gordon, whose results challenge the idea that ant behavior is as rigidly predictable as Wilson's explanations make it.
Edward O. Wilson, referring to ants, once said that "Karl Marx was right, socialism works, it is just that he had the wrong species", meaning that while ants and other eusocial species appear to live in communist-like societies, they only do so because they are forced to do so from their basic biology, as they lack reproductive independence: worker ants, being sterile, need their ant-queen in order to survive as a colony and a species, and individual ants cannot reproduce without a queen and are thus forced to live in centralised societies. Humans, however, do possess reproductive independence so they can give birth to offspring without the need of a "queen", and in fact humans enjoy their maximum level of Darwinian fitness only when they look after themselves and their offspring, while finding innovative ways to use the societies they live in for their own benefit.
Consilience.
In his 1998 book "", Wilson discusses methods that have been used to unite the sciences, and might be able to unite the sciences with the humanities. Wilson prefers and uses the term "consilience" to describe the synthesis of knowledge from different specialized fields of human endeavor. He defines human nature as a collection of epigenetic rules, the genetic patterns of mental development. He argues that culture and rituals are products, not parts, of human nature. He says art is not part of human nature, but our appreciation of art is. He argues that concepts such as art appreciation, fear of snakes, or the incest taboo (Westermarck effect) can be studied by scientific methods of the natural sciences. Previously, these phenomena were only part of psychological, sociological, or anthropological studies. Wilson proposes that they can be part of interdisciplinary research.
The units and target of selection.
Wilson has argued that the ""unit" of selection is a gene, the basic element of heredity. The "target" of selection is normally the individual who carries an ensemble of genes of certain kinds." With regard to the use of kin selection in explaining the behavior of eusocial insects, Wilson said to "Discover" magazine, the "new view that I'm proposing is that it was group selection all along, an idea first roughly formulated by Darwin."
Spiritual and political beliefs.
Views on religion.
As paraphrased by Michael McGoodwin
The predisposition to religious belief is an ineradicable part of human behavior. Mankind has produced 100,000 religions. It is an illusion to think that scientific humanism and learning will dispel religious belief. Men would rather believe than know... A kind of Darwinistic survival of the fittest has occurred with religions... The ecological principle called Gause's law holds that competition is maximal between species with identical needs... Even submission to secular religions such as Communism, capitalism and guru cults involve willing subordination of the individual to the group. Religious practices confer biological advantage. The mechanisms of religion include (1) objectification (the reduction of reality to images and definitions that are easily understood and cannot be refuted), (2) commitment through faith (a kind of tribalism enacted through self-surrender), (3) and myth (the narratives that explain the tribe's favored position on the earth, often incorporating supernatural forces struggling for control, apocalypse, and millennium). The three great religion categories of today are Marxism, traditional religion, and scientific materialism... Though theology is not likely to survive as an independent intellectual discipline, religion will endure for a long time to come and will not be replaced by scientific materialism.
Scientific humanism.
Wilson coined the phrase "scientific humanism" as "the only worldview compatible with science's growing knowledge of the real world and the laws of nature". Wilson argues that it is best suited to improve the human condition. In 2003, he was one of the signers of the Humanist Manifesto.
God and religion.
On the question of God, Wilson has described his position as "provisional deism" and explicitly denied the label of "atheist", preferring "agnostic". He has explained his faith as a trajectory away from traditional beliefs: "I drifted away from the church, not definitively agnostic or atheistic, just Baptist & Christian no more." Wilson argues that the belief in God and rituals of religion are products of evolution. He argues that they should not be rejected or dismissed, but further investigated by science to better understand their significance to human nature. In his book "The Creation", Wilson suggests that scientists ought to "offer the hand of friendship" to religious leaders and build an alliance with them, stating that "Science and religion are two of the most potent forces on Earth and they should come together to save the creation."
Wilson makes a similar suggestion, an appeal to the religious community, on the lecture circuit. An article on his September 17, 2009 lecture at Midland College, Texas, reports, "he said the appeal received a 'massive reply' and a covenant has been written. 'I think that partnership will work to a substantial degree as time goes on,' Wilson said."
Wilson appears in the documentary "Behold the Earth", which inquires into America's "divorce from nature" and the relationship between science and religion.
Ecology.
When discussing the reinvigoration of his original fields of study since the 1960s, Wilson has said that if he could start his life over he would work in microbial ecology. He studied the mass extinctions of the 20th century and their relationship to modern society, arguing strongly for an ecological approach:
Now when you cut a forest, an ancient forest in particular, you are not just removing a lot of big trees and a few birds fluttering around in the canopy. You are drastically imperiling a vast array of species within a few square miles of you. The number of these species may go to tens of thousands. ... Many of them are still unknown to science, and science has not yet discovered the key role undoubtedly played in the maintenance of that ecosystem, as in the case of fungi, microorganisms, and many of the insects.
His understanding of the scale of the extinction crisis has led him to advocate a number of strategies for forest protection, including the Forests Now Declaration, which calls for new markets-based mechanisms to protect tropical forests. In 2014, Wilson called for the global set aside of 50% of the earth's surface for other species to thrive in as the only possible strategy for solving the extinction crisis 
Reception of human sociobiology.
Wilson experienced significant criticism for his sociobiological views from several different communities. The scientific response included several of Wilson's colleagues at Harvard, such as Richard Lewontin and Stephen Jay Gould, who were strongly opposed to his ideas regarding sociobiology. Marshall Sahlins's work "The Use and Abuse of Biology" was a direct criticism of Wilson's theories.
Politically, Wilson's sociobiological ideas have been opposed by some. Sociobiology re-ignited the nature and nurture debate, and Wilson's scientific perspective on human nature led to public debate. He was accused of racism, misogyny, and eugenics. In one incident, his lecture was attacked by the International Committee Against Racism, a front group of the Marxist Progressive Labor Party, where one member poured a pitcher of water on Wilson's head and chanted "Wilson, you're all wet" at an AAAS conference in November 1978. Wilson later spoke of the incident as a source of pride: "I believe...I was the only scientist in modern times to be physically attacked for an idea."
"I believe Gould was a charlatan", Wilson told "The Atlantic". "I believe that he was ... seeking reputation and credibility as a scientist and writer, and he did it consistently by distorting what other scientists were saying and devising arguments based upon that distortion."
Religious objections included those of Paul E. Rothrock, who said: "... sociobiology has the potential of becoming a religion of scientific materialism."
Philosopher Mary Midgley encountered "Sociobiology" in the process of writing "Beast and Man" and significantly rewrote the book to offer a critique of Wilson's views. While Midgley praises the book for its recognition of the study of animal behavior, clarity, scholarship, and encyclopedic scope, she extensively critiques Wilson for conceptual confusion, scientism, and anthropomorphism of genetics.
Awards and honors.
Wilson's scientific and conservation honors include:

</doc>
<doc id="10315" url="http://en.wikipedia.org/wiki?curid=10315" title="Edwin Howard Armstrong">
Edwin Howard Armstrong

Edwin Howard Armstrong (December 18, 1890 – January 31, 1954) was an American electrical engineer and inventor. He has been called "the most prolific and influential inventor in radio history". He invented the regenerative circuit while he was an undergraduate and patented it in 1914, followed by the super-regenerative circuit in 1922, and the superheterodyne receiver in 1918. Armstrong was also the inventor of modern frequency modulation (FM) radio transmission. 
Armstrong was born in New York City, New York, in 1890. He studied at Columbia University where he was a member of the Epsilon Chapter of the Theta Xi Fraternity. He later became a professor at Columbia University. He held 42 patents and received numerous awards, including the first Institute of Radio Engineers now IEEE Medal of Honor, the French Legion of Honor, the 1941 Franklin Medal and the 1942 Edison Medal. He is a member of the National Inventors Hall of Fame and the International Telecommunications Union's roster of great inventors.
Early life.
Armstrong was born in the Chelsea district of New York City to John and Emily Armstrong. His father was the American representative of the Oxford University Press, which published Bibles and standard classical works. John Armstrong, who was also a native of New York, began working at the Oxford University Press at a young age and eventually reached the position of vice president of the American branch. Emily Smith first met John Armstrong in the North Presbyterian Church, which was located at 31st Street and Ninth Avenue. Emily Smith had strong family ties to Chelsea, which centered around the church, in which her family took an active role.
When the church moved further north, the Smith and Armstrong families followed it. In 1895 the Armstrong family moved from their brownstone row house at 347 West 29th Street to another similar house at 26 West 97th Street in the Upper West Side. At the age of eight Armstrong contracted a disease that was known as St. Vitus' Dance, which left him with a lifelong tic when excited or under stress. Because of the illness Armstrong was withdrawn from school for two years. In order to improve his health the Armstrong family moved in 1902 from the Upper West Side into a house at 1032 Warburton Avenue in Yonkers, which overlooked the Hudson River. The Smith family moved into a house next door.
Armstrong's physical tic and the years he was removed from school led him to become withdrawn. Armstrong showed an interest in electrical and mechanical devices, particularly trains, from an early age.
He loved heights and constructed a makeshift radio antenna tower in his back yard. Swinging on a bosun's chair, he would hoist himself up and down the tower to the concern of his neighbors.
In late 1917, Armstrong was invited to join the U.S. Army Signal Corps with the rank of captain and was sent to Paris to help set up a wireless communication system for the Army. He returned to the United States in the fall of 1919.
During his service in both world wars, Armstrong gave the U.S. military free use of his patents. Use of these was critical to the Allied victories.
Unlike many engineers, Armstrong was never a corporate employee. He performed research and development by himself and owned his patents outright. He did not subscribe to conventional wisdom and was quick to question the opinions of his professors and his peers.
Early work.
As an undergraduate, and later as a professor at Columbia University, Armstrong worked from his parent's attic in Yonkers, New York, to develop the regenerative circuit, the superheterodyne receiver, and the superregenerative circuit. He studied under Professor Mihajlo Pupin at the Hartley Laboratories, a separate research unit at Columbia University. Thirty-one years after graduating from Columbia he became Professor of Electrical Engineering, filling the vacancy left by the death of Professor J. H. Morecroft. He held the position until his death.
Armstrong contributed the most to modern electronics technology. His discoveries revolutionized electronic communications. Regeneration, or amplification via positive feedback is still in use to this day. Also, Armstrong discovered that Lee De Forest's Audion would go into oscillation when feedback was increased. Thus, the Audion could not only detect and amplify radio signals, it could transmit them as well.
While De Forest's addition of a third element to the Audion (the grid) and the subsequent move to modulated (voice) radio is not disputed, De Forest did not put his device to work. Armstrong's research and experimentation with the Audion moved radio reception beyond the crystal set and spark-gap transmitters. Radio signals could be amplified via regeneration to the point of human hearing without a headset. Armstrong later published a paper detailing how the Audion worked, something De Forest could not do. De Forest did not understand the workings of his Audion.
Armstrong's service as a signal officer in World War I led to his design of the superheterodyne circuit. The discovery and development of the technology made radio receivers, then the primary communications devices of the time, more sensitive and selective. Before heterodyning, radio signals often overrode and interfered with each other. Heterodyning also made radio receivers much easier to use, rendering obsolete the multitude of tuning controls on radio sets of the time. The superheterodyne technology is still used today. There was a dispute regarding who invented superheterodyne radio. Walter Schottky claimed that he had independently invented super heterodyne radio.
FM radio.
Even as the regenerative-circuit lawsuit continued, Armstrong was working on another momentous invention. Working in the basement laboratory of Columbia's Philosophy Hall, he invented wide-band frequency modulation (FM) radio. Rather than varying ("modulating") the amplitude of a radio wave to encode an audio signal, the new method varied the frequency. FM enabled the transmission and reception of a wider range of audio frequencies, as well as audio free of "static", a common problem in AM radio. (Armstrong received a patent on wide-band FM on December 26, 1933.)
In 1922, John Renshaw Carson of AT&T, inventor of Single-sideband modulation (SSB modulation), had published a paper in the "Proceedings of the IRE" arguing that FM did not appear to offer any particular advantage. Armstrong managed to demonstrate the advantages of FM radio despite Carson's skepticism in a now-famous paper on FM in the "Proceedings of the IRE" in 1936, which was reprinted in the August 1984 issue of "Proceedings of the IEEE".
Today the consensus regarding FM is that narrow band FM is not so advantageous in terms of noise reduction, but wide band FM can bring great improvement in signal to noise ratio if the signal is stronger than a certain threshold. Hence Carson was not entirely wrong, and the Carson bandwidth rule for FM is still important today. Thus, both Carson and Armstrong ultimately contributed significantly to the science and technology of radio. The threshold concept was discussed by Murray G. Crosby (inventor of Crosby system for FM Stereo) who pointed out that for wide band FM to provide better signal to noise ratio, the signal should be above a certain threshold, according to his paper published in "Proceedings of the IRE" in 1937. Thus Crosby's work supplemented Armstrong's paper in 1936.
In 1934 Armstrong began working for RCA at the request of the company's president, David Sarnoff. Sarnoff and Armstrong first met on Christmas Eve, 1913, when Sarnoff, working as Chief Inspector for the Marconi Wireless Telegraph Company of America, witnessed a demonstration of Armstrong's regenerative receiver along with three Marconi engineers. By 1920, Sarnoff was a young executive with an interest in new technologies, including radio broadcasting. In the early 1920s Armstrong drove off with Sarnoff's secretary, Marion MacInnes, in a French sports car. Armstrong and MacInnes were married in 1923. While Sarnoff was understandably impressed with Armstrong's FM system, he also understood that it was not compatible with his own AM empire. Sarnoff came to regard FM as a threat and refused to support it any further.
From May 1934 until October 1935, Armstrong conducted the first large scale field tests of his FM radio technology from a laboratory constructed by RCA on the 85th floor of the Empire State Building. An antenna attached to the spire of the building fired radio waves at receivers about 80 miles away. However RCA had its eye on television broadcasting, and chose not to buy the patents for the FM technology. A June 17, 1936, presentation at the Federal Communications Commission (FCC) headquarters made headlines nationwide. He played a jazz record over conventional AM radio, then switched to an FM broadcast. "[I]f the audience of 50 engineers had shut their eyes they would have believed the jazz band was in the same room. There were no extraneous sounds," noted one reporter. He added that several engineers described the invention "as one of the most important radio developments since the first earphone crystal sets were introduced."
In 1937, Armstrong financed construction of the first FM radio station, W2XMN, a 40 kilowatt broadcaster in Alpine, New Jersey. The signal (at 42.8 MHz) could be heard clearly 100 mi away, despite the use of less power than an AM radio station.
RCA began to lobby for a change in the law or FCC regulations that would ultimately prevent FM from challenging AM's dominance. By June 1945, the RCA had pushed the FCC hard on the allocation of electromagnetic frequencies for the fledgling television industry. Although they denied wrongdoing, David Sarnoff and RCA managed to get the FCC to move the FM radio spectrum from 42–50 MHz to 88–108 MHz, while getting new low-powered community television stations allocated to a new Channel 1 in the 44-50 MHz range. In fairness to the FCC, the 42–50 MHz band was plagued by frequent tropospheric and Sporadic E propagation which caused distant high powered stations to interfere with each other. The problem becomes even more severe on a cyclical basis when sunspot levels reach a maximum every 11 years and lower VHF band signals below 50 MHz can travel across the Atlantic Ocean or from coast to coast within North America on occasion. Sunspot levels were near their cyclical peak when the FCC reallocated FM in 1945. The 88–108 MHz range is a technically better location for FM broadcast because it is less susceptible to this kind of frequent interference. (Channel 1 eventually had to be deleted as well, with all TV broadcasts licensed at frequencies 54 MHz or higher, and the band is no longer widely used for emergency first responders either, those services having moved mostly to UHF.)
But the immediate economic impact of the shift, whatever its technical merit, was devastating to early FM broadcasters. This single FCC action rendered all existing FM receivers and transmitters obsolete as stations were moved to the new band, while it also protected both RCA's AM-radio stronghold and that of the other major competing networks including CBS, ABC and Mutual. Armstrong's radio network did not survive the shift into the higher frequencies and was set back by the FCC decision. This change was strongly supported by AT&T, because of loss of FM relaying stations forced radio stations to buy wired links from AT&T.
Furthermore, RCA also claimed invention of FM radio and won its own patent on the technology. A patent fight between RCA and Armstrong ensued. RCA's momentous victory in the courts left Armstrong unable to claim royalties on any FM receivers, including televisions, which were sold in the United States. The undermining of the Yankee Network and his costly legal battles brought ruin to Armstrong, by then almost penniless and emotionally distraught. Eventually, after Armstrong's death, many of the lawsuits were decided or settled in his favor, greatly enriching his estate and heirs. But the decisions came too late for Armstrong himself to enjoy his legal vindication.
Personal life.
Armstrong married Sarnoff's secretary, Marion MacInnis, in December 1922. He gave Marion the world's first portable radio as a wedding gift. Armstrong bought a Hispano-Suiza motor car before the wedding, which they drove to Palm Beach, Florida for their honeymoon. He kept the car until his death. MacInnis, who was born in 1898, was survived by two nephews and a niece after her death in 1979.
He was an avid tennis player until an injury in 1940, and drank an Old Fashioned with dinner.
Suicide.
Financially broken and mentally beaten after years of legal tussles with RCA and others, Armstrong lashed out at his wife one day with a fireplace poker, striking her on the arm. MacInnis left their apartment to stay with her sister, Marjorie Tuttle, in Granby, Connecticut.
On January 31, 1954, Armstrong removed the air conditioner from the window and jumped to his death from the thirteenth floor of his New York City apartment. His body was found fully clothed, with a hat, overcoat and gloves, the next morning by a River House employee on a third-floor balcony. The New York Times described the contents of his two-page suicide note to his wife: "he was heartbroken at being unable to see her once again, and expressing deep regret at having hurt her, the dearest thing in his life." The note concluded, "God keep you and Lord have mercy on my Soul." After his death, a friend of Armstrong estimated that 90 percent of his time was spent on litigation against RCA. Upon hearing the news, David Sarnoff supposedly remarked, "I did not kill Armstrong."
MacInnis was able to formally establish Armstrong as the inventor of FM following protracted court proceedings over five of his basic FM patents. Until her death in 1979 she participated in the Armstrong Memorial Research Foundation that she founded.
Edwin Armstrong was buried in Locust Grove Cemetery, Merrimac, Massachusetts.
Legacy.
Armstrong invented a large part of the technology of modern radio. A modern biographer has written
Why, then, have so many people never heard of Armstrong? The answer is ironic: Armstrong was all substance and no style. He could not play public relations games and was naive enough to underestimate the power of those whose interests were threatened by his inventions. At the same time, he refused to compromise. In the end, he fell victim to the very stubbornness that made possible his spectacular technical successes.
It took decades following Armstrong's death for FM broadcasting to meet and surpass the saturation of the AM band, and longer still for FM radio to become profitable for broadcasters. Two developments made a difference in the 1960s. One was the development of true stereophonic broadcasting on FM by General Electric, which resulted in the approval of an FM stereo broadcast standard by the FCC in 1961, and the conversion of hundreds of stations to stereo within a few years.
The other was an FCC rulemaking in 1966 that required broadcasters who owned both full-time AM stations and FM properties in the same city to program each of them with separate programming during a majority of the day. This meant FM no longer just simulcast AM with better sound quality, but offered unique program choices expanding what listeners could hear. Programmers took advantage by turning their FM stations into venues for formats from country to progressive rock to jazz and classical music, all with the enhanced quality that stereo sound could bring. For example, some AM stations paused Sunday morning music programming for religious voicetracks by ministers. Sister stations on FM continued the music programming.
Within a few years a majority of households were FM equipped, by the 1980s a majority of cars sold had FM stereo radios and a majority of listening in the U.S. was devoted to FM signals according to the Arbitron rating service. The stereo sound revolution, followed by the programming revolution, accomplished what cleaner and crisper sound alone was unable to achieve, and made FM radio a permanent and important part of the communications landscape.
Armstrong was of the opinion that anyone who had actual contact with the development of radio understood that the radio art was the product of experiment and work based on physical reasoning, rather than on the mathematicians' calculations and formulae (known today as part of "mathematical physics"). His work, as important as it was in its own right, was a part of a continuum of progress in communications and electronics that since his time has brought forward color television, the personal computer, the Internet, cable and satellite radio and TV, personal mobile phones, audio, video and computing, digital stereo radio on both the medium wave and VHF-FM bands, and digital high definition television on VHF, UHF, cable and satellite. Armstrong's FM system was used for communications between NASA and the Apollo program astronauts. He is of no known relation to the well-known Apollo astronaut Neil Armstrong.
After her husband's death, Armstrong’s estate finally won the case against RCA. Dana Raymond of Cravath, Swaine & Moore in New York served as counsel in that litigation. Marion Armstrong became extraordinarily wealthy as a result of FM’s commercial success and acceptance worldwide.
In 1984 Robert Mondlock copyrighted an original screenplay about Armstrong's life titled "High Fidelity".
Honors.
In 1917 Armstrong was the first recipient of the IRE's, now IEEE Medal of Honor. For his wartime work on radio the French government gave him the Legion of Honor in 1919. He was awarded the 1941 Franklin Medal. He received in 1942 the AIEEs Edison Medal "for distinguished contributions to the art of electric communication, notably the regenerative circuit, the superheterodyne, and frequency modulation". The ITU added him to its roster of great inventors of electricity in 1955. In 1980 he was inducted into the National Inventors Hall of Fame, and was on a U.S. postage stamp in 1983. The Consumer Electronics Hall of Fame inducted him in 2000, "in recognition of his contributions and pioneering spirit that have laid the foundation for consumer electronics."
Philosophy Hall, the Columbia building where Armstrong developed FM, was declared a National Historic Landmark in 2003 in recognition of that fact. Armstrong's home in Yonkers also received designation in both the NHL and the National Register of Historic Places, but both were withdrawn when the house was later demolished.
Armstrong Hall at Columbia is also named in his honor. The building, at the northeast corner of Broadway and 112th Street, was originally an apartment house but was converted to research space after Columbia bought it. It is now home to the Goddard Institute for Space Studies, a research institute jointly operated by Columbia and the National Aeronautics and Space Administration dedicated to atmospheric and climate science. A storefront in the corner of the building houses Tom's Restaurant, a longtime neighborhood fixture that was featured as the fictional diner "Monk's" for establishing shots in the television series "Seinfeld". The same restaurant also inspired Susanne Vega's song "Tom's Diner".
In addition, Columbia established the Edwin Howard Armstrong Professorship in Computer Science in Armstrong's memory.
Also, the United States Army Communications and Electronics Life Cycle Management Command (CECOM-LCMC) Headquarters at Aberdeen Proving Ground, Maryland is named Armstrong Hall in his honor.
Patents.
Armstrong received 42 patents in total; a selection are listed below: 
Patent disputes.
Many of Armstrong's inventions were ultimately claimed by others in patent lawsuits. In particular, the regenerative circuit, which Armstrong patented in 1914 as a "wireless receiving system," was subsequently patented by Lee De Forest in 1916; De Forest then sold the rights to his patent to AT&T. Between 1922 and 1934, Armstrong found himself embroiled in a patent war, between himself, RCA, and Westinghouse on one side, and De Forest and AT&T on the other. At the time, this action was the longest patent lawsuit ever litigated, at 12 years. Armstrong won the first round of the lawsuit, lost the second, and stalemated in a third. Before the Supreme Court of the United States, De Forest was granted the regeneration patent in what is today widely regarded as a misunderstanding of the technical facts by the Supreme Court justices.
By early 1923, however, Armstrong was a millionaire as a result of licensing his patents to RCA. In 1946 the FCC's decision to use Armstrong's FM system as the standard for NTSC television sound gave Armstrong another chance at royalty payments. However, RCA refused to pay him royalties and encouraged other television makers not to pay them either.

</doc>
<doc id="10322" url="http://en.wikipedia.org/wiki?curid=10322" title="EverQuest">
EverQuest

EverQuest is a 3D fantasy-themed massively multiplayer online role-playing game (MMORPG) released on March 16, 1999.
EverQuest was the second major MMORPG to be released, after Ultima Online, and the first to employ 3D graphics. As such it has exerted enormous influence on all later games of the genre. It has earned numerous awards, including 1999 GameSpot Game of the Year and a 2007 Technology & Engineering Emmy Award.
After 16 years and 21 expansions, EverQuest is also one of the oldest MMORPGs still running.
History.
Development.
"EverQuest" began as a concept by John Smedley in 1996. The original design is credited to Brad McQuaid, Steve Clover, and Bill Trost. It was developed by Sony's 989 Studios and its early-1999 spin-off Verant Interactive, and published by Sony Online Entertainment (SOE).
Since its acquisition of Verant in late 1999, EverQuest was developed by Sony Online Entertainment.<ref name="Sony/Verant"></ref>
The design and concept of "EverQuest" is heavily indebted to text-based MUDs, in particular DikuMUD, and as such "EverQuest" is considered a 3D evolution of the text MUD genre like some of the MMOs that preceded it, such as "Meridian 59" and "The Realm Online". John Smedley, Brad McQuaid, Steve Clover and Bill Trost, who jointly are credited with creating the world of "EverQuest", have repeatedly pointed to their shared experiences playing MUDs such as "Sojourn" and "TorilMUD" as the inspiration for the game. Famed book cover illustrator Keith Parkinson created the box covers for earlier installments of EverQuest.
Development of "EverQuest" began in 1996 when Sony Interactive Studios America (SISA) executive John Smedley secured funding for a 3D game like text-based MUDs following the successful launch of "Meridian 59" the previous year. To implement the design, Smedley hired programmers Brad McQuaid and Steve Clover, who had come to Smedley's attention through their work on the single player RPG "Warwizard". McQuaid soon rose through the ranks to become executive producer for the "EverQuest" franchise and emerged during development of "EverQuest" as a popular figure among the fan community through his in-game avatar, Aradune. Other key members of the development team included Bill Trost, who created the history, lore and major characters of Norrath (including "EverQuest" protagonist Firiona Vie), Geoffrey "GZ" Zatkin, who implemented the spell system, and artist Milo D. Cooper, who did the original character modeling in the game.
Release.
"EverQuest" launched with modest expectations from Sony on 16 March 1999 under its Verant Interactive brand and quickly became successful. By the end of the year, it had surpassed competitor "Ultima Online" in number of subscriptions. Numbers continued rising rapidly until mid-2001 when growth slowed. Sony's last reported subscription numbers were given as more than 430,000 players on 14 January 2004.
"EverQuest" initially launched with volunteer "Guides" who would act as basic customer service/support via 'petitions'. Issues could be forwarded to the Game Master assigned to the server or resolved by the volunteer. Other guides would serve in administrative functions within the program or assisting the Quest Troupe with dynamic and persistent live events throughout the individual servers. Volunteers were compensated with free subscription and expansions to the game. In 2003 the program changed for the volunteer guides taking them away from the customer service focus and placing them into their current roles as roving 'persistent characters' role-playing with the players.
In anticipation of PlayStation's launch, Sony Interactive Studios America made the decision to focus primarily on console titles under the banner 989 Studios, while spinning off its sole computer title, "EverQuest", which was ready to launch, to a new computer game division named Redeye (renamed Verant Interactive). Executives initially had very low expectations for "EverQuest", but in 2000, following the surprising continued success and unparalleled profits of "EverQuest", Sony reorganized Verant Interactive into Sony Online Entertainment (SOE) with Smedley retaining control of the company.
Many of the original "EverQuest" team, including Brad McQuaid, Steve Clover and Geoffrey Zatkin left SOE by 2002.
Subscription numbers.
Verant, from 1999 to 2001, and SOE, from 2001 to 14 January 2004, issued formal statements giving some indications of the number of "EverQuest" subscriptions and peak numbers of players online at any given moment. These records show more than 225,000 subscriptions on 1 November 1999, with an increase to more than 450,000 subscriptions by 25 September 2003.
Growth and sequels.
The first four expansions were released in traditional physical boxes at roughly one-year intervals. These were highly ambitious and offered huge new landmasses, new playable races and new classes. The (2001) gave a significant facelift to player character models, bringing the by-then dated 1999 graphics up to modern standards. However, non-player characters which do not correspond to any playable race-gender-class combination (such as vendors) were not updated, leading to the coexistence of 1999-era and 2001-era graphics in many locations. The (2002) introduced The Plane of Knowledge, a hub zone from which players could instantly teleport to many zones. This made the pre-existing roads and ships largely redundant, and long-distance overland travel is now virtually unheard of.
EverQuest made a push to enter the European market in 2002 with the "New Dawn" promotional campaign, which not only established local servers in Germany, France and Great Britain but also offered localized versions of the game in German and French to accommodate players who prefer those languages to English. In the following year the game also moved beyond the PC market with a Mac OS X version.
In 2003 experiments began with digital distribution of expansions, starting with the Legacy of Ykesha. From this point on expansions would be less ambitious in scope than the original four, but on the other hand the production rate increased to two expansions a year instead of one.
This year the franchise also ventured into the console market with EverQuest Online Adventures, released for Sony's internet-capable Playstation 2. It was the second MMORPG for this console, after Final Fantasy XI. Story-wise it was a prequel, with the events taking place 500 years before the original EverQuest. Other spin-off projects were the PC strategy game Lords of EverQuest (2003) and the co-op Champions of Norrath (2004) for the Playstation 2.
After these side projects, the first proper sequel was released in late 2004, titled simply "EverQuest II" . The game is set 500 years "after" the original, as opposed to EverQuest Online Adventures which took place 500 years "before". EverQuest II would face severe competition from Blizzard's World of Warcraft, which was released at virtually the same time and quickly grew to dominate the MMORPG genre.
Decline.
Since the release of World of Warcraft and other modern MMORPGs, there have been a number of signs that the EverQuest population is shrinking. The national "New Dawn" servers were discontinued in 2005 and merged into a general (English-language) European server.
The 2006 expansion The Serpent's Spine introduced the "adventure-friendly" city of Crescent Reach in which all races and classes are able -and encouraged to- start. Crescent Reach is supposed to provide a more pedagogic starting environment than the original 1999 cities, where players were given almost no guidance on what to do. The common starting city also concentrates the dwindling amount of new players in a single location, making grouping easier. Later on, (2008) introduced "mercenaries" - computer controlled companions that can join groups in place of human players; a response to the increasing difficulty of finding other players of appropriate level for group activities. As of "Seeds" the production rate also returned to one expansion a year instead of two.
In March 2012 EverQuest departed from the traditional monthly subscription business model by introducing three tiers of commitment: a completely free-to-play Bronze Level, a one-time fee Silver Level, and a subscription Gold Level. The same month saw the closure of EverQuest Online Adventures. Just a few months earlier EverQuest II had gone free-to-play and SOE flagship Star Wars Galaxies been closed.
In June of the same year SOE removed the ability to buy game subscription time with Station Cash without any warning to players. SOE apologized for this abrupt change in policy and reinstated the option for an additional week, after which it was removed permanently.
November 2013 saw the closure of the sole Mac OS server Al'Kabor.
In February 2015 Sony sold its online entertainment division to private equity group Columbus Nova, with Sony Online Entertainment subsequently renamed Daybreak Game Company. An initial period of uncertainty followed, with all projects such as expansions and sequels put on hold and staff laid off. The situation stabilized around the game's 16th anniversary celebrations, with the announcement of a new progression server (long in demand by players) and attempts to address unpopular game mechanics. Under the new label, expansions will be replaced with smaller updates called "campaigns", which will contain about half the amount of content of previous annual expansions.
EverQuest Next.
The third iteration in the series, with the working title "EverQuest Next", is currently under development. In-game screenshots, concept art and more information were revealed at the SOE Fan Faire in August 2010, . However, this early version of the game has since been scrapped.
Gameplay.
Many of the elements in "EverQuest" have been drawn from text-based MUD (Multi-User Dungeon) games, particularly DikuMUDs, which in turn were inspired by traditional role-playing games such as "Dungeons & Dragons". In "EverQuest", players create a character (also known as an avatar, or colloquially as a "char" or "toon") by selecting one of sixteen races in the game, which range from humans (basic Caucasian-looking human, dark-skinned Erudite, and barbarian), elves (high elves, wood elves, and dark elves), half-elves, dwarves, gnomes, halflings, trolls, and ogres, to cat-people (Vah Shir), lizard-people (Iksar), frog-people (Froglok), and dragon-people (Drakkin). At creation, players select each character's adventuring occupation (such as a wizard, ranger, or cleric — called a "class" — see below for particulars), a patron deity, and starting city. Customization of the character facial appearance is available at creation (hair, hair color, face style, facial hair, facial hair color, eye color, etc.).
Players move their character throughout the medieval fantasy world of Norrath, often fighting monsters and enemies for treasure and experience points, and optionally mastering trade skills. As they progress, players advance in level, gaining power, prestige, spells, and abilities through valorous deeds such as entering overrun castles and keeps, defeating worthy opponents found within, and looting their remains. Experience and prestigious equipment can also be obtained by completing quests given out by non-player characters found throughout the land.
"EverQuest" allows players to interact with other people through role-play, joining player guilds, and dueling other players (in restricted situations – "EverQuest" only allows player versus player (PVP) combat on the PvP-specific server, specified arena zones and through agreed upon dueling).
The game-world of "EverQuest" consists of over five hundred zones.
Multiple instances of the world exist on various servers. In the past, game server populations were visible during log-in, and showed peaks of more than 3000 players per server. The design of "EverQuest", like other massively multiplayer online role-playing games, makes it highly amenable to cooperative play, with each player having a specific role within a given group.
Classes.
The fourteen classes of the original 1999 version of "EverQuest" were later expanded to include the Beastlord and Berserker classes with the ' (2001) and ' (2004) expansions, respectively.
The classes can be grouped into five general roles that share similar characteristics, as described below.
Tank classes.
Members of this group have a high number of hitpoints for their level, and can equip heavy armor. They have the ability to taunt enemies into focusing on them, either directly or through the use of highly aggravating spells and abilities, rather than other party members who may be more susceptible to damage and debilitation.
Damage dealers.
The following classes are able to deal high corporeal damage to opponents. Within the game, these classes are often referred to as 'DPS', which stands for Damage Per Second. There isn't a definitive top DPS class, as damage dealt will depend on numerous factors which vary from one encounter to another (such as the enemy's armor, its positioning, and its magic resistance). Another complication is that while Wizards can readily deal tremendous damage to enemies, their ability to do so is limited by their remaining mana pool, as well as how fast they are able to regenerate mana. That said, Berserkers, Rogues, and Wizards are three classes most commonly cited as the highest overall damage dealers.
These melee damage dealers have a medium number of hit points per level, but cannot wear the heaviest armors and are less likely than a tank class to be able to survive direct attacks for a sustained period of time.
Casters.
Caster classes have the lowest hit points per level and can only wear the lightest of armors. Casters draw their power from an internal pool of "mana", which takes some time to regenerate and thus demands judicious and efficient use of spells.
Crowd control / utility.
These classes share the ability to prevent enemies from attacking the party, as well as improving mana regeneration for themselves, teammates, and in the Enchanter's case, anyone they come across.
Healers.
Priest classes have medium level of hit points per level and have access to healing and "buff" spells.
Deities.
There are several deities in "EverQuest" who each have a certain area of responsibility and play a role in the backstory of the game setting. A wide array of armor and weapons are also deity-tied, making it possible for only those who worship that deity to wear/equip them. Additionally, deities determine, to some extent, where characters may and may not go without being attacked on sight.
Zones.
The "EverQuest" universe is divided into more than five hundred zones. These zones represent a wide variety of geographical features, including plains, oceans, cities, deserts, and other planes of existence. One of the most popular zones in the game is the Plane of Knowledge, one of the few zones in which all races and classes can coexist harmoniously without interference. The Plane of Knowledge is also home to portals to many other zones, including portals to other planes and to the outskirts of nearly every starting city.
Expansions.
There have been 21 expansions to the original game since release. Expansions are purchased separately and provide additional content to the game (for example: raising the maximum character level; adding new races, classes, zones, continents, quests, equipment, game features). Additionally, the game is updated through downloaded patches. The "EverQuest" expansions:
Servers.
The game runs on multiple game servers, each with a unique name for identification. These names were originally the deities of the world of Norrath. In technical terms, each game server is actually a cluster of server machines. Once a character is created, it can be played only on that server unless the character is transferred to a new server by the customer service staff, generally for a fee. Each server often has a unique community and people often include the server name when identifying their character outside of the game.
OS X.
SOE devoted one server (Al'Kabor) to an OS X version of the game. The game was never developed beyond the Planes of Power expansion. In January 2012, SOE announced plans to shut down the server, but based on the passionate response of the player base, rescinded the decision and changed Al'Kabor to a free-to-play subscription model. At about the same time, SOE revised the Macintosh client software to run natively on Intel processors. Players running on older, PowerPC-based systems lost access to the game at that point. Finally in November 2013, SOE closed Al'Kabor.
European.
Two SOE servers were set up to better support players in and around Europe: Antonius Bayle and Kane Bayle. Kane Bayle was merged into Antonius Bayle.
With the advent of the "New Dawn" promotion, three additional servers were set up and maintained by Ubisoft: Venril Sathir (British), Sebilis (French) and Kael Drakkal (German). The downside of the servers was that while it was possible to transfer to them, it was impossible to transfer off.
The servers were subsequently acquired by SOE and all three were merged into Antonius Bayle server.
Controversies, social issues, and game problems.
Sale of in-game objects/real world economics.
"EverQuest" has been the subject of various criticisms. One example involves the sale of in-game objects for real currency (often through eBay). The developers of "EQ" have always forbidden the practice.
Because items can be traded within the game and also because of illegal online trading on websites, virtual currency to real currency exchange rates have been calculated. The BBC reported that in 2002 work done by Edward Castronova showed that "EverQuest" was the 77th richest country in the world, sandwiched between Russia and Bulgaria and its GDP per capita was higher than that of the People's Republic of China and India. In 2004, a follow-up analysis of the entire online gaming industry indicated that the combined GDP of the online "worlds" populated by the two million players was approximately the same as that of Namibia.
Companies created characters, leveled them to make them powerful, and then resold the characters or specialized in exchanging money between games. A player could exchange a house in "The Sims Online" for "EverQuest" platinum pieces, depending solely on market laws of supply and demand.
Sony officially discourages the payment of real-world money for online goods, except on certain "Station Exchange" servers in "EverQuest II", launched in July 2005. The program facilitates buying in-game items for real money from fellow players for a nominal fee. At this point this system only applies to select "EverQuest II" servers; none of the pre-"Station Exchange" "EverQuest II" or "EverQuest" servers are affected.
Intellectual property and role-playing.
Another well-publicized incident from October 2000, usually referred to as the "Mystere incident", involved Verant banning a player for creating controversial fan fiction, causing outrage among "EverQuest" players and sparking a major industry-wide debate about players' rights and the line between roleplaying and intellectual property infringement. The case was used by several academics in discussing such rights in the digital age.
Fans have created the open source server emulator EQEmu, allowing users to run their own servers with custom rules.
Addiction.
The game is renowned and berated (by some psychologists specializing in computer addiction) for its addictive qualities. Many players refer to it half-jokingly as "EverCrack" (a disparaging comparison to crack cocaine). There has been one well-publicized suicide of an "EverQuest" user named Shawn Woolley that resulted in his mother, Liz, founding Online Gamers Anonymous.
Sociological aspects of MMORPGs.
Massively multiplayer online role-playing games (MMORPGs) are described by some players as "chat rooms with a graphical interface". The sociological aspects of "EverQuest" (and other MMORPGs) are explored in a series of online studies on a site known as "the HUB". The studies make use of data gathered from player surveys and discuss topics like virtual relationships, player personalities, gender issues, and more.
Organized protests.
In May 2004, Woody Hearn of GU Comics called for all "EverQuest" gamers to boycott the "Omens of War" expansion in an effort to force SOE to address existing issues with the game rather than release another "quick-fire" expansion. The call to boycott was rescinded after SOE held a summit to address player concerns, improve (internal and external) communication, and correct specific issues within the game.
Prohibition in Brazil.
On 17 January 2008, the Judge of the 17th Federal Court of Minas Gerais State forbade the sales of the game in the whole Brazilian territory. The reason was that the game leads the players to a loss of moral virtue and takes them into "heavy" psychological conflicts because of the game quests.
EverQuest universe.
Since "EverQuest"‍ '​s release, Sony Online Entertainment has added several "EverQuest"-related games. These include:
A line of novels have been published in the world of "EverQuest", including:

</doc>
<doc id="10326" url="http://en.wikipedia.org/wiki?curid=10326" title="Human evolution">
Human evolution

Human evolution is the evolutionary process leading up to the appearance of anatomically modern humans. The topic usually covers the evolutionary history of primates, in particular the genus "Homo", and the emergence of "Homo sapiens" as a distinct species of hominids (or "great apes") rather than studying the evolutionary history that led to primates. The study of human evolution involves many scientific disciplines, including physical anthropology, primatology, archaeology, paleontology, ethology, linguistics, evolutionary psychology, embryology and genetics.
Genetic studies show that primates diverged from other mammals about million years ago, in the Late Cretaceous period, and the earliest fossils appear in the Paleocene, around million years ago. The Hominidae family diverged from the Hylobatidae (gibbon) family 15–20 million years ago, and around million years ago, the subfamily Ponginae (orangutans) diverged from the Hominidae family.
Bipedalism is the basic adaption of the hominin line. The earliest bipedal hominin is considered to be either "Sahelanthropus" or "Orrorin"; alternatively, either "Sahelanthropus" or "Orrorin" may instead be the last shared ancestor between chimps and humans. "Ardipithecus", a full bipedal, arose somewhat later, and the early bipedals eventually evolved into the australopithecines, and later into the genus "Homo".
The earliest documented members of the genus "Homo" are "Homo habilis", which evolved around million years ago; and it is arguably the earliest species for which there is positive evidence of use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization began, and with the arrival of "Homo erectus" in the fossil record, cranial capacity had doubled to 850 cm3. This increase in human brain size is equivalent to every generation having an additional 125,000 neurons more than their parents. It is believed that these species were the first to use fire and complex tools. "Homo erectus" and "Homo ergaster" were also the first of the hominin line to leave Africa, and these species spread through Africa, Asia, and Europe between 1.8.
According to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from "Homo heidelbergensis", "Homo rhodesiensis" or "Homo antecessor" and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of "Homo erectus", Denisova hominins, "Homo floresiensis" and "Homo neanderthalensis". Archaic "Homo sapiens", the forerunner of anatomically modern humans, evolved between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited inter-breeding between these species.<ref name="10.1126/science.1209202"></ref> Anatomically modern humans evolved from archaic "Homo sapiens" in the Middle Paleolithic, about 200,000 years ago. The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago according to many anthropologists although some suggest a gradual change in behavior over a longer time span.
History of study.
Before Darwin.
The word "homo", the name of the biological genus to which humans belong, is Latin for "human." It was chosen originally by Carolus Linnaeus in his classification system. The word "human" is from the Latin "humanus", the adjectival form of "homo". The Latin "homo" derives from the Indo-European root *"dhghem", or "earth." Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.
Darwin.
The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's "On the Origin of Species", in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."
The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and apes, and did so particularly in his 1863 book "Evidence as to Man's Place in Nature". However, many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans when he published "The Descent of Man" in 1871.
First fossils.
A major problem at that time was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of "On the Origin of Species", and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were human remains of a creature suffering some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called "Homo erectus" at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described "Australopithecus africanus". The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocranial cast of the brain.
Although the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.
The East African fossils.
During the 1960s and 1970s, hundreds of fossils were found, particularly in East Africa in the regions of the Olduvai Gorge and Lake Turkana. The driving force in the East African researches was the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter in-law Meave being among the most successful fossil hunters and palaeoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed fossils of australopithecines, early "Homo" and even "Homo erectus".
These finds cemented Africa as the cradle of humankind. In the 1980s, Ethiopia emerged as the new hot spot of palaeoanthropology as "Lucy," the most complete fossil member of the species "Australopithecus afarensis", was found by Donald Johanson in Hadar in the desertic Middle Awash region of northern Ethiopia. This area would be the location of many new hominin fossils, particularly those uncovered by the team headed by Tim D. White in the 1990s, such as "Ardipithecus ramidus".
The genetic revolution.
The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.
In their seminal 1967 paper in "Science", Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy," and reinterpretation of older fossil materials, notably "Ramapithecus", showed the younger estimates to be correct and validated the albumin method.
Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.
On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimps noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimps to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimp populations in 8 locations suggests that chimps reproduce at age 26.5 years, on average; which suggests the human divergence from chimps occurred between 7 to 13 million years ago. And these data suggest that "Ardipithecus" (4.5 Ma), "Orrorin" (6 Ma) and "Sahelanthropus" (7 Ma) all may be on the hominin lineage, and even that the separation may have occurred outside the East African Rift region.
Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimps, interspecies mating between "proto-human" and "proto-chimps" nonetheless occurred regularly enough to change certain genes in the new gene pool:
The research suggests:
The quest for the earliest hominin.
In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered "Australopithecus anamensis". The find was overshadowed by Tim D. White's 1995 discovery of "Ardipithecus ramidus", which pushed back the fossil record to million years ago.
In 2000, Martin Pickford and Brigitte Senut discovered in the Tugen Hills of Kenya a 6-million-year-old bipedal hominin which they named "Orrorin tugenensis". And in 2001, a team led by Michel Brunet discovered the skull of "Sahelanthropus tchadensis" which was dated as million years ago, and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin (cf Hominidae; terms "hominids" and hominins).
Human dispersal.
Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the "Homo" genus. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that genus "Homo" have migrated out of Africa at least three times (e.g. "Homo erectus", "Homo heidelbergensis" and "Homo sapiens").
The "out of Africa" model proposed that modern "H. sapiens" speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in nearly complete replacement of other "Homo" species. This model has been developed by Chris B. Stringer and Peter Andrews.
In contrast, the multiregional hypothesis proposed that "Homo" genus contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple million years. This model was proposed in 1988 by Milford H. Wolpoff.
Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage. Aligned in genetic tree differences were interpreted as supportive of a recent single origin. Analyses have shown a greater diversity of DNA patterns throughout Africa, consistent with the idea that Africa is the ancestral home of mitochondrial Eve and Y-chromosomal Adam.
"Out of Africa" has gained support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.
A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters." The research also located the origin of modern human migration in south-western Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for Richard Leakey to resolve this debate. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. Evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans had been suggested by some studies.
Recent sequencing of Neanderthal and Denisovan genomes shows that some admixture occurred. Modern humans outside Africa have 2–4% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation. After recovery from a genetic bottleneck that might be due to the Toba supervolcano catastrophe, a fairly small group left Africa and briefly interbred with Neanderthals, probably in the middle-east or even North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in south-east Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations.
There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory, which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant.
A second wave of humans may have dispersed across the Sinai Peninsula into Asia, resulting in the bulk of human population for Eurasia. This second group possibly possessed a more sophisticated tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA lineages, which support a single migration out of Africa that gave rise to all non-African populations.
Anatomical changes.
Human evolution is characterized by a number of morphological, developmental, physiological, and behavioral changes that have taken place since the split between the last common ancestor of humans and chimpanzees. The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in "H. erectus".
Bipedalism.
Bipedalism is the basic adaption of the hominin and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominins. The earliest hominin, of presumably primitive bipedalism, is considered to be either "Sahelanthropus" or "Orrorin", both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorilla and chimpanzee, diverged from the hominin line over a period covering the same time, so either of "Sahelanthropus" or "Orrorin" may instead be our last shared ancestor. "Ardipithecus", a full bipedal, arose somewhat later. 
The early bipedals eventually evolved into the australopithecines and later the genus "Homo". There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed up the hands for reaching and carrying food, saved energy during locomotion, enabled long distance running and hunting, enhanced field of vision and helped avoid hyperthermia by reducing the surface area exposed to direct sun; all this mainly for thriving in the new grassland type environment rather than the previous forest type. A new study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking.
Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.
The most significant changes are in the pelvic region, where the long downward facing iliac blade was shortened and became wide as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broad, bowl-like pelvis due to this. A drawback is that the birth canal of these apes is smaller than regular knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, permitting the passage of newborns due to the increase in cranial size but this is limited to the upper portion, since further increase can hinder normal bipedal movement.
The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller size of the birth canal became an obstacle when the brain size began to increase in early humans, prompted a shorter gestation period and the reason why humans give birth to immature offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are motile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a big effect upon the female reproductive cycle, and the more frequent appearance of monogamous relationships in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation saying that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more of their own.
Encephalization.
The human species developed a much larger brain than that of other primates—typically 1,330  cm3 in modern humans, over twice the size of that of a chimpanzee or gorilla. The pattern of encephalization started with "Homo habilis", which at approximately 600  cm3 had a brain slightly larger than that of chimpanzees, and continued with "Homo erectus" (800–1,100  cm3), reaching a maximum in Neanderthals with an average size of (1,200–1,900  cm3), larger even than "Homo sapiens". The pattern of human postnatal brain growth differs from that of other apes (heterochrony) and allows for extended periods of social learning and language acquisition in juvenile humans. However, the differences between the structure of human brains and those of other apes may be even more significant than differences in size.
The increase in volume over time has affected areas within the brain unequally—the temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex which has been related to complex decision-making and moderating social behavior. Encephalization has been tied to an increasing emphasis on meat in the diet, or with the development of cooking, and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex. The human brain was able to expand because of the changes in the morphology of smaller mandibles and mandible muscle attachments to the skull into allowing more room for the brain to grow.
The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Traditionally the cerebellum has been associated with a paleocerebellum and archicerebellum as well as a neocerebellum. Its function has also traditionally been associated with balance, fine motor control but more recently speech and cognition. The great apes including humans and its antecessors had a more pronounced development of the cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and assisting in learning complex muscular action sequences, the cerebellum may have underpinned the evolution of human's technological adaptations including the preadaptation of speech.
Sexual dimorphism.
The reduced degree of sexual dimorphism is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans and bonobos are the only apes in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling during estrus).
Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.
Other changes.
A number of other changes have also characterized the evolution of humans, among them an increased importance on vision rather than smell; a smaller gut; loss of body hair; evolution of sweat glands; a change in the shape of the dental arcade from being u-shaped to being parabolic; development of a chin (found in "Homo sapiens" alone); development of styloid processes; and the development of a descended larynx.
Evidence.
The evidence on which scientific accounts of human evolution is based comes from many fields of natural science. The main sources of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.
Evidence from molecular biology.
The closest living relatives of humans are bonobos and chimpanzees (both genus "Pan") and gorillas (genus "Gorilla"). With the sequencing of both the human and chimpanzee genome, current estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated. 
The gibbons (family Hylobatidae) and then orangutans (genus "Pongo") were the first groups to split from the line leading to the hominins, including humans—followed by gorillas, and, ultimately, by the chimpanzees (genus "Pan"). The splitting date between hominin and chimpanzee lineages is placed by some between 8, that is, during the Late Miocene.Speciation, however, appears to have been unusually drawn-out. Initial divergence occurred sometime between 13, but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at 6.
Genetic evidence has also been employed to resolve the question of whether there was any gene flow between early modern humans and Neanderthals, and to enhance our understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.
Each time a certain mutation (Single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.
Genetics.
Human evolutionary genetics studies how one human genome differs from the other, the evolutionary past that gave rise to it, and its current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.
Evidence from the fossil record.
There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are "Sahelanthropus tchadensis" dating from million years ago, "Orrorin tugenensis" dating from million years ago, and "Ardipithecus kadabba" dating to million years ago. Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.
The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around million years ago and diverged into robust (also called "Paranthropus") and gracile branches, one of which (possibly "A. garhi") probably went on to become ancestors of the genus "Homo". The australopithecine species that is best represented in the fossil record is "Australopithecus afarensis" with more than one hundred fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as "A. robustus" (or alternatively "Paranthropus robustus") and "A./P. boisei" are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.
The earliest member of the genus "Homo" is "Homo habilis" which evolved around million years ago. "Homo habilis" is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider "Homo rudolfensis", a larger bodied group of fossils with similar morphology to the original "H. habilis" fossils, to be a separate species while others consider them to be part of "H. habilis"—simply representing species internal variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.
During the next million years, a process of encephalization began and, by the arrival (about million years ago) of "Homo erectus" in the fossil record, cranial capacity had doubled. "Homo erectus" were the first of the hominins to emigrate from Africa, and, from 1.3, this species spread through Africa, Asia, and Europe. One population of "H. erectus", also sometimes classified as a separate species "Homo ergaster", stayed in Africa and evolved into "Homo sapiens". It is believed that these species, "H. erectus" and "H. ergaster", were the first to use fire and complex tools.
The earliest transitional fossils between "H. ergaster/erectus" and archaic "H. sapiens" are from Africa, such as "Homo rhodesiensis", but seemingly transitional forms were also found at Dmanisi, Georgia. These descendants of African "H. erectus" spread through Eurasia from ca. 500,000 years ago evolving into "H. antecessor", "H. heidelbergensis" and "H. neanderthalensis". The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 200,000 years ago such as the Omo remains of Ethiopia; later fossils from Es Skhul cave in Israel and Southern Europe begin around 90,000 years ago ( million years ago).
As modern humans spread out from Africa, they encountered other hominins such as "Homo neanderthalensis" and the so-called Denisovans, who may have evolved from populations of "Homo erectus" that had left Africa around million years ago. The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.
This migration out of Africa is estimated to have begun about 70,000 years BP (Before Present) and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.
Before "Homo".
Early evolution of primates.
Evolutionary history of the primates can be traced back 65 million years. The oldest known primate-like mammal species, the "Plesiadapis", came from North America, but they were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.
David R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to "Dryopithecus", migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominins.
The earliest known catarrhine is "Kamoyapithecus" from uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to "Aegyptopithecus", "Propliopithecus", and "Parapithecus" from the Faiyum, at around 35 million years ago. In 2010, "Saadanius" was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 million years ago, helping to fill an 11-million-year gap in the fossil record.
In the Early Miocene, about 22 million years ago, the many kinds of arboreally adapted primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to "Victoriapithecus", the earliest Old World Monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are "Proconsul", "Rangwapithecus", "Dendropithecus", "Limnopithecus", "Nacholapithecus", "Equatorius", "Nyanzapithecus", "Afropithecus", "Heliopithecus", and "Kenyapithecus", all from East Africa.
The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant—"Otavipithecus" from cave deposits in Namibia, and "Pierolapithecus" and "Dryopithecus" from France, Spain and Austria—is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, "Oreopithecus", is from coal beds in Italy that have been dated to 9 million years ago.
Molecular evidence indicates that the lineage of gibbons (family Hylobatidae) diverged from the line of great apes some 18–12 million years ago, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown South East Asian hominoid population, but fossil proto-orangutans may be represented by "Sivapithecus" from India and "Griphopithecus" from Turkey, dated to around 10 million years ago.
Divergence of the human clade from other great apes.
Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by "Nakalipithecus" fossils found in Kenya and "Ouranopithecus" found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus "Pan") split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation—rain forest soils tend to be acidic and dissolve bone—and sampling bias probably contribute to this problem.
Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are "Sahelanthropus tchadensis" (7 Ma) and "Orrorin tugenensis" (6 Ma), followed by "Ardipithecus" (5.5–4.4 Ma), with species "A. kadabba" and "A. ramidus".
Genus "Australopithecus".
The "Australopithecus" genus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including "Australopithecus anamensis", "A. afarensis", "A. sediba", and "A. africanus". There is still some debate amongst academics whether certain African hominid species of this time, such as "A. robustus" and "A. boisei", constitute members of the same genus; if so, they would be considered to be "A. robust australopiths" whilst the others would be considered "A. gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, the "Paranthropus".
Genus "Homo".
"Homo sapiens" is the only extant species of its genus, "Homo". While some (extinct) "Homo" species might have been ancestors of "Homo sapiens", many, perhaps most, were likely "cousins," having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should inferred as a separate species and which as subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the "Homo" genus. The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the early variation in the genus "Homo".
Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various "Homo" species and to study the role of diet in physical and behavioral evolution within "Homo".
Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatra island in Indonesia some 70,000 years ago caused global consequences, killing most humans then alive and creating a population bottleneck that affected the genetic inheritance of all humans today.
"H. habilis" and "H. gautengensis".
"Homo habilis" lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines. "Homo habilis" had smaller molars and larger brains than the australopithecines, and made tools from stone and perhaps animal bones. One of the first known hominins, it was nicknamed 'handy man' by discoverer Louis Leakey due to its association with stone tools. Some scientists have proposed moving this species out of "Homo" and into "Australopithecus" due to the morphology of its skeleton being more adapted to living on trees rather than to moving on two legs like "Homo sapiens".
In May 2010, a new species, "Homo gautengensis" was discovered in South Africa.
"H. rudolfensis" and "H. georgicus".
These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to "Homo habilis" is not yet clear.
"H. ergaster" and "H. erectus".
The first fossils of "Homo erectus" were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material "Pithecanthropus erectus" based on its morphology, which he considered to be intermediate between that of humans and apes. "Homo erectus" lived from about 1.8 Ma to about 70,000 years ago—which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby "Homo floresiensis" survived it. The early phase of"Homo erectus", from 1.8 to 1.25 Ma, is considered by some to be a separate species, "Homo ergaster", or as "Homo erectus ergaster", a subspecies of "Homo erectus".
In Africa in the Early Pleistocene, 1.5–1 Ma, in some populations of "Homo habilis" are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, "Homo erectus"—in Africa. The evolution of locking knees and the movement of the foramen magnum (the hole in the skull where the spine enters) are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. 
A famous example of "Homo erectus" is Peking Man; others were found in Asia (notably in Indonesia), Africa, and Europe. Many paleoanthropologists now use the term "Homo ergaster" for the non-Asian forms of this group, and reserve "Homo erectus" only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from "H. ergaster".
"H. cepranensis" and "H. antecessor".
These are proposed as species that may be intermediate between "H. erectus" and "H. heidelbergensis".
"H. heidelbergensis".
"H. heidelbergensis" ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as "Homo sapiens heidelbergensis" or "Homo sapiens paleohungaricus".
Neanderthal and Denisovan.
"Homo neanderthalensis", alternatively designated as "Homo sapiens neanderthalensis", lived in Europe and Asia from 400,000 to about 30,000 years ago. Evidence from sequencing mitochondrial DNA indicated that no significant gene flow occurred between "H. neanderthalensis" and "H. sapiens", and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans "circa" 45,000 to 80,000 years ago (at the approximate time that modern humans migrated out from Africa, but before they dispersed into Europe, Asia and elsewhere).
Nearly all modern non-African humans have 1% to 4% of their DNA derived from Neanderthal DNA, and this finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although the interpretation of these studies has been questioned. Neanderthals and "Homo sapiens" could have co-existed in Europe for as long as 10,000 years, during which human populations exploded vastly outnumbering Neanderthals, possibly outcompeting them by sheer numerical strength.
In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear genomic DNA were sequenced.
While the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from that lineage giving rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe for more than 10,000 years, and the discovery raises the possibility that Neanderthals, modern humans and the Denisova hominin may have co-existed. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the genomes of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.
Alleles thought to have originated in Neanderthal and the Denisova hominin have been identified at several genetic loci in the genomes of modern humans outside of Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians, indicating strong positive selection for these introgressed alleles.
"H. floresiensis".
"H. floresiensis", which lived from approximately 100,000 to 12,000 years before present, has been nicknamed "hobbit" for its small size, possibly a result of insular dwarfism. "H. floresiensis" is intriguing both for its size and its age, being an example of a recent species of the genus "Homo" that exhibits derived traits not shared with modern humans. In other words, "H. floresiensis" shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003 it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the "H. sapiens" average of 1400 cm3). 
However, there is an ongoing debate over whether "H. floresiensis" is indeed a separate species. Some scientists hold that "H. floresiensis" was a modern "H. sapiens" with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could possibly create a hobbit-like human. The other major attack on "H. floresiensis" as a separate species is that it was found with tools only associated with "H. sapiens".
The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.
"H. sapiens".
"H. sapiens" (the adjective "sapiens" is Latin for "wise" or "intelligent") have lived from about 250,000 years ago to the present. Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in skull expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from "H. erectus" to "H. sapiens". The direct evidence suggests there was a migration of "H. erectus" out of Africa, then a further speciation of "H. sapiens" from "H. erectus" in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed "H. erectus". This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. Current evidence does not preclude some multiregional evolution or some admixture of the migrant "H. sapiens" with existing "Homo" populations. This is a hotly debated area of paleoanthropology.
Current research has established that humans are genetically highly homogenous; that is, the DNA of individuals is more alike than usual for most species, which may have resulted from their relatively recent evolution or the possibility of a population bottleneck resulting from cataclysmic natural events such as the Toba catastrophe. Distinctive genetic characteristics have arisen, however, primarily as the result of small groups of people moving into new environmental circumstances. These adapted traits are a very small component of the "Homo sapiens" genome, but include various characteristics such as skin color and nose form, in addition to internal characteristics such as the ability to breathe more efficiently at high altitudes.
H. sapiens idaltu, from Ethiopia, is an extinct sub-species from about 160,000 years ago.
Use of tools.
The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes about 13 watts (260 kilocalories per day), a fifth of body's total energy consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.
Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.
It should be noted that many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are the Oldowan stone tools from Ethiopia, 2.5–2.6 million years old. A "Homo" fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the "Homo" species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The Third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.
Bernard Wood noted that "Paranthropus" co-existed with the early "Homo" species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies "Paranthropus" as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early "Homo" species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, "Homo" was always present, but "Paranthropus" was not.
In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the "Homo" and "Paranthropus" species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.
Stone tools.
Stone tools are first attested around 2.6 Ma, when "H. habilis" in Eastern Africa used so-called pebble tools, choppers made out of round pebbles that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000-10,000 years ago.
Archaeologists working in the Great Rift Valley in Kenya claim to have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.
The period from 700,000–300,000 years ago is also known as the Acheulean, when "H. ergaster" (or "erectus") made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). In this period they also started to make tools out of bone.
Transition to behavioral modernity.
Until about 50,000–40,000 years ago the use of stone tools seems to have progressed stepwise. Each phase ("H. habilis", "H. ergaster", "H. neanderthalensis") started at a higher level than the previous one, but after each phase started, further development was slow. Currently paleoanthropologists are debating whether these "Homo" species possessed some or many of the cultural and behavioral traits associated with modern humans such as language, complex symbolic thinking, technological creativity etc. It seems that they were culturally conservative maintaining simple technologies and foraging patterns over very long periods. 
Around 50,000 BP modern human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a Eurasian "Great Leap Forward," or as the "Upper Palaeolithic Revolution," due to the sudden appearance of distinctive signs of modern behavior in the archaeological record. Other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African "Homo sapiens" since 200,000 years ago.
Modern humans started burying their dead, using animal hides to make clothing, hunting with more sophisticated techniques (such as using trapping pits or driving animals off cliffs), and engaging in cave painting. As human culture advanced, different populations of humans introduced novelty to existing technologies: artifacts such as fish hooks, buttons and bone needles show signs of variation among different populations of humans, something that had not been seen in human cultures prior to 50,000 BP. Typically, "H. neanderthalensis" populations do not vary in their technologies.
Among concrete examples of modern human behavior, anthropologists include specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (for example, burials with grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks. Debate continues as to whether a "revolution" led to modern humans ("the big bang of human consciousness"), or whether the evolution was more gradual.
Recent and current human evolution.
Natural selection still exerts its ancient methodologies on modern human populations. For example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations .
It has been argued that human evolution has accelerated since the development of agriculture and civilization some 10,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations. The retention of lactase persistence into adulthood is an example of such recent evolution. Recent human evolution seems to have been largely confined to genetic resistance to infectious disease that have appeared in human populations by crossing the species barrier from domesticated animals.
Without a 'natural selection' pressure or opportunity for speciation—such as geographic isolation of a local human population——recent human evolution has been predominantly subject to genetic drift.
Species list.
This list is in chronological order across the page by genus.
Bibliography.
</dl>
Further reading.
</dl>

</doc>
<doc id="10328" url="http://en.wikipedia.org/wiki?curid=10328" title="Evliya Çelebi">
Evliya Çelebi

Mehmed Zilli (25 March 1611 – after 1682), known as Evliya Çelebi (Ottoman Turkish: اوليا چلبى), was an Ottoman Turk who travelled through the territory of the Ottoman Empire and neighboring lands over a period of forty years, recording his commentary in a travelogue called "Seyâhatnâme".
Life.
Evliya Çelebi was born in 1611 Constantinople to a wealthy family from Kütahya. His father was Derviş Mehmed Zilli, an Ottoman court jeweller, and mother an Abkhazian relative of the later grand vizier Melek Ahmed Pasha. In his book, Evliya Çelebi traces his paternal genealogy back to Ahmed Yesevi. Evliya Çelebi received a court education from the Imperial ulema. He may have joined the Gülşenî sufi order, as he shows an intimate knowledge of the sufi lodge in Cairo, and a graffito in which he referred to himself as "Evliya-yı Gülşenî" (Evliya of the Gülşenî). A devout Muslim opposed to fanaticism, Evliya could recite the Koran from memory and joked freely about Islam. Though employed as a religious expert and entertainer to the Ottoman grandees, Evliya refused employment that would keep him from travelling. His journal writing began in Constantinople, taking notes on buildings, markets, customs and culture, and in 1640 it was extended with accounts of his travels beyond the confines of the city. The collected notes of Evilya Çelebi's travels form a ten-volume work called the "Seyahatname" ("Travelogue").
He fought the Habsburgs in Transylvania.
Evliya Çelebi died sometime after 1682: it is unclear whether he was in Constantinople or Cairo at the time.
Travels.
Mostar.
According to Evliya Çelebi, the name "Mostar" means "bridge-keeper." Of the bridge, 28 meters long and 20 meters high, Çelebi wrote that "the bridge is like a rainbow arch soaring up to the skies, extending from one cliff to the other. ...I, a poor and miserable slave of Allah, have passed through 16 countries, but I have never seen such a high bridge. It is thrown from rock to rock as high as the sky."
Europe.
Çelebi claimed to have encountered Native Americans as a guest in Rotterdam during his visit of 1663. He wrote: “[they] cursed those Jesuits, saying, "Our world used to be peaceful, but it has been filled by greedy colonialists, who make war every year and shorten our lives”.
While visiting Vienna in 1665–66, Çelebi noted some similarities between words in German and Persian, an early observation of the genetic relationship between what would later be known as two Indo-European languages.
Azerbaijan.
Of oil merchants in Baku Çelebi wrote: "By Allah's decree oil bubbles up out of the ground, but in the manner of hot springs, pools of water are formed with oil congealed on the surface like cream. Merchants wade into these pools and collect the oil in ladles and fill goatskins with it, these oil merchants then sell them in different regions. Revenues from this oil trade are delivered annually directly to the Safavid Shah."
Crimean Khanate.
Evliya Çelebi remarked on the impact of Cossack raids from Azak upon the territories of the Crimean Khanate, destroying trade routes and severely depopulating the regions. By the time of Çelebi's arrival, many of the towns visited were affected by the Cossacks, and the only place he reported as safe was the Ottoman fortress at Arabat.
Çelebi wrote of the slave trade in the Crimea:
A man who had not seen this market, had not seen anything in this world. A mother is severed from her son and daughter there, a son—from his father and brother, and they are sold amongst lamentations, cries of help, weeping and sorrow.
Parthenon.
In 1667 Çelebi expressed his marvel at the Parthenon's sculptures and described the building as "like some impregnable fortress not made by human agency." He composed a poetic supplication that the Parthenon, as "a work less of human hands than of Heaven itself, should remain standing for all time."
The "Seyâhatnâme".
Although many of the descriptions the "Seyâhatnâme" were written in an exaggerated manner or were plainly inventive fiction or 3rd-source misinterpretation, his notes remain a useful guide to the culture and lifestyles of the 17th century Ottoman Empire. The first volume deals exclusively with Constantinople, the final volume with Egypt.
Currently there is no English translation of the entire "Seyahatname", although there are translations of various parts. The longest single English translation was published in 1834 by Ritter Joseph von Hammer-Purgstall, an Austrian orientalist: it may be found under the name "Evliya Efendi." Von Hammer's work covers the first two volumes (Constantinople and Anatolia) but its language is antiquated. Other translations include Erich Prokosch's nearly complete translation into German of the tenth volume, the 2004 introductory work entitled "The World of Evliya Çelebi: An Ottoman Mentality" written by University of Chicago professor Robert Dankoff, and Dankoff and Sooyong Kim's 2010 translation of select excerpts of the ten volumes, "An Ottoman Traveller: Selections from the Book of Travels of Evliya Çelebi".
Evliya is noted for having collected specimens of the languages in each region he traveled in. There are some 30 Turkic dialects and languages cataloged in the "Seyâhatnâme". Çelebi notes the similarities between several words from the German and Persian, though he denies any common Indo-European heritage. The "Seyâhatnâme" also contains the first transcriptions of many Caucasian languages and Tsakonian, and the only extant specimens of written Ubykh outside the linguistic literature.
In the 10 volumes of his Seyâhatnâme, he describes the following journeys:
Popular culture.
"İstanbul Kanatlarımın Altında" (Istanbul Under My Wings, 1996) is a film about the lives of Hezarfen Ahmet Çelebi, his brother Lagari Hasan Çelebi, and the Ottoman society in the early 17th century, during the reign of Murad IV, as witnessed and narrated by Evliya Çelebi.
Çelebi appears in Orhan Pamuk's novel "The White Castle", and is featured in the "The Adventures of Captain Bathory" (Dobrodružstvá kapitána Báthoryho) novels by Slovak writer Juraj Červenák.
United Nations Educational, Scientific and Cultural Organization, UNESCO included the 400th anniversary of Evliya Celebi's birth in its timetable for the celebration of anniversaries.

</doc>
<doc id="10331" url="http://en.wikipedia.org/wiki?curid=10331" title="Ancient Egyptian religion">
Ancient Egyptian religion

Ancient Egyptian religion was a complex system of polytheistic beliefs and rituals which were an integral part of ancient Egyptian society. It centered on the Egyptians' interaction with many deities who were believed to be present in, and in control of, the forces and elements of nature. The practices of Egyptian religion were efforts to provide for the gods and gain their favor. Formal religious practice centered on the pharaoh, the king of Egypt. Although a human, the Pharaoh was believed to be descended from the gods. He acted as the intermediary between his people and the gods, and was obligated to sustain the gods through rituals and offerings so that they could maintain order in the universe. The state dedicated enormous resources to Egyptian rituals and to the construction of the temples.
Individuals could interact with the gods for their own purposes, appealing for their help through prayer or compelling them to act through magic. These practices were distinct from, but closely linked with, the formal rituals and institutions. The popular religious tradition grew more prominent in the course of Egyptian history as the status of the Pharaoh declined. Another important aspect was the belief in the afterlife and funerary practices. The Egyptians made great efforts to ensure the survival of their souls after death, providing tombs, grave goods, and offerings to preserve the bodies and spirits of the deceased.
The religion had its roots in Egypt's prehistory and lasted for more than 3,000 years. The details of religious belief changed over time as the importance of particular gods rose and declined, and their intricate relationships shifted. At various times, certain gods became preeminent over the others, including the sun god Ra, the creator god Amun, and the mother goddess Isis. For a brief period, in the aberrant theology promulgated by the Pharaoh Akhenaten, a single god, the Aten, replaced the traditional pantheon. Ancient Egyptian religion and mythology left behind many writings and monuments, along with significant influences on ancient and modern cultures.
Theology.
The beliefs and rituals now referred to as "Ancient Egyptian religion" were integral within every aspect of Egyptian culture. Their language possessed no single term corresponding to the modern European concept of religion. Ancient Egyptian religion was not a monolithic institution, but consisted of a vast and varying set of beliefs and practices, linked by their common focus on the interaction between the world of humans and the world of the divine. The characteristics of the gods who populated the divine realm were inextricably linked to the Egyptians' understanding of the properties of the world in which they lived.
Deities.
The Egyptians believed that the phenomena of nature were divine forces in and of themselves. These deified forces included the elements, animal characteristics, or abstract forces. The Egyptians believed in a pantheon of gods, which were involved in all aspects of nature and human society. Their religious practices were efforts to sustain and placate these phenomena and turn them to human advantage. This polytheistic system was very complex, as some deities were believed to exist in many different manifestations, and some had multiple mythological roles. Conversely, many natural forces, such as the sun, were associated with multiple deities. The diverse pantheon ranged from gods with vital roles in the universe to minor deities or "demons" with very limited or localized functions. It could include gods adopted from foreign cultures, and sometimes humans: deceased Pharaohs were believed to be divine, and occasionally, distinguished commoners such as Imhotep also became deified.
The depictions of the gods in art were not meant as literal representations of how the gods might appear if they were visible, as the gods' true natures were believed to be mysterious. Instead, these depictions gave recognizable forms to the abstract deities by using symbolic imagery to indicate each god's role in nature. Thus, for example, the funerary god Anubis was portrayed as a jackal, a creature whose scavenging habits threatened the preservation of the body, in an effort to counter this threat and employ it for protection. His black skin was symbolic of the color of mummified flesh and the fertile black soil that Egyptians saw as a symbol of resurrection. This iconography was not fixed, and many of the gods could be depicted in more than one form.
Many gods were associated with particular regions in Egypt where their cults were most important. However, these associations changed over time, and they did not mean that the god associated with a place had originated there. For instance, the god Monthu was the original patron of the city of Thebes. Over the course of the Middle Kingdom, however, he was displaced in that role by Amun, who may have arisen elsewhere. The national popularity and importance of individual gods fluctuated in a similar way.
Associations between deities.
The Egyptian gods had complex interrelationships, which partly reflected the interaction of the forces they represented. The Egyptians often grouped gods together to reflect these relationships. Some groups of deities were of indeterminate size, and were linked by their similar functions. These often consisted of minor deities with little individual identity. Other combinations linked independent deities based on the symbolic meaning of numbers in Egyptian mythology; for instance, pairs of deities usually represent the duality of opposite phenomena. One of the more common combinations was a family triad consisting of a father, mother, and child, who were worshipped together. Some groups had wide-ranging importance. One such group, the Ennead, assembled nine deities into a theological system that was involved in the mythological areas of creation, kingship, and the afterlife.
The relationships between deities could also be expressed in the process of syncretism, in which two or more different gods were linked to form a composite deity. This process was a recognition of the presence of one god "in" another when the second god took on a role belonging to the first. These links between deities were fluid, and did not represent the permanent merging of two gods into one; therefore, some gods could develop multiple syncretic connections. Sometimes syncretism combined deities with very similar characteristics. At other times it joined gods with very different natures, as when Amun, the god of hidden power, was linked with Ra, the god of the sun. The resulting god, Amun-Ra, thus united the power that lay behind all things with the greatest and most visible force in nature.
Unifying tendencies.
Many deities could be given epithets that seem to indicate that they were greater than any other god, suggesting some kind of unity beyond the multitude of natural forces. In particular, this is true of a few gods who, at various times in history, rose to supreme importance in Egyptian religion. These included the royal patron Horus, the sun god Ra, and the mother goddess Isis. During the New Kingdom (c. 1550–1070 BC), Amun held this position. The theology of the period described in particular detail Amun's presence in and rule over all things, so that he, more than any other deity, embodied the all-encompassing power of the divine.
Because of theological statements like this, many past Egyptologists, such as Siegfried Morenz, believed that beneath the polytheistic traditions of Egyptian religion there was an increasing belief in a unity of the divine, moving toward monotheism. Instances in Egyptian literature where "god" is mentioned without reference to any specific deity would seem to give this view added weight. However, in 1971 Erik Hornung pointed out that the traits of an apparently supreme being could be attributed to many different gods, even in periods when other gods were preeminent, and further argued that references to an unspecified "god" are meant to refer flexibly to any deity. He therefore argued that, while some individuals may have henotheistically chosen one god to worship, Egyptian religion as a whole had no notion of a divine being beyond the immediate multitude of deities. Yet the debate did not end there; Jan Assmann and James P. Allen have since asserted that the Egyptians did to some degree recognize a single divine force. In Allen's view, the notion of an underlying unity of the divine coexisted inclusively with the polytheistic tradition. It is possible that only the Egyptian theologians fully recognized this underlying unity, but it is also possible that ordinary Egyptians identified the single divine force with a single god in particular situations.
Atenism.
The Egyptians did have an aberrant period of some form of monotheism during the New Kingdom, in which the pharaoh Akhenaten abolished the official worship of other gods in favor of the sun-disk Aten. This is often seen as the first instance of true monotheism in history, although the details of Atenist theology are still unclear. The exclusion of all but one god was a radical departure from Egyptian tradition and some see Akhenaten as a practitioner of monolatry rather than monotheism, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten. Under Akhenaten's successors Egypt reverted to its traditional religion, and Akhenaten himself came to be reviled as a heretic.
Other important concepts.
Cosmology.
The Egyptian conception of the universe centered on "Ma'at", a word that encompasses several concepts in English, including "truth," "justice," and "order." It was the fixed, eternal order of the universe, both in the cosmos and in human society. It had existed since the creation of the world, and without it the world would lose its cohesion. In Egyptian belief, Ma'at was constantly under threat from the forces of disorder, so all of society was required to maintain it. On the human level this meant that all members of society should cooperate and coexist; on the cosmic level it meant that all of the forces of nature—the gods—should continue to function in balance. This latter goal was central to Egyptian religion. The Egyptians sought to maintain Ma'at in the cosmos by sustaining the gods through offerings and by performing rituals which staved off disorder and perpetuated the cycles of nature.
The most important part of the Egyptian view of the cosmos was the conception of time, which was greatly concerned with the maintenance of Ma'at. Throughout the linear passage of time, a cyclical pattern recurred, in which Ma'at was renewed by periodic events which echoed the original creation. Among these events were the annual Nile flood and the succession from one king to another, but the most important was the daily journey of the sun god Ra.
When envisioning the shape of the cosmos, the Egyptians saw the earth as a flat expanse of land, personified by the god Geb, over which arched the sky goddess Nut. The two were separated by Shu, the god of air. Beneath the earth lay a parallel underworld and undersky, and beyond the skies lay the infinite expanse of Nu, the chaos that had existed before creation. The Egyptians also believed in a place called the Duat, a mysterious region associated with death and rebirth, that may have lain in the underworld or in the sky. Each day, Ra traveled over the earth across the underside of the sky, and at night he passed through the Duat to be reborn at dawn.
In Egyptian belief, this cosmos was inhabited by three types of sentient beings. One was the gods; another was the spirits of deceased humans, who existed in the divine realm and possessed many of the gods' abilities. Living humans were the third category, and the most important among them was the pharaoh, who bridged the human and divine realms.
Divine pharaoh.
Egyptologists have long debated the degree to which the Pharaoh was considered a god. It seems most likely that the Egyptians viewed royal authority itself as a divine force. Therefore, although the Egyptians recognized that the Pharaoh was human and subject to human weakness, they simultaneously viewed him as a god, because the divine power of kingship was incarnated in him. He therefore acted as intermediary between Egypt's people and the gods. He was key to upholding Ma'at, both by maintaining justice and harmony in human society and by sustaining the gods with temples and offerings. For these reasons, he oversaw all state religious activity. However, the Pharaoh’s real-life influence and prestige could differ from that depicted in official writings and depictions, and beginning in the late New Kingdom his religious importance declined drastically.
The king was also associated with many specific deities. He was identified directly with Horus, who represented kingship itself, and he was seen as the son of Ra, who ruled and regulated nature as the Pharaoh ruled and regulated society. By the New Kingdom he was also associated with Amun, the supreme force in the cosmos. Upon his death, the king became fully deified. In this state, he was directly identified with Ra, and was also associated with Osiris, god of death and rebirth and the mythological father of Horus. Many mortuary temples were dedicated to the worship of deceased pharaohs as gods.
Afterlife.
The Egyptians had elaborate beliefs about death and the afterlife. They believed that humans possessed a "ka", or life-force, which left the body at the point of death. In life, the ka received its sustenance from food and drink, so it was believed that, to endure after death, the ka must continue to receive offerings of food, whose spiritual essence it could still consume. Each person also had a "ba", the set of spiritual characteristics unique to each individual. Unlike the ka, the ba remained attached to the body after death. Egyptian funeral rituals were intended to release the ba from the body so that it could move freely, and to rejoin it with the ka so that it could live on as an akh. However, it was also important that the body of the deceased be preserved, as the Egyptians believed that the ba returned to its body each night to receive new life, before emerging in the morning as an akh.
Originally, however, the Egyptians believed that only the pharaoh had a ba, and only he could become one with the gods; dead commoners passed into a dark, bleak realm that represented the opposite of life. The nobles received tombs and the resources for their upkeep as gifts from the king, and their ability to enter the afterlife was believed to be dependent on these royal favors. In early times the deceased pharaoh was believed to ascend to the sky and dwell among the stars. Over the course of the Old Kingdom (c. 2686–2181 BC), however, he came to be more closely associated with the daily rebirth of the sun god Ra and with the underworld ruler Osiris as those deities grew more important.
During the late Old Kingdom and the First Intermediate Period (c. 2181–2055 BC), the Egyptians gradually came to believe that possession of a "ba" and the possibility of a paradisiacal afterlife extended to everyone. In the fully developed afterlife beliefs of the New Kingdom, the soul had to avoid a variety of supernatural dangers in the Duat, before undergoing a final judgment known as the "Weighing of the Heart". In this judgment, the gods compared the actions of the deceased while alive (symbolized by the heart) to Ma'at, to determine whether he or she had behaved in accordance with Ma'at. If the deceased was judged worthy, his or her ka and ba were united into an akh. Several beliefs coexisted about the akh's destination. Often the dead were said to dwell in the realm of Osiris, a lush and pleasant land in the underworld. The solar vision of the afterlife, in which the deceased soul traveled with Ra on his daily journey, was still primarily associated with royalty, but could extend to other people as well. Over the course of the Middle and New Kingdoms, the notion that the akh could also travel in the world of the living, and to some degree magically affect events there, became increasingly prevalent.
Writings.
While the Egyptians had no unified religious scripture, they produced many religious writings of various types. Together the disparate texts provide a very extensive, but still incomplete, understanding of Egyptian religious practices and beliefs.
Mythology.
Egyptian myths were metaphorical stories intended to illustrate and explain the gods' actions and roles in nature. The details of the events they recounted could change to convey different symbolic perspectives on the mysterious divine events they described, so many myths exist in different and conflicting versions. Mythical narratives were rarely written in full, and more often texts only contain episodes from or allusions to a larger myth. Knowledge of Egyptian mythology, therefore, is derived mostly from hymns that detail the roles of specific deities, from ritual and magical texts which describe actions related to mythic events, and from funerary texts which mention the roles of many deities in the afterlife. Some information is also provided by allusions in secular texts. Finally, Greeks and Romans such as Plutarch recorded some of the extant myths late in Egyptian history.
Among the significant Egyptian myths were the creation myths. According to these stories, the world emerged as a dry space in the primordial ocean of chaos. Because the sun is essential to life on earth, the first rising of Ra marked the moment of this emergence. Different forms of the myth describe the process of creation in various ways: a transformation of the primordial god Atum into the elements that form the world, as the creative speech of the intellectual god Ptah, and as an act of the hidden power of Amun. Regardless of these variations, the act of creation represented the initial establishment of maat and the pattern for the subsequent cycles of time.
The most important of all Egyptian myths was the myth of Osiris and Isis. It tells of the divine ruler Osiris, who was murdered by his jealous brother Set, a god often associated with chaos. Osiris' sister and wife Isis resurrected him so that he could conceive an heir, Horus. Osiris then entered the underworld and became the ruler of the dead. Once grown, Horus fought and defeated Set to become king himself. Set's association with chaos, and the identification of Osiris and Horus as the rightful rulers, provided a rationale for Pharaonic succession and portrayed the Pharaohs as the upholders of order. At the same time, Osiris' death and rebirth were related to the Egyptian agricultural cycle, in which crops grew in the wake of the Nile inundation, and provided a template for the resurrection of human souls after death.
Another important mythic motif was the journey of Ra through the Duat each night. In the course of this journey, Ra met with Osiris, who again acted as an agent of regeneration, so that his life was renewed. He also fought each night with Apep, a serpentine god representing chaos. The defeat of Apep and the meeting with Osiris ensured the rising of the sun the next morning, an event that represented rebirth and the victory of order over chaos.
Ritual and magical texts.
The procedures for religious rituals were frequently written on papyri, which were used as instructions for those performing the ritual. These ritual texts were kept mainly in the temple libraries. Temples themselves are also inscribed with such texts, often accompanied by illustrations. Unlike the ritual papyri, these inscriptions were not intended as instructions, but were meant to symbolically perpetuate the rituals even if, in reality, people ceased to perform them. Magical texts likewise describe rituals, although these rituals were part of the spells used for specific goals in everyday life. Despite their mundane purpose, many of these texts also originated in temple libraries and later became disseminated among the general populace.
Hymns and prayers.
The Egyptians produced numerous prayers and hymns, written in the form of poetry. Hymns and prayers follow a similar structure and are distinguished mainly by the purposes they serve. Hymns were written to praise particular deities. Like ritual texts, they were written on papyri and on temple walls, and they were probably recited as part of the rituals they accompany in temple inscriptions. Most are structured according to a set literary formula, designed to expound on the nature, aspects, and mythological functions of a given deity. They tend to speak more explicitly about fundamental theology than other Egyptian religious writings, and became particularly important in the New Kingdom, a period of particularly active theological discourse. Prayers follow the same general pattern as hymns, but address the relevant god in a more personal way, asking for blessings, help, or forgiveness for wrongdoing. Such prayers are rare before the New Kingdom, indicating that in earlier periods such direct personal interaction with a deity was not believed possible, or at least was less likely to be expressed in writing. They are known mainly from inscriptions on statues and stelae left in sacred sites as votive offerings.
Funerary texts.
Among the most significant and extensively preserved Egyptian writings are funerary texts designed to ensure that deceased souls reached a pleasant afterlife. The earliest of these are the Pyramid Texts. They are a loose collection of hundreds of spells inscribed on the walls of royal pyramids during the Old Kingdom, intended to magically provide the king with the means to join the company of the gods in the afterlife. The spells appear in differing arrangements and combinations, and few of them appear in all of the pyramids.
At the end of the Old Kingdom a new body of funerary spells, which included material from the Pyramid Texts, began appearing in tombs, inscribed primarily on coffins. This collection of writings is known as the Coffin Texts, and was not reserved for royalty, but appeared in the tombs of non-royal officials. In the New Kingdom, several new funerary texts emerged, of which the best-known is the Book of the Dead. Unlike the earlier books, it often contains extensive illustrations, or vignettes. The book was copied on papyrus and sold to commoners to be placed in their tombs.
The Coffin Texts included sections with detailed descriptions of the underworld and instructions on how to overcome its hazards. In the New Kingdom, this material gave rise to several "books of the netherworld", including the Book of Gates, the Book of Caverns, and the Amduat. Unlike the loose collections of spells, these netherworld books are structured depictions of Ra's passage through the Duat, and by analogy, the journey of the deceased person's soul through the realm of the dead. They were originally restricted to pharaonic tombs, but in the Third Intermediate Period they came to be used more widely.
Practices.
Temples.
Temples existed from the beginning of Egyptian history, and at the height of the civilization they were present in most of its towns. They included both mortuary temples to serve the spirits of deceased pharaohs and temples dedicated to patron gods, although the distinction was blurred because divinity and kingship were so closely intertwined. The temples were not primarily intended as places for worship by the general populace, and the common people had a complex set of religious practices of their own. Instead, the state-run temples served as houses for the gods, in which physical images which served as their intermediaries were cared for and provided with offerings. This service was believed to be necessary to sustain the gods, so that they could in turn maintain the universe itself. Thus, temples were central to Egyptian society, and vast resources were devoted to their upkeep, including both donations from the monarchy and large estates of their own. Pharaohs often expanded them as part of their obligation to honor the gods, so that many temples grew to enormous size. However, not all gods had temples dedicated to them, as many gods who were important in official theology received only minimal worship, and many household gods were the focus of popular veneration rather than temple ritual.
The earliest Egyptian temples were small, impermanent structures, but through the Old and Middle Kingdoms their designs grew more elaborate, and they were increasingly built out of stone. In the New Kingdom, a basic temple layout emerged, which had evolved from common elements in Old and Middle Kingdom temples. With variations, this plan was used for most of the temples built from then on, and most of those that survive today adhere to it. In this standard plan, the temple was built along a central processional way that led through a series of courts and halls to the sanctuary, which held a statue of the temple's god. Access to this most sacred part of the temple was restricted to the pharaoh and the highest-ranking priests. The journey from the temple entrance to the sanctuary was seen as a journey from the human world to the divine realm, a point emphasized by the complex mythological symbolism present in temple architecture. Well beyond the temple building proper was the outermost wall. In the space between the two lay many subsidiary buildings, including workshops and storage areas to supply the temple's needs, and the library where the temple's sacred writings and mundane records were kept, and which also served as a center of learning on a multitude of subjects.
Theoretically it was the duty of the pharaoh to carry out temple rituals, as he was Egypt's official representative to the gods. In reality, ritual duties were almost always carried out by priests. During the Old and Middle Kingdoms, there was no separate class of priests; instead, many government officials served in this capacity for several months out of the year before returning to their secular duties. Only in the New Kingdom did professional priesthood become widespread, although most lower-ranking priests were still part-time. All were still employed by the state, and the pharaoh had final say in their appointments. However, as the wealth of the temples grew, the influence of their priesthoods increased, until it rivaled that of the pharaoh. In the political fragmentation of the Third Intermediate Period (c. 1070–664 BC), the high priests of Amun at Karnak even became the effective rulers of Upper Egypt. The temple staff also included many people other than priests, such as musicians and chanters in temple ceremonies. Outside the temple were artisans and other laborers who helped supply the temple's needs, as well as farmers who worked on temple estates. All were paid with portions of the temple's income. Large temples were therefore very important centers of economic activity, sometimes employing thousands of people.
Official rituals and festivals.
State religious practice included both temple rituals involved in the cult of a deity, and ceremonies related to divine kingship. Among the latter were coronation ceremonies and the sed festival, a ritual renewal of the pharaoh's strength that took place periodically during his reign. There were numerous temple rituals, including rites that took place across the country and rites limited to single temples or to the temples of a single god. Some were performed daily, while others took place annually or on rarer occasions. The most common temple ritual was the morning offering ceremony, performed daily in temples across Egypt. In it, a high-ranking priest, or occasionally the pharaoh, washed, anointed, and elaborately dressed the god's statue before presenting it with offerings. Afterward, when the god had consumed the spiritual essence of the offerings, the items themselves were taken to be distributed among the priests.
The less frequent temple rituals, or festivals, were still numerous, with dozens occurring every year. These festivals often entailed actions beyond simple offerings to the gods, such as reenactments of particular myths or the symbolic destruction of the forces of disorder. Most of these events were probably celebrated only by the priests and took place only inside the temple. However, the most important temple festivals, like the Opet Festival celebrated at Karnak, usually involved a procession carrying the god's image out of the sanctuary in a model barque to visit other significant sites, such as the temple of a related deity. Commoners gathered to watch the procession and sometimes received portions of the unusually large offerings given to the gods on these occasions.
Animal cults.
At many sacred sites, the Egyptians worshipped individual animals which they believed to be manifestations of particular deities. These animals were selected based on specific sacred markings which were believed to indicate their fitness for the role. Some of these cult animals retained their positions for the rest of their lives, as with the Apis bull worshipped in Memphis as a manifestation of Ptah. Other animals were selected for much shorter periods. These cults grew more popular in later times, and many temples began raising stocks of such animals from which to choose a new divine manifestation. A separate practice developed in the Twenty-sixth Dynasty, when people began mummifying any member of a particular animal species as an offering to the god whom the species represented. Millions of mummified cats, birds, and other creatures were buried at temples honoring Egyptian deities. Worshippers paid the priests of a particular deity to obtain and mummify an animal associated with that deity, and the mummy was placed in a cemetery near the god's cult center.
Oracles.
The Egyptians used oracles to ask the gods for knowledge or guidance. Egyptian oracles are known mainly from the New Kingdom and afterward, though they probably appeared much earlier. People of all classes, including the king, asked questions of oracles, and, especially in the late New Kingdom their answers could be used to settle legal disputes or inform royal decisions. The most common means of consulting an oracle was to pose a question to the divine image while it was being carried in a festival procession, and interpret an answer from the barque's movements. Other methods included interpreting the behavior of cult animals, drawing lots, or consulting statues through which a priest apparently spoke. The means of discerning the god's will gave great influence to the priests who spoke and interpreted the god's message.
Popular religion.
While the state cults were meant to preserve the stability of the Egyptian world, lay individuals had their own religious practices that related more directly to daily life. This popular religion left less evidence than the official cults, and because this evidence was mostly produced by the wealthiest portion of the Egyptian population, it is uncertain to what degree it reflects the practices of the populace as a whole.
Popular religious practice included ceremonies marking important transitions in life. These included birth, because of the danger involved in the process, and naming, because the name was held to be a crucial part of a person's identity. The most important of these ceremonies were those surrounding death (see "Funerary practices" below), because they ensured the soul's survival beyond it. Other religious practices sought to discern the gods' will or seek their knowledge. These included the interpretation of dreams, which could be seen as messages from the divine realm, and the consultation of oracles. People also sought to affect the gods' behavior to their own benefit through magical rituals (see "Magic" below).
Individual Egyptians also prayed to gods and gave them private offerings. Evidence of this type of personal piety is sparse before the New Kingdom. This is probably due to cultural restrictions on depiction of nonroyal religious activity, which relaxed during the Middle and New Kingdoms. Personal piety became still more prominent in the late New Kingdom, when it was believed that the gods intervened directly in individual lives, punishing wrongdoers and saving the pious from disaster. Official temples were important venues for private prayer and offering, even though their central activities were closed to laypeople. Egyptians frequently donated goods to be offered to the temple deity and objects inscribed with prayers to be placed in temple courts. Often they prayed in person before temple statues or in shrines set aside for their use. Yet in addition to temples, the populace also used separate local chapels, smaller but more accessible than the formal temples. These chapels were very numerous, and probably staffed by members of the community. Households, too, often had their own small shrines for offering to gods or deceased relatives.
The deities invoked in these situations differed somewhat from those at the center of state cults. Many of the important popular deities, such as the fertility goddess Taweret and the household protector Bes, had no temples of their own. However, many other gods, including Amun and Osiris, were very important in both popular and official religion. Some individuals might be particularly devoted to a single god. Often they favored deities affiliated with their own region, or with their role in life. The god Ptah, for instance, was particularly important in his cult center of Memphis, but as the patron of craftsmen he received the nationwide veneration of many in that occupation.
Magic.
The word "magic" is used to translate the Egyptian term "heka", which meant, as James P. Allen puts it, "the ability to make things happen by indirect means". Heka was believed to be a natural phenomenon, the force which was used to create the universe and which the gods employed to work their will. Humans could also use it, however, and magical practices were closely intertwined with religion. In fact, even the regular rituals performed in temples were counted as magic. Individuals also frequently employed magical techniques for personal purposes. Although these ends could be harmful to other people, no form of magic was considered inimical in itself. Instead, magic was seen primarily as a way for humans to prevent or overcome negative events.
Magic was closely associated with the priesthood. Because temple libraries contained numerous magical texts, great magical knowledge was ascribed to the lector priests who studied these texts. These priests often worked outside their temples, hiring out their magical services to laymen. Other professions also commonly employed magic as part of their work, including doctors, scorpion-charmers, and makers of magical amulets. It is also likely that the peasantry used simple magic for their own purposes, but because this magical knowledge would have been passed down orally, there is limited evidence of it.
Language was closely linked with heka, to such a degree that Thoth, the god of writing, was sometimes said to be the inventor of heka. Therefore, magic frequently involved written or spoken incantations, although these were usually accompanied by ritual actions. Often these rituals invoked the power of an appropriate deity to perform the desired action, using the power of heka to compel it to act. Sometimes this entailed casting the practitioner or subject of a ritual in the role of a character in mythology, thus inducing the god to act toward that person as it had in the myth. Rituals also employed sympathetic magic, using objects believed to have a magically significant resemblance to the subject of the rite. The Egyptians also commonly used objects believed to be imbued with heka of their own, such as the magically protective amulets worn in great numbers by ordinary Egyptians.
Funerary practices.
Because it was considered necessary for the survival of the soul, preservation of the body was a central part of Egyptian funerary practices. Originally the Egyptians buried their dead in the desert, where the arid conditions mummified the body naturally. In the Early Dynastic Period, however, they began using tombs for greater protection, and the body was insulated from the desiccating effect of the sand and was subject to natural decay. Thus the Egyptians developed their elaborate embalming practices, in which the corpse was artificially desiccated and wrapped to be placed in its coffin. The quality of the process varied according to cost, however, and those who could not afford it were still buried in desert graves.
Once the mummification process was complete, the mummy was carried from the deceased person's house to the tomb in a funeral procession that included his or her friends and relatives, along with a variety of priests. Before the burial, these priests performed several rituals, including the Opening of the mouth ceremony intended to restore the dead person's senses and give him or her the ability to receive offerings. Then the mummy was buried and the tomb sealed. Afterward, relatives or hired priests gave food offerings to the deceased in a nearby mortuary chapel at regular intervals. Over time, families inevitably neglected offerings to long-dead relatives, so most mortuary cults only lasted one or two generations. However, while the cult lasted, the living sometimes wrote letters asking deceased relatives for help, in the belief that the dead could affect the world of the living as the gods did.
The first Egyptian tombs were mastabas, rectangular brick structures where kings and nobles were entombed. Each of them contained a subterranean burial chamber and a separate, aboveground chapel for mortuary rituals. In the Old Kingdom the mastaba developed into the pyramid, which symbolized the primeval mound of Egyptian myth. Pyramids were reserved for royalty, and were accompanied by large mortuary temples sitting at their base. Middle Kingdom pharaohs continued to build pyramids, but the popularity of mastabas waned. Increasingly, commoners with sufficient means were buried in rock-cut tombs with separate mortuary chapels nearby, an approach which was less vulnerable to tomb robbery. By the beginning of the New Kingdom even the pharaohs were buried in such tombs, and they continued to be used until the decline of the religion itself.
Tombs could contain a great variety of other items, including statues of the deceased to serve as substitutes for the body in case it was damaged. Because it was believed that the deceased would have to do work in the afterlife, just as in life, burials often included small models of humans to do work in place of the deceased. The tombs of wealthier individuals could also contain furniture, clothing, and other everyday objects intended for use in the afterlife, along with amulets and other items intended to provide magical protection against the hazards of the spirit world. Further protection was provided by funerary texts included in the burial. The tomb walls also bore artwork, including images of the deceased eating food which were believed to allow him or her to magically receive sustenance even after the mortuary offerings had ceased.
History.
Predynastic and Early Dynastic periods.
The beginnings of Egyptian religion extend into prehistory, and evidence for them comes only from the sparse and ambiguous archaeological record. Careful burials during the Predynastic period imply that the people of this time believed in some form of an afterlife. At the same time, animals were ritually buried, a practice which may reflect the development of zoomorphic deities like those found in the later religion. The evidence is less clear for gods in human form, and this type of deity may have emerged more slowly than those in animal shape. Each region of Egypt originally had its own patron deity, but it is likely that as these small communities conquered or absorbed each other, the god of the defeated area was either incorporated into the other god's mythology or entirely subsumed by it. This resulted in a complex pantheon in which some deities remained only locally important while others developed more universal significance. As the time changed and the shifting of the empires changed like the middle kingdom, new kingdom, and old kingdom, usually the religion followed stayed within the border of that territory.
The Early Dynastic period began with the unification of Egypt around 3000 BC. This event transformed Egyptian religion, as some deities rose to national importance and the cult of the divine pharaoh became the central focus of religious activity. Horus was identified with the king, and his cult center in the Upper Egyptian city of Nekhen was among the most important religious sites of the period. Another important center was Abydos, where the early rulers built large funerary complexes.
Old and Middle Kingdoms.
During the Old Kingdom, the priesthoods of the major deities attempted to organize the complicated national pantheon into groups linked by their mythology and worshipped in a single cult center, such as the Ennead of Heliopolis which linked important deities such as Atum, Ra, Osiris, and Set in a single creation myth. Meanwhile, pyramids, accompanied by large mortuary temple complexes, replaced mastabas as the tombs of pharaohs. In contrast with the great size of the pyramid complexes, temples to gods remained comparatively small, suggesting that official religion in this period emphasized the cult of the divine king more than the direct worship of deities. The funerary rituals and architecture of this time greatly influenced the more elaborate temples and rituals used in worshipping the gods in later periods.
Early in the Old Kingdom, Ra grew in influence, and his cult center at Heliopolis became the nation's most important religious site. By the Fifth Dynasty, Ra was the most prominent god in Egypt, and had developed the close links with kingship and the afterlife that he retained for the rest of Egyptian history. Around the same time, Osiris became an important afterlife deity. "The Pyramid Texts," first written at this time, reflect the prominence of the solar and Osirian concepts of the afterlife, although they also contain remnants of much older traditions. The texts are an extremely important source for understanding early Egyptian theology.
In the 22nd century BC, the Old Kingdom collapsed into the disorder of the First Intermediate Period, with important consequences for Egyptian religion. Old Kingdom officials had already begun to adopt the funerary rites originally reserved for royalty, but now, less rigid barriers between social classes meant that these practices and the accompanying beliefs gradually extended to all Egyptians, a process called the "democratization of the afterlife". The Osirian view of the afterlife had the greatest appeal to commoners, and thus Osiris became one of the most important gods.
Eventually rulers from Thebes reunified the Egyptian nation in the Middle Kingdom (c. 2055–1650 BC). These Theban pharaohs initially promoted their patron god Monthu to national importance, but during the Middle Kingdom, he was eclipsed by the rising popularity of Amun. In this new Egyptian state, personal piety grew more important and was expressed more freely in writing, a trend which continued in the New Kingdom.
New Kingdom.
The Middle Kingdom crumbled in the Second Intermediate Period (c. 1650–1550 BC), but the country was again reunited by Theban rulers, who became the first pharaohs of the New Kingdom. Under the new regime, Amun became the supreme state god. He was syncretized with Ra, the long-established patron of kingship, and his temple at Karnak in Thebes became Egypt's most important religious center. Amun's elevation was partly due to the great importance of Thebes, but it was also due to the increasingly professional priesthood. Their sophisticated theological discussion produced detailed descriptions of Amun's universal power.
Increased contact with outside peoples in this period led to the adoption of many Near Eastern deities into the pantheon. At the same time, the subjugated Nubians absorbed Egyptian religious beliefs, and in particular, adopted Amun as their own.
The New Kingdom religious order was disrupted when Akhenaten acceded, and replaced Amun with the Aten as the state god. Eventually he eliminated the official worship of most other gods, and moved Egypt's capital to a new city at Amarna. This part of Egyptian history, the Amarna period, is named after this. In doing so, Akhenaten claimed unprecedented status: only he could worship the Aten, and the populace directed their worship toward him. The Atenist system lacked well-developed mythology and afterlife beliefs, and the Aten seemed distant and impersonal, so the new order did not appeal to ordinary Egyptians. Thus, many probably continued to worship the traditional gods in private. Nevertheless, the withdrawal of state support for the other deities severely disrupted Egyptian society. Akhenaten's successors restored the traditional religious system, and eventually they dismantled all Atenist monuments.
Before the Amarna period, popular religion had trended toward more personal relationships between worshippers and their gods. Akhenaten's changes had reversed this trend, but once the traditional religion was restored, there was a backlash. The populace began to believe that the gods were much more directly involved in daily life. Amun, the supreme god, was increasingly seen as the final arbiter of human destiny, the true ruler of Egypt. The pharaoh was correspondingly more human and less divine. The importance of oracles as a means of decision-making grew, as did the wealth and influence of the oracles' interpreters, the priesthood. These trends undermined the traditional structure of society and contributed to the breakdown of the New Kingdom.
Later periods.
In the 1st millennium BC, Egypt was significantly weaker than in earlier times, and in several periods foreigners seized the country and assumed the position of pharaoh. The importance of the pharaoh continued to decline, and the emphasis on popular piety continued to increase. Animal cults, a characteristically Egyptian form of worship, became increasingly popular in this period, possibly as a response to the uncertainty and foreign influence of the time. Isis grew more popular as a goddess of protection, magic, and personal salvation, and became the most important goddess in Egypt.
In the 4th century BC, Egypt became a Hellenistic kingdom under the Ptolemaic dynasty (305–30 BC), which assumed the pharaonic role, maintaining the traditional religion and building or rebuilding many temples. The kingdom's Greek ruling class identified the Egyptian deities with their own. From this cross-cultural syncretism emerged Serapis, a god who combined Osiris and Apis with characteristics of Greek deities, and who became very popular among the Greek population. Nevertheless, for the most part the two belief systems remained separate, and the Egyptian deities remained Egyptian.
Ptolemaic-era beliefs changed little after Egypt became a province of the Roman Empire in 30 BC, with the Ptolemaic kings replaced by distant emperors. The cult of Isis appealed even to Greeks and Romans outside Egypt, and in Hellenized form it spread across the empire. In Egypt itself, as the empire weakened, official temples fell into decay, and without their centralizing influence religious practice became fragmented and localized. Meanwhile, Christianity spread across Egypt, and in the third and fourth centuries AD, edicts by Christian emperors and iconoclasm by local Christians eroded traditional beliefs. While it persisted among the populace for some time, Egyptian religion slowly faded away.
Legacy.
Egyptian religion produced the temples and tombs which are ancient Egypt's most enduring monuments, but it also influenced other cultures. In pharaonic times many of its symbols, such as the sphinx and winged solar disk, were adopted by other cultures across the Mediterranean and Near East, as were some of its deities, such as Bes. Some of these connections are difficult to trace. The Greek concept of Elysium may have derived from the Egyptian vision of the afterlife. In late antiquity, the Christian conception of Hell was most likely influenced by some of the imagery of the Duat. Biblical accounts of Jesus and Mary may have been influenced by that of Isis and Orisis. Egyptian beliefs also influenced or gave rise to several esoteric belief systems developed by Greeks and Romans, who considered Egypt as a source of mystic wisdom. Hermeticism, for instance, derived from the tradition of secret magical knowledge associated with Thoth.
Modern Times.
Traces of ancient beliefs remained in Egyptian folk traditions into modern times, but its influence on modern societies greatly increased with the French Campaign in Egypt and Syria in 1798 and their seeing the monuments and images. As a result of it, Westerners began to study Egyptian beliefs firsthand, and Egyptian religious motifs were adopted into Western art. Egyptian religion has since had a significant influence in popular culture. Due to continued interest in Egyptian belief, in the late 20th century, several new religious groups have formed based on different reconstructions of ancient Egyptian religion.
Bibliography.
</dl>

</doc>
<doc id="10332" url="http://en.wikipedia.org/wiki?curid=10332" title="Educational psychology">
Educational psychology

Educational psychology is the branch of psychology concerned with the scientific study of human learning. The study of learning processes, from both cognitive and behavioral perspectives, allows researchers to understand individual differences in intelligence, cognitive development, affect, motivation, self-regulation, and self-concept, as well as their role in learning. The field of educational psychology relies heavily on quantitative methods, including testing and measurement, to enhance educational activities related to instructional design, classroom management, and assessment, which serve to facilitate learning processes in various educational settings across the lifespan. 
Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. It is also informed by neuroscience. Educational psychology in turn informs a wide range of specialities within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks.
The field of educational psychology involves the study of memory, conceptual processes, and individual differences (via cognitive psychology) in conceptualizing new strategies for learning processes in humans. Educational psychology has been built upon theories of Operant conditioning, functionalism, structuralism, constructivism, humanistic psychology, Gestalt psychology, and information processing.
Educational Psychology has seen rapid growth and development as a profession in the last twenty years. School psychology began with the concept of intelligence testing leading to provisions for special education students, whom could not follow the regular classroom curriculum in the early part of the 20th century. However, "School Psychology" itself has built a fairly new profession based upon the practices and theories of several psychologists among many different fields. Educational Psychologists are working side by side with psychiatrists, social workers, teachers, speech and language therapists, and counselors in attempt to understand the questions being raised when combining behavioral, cognitive, and social psychology in the classroom setting.
History.
Early years.
Educational Psychology is a fairly new and growing field of study. Though it can date back as early as the days of Plato and Aristotle, it was not identified as a specific practice. It was unknown that everyday teaching and learning in which individuals had to think about individual differences, assessment, development, the nature of a subject being taught, problem solving, and transfer of learning was the beginning to the field of educational psychology. These topics are important to education and as a result it is important to understanding human cognition, learning, and social perception.
Plato and Aristotle.
Educational psychology dates back to the time of Aristotle and Plato. Plato and Aristotle researched individual differences in the field of education, training of the body and the cultivation of psycho-motor skills, the formation of good character, the possibilities and limits of moral education. Some other educational topics they spoke about were the effects of music, poetry, and the other arts on the development of individual, role of teacher, and the relations between teacher and student. Plato saw knowledge as an innate ability, which evolves through experience and understanding of the world. Such a statement has evolved into a continuing argument of nature vs. nurture in understanding conditioning and learning today. Aristotle observed the phenomenon of "association." His four laws of association included succession, contiguity, similarity, and contrast. His studies examined recall and facilitated learning processes 
John Locke.
John Locke followed by contrasting Plato's theory of innate learning processes. Rather, he introduced the term "tabula rasa" meaning "blank slate." Locke explained that learning was primarily understood through experience only, and we were all born without knowledge. Locke introduced this idea as "empiricism," or the understanding that knowledge is only built on learning and experience.
Before 1890.
Philosophers of education such as Juan Vives, Johann Pestalozzi, Friedrich Fröbel, and Johann Herbart had examined, classified and judged the methods of education centuries before the beginnings of psychology in the late 1800s.
Juan Vives.
Juan Vives (1493–1540) proposed induction as the method of study and believed in the direct observation and investigation of the study of nature. His studies focus of humanistic learning, which opposed scholasticism and was influenced by a variety of sources including philosophy, psychology, politics,religion, and history. He was one of the first to emphasize that the location of the school is important to learning. He suggested that the school should be located away from disturbing noises; the air quality should be good and there should be plenty of food for the students and teachers. Vives emphasized the importance of understanding individual differences of the students and suggested practice as an important tool for learning.
Vives introduced his educational ideas in his writing, "De anima et vita" in 1538. In this publication, Vives explores moral philosophy as a setting for his educational ideals; with this, he explains that the different parts of the soul (similar to that of Aristotle's ideas) are each responsible for different operations, which function distinctively. The first book covers the different "souls": "The Vegatative Soul;" this is the soul of nutrition, growth, and reproduction, "The Sensitive Soul," which involves the five external senses; "The Cogitative soul," which includes internal senses and cognitive facilities. The second book involves functions of the rational soul: mind, will, and memory. Lastly, the third book explains the analysis of emotions.
Johann Pestalozzi.
Johann Pestalozzi (1746–1827), a German educational reformer, emphasized the child rather than the content of the school. Pestalozzi fostered an educational reform backed by the idea that early education was crucial for children, and could be manageable for mothers. Eventually, this experience with early education would lead to a "wholesome person characterized by morality" Pestalozzi has been acknowledged for opening institutions for education, writing books for mother's teaching home education, and elementary books for students, mostly focusing on the kindergarten level. In his later years, he published teaching manuals and methods of teaching.
During the time of The Enlightenment, Pestalozzi's ideals introduced "educationalisation." This created the bridge between social issues and education by introducing the idea of social issues to be solved through education. Horlacher describes the most prominent example of this during The Enlightenment to be "improving agricultural production methods." 
Johann Herbart.
Johann Herbart (1776–1841) is considered the father of educational psychology. He believed that learning was influenced by interest in the subject and the teacher. He thought that teachers should consider the students' existing mental sets--what they already know--when presenting new information or material. Herbart came up with what are now known as the formal steps. The 5 steps that teachers should use are:
1890–1920.
William James.
The period of 1890–1920 is considered the golden era of educational psychology where aspirations of the new discipline rested on the application of the scientific methods of observation and experimentation to educational problems. From 1840 to 1920 37 million people immigrated to the United States. This created an expansion of elementary schools and secondary schools. The increase in immigration also provided educational psychologists the opportunity to use intelligence testing to screen immigrants at Ellis Island. Darwinism influenced the beliefs of the prominent educational psychologists. Even in the earliest years of the discipline, educational psychologists recognized the limitations of this new approach. The pioneering American psychologist William James commented that: 
Psychology is a science, and teaching is an art; and sciences never generate arts directly out of themselves. An intermediate inventive mind must make that application, by using its originality".
James is the father of psychology in America but he also made contributions to educational psychology. In his famous series of lectures "Talks to Teachers on Psychology", published in 1899 and now regarded as the first educational psychology textbook, James defines education as "the organization of acquired habits of conduct and tendencies to behavior". He states that teachers should "train the pupil to behavior" so that he fits into the social and physical world. Teachers should also realize the importance of habit and instinct. They should present information that is clear and interesting and relate this new information and material to things the student already knows about. He also addresses important issues such as attention, memory, and association of ideas.
Alfred Binet.
Alfred Binet published "Mental Fatigue" in 1898, in which he attempted to apply the experimental method to educational psychology. In this experimental method he advocated for two types of experiments, experiments done in the lab and experiments done in the classroom. In 1904 he was appointed the Minister of Public Education. This is when he began to look for a way to distinguish children with developmental disabilities. Binet strongly supported special education programs because he believed that "abnormality" could be cured. The Binet-Simon test was the first intelligence test and was the first to distinguish between "normal children" and those with developmental disabilities. Binet believed that it was important to study individual differences between age groups and children of the same age. He also believed that it was important for teachers to take into account individual students strengths and also the needs of the classroom as a whole when teaching and creating a good learning environment. He also believed that it was important to train teachers in observation so that they would be able to see individual differences among children and adjust the curriculum to the students. Binet also emphasized that practice of material was important. In 1916 Lewis Terman revised the Binet-Simon so that the average score was always 100. The test became known as the Stanford-Binet and was one of the most widely used tests of intelligence. Terman, unlike Binet, was interested in using intelligence test to identify gifted children who had high intelligence. In his longitudinal study of gifted children, who became known as the Termites, Terman found that gifted children become gifted adults.
Edward Thorndike.
Edward Thorndike (1874–1949) supported the scientific movement in education. He based teaching practices on empirical evidence and measurement. Thorndike developed the theory of instrumental conditioning or the law of effect. The law of effect states that associations are strengthened when it is followed by something pleasing and associations are weakened when followed by something not pleasing. He also found that learning is done a little at a time or in increments, learning is an automatic process and all the principles of learning apply to all mammals. Thorndike's research with Robert Woodworth on the theory of transfer found that learning one subject will only influence your ability to learn another subject if the subjects are similar. This discovery led to less emphasis on learning the classics because they found that studying the classics does not contribute to overall general intelligence. Thorndike was one of the first to say that individual differences in cognitive tasks were due to how many stimulus response patterns a person had rather than a general intellectual ability. He contributed word dictionaries that were scientifically based to determine the words and definitions used. The dictionaries were the first to take into consideration the users maturity level. He also integrated pictures and easier pronunciation guide into each of the definitions. Thorndike contributed arithmetic books based on learning theory. He made all the problems more realistic and relevant to what was being studied, not just to improve the general intelligence. He developed tests that were standardized to measure performance in school related subjects. His biggest contribution to testing was the CAVD intelligence test which used a multidimensional approach to intelligence and the first to use a ratio scale. His later work was on programmed instruction, mastery learning and computer-based learning:
If, by a miracle of mechanical ingenuity, a book could be so arranged that only to him who had done what was directed on page one would page two become visible, and so on, much that now requires personal instruction could be managed by print.
John Dewey.
John Dewey (1859–1952) had a major influence on the development of progressive education in the United States. He believed that the classroom should prepare children to be good citizens and facilitate creative intelligence. He pushed for the creation of practical classes that could be applied outside of a school setting. He also thought that education should be student-oriented, not subject-oriented. For Dewey, education was a social experience that helped bring together generations of people. He stated that students learn by doing. He believed in an active mind that was able to be educated through observation, problem solving and enquiry. In his 1910 book "How We Think", he emphasizes that material should be provided in a way that is stimulating and interesting to the student since it encourages original thought and problem solving. He also stated that material should be relative to the student's own experience.
"The material furnished by way of information should be relevant to a question that is vital in the students own experience"
Jean Piaget.
Jean Piaget (1896–1980) developed the theory of cognitive development. The theory stated that intelligence developed in four different stages. The stages are the sensorimotor stage from birth to 2 years old, the preoperational state from 2 years old to 7 years old, the concrete operational stage from 7 years old to 10 years old, and formal operational stage from 11 years old and up. He also believed that learning was constrained to the child's cognitive development. Piaget influenced educational psychology because he was the first to believe that cognitive development was important and something that should be paid attention to in education. Most of the research on Piagetian theory was mainly tested and done by American educational psychologists
1920–present.
The number of people receiving a high school and college education increased dramatically from 1920 to 1960. Because very few jobs were available to teens coming out of eighth grade there was an increase in high school attendance in the 1930s. The progressive movement in the United State took off at this time and led to the idea of progressive education. John Flanagan, an educational psychologist, developed tests for combat trainees and instructions in combat training. In 1954 the work of Kenneth Clark and his wife on the effects of segregation on black and white children was influential in the Supreme Court case Brown v. Board of Education. From the 1960s to present day educational psychology has switched from a behaviorist perspective to a more cognitive based perspective because of the influence and development of cognitive psychology at this time.
Jerome Bruner.
Jerome Bruner is notable for integrating Jean Piaget's cognitive approaches into educational psychology. He advocated for discovery learning where teachers create a problem solving environment that allows the student to question, explore and experiment. In his book "The Process of Education" Bruner stated that the structure of the material and the cognitive abilities of the person are important in learning. He emphasized the importance of the subject matter. He also believed that how the subject was structured was important for the students understanding of the subject and it is the goal of the teacher to structure the subject in a way that was easy for the student to understand. In the early 1960s Bruner went to Africa to teach math and science to schoolchildren, which influenced his view as schooling as a cultural institution. Bruner was also influential in the development of MACOS, Man a Course of Study, which was an educational program that combined anthropology and science. The program explored human evolution and social behavior. He also helped with the development of the head start program. He was interested in the influence of culture on education and looked at the impact of poverty on educational development.
Benjamin Bloom.
Benjamin Bloom (1913–1999) spent over 50 years at the University of Chicago where he worked in the department of education. He believed that all students can learn. He developed taxonomy of educational objectives. The objectives were divided into three domains: cognitive, affective, and psychomotor. The cognitive domain deals with how we think. It is divided into categories that are on a continuum from easiest to more complex. The categories are knowledge or recall, comprehension application, analysis, synthesis and evaluation. The affective domain deals with emotions and has 5 categories. The categories are receiving phenomenon, responding to that phenomenon, valuing, organization, and internalizing values. The psychomotor domain deals with the development of motor skills, movement and coordination and has 7 categories, that also goes from simplest to complex. The 7 categories of the psychomotor domain are perception, set, guided response, mechanism, complex overt response, adaptation, and origination. The taxonomy provided broad educational objectives that could be used to help expand the curriculum to match the ideas in the taxonomy. The taxonomy is considered to have a greater influence internationally than in the United States. Internationally, the taxonomy is used in every aspect of education from training of the teachers to the development of testing material. Bloom believed in communicating clear learning goals and promoting an active student. He thought that teachers should provide feedback to the students on their strengths and weaknesses. Bloom also did research on college students and their problem solving processes. He found that they differ in understanding the basis of the problem and the ideas in the problem. He also found that students differ in process of problem solving in their approach and attitude toward the problem.
Nathaniel Gage.
Nathaniel Gage is important in educational psychology because he did research to improve teaching and understand the processes involved in teaching. In 1963 he was the editor of the "Handbook of Research on Teaching", which became an influential book in educational psychology. The handbook helped set up research on teaching and made research on teaching important to educational psychology. He also was influential in the founding of the Stanford Center for Research and Development in teaching, which not only contributed important research on teaching but also influenced the teaching of important educational psychologists.
Perspectives.
Cognitive.
Each person has an individual profile of characteristics, abilities and challenges that result from predisposition, learning and development. These manifest as individual differences in intelligence, creativity, cognitive style, motivation and the capacity to process information, communicate, and relate to others. The most prevalent disabilities found among school age children are attention deficit hyperactivity disorder (ADHD), learning disability, dyslexia, and speech disorder. Less common disabilities include intellectual disability, hearing impairment, cerebral palsy, epilepsy, and blindness.
Although theories of intelligence have been discussed by philosophers since Plato, intelligence testing is an invention of educational psychology, and is coincident with the development of that discipline. Continuing debates about the nature of intelligence revolve on whether intelligence can be characterized by a single factor known as general intelligence, multiple factors (e.g., Gardner's theory of multiple intelligences), or whether it can be measured at all. In practice, standardized instruments such as the Stanford-Binet IQ test and the WISC are widely used in economically developed countries to identify children in need of individualized educational treatment. Children classified as gifted are often provided with accelerated or enriched programs. Children with identified deficits may be provided with enhanced education in specific skills such as phonological awareness. In addition to basic abilities, the individual's personality traits are also important, with people higher in conscientiousness and hope attaining superior academic achievements, even after controlling for intelligence and past performance.
Behavioral.
Applied behavior analysis, a research-based science utilizing behavioral principles of operant conditioning, is effective in a range of educational settings. For example, teachers can alter student behavior by systematically rewarding students who follow classroom rules with praise, stars, or tokens exchangeable for sundry items. Despite the demonstrated efficacy of awards in changing behavior, their use in education has been criticized by proponents of self-determination theory, who claim that praise and other rewards undermine intrinsic motivation. There is evidence that tangible rewards decrease intrinsic motivation in specific situations, such as when the student already has a high level of intrinsic motivation to perform the goal behavior. But the results showing detrimental effects are counterbalanced by evidence that, in other situations, such as when rewards are given for attaining a gradually increasing standard of performance, rewards enhance intrinsic motivation. Many effective therapies have been based on the principles of applied behavior analysis, including pivotal response therapy which is used to treat autism spectrum disorders.
Cognitive.
Among current educational psychologists, the cognitive perspective is more widely held than the behavioral perspective, perhaps because it admits causally related mental constructs such as traits, beliefs, memories, motivations and emotions. Cognitive theories claim that memory structures determine how information is perceived, processed, stored, retrieved and forgotten. Among the memory structures theorized by cognitive psychologists are separate but linked visual and verbal systems described by Allan Paivio's dual coding theory. Educational psychologists have used dual coding theory and cognitive load theory to explain how people learn from multimedia presentations.
The spaced learning effect, a cognitive phenomenon strongly supported by psychological research, has broad applicability within education. For example, students have been found to perform better on a test of knowledge about a text passage when a second reading of the passage is delayed rather than immediate (see figure). Educational psychology research has confirmed the applicability to education of other findings from cognitive psychology, such as the benefits of using mnemonics for immediate and delayed retention of information.
Problem solving, according to prominent cognitive psychologists, is fundamental to learning. It resides as an important research topic in educational psychology. A student is thought to interpret a problem by assigning it to a schema retrieved from long-term memory. A problem students run into while reading is called "activation." This is when the student's representations of the text are present during working memory. This causes the student to read through the material without absorbing the information and being able to retain it. When working memory is absent from the readers representations of the working memory they experience something called "deactivation." When deactivation occurs, the student has an understanding of the material and is able to retain information. If deactivation occurs during the first reading, the reader does not need to undergo deactivation in the second reading. The reader will only need to reread to get a "gist" of the text to spark their memory. When the problem is assigned to the wrong schema, the student's attention is subsequently directed away from features of the problem that are inconsistent with the assigned schema. The critical step of finding a mapping between the problem and a pre-existing schema is often cited as supporting the centrality of analogical thinking to problem solving.
Developmental.
Developmental psychology, and especially the psychology of cognitive development, opens a special perspective for educational psychology. This is so because education and the psychology of cognitive development converge on a number of crucial assumptions. First, the psychology of cognitive development defines human cognitive competence at successive phases of development. Education aims to help students acquire knowledge and develop skills which are compatible with their understanding and problem-solving capabilities at different ages. Thus, knowing the students' level on a developmental sequence provides information on the kind and level of knowledge they can assimilate, which, in turn, can be used as a frame for organizing the subject matter to be taught at different school grades. This is the reason why Piaget's theory of cognitive development was so influential for education, especially mathematics and science education. In the same direction, the neo-Piagetian theories of cognitive development suggest that in addition to the concerns above, sequencing of concepts and skills in teaching must take account of the processing and working memory capacities that characterize successive age levels.
Second, the psychology of cognitive development involves understanding how cognitive change takes place and recognizing the factors and processes which enable cognitive competence to develop. Education also capitalizes on cognitive change, because the construction of knowledge presupposes effective teaching methods that would move the student from a lower to a higher level of understanding. Mechanisms such as reflection on actual or mental actions vis-à-vis alternative solutions to problems, tagging new concepts or solutions to symbols that help one recall and mentally manipulate them are just a few examples of how mechanisms of cognitive development may be used to facilitate learning.
Finally, the psychology of cognitive development is concerned with individual differences in the organization of cognitive processes and abilities, in their rate of change, and in their mechanisms of change. The principles underlying intra- and inter-individual differences could be educationally useful, because knowing how students differ in regard to the various dimensions of cognitive development, such as processing and representational capacity, self-understanding and self-regulation, and the various domains of understanding, such as mathematical, scientific, or verbal abilities, would enable the teacher to cater for the needs of the different students so that no one is left behind.
Constructivist.
Constructivism is a category of learning theory in which emphasis is placed on the agency and prior "knowing" and experience of the learner, and often on the social and cultural determinants of the learning process. Educational psychologists distinguish individual (or psychological) constructivism, identified with Piaget's theory of cognitive development, from social constructivism. A dominant influence on the latter type is Lev Vygotsky's work on sociocultural learning, describing how interactions with adults, more capable peers, and cognitive tools are internalized to form mental constructs. Elaborating on Vygotsky's theory, Jerome Bruner and other educational psychologists developed the important concept of instructional scaffolding, in which the social or information environment offers supports for learning that are gradually withdrawn as they become internalized.
Conditioning and learning.
To understand the characteristics of learners in childhood, adolescence, adulthood, and old age, educational psychology develops and applies theories of human development. Often represented as stages through which people pass as they mature, developmental theories describe changes in mental abilities (cognition), social roles, moral reasoning, and beliefs about the nature of knowledge.
For example, educational psychologists have conducted research on the instructional applicability of Jean Piaget's theory of development, according to which children mature through four stages of cognitive capability. Piaget hypothesized that children are not capable of abstract logical thought until they are older than about 11 years, and therefore younger children need to be taught using concrete objects and examples. Researchers have found that transitions, such as from concrete to abstract logical thought, do not occur at the same time in all domains. A child may be able to think abstractly about mathematics, but remain limited to concrete thought when reasoning about human relationships. Perhaps Piaget's most enduring contribution is his insight that people actively construct their understanding through a self-regulatory process.
Piaget proposed a developmental theory of moral reasoning in which children progress from a naïve understanding of morality based on behavior and outcomes to a more advanced understanding based on intentions. Piaget's views of moral development were elaborated by Kohlberg into a stage theory of moral development. There is evidence that the moral reasoning described in stage theories is not sufficient to account for moral behavior. For example, other factors such as modeling (as described by the social cognitive theory of morality) are required to explain bullying.
Rudolf Steiner's model of child development interrelates physical, emotional, cognitive, and moral development in developmental stages similar to those later described by Piaget.
Developmental theories are sometimes presented not as shifts between qualitatively different stages, but as gradual increments on separate dimensions. Development of epistemological beliefs (beliefs about knowledge) have been described in terms of gradual changes in people's belief in: certainty and permanence of knowledge, fixedness of ability, and credibility of authorities such as teachers and experts. People develop more sophisticated beliefs about knowledge as they gain in education and maturity.
Motivation.
Motivation is an internal state that activates, guides and sustains behavior. Motivation can have several impacting effects on how students learn and how they behave towards subject matter: 
Educational psychology research on motivation is concerned with the volition or will that students bring to a task, their level of interest and intrinsic motivation, the personally held goals that guide their behavior, and their belief about the causes of their success or failure. As intrinsic motivation deals with activities that act as their own rewards, extrinsic motivation deals with motivations that are brought on by consequences or punishments. A form of attribution theory developed by Bernard Weiner describes how students' beliefs about the causes of academic success or failure affect their emotions and motivations. For example, when students attribute failure to lack of ability, and ability is perceived as uncontrollable, they experience the emotions of shame and embarrassment and consequently decrease effort and show poorer performance. In contrast, when students attribute failure to lack of effort, and effort is perceived as controllable, they experience the emotion of guilt and consequently increase effort and show improved performance.
The self-determination theory (SDT) was developed by psychologists Edward Deci and Richard Ryan. SDT focuses on the importance of intrinsic and extrinsic motivation in driving human behavior and posits inherent growth and development tendencies. It emphasizes the degree to which an individual's behavior is self-motivated and self-determined. When applied to the realm of education, the self-determination theory is concerned primarily with promoting in students an interest in learning, a value of education, and a confidence in their own capacities and attributes.
Motivational theories also explain how learners' goals affect the way they engage with academic tasks. Those who have "mastery goals" strive to increase their ability and knowledge. Those who have "performance approach goals" strive for high grades and seek opportunities to demonstrate their abilities. Those who have "performance avoidance" goals are driven by fear of failure and avoid situations where their abilities are exposed. Research has found that mastery goals are associated with many positive outcomes such as persistence in the face of failure, preference for challenging tasks, creativity and intrinsic motivation. Performance avoidance goals are associated with negative outcomes such as poor concentration while studying, disorganized studying, less self-regulation, shallow information processing and test anxiety. Performance approach goals are associated with positive outcomes, and some negative outcomes such as an unwillingness to seek help and shallow information processing.
Locus of control is a salient factor in the successful academic performance of students. During the 1970s and '80s, Cassandra B. Whyte did significant educational research studying locus of control as related to the academic achievement of students pursuing higher education coursework. Much of her educational research and publications focused upon the theories of Julian B. Rotter in regard to the importance of internal control and successful academic performance. Whyte reported that individuals who perceive and believe that their hard work may lead to more successful academic outcomes, instead of depending on luck or fate, persist and achieve academically at a higher level. Therefore, it is important to provide education and counseling in this regard.
Technology.
Instructional design, the systematic design of materials, activities and interactive environments for learning, is broadly informed by educational psychology theories and research. For example, in defining learning goals or objectives, instructional designers often use a taxonomy of educational objectives created by Benjamin Bloom and colleagues. Bloom also researched mastery learning, an instructional strategy in which learners only advance to a new learning objective after they have mastered its prerequisite objectives. Bloom discovered that a combination of mastery learning with one-to-one tutoring is highly effective, producing learning outcomes far exceeding those normally achieved in classroom instruction. Gagné, another psychologist, had earlier developed an influential method of task analysis in which a terminal learning goal is expanded into a hierarchy of learning objectives connected by prerequisite relationships.
The following list of technological resources incorporate computer-aided instruction and intelligence for educational psychologists and their students: 
Technology is essential to the field of educational psychology, not only for the psychologist themselves as far as testing, organization, and resources, but also for students. Educational Psychologists whom reside in the K- 12 setting focus the majority of their time with Special Education students. It has been found that students with disabilities learning through technology such as IPad applications and videos are more engaged and motivated to learn in the classroom setting. Liu et al. explain that learning-based technology allows for students to be more focused, and learning is more efficient with learning technologies. The authors explain that learning technology also allows for students with social- emotional disabilities to participate in distance learning.
Applications.
Teaching.
Research on classroom management and pedagogy is conducted to guide teaching practice and form a foundation for teacher education programs. The goals of classroom management are to create an environment conducive to learning and to develop students' self-management skills. More specifically, classroom management strives to create positive teacher–student and peer relationships, manage student groups to sustain on-task behavior, and use counseling and other psychological methods to aid students who present persistent psychosocial problems.
Introductory educational psychology is a commonly required area of study in most North American teacher education programs. When taught in that context, its content varies, but it typically emphasizes learning theories (especially cognitively oriented ones), issues about motivation, assessment of students' learning, and classroom management. A developing gives more detail about the educational psychology topics that are typically presented in preservice teacher education.
Counseling.
Training.
In order to become an educational psychologist, students can complete an undergraduate degree in their choice. They then must go to graduate school to study education psychology, counseling psychology, and/ or school counseling. Most students today are also receiving their doctorate degrees in order to hold the "psychologist" title.Educational psychologists work in a variety of settings. Some work in university settings where they carry out research on the cognitive and social processes of human development, learning and education. Educational psychologists may also work as consultants in designing and creating educational materials, classroom programs and online courses.Educational psychologists who work in k–12 school settings (closely related are school psychologists in the US and Canada) are trained at the master's and doctoral levels. In addition to conducting assessments, school psychologists provide services such as academic and behavioral intervention, counseling, teacher consultation, and crisis intervention. However, school psychologists are generally more individual-oriented towards students.
Employment outlook.
Employment for psychologists in the United States is expected to grow faster than most occupations through the year 2014, with anticipated growth of 18–26%. One in four psychologists are employed in educational settings. In the United States, the median salary for psychologists in primary and secondary schools is US$58,360 as of May 2004.
In recent decades the participation of women as professional researchers in North American educational psychology has risen dramatically.
Methods of research.
Educational psychology, as much as any other field of psychology heavily relies on a balance of pure observation and quantitative methods in psychology. The study of education generally combines the studies of history, sociology, and ethics with theoretical approaches. Smeyers and Depaepe explain that historically, the study of education and child rearing have been associated with the interests of policymakers and practitioners within the educational field, however, the recent shift to sociology and psychology has opened the door for new findings in education as a social science. Now being its own academic discipline, educational psychology has proven to be helpful for social science researchers.
Quantitative research is the backing to most observable phenomenon in psychology. This involves observing, creating, and understanding a distribution of data based upon the studies subject matter. Researchers use particular variables to interpret their data distributions from their research and employ statistics as a way of creating data tables and analyzing their data. Psychology has moved from the "common sense" reputations initially posed by Thomas Reid to the methodology approach comparing independent and dependent variables through natural observation, experiments, or combinations of the two. Though results are still, with statistical methods, objectively true based upon significance variables or p- values.
Further reading.
Among the most prominent journals in educational psychology are:

</doc>
<doc id="10333" url="http://en.wikipedia.org/wiki?curid=10333" title="EFTPOS">
EFTPOS

EFTPOS (pronounced ) — electronic funds transfer at point of sale — is an electronic payment system involving electronic funds transfers based on the use of payment cards, such as debit or credit cards, at payment terminals located at points of sale. In Australia and New Zealand it is also the brand name of a specific system used for such payments. The Australian and New Zealand systems are country specific and do not interconnect. EFTPOS technology originated in the United States in 1981 and was quickly adopted by other countries.
Debit and credit cards are embossed plastic cards complying with ISO/IEC 7810 ID-1 standard. The cards have an embossed bank card number conforming with the ISO/IEC 7812 numbering standard.
History.
EFTPOS technology originated in the United States in 1981 and was rolled out in 1982. Initially, a number of nationwide systems were set up, such as "Interlink", which were limited to participating correspondent banking relationships, not being linked to each other. Consumers and merchants were slow to accept it, and there was minimal marketing. As a result, growth and market penetration of EFTPOS was minimal up to the turn of the century. Since 2002 the use of EFTPOS has grown significantly, and it has become the standard payment method, displacing the use of cash. Subsequently, networks facilitating the process of money transfer and payment settlement between the consumer and the merchant grew from a small number of nationwide systems to the majority of payment processing transactions. For EFTPOS, US based systems allow the use of debit cards or credit cards.
In a short time, other countries adopted the EFTPOS technology, but these systems too were limited to the national borders. Each country adopted various interbank co-operative models. In New Zealand, Bank of New Zealand started issuing EFTPOS debit cards in 1985 with the first merchant terminals being installed in petrol stations. In Australia, the major Australian banks started issuing debit or EFTPOS cards (each under a different brand name) starting in 1986 and merchants started installing EFTPOS terminals at the same time. Debit cards issued by all banks could be used at all EFTPOS terminals nationally, but debit cards issued in other countries could not. Prior to 1986, the Australian banks organized a widespread uniform credit card, called Bankcard, which had been in existence since 1974. There was a dispute between the banks whether Bankcard (or credit cards in general) should be permitted into the proposed EFTPOS system. At that time several banks were actively promoting MasterCard and Visa credit cards. Store cards and proprietary cards were shut out of the new system.
In recent years, MasterCard and Visa have introduced a debit card which is widely accepted internationally. International transactions are generally in the local currency, requiring a currency exchange by the card company to the currency of the primary account. Other charges may also apply.
Australia.
In Australia, debit and credit cards are the most common non-cash payment methods at “points of sale” (POS) or via ATMs. Not all merchants provide eftpos facilities, but those who wish to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent an EFTPOS terminal to the merchant. The eftpos system in Australia is managed by Eftpos Payments Australia Ltd, which also sets the EFTPOS interchange fee. For credit cards to be accepted by a merchant a separate agreement must be entered into with each credit card company, each of which has its own flexible merchant fee rate. 
The clearing arrangements for EFTPOS are regulated by Australian Payments Clearing Association (APCA). The system for ATM and EFTPOS interchanges is called Consumer Electronic Clearing System (CECS) also called CS3. CECS required authorisations from the Australian Competition and Consumer Commission (ACCC), which was obtained in 2001 and reaffirmed in 2009. ATM and EFTPOS clearings are the subject of individual bilateral arrangements between the institutions involved.
Debit cards.
Australian financial institutions provide their customers a plastic card, which can be used as a debit card or as an ATM card, and sometimes as a credit card. The card merely provides the means by which a customer's linked bank or other accounts can be accessed using an EFTPOS terminal or ATM. These cards can also be used on some vending machines and other automatic payment mechanisms, such as ticket vending machines. Australian debit cards cannot be used for online and telephone banking transactions, unless they are also a credit card.
Each Australian bank has given a different name to its debit cards, such as:
Some banks offer alternative debit card facilities to their customers using the Visa or MasterCard clearance system. For example, St George Bank offers a Visa Debit Card, as does the National Australia Bank. The main difference with regular debit cards is that these cards can be used outside Australia where the respective credit card is accepted.
Those merchants that enter the EFTPOS payment system must accept debit cards issued by any Australian bank, and some also accept various credit cards and other cards. Some merchants set minimum transaction amounts for EFTPOS transactions, which can be different for debit and credit card transactions. Some merchant impose a surcharge on the use of EFTPOS. These can vary between merchants and on the type of card being used, and generally are not imposed on debit card transactions, and widely not on MasterCard and a Visa credit card transactions.
A feature of a debit card is that an EFTPOS transaction will only be accepted if there is an available credit balance in the bank cheque or savings account linked to the card.
Australian debit cards normally cannot be used outside Australia. They can only be used outside Australia if they carry the MasterCard/Maestro/Cirrus or Visa/Plus or other similar logos, in which case the non-Australian transaction will be processed through those transaction systems. Similarly, non-Australian debit and credit cards can only be used at Australian EFTPOS terminals or ATMs if they have these logos or the MasterCard or Visa logos. Diners Club and/or American Express cards will be accepted only if the merchant has an agreement with those card companies. The Discover card is accepted in Australia as a Diners Club card .
In addition, credit card companies issue prepaid cards which act like generic gift cards, which are anonymous and not linked to any bank accounts. These cards are accepted by merchants who accept credit cards and are processed through the EFTPOS terminal in the same way as credit cards.
Cash out.
A number of merchants permit customers using a debit card to withdraw cash as part of the EFTPOS transaction. In Australia, this facility (known as debit card cashback in many other countries) is known as "cash out". For the merchant, cash out is a way of reducing their net cash takings, saving on banking of cash. There is no additional cost to the merchant in providing cash out because banks charge a merchant a debit card transaction fee per EFTPOS transaction, and not on the transaction value. Cash out is a facility provided by the merchant, and not the bank, so the merchant can limit or vary how much cash can be withdrawn at a time, or suspend the facility at any time. When available, cash out is convenient for the customer, who can bypass having to visit a bank branch or ATM. Cash out is also cheaper for the customer, since only one bank transaction is involved. For people in some remote areas, cash out may be the only way they can withdraw cash from their personal accounts. However, most merchants who provide the facility set a relatively low limit on cash out, generally $50, and some also charge for the service. Some merchants in Australia only allow cash out with the purchase of goods; other merchants allow cash out whether or not customers buy any goods. Cash out is not available in association with credit card sales because on credit card transactions the merchant is charged a percentage commission based on the transaction value, and also because cash withdrawals are treated differently from purchase transactions by the credit card company. (However, though inconsistent with a merchant's agreement with each credit card company, the merchant may treat a cash withdrawal as part of an ordinary credit card sale.)
Cardholder verification.
EFTPOS transactions involving a debit, credit or prepaid card can be authenticated in a number of ways. The oldest system requires the printing and signing of a receipt, with the merchant verifying the signature on the receipt against the signature on the card. It is not very common in Australia for a merchant to require a cardholder to provide photo identification. More recently, authentication was by a personal identification number (PIN) that was entered into a PIN pad at the terminal and transmitted to the bank for verification. However, financial institutions are in the process of rolling out smart cards with integrated circuits ("chips") that will enable verification of the PIN at the EFTPOS terminal. PIN management is governed by international standard ISO 9564. At ATMs, only PIN verification is available, and all new credit cards are now issued with PINs regardless of whether or not they have a chip. In 2014, banks have indicated that the option for a customer to sign receipts is to be phased out, and that only PIN authentication is to be available. For some merchants, transactions below a specific threshold value (up to $100) can be approved without authentication (either signature or PIN).
As a further security measure, if a user enters an incorrect PIN three times, the card may be locked out of EFTPOS and require reactivation over the phone or at a bank branch. In the case of an ATM, the card will not be returned, and the cardholder will need to visit the branch to retrieve the card, or request a new card to be issued.
All debit cards now have a magnetic stripe on which is encoded the card's service codes, consisting of three-digit values. These codes are used to convey instructions to merchant terminals on how a card should be processed. The first digit indicates if a card can be used internationally or is valid for domestic use only. It is also used to signal if the card is chip-enabled. The second digit indicates if the transaction must be sent online for authorization always or if transactions that are below floor limit can take place without authorization. The third digit is used to indicate the preferred card verification method (e.g. PIN) and the environment where the card can be used (e.g. at point of sale only). Merchant terminals are required to recognize and act on service codes or send all transactions for online authorization.
Contactless smart card.
In late 2000s, MasterCard and Visa introduced contactless smart debit cards under the brand names MasterCard PayPass and Visa payWave. These payments are made using electronic payment networks separate from the regular eftpos payment networks, and is an alternative to the previous swipe or chip systems. These networks are operated by MasterCard and Visa, and not by the banks as is the eftpos network, through eftpos Payments Australia Limited (ePAL).
These cards are based on EMV technology and contain a RFID chip and antenna loop embedded in the plastic of the card. To pay using this system, a customer passes the card within 4 cm of a reader at a merchant checkout. Using this method, for transactions under $100, the customer does not need to authenticate his or her identity by PIN entry or signature, as on a regular eftpos machine. For transactions over $100, PIN verification is required. 
The facility is only available for cards branded with the MasterCard PayPass or Visa payWave logos, indicating that they have the system-permitted embedded chip. ANZ has launched ATM solution based on Visa payWave in 2015, where the consumer tap the card on a reader at the ATM and insert a PIN to finalize the cash withdrawals, and not all merchants offer the facility. Bank debit cards and other credit cards do not currently offer a contactless payment facility. ePAL is developing a contactless payment system for debit cards based on EMV technology as well as an extension of debit cards for use for on-line transactions, and a mobile payment system.
History.
The name and logo for EFTPOS in Australia were originally owned by the National Australia Bank and were trade marks from 1986 until 1991. The major Australian banks started issuing debit or EFTPOS cards (each under a different brand name) starting in 1986 and merchants started installing EFTPOS terminals at the same time.
In April 2009, a company, “EFTPOS Payments Australia Ltd” (ePal) was formed to manage and promote the EFTPOS system in Australia. 
ePal regulation commenced in January 2011. The initial members of EFTPOS Payments Australia Ltd were:
In 2006 Commonwealth Bank and MasterCard ran a six-month trial of the contactless smart card system PayPass in Sydney and Wollongong, 
supplementing the traditional EFTPOS swipe or chip system. The system was rolled out across Australia in 2009; other systems being rolled out are Westpac Bank's MasterCard PayPass and Visa payWave branded cards.
In Australia, store cards have been excluded from participation in the EFTPOS and ATM systems. Consequently, several larger store accounts have entered into co-branding arrangements with credit card networks for the store-based accounts to be widely accepted. This was the case with Coles (previously, Coles-Myer) which co-branded with MasterCard, and David Jones which co-branded with American Express. Woolworths organized its credit card called Everyday Rewards (now Woolworths Money) which initially was partnered with credit provider HSBC Bank, but will change on 26 October 2014 to Macquarie Bank.
Usage.
As of December 2010, there were over 707,000 EFTPOS terminals in Australia and over 28,000 ATMs. Of the terminals, over 60,000 offered cash withdrawals. In 2010, 183 million transactions, worth A$12 billion, were made using Australian EFTPOS terminals per month.
In 2011, these figures increased to 750,000 terminals, with 325,000 individual businesses, processing over 2 billion transactions with combined value of approximately $131 billion for the year.
Network.
The EFT network in Australia is made up of seven proprietary networks in which peers have interchange agreements, making an effective single network. A merchant who wishes to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent the terminal to the merchant. All the merchant's EFTPOS transactions are processed through one of these gateways. Some of these peers are:
Other organisations may have peering agreements with the one or more of the central peers.
The network uses the AS 2805 protocol.
New Zealand.
Eftpos is highly popular in New Zealand. The system is operated by two providers, Paymark Limited (formerly Electronic Transaction Services Limited) which processes 75% of all electronic transactions in New Zealand, and EFTPOS NZ. Although the term eftpos is popularly used to describe the system, EFTPOS is a trademark of EFTPOS NZ the smaller of the two providers. Both providers run an interconnected financial network that allows the processing of not only of debit cards at point of sale terminals but also credit cards and charge cards.
History.
The Bank of New Zealand introduced EFTPOS to New Zealand in 1985 through a pilot scheme with petrol stations.
In 1989 the system was officially launched and two providers owned by the major banks now run the system. The largest of the two providers, Paymark Limited (formerly Electronic Transaction Services Limited) is owned equally by ASB Bank, Westpac, Bank of New Zealand and ANZ Bank New Zealand (formerly ANZ National Bank). The second is operated by EFTPOS NZ which is fully owned by VeriFone Systems, following its sale by ANZ New Zealand in December 2012.
During July 2006 the five billionth EFTPOS payment was processed, and at the start of 2012 the 10 billionth transaction was processed.
Usage.
EFTPOS is highly popular in New Zealand, and being used for about 60% of all retail transactions.In 2009, there were 200 EFTPOS transactions per person.
Paymark process over 900 million transactions (worth over NZ$48 billion) yearly. More than 75,000 merchants and over 110,000 EFTPOS terminals are connected to Paymark.

</doc>
<doc id="10334" url="http://en.wikipedia.org/wiki?curid=10334" title="Epistle to the Laodiceans">
Epistle to the Laodiceans

The Epistle to the Laodiceans is a possible lost letter of Paul, the original existence of which is inferred from an instruction to the church in Colossae to send their letter to the church in Laodicea, and likewise obtain a copy of the letter "from Laodicea" (Greek "ek laodikeas" ἐκ Λαοδικείας).
And when this letter has been read to you, see that it is also read before the church at Laodicea, and that you yourselves read the letter which will be forwarded from there.— Colossians 4:16 OEB
Several ancient texts purporting to be the missing "Epistle to the Laodiceans" have been known to have existed, most of which are now lost. These were generally considered, both at the time and by modern scholarship, to be attempts to supply a forged copy of a lost document. The exception is a Latin "Epistle to the Laodiceans", which is actually a short compilation of verses from other Pauline epistles, principally Philippians, and on which scholarly opinion is divided as to whether it is the lost Marcionite forgery or alternatively an orthodox replacement of the Marcionite text. In either case it is generally considered a "clumsy forgery" and an attempt to seek to fill the "gap" suggested by Colossians 4:16.
Some ancient sources, such as Hippolytus, and some modern scholars consider that the epistle "from Laodicea" was never a lost epistle, but simply Paul recycling one of his other letters (the most common candidate is the contemporary Letter to the Ephesians), just as he asks for the copying and forwarding of the Letter to Colossians to Laodicea.
The Colossians 4:16 mention.
Paul, the earliest known Christian author, wrote several letters (or epistles) in Greek to various churches. Paul apparently dictated all his epistles through a secretary (or amanuensis), but wrote the final few paragraphs of each letter by his own hand. Many survived and are included in the New Testament, but others are known to have been lost. The Epistle to the Colossians states "After this letter has been read to you, see that it is also read in the church of the Laodiceans and that you in turn read the letter from Laodicea." The last words can be interpreted as "letter written to the Laodiceans", but also "letter written from Laodicea". The New American Standard Bible (NASB) translates this verse in the latter manner, and translations in other languages such as the Dutch Statenvertaling translate it likewise: "When this letter is read among you, have it also read in the church of the Laodiceans; and you, for your part read my letter (that is coming) from Laodicea." Those who read here "letter written to the Laodiceans" presume that, at the time that the Epistle to the Colossians was written, Paul also had written an epistle to the Laodicean Church.
Identification with the Epistle to the Ephesians.
Some scholars have suggested that this refers to the canonical "Epistle to the Ephesians", contending that it was a circular letter (an "encyclical") to be read to many churches in the Laodicean area. Others dispute this view.
The Marcionist "Epistle to the Laodiceans".
According to the Muratorian fragment, Marcion's canon contained an epistle entitled "Epistle to the Laodiceans" which is commonly thought to be a forgery written to conform to his own point of view. This is not at all clear, however, since none of the text survives. It is not known what this letter might have contained. Some scholars suggest it may have been the Vulgate epistle described below, while others believe it must have been more explicitly Marcionist in its outlook. Others believe it to be the Epistle to the Ephesians.
The Latin Vulgate "Epistle to the Laodiceans".
For centuries some Western Latin Bibles used to contain a small Epistle from Paul to the Laodiceans. The oldest known Bible copy of this epistle is in a Fulda manuscript written for Victor of Capua in 546. It is mentioned by various writers from the fourth century onwards, notably by Pope Gregory the Great, to whose influence may ultimately be due the frequent occurrence of it in Bibles written in England; for it is commoner in English Bibles than in others. John Wycliffe included Paul's letter to the Laodiceans in his Bible translation from the Latin to English. However this epistle is not without controversy because there is no evidence of a Greek text. It contains almost no doctrine, teachings, or narrative not found elsewhere, and its exclusion from the Biblical canon has little effect.
The text was almost unanimously considered pseudepigraphal when the Christian Biblical canon was decided upon, and does not appear in any Greek copies of the Bible at all, nor is it known in Syriac or other versions. Jerome, who wrote the Latin Vulgate translation, wrote in the 4th century, "it is rejected by everyone". However, it evidently gained a certain degree of respect. It appeared in over 100 surviving early Latin copies of the Bible. According to "Biblia Sacra iuxta vulgatum versionem", there are Latin Vulgate manuscripts containing this epistle dating between the 6th and 12th century, including Latin manuscripts F (Codex Fuldensis), M, Q, B, D (Ardmachanus), C, and Lambda.
The apocryphal epistle is generally considered a transparent attempt to supply this supposed lost sacred document. Some scholars suggest that it was created to offset the popularity of the Marcionite epistle.
Wilhelm Schneemelcher's standard work, "New Testament Apocrypha" (Chapter 14 "Apostolic Pseudepigrapha") includes a section on the Latin Epistle to the Laodiceans and a translation of the Latin text.

</doc>
<doc id="10335" url="http://en.wikipedia.org/wiki?curid=10335" title="Extermination camp">
Extermination camp

German Extermination Camps or Death Camps were designed and built by Nazi Germany during World War II (1939–45) to systematically kill millions, primarily by gassing, but also by execution and extreme work under starvation conditions.
The idea of mass extermination with the use of stationary facilities built exclusively for that purpose was a result of earlier Nazi experimentation with the chemically manufactured poison gas during the secretive Action T4 euthanasia programme against mental patients. It was adapted, expanded and applied to victims from many ethnic and national groups; the Jews however became the most numerous Nazi targets. This genocide of the Jewish people was the Third Reich's "Final Solution to the Jewish question". The Nazi attempts at Jewish genocide are now collectively known as the Holocaust.
Although not responsible for an extermination camp death count of millions like that the Nazis perpetrated, the fascist Ustaše forces of the Independent State of Croatia operated extermination camps culminating in a six figure death toll.
Background.
The top secret Action T4 euthanasia programme was initiated by the "SS" in 1939 to exterminate "life unworthy of life" (in German: "Lebensunwertes Leben") a Nazi designation for the segments of populace which had no right to life. The experience gained led to creation of extermination camps. Following the invasion of Poland in September 1939 the Jews along with other targeted groups were confined to new ghettos and interned in Nazi concentration camps. However, in 1941 the Nazis embarked on "die Endlösung der Judenfrage" (The Final Solution of the Jewish Question) by the systematic killing of Europe's Jews. This adoption of the Action T4 programme by Nazi leaders during the first half of 1941, was derived from the then accepted ideas of racial hygiene and racial science. The initial, formal killings of the Final Solution were undertaken by the SS "Einsatzgruppen" (Task Forces) death squads who followed the Wehrmacht during the "Operation Barbarossa" invasion of the USSR in June 1941.
When the Nazis began to establish separate camps specifically for mass extermination, this was not a coordinated strategy. That changed with the Wannsee Conference chaired by Reinhard Heydrich in January 1942 in which the principle was made clear that the Jews of Europe were to be exterminated. Responsibility for the logistics were to be executed by the administrator, Adolf Eichmann.
In 1942, the "Reichsführer" Heinrich Himmler ordered the Lublin District "SS- und Polizeiführer" Odilo Globocnik to build the first extermination camps during "The Final Solution" (1941–43), the operation to annihilate every Jew in the General Government (occupied Poland).
Definition.
The Nazi perpetrators distinguished between extermination camps and concentration camps, although the terms "extermination camp" ("Vernichtungslager") and "death camp" ("Todeslager") are interchangeable, each referring to camps whose primary function was genocide. They weren't used for punishing crime or containing political prisoners, but for the systematic killing of the prisoners delivered there en masse by the Holocaust trains. The executioners did not expect the majority of prisoners taken to the Belzec, Sobibór or Treblinka extermination camps to survive more than a few hours beyond arrival. As early as September 1942, Dr. Johann Paul Kremer, M.D., an SS physician, witnessed a gassing of prisoners, and in his diary wrote: "They don't call Auschwitz the camp of annihilation ["das Lager der Vernichtung"] for nothing!" The first extermination camps were under Globocnik's direct command and operated by SS Police battalions and Trawnikis – volunteers from Eastern Europe.
These differed from concentration camps, such as Dachau and Belsen, which were initially prison camps for people defined as socially or politically undesirable in Nazi society. 
The "SS-Totenkopfverbände" managed the Nazi concentration camps such as Dachau and Ravensbrück.
The distinction was evident during the Nuremberg trials, when Dieter Wisliceny (a deputy to Adolf Eichmann) was asked to name the "extermination" camps, and he identified Auschwitz and Majdanek as such. Then, when asked "How do you classify the camps Mauthausen, Dachau, and Buchenwald?" he replied, "They were normal concentration camps, from the point of view of the department of Eichmann."
Extermination camps are distinguished from the "Arbeitslager" (forced labor camps) established in German-occupied countries to use the prisoners, including prisoners of war, as slave labor. In most camps (excepting PoW camps for the non-Soviet soldiers and certain labor camps), the high death rates resulted from execution, starvation, disease, exhaustion, and physical brutality.
Death camps.
In the early years of the Holocaust, the Jews were primarily sent to concentration camps, but from 1942 onwards they were mostly deported to the extermination camps. For political and logistical reasons, the most infamous extermination camps Nazi Germany built were in Occupied Poland, where most of the intended victims lived; Poland had the greatest European Jewish populace. On top of that, the camps outside of the Third Reich proper could be kept secret from the German civil populace.
Pure extermination camps.
Operationally, there were two types of death camps. Initially, gas vans producing poisonous carbon monoxide exhaust fumes were developed in occupied Soviet Union and at the Chełmno extermination camp in occupied Poland, before being used elsewhere.
The camps at Treblinka, Bełżec, and Sobibór were constructed during Operation Reinhard (October 1941 – November 1943), for the extermination of Poland's Jews. Prisoners were promptly killed upon arrival. Initially, the camps used carbon monoxide gas chambers; at first, the corpses were buried, but then incinerated atop pyres. Later, gas chambers and crematoria were built in Treblinka and Belzec; Zyklon-B was used in Belzec.
Whereas the Auschwitz II (Auschwitz–Birkenau) and Majdanek camps were parts of a labor camp complex, the Operation Reinhard camps and the Chełmno camp were exclusively for the quick extermination of many people (primarily Jews) within hours of their arrival. Some able-bodied prisoners delivered to the death camp were not immediately killed, but were forced into labor units ("Sonderkommando") to work at the extermination process, removing corpses from the gas chambers and burning them. Because the extermination camps were physically small (only several hundred metres long and wide) and equipped with minimal housing and support installations, the Nazis deceived the prisoners upon their arrival, telling them that they were at a temporary transit stop, and soon would continue to an "Arbeitslager" (work camp) farther east.
Concentration and extermination camps.
At the camps of Operation Reinhard including Bełżec, Sobibór, and Treblinka trainloads of prisoners were destined for immediate death in gas chambers built for that purpose. The mass killing facilities were developed at the Majdanek concentration camp, and at Auschwitz II-Birkenau at about the same time. In most other camps prisoners were selected for slave labor first; they were kept alive on starvation rations and made available to work wherever the rulers required. Auschwitz, Majdanek, and Jasenovac were retrofitted with Zyklon-B gas chambers and crematoria as the time went on, remaining operational until war's end in 1945. The Maly Trostenets extermination camp in the USSR initially operated as a prison camp. It became an extermination camp later in the war with victims undergoing mass shootings. This was supplemented with exhaust fume gassing in a van from October 1943.
The Sajmište concentration camp operated by the Nazis in Yugoslavia had a gas van stationed for use from March to June 1942. Once the industrial killings were completed, the van was returned to Berlin. After a refit the van was then sent to Maly Trostinets for use at the camp there. The Janowska concentration camp near Lwow in occupied eastern Poland implemented a selection process. Some prisoners were assigned to work before death. Others were either transported to Belzec or victims of mass shootings on two slopes in the Piaski sand-hills behind the camp. The Warsaw concentration camp was a camp complex of the German concentration camps, possibly including an extermination camp located in German-occupied Warsaw. The various details regarding the camp are very controversial and remain subject of historical research and public debate.
Other means of extermination.
With the support of Nazi Germany and fascist Italy on 10 April 1941 the Independent State of Croatia (NDH) was established, and adopted parallel racial and political doctrines. Death camps were established by the fascist Ustaše government for contributing to the Nazi "final solution" to the "Jewish problem", the killing of Roma people, and the elimination of political opponents, but most significantly to achieve the destruction of the Serbian population of the NDH. The degree of cruelty with which the Serb population was persecuted by Ustaše men shocked even the Germans.
The Jadovno concentration camp was located in a secluded area about 20 km from the town of Gospić. It held thousands of Serbs and Jews over a period of 122 days from May to August 1941. Prisoners were usually but by no means exclusively killed by being pushed into deep ravines located near the camp.
The Jasenovac concentration camp complex of five sub-camps replaced Jadovno. Many inmates arriving at Jasenovac were scheduled for systematic extermination. An important criterion for selection was the duration of a prisoner's anticipated detention. Strong men capable of labour and sentenced to less than three years of incarceration were allowed to live. All inmates with indeterminate sentences or sentences of three years or more were immediately scheduled for execution, regardless of their fitness. Some of the mass executions were mechanical following Nazi methodology. Others were performed manually utilising tools such as mallets and agricultural knives and often in conjunction with throwing the victims off the end of a ramp into the River Sava.
Robert Conquest argues that the regime in labour camps in the Soviet Union, principally those in Siberia, was designed to bring about the death of prisoners after extracting 3–6 months' labour from them. Aleksandr Solzhenitsyn concurs with him.
Extermination procedure.
Heinrich Himmler visited the outskirts of Minsk in 1941 to witness mass shooting. He was told by the commanding officer there that the shootings were proving psychologically damaging to those being asked to pull the triggers. Thus Himmler knew another method of mass killing was required. After the war, the diary of the Auschwitz Commandant, Rudolf Höss, revealed that psychologically "unable to endure wading through blood any longer", many "Einsatzkommandos" – the killers – either went mad or killed themselves.
The Nazis had first used gassing with carbon monoxide cylinders to kill 70,000 disabled people in Germany in what they called a 'euthanasia programme' to disguise that mass murder was taking place. Despite the lethal effects of carbon monoxide, this was seen as unsuitable for use in the East due to the cost of transporting the carbon monoxide in cylinders.
Each extermination camp operated differently, yet each had designs for quick and efficient industrialized killing. While Höss was away on an official journey in late August 1941 his deputy, Karl Fritzsch, tested out an idea. At Auschwitz clothes infected with lice were treated with crystallised prussic acid. The crystals were made to order by the IG Farben chemicals company for which the brand name was Zyklon-B. Once released from their container, Zyklon-B crystals in the air released a lethal cyanide gas. Fritzch tried out the effect of Zyklon B on Soviet POWs, who were locked up in cells in the basement of the bunker for this experiment. Höss on his return was briefed and impressed with the results and this became the camp strategy for extermination as it was also to be at Majdanek. Besides gassing, the camp guards continued killing prisoners via mass shooting, starvation, torture, etc.
Gassings.
SS "Obersturmführer" Kurt Gerstein, of the Institute for Hygiene of the Waffen-SS, during the war told a Swedish diplomat of life in a death camp, of how, on 19 August 1942, he arrived at Belzec extermination camp (which was equipped with carbon monoxide gas chambers) and was shown the unloading of 45 train cars filled with 6,700 Jews, many already dead, but the rest were marched naked to the gas chambers, where:
"Unterscharführer" Hackenholt was making great efforts to get the engine running. But it doesn't go. Captain Wirth comes up. I can see he is afraid, because I am present at a disaster. Yes, I see it all and I wait. My stopwatch showed it all, 50 minutes, 70 minutes, and the diesel [engine] did not start. The people wait inside the gas chambers. In vain. They can be heard weeping, "like in the synagogue", says Professor Pfannenstiel, his eyes glued to a window in the wooden door. Furious, Captain Wirth lashes the Ukrainian [prisoner] assisting Hackenholt twelve, thirteen times, in the face. After 2 hours and 49 minutes – the stopwatch recorded it all – the diesel started. Up to that moment, the people shut up in those four crowded chambers were still alive, four times 750 persons, in four times 45 cubic meters. Another 25 minutes elapsed. Many were already dead, that could be seen through the small window, because an electric lamp inside lit up the chamber for a few moments. After 28 minutes, only a few were still alive. Finally, after 32 minutes, all were dead ... Dentists [then] hammered out gold teeth, bridges, and crowns. In the midst of them stood Captain Wirth. He was in his element, and, showing me a large can full of teeth, he said: "See, for yourself, the weight of that gold! It's only from yesterday, and the day before. You can't imagine what we find every day – dollars, diamonds, gold. You'll see for yourself!" 
Auschwitz Camp Commandant Rudolf Höss reported that the first time Zyklon B gas was used on the Jews, many suspected they were to be killed – despite having been deceived into believing they were to be deloused and then returned to the camp. As a result, the Nazis identified and isolated "difficult individuals" who might alert the prisoners, and removed them from the mass – lest they incite revolt among the deceived majority of prisoners en route to the gas chambers. The "difficult" prisoners were led to a site out of view to be killed off discreetly.
A prisoner "Sonderkommando" (Special Detachment) effected the most of the processes of extermination; they accompanied the Jews into the gas chamber (a chamber room, usually outfitted to appear as a large shower room, with (nonworking) water nozzles, tile walls, etc.) and remained with them until just before the chamber door closed. To psychologically maintain the "calming effect" of the delousing deception, an SS guard stood at the door, as if awaiting the prisoners. The "Sonderkommando" hurried them to undress and enter the "shower room" as quickly as possible; to that effect, they also assisted the aged and the very young in undressing.
To further persuade the prisoners that nothing harmful was happening, the "Sonderkommando" deceived them with small talk about camp life. Fearing that the delousing "disinfectant" might harm their children, many mothers hid their infants beneath their piled clothes. Camp Commandant Höss reported that the "men of the Special Detachment were particularly on the look-out for this", and encouraged the women to take their children into the "shower room". Likewise, the "Sonderkommando" comforted older children who might cry "because of the strangeness of being undressed in this fashion".
Yet, not every prisoner was deceived by such psychological warfare tactics; Commandant Höss reported of Jews "who either guessed, or knew, what awaited them, nevertheless ... [they] found the courage to joke with the children, to encourage them, despite the mortal terror visible in their own eyes". Some women would suddenly "give the most terrible shrieks while undressing, or tear their hair, or scream like maniacs"; the "Sonderkommando" immediately took them away for execution by shooting. In such circumstances, others, meaning to save themselves at the gas chamber's threshold, betrayed the identities and "revealed the addresses of those members of their race still in hiding".
Once the door of the filled gas chamber was sealed, pellets of Zyklon B were dropped through special holes in the roof. Regulations required that the Camp Commandant supervise the preparations, the gassing (through a peephole), and the aftermath looting of the corpses. Commandant Höss reported that the gassed victims "showed no signs of convulsion"; the Auschwitz camp physicians attributed that to the "paralyzing effect on the lungs" of the Zyklon-B gas, which killed "before" the victim began suffering convulsions.
As a matter of political training, some high-ranked Nazi Party leaders and SS officers were sent to Auschwitz–Birkenau to witness the gassings; Höss reported that "all were deeply impressed by what they saw ... [yet some] ... who had previously spoken most loudly, about the necessity for this extermination, fell silent once they had actually seen the 'final solution of the Jewish problem'." As the Auschwitz Camp Commandant Rudolf Höss justified the extermination by explaining the need for "the iron determination with which we must carry out Hitler's orders"; yet saw that even "[Adolf] Eichmann, who certainly [was] tough enough, had no wish to change places with me."
Corpse disposal.
After the gassings, the "Sonderkommando" removed the corpses from the gas chambers, then extracted any gold teeth. Initially, the victims were buried in mass graves, but were later cremated during "Sonderaktion 1005" in all camps of Operation Reinhard.
The "Sonderkommando" were responsible for burning the corpses in the pits, stoking the fires, draining surplus body fat and turning over the "mountain of burning corpses... so that the draught might fan the flames" wrote Commandant Höss in his memoir while in the Polish custody. He was impressed by the diligence of prisoners from the so-called Special Detachment who carried out their duties despite their being well aware that they, too, would meet exactly the same fate in the end. At the Lazaret killing station they held the sick so they would never see the gun while being shot. They did it "in such a matter-of-course manner that they might, themselves, have been the exterminators" wrote Höss. He further said that the men ate and smoked "even when engaged in the grisly job of burning corpses which had been lying for some time in mass graves." They occasionally encountered the corpse of a relative, or saw them entering the gas chambers. According to Höss they were obviously shaken by this but "it never led to any incident." He mentioned the case of a "Sonderkommando" who found the body of his wife, yet continued to drag corpses along "as though nothing had happened."
At Auschwitz, the corpses were incinerated in crematoria and the ashes either buried, scattered, or dumped in the river. At Sobibór, Treblinka, Belzec, and Chełmno, the corpses were incinerated on pyres. The efficiency of industrialised killing at Auschwitz-Birkenau led to the construction of three buildings with crematoria designed by specialists from Topf und Söhne. They handled the body disposal around the clock, day and night, and yet the speed of gassing required that some corpses burn in an open air pit also.<ref name="B/G-199"></ref>
Ustaše camps.
The United States Holocaust Memorial Museum (USHMM) in Washington, DC, presently estimates that the Ustaša regime in Croatia murdered between 77,000 and 99,000 people at their own Jasenovac concentration camp between 1941 and 1945. The Jasenovac Memorial Site quotes a similar figure of between 80,000 and 100,000 victims. The television documentary, "Nazi Collaborators" on Dinko Sakic stated that over 300,000 people were killed at Jasenovac. The Jasenovac mechanical means of mass killing included the use initially of gas vans and later Zyklon B in stationary gas chambers. The Jasenovac guards have also been reported to have cremated living inmates in the crematorium. A notable difference of the Ustaše camps as compared to the German "SS" camps was the widespread use of manual methods in the mass killings. These involved instruments such as mallets and agricultural knives which evolved to often to be done in a manner where still alive victims were thrown off the end of a ramp into the River Sava.
The estimates for the Jadovno concentration camp generally offer a range of 10,000 – 72,000 deaths at the camp over a period of 122 days (May to August 1941). Most commonly Jadovno victims were bound together in a line and the first few victims were murdered with rifle butts or other objects. Afterwards, an entire row of inmates were pushed into the ravine. Hand grenades were hurled inside in order to finish off the victims. Dogs would also be thrown in to feed on the wounded and the dead. Inmates were also killed by machine gunfire, as well as with knives and blunt objects.
Death toll.
The estimated total number of people executed in the Nazi camps in the table below is over three million:
Dismantlement and attempted concealment.
The Nazis attempted to either partly or completely dismantle the extermination camps in order to hide any evidence that people had been murdered. This was an attempt to conceal not only the extermination process but also the buried remains. As a result of the secretive Sonderaktion 1005, camps were dismantled by commandos of condemned prisoners, records destroyed, and mass graves were dug up. Some extermination camps that remained uncleared of evidence were liberated by Soviet troops, who had different standards of documentation and openness than the Western allies.
The notable exception in this is Majdanek. Majdanek was ordered to share a similar fate to the other camps by the Nazi leadership in their attempt to cover up the murderous events. Majdanek though was captured nearly intact. This was due to the rapid advance of the Soviet Red Army during Operation Bagration preventing the SS from destroying most of its infrastructure. Commandant Anton Thernes ineptly failed in his task of removing incriminating evidence of war crimes.
Commemoration.
In the post-war period the government of the People's Republic of Poland created monuments at the extermination camp sites. These early monuments mentioned no ethnic, religious, or national particulars of the Nazi victims. The extermination camps sites have been accessible to everyone in recent decades. They are popular destinations for visitors from all over the world, especially the most-infamous Nazi death camp, Auschwitz near the town of Oświęcim. In the early 1990s, the Jewish Holocaust organisations debated with the Polish Catholic groups about "What religious symbols of martyrdom are appropriate as memorials in a Nazi death camp such as Auschwitz?" The Jews opposed the placement of Christian memorials such as the Auschwitz cross near Auschwitz I where mostly Poles were killed. The Jewish victims of the Holocaust were mostly killed at Auschwitz II Birkenau.
The March of the Living is organized in Poland annually since 1988. Marchers come from countries as diverse as Estonia, New Zealand, Panama, and Turkey.
The camps and Holocaust denial.
Holocaust deniers are people and organisations who assert that the Holocaust did not occur, or that it did not occur in the historically recognized manner and extent.
Extermination camp research is difficult because of extensive attempts by the SS and Nazi regime to conceal the existence of the extermination camps. The existence of the extermination camps is firmly established by testimonies of camp survivors and Final Solution perpetrators, material evidence (the remaining camps, etc.), Nazi photographs and films of the killings, and camp administration records.
Holocaust deniers often start by pointing out legitimate public misconceptions about the extermination camps. For example, widely published images in America were mostly of typhoid victims and Soviet POWs at the Buchenwald and Dachau concentration camps – the first to be liberated by American troops and the most available imagery in America. In early news reports and for years afterwards these images were often used by the news media somewhat inaccurately in conjunction with descriptions of extermination camps and Jewish suffering. Holocaust deniers, after pointing out such common errors, put it forward as evidence that extermination camps did not exist and the limited evidence about them is mostly a hoax arising out of a deliberate Jewish conspiracy.
Holocaust denial is highly discredited by scholars and is a criminal offence in Austria, Belgium, France, Germany, Lithuania, the Netherlands, Poland, and Switzerland.

</doc>
<doc id="10336" url="http://en.wikipedia.org/wiki?curid=10336" title="Enterprise">
Enterprise

Enterprise (occasionally used with the archaic spelling Enterprize) may refer to:

</doc>
<doc id="10338" url="http://en.wikipedia.org/wiki?curid=10338" title="Excommunication">
Excommunication

Excommunication is an institutional act of religious censure used to deprive, suspend, or limit membership in a religious community or to restrict certain rights within it, in particular reception of the sacraments. Some Protestants use the term disfellowship instead.
The word "excommunication" means putting a specific individual or group out of communion. In some religions, excommunication includes spiritual condemnation of the member or group. Excommunication may involve banishment, shunning, and shaming, depending on the religion, the offense that caused excommunication, or the rules or norms of the religious community. The grave act is often revoked in response to sincere penance, which may be manifested through public recantation, sometimes through the Sacrament of Confession, piety, and/or through mortification of the flesh.
Christianity.
In Jesus says that an offended person should first draw the offender's fault to the offender's attention privately; then, if the offender refuses to listen, bring one or two others, that there may be more than a single witness to the charge; next, if the offender still refuses to listen, bring the matter before the church, and if the offender refuses to listen to the church, treat the offender as "a Gentile and a tax collector".
 directs the church at Corinth to excommunicate a man for sexual immorality (incest). In , the man, having repented and suffered the "punishment by the majority" is restored to the church. Fornication is not the only ground for excommunication, according to the apostle: in , Paul says, "I am writing to you not to associate with anyone who bears the name of brother if he is guilty of sexual immorality or greed, or is an idolater, reviler, drunkard, or swindler - not even to eat with such a one."
In , Paul writes to "mark those who cause divisions contrary to the doctrine which ye have learned and avoid them." Also, in , the writer advises believers that "whosoever transgresseth, and abideth not in the doctrine of Christ, hath not God. He that abideth in the doctrine of Christ, he hath both the Father and the Son. If there come any unto you, and bring not this doctrine, receive him not into "your" house [οἰκίαν, residence or abode, or "inmates of the house" (family)], neither bid him God speed: for he that biddeth him God speed is partaker of his evil deeds."
Catholic Church.
Within the Catholic Church, there are differences between the discipline of the majority Latin Church regarding excommunication and that of the Eastern Catholic Churches.
Latin Church.
In Catholic canon law, excommunication is a rarely applied censure and thus a "medicinal penalty" intended to invite the person to change behaviour or attitude, repent, and return to full communion. It is not an "expiatory penalty" designed to make satisfaction for the wrong done, much less a "vindictive penalty" designed solely to punish: "excommunication, which is the gravest penalty of all and the most frequent, is always medicinal", and is "not at all vindictive".
Excommunication can be either "latae sententiae" (automatic, incurred at the moment of committing the offense for which canon law imposes that penalty) or "ferendae sententiae" (incurred only when imposed by a legitimate superior or declared as the sentence of an ecclesiastical court).
The Code of Canon Law of 1917 stated that excommunication excluded a person from the communion of the faithful. The 1983 revision of the Code of Canon Law removed this statement from the account of the effects of excommunication in the Catholic Church. Excommunicated persons are "cut off from the Church", barred from receiving the Eucharist and from taking an active part in the liturgy (reading, bringing the offerings, etc.), but they remain Catholics. They are urged to retain a relationship with the Church, as the goal is to encourage them to repent and return to active participation in its life.
All excommunicated persons are barred from participating in the liturgy in a ministerial capacity (e.g., as a reader if a layperson or as a deacon or priest if a clergyman) and from receiving the Eucharist or other sacraments, but they are not barred from attending these (i.e., an excommunicated person may not receive the Eucharist but is not barred from attending Mass). They are also forbidden to exercise any ecclesiastical office or the like.
These are the only effects for those who have incurred a "latae sententiae" excommunication. For instance, a priest may not refuse Communion publicly to those who are under an automatic excommunication, as long as it has not been officially declared to have been incurred by them, even if the priest knows that they have incurred it. On the other hand, if the priest knows that excommunication has been imposed on someone or that an automatic excommunication has been declared (and is no longer merely an undeclared automatic excommunication), he is forbidden to administer Holy Communion to that person. (see canon 915).
Other effects of an excommunication that has been imposed or declared are:
In the Catholic Church, excommunication is normally resolved by a declaration of repentance, profession of the Creed (if the offense involved heresy), or renewal of obedience (if that was a relevant part of the offending act) by the excommunicated person and the lifting of the censure (absolution) by a priest or bishop empowered to do this. "The absolution can be in the internal (private) forum only, or also in the external (public) forum, depending on whether scandal would be given if a person were privately absolved and yet publicly considered unrepentant." Since excommunication excludes from reception of the sacraments, absolution from excommunication is required before absolution can be given from the sin that led to the censure. In many cases, the whole process takes place on a single occasion in the privacy of the confessional. For some more serious wrongdoings, absolution from excommunication is reserved to a bishop, another ordinary, or even the Pope. These can delegate a priest to act on their behalf.
Before the 1983 Code of Canon Law, there were two degrees of excommunication: The excommunicate was either a "vitandus" (shunned, literally "to be avoided" by other Catholics), or a "toleratus" (tolerated, allowing Catholics to continue to have business and social relationships with the excommunicated person). This distinction no longer applies.
In the Middle Ages, formal acts of public excommunication were sometimes accompanied by a ceremony wherein a bell was tolled (as for the dead), the Book of the Gospels was closed, and a candle snuffed out — hence the idiom "to condemn with bell, book, and candle." Such ceremonies are not held today.
Interdict is a censure similar to excommunication. It too excludes from ministerial functions in public worship and from reception of the sacraments, but not from the exercise of governance.
Eastern Catholic Churches.
In the Eastern Catholic Churches, excommunications is imposed only by decree, never incurred automatically by "latae sententiae" excommunication.
A distinction is made between minor and major excommunication.
Those on whom minor excommunication has been imposed are excluded from receiving the Eucharist and can also be excluded from participating in the Divine Liturgy. They can even be excluded from entering a church when divine worship is being celebrated there. The decree of excommunication must indicate the precise effect of the excommunication and, if required, its duration.
Those under major excommunication are in addition forbidden to receive not only the Eucharist but also the other sacraments, to administer sacraments or sacramentals, to exercise any ecclesiastical offices, ministries, or functions whatsoever, and any such exercise by them is null and void. They are to be removed from participation in the Divine Liturgy and any public celebrations of divine worship. They are forbidden to make use of any privileges granted to them and cannot be given any dignity, office, ministry, or function in the Church, they cannot receive any pension or emoluments associated with these dignities etc., and they are deprived of the right to vote or to be elected.
Eastern Orthodox churches.
In the Eastern Orthodox churches, excommunication is the exclusion of a member from the Eucharist. It is not expulsion from the churches. This can happen for such reasons as not having confessed within that year; excommunication can also be imposed as part of a penitential period. It is generally done with the goal of restoring the member to full communion. Before an excommunication of significant duration is imposed, the bishop is usually consulted. The Orthodox churches do have a means of expulsion, by pronouncing anathema, but this is reserved only for acts of serious and unrepentant heresy. As an example of this, the Second Council of Constantinople in 553, in its eleventh capitula, declared: "If anyone does not anathematize Arius, Eunomius, Macedonius, Apollinarius Nestorius, Eutyches and Origen, as well as their heretical books, and also all other heretics who have already been condemned and anathematized by the holy, catholic and apostolic church and by the four holy synods which have already been mentioned, and also all those who have thought or now think in the same way as the aforesaid heretics and who persist in their error even to death: let him be anathema."
Lutheranism.
Although Lutheranism technically has an excommunication process, some denominations and congregations do not use it. The Lutheran definition, in its earliest and most technical form, would be found in Martin Luther's Small Catechism, defined beginning at Questions No. 277-283, in "The Office of Keys." Luther endeavored to follow the process that Jesus laid out in the 18th chapter of the Gospel of Matthew. According to Luther, excommunication requires:
Beyond this, there is little agreement. Many Lutheran denominations operate under the premise that the entire congregation (as opposed to the pastor alone) must take appropriate steps for excommunication, and there are not always precise rules, to the point where individual congregations often set out rules for excommunicating laymen (as opposed to clergy). For example, churches may sometimes require that a vote must be taken at Sunday services; some congregations require that this vote be unanimous.
The Lutheran process, though rarely used, has created unusual situations in recent years due to its somewhat democratic excommunication process. One example was an effort to get serial killer Dennis Rader excommunicated from his denomination (the Evangelical Lutheran Church in America) by individuals who tried to "lobby" Rader's fellow church members into voting for his excommunication.
Anglican Communion.
Church of England.
The Church of England does not have any specific canons regarding how or why a member can be excommunicated, although it has a canon according to which ecclesiastical burial may be refused to someone "declared excommunicate for some grievous and notorious crime and no man to testify to his repentance".
Episcopal Church of the United States of America.
The ECUSA is in the Anglican Communion, and shares many canons with the Church of England which would determine its policy on excommunication.
Reformed view.
In the Reformed churches, excommunication has generally been seen as the culmination of church discipline, which is one of the three marks of the Church. The Westminster Confession of Faith sees it as the third step after "admonition" and "suspension from the sacrament of the Lord's Supper for a season." Yet, John Calvin argues in his "Institutes of the Christian Religion" that church censures do not "consign those who are excommunicated to perpetual ruin and damnation," but are designed to induce repentance, reconciliation and restoration to communion. Calvin notes, "though ecclesiastical discipline does not allow us to be on familiar and intimate terms with excommunicated persons, still we ought to strive by all possible means to bring them to a better mind, and recover them to the fellowship and unity of the Church."
At least one modern Reformed theologian argues that excommunication is not the final step in the disciplinary process. Jay E. Adams argues that in excommunication, the offender is still seen as a brother, but in the final step they become "as the heathen and tax collector" (Matthew 18:17). Adams writes, "Nowhere in the Bible is excommunication (removal from the fellowship of the Lord's Table, according to Adams) equated with what happens in step 5; rather, step 5 is called "removing from the midst, handing over to Satan," and the like."
Former Yale president and theologian, Jonathan Edwards, addresses the notion of excommunication as "removal from the fellowship of the Lord's Table" in his treatise entitled "The Nature and End of Excommunication". Edwards argues that "Particularly, we are forbidden such a degree of associating ourselves with (excommunicants), as there is in making them our guests at our tables, or in being their guests at their tables; as is manifest in the text, where we are commanded to have no company with them, no not to eat". Edwards insists, "That this respects not eating with them at the Lord's supper, but a common eating, is evident by the words, that the eating here forbidden, is one of the lowest degrees of keeping company, which are forbidden. Keep no company with such a one, saith the apostle, no not to eat — as much as to say, no not in so low a degree as to eat with him. But eating with him at the Lord's supper, is the very highest degree of visible Christian communion. Who can suppose that the apostle meant this: Take heed and have no company with a man, no not so much as in the highest degree of communion that you can have? Besides, the apostle mentions this eating as a way of keeping company which, however, they might hold with the heathen. He tells them, not to keep company with fornicators. Then he informs them, he means not with fornicators of this world, that is, the heathens; but, saith he, “if any man that is called a brother be a fornicator, etc. with such a one keep no company, no not to eat.” This makes it most apparent, that the apostle doth not mean eating at the Lord's table; for so, they might not keep company with the heathens, any more than with an excommunicated person."
Anabaptist tradition.
When believers were baptized and taken into membership of the church by Anabaptists, it was not only done as symbol of cleansing of sin but was also done as a public commitment to identify with Jesus Christ and to conform one's life to the teaching and example of Jesus as understood by the church. Practically, that meant membership in the church entailed a commitment to try to live according to norms of Christian behavior widely held by the Anabaptist tradition.
In the ideal, discipline in the Anabaptist tradition requires the church to confront a notoriously erring and unrepentant church member, first directly in a very small circle and, if no resolution is forthcoming, expanding the circle in steps eventually to include the entire church congregation. If the errant member persists without repentance and rejects even the admonition of the congregation, that person is excommunicated or excluded from church membership. Exclusion from the church is recognition by the congregation that this person has separated himself or herself from the church by way of his or her visible and unrepentant sin. This is done ostensibly as a final resort to protect the integrity of the church. When this occurs, the church is expected to continue to pray for the excluded member and to seek to restore him or her to its fellowship. There was originally no "inherent" expectation to shun (completely sever all ties with) an excluded member, however differences regarding this very issue led to early schisms between different Anabaptist leaders and those who followed them.
Amish.
Jakob Ammann, founder of the Amish sect, believed that the shunning of those under the ban should be systematically practiced among the Swiss Anabaptists as it was in the north and as was outlined in the Dordrecht Confession. Ammann's uncompromising zeal regarding this practice was one of the main disputes that led to the schism between the Anabaptist groups that became the Amish and those that eventually would be called Mennonite. Recently more moderate Amish groups have become less strict in their application of excommunication as a discipline. This has led to splits in several communities, an example of which is the Swartzetruber Amish who split from the main body of Old Order Amish because of the latter's practice of lifting the ban from members who later join other churches. In general, the Amish will excommunicate baptized members for failure to abide by their Ordnung (church rules) as it is interpreted by the local Bishop if certain repeat violations of the Ordnung occur.
Excommunication among the Old Order Amish results in shunning or "the Meidung", the severity of which depends on many factors, such as the family, the local community as well as the type of Amish. Some Amish communities cease shunning after one year if the person joins another church later on, especially if it is another Mennonite church. At the most severe, other members of the congregation are prohibited almost all contact with an excommunicated member including social and business ties between the excommunicant and the congregation, sometimes even marital contact between the excommunicant and spouse remaining in the congregation or family contact between adult children and parents.
Mennonites.
In the Mennonite Church excommunication is rare and is carried out only after many attempts at reconciliation and on someone who is flagrantly and repeatedly violating standards of behavior that the church expects. Occasionally excommunication is also carried against those who repeatedly question the church's behavior and/or who genuinely differ with the church's theology as well, although in almost all cases the dissenter will leave the church before any discipline need be invoked. In either case, the church will attempt reconciliation with the member in private, first one on one and then with a few church leaders. Only if the church's reconciliation attempts are unsuccessful, the congregation formally revokes church membership. Members of the church generally pray for the excluded member.
Some regional conferences (the Mennonite counterpart to dioceses of other denominations) of the Mennonite Church have acted to expel member congregations that have openly welcomed non-celibate homosexuals as members. This internal conflict regarding homosexuality has also been an issue for other moderate denominations, such as the American Baptists and Methodists.
The practice among Old Order Mennonite congregations is more along the lines of Amish, but perhaps less severe typically. An Old Order member who disobeys the "Ordnung" (church regulations) must meet with the leaders of the church. If a church regulation is broken a second time there is a confession in the church. Those who refuse to confess are excommunicated. However upon later confession, the church member will be reinstated. An excommunicated member is placed under the ban. This person is not banned from eating with their own family. Excommunicated persons can still have business dealings with church members and can maintain marital relations with a marriage partner, who remains a church member.
Hutterites.
The separatist, communal, and self-contained Hutterites also use excommunication and shunning as form of church discipline. Since Hutterites have communal ownership of goods, the effects of excommunication could impose a hardship upon the excluded member and family leaving them without employment income and material assets such as a home. However, often arrangements are made to provide material benefits to the family leaving the colony such as an automobile and some transition funds for rent, etc. One Hutterite colony in Manitoba (Canada) had a protracted dispute when leaders attempted to force the departure of a group that had been excommunicated but would not leave. About a dozen lawsuits in both Canada and the United States were filed between the various Hutterite factions and colonies concerning excommunication, shunning, the legitimacy of leadership, communal property rights, and fair division of communal property when factions have separated.
The Church of Jesus Christ of Latter-day Saints.
The Church of Jesus Christ of Latter-day Saints (LDS Church) practices excommunication as penalty for those who commit serious sins, "i.e.", actions that significantly impair the name or moral influence of the church or pose a threat to other people. According to the church leadership "Handbook", the purposes of church discipline are (1) to save the souls of transgressors, (2) to protect the innocent, and (3) to safeguard the purity, integrity, and good name of the church.
The LDS Church also practices the lesser sanctions of private counsel and caution, informal probation, formal probation, and disfellowshipment.
Disfellowshipped is used for serious sins that do not rise to the level of excommunication. Disfellowshipment denies some privileges but does not include a loss of church membership. Once disfellowshipped, persons may not take the sacrament or enter church temples, nor may they offer public prayers or sermons. Disfellowshipped persons may continue to attend most church functions and are permitted to wear temple garments, pay tithes and offerings, and participate in church classes if their conduct is orderly. Disfellowshipment typically lasts for one year, after which one may be reinstated as a member in good standing.
In the more grievous or recalcitrant cases, excommunication becomes a disciplinary option. Excommunication is generally reserved for what are seen as the most serious sins, including committing serious crimes such as murder, child abuse, and incest; committing adultery; involvement in or teaching of polygamy; involvement in homosexual conduct; apostasy; participation in an abortion; teaching false doctrine; or openly criticizing church leaders. A 2006 revision to the "Handbook" states that formally joining another church constitutes apostasy and is an excommunicable offense; however, merely attending another church does not constitute apostasy.
An excommunication can occur only after a formal disciplinary council. Formerly called a "church court," the councils were renamed to avoid focusing on guilt and instead to emphasize the availability of repentance.
The decision to excommunicate a Melchizedek priesthood holder is generally the province of the leadership of a stake. In such a disciplinary council, the stake presidency and stake high council attend. The twelve members of the high council are split in half: one group represents the member in question and is charged with "prevent[ing] insult or injustice"; the other group represents the church as a whole. The member under scrutiny is invited to attend the disciplinary proceedings, but the council can go forward without him. In making a decision, the leaders of the high council consult with the stake presidency, but the decision about which discipline is necessary is the stake president's alone. It is possible to appeal a decision of a stake disciplinary council to the church's First Presidency.
For females and for male members not initiated into the Melchizedek priesthood, a ward disciplinary council is held. In such cases, a bishop determines whether excommunication or a lesser sanction is warranted. He does this in consultation with his two counselors, with the bishop making the final determination after prayer. The decision of a ward disciplinary council can be appealed to the stake president.
The following list of variables serves as a general set of guidelines for when excommunication or lesser action may be warranted, beginning with those more likely to result in severe sanction:
Notices of excommunication may be made public, especially in cases of apostasy, where members could be misled; however, the specific reasons for individual excommunications are typically kept confidential and are seldom made public by church leadership.
Those who are excommunicated lose their church membership and the right to partake of the sacrament. Such persons are usually allowed to attend church meetings but participation is limited: they cannot offer public prayers or preach sermons and cannot enter temples. Excommunicated members are also barred from wearing or purchasing temple garments and from paying tithes. Excommunicated members may be re-baptized after a waiting period and sincere repentance, as judged by a series of interviews with church leaders.
Some critics have charged that LDS Church leaders have used the threat of excommunication to silence or punish church members and researchers who disagree with established policy and doctrine, who study or discuss controversial subjects, or who may be involved in disputes with local, stake leaders or general authorities; see, e.g., Brian Evenson, a former BYU professor and writer whose fiction came under criticism from BYU officials and LDS Leadership. Another notable case of excommunication from the LDS Church was the "September Six," a group of intellectuals and professors, five of whom were excommunicated and the sixth disfellowshipped.
However, church policy dictates that local leaders are responsible for excommunication, without influence from church headquarters. The church thus argues that this policy is evidence against any systematic persecution of scholars.
Jehovah's Witnesses.
Jehovah's Witnesses practice a form of excommunication, using the term "disfellowshipping", in cases where a member is believed to have unrepentantly committed one or more of several documented "serious sins". The practice is based on their interpretation of 1 Corinthians 5:11-13 ("quit mixing in company with anyone called a brother that is a fornicator or greedy person or an idolater or a reviler or a drunkard or an extortioner, not even eating with such a man...remove the wicked man from your midst") and 2 John 10 ("never receive him in your home or say a greeting to him"). They interpret these verses to mean that any baptized believer who engages in "gross sins" is to be expelled from the congregation and shunned.
When a member confesses to, or is accused of, a "serious sin", a "judicial committee" of at least three elders is formed. This committee investigates the case and determines the magnitude of the sin committed. If the person is deemed guilty of a disfellowshipping offense, the committee then decides, on the basis of the person's attitude and "works befitting repentance" (), whether the person is to be considered repentant. The "works" may include trying to correct the wrong, making apologies to any offended individuals, and compliance with earlier counsel. If deemed guilty but repentant, the person is not disfellowshipped but is formally "reproved" and has "restrictions" imposed, which preclude the individual from various activities such as presenting talks, offering public prayers or making comments at religious meetings. If the person is deemed guilty and unrepentant, he or she will be disfellowshipped. Unless an appeal is made within seven days, the disfellowshipping is made formal by an announcement at the congregation's next Service Meeting. Appeals are granted to determine if procedural errors are felt to have occurred that may have affected the outcome.
Disfellowshipping is a severing of friendly relationships between all Jehovah's Witnesses and the disfellowshipped person. Interaction with extended family is typically restricted to a minimum, such as presence at the reading of wills and providing essential care for the elderly. Within a household, typical family contact may continue, but without spiritual fellowship such as family Bible study and religious discussions. Parents of disfellowshipped minors living in the family home may continue to attempt to convince the child about the religion's teachings. Jehovah's Witnesses believe that this form of discipline encourages the disfellowshipped individual to conform to biblical standards and prevents the person from influencing other members of the congregation.
Along with breaches of the Witnesses' moral code, openly disagreeing with the teachings Jehovah's Witnesses is considered grounds for shunning. These persons are labeled as "apostates", and are described in Watch Tower Society literature as "mentally diseased". Descriptions of "apostates" appearing in the Witnesses literature have been the subject of investigation in the UK to determine if they violate religious hatred laws. Sociologist Andrew Holden claims many Witnesses who would otherwise defect because of disillusionment with the organization and its teachings, remain affiliated out of fear of being shunned and losing contact with friends and family members. Shunning employs what is known "as relational aggression" in psychological literature. When used by church members and member-spouse parents against excommunicant parents it contains elements of what psychologists call "parental alienation". Extreme shunning may cause trauma to the shunned (and to their dependents) similar to what is studied in the psychology of torture.
Disassociation is a form of shunning where a member expresses verbally or in writing that they do not wish to be associated with Jehovah's Witnesses, rather than for having committed any specific 'sin'. Elders may also decide that an individual has disassociated, without any formal statement by the individual, by actions such as accepting a blood transfusion, or for joining another religion or military organization. Individuals who are deemed by the elders to have disassociated are given no right of appeal.
Each year, congregation elders are instructed to consider meeting with disfellowshipped individuals to determine changed circumstances and encourage them to pursue reinstatement. Reinstatement is not automatic after a certain time period, nor is there a minimum duration; disfellowshipped persons may talk to elders at any time but must apply in writing to be considered for reinstatement into the congregation. Elders consider each case individually, and are instructed to ensure "that sufficient time has passed for the disfellowshipped person to prove that his profession of repentance is genuine." A judicial committee meets with the individual to determine their repentance, and if this is established, the person is reinstated into the congregation and may participate with the congregation in their formal ministry (such as house-to-house preaching), but is prohibited from commenting at meetings or holding any privileges for a period set by the judicial committee. If possible, the same judicial committee members who disfellowshipped the individual are selected for the reinstatement hearing. If the applicant is in a different area, the person will meet with a local judicial committee that will communicate with either the original judicial committee if available or a new one in the original congregation.
A Witness who has been formally reproved or reinstated cannot be appointed to any "special privilege of service" for at least one year. Serious sins involving child sex abuse permanently disqualify the sinner from appointment to any congregational "privilege of service", regardless of whether the sinner was convicted of any secular crime.
Christadelphians.
Similarly to many groups having their origins in the 1830s Restoration Movement, Christadelphians call their form of excommunication "disfellowshipping", though they do not practice "shunning". Disfellowshipping can occur for moral reasons, changing beliefs, or (in some ecclesias) for not attending communion (referred to as "the emblems" or "the breaking of bread").
In such cases, the person involved is usually required to discuss the issues. If they do not conform, the church ('meeting' or 'ecclesia') is recommended by the management committee ("Arranging Brethren") to vote on disfellowshipping the person. These procedures were formulated 1863 onwards by early Christadelphians, and then in 1883 codified by Robert Roberts in "A Guide to the Formation and Conduct of Christadelphian Ecclesias" (colloquially "The Ecclesial Guide"). However Christadelphians justify and apply their practice not only from this document but also from passages such as the exclusion in 1Co.5 and recovery in 2Co.2.
Christadelphians typically avoid the term "excommunication" which many associate with the Catholic Church; and may feel the word carries implications they do not agree with, such as undue condemnation and punishment, as well as failing to recognise the remedial intention of the measure.
In the case of adultery and divorce, the passage of time usually means a member can be restored if he or she wants to be. In the case of ongoing behaviour, cohabitation, homosexual activity, then the terms of the suspension have not been met.
The mechanics of "refellowship" follow the reverse of the original process; the individual makes an application to the "ecclesia", and the "Arranging Brethren" give a recommendation to the members who vote. If the "Arranging Brethren" judge that a vote may divide the ecclesia, or personally upset some members, they may seek to find a third party ecclesia which is willing to "refellowship" the member instead. According to the Ecclesial Guide a third party ecclesia may also take the initiative to "refellowship" another meeting's member. However this cannot be done unilaterally, as this would constitute heteronomy over the autonomy of the original ecclesia's members.
Society of Friends (Quakers).
Among many of the Society of Friends groups (Quakers) one is "read out of meeting" for behaviour inconsistent with the sense of the meeting. However it is the responsibility of each meeting, quarterly meeting, and yearly meeting, to act with respect to their own members. For example, during the Vietnam War many Friends were concerned about Friend Richard Nixon's position on war which seemed at odds with their beliefs; however, it was the responsibility of Nixon's own meeting, the East Whittier Meeting of Whittier, California, to act if indeed that meeting felt the leaning. They did not.
In the 17th century, before the founding of abolitionist societies, Friends who too forcefully tried to convince their coreligionists of the evils of slavery were read out of meeting. Benjamin Lay was read out of the Philadelphia Yearly Meeting for this. During the American Revolution over 400 Friends were read out of meeting for their military participation or support.
Buddhism.
There is no direct equivalent to excommunication in Buddhism. However, in the Theravadan monastic community monks can be expelled from monasteries for heresy and/or other acts. In addition, the monks have four vows, called the four defeats, which are abstaining from sexual intercourse, stealing, murder, and refraining from lying about spiritual gains (e.g., having special power or ability to perform miracles). If even one is broken, the monk is automatically a layman again and can never become a monk in his or her current life.
Most Japanese Buddhist sects hold ecclesiastical authority over its followers and have their own rules for expelling members of the sangha, lay or bishopric. The lay Japanese Buddhist organization Sōka Gakkai was expelled from the Nichiren Shoshu sect in 1991 (1997).
Hinduism.
Hinduism has been too diverse to be seen as a monolithic religion, and with a conspicuous absence of any listed dogma or ecclesia (organised church), has no concept of excommunication and hence no Hindu may be ousted from the Hindu religion, although a person may easily lose caste status for a very wide variety of infringements of caste prohibitions. This may or may not be recoverable. However, some of the modern organized sects within Hinduism may practice something equivalent to excommunication today, by ousting a person from their own sect.
In medieval and early-modern times (and sometimes even now) in South Asia, excommunication from one's "caste" ("jati" or "varna") used to be practiced (by the caste-councils) and was often with serious consequences, such as abasement of the person's caste status and even throwing him into the sphere of the untouchables or bhangi. In the 19th century, a Hindu faced excommunication for going abroad, since it was presumed he would be forced to break caste restrictions and, as a result, become polluted.
After excommunication, it would depend upon the caste-council whether they would accept any form of repentance (ritual or otherwise) or not. Such current examples of excommunication in Hinduism are often more political or social rather than religious, for example the excommunication of lower castes for refusing to work as scavengers in Tamil Nadu.
An earlier example of excommunication in Hinduism is that of Shastri Yagnapurushdas, who voluntarily left and was later expelled from the Vadtal Gadi of the Swaminarayan Sampraday by the then Vadtal acharya in 1906. He went on to form his own institution, "Bochasanwasi Swaminarayan Sanstha" or "BSS" (now BAPS) claiming Gunatitanand Swami was the rightful spiritual successor to Swaminarayan.
Islam.
Excommunication as it exists in Christian faiths does not exist in Islam. The nearest approximation is "takfir", a declaration that an individual or group is "kafir" (or "kuffar" in plural), a non-believer. This does not prevent an individual from taking part in any Islamic rite or ritual, and since the matter of whether a person is "kafir" is a rather subjective matter, a declaration of "takfir" is generally considered null and void if the target refutes it or if the Islamic community in which he or she lives refuses to accept it.
"Takfir" has usually been practiced through the courts. More recently, cases have taken place where individuals have been considered kuffar. These decisions followed lawsuits against individuals, mainly in response to their writings that some have viewed as anti-Islamic. The most famous cases are of Salman Rushdie, Nasr Abu Zayd, and Nawal El-Saadawi. The repercussions of such cases have included divorce, since under traditional interpretations of Islamic law, Muslim women are not permitted to marry non-Muslim men.
However, "takfir" remains a highly contentious issue in Islam, primarily because there is no universally accepted authority in Islamic law. Indeed, according to classical commentators, the reverse seems to hold true, in that Muhammad reportedly equated the act of declaring someone a "kafir" itself to blasphemy if the accused individual maintained that he was a Muslim.
Judaism.
"Cherem" is the highest ecclesiastical censure in Judaism. It is the total exclusion of a person from the Jewish community. Except for cases in the Charedi community, "cherem" stopped existing after The Enlightenment, when local Jewish communities lost their political autonomy, and Jews were integrated into the gentile nations in which they lived. A "siruv" order, equivalent to a contempt of court, issued by a Rabbinical court may also limit religious participation.

</doc>
<doc id="10339" url="http://en.wikipedia.org/wiki?curid=10339" title="Electrochemical cell">
Electrochemical cell

An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or facilitating chemical reactions through the introduction of electrical energy. A common example of an electrochemical cell is a standard 1.5-volt battery meant for consumer use. A conventional battery of this type is known as a single Galvanic cell battery consisting of multiple cells, connected in either parallel or series pattern.
Half-cells.
An electrochemical cell consists of two half-cells. Each "half-cell" consists of an electrode and an electrolyte. The two half-cells may use the same electrolyte, or they may use different electrolytes. The chemical reactions in the cell may involve the electrolyte, the electrodes, or an external substance (as in fuel cells that may use hydrogen gas as a reactant). In a full electrochemical cell, species from one half-cell lose electrons (oxidation) to their electrode while species from the other half-cell gain electrons (reduction) from their electrode. 
A "salt bridge" (e.g., filter paper soaked in KNO3 or some other electrolyte) is often employed to provide ionic contact between two half-cells with different electrolytes, yet prevent the solutions from mixing and causing unwanted side reactions. An alternative to a salt bridge is to allow direct contact (and mixing) between the two half-cells, for example in simple electrolysis of water. 
As electrons flow from one half-cell to the other through an external circuit, a difference in charge is established. If no ionic contact were provided, this charge difference would quickly prevent the further flow of electrons. A salt bridge allows the flow of negative or positive ions to maintain a steady-state charge distribution between the oxidation and reduction vessels, while keeping the contents otherwise separate. Other devices for achieving separation of solutions are porous pots and gelled solutions. A porous pot is used in the Bunsen cell (right).
Equilibrium reaction.
Each half-cell has a characteristic voltage. Various choices of substances for each half-cell give different potential differences. Each reaction is undergoing an equilibrium reaction between different oxidation states of the ions: When equilibrium is reached, the cell cannot provide further voltage. In the half-cell that is undergoing oxidation, the closer the equilibrium lies to the ion/atom with the more positive oxidation state the more potential this reaction will provide. Likewise, in the reduction reaction, the closer the equilibrium lies to the ion/atom with the more "negative" oxidation state the higher the potential.
Cell potential.
The cell potential can be predicted through the use of electrode potentials (the voltages of each half-cell). These half-cell potentials are defined relative to the assignment of 0 volts to the standard hydrogen electrode (SHE). (See table of standard electrode potentials). The difference in voltage between electrode potentials gives a prediction for the potential measured. When calculating the difference in voltage, one must first rewrite the half-cell reaction equations to obtain a balanced oxidation-reduction equation.
Note that the cell potential does not change when the reaction is multiplied by a constant.
Cell potentials have a possible range of roughly zero to 6 volts. Cells using water-based electrolytes are usually limited to cell potentials less than about 2.5 volts, because the very powerful oxidizing and reducing agents that would be required to produce a higher cell potential tend to react with the water. Higher cell potentials are possible with cells using other solvents instead of water. For instance, lithium cells with a voltage of 3 volts are commonly available.
The cell potential depends on the concentration of the reactants, as well as their type. As the cell is discharged, the concentration of the reactants decreases, and the cell potential also decreases.

</doc>
<doc id="10340" url="http://en.wikipedia.org/wiki?curid=10340" title="Ecdysis">
Ecdysis

Ecdysis is the moulting of the cuticle in many invertebrates. This process of moulting is the defining feature of the clade Ecdysozoa, comprising the arthropods, nematodes, velvet worms, horsehair worms, tardigrades, and Cephalorhyncha. Since the cuticle of these animals typically forms a largely inelastic exoskeleton, it is shed during growth and a new, larger covering is formed. The remnants of the old, empty exoskeleton are called exuviae.
After moulting, an arthropod is described as "teneral", a "callow"; it is "fresh", pale and soft-bodied. Within one or two hours, the cuticle hardens and darkens following a tanning process analogous to the production of leather. During this short phase the animal expands, since growth is otherwise constrained by the rigidity of the exoskeleton. Growth of the limbs and other parts normally covered by hard exoskeleton is achieved by transfer of body fluids from soft parts before the new skin hardens. A spider with a small abdomen may be undernourished but more probably has recently undergone ecdysis. Some arthropods, especially large insects with tracheal respiration, expand their new exoskeleton by swallowing or otherwise taking in air. The maturation of the structure and coloration of the new exoskeleton might take days or weeks in a long-lived insect; this can make it difficult to identify an individual if it has recently undergone ecdysis. 
Ecdysis allows damaged tissue and missing limbs to be regenerated or substantially re-formed. Complete regeneration may require a series of moults, the stump becoming a little larger with each moult until it is a normal, or near normal, size.
Etymology.
The term "ecdysis" comes from Ancient Greek: ἐκδύω ("ekduo"), "to take off, strip off".
Process.
In preparation for ecdysis, the arthropod becomes inactive for a period of time, undergoing apolysis or separation of the old exoskeleton from the underlying epidermal cells. For most organisms, the resting period is a stage of preparation during which the secretion of fluid from the moulting glands of the epidermal layer and the loosening of the underpart of the cuticle occur.
Once the old cuticle has separated from the epidermis, a digesting fluid is secreted into the space between them. However, this fluid remains inactive until the upper part of the new cuticle has been formed. Then, by crawling movements, the organism pushes forward in the old integumentary shell, which splits down the back allowing the animal to emerge. Often, this initial crack is caused by a combination of movement and increase in blood pressure within the body, forcing an expansion across its exoskeleton, leading to an eventual crack that allows for certain organisms such as spiders to extricate themselves.
While the old cuticle is being digested, the new layer is secreted. All cuticular structures are shed at ecdysis, including the inner parts of the exoskeleton, which includes terminal linings of the alimentary tract and of the tracheae if they are present.
Insects.
Moulting (ecdysis) in southern hawker, "Aeshna cyanea"
Each stage of development between moults for insects in the taxon endopterygota is called an instar, or stadium, and each stage between moults of insects in the Exopterygota is called a nymph: there may be up to 15 nymphal stages. Endopterygota tend to have only four or five instars. Endopterygotes have more alternatives to moulting, such as expansion of the cuticle and collapse of air sacs to allow growth of internal organs.
The process of moulting in insects begins with the separation of the cuticle from the underlying epidermal cells (apolysis) and ends with the shedding of the old cuticle (ecdysis). In many species it is initiated by an increase in the hormone ecdysone. This hormone causes:
After apolysis the insect is known as a pharate. Moulting fluid is then secreted into the exuvial space between the old cuticle and the epidermis, this contains inactive enzymes which are activated only after the new epicuticle is secreted. This prevents the new procuticle from getting digesting as it is laid down. The lower regions of the old cuticle, the endocuticle and mesocuticle, are then digested by the enzymes and subsequently absorbed. The exocuticle and epicuticle resist digestion and are hence shed at ecdysis.
Spiders.
Spiders generally change their skin for the first time while still inside the egg sac, and the spiderling that emerges broadly resembles the adult. The number of moults varies, both between species and genders, but generally will be between five times and nine times before the spider reaches maturity. Not surprisingly, since males are generally smaller than females, the males of many species mature faster and do not undergo ecdysis as many times as the females before maturing.
Members of the Mygalomorphae are very long-lived, sometimes 20 years or more; they moult annually even after they mature.
Spiders stop feeding at some time before moulting, usually for several days. The physiological processes of releasing the old exoskeleton from the tissues beneath typically cause various colour changes, such as darkening. If the old exoskeleton is not too thick it may be possible to see new structures, such as setae, from outside. However, contact between the nerves and the old exoskeleton is maintained until a very late stage in the process.
The new, teneral exoskeleton has to accommodate a larger frame than the previous instar, while the spider has had to fit into the previous exoskeleton until it has been shed. This means the spider does not fill out the new exoskeleton completely, so it commonly appears somewhat wrinkled.
Most species of spiders hang from silk during the entire process, either dangling from a drop line, or fastening their claws into webbed fibres attached to a suitable base. The discarded, dried exoskeleton typically remains hanging where it was abandoned once the spider has left. 
To open the old exoskeleton, the spider generally contracts its abdomen (opisthosoma) to supply enough fluid to pump into the prosoma with sufficient pressure to crack it open along its lines of weakness. The carapace lifts off from the front, like a helmet, as its surrounding skin ruptures, but it remains attached at the back. Now the spider works its limbs free and typically winds up dangling by a new thread of silk attached to its own exuviae, which in turn hang from the original silk attachment. 
At this point the spider is a callow; it is teneral and vulnerable. As it dangles, its exoskeleton hardens and takes shape. The process may take minutes in small spiders, or some hours in the larger Mygalomorphs. Some spiders, such as some "Synema" species, members of the Thomisidae (crab spiders), mate while the female is still callow, during which time she is unable to eat the male.
Eurypterids.
Eurypterids are a group of chelicerates that became extinct in the late Permian. They underwent ecdysis in a similar manner to extant chelicerates, and most fossils are thought to be of exuviae, rather than cadavers.

</doc>
<doc id="10343" url="http://en.wikipedia.org/wiki?curid=10343" title="Ebor, New South Wales">
Ebor, New South Wales

Ebor is a village on Waterfall Way on the Northern Tablelands, New South Wales, Australia. It is situated about 80 km east of Armidale and about a third of the way between Armidale and the coast. At the 2006 census, Ebor and the surrounding area had a population of 160. There is a post office and coffee shop, a hotel/motel and a primary school.
The surrounding district is a noted rich sheep and cattle grazing area. The main tourist attractions are Ebor Falls, Wollomombi Falls, the cool temperate rainforest walks in New England National Park and recreational trout fishing. The Dutton Trout Hatchery on Point Lookout Road was established in 1950 and is one of the largest hatcheries in the state. Visitors can see the various stages of trout development prior to their release in the mountain streams.
History.
Ebor Post Office opened on 2 March 1868, closed in 1869 and reopened in 1910.
Climate.
Ebor is at fairly high altitude, about 1300 m and by Australian standards, has cold winters with overnight frost and occasional light snow falls. The average rain fall is 950 mm.

</doc>
<doc id="10344" url="http://en.wikipedia.org/wiki?curid=10344" title="Pre-Islamic period of Afghanistan">
Pre-Islamic period of Afghanistan

Archaeological exploration of the pre-Islamic period of Afghanistan began in Afghanistan in earnest after World War II and proceeded until the late 1970s when the nation was invaded by the Soviet Union. Archaeologists and historians suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world. Urbanized culture has existed in the land between 3000 and 2000 BC. Artifacts typical of the Paleolithic, Mesolithic, Neolithic, Bronze, and Iron ages have been found inside Afghanistan.
After the Indus Valley Civilisation which stretched up to northeast Afghanistan, it was inhabited by the Aryan tribes and controlled by the Medes until about 500 BC when Darius the Great (Darius I) marched with his Persian army to make it part of the Achaemenid Empire. In 330 BC, Alexander the Great of Macedonia invaded the land after defeating Darius III of Persia in the Battle of Gaugamela. Much of Afghanistan became part of the Seleucid Empire followed by the Greco-Bactrian Kingdom. The area south of the Hindu Kush had been given by Seleucus I Nicator to Chandragupta Maurya and became part of the Maurya Empire. The land was inhabited by various tribes and ruled by many different kingdoms for the next two millenniums. Before the arrival of Islam in the 7th century, there were a number of religions practiced in ancient Afghanistan, including Zoroastrianism, Surya worship, Paganism, Buddhism and Hinduism. The Kaffirstan region, in the Hindu Kush, resisted conversion until the 1890s.
Prehistoric era.
Louis Dupree, the University of Pennsylvania, the Smithsonian Institution and others suggest that humans were living in Afghanistan at least 50,000 years ago, and that farming communities of the region were among the earliest in the world. Archaeologists have found evidence of human habitation in Afghanistan from as far back as 50,000 BC. The artifacts indicate that the indigenous people were small farmers and herdsmen, as they are today, very probably grouped into tribes, with small local kingdoms rising and falling through the ages.
Afghanistan seems in prehistory, as well as in ancient and modern times, to have been connected by culture and trade with the neighbouring regions. Urban civilization, which includes modern-day Afghanistan, North India, and Pakistan, may have begun as early as 3000 to 2000 BC. Archaeological finds indicate the possible beginnings of the Bronze Age, which would ultimately spread throughout the ancient world from Afghanistan. It is also believed that the region had early trade contacts with Mesopotamia.
Indus Valley Civilisation.
The Indus Valley Civilisation (IVC) was a Bronze Age civilisation (3300-1300 BCE; mature period 2600-1900 BCE) extending from what today is northwest Pakistan to northwest India and northeast Afghanistan. An Indus Valley site has been found on the Oxus River at Shortugai in northern Afghanistan. Apart from Shortughai is Mundigak another notable site. There are several smaller IVC colonies to be found in Afghanistan.
Aryans and the Medes rule (1500 BC–551 BC).
Between 2000–1200 BC, a branch of Indo-European-speaking tribes known as the Aryans began migrating into the region. This is part of a dispute in regards to the Aryan invasion theory. They appear to have split into old Persian peoples, Nuristani, and Indian groups at an early stage, possibly between 1500 and 1000 BC in what is today Afghanistan or much earlier as eastern remnants of the Indo-Aryans drifted much further west as with the Mitanni. The Iranians dominated the modern day plateau, while the Indo-Aryans ultimately headed towards the Indian subcontinent. The Avesta is believed to have been composed possibly as early as 1800 BC and written in ancient Ariana (Aryana), the earliest name of Afghanistan which indicates an early link with today's Iranian tribes to the west, or adjacent regions in Central Asia or northeastern Iran in the 6th century BC. Due to the similarity between early Avestan and Sanskrit (and other related early Indo-European languages such as Latin and Ancient Greek), it is believed that the split between the old Persians and Indo-Aryan tribes had taken place at least by 1000 BC. There are striking similarities between the Old Afghan language of Avestan and Sanskrit, which may support the notion that the split was contemporary with the Indo-Aryans living in Afghanistan at a very early stage. Also, the Avesta itself divides into Old and New sections and neither mention the Medes who are known to have ruled Afghanistan starting around 700 BC. This suggests an early time-frame for the Avesta that has yet to be exactly determined as most academics believe it was written over the course of centuries if not millennia. Much of the archaeological data comes from the Bactria-Margiana Archaeological Complex (BMAC and Indus Valley Civilization) that probably played a key role in early Aryanic civilization in Afghanistan.
It has also been surmised by many researchers that the Aryan prophet Zoroaster was born somewhere in ancient Aryana, possibly in the ancient northern Persia, and the timeframe of his life literally spans millennia from as early 2000 BC to as late as 600 BC. Zoroastrianism spread throughout the region alongside early pagan beliefs and centuries later Buddhism.
The Medes, a Western Persian people, arrived from what is today Kurdistan sometime around the 700s BC and came to dominate most of ancient Afghanistan. They were an early tribe that forged the first empire on the present Iranian plateau and were rivals of the Persians whom they initially dominated in the province of Fars to the south. Median domination of Afghanistan would last until the Persians challenged and ultimately replaced them from their original base in Fars in southern Iran near ancient Elam.
Achaemenid invasion and Zoroastrianism (550 BC–331 BC).
The city of Bactria (which later became Balkh), is believed to have been the home of Zarathustra, who founded the Zoroastrian religion. The Avesta refers to eastern Bactria as being the home of the Zoroastrian faith, but this can be a reference to either a region in modern Afghanistan or Border line of Afghan-Pakistan. Regardless of the debate as to where Zoroaster was from, Zoroastrianism spread to become one of the world's most influential religions and became the main faith of the old Aryan people for centuries. It also remained the official religion of Persia until the defeat of the Sassanian ruler Yazdegerd III—over a thousand years after its founding—by Muslim Arabs. In what is today southern Iran, the Persians emerged to challenge Median supremacy on the Iranian plateau. By 550 BC, the Persians had replaced Median rule with their own dominion and even began to expand past previous Median imperial borders. Both Gandhara and Kamboja Mahajanapadas of the Buddhist texts soon fell a prey to the Achaemenian Dynasty during the reign of Achaemenid, Cyrus the Great (558–530 BC), or in the first year of Darius I, marking the region or of the more Eastern provinces of the empire. According to Pliny's evidence, Cyrus the Great (Cyrus II) had destroyed Kapisa in Capiscene which was a Kamboja city. The former region of Gandhara and Kamboja (upper Indus) had constituted seventh satrapy of the Achaemenid Empire and annually contributed 170 talents of gold dust as a tribute to the Achaemenids.
Bactria had a special position in old Afghanistan, being the capital of a vice-kingdom. By the 4th century BC, Persian control of outlying areas and the internal cohesion of the empire had become somewhat tenuous. Although distant provinces like Bactriana had often been restless under Achaemenid rule, Bactrian troops nevertheless fought in the decisive Battle of Gaugamela in 330 BC against the advancing armies of Alexander the Great. The Achaemenids were decisively defeated by Alexander and retreated from his advancing army of Greco-Macedonians and their allies. Darius III, the last Achaemenid ruler, tried to flee to Bactria but was assassinated by a subordinate lord, the Bactrian-born Bessus, who proclaimed himself the new ruler of Persia as Artaxerxes (V). Bessus was unable to mount a successful resistance to the growing military might of Alexander's army so he fled to his native Bactria, where he attempted to rally local tribes to his side but was instead turned over to Alexander who proceeded to have him tortured and executed for having committed regicide.
Alexander the Great to Greco-Bactrian rule (330 BC–ca. 150 BC).
It had taken Alexander only six months to conquer Persia (Iran), but it took him nearly thirty years (from about 330 BC–363 BC) to subdue Afghanistan. Moving eastward from Persia, the Macedonian leader encountered fierce resistance from the local tribes of Aria (satrapy), Drangiana (now part of Afghanistan, Pakistan and Eastern Iran), Arachosia (South and Eastern Afghanistan, North-West Pakistan) and Bactria (North and Central Afghanistan).
Upon Alexander's death in 323 BC, his empire, which had never been politically consolidated, broke apart as his companions began to divide it amongst themselves. Alexander's cavalry commander, Seleucus, took nominal control of the eastern lands and founded the Seleucid dynasty. Under the Seleucids, as under Alexander, Greek colonists and soldiers colonized Bactria, roughly corresponding to modern Afghanistan's borders. However, the majority of Macedonian soldiers of Alexander the Great wanted to leave the east and return home to Greece. Later, Seleucus sought to guard his eastern frontier and moved Ionian Greeks (also known as Yavanas to many local groups) to Bactria in the 3rd century BC.
Maurya Empire.
While the Diadochi were warring amongst themselves, the Mauryan Empire was developing in the northern part of the Indian subcontinent. The founder of the empire, Chandragupta Maurya, confronted a Macedonian invasion force led by Seleucus I in 305 BC and following a brief conflict, an agreement was reached as Seleucus ceded Gandhara and Arachosia (centered around ancient Kandahar) and areas south of Bagram (corresponding to the extreme south-east of modern Afghanistan) to the Mauryans. During the 120 years of the Mauryans in southern Afghanistan, Buddhism was introduced and eventually become a major religion alongside Zoroastrianism and local pagan beliefs. The ancient Grand Trunk Road was built linking what is now Kabul to various cities in the Punjab and the Gangetic Plain. Commerce, art, and architecture (seen especially in the construction of stupas) developed during this period. It reached its high point under Emperor Ashoka whose edicts, roads, and rest stops were found throughout the subcontinent. Although the vast majority of them throughout the subcontinent were written in Prakrit, Afghanistan is notable for the inclusion of 2 Greek and Aramaic ones alongside the court language of the Mauryans.
Inscriptions made by the Mauryan Emperor Ashoka, a fragment of Edict 13 in Greek, as well as a full Edict, written in both Greek and Aramaic has been discovered in Kandahar. It is said to be written in excellent Classical Greek, using sophisticated philosophical terms. In this Edict, Ashoka uses the word Eusebeia ("Piety") as the Greek translation for the ubiquitous "Dharma" of his other Edicts written in Prakrit:
The last ruler in the region was probably Subhagasena (Sophagasenus of Polybius), who, in all probability, belonged to the Ashvaka (q.v.) background.
Greco-Bactrians.
In the middle of the 3rd century BC, an independent, Hellenistic state was declared in Bactria and eventually the control of the Seleucids and Mauryans was overthrown in western and southern Afghanistan. Graeco-Bactrian rule spread until it included a large territory which stretched from northeastern Iran in the west to the Punjab in India in the east by about 170 BC. Graeco-Bactrian rule was eventually defeated by a combination of internecine disputes that plagued Greek and Hellenized rulers to the west, continual conflict with Indian kingdoms, as well as the pressure of two groups of nomadic invaders from Central Asia—the Parthians and Sakas.
Kushan Empire (150 BC–300 AD).
In the 3rd and 2nd centuries BC, the Parthians, a nomadic Iranian peoples, arrived in ancient Afghanistan. The Parthians established control in most of what is now Iran as early as the middle of the 3rd century BC; about 100 years later another Indo-European group from the north—the Tocharian Kushans (a subgroup of the tribe called the Yuezhi by the Chinese)—entered the region Afghanistan and established an empire lasting almost four centuries.
The Kushan Empire spread from the Kabul River valley to defeat other Central Asian tribes that had previously conquered parts of the northern central Iranian Plateau once ruled by the Parthians. By the middle of the 1st century BC, the Kushans' base of control became Afghanistan and their empire spanned from the north of the Pamir mountains to the Ganges river valley in India. Early in the 2nd century under Kanishka, the most powerful of the Kushan rulers, the empire reached its greatest geographic and cultural breadth to become a center of literature and art. Kanishka extended Kushan control to the mouth of the Indus River on the Arabian Sea, into Kashmir, and into what is today the Chinese-controlled area north of Tibet. Kanishka was a patron of religion and the arts. It was during his reign that Buddhism, which was promoted in northern India earlier by the Mauryan emperor Ashoka (c. 260 BC–232 BC), reached its zenith in Central Asia. Though the Kushanas supported local Buddhists and Hindus as well as the worship of various local deities.
Sassanian invasion (300–650).
In the 3rd century, Kushan control fragmented into semi-independent kingdoms that became easy targets for conquest by the rising Iranian dynasty, the Sassanians (c. 224–561) which annexed Afghanistan by 300 AD. Sassanian control was tenuous at times as numerous challenges from Central Asian tribes led to instability and constant warfare in the region.
The disunited Kushan and Sassanian kingdoms were in a poor position to meet the threat of a new wave of nomadic, Indo-European invaders from the north. The Hephthalites (or White Huns) swept out of Central Asia around the 4th century into Bactria and to the south, overwhelming the last of the Kushan and Sassanian kingdoms. Some have speculated that the name "Afghanistan land of the Afghans" derives from which could be an adjective such as brave, chivlarious, valour, which was to use for the people in today's Afghanistan. Historians believe that Hepthalite control continued for a century and was marked by constant warfare with the Sassanians to the west who exerted nominal control over the region.
By the middle of the 6th century the Hephthalites were defeated in the territories north of the Amu Darya (the Oxus River of antiquity) by another group of Central Asian nomads, the Göktürks, and by the resurgent Sassanians in the lands south of the Amu Darya. It was the ruler of western Göktürks, Sijin (a.k.a. Sinjibu, Silzibul and Yandu Muchu Khan) who led the forces against the Hepthalites who were defeated at the Battle of Chach (Tashkent) and at the Battle of Bukhara.
Kabul Shahi.
The Shahi dynasties ruled portions of the Kabul Valley (in eastern Afghanistan) and the old province of Gandhara (northern Pakistan and Kashmir) from the decline of the Kushan Empire in the 3rd century to the early 9th century. They are split into two eras the Buddhist-Shahis and the later Hindu-Shahis with the change-over occurring around 870, and ruled up until the Islamic conquest of Afghanistan.
When Xuanzang visited the region early in the 7th century, the Kabul region was ruled by a Kshatriya king, who is identified as the "Shahi Khingal", and whose name has been found in an inscription found in Gardez. The Turkic Shahi regency was overthrown and replaced by a Mohyal Shahi dynasty of Brahmins who began the first phase of the Brahmana Hindu Shahi dynasty.
These Hindu kings of Kabul and Gandhara may have had links to some ruling families in neighboring Kashmir and other areas to the east. The Shahis were rulers of predominantly Buddhist, Zoroastrian, Hindu and Muslim populations and were thus patrons of numerous faiths, and various artifacts and coins from their rule have been found that display their multicultural domain. In 964 AD, the last Mohyal Shahi was succeeded by the Janjua overlord, Jayapala, of the Panduvanshi dynasty. The last Shahi emperors Jayapala, Anandapala and Tirlochanpala fought the Muslim Ghaznavids of Ghazna and were gradually defeated. Their remaining army were eventually exiled into northern India.
Archaeological remnants.
Most of the Zoroastrian, Greek, Hellenistic, Buddhist, Hindu and other indigenous cultures were replaced by the coming of Islam and little influence remains in Afghanistan today. Along ancient trade routes, however, stone monuments of the once flourishing Buddhist culture did exist as reminders of the past. The two massive sandstone Buddhas of Bamyan, thirty-five and fifty-three meters high overlooked the ancient route through Bamyan to Balkh and dated from the 3rd and 5th centuries. They survived until 2001, when they were destroyed by the Taliban. In this and other key places in Afghanistan, archaeologists have located frescoes, stucco decorations, statuary, and rare objects from as far away as China, Phoenicia, and Rome, which were crafted as early as the 2nd century and bear witness to the influence of these ancient civilizations upon Afghanistan.
One of the early Buddhist schools, the Mahāsāṃghika-Lokottaravāda, were known to be prominent in the area of Bamiyan. The Chinese Buddhist monk Xuanzang visited a Lokottaravāda monastery in the 7th century CE, at Bamiyan, Afghanistan, and this monastery site has since been rediscovered by archaeologists. Birchbark and palm leaf manuscripts of texts in this monastery's collection, including Mahāyāna sūtras, have been discovered at the site, and these are now located in the Schøyen Collection. Some manuscripts are in the Gāndhārī language and Kharoṣṭhī script, while others are in Sanskrit and written in forms of the Gupta script. Manuscripts and fragments that have survived from this monastery's collection include well-known Buddhist texts such as the "Mahāparinirvāṇa Sūtra" (from the "Āgamas"), the "Diamond Sūtra" ("Vajracchedikā Prajñāpāramitā"), the "Medicine Buddha Sūtra", and the "Śrīmālādevī Siṃhanāda Sūtra".
In 2010, reports stated that about 42 Buddhist relics have been discovered in the Logar Province of Afghanistan, which is south of Kabul. Some of these items date back to the 2nd century according to Archaeologists. The items included two Buddhist temples (Stupas), Buddha statues, frescos, silver and gold coins and precious beads."There is a temple, stupas, beautiful rooms, big and small statues, two with the length of seven and nine meters, colorful frescos ornamented with gold and some coins... Some of the relics date back to the fifth century (AD)... We have come across signs that there are items maybe going back to the era before Christ or prehistory... We need foreign assistance to preserve these and their expertise to help us with further excavations."—Mohammad Nader Rasouli, Afghan Archaeological Department
Chronological Chart for the historical periods of Afghanistan</div Scale><div id=ScaleBar style="width:1px; float:left; height:60em; padding:0; background-color:#242020" />em;
 height:2.8em;
 margin-left:0.9em;
 width:9.72em;
">em; 
">Herāt
em;
 height:2.8em;
 margin-left:10.8em;
 width:9.72em;
">em; 
">Bādghis
em;
 height:2.8em;
 margin-left:20.7em;
 width:9.72em;
">em; 
">Balkh<br>Badakhshān 
em;
 height:2.8em;
 margin-left:30.6em;
 width:9.72em;
">em; 
">Kābul
em;
 height:2.8em;
 margin-left:40.5em;
 width:9.72em;
">em; 
">Jalālābād
em;
 height:2.8em;
 margin-left:50.4em;
 width:9.72em;
">em; 
">Ghazni
em;
 height:2.8em;
 margin-left:60.3em;
 width:9.72em;
">em; 
">Kandahār
em;
 height:2.8em;
 margin-left:70.2em;
 width:9.72em;
">em; 
">Bust
em;
 height:2.8em;
 margin-left:80.1em;
 width:9.9em;
">em; 
">Seistān
em;
 height:6em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">Proto Elamite Culture<br>in Gardān Rīg and Dam (Nīmrūz Province)
em;
 height:5em;
 margin-left:30.6em;
 width:39.42em;
">em; 
">Indus Valley Civilization<br>in Mundigak and Deh Morāsi Ghūndai (Kandahār Province) and Shūrtūghai (Takhār Province)
em;
 height:3em;
 margin-left:0.9em;
 width:29.52em;
">em; 
">Bactria–Margiana Archaeological Complex<br>in Dashli (Jawzjān Province) and Tepe Fullol (Baghlān Province)
em;
 height:16em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Coming of Iranians<br><br>c.1700-1100 BC: The Rigveda, one of the oldest known texts written in an Indo-European language, is composed in a region described as "Sapta Sindhu" ('land of seven great rivers', which may correspond to the Kabul Valley).<br>c. 1350 BC: Migration of waves of Iranian tribes begin from the Bactria–Margiana Archaeological Complex westwards to the Iranian plateau, western Afghanistan and western Iran. According to the Avesta (Vendidad 1.1-21), they are compelled to leave their homeland "Airyana Vaēǰah" because Aŋra Mainyu so altered the climate that the winter became ten months long and the summer only two. Along the way, they settle down near large rivers, such as "Bāxδī", "Harōiva", "Haraxᵛaitī", etc. (See Avestan geography.)<br>c. 1100-550 BC: Zoroaster introduces a new religion at Bactra (Present-day Balkh) - Zoroastrianism - which spreads across Iranian plateau. He composes Older (i.e. 'Gathic') Avesta and later Younger Avesta is composed - at least - in Sīstān/Arachosia, Herāt, Merv and Bactria.
em;
 height:2em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Achaemenids
em;
 height:1em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Seleucids
em;
 height:1em;
 margin-left:80.1em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:40.5em;
 width:39.6em;
">em; 
">Mauryans
em;
 height:1em;
 margin-left:0.9em;
 width:19.8em;
">em; 
">em;
 height:1em;
 margin-left:20.7em;
 width:19.8em;
">em; 
">Greco-Bactrians
em;
 height:0em;
 margin-left:40.5em;
 width:49.5em;
">em; 
">em;
 height:0em;
 margin-left:30.6em;
 width:29.7em;
">em; 
">Indo-Greeks
em;
 height:5em;
 margin-left:0.9em;
 width:19.8em;
">em; 
">Parthians
em;
 height:2em;
 margin-left:80.1em;
 width:9.9em;
">em; 
">Parthians
em;
 height:0em;
 margin-left:60.3em;
 width:29.7em;
">em; 
">em;
 height:2em;
 margin-left:20.7em;
 width:9.9em;
">em; 
">Early Kushans
em;
 height:1em;
 margin-left:20.7em;
 width:39.6em;
">em; 
">Great Kushans
em;
 height:1em;
 margin-left:30.6em;
 width:49.5em;
">em; 
">Sakas
em;
 height:2em;
 margin-left:60.3em;
 width:29.7em;
">em; 
">Indo-Parthians
em;
 height:2em;
 margin-left:60.3em;
 width:29.7em;
">em; 
">Indo-Parthians
em;
 height:0em;
 margin-left:30.6em;
 width:59.4em;
">em; 
">em;
 height:5em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Sasanians
em;
 height:1em;
 margin-left:20.7em;
 width:29.7em;
">em; 
">Kushano-Sasanians 
em;
 height:0em;
 margin-left:20.7em;
 width:29.7em;
">em; 
em;
 height:1em;
 margin-left:20.7em;
 width:9.9em;
">em; 
">em;
 height:0em;
 margin-left:30.6em;
 width:39.6em;
">em; 
">Hephthalites
em;
 height:1em;
 margin-left:10.8em;
 width:9.9em;
">em; 
">em;
 height:2em;
 margin-left:0.9em;
 width:9.9em;
">em; 
">Arabs
em;
 height:2em;
 margin-left:80.1em;
 width:9.9em;
">em; 
">Arabs
em;
 height:1em;
 margin-left:9.0em;
 width:21.6em;
">em; 
">em;
 height:0em;
 margin-left:10.8em;
 width:19.8em;
">em; 
">Turkish Khanates
em;
 height:2em;
 margin-left:30.6em;
 width:39.6em;
">em; 
">Turki Shahis
em;
 height:1em;
 margin-left:70.2em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:40.5em;
 width:9.9em;
">em; 
">Hindu Shahis
em;
 height:1em;
 margin-left:30.6em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:50.4em;
 width:9.9em;
">em; 
">em;
 height:0em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">Kharijites
em;
 height:0em;
 margin-left:0.9em;
 width:29.7em;
">em; 
">Tahirids
em;
 height:0em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">Saffarids
em;
 height:0em;
 margin-left:60.3em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:0.9em;
 width:29.7em;
">em; 
">Samanids
em;
 height:1em;
 margin-left:60.3em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">Samanids
em;
 height:0em;
 margin-left:0.9em;
 width:29.7em;
">em; 
">em;
 height:2em;
 margin-left:30.6em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:40.5em;
 width:9.9em;
">em; 
">Ghaznavids
em;
 height:2em;
 margin-left:50.4em;
 width:9.9em;
">em; 
">em;
 height:2em;
 margin-left:60.3em;
 width:9.9em;
">em; 
">em;
 height:1em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">em;
 height:1em;
 margin-left:0.9em;
 width:29.7em;
">em; 
">Seljuks
em;
 height:0em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Ghurids
em;
 height:0em;
 margin-left:0.9em;
 width:89.1em;
">em; 
em;
 height:0em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">em;
 height:1em;
 margin-left:20.7em;
 width:49.5em;
">em; 
">Ilkhanate (Mongols)
em;
 height:0em;
 margin-left:20.7em;
 width:49.5em;
">em; 
">Chaghatais
em;
 height:0em;
 margin-left:0.9em;
 width:19.8em;
">em; 
">em;
 height:1em;
 margin-left:0.9em;
 width:19.8em;
">em; 
">Kartids
em;
 height:1em;
 margin-left:70.2em;
 width:19.8em;
">em; 
">Kayanis
em;
 height:1em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Timurids
em;
 height:1em;
 margin-left:20.7em;
 width:9.72em;
">em; 
em;
 height:1em;
 margin-left:20.7em;
 width:9.72em;
">em; 
">Khanate of Bukhara
em;
 height:3em;
 margin-left:30.6em;
 width:39.6em;
">em; 
">Mughals
em;
 height:0em;
 margin-left:20.7em;
 width:49.5em;
">em; 
">em;
 height:2em;
 margin-left:0.9em;
 width:19.62em;
">em; 
">Safavids
em;
 height:2em;
 margin-left:60.3em;
 width:29.7em;
">em; 
">Safavids
em;
 height:0em;
 margin-left:0.9em;
 width:19.8em;
">em; 
em;
 height:0em;
 margin-left:60.3em;
 width:29.7em;
">em; 
em;
 height:0em;
 margin-left:0.9em;
 width:89.1em;
">em; 
em;
 height:1em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Durrani Empire
em;
 height:0em;
 margin-left:0.9em;
 width:19.8em;
">em; 
em;
 height:0em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Barakzai dynasty
em;
 height:1em;
 margin-left:0.9em;
 width:89.1em;
">em; 
">Modern Afghanistan
</div Timeline>em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
">em;
"></div containsTSN></div Legend></div caption></div Container>

</doc>
<doc id="10346" url="http://en.wikipedia.org/wiki?curid=10346" title="Gravitational redshift">
Gravitational redshift

In astrophysics, gravitational redshift or Einstein shift is the process by which electromagnetic radiation originating from a source that is in a gravitational field is reduced in frequency, or redshifted, when observed in a region of a weaker gravitational field. This is a direct result of gravitational time dilation - as one moves away from a source of gravitational field, the rate at which time passes is increased relative to the case when one is near the source. As frequency is inverse of time (specifically, time required for completing one wave oscillation), frequency of the electromagnetic radiation is reduced in an area of a higher gravitational potential (i.e., equivalently, of lower gravitational field) . There is a corresponding reduction in energy when electromagnetic radiation is red-shifted, as given by Planck's relation, due to the electromagnetic radiation propagating in opposition to the gravitational gradient. There also exists a corresponding blueshift when electromagnetic radiation propagates from an area of a weaker gravitational field to an area of a stronger gravitational field.
If applied to optical wavelengths, this manifests itself as a change in the colour of visible light as the wavelength of the light is increased toward the red part of the light spectrum. Since frequency and wavelength are inversely proportional, this is equivalent to saying that the frequency of the light is reduced towards the red part of the light spectrum, giving this phenomenon the name redshift.
Definition.
Redshift is often denoted with the dimensionless variable formula_1, defined as the fractional change of the wavelength
formula_2
where
formula_3 is the wavelength of the electromagnetic radiation (photon) as measured by the observer.
formula_4 is the wavelength of the electromagnetic radiation (photon) when measured at the source of emission.
The gravitational redshift of a photon can be calculated in the framework of general relativity (using the Schwarzschild metric) as
formula_5
with the Schwarzschild radius
formula_6,
where formula_7 denotes Newton's gravitational constant, formula_8 the mass of the gravitating body, formula_9 the speed of light, and formula_10 the distance between the center of mass of the gravitating body and the point at which the photon is emitted. The redshift is not defined for photons emitted inside the Schwarzschild radius, the distance from the body where the escape velocity is greater than the speed of light. Therefore this formula only applies when formula_10 is at least as large as formula_12. When the photon is emitted at a distance equal to the Schwarzschild radius, the redshift will be infinitely large. When the photon is emitted at an infinitely large distance, there is no redshift.
In the Newtonian limit, i.e. when formula_10 is sufficiently large compared to the Schwarzschild radius formula_12, the redshift can be approximated by a binomial expansion to become
formula_15
History.
The gravitational weakening of light from high-gravity stars was predicted by John Michell in 1783 and Pierre-Simon Laplace in 1796, using Isaac Newton's concept of light corpuscles (see: emission theory) and who predicted that some stars would have a gravity so strong that light would not be able to escape. The effect of gravity on light was then explored by Johann Georg von Soldner (1801), who calculated the amount of deflection of a light ray by the sun, arriving at the Newtonian answer which is half the value predicted by general relativity. All of this early work assumed that light could slow down and fall, which was inconsistent with the modern understanding of light waves.
Once it became accepted that light is an electromagnetic wave, it was clear that the frequency of light should not change from place to place, since waves from a source with a fixed frequency keep the same frequency everywhere. One way around this conclusion would be if time itself were altered—if clocks at different points had different rates.
This was precisely Einstein's conclusion in 1911. He considered an accelerating box, and noted that according to the special theory of relativity, the clock rate at the bottom of the box was slower than the clock rate at the top. Nowadays, this can be easily shown in accelerated coordinates. The metric tensor in units where the speed of light is one is:
and for an observer at a constant value of r, the rate at which a clock ticks, R(r), is the square root of the time coefficient, R(r)=r. The acceleration at position r is equal to the curvature of the hyperbola at fixed r, and like the curvature of the nested circles in polar coordinates, it is equal to 1/r.
So at a fixed value of g, the fractional rate of change of the clock-rate, the percentage change in the ticking at the top of an accelerating box vs at the bottom, is:
The rate is faster at larger values of R, away from the apparent direction of acceleration. The rate is zero at r=0, which is the location of the acceleration horizon.
Using the principle of equivalence, Einstein concluded that the same thing holds in any gravitational field, that the rate of clocks R at different heights was altered according to the gravitational field g. When g is slowly varying, it gives the fractional rate of change of the ticking rate. If the ticking rate is everywhere almost this same, the fractional rate of change is the same as the absolute rate of change, so that:
Since the rate of clocks and the gravitational potential have the same derivative, they are the same up to a constant. The constant is chosen to make the clock rate at infinity equal to 1. Since the gravitational potential is zero at infinity:
where the speed of light has been restored to make the gravitational potential dimensionless.
The coefficient of the formula_20 in the metric tensor is the square of the clock rate, which for small values of the potential is given by keeping only the linear term:
and the full metric tensor is:
where again the c's have been restored. This expression is correct in the full theory of general relativity, to lowest order in the gravitational field, and ignoring the variation of the space-space and space-time components of the metric tensor, which only affect fast moving objects.
Using this approximation, Einstein reproduced the incorrect Newtonian value for the deflection of light in 1909. But since a light beam is a fast moving object, the space-space components contribute too. After constructing the full theory of general relativity in 1916, Einstein solved for the space-space components in a post-Newtonian approximation, and calculated the correct amount of light deflection – double the Newtonian value. Einstein's prediction was confirmed by many experiments, starting with Arthur Eddington's 1919 solar eclipse expedition.
The changing rates of clocks allowed Einstein to conclude that light waves change frequency as they move, and the frequency/energy relationship for photons allowed him to see that this was best interpreted as the effect of the gravitational field on the mass–energy of the photon. To calculate the changes in frequency in a nearly static gravitational field, only the time component of the metric tensor is important, and the lowest order approximation is accurate enough for ordinary stars and planets, which are much bigger than their Schwarzschild radius.
Initial verification.
A number of experimenters initially claimed to have identified the effect using astronomical measurements, and the effect was eventually considered to have been finally identified in the spectral lines of the star Sirius B by W.S. Adams in 1925. However, measurements of the effect before the 1960s have been critiqued by ("e.g.", by C.M. Will), and the effect is now considered to have been definitively verified by the experiments of Pound, Rebka and Snider between 1959 and 1965.
The Pound–Rebka experiment of 1959 measured the gravitational redshift in spectral lines using a terrestrial 57Fe gamma source. This was documented by scientists of the Lyman Laboratory of Physics at Harvard University. A commonly cited experimental verification is the Pound–Snider experiment of 1965. James W. Brault, a graduate student of Robert Dicke at Princeton University, measured the gravitational redshift of the sun using optical methods in 1962.
More information can be seen at Tests of general relativity.
Application.
Gravitational redshift is studied in many areas of astrophysical research.
Exact Solutions.
A table of exact solutions of the Einstein field equations consists of the following:
The more often used exact equation for gravitational redshift applies to the case outside of a non-rotating, uncharged mass which is spherically symmetric. The equation is:
formula_24, where
Gravitational redshift versus gravitational time dilation.
When using special relativity's relativistic Doppler relationships to calculate the change in energy and frequency (assuming no complicating route-dependent effects such as those caused by the frame-dragging of rotating black holes), then the gravitational redshift and blueshift frequency ratios are the inverse of each other, suggesting that the "seen" frequency-change corresponds to the actual difference in underlying clockrate. Route-dependence due to frame-dragging may come into play, which would invalidate this idea and complicate the process of determining globally agreed differences in underlying clock rate.
While gravitational redshift refers to what is seen, gravitational time dilation refers to what is deduced to be "really" happening once observational effects are taken into account.

</doc>
<doc id="10350" url="http://en.wikipedia.org/wiki?curid=10350" title="Easter Rising">
Easter Rising

The Easter Rising (Irish: "Éirí Amach na Cásca"), also known as the Easter Rebellion, was an armed insurrection in Ireland during Easter Week, 1916. The Rising was mounted by Irish republicans to end British rule in Ireland, secede from the United Kingdom of Great Britain and Ireland and establish an independent Irish Republic while the United Kingdom was heavily engaged in World War I. It was the most significant uprising in Ireland since the rebellion of 1798.
Organised by seven members of the Military Council of the Irish Republican Brotherhood, the Rising began on Easter Monday, 24 April 1916, and lasted for six days. Members of the Irish Volunteers — led by schoolmaster and Irish language activist Patrick Pearse, joined by the smaller Irish Citizen Army of James Connolly, along with 200 members of Cumann na mBan — seized key locations in Dublin and proclaimed the Irish Republic independent of the United Kingdom. There were actions in other parts of Ireland: however, except for the attack on the Royal Irish Constabulary barracks at Ashbourne, County Meath, they were minor.
With vastly superior numbers and artillery, the British army quickly suppressed the Rising, and Pearse agreed to an unconditional surrender on Saturday 29 April. Most of the leaders were executed following courts-martial, but the Rising succeeded in bringing physical force republicanism back to the forefront of Irish politics. Support for republicanism continued to rise in Ireland in the context of the ongoing war in Europe and the Middle East and revolutions in other countries, and especially as a result of the Conscription Crisis of 1918 and the failure of the British-sponsored Irish Convention.
In December 1918, republicans (by then represented by the Sinn Féin party) won 73 Irish seats out of 105 in the 1918 General Election to the British Parliament, on a policy of abstentionism and Irish independence. On 21 January 1919 they convened the First Dáil and declared the independence of the Irish Republic, and later that same day the Irish War of Independence began with the Soloheadbeg ambush.
Background.
The Acts of Union 1800 united the Kingdom of Great Britain and the Kingdom of Ireland as the United Kingdom of Great Britain and Ireland, abolishing the Irish Parliament and giving Ireland representation at Westminster. From early on, many Irish nationalists opposed the union as they saw it as an exploitation and impoverishment of their country.
Opposition took various forms: constitutional (the Repeal Association; the Home Rule League), social (disestablishment of the Church of Ireland; the Land League) and revolutionary (Rebellion of 1848; Fenian Rising). Constitutional nationalism seemed to be about to bear fruit when the Irish Parliamentary Party under Charles Stewart Parnell succeeded in having the First Home Rule Bill of 1886 introduced by the Liberal government of William Ewart Gladstone, but it was defeated in the House of Commons. The Second Home Rule Bill of 1893 was passed by the Commons but rejected by the House of Lords.
After the fall of Parnell, younger and more radical nationalists became disillusioned with parliamentary politics and turned toward more extreme forms of separatism. The Gaelic Athletic Association, the Gaelic League and the cultural revival under W. B. Yeats and Lady Augusta Gregory, together with the new political thinking of Arthur Griffith expressed in his newspaper "Sinn Féin" and the organisations the "National Council" and the "Sinn Féin League" led to the identification of Irish people with the concept of a Gaelic nation and culture, completely independent of Britain. This was sometimes referred to by the generic term "Sinn Féin".
The Third Home Rule Bill was introduced by British Prime Minister H. H. Asquith in 1912. The Irish Unionists, led by Sir Edward Carson, opposed home rule in what they saw as an impending Roman Catholic-dominated Dublin government. They formed the Ulster Volunteer Force on 13 January 1913, creating the first armed group of the Home Rule Crisis.
The Irish Republican Brotherhood (IRB) saw an opportunity to create an armed organisation to advance its own ends, and on 25 November 1913 the Irish Volunteers, whose stated object was "to secure and to maintain the rights and liberties common to all the people of Ireland", was formed. Its leader was Eoin MacNeill, who was not an IRB member. A Provisional Committee was formed that included people with a wide range of political views, and the Volunteers' ranks were open to "all able-bodied Irishmen without distinction of creed, politics or social group." Another militant group, the Irish Citizen Army, was formed by trade unionists as a result of the Dublin Lock-out of that year. The increasing militarisation of Irish politics was overshadowed soon after by the outbreak of the First World War — and Ireland's involvement in the conflict.
Though many Irishmen had volunteered for Irish regiments and divisions of the New British Army at the outbreak of war in 1914, the growing likelihood of enforced conscription created a backlash. Opposition to the war was based particularly on the implementation of the Government of Ireland Act 1914 (as previously recommended in March by the Irish Convention) increasingly and controversially linked with a "dual policy" enactment of the Military Service Bill, a dual policy that would require Irish conscription to begin if there would be any hope of Ireland seeing the implementation of the Government of Ireland Act 1914. The linking of conscription and Home Rule outraged the Irish secessionist parties at Westminster, including the IPP, the AFIL and others, who walked out in protest and returned to Ireland to organise opposition.
Planning the Rising.
The Supreme Council of the IRB met on 5 September 1914, just over a month after the UK government had declared war on Germany. At this meeting, they decided to stage a rising before the war ended and to accept whatever help Germany might offer. Responsibility for the planning of the rising was given to Tom Clarke and Seán MacDermott. The Irish Volunteers—the smaller of the two forces resulting from the September 1914 split over support for the British war effort—set up a "headquarters staff" that included Patrick Pearse as Director of Military Organisation, Joseph Plunkett as Director of Military Operations and Thomas MacDonagh as Director of Training. Éamonn Ceannt was later added as Director of Communications. In May 1915, Clarke and MacDermott established a Military Committee within the IRB, consisting of Pearse, Plunkett and Ceannt, to draw up plans for a rising. This dual rôle allowed the Committee, to which Clarke and MacDermott added themselves shortly afterward, to promote their own policies and personnel independently of both the Volunteer Executive and the IRB Executive—in particular Volunteer Chief of Staff Eoin MacNeill, who supported a rising only on condition of an increase in popular support following unpopular moves by the London government, such as the introduction of conscription or an attempt to suppress the Volunteers or its leaders, and IRB President Denis McCullough, who held similar views. IRB members held officer rank in the Volunteers throughout the country and would take their orders from the Military Committee, not from MacNeill.
Plunkett travelled to Germany in April 1915 to join Roger Casement, who had gone there from the United States the previous year with the support of Clan na Gael leader John Devoy, and after discussions with the German Ambassador in Washington, Count von Bernstorff, to try to recruit an "Irish Brigade" from among Irish prisoners of war and secure German support for Irish independence. Together, Plunkett and Casement presented a plan which involved a German expeditionary force landing on the west coast of Ireland, while a rising in Dublin diverted the British forces so that the Germans, with the help of local Volunteers, could secure the line of the River Shannon.
James Connolly—head of the Irish Citizen Army (ICA), a group of armed socialist trade union men and women—was unaware of the IRB's plans, and threatened to start a rebellion on his own if other parties failed to act. If they had gone it alone, the IRB and the Volunteers would possibly have come to their aid; however, the IRB leaders met with Connolly in January 1916 and convinced him to join forces with them. They agreed to act together the following Easter and made Connolly the sixth member of the Military Committee. Thomas MacDonagh would later become the seventh and final member.
Build-up to Easter Week.
In an effort to thwart informers and, indeed, the Volunteers' own leadership, Pearse issued orders in early April for three days of "parades and manoeuvres" by the Volunteers for Easter Sunday (which he had the authority to do, as Director of Organization). The idea was that the republicans within the organisation (particularly IRB members) would know exactly what this meant, while men such as MacNeill and the British authorities in Dublin Castle would take it at face value. However, MacNeill got wind of what was afoot and threatened to "do everything possible short of phoning Dublin Castle" to prevent the rising.
MacNeill was briefly convinced to go along with some sort of action when Mac Diarmada revealed to him that a shipment of German arms was about to land in County Kerry, planned by the IRB in conjunction with Roger Casement; he was certain that the authorities' discovery of such a shipment would inevitably lead to suppression of the Volunteers, thus the Volunteers were justified in taking defensive action, including the originally planned manoeuvres. Casement—disappointed with the level of support offered by the Germans—returned to Ireland on a German U-boat and was captured upon landing at Banna Strand in Tralee Bay. The arms shipment was lost when the German ship carrying it, "Aud", was scuttled after interception by the Royal Navy. The Germans had already attempted a landing, but the local Volunteers had failed to rendezvous with it, arriving at the wrong location.
The following day, MacNeill reverted to his original position when he found out that the ship carrying the arms had been scuttled. With the support of other leaders of like mind, notably Bulmer Hobson and The O'Rahilly, he issued a countermand to all Volunteers, cancelling all actions for Sunday. This succeeded in putting the rising off for only a day, although it greatly reduced the number of Volunteers who turned out.
British Naval Intelligence had been aware of the arms shipment, Casement's return, and the Easter date for the rising through radio messages between Germany and its embassy in the United States that were intercepted by the Navy and deciphered in Room 40 of the Admiralty. The information was passed to the Under-Secretary for Ireland, Sir Matthew Nathan, on 17 April, but without revealing its source, and Nathan was doubtful about its accuracy. When news reached Dublin of the capture of the "Aud" and the arrest of Casement, Nathan conferred with the Lord Lieutenant, Lord Wimborne. Nathan proposed to raid Liberty Hall, headquarters of the Citizen Army, and Volunteer properties at Father Matthew Park and at Kimmage, but Wimborne insisted on wholesale arrests of the leaders. It was decided to postpone action until after Easter Monday, and in the meantime Nathan telegraphed the Chief Secretary, Augustine Birrell, in London seeking his approval. By the time Birrell cabled his reply authorising the action, at noon on Monday 24 April 1916, the Rising had already begun.
The Rising in Dublin.
Easter Monday.
Early on Monday morning, 24 April 1916, roughly 1,200 Volunteers and Citizen Army members took over strongpoints in Dublin city centre. A joint force of about 400 Volunteers and Citizen Army gathered at Liberty Hall under the command of Commandant James Connolly.
The rebel headquarters was the General Post Office (GPO) where James Connolly, overall military commander and four other members of the Military Council: Patrick Pearse, Tom Clarke, Seán Mac Dermott and Joseph Plunkett were. After occupying the Post Office, the Volunteers hoisted two Republican flags and Pearse read a Proclamation of the Republic.
Elsewhere, rebel forces took up positions at the Four Courts, the centre of the Irish legal establishment, at Jacob's Biscuit Factory, Boland's Mill, the South Dublin Union hospital complex and the adjoining Distillery at Marrowbone Lane. Another contingent, under Michal Mallin, dug in on St. Stephen's Green.
Although it was lightly guarded, Volunteer and Citizen Army forces under Seán Connolly failed to take Dublin Castle, the centre of British rule in Ireland, shooting dead a police sentry and overpowering the soldiers in the guardroom, but failing to press home the attack. The Under-secretary, Sir Matthew Nathan, alerted by the shots, helped close the castle gates. The rebels occupied the Dublin City Hall and adjacent buildings. They also failed to take Trinity College, in the heart of the city centre and defended by only a handful of armed unionist students. At midday a small team of Volunteers and Fianna members attacked the Magazine Fort in the Phoenix Park and disarmed the guards, with the intent to seize weapons and blow up the building as a signal that the rising had begun. They set explosives but failed to obtain any arms.
In at least two incidents, at Jacob's and Stephen's Green, the Volunteers and Citizen Army shot dead civilians trying to attack them or dismantle their barricades. Elsewhere, they hit civilians with their rifle butts to drive them off.
The British military were caught totally unprepared by the rebellion and their response of the first day was generally un-coordinated. Two troops of British cavalry, one at the Four Courts and the other on O'Connell Street, sent to investigate what was happening took fire and casualties from rebel forces On Mount Street, a group of Volunteer Training Corps men stumbled upon the rebel position and four were killed before they reached Beggars Bush barracks.
The only substantial combat of the first day of the Rising took place at the South Dublin Union where a piquet from the Royal Irish Regiment encountered an outpost of Éamonn Ceannt's force at the north-western corner of the South Dublin Union. The British troops, after taking some casualties, managed to regroup and launch several assaults on the position before they forced their way inside and the small rebel force in the tin huts at the eastern end of the Union surrendered. However, the Union complex as a whole remained in rebel hands.
Three unarmed Dublin Metropolitan Police were shot dead on the first day of the Rising and their Commissioner pulled them off the streets. Partly as a result of the police withdrawal, a wave of looting broke out in the city centre, especially in the O'Connell Street area. A total of 425 people were arrested after the Rising for looting.
Tuesday to Saturday.
Lord Wimborne, the Lord Lieutenant, declared martial law on Tuesday evening and handed over civil power to Brigadier-General William Lowe. British forces initially put their efforts into securing the approaches to Dublin Castle and isolating the rebel headquarters, which they believed was in Liberty Hall. The British commander, Lowe, worked slowly, unsure of the size of the force he was up against, and with only 1,269 troops in the city when he arrived from the Curragh Camp in the early hours of Tuesday 25 April. City Hall was taken from the rebel unit that had attacked Dublin Castle on Tuesday morning.
The rebels had failed to take either of Dublin's two main train stations or either of its ports, at Dublin Port and Kingstown. As a result, during the following week, the British were able to bring in thousands of reinforcements from England and from their garrisons at the Curragh and Belfast. By the end of the week, British strength stood at over 16,000 men. Their firepower was provided by field artillery summoned from their garrison at Athlone which they positioned on the northside of the city at Phibsborough and at Trinity College, and by the patrol vessel "Helga", which sailed up the Liffey, having been summoned from the port at Kingstown. On Wednesday, 26 April, the guns at Trinity College and "Helga" shelled Liberty Hall, and the Trinity College guns then began firing at rebel positions, first at Boland's Mill and then in O'Connell Street.
The principal rebel positions at the GPO, the Four Courts, Jacob's Factory and Boland's Mill saw little combat. The British surrounded and bombarded them rather than assault them directly. One Volunteer in the GPO recalled, "we did practically no shooting as there was no target". Similarly, the rebel position at St Stephen's Green, held by the Citizen Army under Michael Mallin, was made untenable after the British placed snipers and machine guns in the Shelbourne Hotel and surrounding buildings. As a result, Mallin's men retreated to the Royal College of Surgeons building where they remained for the rest of the week. However, where the insurgents dominated the routes by which the British tried to funnel reinforcements into the city, there was fierce fighting.
Reinforcements were sent to Dublin from England, and disembarked at Kingstown on the morning of 26 April. Heavy fighting occurred at the rebel-held positions around the Grand Canal as these troops advanced towards Dublin. The Sherwood Foresters were repeatedly caught in a cross-fire trying to cross the canal at Mount Street. Seventeen Volunteers were able to severely disrupt the British advance, killing or wounding 240 men. Despite there being alternative routes across the canal nearby, General Lowe ordered repeated frontal assaults on the Mount Street position. The British eventually took the position, which had not been reinforced by the nearby rebel garrison at Boland's Mills, on Thursday but the fighting there inflicted up to two thirds of their casualties for the entire week for a cost of just four dead Volunteers.
The rebel position at the South Dublin Union (site of the present day St. James's Hospital) and Marrowbone Lane, further west along the canal, also inflicted heavy losses on British troops. The South Dublin Union was a large complex of buildings and there was vicious fighting around and inside the buildings. Cathal Brugha, a rebel officer, distinguished himself in this action and was badly wounded. By the end of the week, the British had taken some of the buildings in the Union, but others remained in rebel hands. British troops also took casualties in unsuccessful frontal assaults on the Marrowbone Lane Distillery.
The third major scene of combat during the week was at North King Street, behind the Four Courts, where the British, on Thursday, tried to take a well-barricaded rebel position. By the time of the rebel headquarter's surrender, the South Staffordshire Regiment under Colonel Taylor had advanced only 150 yd down the street at a cost of 11 dead and 28 wounded. The enraged troops broke into the houses along the street and shot or bayonetted 15 male civilians whom they accused of being rebel fighters.
Elsewhere, at Portobello Barracks, an officer named Bowen Colthurst summarily executed six civilians, including the pacifist nationalist activist, Francis Sheehy-Skeffington. These instances of British troops killing Irish civilians would later be highly controversial in Ireland.
Surrender.
The headquarters garrison at the GPO, after days of shelling, was forced to abandon their headquarters when fire caused by the shells spread to the GPO. Connolly had been incapacitated by a bullet wound to the ankle and had passed command on to Pearse. The O'Rahilly was killed in a sortie from the GPO. They tunnelled through the walls of the neighbouring buildings in order to evacuate the Post Office without coming under fire and took up a new position in 16 Moore Street. On Saturday 29 April, from this new headquarters, after realising that they could not break out of this position without further loss of civilian life, Pearse issued an order for all companies to surrender. Pearse surrendered unconditionally to Brigadier-General Lowe. The surrender document read:
In order to prevent the further slaughter of Dublin citizens, and in the hope of saving the lives of our followers now surrounded and hopelessly outnumbered, the members of the Provisional Government present at headquarters have agreed to an unconditional surrender, and the commandants of the various districts in the City and County will order their commands to lay down arms.
The GPO was the only major rebel post to be physically taken during the week. The others surrendered only after Pearse's surrender order, carried by a nurse named Elizabeth O'Farrell, reached them. Sporadic fighting therefore continued until Sunday, when word of the surrender was got to the other rebel garrisons. Command of British forces had passed from Lowe to General John Maxwell, who arrived in Dublin just in time to take the surrender. Maxwell was made temporary military governor of Ireland.
The Rising outside Dublin.
Irish Volunteer units mobilised on Easter Sunday in several places outside of Dublin, but due to Eoin MacNeill's countermanding order, most of them returned home without fighting. In addition, due to the interception of the German arms aboard the "Aud", the provincial Volunteer units were very poorly armed.
In the south, around 1,200 Volunteers mustered in Cork, under Tomás Mac Curtain on the Sunday, but they dispersed after receiving nine contradictory orders by dispatch from the Volunteer leadership in Dublin. Much to the anger of many Volunteers, MacCurtain, under pressure from Catholic clergy, agreed to surrender his men's arms to the British on Wednesday. The only violence in Cork occurred when the Kent family resisted arrest by the RIC, shooting one. One brother was killed in the shootout and another later executed.
Similarly, in the north, several Volunteer companies were mobilised at Coalisland in County Tyrone including 132 men from Belfast led by IRB President Dennis McCullough. However, in part due to the confusion caused by the countermanding order, the Volunteers there dispersed without fighting.
Ashbourne.
The only large-scale engagement outside the city of Dublin occurred at Ashbourne, County Meath. The Volunteers′ Dublin Brigade, 5th Battalion (also known as the Fingal Battalion), led by Thomas Ashe and his second in command Richard Mulcahy, composed of some 60 men, mobilised at Swords, where they seized the RIC Barracks and the Post Office. They did the same in the nearby villages of Donabate and Garristown before attacking the RIC barracks at Ashtown
During the attack on the barracks, an RIC patrol from Slane happened upon the firefight – leading to a five-hour gun battle, in which eight RIC constables were killed and 15 wounded. Two Volunteers were also killed and five wounded. One civilian was also mortally wounded. Ashe's men camped at Kilsalaghan, near Dublin until they received orders to surrender on Saturday.
Volunteer contingents also mobilised nearby in counties Meath and Louth, but proved unable to link up with the North Dublin unit until after it had surrendered. In County Louth, Volunteers shot dead an RIC man near the village of Castlebellingham on 24 April, in an incident in which 15 RIC men were also taken prisoner.
Enniscorthy.
In County Wexford, some 100 Volunteers led by Robert Brennan, Seamus Doyle and J R Etchingham took over Enniscorthy on Thursday 27 April until the following Sunday. They made a brief and unsuccessful attack on the RIC barracks, but unable to take it, resolved to blockade it instead. During their occupation of the town, they made such gestures as flying the tricolour over the Atheneum theatre, which they had made their headquarters, and parading uniformed in the streets.
A small party set off for Dublin, but turned back when they met a train full of British troops (part of a 1,000-strong force, which included the Connaught Rangers) on their way to Enniscorthy. On Saturday, two Volunteer leaders were escorted by the British to Arbour Hill Prison, where Pearse ordered them to surrender.
Galway.
In the west, Liam Mellows led 600–700 Volunteers in abortive attacks on several police stations, at Oranmore and Clarinbridge in County Galway. There was also a skirmish at Carnmore in which one RIC man (Constable Patrick Whelan) was killed. However, his men were poorly armed, with only 25 rifles and 300 shotguns, many of them being equipped only with pikes. Toward the end of the week, Mellows′ followers were increasingly poorly fed and heard that large British reinforcements were being sent westwards. In addition, the British cruiser HMS "Gloucester" arrived in Galway Bay and shelled the fields around Athenry where the rebels were based.
On 29 April, the Volunteers, judging the situation to be hopeless, dispersed from the town of Athenry. Many of these Volunteers were arrested in the period following the rising, while others, including Mellows had to go "on the run" to escape. By the time British reinforcements arrived in the west, the rising there had already disintegrated.
Casualties.
The British Army reported casualties of 116 dead, 368 wounded and nine missing. Sixteen policemen died, and 29 were wounded. Rebel and civilian casualties were 318 dead and 2,217 wounded. The Volunteers and ICA recorded 64 killed in action, but otherwise Irish casualties were not divided into rebels and civilians. All 16 police fatalities and 22 of the British soldiers killed were Irishmen British families came to Dublin Castle in May 1916 to reclaim the bodies and funerals were arranged. British bodies which were not claimed were given military funerals in Grangegorman Military Cemetery.
The majority of the casualties, both killed and wounded, were civilians. Both sides, British and rebel, shot civilians deliberately on occasion when they refused to obey orders such as to stop at checkpoints. On top of that, there were two instances of British troops killing civilians out of revenge or frustration, at Portobello Barracks, where six were shot and North King Street, where 15 were killed.
However, the majority of civilian casualties were killed by indirect fire from artillery, heavy machine guns and incendiary shells. The British, who used such weapons extensively, therefore seem to have caused most non-combatant deaths. One Royal Irish Regiment officer recalled, "they [British troops] regarded everyone as an enemy and fired at everything that moved".
Aftermath.
Arrests and executions.
General Maxwell quickly signalled his intention "to arrest all dangerous Sinn Feiners", including "those who have taken an active part in the movement although not in the present rebellion", reflecting the popular belief that Sinn Féin, a separatist organisation that was neither militant nor republican, was behind the Rising.
A total of 3,430 men and 79 women were arrested, although most were subsequently released. In attempting to arrest members of the Kent family in County Cork on 2 May, a Head Constable was shot dead in a gun battle. Richard Kent was also killed, and Thomas and William Kent were arrested.
In a series of courts martial beginning on 2 May, 90 people were sentenced to death. Fifteen of those (including all seven signatories of the Proclamation) had their sentences confirmed by Maxwell and were executed at Kilmainham Gaol by firing squad between 3 and 12 May (among them the seriously wounded Connolly, shot while tied to a chair due to a shattered ankle). Not all of those executed were leaders: Willie Pearse described himself as "a personal attaché to my brother, Patrick Pearse"; John MacBride had not even been aware of the Rising until it began, but had fought against the British in the Boer War fifteen years before; Thomas Kent did not come out at all—he was executed for the killing of a police officer during the raid on his house the week after the Rising. The most prominent leader to escape execution was Éamon de Valera, Commandant of the 3rd Battalion, who did so partly due to his American birth. The president of the courts martial was Charles Blackader.
1,480 men were interned in England and Wales under Regulation 14B of the Defence of the Realm Act 1914, many of whom, like Arthur Griffith, had little or nothing to do with the affair. Camps such as Frongoch internment camp became "Universities of Revolution" where future leaders like Michael Collins, Terence McSwiney and J. J. O'Connell began to plan the coming struggle for independence. The executions of those Rising leaders condemned to death took place over a nine-day period:
Sir Roger Casement was tried in London for high treason and hanged at Pentonville Prison on 3 August.
Inquiry.
A Royal Commission was set up to enquire into the causes of the Rising. It began hearings on 18 May under the chairmanship of Lord Hardinge of Penshurst. The Commission heard evidence from Sir Matthew Nathan, Augustine Birrell, Lord Wimborne, Sir Neville Chamberlain (Inspector-General of the Royal Irish Constabulary), General Lovick Friend, Major Ivor Price of Military Intelligence and others. The report, published on 26 June, was critical of the Dublin administration, saying that "Ireland for several years had been administered on the principle that it was safer and more expedient to leave the law in abeyance if collision with any faction of the Irish people could thereby be avoided." Birrell and Nathan had resigned immediately after the Rising. Wimborne had also reluctantly resigned, recalled to London by Lloyd George, but was re-appointed in late 1917. Chamberlain resigned soon after.
Reaction of the Dublin public.
At first, many members of the Dublin public were simply bewildered by the outbreak of the Rising. James Stephens, who was in Dublin during the week, thought, "None of these people were prepared for Insurrection. The thing had been sprung on them so suddenly they were unable to take sides".
There was considerable hostility towards the Volunteers in some parts of the city. When occupying positions in the South Dublin Union and Jacob's factory, the rebels got involved in physical confrontations with civilians trying to prevent them from taking over the buildings. The Volunteers′ shooting and clubbing of civilians made them extremely unpopular in these localities. There was outright hostility to the Volunteers from the "separation women" (so-called because they were paid "Separation Money" by the British government), who had husbands and sons fighting in the British Army in World War I, and among unionists. Supporters of the Irish Parliamentary Party also felt the rebellion was a betrayal of their party.
That the Rising caused a great deal of death and destruction, as well as disrupting food supplies also contributed to the antagonism toward the rebels. After the surrender, the Volunteers were hissed at, pelted with refuse, and denounced as "murderers" and "starvers of the people". Volunteer Robert Holland for example remembered being "subjected to very ugly remarks and cat-calls from the poorer classes" as they marched to surrender. He also reported being abused by people he knew as he was marched through the Kilmainham area into captivity and said the British troops saved them from being manhandled by the crowd.
However, there was not universal hostility towards the defeated insurgents. Some onlookers were cowed rather than hostile and it appeared to the Volunteers that some of those watching in silence were sympathetic. Canadian journalist and writer Frederick Arthur McKenzie wrote that in poorer areas, "there was a vast amount of sympathy with the rebels, particularly after the rebels were defeated." Thomas Johnson, the Labour leader, thought there was, "no sign of sympathy for the rebels, but general admiration for their courage and strategy"
The aftermath of the Rising, and in particular the British reaction to it, helped sway a large section of Irish nationalist opinion away from hostility or ambivalence and towards support for the rebels of Easter 1916. Dublin businessman and Quaker, James G. Douglas, for example, hitherto a Home Ruler, wrote that his political outlook changed radically during the course of the Rising due to the British military occupation of the city and that he became convinced that parliamentary methods would not be sufficient to remove the British presence.
Rise of Sinn Féin.
A meeting called by Count Plunkett on 19 April 1917 led to the formation of a broad political movement under the banner of Sinn Féin which was formalised at the Sinn Féin Ard Fheis of 25 October 1917. The Conscription Crisis of 1918 further intensified public support for Sinn Féin before the general elections to the British Parliament on 14 December 1918, which resulted in a landslide victory for Sinn Féin, whose MPs gathered in Dublin on 21 January 1919 to form Dáil Éireann and adopt the Declaration of Independence.
Legacy.
Shortly after the Easter Rising, poet Francis Ledwidge wrote "O’Connell Street" and "Lament for the Poets of 1916," which both describe his sense of loss and an expression of holding the same "dreams", as the Easter Rising's Irish Republicans. He would also go on to write "lament for Thomas MacDonagh" for his fallen friend and fellow Irish Volunteer.
A few months after the Easter Rising, W. B. Yeats commemorated some of the fallen figures of the Irish Republican movement, as well as his torn emotions regarding these events, in the poem "Easter, 1916".
Some of the survivors of the Rising went on to become leaders of the independent Irish state. Those who were executed were venerated by many as martyrs; their graves in Dublin's former military prison of Arbour Hill became a national monument and the Proclamation text was taught in schools. An annual commemorative military parade was held each year on Easter Sunday, culminating in a huge national celebration on the 50th anniversary in 1966. RTÉ the Irish national broadcaster, as one of its first major undertakings made a series of commemorative programmes for the 1966 anniversary of the Rising. Roibéárd Ó Faracháin, head of programming said, "While still seeking historical truth, the emphasis will be on homage, on salutation."
With the outbreak of the Troubles in Northern Ireland, government, academics and the media began to revise the country's militant past, and particularly the Easter Rising. The coalition government of 1973—77, in particular the Minister for Posts and Telegraphs, Conor Cruise O'Brien, began to promote the view that the violence of 1916 was essentially no different from the violence then taking place in the streets of Belfast and Derry.
O'Brien and others asserted that the Rising was doomed to military defeat from the outset, and that it failed to account for the determination of Ulster Unionists to remain in the United Kingdom.
Irish republicans continue to venerate the Rising and its leaders with murals in republican areas of Belfast and other towns celebrating the actions of Pearse and his comrades, and annual parades in remembrance of the Rising. The Irish government, however, discontinued its annual parade in Dublin in the early 1970s, and in 1976 it took the unprecedented step of proscribing (under the Offences against the State Act) a 1916 commemoration ceremony at the GPO organised by Sinn Féin and the Republican commemoration Committee. A Labour Party TD, David Thornley, embarrassed the government (of which Labour was a member) by appearing on the platform at the ceremony, along with Máire Comerford, who had fought in the Rising, and Fiona Plunkett, sister of Joseph Plunkett.
With the advent of a Provisional IRA ceasefire and the beginning of what became known as the Peace Process during the 1990s, the official view of the Rising grew more positive and in 1996 an 80th anniversary commemoration at the Garden of Remembrance in Dublin was attended by the Taoiseach and leader of Fine Gael, John Bruton. In 2005, the Taoiseach, Bertie Ahern, announced the government's intention to resume the military parade past the GPO from Easter 2006, and to form a committee to plan centenary celebrations in 2016. The 90th anniversary was celebrated with a military parade in Dublin on Easter Sunday, 2006, attended by the President of Ireland, the Taoiseach and the Lord Mayor of Dublin. There is now an annual ceremony at Easter attended by relatives of those who fought, by the President, the Taoiseach, ministers, senators and TDs, and by usually large and respectful crowds.
In December 2014 Dublin City Council approved a proposal to create a historical path commemorating the Rising, similar to the Freedom Trail in Boston. Lord Mayor of Dublin Christy Burke announced that the council had committed to building the trail, marking it with a green line or bricks, with brass plates marking the related historic sites such as the Rotunda and the General Post Office.
Dates for the Centenary of the 1916 Easter Rising.
The Easter Rising lasted from Easter Monday 24 April 1916 to Easter Saturday 29 April 1916, so the actual calendrical centenary is from 24 to 29 April 2016. But 1916 commemorations normally occur at Easter, which is a moveable feast, rather than at the calendrical anniversary. In 2016, Easter falls on Sunday , with Easter Monday on 28 March 2016 and Easter Saturday on 2 April 2016.
Bibliography.
</dl>
External links.
 Media related to at Wikimedia Commons

</doc>
<doc id="10353" url="http://en.wikipedia.org/wiki?curid=10353" title="Eschrichtiidae">
Eschrichtiidae

Eschrichtiidae or the gray whales is a family of baleen whale (suborder Mysticeti) with a single extant species, the gray whale ("Eschrichtius robustus"). The family, however, also includes three described fossil genera: "Archaeschrichtius" and "Eschrichtioides" from the Miocene and Pliocene of Italy respectively, and "Gricetoides" from the Pliocene of North Carolina. The names of the extant genus and the family honours Danish zoologist Daniel Eschricht.
Taxonomic history.
A number of 18th century authors described the gray whale as "Balaena gibbosa", the "whale with six bosses", apparently based on a brief note by :
The Scrag Whale is near a kin to the Fin-back, but instead of a Fin upon his Back, the Ridge of the Afterpart of his Back is cragged with half a Dozen Knobs or Nuckles; he is nearest the right Whale in Figure and for Quantity of Oil; his Bone is white, but won't split.
The gray whale was first described as a distinct species by based on a subfossil found in the brackish Baltic Sea, apparently a specimen from the now extinct north Atlantic population. Lilljeborg, however, identified it as "Balaenoptera robusta", a species of rorqual whale. realized that the rib and scapula of the specimen was different from those of any known rorquals, and therefore erected a new genus for it, "Eschrichtius".
 were convinced that the bones described by Lilljeborg could not belong to a living species but that they were similar to fossils that Van Beneden had described from the harbour of Antwerp (most of his named species are now considered nomina dubia) and therefore named the gray whale "Plesiocetus robustus", reducing Lilljeborg's and Gray's names to synonyms.
 produced one of the earliest descriptions of living Pacific gray whales, and notwithstanding that he was among the whalers who nearly drove them to extinction in the lagoons of the Baja California, they were and still are associated with him and his description of the species. At this time, however, the extinct Atlantic population was considered a separate species ("Eschrischtius robustus") from the living Pacific population ("Rhachianectes glaucus").
Things got increasingly confused as 19th century scientists introduced new species at an alarming rate (e.g. "Eschrichtius pusillus", "E. expansus", "E. priscus", "E. mysticetoides"), often based on fragmentary specimens, and taxonomists started to use several generic and specific names interchangeably and not always correctly (e.g. "Agalephus gobbosus", "Balaenoptera robustus", "Agalephus gibbosus"). Things got even worse in the 1930s when it was finally realised that the extinct Atlantic population was the same species as the extant Pacific population, and the new combination "Eschrichtius gibbosus" was proposed.
In his morphological analysis, found that eschrichtiids and Cetotheriidae ("Cetotherium", "Mixocetus" and "Metopocetus") form a monophyletic sister group of Balaenopteridae.
A specimen from the Late Pliocene of northern Italy, named ""Cetotherium" gastaldii" by and renamed ""Balaenoptera" gastaldii" by , was identified as a basal eschrichtiid by who recombined it to "Eschrichtioides gastaldii".
 found that the gray whale is phylogenetically distinct from rorqual whales and that previous morphological studies were correct in the conclusion that the evolution of gulp feeding was a single event in the rorqual lineage.
Evolution.
Fossils of Eschrichtiidae have been found in all major oceanic basins in the Northern Hemisphere, and the family is believed date back to the Late Miocene. Today, gray whales are only present in the northern Pacific, but a population was also present in the northern Atlantic before being driven to extinction by European whalers three centuries ago.
Fossil eschrichtiids from before the Holocene are rare compared to other fossil mysticetes. The only Pleistocene fossil from the Pacific referred to "E. eschrichtius" is a partial skeleton and an associated skull from California, estimated to be about 200 thousand years old. However, a late Pliocene fossil from Hokkaido, Japan, referred to "Eschrichtius" sp. is estimated to be 3.9 and a similar unnamed fossil has been reported from California.
In their description of "Archaeschrichtius ruggieroi" from the late Miocene of Italy, argued that eschrichtiids most likely originated in the Mediterranean Basin about million years ago and remained there, either permanently or intermittently, at least until the Early Pliocene (5–3 Mya), (but see Messinian salinity crisis.)
References.
Sources.
</dl>

</doc>
<doc id="10354" url="http://en.wikipedia.org/wiki?curid=10354" title="Edmund I">
Edmund I

Edmund I (Old English: "Ēadmund"; 921 – 26 May 946), called "the Elder", "the Deed-doer", "the Just", or "the Magnificent", was King of the English from 939 until his death. He was a son of Edward the Elder and half-brother of Æthelstan. Æthelstan died on 27 October 939, and Edmund succeeded him as king.
Military threats.
Edmund came to the throne as the son of Edward the Elder, grandson of Alfred the Great, great-grandson of Æthelwulf of Wessex, great-great grandson of Egbert of Wessex and great-great-great grandson of Ealhmund of Kent. Shortly after his proclamation as king, he had to face several military threats. King Olaf III Guthfrithson conquered Northumbria and invaded the Midlands; when Olaf died in 942, Edmund reconquered the Midlands. In 943, Edmund became the god-father of King Olaf of York. In 944, Edmund was successful in reconquering Northumbria. In the same year, his ally Olaf of York lost his throne and left for Dublin in Ireland. Olaf became the king of Dublin as Amlaíb Cuarán and continued to be allied to his god-father. In 945, Edmund conquered Strathclyde but ceded the territory to King Malcolm I of Scotland in exchange for a treaty of mutual military support. Edmund thus established a policy of safe borders and peaceful relationships with Scotland. During his reign, the revival of monasteries in England began.
Louis IV of France.
One of Edmund's last political movements of which there is some knowledge is his role in the restoration of Louis IV of France to the throne. Louis, son of Charles the Simple and Edmund's half-sister Eadgifu, had resided at the West-Saxon court for some time until 936, when he returned to be crowned King of France. In the summer of 945, he was captured by the Norsemen of Rouen and subsequently released to Duke Hugh the Great, who held him in custody. The chronicler Richerus claims that Eadgifu wrote letters both to Edmund and to Otto I, Holy Roman Emperor in which she requested support for her son. Edmund responded to her plea by sending angry threats to Hugh, who brushed them aside. Flodoard's "Annales", one of Richerus' sources, report:
Edmund, king of the English, sent messengers to Duke Hugh about the restoration of King Louis, and the duke accordingly made a public agreement with his nephews and other leading men of his kingdom. [...] Hugh, duke of the Franks, allying himself with Hugh the Black, son of Richard, and the other leading men of the kingdom, restored to the kingdom King Louis.
Death and succession.
On 26 May 946, Edmund was murdered by Leofa, an exiled thief, while attending St Augustine's Day mass in Pucklechurch (South Gloucestershire). John of Worcester and William of Malmesbury add some lively detail by suggesting that Edmund had been feasting with his nobles, when he spotted Leofa in the crowd. He attacked the intruder in person, but in the event, Leofa killed him. Leofa was killed on the spot by those present.
Edmund's sister Eadgyth, the wife of Otto I, Holy Roman Emperor, died earlier the same year, as Flodoard's "Annales" for 946 report.
Edmund was succeeded as king by his brother Eadred, king from 946 until 955. Edmund's sons later ruled England as:

</doc>
<doc id="10356" url="http://en.wikipedia.org/wiki?curid=10356" title="Endothermic process">
Endothermic process

In thermodynamics, the term endothermic process describes a process or reaction in which the system absorbs energy from its surroundings in the form of (usually, but not always) heat. The term was coined by Marcellin Berthelot from the Greek roots "endo-", derived from the word "endon" (ἔνδον) meaning "within" and the root "therm" (θερμ-) meaning "hot." The intended sense is that of a reaction that depends on taking in heat if it is to proceed. The opposite of an endothermic process is an exothermic process, one that releases, "gives out" energy in the form of (usually, but not always) heat. Thus in each term (endothermic & exothermic) the prefix refers to where heat goes as the reaction occurs, though in reality it only refers to where the energy goes, which need not necessarily be heat.
The concept is frequently applied in physical sciences to, for example, chemical reactions, where thermal energy (heat) is converted to chemical bond energy.
Endothermic (and exothermic) analysis only accounts for the enthalpy (∆H) change of a reaction. The full energy analysis of a reaction is the Gibbs free energy (∆G), which includes an entropy (∆S) and temperature term in addition to the enthalpy. A reaction will be a spontaneous process at a certain temperature if the products have a lower Gibbs free energy (an exergonic reaction) even if the enthalpy of the products is higher. Entropy and enthalpy are different terms, so the change in entropic energy can overcome an opposite change in enthalpic energy and make an endothermic reaction favorable.

</doc>
<doc id="10357" url="http://en.wikipedia.org/wiki?curid=10357" title="Earle Page">
Earle Page

 
Sir Earle Christmas Grafton Page, GCMG, CH (8 August 1880 – 20 December 1961) was an Australian politician who served as the 11th Prime Minister of Australia in 1939. To date, he is the second-longest serving federal parliamentarian in Australian history, with 41 years, 361 days in Parliament.
Early life.
Born in Grafton, New South Wales, Page was educated at Sydney Boys High School and the University of Sydney, where he graduated in medicine at the top of his year in 1901.
Page worked at Sydney's Royal Prince Alfred Hospital, where he met Ethel Blunt, a nurse, whom he married in 1906.
Pre-political career.
In 1903, Page joined a private practice in Grafton; and, in 1904, he became one of the first people in the country to own a car. He practised in Sydney and Grafton before joining the Australian Army as a medical officer in the First World War, serving in Egypt.
After the war, Page went into farming and was elected Mayor of Grafton.
Political career.
In 1919 Page was elected to the House of Representatives from Cowper in northeastern New South Wales. He ran as a candidate of the Farmers and Settlers Association of New South Wales, one of several farmers' groups that won seats in that election. Shortly before parliament opened in 1920, the Farmers and Settlers Association merged with several other rural-based parties to form the Country Party. He became the party's leader in 1921, ousting William McWilliams. The young party found itself with the balance of power in the House after the 1922 election. The Nationalist government of Billy Hughes lost its majority, and could not govern without Country Party support. However, the Country Party had been formed partly due to discontent with Hughes' rural policy, and Page's animosity toward Hughes was such that he would not even consider supporting him. When it was apparent that the Nationalists would have to turn to the Country Party in order to stay in office, Page demanded and got Hughes' resignation as the price for supporting the Nationalist government.
Page then began negotiations with Hughes' successor as leader of the Nationalists, Stanley Bruce. His terms were stiff; he wanted his Country Party to have five seats in an 11-man cabinet, including the post of Treasurer and the second rank in the ministry for himself. These demands were unprecedented for such a new party in a Westminster system. Nonetheless, as the Country Party was the Nationalists' only realistic coalition partner, Bruce accepted Page's terms. For all intents and purposes, Page was the first Deputy Prime Minister of Australia (a title that did not officially exist until 1968). Since then, the leader of the Country/National Party has been the second-ranking member in nearly every non-Labor government.
Page continued his professional medical practice. On 22 October 1924, he had to tell his best friend, Thomas Shorten Cole (1870–1957), the news that his wife Mary Ann Crane had just died on the operating table from complications of intestinal or stomach cancer, reputed by their daughter Dorothy May Cole to be "the worst day of his life".
He was a strong believer in orthodox finance and conservative policies, except where the welfare of farmers was concerned: then he was happy to see government money spent freely. He was also a "high protectionist": a supporter of high tariff barriers to protect Australian rural industries.
The Bruce-Page government was defeated by Labor in 1929 (with Bruce losing his own seat), and Page went into opposition. In 1931, a group of dissident Labor MPs led by Joseph Lyons merged with the Nationalists to form the United Australia Party, with Lyons as leader. Page and the Country Party continued in coalition with the UAP. The UAP-Country Coalition won a comprehensive victory in the 1931 election. However, the UAP was in a strong enough position (only four seats short of a majority) that Lyons was able to form an exclusively UAP minority government with confidence and supply support from the Country Party—to date, the last time that the party has not had any posts in a non-Labor government. In 1934, however, the UAP suffered an eight-seat swing, forcing Lyons to take the Country Party back into his government in a full-fledged Coalition. Page became Minister for Commerce. He was made a Knight Grand Cross of the Order of St Michael and St George (GCMG) in the New Year’s Day Honours of 1938.
Prime Minister.
When Lyons died suddenly in 1939, the Governor-General Lord Gowrie appointed Page as caretaker Prime Minister. He held the office for three weeks until the UAP elected former deputy leader Robert Menzies as its new leader, and hence Prime Minister. While ten Australian Prime Ministers were knighted (and Bruce was elevated to the peerage), Page is the only one who was knighted before becoming Prime Minister.
Page had been close to Lyons, but disliked Menzies, whom he charged publicly with having been disloyal to Lyons. When Menzies was elected UAP leader, Page refused to serve under him, and made an extraordinary personal attack on him in the House, accusing him not only of ministerial incompetence but of physical cowardice (for failing to enlist during World War I). His party soon rebelled, though, and Page was deposed as Country Party leader and replaced by Archie Cameron.
In 1940 Page and Menzies patched up their differences for the sake of the war effort, and Page returned to the cabinet as Minister for Commerce. Nevertheless, Page's accusations were not forgotten and were occasionally raised in parliament by Menzies' opponents (notably Eddie Ward). In 1941, the government fell; and Page spent the eight years of the Curtin and Chifley Labor governments on the opposition backbench. He was made a Companion of Honour (CH) in June 1942.
New states.
In 1949 Page put forward a discussion paper on the redrawing of state boundaries. He proposed that Australia would be divided into 12 states. Queensland would be split into four states: Eden-Monaro and East Gippsland would become another state, Mount Gambier to Mildura and Cape Otway another state, and the Northern Territory divided into two. Page had a particular interest in forming a new state of New England, in his own area of northeast New South Wales. 
Return to the ministry.
Menzies returned to the Prime Ministership in 1949, and Page was made Minister for Health. He held this post until 1956, when he was 76, then retired to the backbench. Upon the death of Billy Hughes in October 1952, Page became the Father of the House of Representatives and Father of the Parliament. 
Family.
Page and his wife Ethel had five children: a daughter Mary born in 1909 and four sons. Ethel Page died in 1958.
On 20 July 1959, Page married his secretary, Jean Thomas. Page's grandson, Don Page, is a National MP in the NSW Parliament and served as Deputy Leader of the NSW Nationals from 2003 to 2007. Another grandson is Canberra poet Geoff Page. His nephew was Robert Page, a soldier and war hero. His second wife, Jean Page, died on 20 June 2011.
Earle Charles Page.
Page's oldest son, Earle Charles Page, was born in 1911 at South Grafton and attended Newington College (1922–1927), during the headmastership of the Rev Dr Charles Prescott. At Newington, he won the Wigram Allen Scholarship in 1927, awarded by Sir George Wigram Allen, for general proficiency, with Dave Cowper receiving it for classics and Bill Morrow for mathematics in the same year. In 1932, Page graduated in veterinary science from the University of Sydney.
Earle Charles Page played Rugby Union and was selected for Combined Australian Universities and, as a reserve, for NSW.
Earle Charles Page was managing a rural property for his father at the time of his death in 1933 by lightning strike.
The other Pages boys were Donald (b. 1912), Iven (b. 1914) and Douglas (b. 1916).
Later life and death.
Page was the first Chancellor of the University of New England, which was established in 1954.
By the 1961 election, Page was gravely ill, suffering from lung cancer. Although he was too sick to actively campaign, Page refused to even consider retiring from Parliament and soldiered on for his 17th general election. In one of the great upsets of Australian electoral history, he lost his seat to Labor challenger Frank McGuren, whom he had defeated soundly in 1958. Page had gone into the election holding Cowper with what appeared to be an insurmountable 11-point majority, but McGuren managed to win the seat on a swing of 13%.
Page had campaigned sporadically before going to Royal Prince Alfred Hospital in Sydney for emergency surgery. He went comatose a few days before the election and never regained consciousness. He died on 20 December, 11 days after the election, without ever knowing that he had been defeated.
Page had represented Cowper for just four days short of 42 years, making him the longest-serving Australian federal parliamentarian who represented the same seat throughout his career. Only Billy Hughes served in Parliament longer than Page, but Hughes represented four different electorates in New South Wales and Victoria.

</doc>
<doc id="10358" url="http://en.wikipedia.org/wiki?curid=10358" title="Ephrem the Syrian">
Ephrem the Syrian

Ephrem the Syrian (Classical Syriac: ܡܪܝ ܐܦܪܝܡ ܣܘܪܝܝܐ ("Mār Aprêm Sûryāyâ"); Greek: Ἐφραίμ ὁ Σῦρος; Latin: "Ephraem Syrus"; ca. 306 – 373) was a Syriac deacon and a prolific Syriac-language hymnographer and theologian of the 4th century from the region of Syria. His works are hailed by Christians throughout the world, and many denominations venerate him as a saint. He has been declared a Doctor of the Church in Roman Catholicism. He is especially beloved in the Syriac Orthodox Church.
Ephrem wrote a wide variety of hymns, poems, and sermons in verse, as well as prose biblical exegesis. These were works of practical theology for the edification of the church in troubled times. So popular were his works, that, for centuries after his death, Christian authors wrote hundreds of pseudepigraphal works in his name. Ephrem's works witness to an early form of Christianity in which Western ideas take little part. He has been called the most significant of all of the fathers of the Syriac-speaking church tradition.
Life.
Ephrem was born around the year 306 in the city of Nisibis (now Nusaybin in Turkey, on the border with Syria, in Roman Mesopotamia, then recently acquired by the Roman Empire). 
Internal evidence from Ephrem's hymnody suggests that both his parents were part of the growing Christian community in the city, although later hagiographers wrote that his father was a pagan priest. Numerous languages were spoken in the Nisibis of Ephrem's day, mostly dialects of Aramaic. The Christian community used the Syriac dialect. The culture included pagan religions, Judaism and early Christian sects.
Jacob, the second bishop of Nisibis, was appointed in 308, and Ephrem grew up under his leadership of the community. Jacob of Nisibis is recorded as a signatory at the First Council of Nicea in 325. Ephrem was baptized as a youth and almost certainly became a son of the covenant, an unusual form of Syrian proto-monasticism. Jacob appointed Ephrem as a teacher (Syriac "malp̄ānâ", a title that still carries great respect for Syriac Christians). He was ordained as a deacon either at his baptism or later. He began to compose hymns and write biblical commentaries as part of his educational office. In his hymns, he sometimes refers to himself as a "herdsman" (ܥܠܢܐ, "‘allānâ"), to his bishop as the "shepherd" (ܪܥܝܐ, "rā‘yâ"), and to his community as a 'fold' (ܕܝܪܐ, "dayrâ"). Ephrem is popularly credited as the founder of the School of Nisibis, which, in later centuries, was the centre of learning of the Syriac Orthodox Church.
In 337, Emperor Constantine I, who had legalised and promoted the practice of Christianity in the Roman Empire, died. Seizing on this opportunity, Shapur II of Persia began a series of attacks into Roman North Mesopotamia. Nisibis was besieged in 338, 346 and 350. During the first siege, Ephrem credits Bishop Jacob as defending the city with his prayers. In the third siege, of 350, Shapur rerouted the River Mygdonius to undermine the walls of Nisibis. The Nisibenes quickly repaired the walls while the Persian elephant cavalry became bogged down in the wet ground. Ephrem celebrated what he saw as the miraculous salvation of the city in a hymn that portrayed Nisibis as being like Noah's Ark, floating to safety on the flood.
One important physical link to Ephrem's lifetime is the baptistery of Nisibis. The inscription tells that it was constructed under Bishop Vologeses in 359. In that year, Shapur attacked again. The cities around Nisibis were destroyed one by one, and their citizens killed or deported. Constantius II was unable to respond; the campaign of Julian in 363 ended with his death in battle. His army elected Jovian as the new emperor, and to rescue his army, he was forced to surrender Nisibis to Persia (also in 363) and to permit the expulsion of the entire Christian population.
Ephrem, with the others, went first to Amida (Diyarbakır), eventually settling in Edessa<ef name=Labourt/> (modern Şanlıurfa) in 363. Ephrem, in his late fifties, applied himself to ministry in his new church and seems to have continued his work as a teacher, perhaps in the School of Edessa. Edessa had always been at the heart of the Syriac-speaking world, and the city was full of rival philosophies and religions. Ephrem comments that orthodox Nicene Christians were simply called "Palutians" in Edessa, after a former bishop. Arians, Marcionites, Manichees, Bardaisanites and various gnostic sects proclaimed themselves as the true church. In this confusion, Ephrem wrote a great number of hymns defending Nicene orthodoxy. A later Syriac writer, Jacob of Serugh, wrote that Ephrem rehearsed all-female choirs to sing his hymns set to Syriac folk tunes in the forum of Edessa. After a ten-year residency in Edessa, in his sixties, Ephrem succumbed to the plague as he ministered to its victims. The most reliable date for his death is 9 June 373.
Writings.
Over four hundred hymns composed by Ephrem still exist. Granted that some have been lost, Ephrem's productivity is not in doubt. The church historian Sozomen credits Ephrem with having written over three million lines. Ephrem combines in his writing a threefold heritage: he draws on the models and methods of early Rabbinic Judaism, he engages skillfully with Greek science and philosophy, and he delights in the Mesopotamian/Persian tradition of mystery symbolism.
The most important of his works are his lyric, teaching hymns (ܡܕܖ̈ܫܐ, "madrāšê"). These hymns are full of rich, poetic imagery drawn from biblical sources, folk tradition, and other religions and philosophies. The madrāšê are written in stanzas of syllabic verse and employ over fifty different metrical schemes. Each madrāšâ had its "qālâ" (ܩܠܐ), a traditional tune identified by its opening line. All of these qālê are now lost. It seems that Bardaisan and Mani composed madrāšê, and Ephrem felt that the medium was a suitable tool to use against their claims. The madrāšê are gathered into various hymn cycles. Each group has a title — "Carmina Nisibena", "On Faith", "On Paradise", "On Virginity", "Against Heresies" — but some of these titles do not do justice to the entirety of the collection (for instance, only the first half of the "Carmina Nisibena" is about Nisibis). Each madrāšâ usually had a refrain (ܥܘܢܝܬܐ, "‘ûnîṯâ"), which was repeated after each stanza. Later writers have suggested that the madrāšê were sung by all-women choirs with an accompanying lyre.
Particularly influential were his "Hymns Against Heresies". Ephrem used these to warn his flock of the heresies that threatened to divide the early church. He lamented that the faithful were "tossed to and fro and carried around with every wind of doctrine, by the cunning of men, by their craftiness and deceitful wiles." He devised hymns laden with doctrinal details to inoculate right-thinking Christians against heresies such as docetism. The "Hymns Against Heresies" employ colourful metaphors to describe the Incarnation of Christ as fully human and divine. Ephrem asserts that Christ's unity of humanity and divinity represents peace, perfection and salvation; in contrast, docetism and other heresies sought to divide or reduce Christ's nature and, in doing so, rend and devalue Christ's followers with their false teachings.
Ephrem also wrote verse homilies (ܡܐܡܖ̈ܐ, "mêmrê"). These sermons in poetry are far fewer in number than the madrāšê. The mêmrê were written in a heptosyllabic couplets (pairs of lines of seven syllables each).
The third category of Ephrem's writings is his prose work. He wrote a biblical commentary on the Diatessaron (the single gospel harmony of the early Syriac church), the Syriac original of which was found in 1957. His "Commentary on Genesis and Exodus" is an exegesis of Genesis and Exodus. Some fragments exist in Armenian of his commentaries on the Acts of the Apostles and Pauline Epistles.
He also wrote refutations against Bardaisan, Mani, Marcion and others.
Ephrem is attributed with writing hagiographies such as The Life of Saint Mary the Harlot, though this credit is called into question.
Ephrem wrote exclusively in the Syriac language, but translations of his writings exist in Armenian, Coptic, Georgian, Greek and other languages. Some of his works are only extant in translation (particularly in Armenian). Syriac churches still use many of Ephrem's hymns as part of the annual cycle of worship. However, most of these liturgical hymns are edited and conflated versions of the originals.
The most complete, critical text of authentic Ephrem was compiled between 1955 and 1979 by Dom Edmund Beck, OSB, as part of the "Corpus Scriptorum Christianorum Orientalium".
Greek Ephrem.
Ephrem's artful meditations on the symbols of Christian faith and his stand against heresy made him a popular source of inspiration throughout the church. This occurred to the extent that there is a huge corpus of Ephrem pseudepigraphy and legendary hagiography. Some of these compositions are in verse, often a version of Ephrem's heptosyllabic couplets. Most of these works are considerably later compositions in Greek. Students of Ephrem often refer to this corpus as having a single, imaginary author called "Greek Ephrem", or "Ephraem Graecus" (as opposed to the real Ephrem the Syrian). This is not to say that all texts ascribed to Ephrem in Greek are by others, but many are. Although Greek compositions are the main source of pseudepigraphal material, there are also works in Latin, Slavonic and Arabic. There has been very little critical examination of these works, and many are still treasured by churches as authentic.
The best known of these writings is the "Prayer of Saint Ephrem", which is recited at every service during Great Lent and other fasting periods in Eastern Christianity.
Veneration as a saint.
Soon after Ephrem's death, legendary accounts of his life began to circulate. One of the earlier "modifications" is the statement that Ephrem's father was a pagan priest of Abnil or Abizal. However, internal evidence from his authentic writings suggest that he was raised by Christian parents. This legend may be anti-pagan polemic or may reflect his father's status prior to converting to Christianity. 
The second legend attached to Ephrem is that he was a monk. In Ephrem's day, monasticism was in its infancy in Egypt. He seems to have been a part of the members of the covenant, a close-knit, urban community of Christians that had "covenanted" themselves to service and had refrained from sexual activity. Some of the Syriac terms that Ephrem used to describe his community were later used to describe monastic communities, but the assertion that he was monk is anachronistic. Later hagiographers often painted a picture of Ephrem as an extreme ascetic, but the internal evidence of his authentic writings show him to have had a very active role, both within his church community and through witness to those outside of it.
Ephrem is venerated as an example of monastic discipline in Eastern Christianity. In the Eastern Orthodox scheme of hagiography, Ephrem is counted as a Venerable Father (i.e., a sainted Monk). His feast day is celebrated on 28 January and on the Saturday of the Venerable Fathers (Cheesefare Saturday), which is the Saturday before the beginning of Great Lent.
Ephrem is popularly believed to have taken legendary journeys. In one of these he visits Basil of Caesarea. This links the Syrian Ephrem with the Cappadocian Fathers and is an important theological bridge between the spiritual view of the two, who held much in common. Ephrem is also supposed to have visited Saint Pishoy in the monasteries of Scetes in Egypt. As with the legendary visit with Basil, this visit is a theological bridge between the origins of monasticism and its spread throughout the church.
On 5 October 1920, Pope Benedict XV proclaimed Ephrem a Doctor of the Church ("Doctor of the Syrians"). This proclamation was made before critical editions of Ephrem's authentic writings were available.
The most popular title for Ephrem is "Harp of the Spirit" (Syriac: ܟܢܪܐ ܕܪܘܚܐ, "Kenārâ d-Rûḥâ"). He is also referred to as the Deacon of Edessa, the Sun of the Syrians and a Pillar of the Church.
His Roman Catholic feast day of 9 June conforms to his date of death. For 48 years (1920–1969), it was on 18 June.
Ephrem is honored with a feast day on the liturgical calendar of the Episcopal Church (USA) on June 10.
Quotations.
About Ephrem:
By Ephrem:

</doc>
<doc id="10359" url="http://en.wikipedia.org/wiki?curid=10359" title="Amiga Enhanced Chip Set">
Amiga Enhanced Chip Set

The Enhanced Chip Set (ECS) is the second generation of the Amiga computer's chipset, offering minor improvements over the original chipset (OCS) design. ECS was introduced in 1990 with the launch of the Amiga 3000. Amigas produced from 1990 onwards featured a mix of OCS and ECS chips, such as later versions of the Amiga 500 and the Commodore CDTV. Other ECS models were the Amiga 500+ in 1991 and lastly the Amiga 600 in 1992. 
Notable improvements were the "Super Agnus" and the "Hires Denise" chips. The sound and floppy controller chip, "Paula", remained unchanged from the OCS design. Super Agnus supports 2 MB of CHIP RAM, whereas the original Agnus/"Fat Agnus" and subsequent "Fatter Agnus" can address 512 KB and 1 MB, respectively. The ECS Denise chip offers "Productivity" (640×480 non-interlaced) and "SuperHires" (1280×200 or 1280×256) display modes (also available in interlaced mode), which are however limited to only 4 on-screen colors. Essentially, a 35 ns pixel mode was added plus the ability to run arbitrary horizontal and vertical scan rates. This made other display modes possible, but only the aforementioned modes were supported originally out of the box. For example, the Linux Amiga framebuffer device driver allows the use of several other display modes. Other improvements were the ability of the blitter to copy regions larger than 1024×1024 pixels in one operation and the ability to display sprites in border regions (outside of any display window where bitplanes are shown). ECS also allows software switching between NTSC and PAL video modes.
These improvements largely favored application software, which benefited from higher resolution and VGA-like display modes, rather than gaming software. As an incremental update, ECS was intended to be backward compatible with software designed for OCS machines, though some pre-ECS games were found to be incompatible. Additionally, features from the improved Kickstart 2 operating system were used in subsequent software, and since these two technologies largely overlap, some users misjudged the significance of ECS. It is possible to upgrade some OCS machines, such as the Amiga 500, to obtain partial or full ECS functionality by replacing OCS chips with ECS versions. ECS was followed by the third generation AGA chipset with the launch of the Amiga 4000 in 1992.

</doc>
<doc id="10361" url="http://en.wikipedia.org/wiki?curid=10361" title="European Space Operations Centre">
European Space Operations Centre

The European Space Operations Centre (ESOC) serves as the main mission control centre for the European Space Agency (ESA) and is located in Darmstadt, Germany. ESOC's primary function is the operation of unmanned spacecraft on behalf of ESA and the launch and early orbit phases (LEOP) of ESA and third-party missions. The Centre is also responsible for a range of operations-related activities within ESA and in cooperation with ESA's industry and international partners, including ground systems engineering, software development, flight dynamics and navigation, development of mission control tools and techniques and space debris studies.
ESOC's current major activities comprise operating planetary and solar missions, such as Mars Express and Rosetta, astronomy & fundamental physics missions, such as Gaia (spacecraft) and XMM Newton, and Earth observation missions such as CryoSat2 and Swarm (spacecraft).
ESOC is responsible for developing, operating and maintaining ESA's European Tracking (ESTRACK) Network of ground stations. Teams at the Centre are also involved in research and development related to advanced mission control concepts and Space Situational Awareness, and standardisation activities related to frequency management; mission operations; tracking, telemetry and telecommanding; and space debris.
Missions.
ESOC's current missions comprise the following:
Planetary and solar missions
Astronomy and fundamental physics missions
Earth observation missions
In addition, the ground segment and mission control teams for several missions are in preparation and training, including:
ESTRACK.
ESOC hosts the control centre for the Agency's European Tracking ESTRACK station network. The core network comprises 10 stations in seven countries: Kourou (French Guiana), Maspalomas, Villafranca and Cebreros (Spain), Redu (Belgium), Santa Maria (Portugal), Kiruna (Sweden), Malargüe (Argentina), Perth and New Norcia (Australia). Operators are on duty at ESOC 24 hours/day, year round, to conduct tracking passes, uploading telecommands and downloading telemetry and data.
Activities.
In addition to 'pure' mission operations, a number of other activities take place at the Centre, most of which are directly related to ESA's broader space operations activities.
History.
The European Space Operations Centre was formally inaugurated in Darmstadt, Germany, on 8 September 1967 by the then-Minister of Research of the Federal Republic of Germany, Gerhard Stoltenberg. Its role was to provide satellite control for the European Space Research Organisation (ESRO), which is today known as its successor organisation, the European Space Agency (ESA).
The 90-person ESOC facility was, as it is today, located on the west side of Darmstadt; it employed the staff and resources previously allocated to the European Space Data Centre (ESDAC), which had been established in 1963 to conduct orbit calculations. These were augmented by mission control staff transferred from ESTEC to operate satellites and manage the ESTRACK tracking station network.
Within just eight months, ESOC, as part of ESRO, was already operating its first mission, ESRO-2B, a scientific research satellite and the first of many operated from ESOC for ESRO, and later ESA.
By July 2012, ESOC had operated over 56 missions spanning science, Earth observation, orbiting observatories, meteorology and space physics.
Location and expansion.
ESOC is located on the west side of the city of Darmstadt, some 500 m from the main train station, at Robert-Bosch-Straße 5. In 2011, ESA announced the first phase of the ESOC II modernisation and expansion project valued at €60 million. The new construction will be located across Robert-Bosch-Straße, opposite the current centre.
Employees.
At ESOC, ESA employs approximately 800, comprising some 250 permanent staff and about 550 contractors. Staff from ESOC are routinely dispatched to work at other ESA establishments, ESTRACK stations, the ATV Control Centre (Toulouse), the Columbus Control Centre (Oberpfaffenhofen) and at partner facilities in several countries.

</doc>
<doc id="10363" url="http://en.wikipedia.org/wiki?curid=10363" title="European Space Agency">
European Space Agency

The European Space Agency (ESA; French: Agence spatiale européenne, ASE) is an intergovernmental organisation dedicated to the exploration of space, with 22 member states. Established in 1975 and headquartered in Paris, France, ESA has a staff of more than 2,000 with an annual budget of about €4.28 billion / US$5.51 billion (2013).
ESA's space flight programme includes human spaceflight, mainly through the participation in the International Space Station programme, the launch and operations of unmanned exploration missions to other planets and the Moon, Earth observation, science, telecommunication as well as maintaining a major spaceport, the Guiana Space Centre at Kourou, French Guiana, and designing launch vehicles. The main European launch vehicle Ariane 5 is operated through Arianespace with ESA sharing in the costs of launching and further developing this launch vehicle.
ESA science missions are based at ESTEC in Noordwijk, Netherlands, Earth Observation missions at ESRIN in Frascati, Italy, ESA Mission Control (ESOC) is in Darmstadt, Germany, the European Astronaut Centre (EAC) that trains astronauts for future missions is situated in Cologne, Germany, and the European Space Astronomy Centre is located in Villanueva de la Cañada, Madrid, Spain.
History.
Foundation.
After World War II, many European scientists left Western Europe in order to work in the United States. Although the 1950s boom made it possible for Western European countries to invest in research and specifically in space-related activities, Western European scientists realized solely national projects would not be able to compete with the two main superpowers. In 1958, only months after the Sputnik shock, Edoardo Amaldi and Pierre Auger, two prominent members of the Western European scientific community at that time, met to discuss the foundation of a common Western European space agency. The meeting was attended by scientific representatives from eight countries, including Harrie Massey (UK).
The Western European nations decided to have two different agencies, one concerned with developing a launch system, ELDO (European Launch Development Organization), and the precursor of the European Space Agency, ESRO (European Space Research Organisation). The latter was established on 20 March 1964 by an agreement signed on 14 June 1962. From 1968 to 1972, ESRO launched seven research satellites.
ESA in its current form was founded with the ESA Convention in 1975, when ESRO was merged with ELDO. ESA has 10 founding member states: Belgium, Denmark, France, Germany, Italy, the Netherlands, Spain, Sweden, Switzerland and the United Kingdom. These signed the ESA Convention in 1975 and deposited the instruments of ratification by 1980, when the convention came into force. During this interval the agency functioned in a de facto fashion. ESA launched its first major scientific mission in 1975, Cos-B, a space probe monitoring gamma-ray emissions in the universe first worked on by ESRO.
Later activities.
ESA joined NASA in the IUE, the world's first high-orbit telescope, which was launched in 1978 and operated very successfully for 18 years. A number of successful Earth-orbit projects followed, and in 1986 ESA began Giotto, its first deep-space mission, to study the comets Halley and Grigg–Skjellerup. Hipparcos, a star-mapping mission, was launched in 1989 and in the 1990s SOHO, Ulysses and the Hubble Space Telescope were all jointly carried out with NASA. Recent scientific missions in cooperation with NASA include the Cassini–Huygens space probe, to which ESA contributed by building the Titan landing module Huygens.
As the successor of ELDO, ESA has also constructed rockets for scientific and commercial payloads. Ariane 1, launched in 1979, brought mostly commercial payloads into orbit from 1984 onward. The next two developments of the Ariane rocket were intermediate stages in the development of a more advanced launch system, the Ariane 4, which operated between 1988 and 2003 and established ESA as the world leader in commercial space launches in the 1990s. Although the succeeding Ariane 5 experienced a failure on its first flight, it has since firmly established itself within the heavily competitive commercial space launch market with 56 successful launches as of September 2011. The successor launch vehicle of Ariane 5, the Ariane 6 is already in the definition stage and is envisioned to enter service in the 2020s.
The beginning of the new millennium saw ESA become, along with agencies like NASA, JAXA, ISRO, CSA and Roscosmos, one of the major participants in scientific space research. Although ESA had relied on cooperation with NASA in previous decades, especially the 1990s, changed circumstances (such as tough legal restrictions on information sharing by the United States military) led to decisions to rely more on itself and on cooperation with Russia. A 2011 press issue thus stated:
Russia is ESA's first partner in its efforts to ensure long-term access to space. There is a framework agreement between ESA and the government of the Russian Federation on cooperation and partnership in the exploration and use of outer space for peaceful purposes, and cooperation is already under way in two different areas of launcher activity that will bring benefits to both partners.
Most notable for its new self-confidence are ESA's own recent successful missions SMART-1, a probe testing cutting-edge new space propulsion technology, the Mars Express and Venus Express missions as well as the development of the Ariane 5 rocket and its role in the ISS partnership. ESA maintains its scientific and research projects mainly for astronomy-space missions such as Corot, launched on 27 December 2006, a milestone in the search for extrasolar planets.
Mission.
The treaty establishing the European Space Agency reads:
ESA's purpose shall be to provide for, and to promote, for exclusively peaceful purposes, cooperation among European States in space research and technology and their space applications, with a view to their being used for scientific purposes and for operational space applications systems
ESA is responsible for setting a unified space and related industrial policy, recommending space objectives to the member states, and integrating national programs like satellite development, into the European program as much as possible.
Jean-Jacques Dordain ESA's Director General (since 2003) outlined the European Space Agency's mission in a 2003 interview:
Today space activities are pursued for the benefit of citizens, and citizens are asking for a better quality of life on earth. They want greater security and economic wealth, but they also want to pursue their dreams, to increase their knowledge, and they want younger people to be attracted to the pursuit of science and technology.
I think that space can do all of this: it can produce a higher quality of life, better security, more economic wealth, and also fulfill our citizens' dreams and thirst for knowledge, and attract the young generation. This is the reason space exploration is an integral part of overall space activities. It has always been so, and it will be even more important in the future.
Member states and budget.
Membership and contribution to ESA.
ESA is an intergovernmental organisation of 21 member states. Member states participate to varying degrees in the mandatory (25% of total expenditures in 2008) and optional space programmes (75% of total expenditures in 2008). The 2008 budget amounted to €3.0 billion the 2009 budget to €3.6 billion. The total budget amounted to about €3.7 billion in 2010, €3.99 billion in 2011, €4.02 billion in 2012, €4.28 billion in 2013 and €4.10 billion in 2014. Languages used are English, French, German, Italian, Dutch and Spanish.
The following table lists all the member states and adjunct members, their ESA convention ratification dates, and their contributions in 2015:
Associate members.
Currently the only associated member of ESA is Canada. Previously associated members were Austria, Norway and Finland, all of which later joined ESA as full members.
Canada.
Since 1 January 1979, Canada has had the special status of a Cooperating State within the ESA. By virtue of this accord, the Canadian Space Agency takes part in the ESA's deliberative bodies and decision-making and also in the ESA's programmes and activities. Canadian firms can bid for and receive contracts to work on programmes. The accord has a provision ensuring a fair industrial return to Canada. The most recent Cooperation Agreement was signed on 2010-12-15 with a term extending to 2020. For 2014, Canada's annual assessed contribution to the ESA general budget was 6,059,449.00 Euros (CAD$8,559,050).
Budget appropriation and allocation.
ESA budget chart by programme for 2011
  Earth Observation: 843.9 M€ (21.1%)  Navigation: 665.7 M€ (16.7%)  Launchers: 612.5 M€ (15.3%)  Science: 464.8 M€ (11.6%)  Human Spaceflight: 410.9 M€ (10.3%)  Telecommunications: 341.3 M€ (8.5%)  Basic Activities: 216.7 M€ (5.4%)  General Budget: 179.9 M€ (4.5%)  Robotic Exploration: 129.4 M€ (3.2%)  Technology: 105.1 M€ (2.5%)  Space Situational Awareness: 15.7 M€ (0.4%)  ECSA: 7.9 M€ (0.2%)  Other (0.3%)
The budget of ESA was €2.977 billion in 2005, €2.904 billion in 2006 and grew to €3.018 billion in 2008, €3.600 billion in 2009, €3.745 billion in 2010, €3.994 billion in 2011 and €4.020 billion in 2012. Every 3–4 years, ESA member states agree on a budget plan for several years at an ESA member states conference. This plan can be amended in future years, however provides the major guideline for ESA for several years. The last major conference was held at the end of 2008, setting the budget for the years to 2012.
The 2011 funding allocations for major areas of ESA activity are shown on the pie-chart on the right. The section called 'Other' includes Technology Development, Space Situational Awareness and spending related to European Cooperating States.
Countries typically have their own space programmes that differ in how they operate organisationally and financially with ESA. For example, the French space agency CNES has a total budget of €2015 million, of which €755 million is paid as direct financial contribution to ESA. Several space-related projects are joint projects between national space agencies and ESA (e.g. COROT). Also, ESA is not the only European space organisation (for example European Union Satellite Centre).
Enlargement.
After the decision of the ESA Council of 21/22 March 2001, the procedure for accession of the European states was detailed as described the document titled "The Plan for European Co-operating States (PECS)".
Nations that want to become a full member of ESA do so in 3 stages. First a Cooperation Agreement is signed between the country and ESA. In this stage, the country has very limited financial responsibilities. If a country wants to cooperate more fully with ESA, it signs a European Cooperating State (ECS) Agreement. The ECS Agreement makes companies based in the country eligible for participation in ESA procurements. The country can also participate in all ESA programmes, except for the Basic Technology Research Programme. While the financial contribution of the country concerned increases, it is still much lower than that of a full member state. The agreement is normally followed by a Plan For European Cooperating State (or PECS Charter). This is a 5-year programme of basic research and development activities aimed at improving the nation's space industry capacity. At the end of the 5-year period, the country can either begin negotiations to become a full member state or an associated state or sign a new PECS Charter. Many countries, most of which joined the EU in both 2004 and 2007, have started to cooperate with ESA on various levels:
EU countries and the European Space Agency.
The political perspective of the European Union (EU) is to make ESA an agency of the EU by 2014, although this date may not be met. The EU is already the largest single donor to ESA's budget and non-ESA EU states are observers at ESA.
The only current EU member state that has not signed an ESA Cooperation Agreement is Croatia. In December 2014, the ESA Ministerial Council authorized officials to begin discussions to establish formal cooperation with Croatia.
Launch vehicle fleet.
ESA has a fleet of different launch vehicles in service with which it competes in all sectors of the launch market. ESA's fleet consists of three major rocket designs: Ariane 5, Soyuz-2 and Vega. Rocket launches are carried out by Arianespace, which has 23 shareholders representing the industry that manufactures the Ariane 5 as well as CNES, at the ESA's Guiana Space Centre. Because many communication satellites have equatorial orbits, launches from French Guiana are able to take larger payloads into space than from spaceports at higher latitudes. In addition, equatorial launches give spacecraft an extra 'push' of nearly 500 m/s due to the higher rotational velocity of the Earth at the equator compared to near the Earth's poles where rotational velocity approaches zero.
Ariane 5.
The Ariane 5 rocket is ESA's primary launcher. It has been in service since 1997 and replaced Ariane 4. Two different variants are currently in use. The heaviest and most used version, the Ariane 5 ECA, delivers two communications satellites of up to 10 tonnes into GTO. It failed during its first test flight in 2002, but has since made 43 consecutive successful flights (as of April 2014). The other version, Ariane 5 ES, was used to launch the Automated Transfer Vehicle (ATV) to the International Space Station (ISS) and will be used to launch four Galileo navigational satellites at a time.
In November 2012, ESA agreed to build an upgraded variant called Ariane 5 ME (Mid-life Evolution) which will increase payload capacity to 11.5 tonnes to GTO and feature a restartable second stage to allow more complex missions. Ariane 5 ME is scheduled to fly in 2018. Some of its new features will also be adopted by the next-generation launcher, Ariane 6, planned to replace Ariane 5 in the 2020s.
ESA's Ariane 1, 2, 3 and 4 launchers (the last of which was ESA's long-time workhorse) have been retired.
Soyuz.
Soyuz-2 (also called the Soyuz-ST or Soyuz-STK) is a Russian medium payload launcher (ca. 3 metric tons to GTO) which was brought into ESA service in October 2011. ESA entered into a €340 million joint venture with the Russian Federal Space Agency over the use of the Soyuz launcher. Under the agreement, the Russian agency manufactures Soyuz rocket parts for ESA, which are then shipped to French Guiana for assembly.
ESA benefits because it gains a medium payload launcher, complementing its fleet while saving on development costs. In addition, the Soyuz rocket—which has been the Russian's space launch workhorse for some 40 years—is proven technology with a very good safety record. Russia benefits in that it gets access to the Kourou launch site. Due to its proximity to the equator, launching from Kourou rather than Baikonur nearly doubles Soyuz's payload to GTO (3.0 tonnes vs. 1.7 tonnes).
Soyuz first launched from Kourou on 21 October 2011, and successfully placed two Galileo satellites into orbit 23,222 kilometres above Earth.
Vega.
Vega is ESA's carrier for small satellites. Developed by seven ESA members lead by Italy, it is capable of carrying a payload with a mass of between 300 and 1500 kg to an altitude of 700 km, for low polar orbit. Its maiden launch from Kourou was on 13 February 2012.
The rocket has three solid propulsion stages and a liquid propulsion upper stage (the AVUM) for accurate orbital insertion and the ability to place multiple payloads into different orbits.
Ariane launch vehicle development funding.
Historically, the Ariane family rockets have been funded primarily "with money contributed by ESA governments seeking to participate in the program rather than through competitive industry bids. This [has meant that] governments commit multiyear funding to the development with the expectation of a roughly 90% return on investment in the form of industrial workshare." ESA is proposing changes to this scheme by moving to competitive bids for the development of the Ariane 6.
Human space flight.
History.
At the time ESA was formed, its main goals did not encompass human space flight; rather it considered itself to be primarily a scientific research organisation for unmanned space exploration in contrast to its American and Soviet counterparts. It is therefore not surprising that the first non-Soviet European in space was not an ESA astronaut on a European space craft; it was Czechoslovak Vladimír Remek who in 1978 became the first non-Soviet European in space (the first European in space being Yuri Gagarin of the Soviet Union) — on a Soviet Soyuz spacecraft, followed by the Pole Mirosław Hermaszewski and East German Sigmund Jähn in the same year. This Soviet co-operation programme, known as Intercosmos, primarily involved the participation of Eastern bloc countries. In 1982, however, Jean-Loup Chrétien became the first non-Communist Bloc astronaut on a flight to the Soviet Salyut 7 space station.
Because Chrétien did not officially fly into space as an ESA astronaut, but rather as a member of the French CNES astronaut corps, the German Ulf Merbold is considered the first ESA astronaut to fly into space. He participated in the STS-9 Space Shuttle mission that included the first use of the European-built Spacelab in 1983. STS-9 marked the beginning of an extensive ESA/NASA joint partnership that included dozens of space flights of ESA astronauts in the following years. Some of these missions with Spacelab were fully funded and organizationally and scientifically controlled by ESA (such as two missions by Germany and one by Japan) with European astronauts as full crew members rather than guests on board. Beside paying for Spacelab flights and seats on the shuttles, ESA continued its human space flight co-operation with the Soviet Union and later Russia, including numerous visits to Mir.
During the latter half of the 1980s, European human space flights changed from being the exception to routine and therefore, in 1990, the European Astronaut Centre in Cologne, Germany was established. It selects and trains prospective astronauts and is responsible for the co-ordination with international partners, especially with regard to the International Space Station. As of 2006, the ESA astronaut corps officially included twelve members, including nationals from most large European countries except the United Kingdom.
In the summer of 2008, the ESA started to recruit new astronauts so that final selection would be due in spring 2009. Almost 10,000 people registered as astronaut candidates before registration ended in June 2008. 8,413 fulfilled the initial application criteria. Of the applicants, 918 were chosen to take part in the first stage of psychological testing, which narrowed down the field to 192. After two-stage psychological tests and medical evaluation in early 2009, as well as formal interviews, six new members of the European Astronaut Corps were selected - five men and one woman.
Astronaut Corps.
The astronauts of the European Space Agency are:
Crew vehicles.
In the 1980s, France pressed for an independent European crew launch vehicle. Around 1978 it was decided to pursue a reusable spacecraft model and starting in November 1987 a project to create a mini-shuttle by the name of Hermes was introduced. The craft was comparable to early proposals for the Space Shuttle and consisted of a small reusable spaceship that would carry 3 to 5 astronauts and 3 to 4 metric tons of payload for scientific experiments. With a total maximum weight of 21 metric tons it would have been launched on the Ariane 5 rocket, which was being developed at that time. It was planned solely for use in Low-Earth orbit space flights. The planning and pre-development phase concluded in 1991; however, the production phase was never fully implemented because at that time the political landscape had changed significantly. With the fall of the Soviet Union ESA looked forward to cooperation with Russia to build a next-generation space vehicle. Thus the Hermes programme was cancelled in 1995 after about 3 billion dollars had been spent. The Columbus space station programme had a similar fate.
In the 21st century, ESA started new programmes in order to create its own crew vehicles, most notable among its various projects and proposals is Hopper, whose prototype by EADS, called Phoenix, has already been tested. While projects such as Hopper are neither concrete nor to be realised within the next decade, other possibilities for human spaceflight in cooperation with the Russian Space Agency have emerged. Following talks with the Russian Space Agency in 2004 and June 2005, a cooperation between ESA and the Russian Space Agency was announced to jointly work on the Russian-designed Kliper, a reusable spacecraft that would be available for space travel beyond LEO (e.g. the moon or even Mars). It was speculated that Europe would finance part of it. However, a €50 million participation study for Kliper, which was expected to be approved in December 2005, was finally not approved by the ESA member states. The Russian state tender for the Kliper project was subsequently cancelled in the summer of 2006.
In June 2006, ESA member states granted 15 million to the Crew Space Transportation System (CSTS) study, a two-year study to design a spacecraft capable of going beyond Low-Earth orbit based on the current Soyuz design. This project is pursued with Roskosmos instead of the previously cancelled Kliper proposal. A decision on the actual implementation and construction of the CSTS spacecraft is contemplated for 2008, with the major design decisions being made before the summer of 2007.
In mid-2009 EADS Astrium was awarded a €21 million study into designing a crew vehicle based on the European ATV which is believed to now be the basis of the Advanced Crew Transportation System design.
In November 2012, ESA decided to join NASA's Orion programme. The ATV would form the basis of a propulsion unit for NASA's new manned spacecraft. ESA may also seek to work with NASA on Orion's launch system as well in order to secure a seat on the spacecraft for its own astronauts.
In September 2014, ESA signed an agreement with Sierra Nevada Corporation for cooperation in Dream Chaser project. Further studies on the Dream Chaser for European Utilization or DC4EU project were funded, including the feasibility of launching a Europeanized Dream Chaser onboard Ariane 5.
Cooperation with other countries and organisations.
ESA has signed cooperation agreements with the following states that currently neither plan to integrate as tightly with ESA institutions as Canada, nor envision future membership of ESA: Argentina, Brazil, China, India (for the Chandrayan mission), Russia and Turkey.
Additionally, ESA has joint projects with the European Union, NASA of the United States and is participating in the International Space Station together with the United States (NASA), Russia and Japan (JAXA).
European Union.
ESA is not an agency or body of the European Union (EU), and has non-EU countries Switzerland and Norway as members. There are however ties between the two, with various agreements in place and being worked on, to define the legal status of ESA with regard to the EU.
There are common goals between the ESA and the EU. The ESA has an EU liaison office in Brussels. On certain projects, the EU and ESA cooperate, such as the upcoming Galileo satellite navigation system. Space policy has since December 2009 been an area for voting in the European Council. Under the European Space Policy of 2007, the EU, ESA and its Member States committed themselves to increasing coordination of their activities and programmes and to organising their respective roles relating to space.
The Lisbon Treaty of 2009 reinforces the case for space in Europe and strengthens the role of ESA as an R&D space agency. Article 189 of the Treaty gives the EU a mandate to elaborate a European space policy and take related measures, and provides that the EU should establish appropriate relations with ESA.
Former Italian astronaut Umberto Guidoni, during his tenure as a Member of the European Parliament from 2004 to 2009, stressed the importance of the European Union as a driving force for space exploration, "since other players are coming up such as India and China it is becoming ever more important that Europeans can have an independent access to space. We have to invest more into space research and technology in order to have an industry capable of competing with other international players."
The first EU-ESA International Conference on Human Space Exploration took place in Prague on 22 and 23 October 2009. A road map which would lead to a common vision and strategic planning in the area of space exploration was discussed. Ministers from all 29 EU and ESA members as well as members of parliament were in attendance.
NASA.
ESA has a long history of collaboration with NASA. Since ESA's astronaut corps was formed, the Space Shuttle has been the primary launch vehicle used by ESA's astronauts to get into space through partnership programmes with NASA. In the 1980s and 1990s, the Spacelab programme was an ESA-NASA joint research programme that had ESA develop and manufacture orbital labs for the Space Shuttle for several flights on which ESA participate with astronauts in experiments.
In robotic science mission and exploration missions, NASA has been ESA's main partner. Cassini–Huygens was a joint NASA-ESA mission, along with the Infrared Space Observatory, INTEGRAL, SOHO, and others. Also, the Hubble space telescope is a joint project of NASA and ESA. Future ESA-NASA joint projects include the James Webb Space Telescope and the proposed Laser Interferometer Space Antenna. NASA has committed to provide support to ESA's proposed mission to return an asteroid sample to Earth for further analysis. NASA and ESA will also likely join together for a Mars Sample Return Mission.
Cooperation with other space agencies.
Since China has started to invest more money into space activities, the Chinese Space Agency has sought international partnerships. ESA is, beside the Russian Space Agency, one of its most important partners. Recently the two space agencies cooperated in the development of the Double Star Mission.
ESA entered into a major joint venture with Russia in the form of the CSTS, the preparation of French Guiana spaceport for launches of Soyuz-2 rockets and other projects. With India, ESA agreed to send instruments into space aboard the ISRO's Chandrayaan-1 in 2008. ESA is also cooperating with Japan, the most notable current project in collaboration with JAXA is the BepiColombo mission to Mercury.
Speaking to reporters at an air show near Moscow in August 2011, ESA head Jean-Jacques Dordain said ESA and Russia's Roskosmos space agency would "carry out the first flight to Mars together."
International Space Station.
With regard to the International Space Station (ISS) ESA is not represented by all of its member states: 10 of the 21 ESA member states currently participate in the project. ESA is taking part in the construction and operation of the ISS with contributions such as Columbus, a science laboratory module that was brought into orbit by NASA's STS-122 Space Shuttle mission and the Cupola observatory module that was completed in July 2005 by Alenia Spazio for ESA. The current estimates for the ISS are approaching €100 billion in total (development, construction and 10 years of maintaining the station) of which ESA has committed to paying €8 billion. About 90% of the costs of ESA's ISS share will be contributed by Germany (41%), France (28%) and Italy (20%). German ESA astronaut Thomas Reiter was the first long-term ISS crew member.
ESA has developed the Automated Transfer Vehicle (ATV) for ISS resupply. Each ATV has a cargo capacity of 7667 kg. The first ATV, "Jules Verne", was launched on 9 March 2008 and on 3 April 2008 successfully docked with the ISS. This manoeuvre, considered a major technical feat, involved using automated systems to allow the ATV to track the ISS, moving at 27,000 km/h, and attach itself with an accuracy of 2 cm.
As of 2013, the spacecraft establishing supply links to the ISS are the Russian Progress and Soyuz, European ATV, Japanese Kounotori (HTV), and the USA COTS program vehicles Dragon and Cygnus.
European Life and Physical Sciences research on board the International Space Station (ISS) is mainly based on the European Programme for Life and Physical Sciences in Space programme that was initiated in 2001.
Miscellaneous.
Languages.
According to Annex 1, Resolution No. 8 of the "ESA Convention and Council Rules of Procedure", English, French and German may be used in all meetings of the Agency, with interpretation provided into these three languages. All official documents are available in English and French with all documents concerning the ESA Council being available in German as well.
ESA and the EU institutions.
The EU flag is the one to be flown in space during missions (for example it was flown by ESA's Andre Kuipers during Delta mission)
The Commission is increasingly working together towards common objectives. Some 20 per cent of the funds managed by ESA now originate from the supranational budget of the European Union.
However, in recent years the ties between ESA and the European institutions have been reinforced by the increasing role that space plays in supporting Europe’s social, political and economic policies.
The legal basis for the EU/ESA cooperation is provided by a Framework Agreement which entered into force in May 2004. According to this agreement, the European Commission and ESA coordinate their actions through the Joint Secretariat, a small team of EC’s administrators and ESA executive. The Member States of the two organisations meet at ministerial level in the Space Council, which is a concomitant meeting of the EU and ESA Councils, prepared by Member States representatives in the High-level Space Policy Group (HSPG).
ESA maintains a liaison office in Brussels to facilitate relations with the European institutions.
A new European Dimension.
In May 2007, the 29 European countries expressed their support for the European Space Policy in a resolution of the Space Council, unifying the approach of ESA with those of the European Union and their member states.
Prepared jointly by the European Commission and ESA’s Director General, the European Space Policy sets out a basic vision and strategy for the space sector and addresses issues such as security and defence, access to space and exploration.
Through this resolution, the EU, ESA and their Member States all commit to increasing coordination of their activities and programmes and their respective roles relating to space.
Further reading.
</dl>

</doc>
<doc id="10365" url="http://en.wikipedia.org/wiki?curid=10365" title="Embouchure">
Embouchure

The embouchure is the use of facial muscles and the shaping of the lips to the mouthpiece of woodwind instruments or the mouthpiece of the brass instruments.
The word is of French origin and is related to the root "bouche" (fr.), 'mouth'.
The proper embouchure allows the instrumentalist to play the instrument at its full range with a full, clear tone and without strain or damage to one's muscles.
Brass embouchure.
While performing on a brass instrument, the sound is produced by the player buzzing his or her lips into a mouthpiece. Pitches are changed in part through altering the amount of muscular contraction in the lip formation. The performer's use of the air, tightening of cheek and jaw muscles, as well as tongue manipulation can affect how the embouchure works.
Even today, many brass pedagogues take a rigid approach to teaching how a brass player's embouchure should function. Many of these authors also disagree with each other regarding which technique is correct. Research suggests efficient brass embouchures depend on the player using the method that suits that player's particular anatomy (see below). Individual differences in dental structure, lip shape and size, jaw shape and the degree of jaw malocclusion, and other anatomical factors will affect whether a particular embouchure technique will be effective or not .
In 1962, Philip Farkas hypothesized that the air stream traveling through the lip aperture should be directed straight down the shank of the mouthpiece. He believed that it would be illogical to "violently deflect" the air stream downward at the point of where the air moves past the lips. In this text, Farkas also recommends that the lower jaw be protruded so that the upper and lower teeth are aligned.
In 1970, Farkas published a second text which contradicted his earlier writing. Out of 40 subjects, Farkas showed that 39 subjects directed the air downward to varying degrees and 1 subject directed the air in an upward direction at various degrees. The lower jaw position seen in these photographs show more variation from his earlier text as well.
This supports what was written by trombonist and brass pedagogue Donald S. Reinhardt in 1942. In 1972, Reinhardt described and labeled different embouchure patterns according to such characteristics as mouthpiece placement and the general direction of the air stream as it travels past the lips. According to this later text, players who place the mouthpiece higher on the lips, so that more upper lip is inside the mouthpiece, will direct the air downwards to varying degrees while playing. Performers who place the mouthpiece lower, so that more lower lip is inside the mouthpiece, will direct the air to varying degrees in an upward manner. In order for the performer to be successful, the air stream direction and mouthpiece placement need to be personalized based on individual anatomical differences. Lloyd Leno confirmed the existence of both upstream and downstream embouchures.
More controversial was Reinhardt's description and recommendations regarding a phenomenon he termed a "pivot". According to Reinhardt, a successful brass embouchure depends on a motion wherein the performer moves both the mouthpiece and lips as a single unit along the teeth in an upward and downward direction. As the performer ascends in pitch, he or she will either move the lips and mouthpiece together slightly up towards the nose or pull them down together slightly towards the chin, and use the opposite motion to descend in pitch. Whether the player uses one general pivot direction or the other, and the degree to which the motion is performed, depends on the performer's anatomical features and stage of development. The placement of the mouthpiece upon the lips doesn't change, but rather the relationship of the rim and lips to the teeth. While the angle of the instrument may change as this motion follows the shape of the teeth and placement of the jaw, contrary to what many brass performers and teachers believe, the angle of the instrument does not actually constitute the motion Reinhardt advised as a pivot.
Later research supports Reinhardt's claim that this motion exists and might be advisable for brass performers to adopt. John Froelich describes how mouthpiece pressure towards the lips (vertical forces) and shear pressure (horizontal forces) functioned in three test groups, student trombonists, professional trombonists, and professional symphonic trombonists. Froelich noted that the symphonic trombonists used the least amount of both direct and shear forces and recommends this model be followed. Other research notes that virtually all brass performers rely upon the upward and downward embouchure motion. Other authors and pedagogues remain skeptical about the necessity of this motion, but scientific evidence supporting this view has not been sufficiently developed at this time to support this view.
Some noted brass pedagogues prefer to instruct the use of the embouchure from a less analytical point of view. Arnold Jacobs, a tubist and well-regarded brass teacher, believed that it was best for the student to focus on his or her use of the air and musical expression to allow the embouchure to develop naturally on its own. Other instructors, such as Carmine Caruso, believed that the brass player's embouchure could best be developed through coordination exercises and drills that bring all the muscles into balance that focus the student's attention on his or her time perception. Still other authors who have differing approaches to embouchure development include Louis Maggio, Jeff Smiley, and Jerome Callet.
Farkas embouchure.
Most professional performers, as well as instructors, use a combination called a puckered smile. Farkas told people to blow as if they were trying to cool soup. Raphael Mendez advised saying the letter "M". The skin under your lower lip will be taut with no air pocket. Your lips do not overlap nor do they roll in or out. The corners of the mouth are held firmly in place. To play with an extended range you should use a pivot, tongue arch and lip to lip compression.
According to Farkas the mouthpiece should have 2/3 upper lip and 1/3 lower lip (French horn), 2/3 lower lip and 1/3 upper lip (trumpet and cornet), and more latitude for lower brass (trombone, baritone, and tuba). For trumpet, some also advocate 1/2 upper lip and 1/2 lower lip . Farkas claimed placement was more important for the instruments with smaller mouthpieces. Your lips should not overlap each other, nor should they roll in or out. The mouth corners should be held firm. Farkas speculated that the horn should be held in a downward angle to allow the air stream to go straight into the mouthpiece, although his later text shows that air stream direction actually is either upstream or downstream and is dependent upon the ratio of upper or lower lip inside the mouthpiece, not the horn angle. Farkas advised to moisten the outside of your lips, then form your embouchure and gently place the mouthpiece on it. He also recommended there must be a gap of ⅓ inch or so between your teeth so that the air flows freely.
Arban vs. Saint-Jacome.
Arban and Saint-Jacome were both cornet soloists and authors of well respected and still used method books. Arban stated undogmatically that he believed the mouthpiece should be placed 1/3 on the top lip. St. Jacome to the contrary said dogmatically that the mouthpiece should be placed "two-thirds for the upper and the rest for the under according to all professors and one-third for the upper and two-thirds for the under according to one sole individual, whom I shall not name."
(Subset) Buzzing embouchure.
The Farkas set is the basis of most lip buzzing embouchures. Mendez did teach lip buzzing and got great results. One can initiate this type of buzz by using the same sensation as spitting seeds, but maintaining a continued flow of air. This technique assists the development of the Farkas approach by preventing the player from using an aperture that is too open.
Stevens-Costello embouchure.
Stevens-Costello embouchure has its origins in the William Costello embouchure and was further developed by Roy Stevens. It uses a slight rolling in of both lips and touching evenly all the way across. It also uses mouthpiece placement of about 40% to 50% top lip and 60% to 50% lower lip. The teeth will be about 1/4 to 1/2 inch apart and the teeth are parallel or the jaw slightly forward.
There is relative mouthpiece pressure to the given air column. One exercise to practice the proper weight to air relationship is the palm exercise where you hold your horn by laying it on its side in the palm of your hand. Do not grasp it. Place your lips on the mouthpiece and blow utilizing the weight of the horn in establishing a sound.
Maggio embouchure.
A puckered embouchure, used my most players, and sometimes used by jazz players for extremely high "screamer" notes. Maggio claimed that the pucker embouchure gives more endurance than some systems. Carlton MacBeth is the main proponent of the pucker embouchure. The Maggio system was established because Louis Maggio had sustained an injury which prevented him from playing. In this system you cushion the lips by extending them or puckering (like a monkey). This puckering enables the players to overcome physical malformations. It also lets the player play for an extended time in the upper register. The pucker can make it easy to use to open an aperture. Lots of very soft practice can help overcome this. Claude Gordon was student of Louis Maggio and Herbert L. Clarke and systematized the concepts of these teachers. Claude Gordon made use of pedal tones for embouchure development as did Maggio and Herbert L. Clarke. All three stressed that the mouthpiece should be placed higher on the top lip for a more free vibration of the lips.
Tongue-controlled embouchure.
This embouchure method, advocated by a minority of brass pedagogues such as Jerome Callet, has not yet been sufficiently researched to support the claims that this system is the most effective approach for all brass performers.
Advocates of Callet's approach believe that this method was recommended and taught by the great brass instructors of the early 20th Century. Two French trumpet technique books, authored by Jean-Baptiste Arban, and St. Jacome, were translated into English for use by American players. According to some, due to a misunderstanding arising from differences in pronunciation between French and English, the commonly used brass embouchure in Europe was incorrectly interpreted. Callet attributes this difference in embouchure technique as the reason the great players of the past were able to play at the level of technical virtuosity which they did, although the increased difficulty of contemporary compositions for brass seem to indicate that the level of brass technique achieved by today's performers equals or even exceeds that of most performers from the late 19th and early 20th centuries.
Callet's method of brass embouchure consists of the tongue remaining forward and through the teeth at all times. The corners of the mouth always remain relaxed, and only a small amount of air is used. The top and bottom lips curl inward and grip the forward tongue. The tongue will force the teeth, and subsequently the throat, wide open, supposedly resulting in a bigger, more open sound. The forward tongue resists the pressure of the mouthpiece, controls the flow of air for lower and higher notes, and protects the lips and teeth from damage or injury from mouthpiece pressure. Because of the importance of the tongue in this method many refer to this as a "tongue-controlled embouchure." This technique facilitates the use of a smaller mouthpiece, and larger bore instruments. It results in improved intonation and stronger harmonically related partials across the player's range.]
Woodwind embouchure.
Flute embouchure.
A variety of transverse flute embouchures are employed by professional flautists, though the most natural form is perfectly symmetrical, the corners of the mouth relaxed (i.e. not smiling), the lower lip placed along and at a short distance from the embouchure hole. It must be stressed, however, that achieving a symmetrical, or perfectly centred blowing hole ought not to be an end in itself. Indeed Marcel Moyse, the owner of the next quotation, did not play with a symmetrical embouchure.
The end-blown xiao, kaval, shakuhachi and hocchiku flutes demand especially difficult embouchures, sometimes requiring many lessons before any sound can be produced.
The embouchure is an important element to tone production. The right embouchure, developed with "time, patience, and intelligent work", will produce a beautiful sound and a correct intonation. The embouchure is produced with the muscles around the lips: principally the orbicularis oris muscle and the depressor anguli oris, whilst avoiding activiation of zygomaticus major, which will produce a smile, flattening the top lip against the maxillary (upper jaw) teeth. Beginner flute-players tend to suffer fatigue in these muscles, and notably struggle to use the depressor muscle, which necessarily helps to keep the top lip directing the flow of air across the embouchure hole. These muscles have to be properly warmed up and exercised before practicing. Tone development exercises including long notes and harmonics must be done as part of the warm up every day.
Some further adjustments to the embouchure are necessary when moving from the transverse orchestral flute to the piccolo. With the piccolo, it becomes necessary to place the near side of the embouchure hole slightly higher on the lower lip, i.e. above the lip margin, and greater muscle tone from the lip muscles is needed to keep the stream/pressure of air directed across the smaller embouchure hole, particularly when playing in higher piccolo registers.
Reed instrument embouchure.
With the woodwinds, aside from the flute, piccolo, and recorder, the sound is generated by a reed and not with the lips. The embouchure is therefore based on sealing the area around the reed and the mouthpiece. This serves to prevent air from escaping while simultaneously supporting the reed allowing it to vibrate, and to constrict the reed preventing it from vibrating too much. With woodwinds, it is important to ensure that the mouthpiece is not placed too far into the mouth, which would result in too much vibration (no control), often creating a sound an octave (or harmonic twelfth for the clarinet) above the intended note. If the mouthpiece is not placed far enough into the mouth, no noise will be generated, as the reed will not vibrate.
The embouchure for single reed woodwinds like the clarinet and saxophone is formed by resting the reed upon the bottom lip, which rests on the teeth and is supported by the chin muscles and the buccinator muscles on the sides of the mouth. The top teeth then rest on top of the mouthpiece. In both saxophone and clarinet playing, the corners of the mouth are brought inwards (similar to a drawstring bag) in order to create a seal. With the less common double-lip embouchure, the top lip is placed under (around) the top teeth. In both instances, the position of the tongue in the mouth plays a vital role in focusing and accelerating the air stream blown by the player. This results in a more mature and full sound, rich in overtones.
The double reed woodwinds, the oboe and bassoon, have no mouthpiece. Instead the reed is two pieces of cane extending from a metal tube (oboe - staple) or placed on a bocal (bassoon, English horn). The reed is placed directly on the lips and then played like the double-lip embouchure described above. Compared to the single reed woodwinds, the reed is very small and subtle changes in the embouchure can have a dramatic effect on tuning, tone and pitch control.
Polyphonic embouchure (finger-embouchure).
Recent "waterflute" installations as fountains in public parks allow for a kind of reverse-embouchure. Whereas traditional instruments are supplied with compressible fluid (air) from the mouth of a player, the new waterflutes supply incompressible fluid (water) to the player, and sound is made when the player resists this supply of fluid. Water flows out through mouths of the instrument and the player blocks this flow of water to make sound. As a result, the player can put one finger in each of several of the instrument's mouths, to play a chord, while independently controlling the embouchure of the sound made at each mouth. Additionally, the player's own mouth is free to sing along with the instrument, while the player can independently affect the sound of each of several different musical parts with this "finger embouchure". Such instruments are referred to as hydraulophones. Finger-embouchure can be used to make a wide variety of sounds, ranging from a buzzing sound like that made by a defective faucet, to a very pure tone similar to the sound made by a glass harmonica. Finger embouchure can also be used to affect the intonation or temperament. For example, a skilled hydraulist can use finger-embouchure to remain in a just intonation while changing keys, or to fluidly vary the intonation of a chord while it is sounding.

</doc>
<doc id="10368" url="http://en.wikipedia.org/wiki?curid=10368" title="The Elephant 6 Recording Company">
The Elephant 6 Recording Company

The Elephant 6 Recording Company (or simply Elephant 6) is a collective of American musicians who spawned many notable independent bands of the 1990s, including the Apples in Stereo, the Olivia Tremor Control, Neutral Milk Hotel, Beulah, Elf Power, of Montreal, and Circulatory System. They are marked by a shared admiration of 1960s pop music. 
History.
Foundation.
The collective was officially founded in Denver, Colorado by childhood friends Robert Schneider, Bill Doss, Will Cullen Hart and Jeff Mangum, along with founding Apples in Stereo members Jim McIntyre and Hilarie Sidney. Schneider, Doss, Hart and Mangum grew up making music and sharing cassette tapes in Ruston, Louisiana while attending high school together. They started several bands and pet projects; Doss and Hart with the Olivia Tremor Control (then called Synthetic Flying Machine), Mangum with Neutral Milk Hotel, and Schneider with the Apples in Stereo. Together, they held a mutual admiration for the music of the 1960s, especially the Beach Boys, considering the unfinished "Smile" project to be its "Holy Grail". 
Wishing to emulate the Beach Boys' Brother Records, Schneider created the Elephant 6 record label when he moved to Denver, Colorado in late 1991 to attend the University of Colorado at Boulder. According to Schneider, Hart coined the name "Elephant 6" and Schneider added "Recording Company." In Denver, Schneider and new friends founded The Apples (which eventually became the Apples in Stereo). Recorded at the newly built Pet Sounds Studio, and released in June 1993, The Apples' "Tidal Wave" 7" EP was the first Elephant 6 product. Hart designed the Art Nouveau-inspired Elephant 6 logo for the label of The Apples EP.
Doss had moved to Athens, Georgia where he joined Hart and Mangum in Synthetic Flying Machine, which became the Olivia Tremor Control. They released "California Demise" as their first recording, and the Elephant 6's second. Afterwards, the base for The Elephant 6 moved from Denver to Athens.
Success, disbandment and continuation.
Several Elephant 6 projects began to find commercial success in the late 1990s, including Beulah, the Minders, Elf Power, Dressy Bessy, the Music Tapes, and of Montreal, as well as the founding bands. Most of the bands subsequently signed with major record labels; E6 as an entity slowly deteriorated until the collective called it quits — due to recording difficulties and lack of organization — in 2002. The collective's bands all moved on to various labels and projects of their own.
However, many band members are still friends and even tour together under various guises. Many live together on the Orange Twin Conservation Community in Athens. The term "Elephant 6" has since come to refer to a broad range of bands and spin-off projects that the record label has spawned.
In 2007 The Apples in Stereo featured the Elephant 6 logo on their album "New Magnetic Wonder", announcing "The Elephant 6 Recording Company re-opens our doors and windows, and invites the world: join together with your friends and make something special, something meaningful, something to remember when you are old." This marked the first major release in five years to bear the Elephant 6 logo.
In 2008, the Elephant 6 logo was also used when Julian Koster released his long awaited Music Tapes For Clouds and Tornadoes under the name the Music Tapes. He also assembled The Elephant 6 Holiday Surprise Tour which featured contributions from Will Cullen Hart and Bill Doss of Olivia Tremor Control, Scott Spillane of the Gerbils, Andrew Reiger and Laura Carter of Elf Power, Theo Hilton of Nana Grizol, John Fernandes and Eric Harris of OTC, The Music Tapes and Circulatory System, Robbie Cucchiaro of The Music Tapes, Charlie Johnston and Suzanne Allison of the 63 Crayons, Nesey Gallons, Jeff Mangum, and Peter Erchick. This ensemble tour was widely seen as a resurgence of Elephant 6 as a productive, cohesive entity. The show took place in the UK, curated by Jeff Mangum of Neutral Milk Hotel as part of the All Tomorrow's Parties festival in March 2012 in Minehead, England.
In recent years, Robert Schneider has explored a number of experimental music projects, such as the Teletron mind-controlled synthesizer and Non-Pythagorean scale of his own invention. Indeed, since the inception of Elephant 6, members of the collective showed a strong interest and participation in experimental music, particularly the Olivia Tremor Control and Von Hemmling.

</doc>
<doc id="10369" url="http://en.wikipedia.org/wiki?curid=10369" title="Echolocation">
Echolocation

Echolocation may refer to:

</doc>
<doc id="10370" url="http://en.wikipedia.org/wiki?curid=10370" title="Evangelicalism">
Evangelicalism

Evangelicalism, Evangelical Christianity, or Evangelical Protestantism is a worldwide, transdenominational movement within Protestant Christianity, maintaining that the essence of the gospel consists in the doctrine of salvation by grace through faith in Jesus Christ's atonement.
Evangelicals are Christians who believe in the centrality of the conversion or "born again" experience in receiving salvation, believe in the authority of the Bible as God's revelation to humanity and have a strong commitment to evangelism or sharing the Christian message.
It gained great momentum in the 18th and 19th centuries with the emergence of Methodism and the Great Awakenings in Britain and North America. The origins of Evangelicalism are usually traced back to the English Methodist movement, Nicolaus Zinzendorf, the Moravian Church, Lutheran pietism, Presbyterianism and Puritanism. Among leaders and major figures of the Evangelical Protestant movement were John Wesley, George Whitefield, Jonathan Edwards, Billy Graham, Harold John Ockenga, John Stott and Martyn Lloyd-Jones.
There are an estimated 285,480,000 Evangelicals, corresponding to 13.1% of the Christian population and 4.1% of the total world population. The Americas, Africa and Asia are home to the majority of Evangelicals. The United States has the largest concentration of Evangelicals. Evangelicalism is gaining popularity both in and outside the English-speaking world, especially in Latin America and the developing world.
Usage.
The word "evangelical" has its etymological roots in the Greek word for "gospel" or "good news": ε’υαγγέλιον ("evangelion"), from "eu-" "good" and "angelion" "message". By the English Middle Ages the term had expanded semantically to include not only the message, but also the New Testament which contained the message, as well as more specifically the Gospels which portray the life, death and resurrection of Jesus. The first published use of "evangelical" in English came in 1531 when William Tyndale wrote "He exhorteth them to proceed constantly in the evangelical truth." One year later Sir Thomas More produced the earliest recorded use in reference to a theological distinction when he spoke of "Tyndale [and] his evangelical brother Barns".
During the Reformation, Protestant theologians embraced the label as referring to "gospel truth". Martin Luther referred to the "evangelische Kirche" ("evangelical church) to distinguish Protestants from Catholics in the Roman Catholic Church. Into the 21st century, "evangelical" has continued in used as a synonym for (mainline) Protestant in continental Europe. This usage is reflected in the names of Protestant denominations such as the Evangelical Church in Germany (a union of Lutheran and Reformed churches) and the Evangelical Lutheran Church in America.
In the English-speaking world, "evangelical" became a common label used to describe the series of revival movements that occurred in Britain and North America during the eighteenth and nineteenth centuries. Christian historian David Bebbington writes that, "Although 'evangelical', with a lower-case initial, is occasionally used to mean 'of the gospel', the term 'Evangelical', with a capital letter, is applied to any aspect of the movement beginning in the 1730s." The term may also occur outside any religious context to characterize a generic missionary, reforming, or redeeming impulse or purpose. For example, the "Times Literary Supplement" refers to "the rise and fall of evangelical fervor within the Socialist movement".
Characteristics.
One influential definition of Evangelicalism has been proposed by historian David Bebbington. Bebbington notes four distinctive aspects of Evangelical faith: "conversionism", "biblicism", "crucicentrism", and "activism", noting, "Together they form a quadrilateral of priorities that is the basis of Evangelicalism."
"Conversionism", or belief in the necessity of being "born again", has been a constant theme of Evangelicalism since its beginnings. To Evangelicals, the central message of the gospel is justification by faith in Christ and repentance, or turning away, from sin. Conversion differentiates the Christian from the non-Christian, and the change in life it leads to is marked by both a rejection of sin and a corresponding personal holiness of life. A conversion experience can be emotional, including grief and sorrow for sin followed by great relief at receiving forgiveness. The stress on conversion is further differentiated from other forms of Protestantism by the belief that an assurance of salvation will accompany conversion. Among Evangelicals, individuals have testified to both sudden and gradual conversions.
"Biblicism" is defined as having a reverence for the Bible and a high regard for biblical authority. All Evangelicals believe in biblical inspiration, though they disagree over how this inspiration should be defined. Many Evangelicals believe in biblical inerrancy, while other Evangelicals believe in biblical infallibility.
"Crucicentrism" refers to the attention that Evangelicals give to the Atonement, the saving death and resurrection of Jesus Christ, the Son of God, that offers forgiveness of sins and new life. This is understood most commonly in terms of a substitutionary atonement, in which Christ died as a substitute for sinful humanity by taking on himself the guilt and punishment for sin.
"Activism" describes the tendency towards active expression and sharing of the gospel in diverse ways that include preaching and social action. This aspect of Evangelicalism continues to be seen today in the proliferation of Evangelical voluntary religious groups and parachurch organizations.
Diversity.
As a trans-denominational movement, Evangelicalism occurs in nearly every Protestant denomination and tradition. The Reformed, Baptist, Wesleyan, and Pentecostal traditions have all had strong influence within modern Evangelicalism. Evangelicals are also represented within the Anabaptist, Anglican and Lutheran traditions.
The early 20th century saw the decline of Evangelical influence within mainline Protestantism and the development of Christian fundamentalism as a distinct religious movement. The second half of the century witnessed the development of a new mainstream Evangelical consensus that sought to be more inclusive and more culturally relevant than fundamentalism, while maintaining conservative Protestant teaching. According to professor of world Christianity Brian Stanley, this new postwar consensus is termed "Neo-Evangelicalism", the "New Evangelicalism", or simply "Evangelicalism" in the United States, while in the United Kingdom and in other English-speaking countries it is commonly termed conservative Evangelicalism. Over the years, less conservative Evangelicals have challenged this mainstream consensus to varying degrees, and such movements have been described by a variety of labels, such as progressive, open, post-conservative, and post-evangelical.
Fundamentalism.
Fundamentalism regards biblical inerrancy, the virgin birth of Jesus, penal substitutionary atonement, the literal resurrection of Christ and the Second Coming of Christ as fundamental Christian doctrines. Fundamentalism arose among Evangelicals in the 1920s to combat modernist or liberal theology in Mainline Protestant churches. Failing to reform the Mainline churches, fundamentalists separated from them and established their own churches, refusing to participate in ecumenical organizations such as the National Council of Churches. They also made separatism (rigid separation from non-fundamentalist churches and culture) a true test of faith. According to historian George Marsden, most fundamentalists are Baptists and dispensationalist.
Mainstream varieties.
Mainstream Evangelicalism is historically divided between two main orientations: confessionalism and revivalism. These two streams have been critical of each other. Confessional Evangelicals have been suspicious of unguarded religious experience, while revivalist Evangelicals have been critical of overly intellectual teaching that (they suspect) stifles vibrant spirituality. In an effort to broaden their appeal, many contemporary Evangelical congregations intentionally avoid identifying with any single form of Evangelicalism. These "generic Evangelicals" are usually theologically and socially conservative, but their churches often present themselves as nondenominational within the broader Evangelical movement.
In the words of Albert Mohler, president of the Southern Baptist Theological Seminary, confessional Evangelicalism refers to "that movement of Christian believers who seek a constant convictional continuity with the theological formulas of the Protestant Reformation". While approving of the Evangelical distinctives proposed by Bebbington, confessional Evangelicals believe that authentic Evangelicalism requires more concrete definition in order to protect the movement from theological liberalism and from heresy. This protection, according to confessional Evangelicals, is found in subscription to the ecumenical creeds and to the Reformation-era confessions of faith (such as the confessions of the Reformed churches). Confessional Evangelicals are represented by conservative Presbyterian churches, certain Baptist churches that emphasize historic Baptist confessions like the Second London Confession, Anglicans who emphasize the Thirty-Nine Articles (such as in the Anglican Diocese of Sydney, Australia), and confessional Lutherans who identify with the Evangelical movement.
The emphasis on historic Protestant orthodoxy among confessional Evangelicals stands in direct contrast to an anti-creedal outlook that has exerted its own influence on Evangelicalism, particularly among churches heavily influenced by revivalism and by pietism. Revivalist Evangelicals are represented by some quarters of Methodism, the Wesleyan Holiness churches, the Pentecostal/charismatic churches, some Anabaptist churches, and some Baptists and Presbyterians. Revivalist Evangelicals tend to place greater emphasis on religious experience than their confessional counterparts.
Non-conservative varieties.
Evangelicals dissatisfied with the movement's conservative mainstream have been variously described as progressive Evangelicals, post-conservative Evangelicals, Open Evangelicals and Post-evangelicals. Progressive Evangelicals, also known as the Evangelical left, share theological or social views with other progressive Christians, while also identifying with Evangelicalism. Progressive Evangelicals commonly advocate for women's equality, pacifism and social justice.
As described by Baptist theologian Roger E. Olson, post-conservative Evangelicalism is a theological school of thought that adheres to the four marks of Evangelicalism, while being less rigid and more inclusive of other Christians. According to Olson, post-conservatives believe that doctrine and propositional truth is secondary to spiritual experience shaped by Scripture. Post-conservative Evangelicals seek greater dialogue with other Christian traditions and support the development of a multicultural Evangelical theology that incorporates the voices of women, racial minorities, and Christians in the developing world. Some post-conservative Evangelicals also support Open Theism and the possibility of near universal salvation.
The term "Open Evangelical" refers to a particular Christian school of thought or churchmanship, primarily in the United Kingdom (especially in the Church of England). Open Evangelicals describe their position as combining a traditional Evangelical emphasis on the nature of scriptural authority, the teaching of the ecumenical creeds and other traditional doctrinal teachings, with an approach towards culture and other theological points-of-view which tends to be more inclusive than that taken by other Evangelicals. Some Open Evangelicals aim to take a middle position between conservative and charismatic Evangelicals, while others would combine conservative theological emphases with more liberal social positions.
British author Dave Tomlinson coined the phrase "post-evangelical" to describe a movement comprising various trends of dissatisfaction among Evangelicals. Others use the term with comparable intent, often to distinguish Evangelicals in the so-called emerging church movement from post-evangelicals and anti-Evangelicals. Tomlinson argues that "linguistically, the distinction "[between evangelical and post-evangelical]" resembles the one that sociologists make between the modern and postmodern eras".
History.
Background.
Evangelicalism did not take recognizable form until the 18th century, first in Britain and its North American colonies. Nevertheless, there were earlier developments within the larger Protestant world that preceded and influenced the later evangelical revivals. According to religion scholar, social activist, and politician Randall Balmer, Evangelicalism resulted "from the confluence of Pietism, Presbyterianism, and the vestiges of Puritanism. Evangelicalism picked up the peculiar characteristics from each strain – warmhearted spirituality from the Pietists (for instance), doctrinal precisionism from the Presbyterians, and individualistic introspection from the Puritans". Historian Mark Noll adds to this list High Church Anglicanism, which contributed to Evangelicalism a legacy of "rigorous spirituality and innovative organization".
During the 17th century, Pietism emerged in Europe as a movement for the revival of piety and devotion within the Lutheran church. As a protest against "cold orthodoxy" or an overly formal and rational Christianity, Pietists advocated for an experiential religion that stressed high moral standards for both clergy and lay people. The movement included both Christians who remained in the liturgical, state churches as well as separatist groups who rejected the use of baptismal fonts, altars, pulpits, and confessionals. As Pietism spread, the movement's ideals and aspirations influenced and were absorbed into early Evangelicalism.
The Presbyterian heritage not only gave Evangelicalism a commitment to Protestant orthodoxy but also contributed a revival tradition that stretched back to the 1620s in Scotland and Northern Ireland. Central to this tradition was the communion season, which normally occurred in the summer months. For Presbyterians, celebrations of Holy Communion were infrequent but popular events preceded by several Sundays of preparatory preaching and accompanied with preaching, singing, and prayers.
Puritanism combined Calvinism with teaching that conversion was a prerequisite for church membership and a stress on the study of Scripture by lay people. It took root in New England, where the Congregational church was an established religion. The Half-Way Covenant of 1662 allowed parents who had not testified to a conversion experience to have their children baptized, while reserving Holy Communion for converted church members alone. By the 18th century, Puritanism was in decline and many ministers were alarmed at the loss of religious piety. This concern over declining religious commitment led many people to support evangelical revival.
High Church Anglicanism also exerted influence on early Evangelicalism. High Churchmen were distinguished by their desire to adhere to primitive Christianity. This desire included imitating the faith and ascetic practices of early Christians as well as regularly partaking of Holy Communion. High Churchmen were also enthusiastic organizers of voluntary religious societies. Two of the most prominent were the Society for Promoting Christian Knowledge, which distributed Bibles and other literature and built schools, and the Society for the Propagation of the Gospel in Foreign Parts, which was created to facilitate missionary work in British colonies. Samuel and Susanna Wesley, the parents of John and Charles Wesley, were both devoted advocates of High Churchmanship.
18th century.
In the 1730s, Evangelicalism emerged as a distinct phenomenon out of religious revivals that began in Britain and New England. While religious revivals had occurred within Protestant churches in the past, the evangelical revivals that marked the 18th century were more intense and radical. Evangelical revivalism imbued ordinary men and women with a confidence and enthusiasm for sharing the gospel and converting others outside of the control of established churches, a key discontinuity with the Protestantism of the previous era.
It was developments in the doctrine of assurance that differentiated Evangelicalism from what went before. Bebbington says, "The dynamism of the Evangelical movement was possible only because its adherents were assured in their faith." He goes on:
Whereas the Puritans had held that assurance is rare, late and the fruit of struggle in the experience of believers, the Evangelicals believed it to be general, normally given at conversion and the result of simple acceptance of the gift of God. The consequence of the altered form of the doctrine was a metamorphosis in the nature of popular Protestantism. There was a change in patterns of piety, affecting devotional and practical life in all its departments. The shift, in fact, was responsible for creating in Evangelicalism a new movement and not merely a variation on themes heard since the Reformation."
The first local revival occurred in Northampton, Massachusetts, under the leadership of Congregationalist minister Jonathan Edwards. In the fall of 1734, Edwards preached a sermon series on "Justification By Faith Alone", and the community's response was extraordinary. Signs of religious commitment among the laity increased, especially among the town's young people. The revival ultimately spread to 25 communities in western Massachusetts and central Connecticut until it began to wane by the spring of 1735. Edwards was heavily influenced by Pietism, so much so that one historian has stressed his "American Pietism." One practice clearly copied from European Pietists was the use of small groups divided by age and gender, which met in private homes to conserve and promote the fruits of revival.
At the same time, students at Yale University (at that time Yale College) in New Haven, Connecticut, were also experiencing revival. Among them was Aaron Burr, who would become a prominent Presbyterian minister and future president of Princeton University. In New Jersey, Gilbert Tennent, another Presbyterian minister, was preaching the evangelical message and urging the Presbyterian Church to stress the necessity of converted ministers.
The spring of 1735 also marked important events in England and Wales. Howell Harris, a Welsh schoolteacher, had a conversion experience on May 25 during a communion service. He described receiving assurance of God's grace after a period of fasting, self-examination, and despair over his sins. Sometime later, Daniel Rowland, the Anglican curate of Llangeitho, Wales, experienced conversion as well. Both men began preaching the evangelical message to large audiences, becoming leaders of the Welsh Methodist revival. At about the same time that Harris experienced conversion in Wales, George Whitefield was converted at Oxford University after his own prolonged spiritual crisis. Whitefield later remarked, "About this time God was pleased to enlighten my soul, and bring me into the knowledge of His free grace, and the necessity of being justified in His sight by "faith only"".
Whitefield's fellow Holy Club member and spiritual mentor, Charles Wesley, reported an evangelical conversion in 1738. In the same week, Charles' brother and future founder of Methodism, John Wesley was also converted after a long period of inward struggle. During this spiritual crisis, John Wesley was directly influenced by Pietism. Two years before his conversion, Wesley had traveled to the newly established colony of Georgia as a missionary for the Society for Promoting Christian Knowledge. He shared his voyage with a group of Moravian Brethren led by August Gottlieb Spangenberg. The Moravians' faith and piety deeply impressed Wesley, especially their belief that it was a normal part of Christian life to have an assurance of one's salvation. Wesley recounted the following exchange with Spangenberg on February 7, 1736:
[Spangenberg] said, "My brother, I must first ask you one or two questions. Have you the witness within yourself? Does the Spirit of God bear witness with your spirit that you are a child of God?" I was surprised, and knew not what to answer. He observed it, and asked, "Do you know Jesus Christ?" I paused, and said, "I know he is the Savior of the world." "True," he replied, "but do you know he has saved you?" I answered, "I hope he has died to save me." He only added, "Do you know yourself?" I said, "I do." But I fear they were vain words.
Wesley finally received the assurance he had been searching for at a meeting of a religious society in London. While listening to a reading from Martin Luther's preface to the Epistle to the Romans, Wesley felt spiritually transformed:
About a quarter before nine, while [the speaker] was describing the change which God works in the heart through faith in Christ, I felt my heart strangely warmed. I felt I did trust in Christ, Christ alone for salvation, and an assurance was given me that he had taken away "my" sins, even "mine", and saved "me" from the law of sin and death.
Pietism continued to influence Wesley, who had translated 33 Pietist hymns from German to English. Numerous German Pietist hymns became part of the English Evangelical repertoire.
By 1737, Whitefield had become a national celebrity in England where his preaching drew large crowds, especially in London. Whitfield joined forces with Edwards to "fan the flame of revival" in the Thirteen Colonies in 1739–40. Soon the First Great Awakening stirred Protestants up and down the Thirteen Colonies.
Evangelical preachers emphasized personal salvation and piety more than ritual and tradition. Pamphlets and printed sermons crisscrossed the Atlantic, encouraging the revivalists. The Awakening resulted from powerful preaching that gave listeners a sense of deep personal revelation of their need of salvation by Jesus Christ. Pulling away from ritual and ceremony, the Great Awakening made Christianity intensely personal to the average person by fostering a deep sense of spiritual conviction and redemption, and by encouraging introspection and a commitment to a new standard of personal morality. It reached people who were already church members. It changed their rituals, their piety and their self-awareness. To the evangelical imperatives of Reformation Protestantism, 18th century American Christians added emphases on divine outpourings of the Holy Spirit and conversions that implanted within new believers an intense love for God. Revivals encapsulated those hallmarks and forwarded the newly created Evangelicalism into the early republic.
19th century.
The start of the 19th century saw an increase in missionary work and many of the major missionary societies were founded around this time (see Timeline of Christian missions). Both the Evangelical and high church movements sponsored missionaries.
The Second Great Awakening (which actually began in 1790) was primarily an American revivalist movement and resulted in substantial growth of the Methodist and Baptist churches. Charles Grandison Finney was an important preacher of this period.
In Britain in addition to stressing the traditional Wesleyan combination of "Bible, cross, conversion, and activism," the revivalist movement sought a universal appeal, hoping to include rich and poor, urban and rural, and men and women. Special efforts were made to attract children and to generate literature to spread the revivalist message.
"Christian conscience" was used by the British Evangelical movement to promote social activism. Evangelicals believed activism in government and the social sphere was an essential method in reaching the goal of eliminating sin in a world drenched in wickedness. The Evangelicals in the Clapham Sect included figures such as William Wilberforce who successfully campaigned for the abolition of slavery.
In the late 19th century, the revivalist Holiness movement, based on the doctrine of "entire sanctification," took a more extreme form in rural America and Canada, where it ultimately broke away from institutional Methodism. In urban Britain the Holiness message was less exclusive and censorious.
John Nelson Darby was a 19th-century Irish Anglican minister who devised modern dispensationalism, an innovative Protestant theological interpretation of the Bible that was incorporated in the development of modern Evangelicalism. Cyrus Scofield further promoted the influence of dispensationalism through the explanatory notes to his Scofield Reference Bible. According to scholar Mark S. Sweetnam, who takes a cultural studies perspective, dispensationalism can be defined in terms of its Evangelicalism, its insistence on the literal interpretation of Scripture, its recognition of stages in God's dealings with humanity, its expectation of the imminent return of Christ to rapture His saints, and its focus on both apocalypticism and premillennialism.
Notable figures of the latter half of the 19th century include Charles Spurgeon in London and Dwight L. Moody in Chicago. Their powerful preaching reached very large audiences.
An advanced theological perspective came from the Princeton theologians from the 1850s to the 1920s, such as Charles Hodge, Archibald Alexander and B.B. Warfield.
20th century.
Evangelicalism in the early part of the 20th century was dominated by the Fundamentalist movement after 1910; it rejected liberal theology and emphasized the inerrancy of the Scriptures.
Following the Welsh Revival, the Azusa Street Revival in 1906 began the spread of Pentecostalism in North America.
In the post–World War II period, a split developed between Evangelicals, as they disagreed among themselves about how a Christian ought to respond to an unbelieving world. Many Evangelicals urged that Christians must engage the culture directly and constructively, and they began to express reservations about being known to the world as "fundamentalists". As Kenneth Kantzer put it at the time, the name "fundamentalist" had become "an embarrassment instead of a badge of honor".
The term "neo-evangelicalism" was coined by Harold Ockenga in 1947 to identify a distinct movement within self-identified fundamentalist Christianity at the time, especially in the English-speaking world. It described the mood of positivism and non-militancy that characterized that generation. The new generation of Evangelicals set as their goal to abandon a militant Bible stance. Instead, they would pursue dialogue, intellectualism, non-judgmentalism, and appeasement. They further called for an increased application of the gospel to the sociological, political, and economic areas.
The self-identified fundamentalists also cooperated in separating their "neo-Evangelical" opponents from the "fundamentalist" name, by increasingly seeking to distinguish themselves from the more open group, whom they often characterized derogatorily by Ockenga's term, "neo-Evangelical" or just Evangelical.
The fundamentalists saw the Evangelicals as often being too concerned about social acceptance and intellectual respectability, and being too accommodating to a perverse generation that needed correction. In addition, they saw the efforts of evangelist Billy Graham, who worked with non-Evangelical denominations, such as the Roman Catholics (which they claimed to be heretical), as a mistake.
The post-war period also saw growth of the ecumenical movement and the founding of the World Council of Churches, which was generally regarded with suspicion by the Evangelical community.
In the United Kingdom, John Stott and Martyn Lloyd-Jones emerged as key leaders in Evangelical Christianity.
The charismatic movement began in the 1960s and resulted in Pentecostal theology and practice being introduced into many mainline denominations. New charismatic groups such as the Association of Vineyard Churches and Newfrontiers trace their roots to this period (see also British New Church Movement).
The closing years of the 20th century saw controversial postmodern influences entering some parts of Evangelicalism, particularly with the emerging church movement.
Global statistics.
According to a 2011 Pew Forum study on global Christianity, 285,480,000 or 13.1 percent of all Christians are Evangelicals. The largest concentration of Evangelicals can be found in the United States, with 26.8% of the U.S. population or 94.38 million, the latter being roughly one third of the world's Evangelicals. The next most populous is Brazil, with 26.3% or 51.33 million.
The World Evangelical Alliance is "a network of churches in 129 nations that have each formed an evangelical alliance and over 100 international organizations joining together to give a world-wide identity, voice, and platform to more than 600 million evangelical Christians". The Alliance was formed in 1951 by Evangelicals from 21 countries. It has worked to support its members to work together globally.
The World Christian Database estimates the number of Evangelicals at 300 million, Pentecostals and Charismatics at 600 million and "Great Commission" Christians at 700 million. These groups are not mutually exclusive. Operation World estimates the number of Evangelicals at 550 million.
From 1960 to 2000, the global growth of the number of reported Evangelicals grew three times the world's population rate, and twice that of Islam.
Africa.
In the 21st century, there are Evangelical churches active in Sudan, Angola, Mozambique, Zimbabwe, Malawi, Rwanda, Uganda, Ghana, Kenya, Zambia, South Africa, and Nigeria. They have grown especially since independence came in the 1960s, the strongest movements are based on Pentecostal-charismatic beliefs, and comprise a way of life that has led to upward social mobility and demands for democracy. There is a wide range of theology and organizations, including some sponsored by European missionaries and others that have emerged from African culture such as the Apostolic and Zionist Churches which enlist 40% of black South Africans, and their Aladura counterparts in western Africa.
In Nigeria the Evangelical Church Winning All (formerly "Evangelical Church of West Africa") is the largest church organization with five thousand congregations and over three million members. It sponsors two seminaries and eight Bible colleges, and 1600 missionaries who serve in Nigeria and other countries with the Evangelical Missionary Society (EMS). There have been serious confrontations since 1999 between Muslims and Evangelical Christians standing in opposition to the expansion of Sharia law in northern Nigeria. The confrontation has radicalized and politicized the Christians. Violence has been escalating.
In Kenya, mainstream Evangelical denominations have taken the lead in promoting political activism and backers, with the smaller Evangelical sects of less importance. Daniel arap Moi was president 1978 to 2002 and claimed to be an Evangelical; he proved intolerant of dissent or pluralism or decentralization of power.
The Berlin Missionary Society (BMS) was one of four German Protestant mission societies active in South Africa before 1914. It emerged from the German tradition of Pietism after 1815 and sent its first missionaries to South Africa in 1834. There were few positive reports in the early years, but it was especially active 1859–1914. It was especially strong in the Boer republics. The World War cut off contact with Germany, but the missions continued at a reduced pace. After 1945 the missionaries had to deal with decolonisation across Africa and especially with the apartheid government. At all times the BMS emphasized spiritual inwardness, and values such as morality, hard work and self-discipline. It proved unable to speak and act decisively against injustice and racial discrimination and was disbanded in 1972.
Since 1974, young professionals have been the active proselytizers of Evangelicalism in the cities of Malawi.
In Mozambique, Evangelical Protestant Christianity emerged around 1900 from black migrants whose converted previously in South Africa. They were assisted by European missionaries, but, as industrial workers, they paid for their own churches and proselytizing. They prepared southern Mozambique for the spread of Evangelical Protestantism. During its time as a colonial power in Mozambique, the Catholic Portuguese government tried to counter the spread of Evangelical Protestantism.
Latin America.
In modern Latin America, the word "Evangelical" is often simply a synonym for "Protestant".
Brazil.
Protestantism in Brazil largely originated with German immigrants and British and American missionaries in the 19th century, following up on efforts that began in the 1820s.
In the late nineteenth century, while the vast majority of Brazilians were nominal Catholics, the nation was underserved by priests, and for large numbers their religion was only nominal. The Catholic Church in Brazil was de-established in 1890, and responded by increasing the number of dioceses and the efficiency of its clergy. Many Protestants came from a large German immigrant community, but they were seldom engaged in proselytism and grew mostly by natural increase.
Methodists were active along with Presbyterians and Baptists. The Scottish missionary Dr. Robert Reid Kalley, with support from the Free Church of Scotland, moved to Brazil in 1855, founding the first Evangelical church among the Portuguese-speaking population there in 1856. It was organized according to the Congregational policy as the Igreja Evangélica Fluminense; it became the mother church of Congregationalism in Brazil. The Seventh-day Adventists arrived in 1894, and the YMCA was organized in 1896. The missionaries promoted schools colleges and seminaries, including a the liberal arts college in São Paulo, later known as Mackenzie, and an agricultural school in Lavras. The Presbyterian schools in particular later became the nucleus of the governmental system. In 1887 Protestants in Rio de Janeiro formed a hospital. The missionaries largely reached a working-class audience, as the Brazilian upper-class was wedded either to Catholicism or to secularism. By 1914, Protestant churches founded by American missionaries had 47,000 communicants, served by 282 missionaries. In general, these missionaries were more successful than they had been in Mexico, Argentina or elsewhere in Latin America.
There were 700,000 Protestants by 1930, and increasingly they were in charge of their own affairs. In 1930, the Methodist Church of Brazil became independent of the missionary societies and elected its own bishop. Protestants were largely from a working-class, but their religious networks help speed their upward social mobility.
Protestants accounted for fewer than 5% of the population until the 1960s, but grew exponentially by proselytizing and by 2000 made up over 15% of Brazilians affiliated with a church. Pentecostals and charismatic groups account for the vast majority of this expansion.
Pentecostal missionaries arrived early in the 20th century. Pentecostal conversions surged during the 1950s and 1960s, when native Brazilians began founding autonomous churches. The most influential included Brasil Para o Cristo (Brazil for Christ), founded in 1955 by Manoel de Mello. With an emphasis on personal salvation, on God's healing power, and on strict moral codes these groups have developed broad appeal, particularly among the booming urban migrant communities. In Brazil, since the mid-1990's, groups committed to uniting black identity, antiracism, and Evangelical theology have rapidly proliferated. Pentecostalism arrived in Brazil with Swedish and American missionaries in 1911. it grew rapidly, but endured numerous schisms and splits. In some areas the Evangelical Assemblies of God churches have taken a leadership role in politics since the 1960s. They claimed major credit for the election of Fernando Collor de Mello as president of Brazil in 1990.
According to the 2000 Census, 15.4% of the Brazilian population was Protestant. A recent research conducted by the Datafolha institute shows that 25% of Brazilians are Protestants, of which 19% are followers of Pentecostal denominations. The 2010 Census found out that 22.2% were Protestant at that date. Protestant denominations saw a rapid growth in their number of followers since the last decades of the 20th century. They are politically and socially conservative, and emphasize that God's favor translates into business success. The rich and the poor remained traditional Catholics, while most Evangelical Protestants were in the new lower-middle class–known as the "C class" (in a A–E classification system).
Chesnut argues that Pentecostalism has become "one of the principal organizations of the poor," for these churches provide the sort of social network that teach members the skills they need to thrive in a rapidly developing meritocratic society.
One large Evangelical church is the Universal Church of the Kingdom of God (IURD), a neo‐Pentecostal denomination begun in 1977. It now has a presence in many countries, and claims millions of members worldwide.
Guatemala.
Protestants remained a small portion of the population until the late-twentieth century, when various Protestant groups experienced a demographic boom that coincided with the increasing violence of the Guatemalan Civil War. Two Guatemalan heads of state, General Efraín Ríos Montt and Jorge Serrano Elias, have been practicing Evangelical Protestants. They are the only two Protestant heads of state in the history of Latin America.
General Efrain Rios Montt, an Evangelical from the Pentecostal tradition, came to power through a coup. He escalated the war against leftist guerrilla insurgents as a holy war against atheistic forces of evil.
Asia.
Korea.
Protestant missionary proselytism in Asia was most successful in Korea. American Presbyterians and Methodists arrived in the 1880s and were well received. Between 1907 and 1945, when Korea was a Japanese colony, Christianity became in part an expression of nationalism in opposition to Japan's efforts to promote the Japanese language and the Shinto religion. In 1914, out of 16 million people, there were 86,000 Protestants and 79,000 Catholics; by 1934, the numbers were 168,000 and 147,000. Presbyterian missionaries were especially successful. Since the Korean War (1950–53), many Korean Christians have migrated to the U.S., while those who remained behind have risen sharply in social and economic status. Most Korean Protestant churches in the 21st century emphasize their Evangelical heritage. Korean Protestantism is characterized by theological conservatism coupled with an emotional revivalistic style. Most churches sponsor revival meetings once or twice a year. Missionary work is a high priority, with 13,000 men and women serving in missions across the world, putting Korea in second place just behind the US.
Sukman argues that since 1945, Protestantism has been widely seen by Koreans as the religion of the middle class, youth, intellectuals, urbanites, and modernists. It has been a powerful force supporting South Korea's pursuit of modernity and emulation of the United States, and opposition to the old Japanese colonialism and to the authoritarianism of North Korea. There are 8.6 million adherents to Protestant Christianity (approximately 19% of the Korean population) in which many identify themselves as Evangelicals.
South Korea has been referred as an "evangelical superpower" for being the home to some of the largest and most dynamic Christian churches in the world; South Korea is also second to the U.S. in the number of missionaries sent abroad.
United Kingdom.
There are an estimated 2 million Evangelicals in the UK. According to research performed by the Evangelical Alliance in 2013, 87% of UK evangelicals attend Sunday morning church services every week and 63% attend weekly or fortnightly small groups. An earlier survey conducted in 2012 found that 92% of evangelicals agree it is a Christian's duty to help those in poverty and 45% attend a church which has a fund or scheme that helps people in immediate need, and 42% go to a church that supports or runs a foodbank. 63% believe a tithing, and so give around 10% of their income to their church, Christian organisations and various charities 83% of UK evangelicals believe that the Bible has supreme authority in guiding their beliefs, views and behaviour and 52% read or listen to the Bible daily. The Evangelical Alliance, formed in 1846, was the first ecumenical evangelical body in the world and works to unite evangelicals, helping them listen to, and be heard by, the government, media and society.
United States.
The contemporary North American usage of the term reflects the impact of the Fundamentalist–Modernist Controversy of the early 20th century. Evangelicalism may sometimes be perceived as the middle ground between the theological liberalism of the mainline denominations and the cultural separatism of fundamentalism. Evangelicalism has therefore been described as "the third of the leading strands in American Protestantism, straddl[ing] the divide between fundamentalists and liberals". In 2004 Andrew Crouch wrote in "Christianity Today": "The emerging movement is a protest against much of evangelicalism as currently practiced. It is post-evangelical in the way that neo-evangelicalism (in the 1950s) was post-fundamentalist. It would not be unfair to call it postmodern evangelicalism."
While the North American perception has a certain importance in understanding some usage of the term, it by no means dominates a wider global view: elsewhere the fundamentalist debate had less direct influence.
D.W. Cloud wrote: "In the first half of the 20th century, evangelicalism in America was largely synonymous with fundamentalism. George Marsden in "Reforming Fundamentalism" (1995) writes, "There was not a practical distinction between fundamentalist and evangelical: the words were interchangeable" (p. 48). When the National Association of Evangelicals (NAE) formed in 1942, for example, participants included such fundamentalist leaders as Bob Jones, Sr., John R. Rice, Charles Woodbridge, Harry Ironside, and David Otis Fuller."
By the mid-1950s, largely due to the ecumenical evangelism of Billy Graham, the terms "Evangelicalism" and "fundamentalism" began to refer to two different approaches. Fundamentalism aggressively attacked its liberal enemies; Evangelicalism downplayed liberalism and emphasized outreach and conversion of new members.
While some conservative Evangelicals believe the label has "broadened" too much beyond its more limiting traditional distinctives, this trend is nonetheless strong enough to create significant ambiguity in the term. As a result, the dichotomy between "Evangelical" and "mainline" denominations is increasingly complex, particularly with such innovations as the "emergent church" movement.
20th century.
By the 1890s, most American Protestants belonged to Evangelical denominations, except for the high church Episcopalians and German Lutherans. In the early 20th century, a divide opened up between the Fundamentalists and the Mainline Protestant denominations, chiefly over the inerrancy of the Bible. The fundamentalists were those Evangelicals who sought to defend their religious traditions, and feared that modern scientific leanings were leading away from the truth. A favored mode of fighting back was to prohibit the teaching of Darwinism or macro-evolution as fact in the public schools, a movement that reached its peak in the Scopes Trial of 1925, and resumed in the 1980s. The more modernistic Protestants largely abandoned the term "evangelical" and tolerated evolutionary theories in modern science and even in Biblical studies.
Evangelicals held the view that the modernist and liberal parties in the Protestant churches had surrendered their heritage as Evangelicals by accommodating the views and values of secularism. At the same time, the modernists criticized fundamentalists for their separatism and their rejection of the Social Gospel.
During and after World War II, Evangelicals increasingly organized, and expanded their vision to include the entire world. There was a great expansion of Evangelical activity within the United States, "a revival of revivalism." Youth for Christ was formed; it later became the base for Billy Graham's revivals. The National Association of Evangelicals formed in 1942 as a counterpoise to the mainline Federal Council of Churches. In 1942–43, the Old-Fashioned Revival Hour had a record-setting national radio audience.
Even more dramatic was the expansion of international missionary activity by the Evangelicals. They had enthusiasm and self-confidence after the national victory in the world war. Many Evangelicals came from poor rural districts, but wartime and postwar prosperity dramatically increased the funding resources available for missionary work. While mainline Protestant denominations cut back on their missionary activities, from 7000 to 3000 overseas workers between 1935 and 1980, the Evangelicals increased their career foreign missionary force from 12,000 in 1935 to 35,000 in 1980. Meanwhile Europe was falling behind, as North Americans comprised 41% of all the world's Protestant missionaries in 1936, rising to 52% in 1952 and 72% in 1969. The most active denominations were the Assemblies of God, which nearly tripled from 230 missionaries in 1935 to 626 in 1952, and the United Pentecostal Church International, formed in 1945. The Southern Baptists more than doubled from 405 to 855, as did the Church of the Nazarene from 88 to 200. Overseas missionaries began to prepare for the postwar challenge, most notably the Far Eastern Gospel Crusade (FEGC; now named "Send International"). After Nazi Germany and fascist Japan had been destroyed, the newly mobilized Evangelicals were now prepared to combat atheistic communism, secularism, Darwinism, liberalism, Catholicism, and (in overseas missions) paganism.
Meaning of Evangelicalism in the US.
The Institute for the Study of American Evangelicals states:
There are three senses in which the term "evangelical" is used today at the beginning of the 21st-century. The first is to view "evangelical" as all Christians who affirm a few key doctrines and practical emphases. British historian David Bebbington approaches evangelicalism from this direction and notes four specific hallmarks of evangelical religion: conversionism, the belief that lives need to be changed; activism, the expression of the gospel in effort; biblicism, a particular regard for the Bible; and crucicentrism, a stress on the sacrifice of Christ on the cross.
A second sense is to look at evangelicalism as an organic group of movements and religious tradition. Within this context "evangelical" denotes a style as much as a set of beliefs. As a result, groups such as black Baptists and Dutch Reformed Churches, Mennonites and Pentecostals, Catholic charismatics and Southern Baptists all come under the evangelical umbrella, thus demonstrating just how diverse the movement really is.
A third sense of the term is as the self-ascribed label for a coalition that arose during the Second World War. This group came into being as a reaction against the perceived anti-intellectual, separatist, belligerent nature of the fundamentalist movement in the 1920s and 1930s. Importantly, its core personalities (like Harold John Ockenga and Billy Graham), institutions (for instance, Moody Bible Institute and Wheaton College), and organizations (such as the National Association of Evangelicals and Youth for Christ) have played a pivotal role in giving the wider movement a sense of cohesion that extends beyond these "card-carrying" evangelicals.
Demographics.
The 2004 survey of religion and politics in the United States identified the Evangelical percentage of the population at 26.3 percent while Roman Catholics are 22 percent and mainline Protestants make up 16 percent. In the 2007 Statistical Abstract of the United States, the figures for these same groups are 28.6 percent (Evangelical), 24.5 percent (Roman Catholic), and 13.9 percent (mainline Protestant.) The latter figures are based on a 2001 study of the self-described religious identification of the adult population for 1990 and 2001 from the Graduate School and University Center at the City University of New York. A 2008 study showed that in the year 2000 about 9 percent of Americans attended an Evangelical service on any given Sunday. The Economist estimated in May 2012, that "over one-third of Americans, more than 100 M, can be considered evangelical," arguing that the percentage is often undercounted because many black Christians espouse Evangelical theology but prefer to refer to themselves as "born again Christians" rather than "evangelical." These estimated figures given by The Economist agree with those in 2012 from Wheaton College's Institute for the Studies of American Evangelicals.
The movement is highly diverse and encompasses a vast number of people. Because the group is diverse, not all of them use the same terminology for beliefs. For instance, several recent studies and surveys by sociologists and political scientists that utilize more complex definitional parameters have estimated the number of Evangelicals in the U.S. in 2012 at about 30–35% of the population, or roughly between 90 and 100 million people.
The National Association of Evangelicals is a U.S. agency which coordinates cooperative ministry for its member denominations.
Types of Evangelical.
John C. Green, a senior fellow at the Pew Forum on Religion and Public Life, used polling data to separate Evangelicals into three camps, which he labels as traditionalist, centrist and modernist:
Politics.
Christian right.
Evangelical political influence in America was first evident in the 1830s with movements such as abolition of slavery and the prohibition movement, which closed saloons and taverns in state after state until it succeeded nationally in 1919. The Christian right is a coalition of numerous groups of traditionalist and observant church-goers of every kind: especially Catholics on issues such as birth control and abortion, Southern Baptists, Missouri Synod Lutherans and others.
Evangelical political activists are not all on the right. There is a small group of liberal white Evangelicals. Most African Americans belong to Baptist, Methodist or other denominations that share Evangelical beliefs; they are firmly in the Democratic coalition and (except for gay and abortion issues) are generally liberal in politics.
Christian left.
Typically, members of the Evangelical left affirm the primary tenets of Evangelical theology, such as the doctrines of Incarnation, atonement, and resurrection, and also see the Bible as a primary authority for the church. A major theological difference, however, which in turn leads to many of the social/political differences, is the issue of how strictly to interpret the Bible, as well as what particular values and principles predominantly constitute the "biblical worldview" believed to be binding upon all followers. Inevitably, battles over how to characterize each other and themselves ensue, with the Evangelical left and right often hyperbolically regarding each other as "mainline/non-evangelical" and "fundamentalist" respectively.
Unlike conservative Evangelicals, the Evangelical left is generally opposed to capital punishment and supportive of gun control. In many cases, Evangelical leftists are pacifistic. Some promote the legalization of same-sex marriage or protection of access to abortion for the society at large without necessarily endorsing the practice themselves. There is considerable dispute over how to even characterize the various segments of the Evangelical theological and political spectra, and whether a singular discernible rift between "right" and "left" is oversimplified. However, to the extent that some simplifications are necessary to discuss any complex issue, it is recognized that modern trends like focusing on non-contentious issues (like poverty) and downplaying hot-button social issues (like abortion) tend to be key distinctives of the modern "evangelical left" or "emergent church" movement.
Recurrent themes.
Abortion.
Since 1980, a central issue motivating conservative Evangelicals' political activism is abortion. The 1973 decision in "Roe v. Wade" by the Supreme Court, which legalized abortion, proved decisive in bringing together Catholics and Evangelicals in a political coalition, which became known as the Religious Right when it successfully mobilized its voters behind presidential candidate Ronald Reagan in 1980.
Secularism.
In the United States, Supreme Court decisions that outlawed organized prayer in school and restricted church-related schools also played a role in mobilizing the Religious Right. In addition, questions of sexual morality and homosexuality have been energizing factors—and above all, the notion that "elites" are pushing America into secularism.
Christian nation.
Opponents criticise the Evangelicals, whom they say actually want a Christian America—America being a nation in which Christianity is given a privileged position. Survey data shows that "between 64 and 75 percent do not favor a 'Christian Nation' amendment", though between 60 and 75 percent also believe that Christianity and Political Liberalism are incompatible. Evangelical leaders, in turn, counter that they merely seek freedom from the imposition by national elites of an equally subjective secular worldview, and feel that it is their opponents who are violating their rights.
Other issues.
According to recent reports in the "New York Times", some Evangelicals have sought to expand their movement's social agenda to include poverty, combating AIDS in the Third World, and protecting the environment. This is highly contentious within the Evangelical community, since more conservative Evangelicals believe that this trend is compromising important issues and prioritizing popularity and consensus too highly. Personifying this division were the Evangelical leaders James Dobson and Rick Warren, the former who warned of the dangers of a Barack Obama victory in 2008 from his point of view, in contrast with the latter who declined to endorse either major candidate on the grounds that he wanted the church to be less politically divisive and that he agreed substantially with both men.

</doc>
<doc id="10371" url="http://en.wikipedia.org/wiki?curid=10371" title="Euphonium">
Euphonium

Euphonium is a conical-bore, baritone-voiced brass instrument. The euphonium derives its name from the Greek word "euphonos", meaning "well-sounding" or "sweet-voiced" ("eu" means "well" or "good" and "phonos" means "of sound", so "of good sound"). The euphonium is a valved instrument; nearly all current models are piston valved, though rotary valved models do exist. The euphonium is a non-transposing instrument known for its distinctive tone color, wide range, variety of character and agility. A person who plays the euphonium is sometimes called a "euphoniumist", "euphophonist", or a "euphonist", while British players often colloquially refer to themselves as "euphists", or "euphologists." Similarly, the instrument itself is often referred to as "eupho" or "euph".
Name recognition and misconceptions.
The euphonium is part of the family of brass instruments. It is sometimes confused with the baritone horn. The euphonium and the baritone differ in that the bore size of the baritone horn is typically smaller than that of the euphonium, (leading to a "darker" tone from the euphonium and a brighter sound from the baritone horn) and the baritone is primarily cylindrical bore, whereas the euphonium is predominantly conical bore. The two instruments are easily interchangeable to the player, with some modification of breath and embouchure, since the two have identical range and essentially identical fingering. The cylindrical baritone offers a brighter sound and the conical euphonium offers a mellower sound.
The so-called American baritone, featuring three valves on the front of the instrument and a curved, forward-pointing bell, was dominant in American school bands throughout most of the 20th century, its weight, shape and configuration conforming to the needs of the marching band. While this instrument is in reality a conical-cylindrical bore hybrid, neither fully euphonium nor baritone, it was almost universally labeled a "baritone" by both band directors and composers, thus contributing to the confusion of terminology in the United States.
Several late 19th century music catalogs (such as Pepper and Lyon & Healy) sold a euphonium-like instrument called the "B♭ Bass" (to distinguish it from the E♭ and BB♭ bass). In these catalog drawings, the B♭ Bass had thicker tubing than the baritone; both had 3 valves. Along the same lines, drum and bugle corps introduced the "Bass-baritone", and distinguished it from the baritone. The thicker tubing of the 3-valve B♭ Bass allowed for production of strong false-tones, providing chromatic access to the pedal register.
History and development.
As a baritone-voiced brass instrument, the euphonium traces its ancestry to the ophicleide and ultimately back to the serpent. The search for a satisfactory foundational wind instrument that could support masses of sound above it took some time. While the serpent was used for over two centuries dating back to the late Renaissance, it was notoriously difficult to control its pitch and tone quality due to its disproportionately small open finger holes. The ophicleide, which was used in bands and orchestras for a few decades in the early to mid-19th century, used a system of keys and was an improvement over the serpent but was still unreliable, especially in the high register.
With the invention of the piston valve system c. 1818, the construction of brass instruments with an even sound and facility of playing in all registers became possible. The euphonium is said to have been invented, as a "wide-bore, valved bugle of baritone range", by Ferdinand Sommer of Weimar in 1843, though Carl Moritz in 1838 and Adolphe Sax in 1843 have also been credited. While Sax's family of saxhorns were invented at about the same time and the bass saxhorn is very similar to a euphonium, there are also differences.
The "British-style" compensating euphonium was developed by David Blaikley in 1874, and has been in use in Britain ever since; since this time, the basic construction of the euphonium in Britain has changed little.
Construction and general characteristics.
The euphonium, like tenor trombone, is pitched in concert B♭. For a valved brass instrument like the euphonium, this means that when no valves are in use the instrument will produce partials of the B♭ harmonic series. It is generally orchestrated as a non-transposing instrument like the trombone, written at concert pitch in the bass clef with higher passages in the tenor clef. Treble clef euphonium parts transposing down a major ninth are included in much concert band music: in the British-style brass band tradition, euphonium music is always written this way. In continental European band music, parts for the euphonium may be written in the bass clef as a B♭ transposing instrument sounding a major second lower than written.
Professional models have three top-action valves, played with the first three fingers of the right hand, plus a "compensating" fourth valve, generally found midway down the right side of the instrument, played with the left index finger; such an instrument is shown at the top of this page. Beginner models often have only the three top-action valves, while some intermediate "student" models may have a fourth top-action valve, played with the fourth finger of the right hand. Compensating systems are expensive to build, and there is in general a substantial difference in price between compensating and non-compensating models. For a thorough discussion of the valves and the compensation system, see the article on brass instruments.
The euphonium has an extensive range, comfortably from E2 to about B♭4 for intermediate players (using scientific pitch notation). In professional hands this may extend from B0 to as high as B♭5. The lowest notes obtainable depend on the valve set-up of the instrument. All instruments are chromatic down to E2, but 4-valved instruments extend that down to at least C2. Non-compensating four-valved instruments suffer from intonation problems from E♭2 down to C2 and cannot produce the low B1; compensating instruments do not have such intonation problems and can play the low B-natural.
From B♭1 down lies the "pedal range", i.e. the fundamentals of the instrument's harmonic series. They are easily produced on the euphonium as compared to other brass instruments, and the extent of the range depends on the make of the instrument in exactly the same way as just described. Thus, on a compensating four-valved instrument, the lowest note possible is B0, sometimes called double pedal B, which is six ledger lines below the bass clef.
As with the other conical-bore instruments, the cornet, flugelhorn, horn, and tuba, the euphonium's tubing gradually increases in diameter throughout its length, resulting in a softer, gentler tone compared to cylindrical-bore instruments such as the trumpet, trombone, sudrophone, and baritone horn. While a truly characteristic euphonium sound is rather hard to define precisely, most players would agree that an ideal sound is dark, rich, warm, and velvety, with virtually no hardness to it. On the other hand, the desired sound varies geographically; European players, especially British ones, generally use a faster, more constant vibrato and a more veiled tone, while Americans tend to prefer a more straightforward, open sound with slower and less frequent vibrato. This also has to do with the different models preferred by British and American players.
Though the euphonium's fingerings are no different from those of the trumpet or tuba, beginning euphoniumists will likely experience significant problems with intonation, response, and range compared to other beginning brass players. In addition, it is very difficult for students, even of high-school age, to develop the rich sound characteristic of the euphonium, due partly to the instrument models used in schools and partly to the lack of awareness of good euphonium sound models.
Popular models.
Very generally speaking, the most popular professional models of euphonium in the United Kingdom are Besson Prestige and Sovereign models. The most popular in the United States are the Wilson 2900 and 2950. In both cases, these models have gained popularity through the use and sponsorship of extremely highly respected players and teachers; in Britain, by Steven Mead, and in America, by Dr. Brian Bowman. In recent years, the Yamaha YEP-842 Custom has gained popularity in the United States due to similar activities by Adam Frey. Most recently, Demondrae Thurman has worked in conjunction with Miraphone to develop the Ambassador 5050.
In recent years, the Besson company got into financial difficulties and various aspects of the business and name were acquired by Buffet Crampon of France. The remaining assets were acquired by the German company Schreiber-Keilwerth who lost no time in bringing rival instruments, with the York brand name, to market. In 2010, Schrieber-Keilwerth was also acquired by Buffet Crampon, and the York brand was dropped.
Other highly regarded professional models found around the world are the Yamaha 642, the Hirsbrunner Standard, Exclusive, and Stealth, the Sterling Virtuoso, and the Meinl-Weston 451 and 551.
A well-selling and school-inventoried intermediate-model horn in the United States is the Yamaha YEP-321 (and silver-plate 321S), which has four valves and is non-compensating (though a removable 5th valve was offered as an option early on, but discontinued pushing buyers to their "professional" instruments). Other similar models of euphonium are made by Besson, Willson, Jupiter, and under the names of many storied American manufacturers now within the Conn-Selmer umbrella among others. Besson produces a four-valve non-compensating euphonium with the fourth valve on the side.
Double-belled euphonium.
A creation unique to the United States was the double-bell euphonium, featuring a second smaller bell in addition to the main one; the player could switch bells for certain passages or even for individual notes by use of an additional valve, operated with the left hand. Ostensibly, the smaller bell was intended to emulate the sound of a trombone (it was cylindrical-bore) and was possibly intended for performance situations in which trombones were not available. The extent to which the difference in sound and timbre was apparent to the listener, however, is up for debate. "Harry Whittier" of the Patrick S. Gilmore band introduced the instrument in 1888, and it was used widely in both school and service bands for several decades. "Harold Brasch" (see "List of important players" below) brought the British-style compensating euphonium to the United States c. 1939, but the double-belled euphonium may have remained in common use even into the 1950s and 1960s. In any case, they have become rare (they were last in Conn's advertisements in the 1940s, and King's catalog in the 1960s), and are generally unknown to younger players. They are chiefly known now through their mention in the song "Seventy-Six Trombones" from the musical "The Music Man" by Meredith Willson.
Marching euphonium.
A marching version of the euphonium may be found in marching band, though it is often replaced by its smaller, easier-to-carry cousin, the marching baritone (which has a similar bell and valve configuration to a trumpet). Marching euphoniums are used by marching bands in schools, and in Drum and Bugle Corps, and some corps (such as the Blue Devils and Phantom Regiment) march all-euphonium sections rather than only marching Baritone or a mix of both. Depending on the manufacturer, the weight of these instruments can be straining to the average marcher and require great strength to hold during practices and performances, leading to nerve problems in the right pinky, a callous on the left hand, and possibly back and arm problems.
Another form of the marching euphonium is the convertible euphonium. Recently widely produced, the horn resembles a convertible tuba, being able to change from a concert upright to a marching forward bell on either the left or right shoulder. These are mainly produced by Jupiter or Yamaha, but other less expensive versions can be found.
Five valve euphonium.
The five valve euphonium (noncompensating) is an extremely rare variation of the euphonium manufactured in the late 19th and early 20th centuries by Britain's Besson musical instrument company and Highams of Manchester Musical Instrument Company. Besson and Highams's "Clearbore" five valve vintage euphoniums are among the rarest and most valuable in existence.
The Besson five valve euphonium featured the standard three piston valves horizontally on top, but had an additional two piston valves off the side. The standard euphonium has eight possible fingering and non-fingering positions by which sound is produced. The Besson and the Highams 'clearbore' model rare fourth and fifth extra 'side' valves change the possible fingering and non-fingering positions from eight to thirty-two.
The term 'five valve euphonium' does not refer to variations of the double bell euphonium made by various brass instrument companies during the same time period. Some of the double-bell euphoniums had five valves, with the fifth valve either on top with the other four, or by itself off to the side, but the double-bell fifth valve was used for switching the sound to the second smaller trombone-sized bell, and not for changing the fingering pitch of the instrument. Also, Cerveny Musical Instruments manufactures several euphoniums with five vertical rotary valves today, but this is an unrelated recent development.
Performance venues and professional job opportunities.
The euphonium has historically been exclusively a "band" instrument (rather than an "orchestra" or "jazz" instrument), whether of the "wind" or "brass" variety, where it is frequently featured as a solo instrument. Because of this, the euphonium has been called the "king of band instruments", or the "cello of the band", because of its similarity in timbre and ensemble role to the stringed instrument. Euphoniums typically have extremely important parts in many marches (such as those by John Philip Sousa), and in brass band music of the British tradition.
Other performance venues for the euphonium are the tuba-euphonium quartet or larger tuba-euphonium ensemble; the brass quintet, where it can supply the tenor voice, though the trombone is much more common in this role; or in mixed brass ensembles. Though these are legitimate performance venues, paid professional jobs in these areas are almost non-existent; they are much more likely to be semi-professional or amateur in nature. Most of the United States Armed Forces service bands include a tuba-euphonium quartet made up of players from the band that occasionally performs in its own right.
The euphonium is not traditionally an orchestral instrument and has never been common in symphony orchestras. However, there are a handful of works, mostly from the late Romantic period, in which composers wrote a part for "baryton" (German) or "tenor tuba" (most notably, Holst's "Planets Suite", which has many solos for baritone and euphonium), and these are universally played on euphonium, frequently by a trombone player. In addition, the euphonium is sometimes used in older orchestral works as a replacement for its predecessors, such as the ophicleide, or, less correctly, the bass trumpet or the Wagner tuba, both of which are significantly different instruments, and still in use today. At the bottom of the article are some of the well-known orchestral works in which the euphonium is commonly used (whether or not the composer originally specified it).
Finally, while the euphonium was not historically part of the standard jazz big band or combo, the instrument's technical facility and large range make it well-suited to a jazz solo role, and a jazz euphonium niche has been carved out over the last 40 or so years, largely starting with the pioneer Rich Matteson (see "List of important players" below). The euphonium can also double on a trombone part in a jazz combo. Jazz euphoniums are most likely to be found in tuba-euphonium groups, though modern funk or rock bands occasionally feature a brass player doubling on euphonium, and this trend is growing.
Due to this dearth of performance opportunities, aspiring euphonium players in the United States are in a rather inconvenient position when seeking future employment. Often, college players must either obtain a graduate degree and go on to teach at the college level, or audition for one of the major or regional military service bands. Because these bands are relatively few in number and the number of euphonium positions in the bands is small (2–4 in most service bands), job openings do not occur very often and when they do are highly competitive; before the current slate of openings in four separate bands, the last opening for a euphonium player in an American service band was in May 2004. A career strictly as a solo performer, unaffiliated with any university or performing ensemble, is a very rare sight, but some performers, such as Riki McDonnell, have managed to do it.
In Britain the strongest euphonium players are most likely to find a position in a brass band, but even though they often play at world-class levels, the members of the top brass bands are in most cases unpaid amateurs. There are hundreds, if not thousands, of brass bands in Britain ranging in standard from world class to local bands. Almost all brass bands in Britain perform regularly, particularly during the summer months. A large number of bands also enter contests against other brass bands of a similar standard. Each band requires two euphoniums (principal and second) and consequently there are considerable opportunities for euphonium players.
Due to limited vocational opportunities, there are a considerable number of relatively serious, quasi-professional avocational euphonium players participating in many higher-caliber unpaid ensembles. 
College use in the United States.
Unlike a generation or two ago, many colleges with music programs now offer students the opportunity to major in euphonium. However, due to the small number of euphonium students at most schools (2-4 is common), it is possible, and even likely, that they will study with a professor whose major instrument is not the euphonium. Most often tubas and euphoniums will be combined into a studio taught by one professor, and at small schools they may be grouped with trombones and/or French horns as well, taught by one low brass professor. Universities will usually require professors in this situation to have a high level of proficiency on all the instruments they teach, and some of the best college euphonium studios are taught by non-euphonium players.
Notable euphonists.
The euphonium world is and has become more crowded than is commonly thought, and there have been many noteworthy players throughout the instrument's history. Traditionally, three main national schools of euphonium playing have been discernible: American, British, and Japanese. Now, euphoniumists are able to learn this specific art in many other countries around the world today.
German Ferdinand Sommer, if one discounts the claims of Moritz and Sax each of whose horns also approached a euphonium in nature, in addition to being credited with inventing the euphonium as the Sommerhorn in 1843, as a soloist on the horn, qualifies as the first euphonium player to significantly advance and alter the understanding of the instrument. Below are a select few of the players most famous and influential in their respective countries, and whose contributions to the euphonium world are undeniable, in terms of recordings, commissions, pedagogy, and increased recognition of the instrument.
Literature.
The euphonium repertoire consists of solo literature and orchestral or, more commonly, band parts written for the euphonium. Since its invention in 1843, the euphonium has always had an important role in ensembles, but solo literature was slow to appear, consisting of only a handful of lighter solos until the 1960s. Since then, however, the breadth and depth of the solo euphonium repertoire has increased dramatically.
In the current age, there has been a huge number of new commissions and repertoire development and promotion through Steven Mead’s World of the Euphonium Series and the Beyond the Horizon series from Euphonium.com. There has also been a vast number of new commissions by more and more players and a proliferation of large scale Consortium Commissions that are occurring including current ones in 2008 and 2009 organized by Brian Meixner (Libby Larson), Adam Frey (The Euphonium Foundation Consortium), and Jason Ham (David Gillingham).
Upon its invention, it was clear that the euphonium had, compared to its predecessors the serpent and ophicleide, a wide range and had a consistently rich, pleasing sound throughout that range. It was flexible both in tone quality and intonation and could blend well with a variety of ensembles, gaining it immediate popularity with composers and conductors as the principal tenor-voices solo instrument in brass band settings, especially in Britain. It is no surprise, then, that when British composers – some of the same ones who were writing for brass bands – began to write serious, original music for the concert band in the early 20th century, they used the euphonium in a very similar role.
When American composers also began writing for the concert band as its own artistic medium in the 1930s and 1940s, they continued the British brass and concert band tradition of using the euphonium as the principal tenor-voiced solo.
This is not to say that composers, then and now, valued the euphonium only for its lyrical capabilities. Indeed, examination of a large body of concert band literature reveals that the euphonium functions as a "jack of all trades."
Though the euphonium was, as previously noted, embraced from its earliest days by composers and arrangers in band settings, orchestral composers have, by and large, not taken advantage of this capability. There are, nevertheless, several orchestral works, a few of which are standard repertoire, in which composers have called for instruments, such as the Wagner tuba, for which euphonium is commonly substituted today.
In contrast to the long-standing practice of extensive euphonium use in wind bands and orchestras, there was until approximately forty years ago literally no body of solo literature written specifically for the euphonium, and euphoniumists were forced to borrow the literature of other instruments. Fortunately, given the instrument's multifaceted capabilities discussed above, solos for many different instruments are easily adaptable to performance on the euphonium.
The earliest surviving solo composition written specifically for euphonium or one of its saxhorn cousins is the "Concerto per Flicorno Basso" (1872) by Amilcare Ponchielli. For almost a century after this, the euphonium solo repertoire consisted of only a dozen or so virtuosic pieces, mostly light in character. However, in the 1960s and 1970s, American composers began to write the first of the "new school" of serious, artistic solo works specifically for euphonium. Since then, there has been a virtual explosion of solo repertoire for the euphonium. In a mere four decades, the solo literature has expanded from virtually zero to thousands of pieces. More and more composers have become aware of the tremendous soloistic capabilities of the euphonium, and have constantly "pushed the envelope" with new literature in terms of tessitura, endurance, technical demands, and extended techniques.
Finally, the euphonium has, thanks to a handful of enterprising individuals, begun to make inroads in jazz, pop and other non-concert performance settings.
References.
Explanatory notes
Citations
Sources

</doc>
<doc id="10372" url="http://en.wikipedia.org/wiki?curid=10372" title="Entire function">
Entire function

In complex analysis, an entire function, also called an integral function, is a complex-valued function that is holomorphic over the whole complex plane. Typical examples of entire functions are polynomials and the exponential function, and any sums, products and compositions of these, such as the trigonometric functions sine and cosine and their hyperbolic counterparts sinh and cosh, as well as derivatives and integrals of entire functions such as the error function. If an entire function "f"("z") has a root at "w", then "f"("z")/("z−w") is an entire function. On the other hand, neither the natural logarithm nor the square root is an entire function, nor can they be continued analytically to an entire function.
A transcendental entire function is an entire function that is not a polynomial.
Properties.
Every entire function "f"("z") can be represented as a power series
that converges everywhere in the complex plane, hence uniformly on compact sets. The radius of convergence is infinite, which implies that
or
Any power series satisfying this criterion will represent an entire function.
If the real part of an entire function is known in a neighborhood of a point then both the real and imaginary parts are known for the whole complex plane, up to an imaginary constant. For instance, if the real part is known in a neighborhood of zero, then we can find the coefficients for "n" > 0 from the following derivatives with respect to a real variable "r":
The Weierstrass factorization theorem asserts that any entire function can be represented by a product involving its zeroes (or "roots").
The entire functions on the complex plane form an integral domain (in fact a Prüfer domain). They also form a commutative unital associative algebra over the complex numbers.
Liouville's theorem states that any bounded entire function must be constant. Liouville's theorem may be used to elegantly prove the fundamental theorem of algebra.
As a consequence of Liouville's theorem, any function that is entire on the whole Riemann sphere (complex plane "and" the point at infinity) is constant. Thus any non-constant entire function must have a singularity at the complex point at infinity, either a pole for a polynomial or an essential singularity for a transcendental entire function. Specifically, by the Casorati–Weierstrass theorem, for any transcendental entire function "f" and any complex "w" there is a sequence ("zm")"m"∈N with formula_6 and formula_7.
Picard's little theorem is a much stronger result: any non-constant entire function takes on every complex number as value, possibly with a single exception. When an exception exists, it is called a lacunary value of the function. The possibility of a lacunary value is illustrated by the exponential function, which never takes on the value 0. One can take a logarithm of an entire function that never hits 0, and this will also be an entire function (according to the Weierstrass factorization theorem). The logarithm hits every complex number except possibly one number, which implies that the first function will hit any value other than 0 an infinite number of times. Similarly, a non-constant, entire function that does not hit a particular value will hit every other value an infinite number of times.
Liouville's theorem is a special case of the following statement:
Theorem: Assume "M, R" are positive constants and that "n" is a non-negative integer. An entire function "f" satisfying the inequality formula_8 for all "z" with formula_9, is necessarily a polynomial, of degree at most "n". Similarly, an entire function "f" satisfying the inequality formula_10 for all "z" with formula_9, is necessarily a polynomial, of degree at least "n".
If (and only if) the coefficients of the power series are all real then the function (obviously) takes real values for real arguments, and the value of the function at the complex conjugate of "z" will be the complex conjugate of the value at "z". Such functions are sometimes called self-conjugate (the conjugate function, formula_12, being given by formula_13
Growth.
Entire functions may grow as fast as any increasing function: for any increasing function "g" : [0,+∞) → [0,+∞) there exists an entire function "f(z)" such that "f"("x") > "g"( |"x"| ) for all real "x". Such a function "f" may be easily found of the form:
for a constant "c" and a strictly increasing sequence of positive integers "nk". Any such sequence defines an entire function "f"("z"), and if the powers are chosen appropriately we may satisfy the inequality "f"("x") > "g"( |"x"| ) for all real "x". (For instance, it certainly holds if one chooses "c":="g("2")" and, for any integer "k ≥ 1", formula_15 although this gives powers that may be about twice as high as needed.)
Order and type.
The order (at infinity) of an entire function "f(z)" is defined using the limit superior as:
where "Br" is the disk of radius "r" and formula_17 denotes the supremum norm of "f(z)" on "Br". The order is a non-negative real number or infinity (except if "f(z)"=0 for all "z"). The order of "f"("z") is the infimum of all "m" such that "f(z)" = O(exp(|"z"|"m")) as "z" → ∞. (A function like formula_18 shows that this does not mean "f(z)" = O(exp(|"z"|"m")) if "f(z)" is of order "m".)
If 0<ρ<∞, one can also define the type:
If the order is 1 and the type is σ, the function is said to be "of exponential type σ". If it is of order less than 1 it is said to be of exponential type 0.
If
then the order and type can be found by the formulas
If we denote the "n"th derivative of a function "f"() by "f"("n")(), then we may restate these formulas in terms of the derivatives at any arbitrary point "z"0:
The type may be infinite, as in the case of the reciprocal gamma function, or zero (see example below under #Order 1).
Examples.
Here are some examples of functions of various orders:
Order ρ.
For arbitrary positive numbers ρ and σ one can construct an example of an entire function of order ρ and type σ using:
Genus of an entire function.
Entire functions of finite order have Hadamard's canonical representation:
where "zk" are the non-zero roots of "f", "P" a polynomial (whose degree we shall call "q"), and "p" is the smallest non-negative integer such that the series
converges. The non-negative integer "g" = max{"p", "q"} is called the genus of the entire function "f".
If the order ρ is not an integer, then "g" = [ρ] is the integer part of ρ. If the order is a positive integer, then there are two possibilities: "g" = ρ-1 or "g" = ρ.
For example, "sin", "cos" and "exp" are entire functions of genus "1".
Other examples.
According to J. E. Littlewood, the Weierstrass sigma function is a 'typical' entire function. This statement can be made precise in the theory of random entire functions: the asymptotic behaviour of almost all entire functions is similar to that of the sigma function. Other examples include the Fresnel integrals, the Jacobi theta function, and the reciprocal Gamma function. The exponential function and the error function are special cases of the Mittag-Leffler function. According to the fundamental theorem of Paley and Wiener, Fourier transforms of functions with bounded support are entire functions or order "1" and finite type.
Other examples are solutions of linear differential equations with polynomial coefficients. If the coefficient at the highest derivative is constant, then all solutions of such equations are entire functions. For example, the exponential function, sine, cosine, Airy functions and Parabolic cylinder functions arise in this way. The class of entire functions is closed with respect to compositions. This makes it possible to study dynamics of entire functions.
An entire function of the square root of a complex number is entire if the original function is even, for example formula_33.
If a sequence of polynomials all of whose roots are real converges in a neighborhood of the origin to a limit which is not identically equal to zero, then this limit
is an entire function. Such entire functions form the Laguerre–Pólya class, which can also be characterized in terms of the Hadamard product, namely, "f" belongs to this class if and only if in the Hadamard representation all "zn" are real, "p" ≤ 1, and "P"("z") = "a" + "bz" + "cz"2, where "b" and "c" are real, and "c" ≤ 0. For example, the sequence of polynomials formula_34 converges, as "n" increases, to exp(−("z"−"d")2). The polynomials formula_35 have all real roots, and converge to cos("z"). Interestingly, the polynomials formula_36 also converge to cos("z"), showing the buildup of the Hadamard product for cosine.

</doc>
<doc id="10374" url="http://en.wikipedia.org/wiki?curid=10374" title="Essay">
Essay

Essays are generally scholarly pieces of writing giving the author's own argument, but the definition is vague, overlapping with those of an article, a pamphlet and a short story.
Essays can consist of a number of elements, including: literary criticism, political manifestos, learned arguments, observations of daily life, recollections, and reflections of the author. Almost all modern essays are written in prose, but works in verse have been dubbed essays (e.g. Alexander Pope's "An Essay on Criticism" and "An Essay on Man"). While brevity usually defines an essay, voluminous works like John Locke's "An Essay Concerning Human Understanding" and Thomas Malthus's "An Essay on the Principle of Population" are counterexamples.
In some countries (e.g., the United States and Canada), essays have become a major part of formal education. Secondary students are taught structured essay formats to improve their writing skills, and admission essays are often used by universities in selecting applicants and, in the humanities and social sciences, as a way of assessing the performance of students during final exams.
The concept of an "essay" has been extended to other mediums beyond writing. A film essay is a movie that often incorporates documentary film making styles and which focuses more on the evolution of a theme or an idea. A photographic essay is an attempt to cover a topic with a linked series of photographs; it may or may not have an accompanying text or captions.
Definitions.
An essay has been defined in a variety of ways. One definition is a "prose composition with a focused subject of discussion" or a "long, systematic discourse".
It is difficult to define the genre into which essays fall. Aldous Huxley, a leading essayist, gives guidance on the subject. He notes that "the essay is a literary device for saying almost everything about almost anything", and adds that "by tradition, almost by definition, the essay is a short piece". Furthermore, Huxley argues that "essays belong to a literary species whose extreme variability can be studied most effectively within a three-poled frame of reference". 
These three poles (or worlds in which the essay may exist) are:
Huxley adds that "the most richly satisfying essays are those which make the best not of one, not of two, but of all the three worlds in which it is possible for the essay to exist".
The word "essay" derives from the French infinitive "essayer", "to try" or "to attempt". In English "essay" first meant "a trial" or "an attempt", and this is still an alternative meaning. The Frenchman Michel de Montaigne (1533–1592) was the first author to describe his work as essays; he used the term to characterize these as "attempts" to put his thoughts into writing, and his essays grew out of his commonplacing. Inspired in particular by the works of Plutarch, a translation of whose "Oeuvres Morales" ("Moral works") into French had just been published by Jacques Amyot, Montaigne began to compose his essays in 1572; the first edition, entitled "Essais", was published in two volumes in 1580. For the rest of his life he continued revising previously published essays and composing new ones. Francis Bacon's essays, published in book form in 1597, 1612, and 1625, were the first works in English that described themselves as "essays". Ben Jonson first used the word "essayist" in English in 1609, according to the "Oxford English Dictionary".
History.
Europe.
English essayists included Robert Burton (1577–1641) and Sir Thomas Browne (1605–1682). In France, Michel de Montaigne's three volume "Essais" in the mid 1500s contain over 100 examples widely regarded as the predecessor of the modern essay. In Italy, Baldassare Castiglione wrote about courtly manners in his essay "Il libro del cortegiano". In the 17th century, the Jesuit Baltasar Gracián wrote about the theme of wisdom. During the Age of Enlightenment, essays were a favored tool of polemicists who aimed at convincing readers of their position; they also featured heavily in the rise of periodical literature, as seen in the works of Joseph Addison, Richard Steele and Samuel Johnson. In the 18th and 19th centuries, Edmund Burke and Samuel Taylor Coleridge wrote essays for the general public. The early 19th century in particular saw a proliferation of great essayists in English – William Hazlitt, Charles Lamb, Leigh Hunt and Thomas de Quincey all penned numerous essays on diverse subjects. In the 20th century, a number of essayists tried to explain the new movements in art and culture by using essays (e.g., T.S. Eliot). Whereas some essayists used essays for strident political themes, Robert Louis Stevenson and Willa Cather wrote lighter essays. Virginia Woolf, Edmund Wilson, and Charles du Bos wrote literary criticism essays.
Japan.
As with the novel, essays existed in Japan several centuries before they developed in Europe with a genre of essays known as "zuihitsu" —loosely connected essays and fragmented ideas—. Zuihitsu have existed since almost the beginnings of Japanese literature. Many of the most noted early works of Japanese literature are in this genre. Notable examples include "The Pillow Book" (c. 1000), by court lady Sei Shōnagon, and "Tsurezuregusa" (1330), by particularly renowned Japanese Buddhist monk Yoshida Kenkō. Kenkō described his short writings similarly to Montaigne, referring to them as "nonsensical thoughts" written in "idle hours". Another noteworthy difference from Europe is that women have traditionally written in Japan, though the more formal, Chinese-influenced writings of male writers were more prized at the time.
As an educational tool.
In countries like the United States and the United Kingdom, essays have become a major part of a formal education in the form of free response questions. Secondary students in these countries are taught structured essay formats to improve their writing skills, and essays are often used by universities in these countries in selecting applicants ("see" admissions essay). In both secondary and tertiary education, essays are used to judge the mastery and comprehension of material. Students are asked to explain, comment on, or assess a topic of study in the form of an essay. During some courses, university students will often be required to complete one or more essays that are prepared over several weeks or months. In addition, in fields such as the humanities and social sciences, mid-term and end of term examinations often require students to write a short essay in two or three hours.
In these countries, so-called academic essays, which may also be called "papers", are usually more formal than literary ones. They may still allow the presentation of the writer's own views, but this is done in a logical and factual manner, with the use of the first person often discouraged. Longer academic essays (often with a word limit of between 2,000 and 5,000 words) are often more discursive. They sometimes begin with a short summary analysis of what has previously been written on a topic, which is often called a literature review.
Longer essays may also contain an introductory page in which words and phrases from the title are tightly defined. Most academic institutions will require that all substantial facts, quotations, and other porting material used in an essay be referenced in a bibliography or works cited page at the end of the text. This scholarly convention allows others (whether teachers or fellow scholars) to understand the basis of the facts and quotations used to support the essay's argument, and thereby help to evaluate to what extent the argument is supported by evidence, and to evaluate the quality of that evidence. The academic essay tests the student's ability to present their thoughts in an organized way and is designed to test their intellectual capabilities.
One essay guide of a US university makes the distinction between research papers and discussion papers. The guide states that a "research paper is intended to uncover a wide variety of sources on a given topic". As such, research papers "tend to be longer and more inclusive in their scope and with the amount of information they deal with." While discussion papers "also include research, ...they tend to be shorter and more selective in their approach...and more analytical and critical". Whereas a research paper would typically quote "a wide variety of sources", a discussion paper aims to integrate the material in a broader fashion.
One of the challenges facing US universities is that in some cases, students may submit essays which have been purchased from an essay mill (or "paper mill") as their own work. An "essay mill" is a ghostwriting service that sells pre-written essays to university and college students. Since plagiarism is a form of academic dishonesty or academic fraud, universities and colleges may investigate papers suspected to be from an essay mill by using Internet plagiarism detection software, which compares essays against a database of known mill essays and by orally testing students on the contents of their papers.
Forms and styles.
This section describes the different forms and styles of essay writing. These forms and styles are used by an array of authors, including university students and professional essayists.
Cause and effect.
The defining features of a "cause and effect" essay are causal chains that connect from a cause to an effect, careful language, and chronological or emphatic order. A writer using this rhetorical method must consider the subject, determine the purpose, consider the audience, think critically about different causes or consequences, consider a thesis statement, arrange the parts, consider the language, and decide on a conclusion.
Classification and division.
Classification is the categorization of objects into a larger whole while division is the breaking of a larger whole into smaller parts.
Compare and contrast.
Compare and contrast essays are characterized by a basis for comparison, points of comparison, and analogies. It is grouped by object (chunking) or by point (sequential). Comparison highlights the similarities between two or more similar objects while contrasting highlights the differences between two or more objects. When writing a compare/contrast essay, writers need to determine their purpose, consider their audience, consider the basis and points of comparison, consider their thesis statement, arrange and develop the comparison, and reach a conclusion. Compare and contrast is arranged emphatically.
Descriptive.
Descriptive writing is characterized by sensory details, which appeal to the physical senses, and details that appeal to a reader's emotional, physical, or intellectual sensibilities. Determining the purpose, considering the audience, creating a dominant impression, using descriptive language, and organizing the description are the rhetorical choices to be considered when using a description. A description is usually arranged spatially but can also be chronological or emphatic. The focus of a description is the scene. Description uses tools such as denotative language, connotative language, figurative language, metaphor, and simile to arrive at a dominant impression. One university essay guide states that "descriptive writing says what happened or what another author has discussed; it provides an account of the topic".
Lyric essays are an important form of descriptive essays.
Dialectic.
In the dialectic form of essay, which is commonly used in Philosophy, the writer makes a thesis and argument, then objects to their own argument (with a counterargument), but then counters the counterargument with a final and novel argument. This form benefits from presenting a broader perspective while countering a possible flaw that some may present.
Exemplification.
An exemplification essay is characterized by a generalization and relevant, representative, and believable examples including anecdotes. Writers need to consider their subject, determine their purpose, consider their audience, decide on specific examples, and arrange all the parts together when writing an exemplification essay.
Familiar.
A familiar essay is one in which the essayist speaks as if to a single reader. He speaks about both himself and a particular subject. Anne Fadiman notes that "the genre's heyday was the early nineteenth century," and that its greatest exponent was Charles Lamb. She also suggests that while critical essays have more brain than heart, and personal essays have more heart than brain, familiar essays have equal measures of both.
History (thesis).
A history essay, sometimes referred to as a thesis essay, will describe an argument or claim about one or more historical events and will support that claim with evidence, arguments and references. The text makes it clear to the reader why the argument or claim is as such.
Narrative.
A narrative uses tools such as flashbacks, flash-forwards, and transitions that often build to a climax. The focus of a narrative is the plot. When creating a narrative, authors must determine their purpose, consider their audience, establish their point of view, use dialogue, and organize the narrative. A narrative is usually arranged chronologically.
Critical.
A critical essay is an argumentative piece of writing, aimed at presenting objective analysis of the subject matter, narrowed down to a single topic. The main idea of all the criticism is to provide an opinion either of positive or negative implication. As such, a critical essay requires research and analysis, strong internal logic and sharp structure. Each argument should be supported with sufficient evidence, relevant to the point.
Economics.
An economic essay can start with a thesis, or it can start with a theme. It can take a narrative course and a descriptive course. It can even become an argumentative essay if the author feels the need. After the introduction the author has to do his/her best to expose the economic matter at hand, to analyse it, evaluate it, and draw a conclusion. If the essay takes more of a narrative form then the author has to expose each aspect of the economic puzzle in a way that makes it clear and understandable for the reader
Other logical structures.
The logical progression and organizational structure of an essay can take many forms. Understanding how the movement of thought is managed through an essay has a profound impact on its overall cogency and ability to impress. A number of alternative logical structures for essays have been visualized as diagrams, making them easy to implement or adapt in the construction of an argument.
Magazine or newspaper.
Essays often appear in magazines, especially magazines with an intellectual bent, such as "The Atlantic" and "Harpers". Magazine and newspaper essays use many of the essay types described in the section on forms and styles (e.g., descriptive essays, narrative essays, etc.). Some newspapers also print essays in the op-ed section.
Employment.
Employment essays detailing experience in a certain occupational field are required when applying for some jobs, especially government jobs in the United States. Essays known as Knowledge Skills and Executive Core Qualifications are required when applying to certain US federal government positions.
A KSA, or "Knowledge, Skills, and Abilities," is a series of narrative statements that are required when applying to Federal government job openings in the United States. KSAs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The knowledge, skills and abilities necessary for the successful performance of a position are contained on each job vacancy announcement. KSAs are brief and focused essays about one's career and educational background that presumably qualify one to perform the duties of the position being applied for.
An Executive Core Qualification, or ECQ, is a narrative statement that is required when applying to Senior Executive Service positions within the US Federal government. Like the KSAs, ECQs are used along with resumes to determine who the best applicants are when several candidates qualify for a job. The Office of Personnel Management has established five executive core qualifications that all applicants seeking to enter the Senior Executive Service must demonstrate.
Non-literary types.
Visual Arts.
In the visual arts, an essay is a preliminary drawing or sketch upon which a final painting or sculpture is based, made as a test of the work's composition (this meaning of the term, like several of those following, comes from the word "essay"'s meaning of "attempt" or "trial").
Music.
In the realm of music, composer Samuel Barber wrote a set of "Essays for Orchestra," relying on the form and content of the music to guide the listener's ear, rather than any extra-musical plot or story.
Film.
A film essay (or "cinematic essay") consists of the evolution of a theme or an idea rather than a plot per se; or the film literally being a cinematic accompaniment to a narrator reading an essay. From another perspective, an essay film could be defined as a documentary film visual basis combined with a form of commentary that contains elements of self-portrait (rather than autobiography), where the signature (rather than the life story) of the filmmaker is apparent. The cinematic essay often blends documentary, fiction, and experimental film making using a tones and editing styles.
The genre is not well-defined but might include propaganda works of early Soviet parliamentarians like Dziga Vertov, present-day filmmakers including Chris Marker, Michael Moore ("Roger & Me" (1989), "Bowling for Columbine" (2002) and "Fahrenheit 9/11" (2004)), Errol Morris ("The Thin Blue Line" (1988)), Morgan Spurlock ("Supersize Me: A Film of Epic Proportions") and Agnès Varda. Jean-Luc Godard describes his recent work as "film-essays". Two filmmakers whose work was the antecedent to the cinematic essay include Georges Méliès and Bertolt Brecht. Méliès made a short film ("The Coronation of Edward VII" (1902)) about the 1902 coronation of King Edward VII, which mixes actual footage with shots of a recreation of the event. Brecht was a playwright who experimented with film and incorporated film projections into some of his plays. Orson Welles made an essay film in his own pioneering style which was released in 1974 called "F for Fake", which dealt specifically with art forger Elmyr de Hory and with the themes of deception, "fakery," and authenticity in general.
David Winks Gray's article "The essay film in action" states that the "essay film became an identifiable form of film making in the 1950s and '60s". He states that since that time, essay films have tended to be "on the margins" of the film making world. Essay films have a "peculiar searching, questioning tone" which is "between documentary and fiction" but without "fitting comfortably" into either genre. Gray notes that just like written essays, essay films "tend to marry the personal voice of a guiding narrator (often the director) with a wide swath of other voices". The University of Wisconsin Cinematheque website echoes some of Gray's comments; it calls a film essay an "intimate and allusive" genre that "catches filmmakers in a pensive mood, ruminating on the margins between fiction and documentary" in a manner that is "refreshingly inventive, playful, and idiosyncratic".
Photography.
A photographic essay is an attempt to cover a topic with a linked series of photographs. Photo essays range from purely photographic works to photographs with captions or small notes to full text essays with a few or many accompanying photographs. Photo essays can be sequential in nature, intended to be viewed in a particular order, or they may consist of non-ordered photographs which may be viewed all at once or in an order chosen by the viewer. All photo essays are collections of photographs, but not all collections of photographs are photo essays. Photo essays often address a certain issue or attempt to capture the character of places and events.

</doc>
<doc id="10375" url="http://en.wikipedia.org/wiki?curid=10375" title="Error detection and correction">
Error detection and correction

In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.
Definitions.
The general definitions of the terms are as follows:
History.
An early systematic use of error detection was by Jewish scribes in the precise copying of the Jewish bible, beginning before Christ. An emphasis on minute details of words and spellings evolved into the idea of a perfect text in 135 CE, and with it increasingly forceful strictures that a deviation in even a single letter would make a Torah scroll invalid. The scribes used methods such as summing the number of words per line and per page (Numerical Masorah), and checking the middle paragraph, word and letter against the original. The page was thrown out if a single mistake was found, and three mistakes on a single page would result in the entire manuscript being destroyed (the equivalent of retransmission on a telecommunications channel). The effectiveness of their methods was verified by the accuracy of copying through the centuries demonstrated by discovery of the Dead Sea Scrolls in 1947–1956, dating from c.408 BCE-75 CE.
Introduction.
The general idea for achieving error detection and correction is to add some redundancy (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message, and to recover data determined to be corrupted. Error-detection and correction schemes can be either systematic or non-systematic: In a systematic scheme, the transmitter sends the original data, and attaches a fixed number of "check bits" (or "parity data"), which are derived from the data bits by some deterministic algorithm. If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. In a system that uses a non-systematic code, the original message is transformed into an encoded message that has at least as many bits as the original message.
Good error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common channel models include memory-less models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in bursts. Consequently, error-detecting and correcting codes can be generally distinguished between "random-error-detecting/correcting" and "burst-error-detecting/correcting". Some codes can also be suitable for a mixture of random errors and burst errors.
If the channel capacity cannot be determined, or is highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as automatic repeat request (ARQ), and is most notably used in the Internet. An alternate approach for error control is hybrid automatic repeat request (HARQ), which is a combination of ARQ and error-correction coding.
Implementation.
Error correction may generally be realized in two different ways:
ARQ and FEC may be combined, such that minor errors are corrected without retransmission, and major errors are corrected via a request for retransmission: this is called "hybrid automatic repeat-request (HARQ)".
Error detection schemes.
Error detection is most commonly realized using a suitable hash function (or checksum algorithm). A hash function adds a fixed-length "tag" to a message, which enables receivers to verify the delivered message by recomputing the tag and comparing it with the one provided.
There exists a vast variety of different hash function designs. However, some are of particularly widespread use because of either their simplicity or their suitability for detecting certain kinds of errors (e.g., the cyclic redundancy check's performance in detecting burst errors).
A random-error-correcting code based on minimum distance coding can provide a strict guarantee on the number of detectable errors, but it may not protect against a preimage attack. a repetition code, described below, is a special case of error-correcting codes: although rather inefficient, a repetition code is suitable in some applications of error correction and detection due to its simplicity.
Repetition codes.
A "repetition code" is a coding scheme that repeats the bits across a channel to achieve error-free communication. Given a stream of data to be transmitted, the data are divided into blocks of bits. Each block is transmitted some predetermined number of times. For example, to send the bit pattern "1011", the four-bit block can be repeated three times, thus producing "1011 1011 1011". However, if this twelve-bit pattern was received as "1010 1011 1011" – where the first block is unlike the other two – it can be determined that an error has occurred.
A repetition code is very inefficient, and can be susceptible to problems if the error occurs in exactly the same place for each group (e.g., "1010 1010 1010" in the previous example would be detected as correct). The advantage of repetition codes is that they are extremely simple, and are in fact used in some transmissions of numbers stations.
Parity bits.
A "parity bit" is a bit that is added to a group of source bits to ensure that the number of set bits (i.e., bits with value 1) in the outcome is even or odd. It is a very simple scheme that can be used to detect single or any other odd number (i.e., three, five, etc.) of errors in the output. An even number of flipped bits will make the parity bit appear correct even though the data is erroneous.
Extensions and variations on the parity bit mechanism are horizontal redundancy checks, vertical redundancy checks, and "double," "dual," or "diagonal" parity (used in RAID-DP).
Checksums.
A "checksum" of a message is a modular arithmetic sum of message code words of a fixed word length (e.g., byte values). The sum may be negated by means of a ones'-complement operation prior to transmission to detect errors resulting in all-zero messages.
Checksum schemes include parity bits, check digits, and longitudinal redundancy checks. Some checksum schemes, such as the Damm algorithm, the Luhn algorithm, and the Verhoeff algorithm, are specifically designed to detect errors commonly introduced by humans in writing down or remembering identification numbers.
Cyclic redundancy checks (CRCs).
A "cyclic redundancy check (CRC)" is a non-secure hash function designed to detect accidental changes to digital data in computer networks; it is not suitable for detecting maliciously introduced errors. It is characterized by specification of what is called a "generator polynomial", which is used as the divisor in a polynomial long division over a finite field, taking the input data as the dividend, such that the remainder becomes the result.
A cyclic code has favorable properties that make it well suited for detecting burst errors. CRCs are particularly easy to implement in hardware, and are therefore commonly used in digital networks and storage devices such as hard disk drives.
Even parity is a special case of a cyclic redundancy check, where the single-bit CRC is generated by the divisor "x" + 1.
Cryptographic hash functions.
The output of a "cryptographic hash function", also known as a "message digest", can provide strong assurances about data integrity, whether changes of the data are accidental (e.g., due to transmission errors) or maliciously introduced. Any modification to the data will likely be detected through a mismatching hash value. Furthermore, given some hash value, it is infeasible to find some input data (other than the one given) that will yield the same hash value. If an attacker can change not only the message but also the hash value, then a "keyed hash" or message authentication code (MAC) can be used for additional security. Without knowing the key, it is infeasible for the attacker to calculate the correct keyed hash value for a modified message.
Error-correcting codes.
Any error-correcting code can be used for error detection. A code with "minimum Hamming distance", "d", can detect up to "d" − 1 errors in a code word. Using minimum-distance-based error-correcting codes for error detection can be suitable if a strict limit on the minimum number of errors to be detected is desired.
Codes with minimum Hamming distance "d" = 2 are degenerate cases of error-correcting codes, and can be used to detect single errors. The parity bit is an example of a single-error-detecting code.
Error correction.
Automatic repeat request (ARQ).
Automatic Repeat reQuest (ARQ) is an error control method for data transmission that makes use of error-detection codes, acknowledgment and/or negative acknowledgment messages, and timeouts to achieve reliable data transmission. An "acknowledgment" is a message sent by the receiver to indicate that it has correctly received a data frame.
Usually, when the transmitter does not receive the acknowledgment before the timeout occurs (i.e., within a reasonable amount of time after sending the data frame), it retransmits the frame until it is either correctly received or the error persists beyond a predetermined number of retransmissions.
Three types of ARQ protocols are Stop-and-wait ARQ, Go-Back-N ARQ, and Selective Repeat ARQ.
ARQ is appropriate if the communication channel has varying or unknown capacity, such as is the case on the Internet. However, ARQ requires the availability of a back channel, results in possibly increased latency due to retransmissions, and requires the maintenance of buffers and timers for retransmissions, which in the case of network congestion can put a strain on the server and overall network capacity.
For example, ARQ is used on shortwave radio data links in the form of ARQ-E, or combined with multiplexing as ARQ-M.
Error-correcting code.
An error-correcting code (ECC) or forward error correction (FEC) code is a system of adding redundant data, or "parity data", to a message, such that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) were introduced, either during the process of transmission, or on storage. Since the receiver does not have to ask the sender for retransmission of the data, a back-channel is not required in forward error correction, and it is therefore suitable for simplex communication such as broadcasting. Error-correcting codes are frequently used in lower-layer communication, as well as for reliable storage in media such as CDs, DVDs, hard disks, and RAM.
Error-correcting codes are usually distinguished between convolutional codes and block codes:
Shannon's theorem is an important theorem in forward error correction, and describes the maximum information rate at which reliable communication is possible over a channel that has a certain error probability or signal-to-noise ratio (SNR). This strict upper limit is expressed in terms of the channel capacity. More specifically, the theorem says that there exist codes such that with increasing encoding length the probability of error on a discrete memoryless channel can be made arbitrarily small, provided that the code rate is smaller than the channel capacity. The code rate is defined as the fraction "k/n" of "k" source symbols and "n" encoded symbols.
The actual maximum code rate allowed depends on the error-correcting code used, and may be lower. This is because Shannon's proof was only of existential nature, and did not show how to construct codes which are both optimal and have efficient encoding and decoding algorithms.
Hybrid schemes.
Hybrid ARQ is a combination of ARQ and forward error correction. There are two basic approaches:
The latter approach is particularly attractive on an erasure channel when using a rateless erasure code.
Applications.
Applications that require low latency (such as telephone conversations) cannot use Automatic Repeat reQuest (ARQ); they must use forward error correction (FEC). By the time an ARQ system discovers an error and re-transmits it, the re-sent data will arrive too late to be any good.
Applications where the transmitter immediately forgets the information as soon as it is sent (such as most television cameras) cannot use ARQ; they must use FEC because when an error occurs, the original data is no longer available. (This is also why FEC is used in data storage systems such as RAID and distributed data store).
Applications that use ARQ must have a return channel; applications having no return channel cannot use ARQ. Applications that require extremely low error rates (such as digital money transfers) must use ARQ. Reliability and inspection engineering also make use of the theory of error-correcting codes.
Internet.
In a typical TCP/IP stack, error control is performed at multiple levels:
Deep-space telecommunications.
Development of error-correction codes was tightly coupled with the history of deep-space missions due to the extreme dilution of signal power over interplanetary distances, and the limited power availability aboard space probes. Whereas early missions sent their data uncoded, starting from 1968 digital error correction was implemented in the form of (sub-optimally decoded) convolutional codes and Reed–Muller codes. The Reed–Muller code was well suited to the noise the spacecraft was subject to (approximately matching a bell curve), and was implemented at the Mariner spacecraft for missions between 1969 and 1977.
The Voyager 1 and Voyager 2 missions, which started in 1977, were designed to deliver color imaging amongst scientific information of Jupiter and Saturn. This resulted in increased coding requirements, and thus the spacecraft were supported by (optimally Viterbi-decoded) convolutional codes that could be concatenated with an outer Golay (24,12,8) code.
The Voyager 2 craft additionally supported an implementation of a Reed–Solomon code: the concatenated Reed–Solomon–Viterbi (RSV) code allowed for very powerful error correction, and enabled the spacecraft's extended journey to Uranus and Neptune. Both craft use V2 RSV coding due to ECC system upgrades after 1989.
The CCSDS currently recommends usage of error correction codes with performance similar to the Voyager 2 RSV code as a minimum. Concatenated codes are increasingly falling out of favor with space missions, and are replaced by more powerful codes such as Turbo codes or LDPC codes.
The different kinds of deep space and orbital missions that are conducted suggest that trying to find a "one size fits all" error correction system will be an ongoing problem for some time to come. For missions close to Earth the nature of the channel noise is different from that which a spacecraft on an interplanetary mission experiences. Additionally, as a spacecraft increases its distance from Earth, the problem of correcting for noise gets larger.
Satellite broadcasting (DVB).
The demand for satellite transponder bandwidth continues to grow, fueled by the desire to deliver television (including new channels and High Definition TV) and IP data. Transponder availability and bandwidth constraints have limited this growth, because transponder capacity is determined by the selected modulation scheme and Forward error correction (FEC) rate.
Overview
Data storage.
Error detection and correction codes are often used to improve the reliability of data storage media. A "parity track" was present on the first magnetic tape data storage in 1951. The "Optimal Rectangular Code" used in group code recording tapes not only detects but also corrects single-bit errors. Some file formats, particularly archive formats, include a checksum (most often CRC32) to detect corruption and truncation and can employ redundancy and/or parity files to recover portions of corrupted data. Reed Solomon codes are used in compact discs to correct errors caused by scratches.
Modern hard drives use CRC codes to detect and Reed–Solomon codes to correct minor errors in sector reads, and to recover data from sectors that have "gone bad" and store that data in the spare sectors. RAID systems use a variety of error correction techniques, to correct errors when a hard drive completely fails. Filesystems such as ZFS or Btrfs, as well as some RAID implementations, support data scrubbing and resilvering, which allows bad blocks to be detected and (hopefully) recovered before they are used. The recovered data may be re-written to exactly the same physical location, to spare blocks elsewhere on the same piece of hardware, or to replacement hardware.
Error-correcting memory.
DRAM memory may provide increased protection against soft errors by relying on error correcting codes. Such error-correcting memory, known as "ECC" or "EDAC-protected" memory, is particularly desirable for high fault-tolerant applications, such as servers, as well as deep-space applications due to increased radiation.
Error-correcting memory controllers traditionally use Hamming codes, although some use triple modular redundancy.
Interleaving allows distributing the effect of a single cosmic ray potentially upsetting multiple physically neighboring bits across multiple words by associating neighboring bits to different words. As long as a single event upset (SEU) does not exceed the error threshold (e.g., a single error) in any particular word between accesses, it can be corrected (e.g., by a single-bit error correcting code), and the illusion of an error-free memory system may be maintained.
In addition to hardware providing features required for ECC memory to operate, operating systems usually contain related reporting facilities that are used to provide notifications when soft errors are transparently recovered. An increasing rate of soft errors might indicate that a DIMM module needs replacing, and such feedback information would not be easily available without the related reporting capabilities. An example is the Linux kernel's "EDAC" subsystem (previously known as "bluesmoke"), which collects the data from error-checking–enabled components inside a computer system; beside collecting and reporting back the events related to ECC memory, it also supports other checksumming errors, including those detected on the PCI bus.
A few systems also support memory scrubbing.

</doc>
<doc id="10376" url="http://en.wikipedia.org/wiki?curid=10376" title="Euclidean domain">
Euclidean domain

In mathematics, more specifically in abstract algebra and ring theory, a Euclidean domain (also called a Euclidean ring) is a ring that can be endowed with a Euclidean function (explained below) which allows a suitable generalization of the Euclidean division of the integers. This generalized Euclidean algorithm can be put to many of the same uses as Euclid's original algorithm in the ring of integers: in any Euclidean domain, one can apply the Euclidean algorithm to compute the greatest common divisor of any two elements. In particular, the greatest common divisor of any two elements exists and can be written as a linear combination 
of them (Bézout's identity). Also every ideal in a Euclidean domain is principal, which implies a suitable generalization of the fundamental theorem of arithmetic: every Euclidean domain is a unique factorization domain.
It is important to compare the class of Euclidean domains with the larger class of principal ideal domains (PIDs). An arbitrary PID has much the same "structural properties" of a Euclidean domain (or, indeed, even of the ring of integers), but when an explicit algorithm for Euclidean division is known, one may use Euclidean algorithm and extended Euclidean algorithm to compute greatest common divisors and Bézout's identity. In particular, the existence of efficient algorithms for Euclidean division of integers and of polynomials in one variable over a field is of basic importance in computer algebra.
So, given an integral domain "R", it is often very useful to know that "R" has a Euclidean function: in particular, this implies that "R" is a PID. However, if there is no "obvious" Euclidean function, then determining whether "R" is a PID is generally a much easier problem than determining whether it is a Euclidean domain.
Euclidean domains appear in the following chain of class inclusions:
Definition.
Let "R" be an integral domain. A Euclidean function on "R" is a function formula_1 from 
formula_2 to the non-negative integers satisfying the following fundamental division-with-remainder property:
A Euclidean domain is an integral domain which can be endowed with at least one Euclidean function. It is important to note that a particular Euclidean function "f" is "not" part of the structure of a Euclidean domain: in general, a Euclidean domain will admit many different Euclidean functions.
Most algebra texts require a Euclidean function to have the following additional property:
However, one can show that (EF2) is superfluous in the following sense: any domain "R" which 
can be endowed with a function "g" satisfying (EF1) can also be endowed with a function "f" satisfying (EF1) and (EF2): indeed, for formula_3 one can define "f"("a") as follows
In words, one may define "f"("a") to be the minimum value attained by "g" on the set of all non-zero elements of the principal ideal generated by "a".
A multiplicative Euclidean function is one such that "f"("ab")="f"("a")"f"("b") and "f"("a") is never zero. It follows that "f"(1)=1 and in fact "f"("a")=1 if and only if "a" is a unit.
Notes on the definition.
Many authors use other terms such as "degree function", "valuation function", "gauge function" or "norm function", in place of "Euclidean function". Some authors also require the domain of the Euclidean function to be the entire ring "R"; however this does not essentially affect the definition, since (EF1) does not involve the value of "f(0)". The definition is sometimes generalized by allowing the Euclidean function to take its values in any well-ordered set; this weakening does not affect the most important implications of the Euclidean property.
The property (EF1) can be restated as follows: for any principal ideal "I" of "R" with nonzero generator "b", all nonzero classes of the quotient ring "R"/"I" have a representative "r" with "f"("r") < "f"("b"). Since the possible values of "f" are well-ordered, this property can be established by proving "f"("r") < "f"("b") for any "r" (not in "I") with minimal value of "f"("r") in its class. Note that for a Euclidean function that is so established there need not exist an effective method to determine "q" and "r" in (EF1).
Examples.
Examples of Euclidean domains include:
Example of domains that are not Euclidean domains include
Properties.
Let "R" be a domain and "f" a Euclidean function on "R". Then:
However, in many finite extensions of Q with trivial class group, the ring of integers is Euclidean (not necessarily with respect to the absolute value of the field norm; see below).
Assuming the extended Riemann hypothesis, if "K" is a finite extension of Q and the ring of integers of "K" is a PID with an infinite number of units, then the ring of integers is Euclidean.
In particular this applies to the case of totally real quadratic number fields with trivial class group.
In addition (and without assuming ERH), if the field "K" is a Galois extension of Q, has trivial class group and unit rank strictly greater than three, then the ring of integers is Euclidean.
An immediate corollary of this is that if the number field is Galois over Q, its class group is trivial and the extension has degree greater than 8 then the ring of integers is necessarily Euclidean.
Norm-Euclidean fields.
Algebraic number fields "K" come with a canonical norm function on them: the absolute value of the field norm "N" that takes an algebraic element "α" to the product of all the conjugates of "α". This norm maps the ring of integers of a number field "K", say "O""K", to the nonnegative rational integers, so it is a candidate to be a Euclidean norm on this ring. If this norm satisfies the axioms of a Euclidean function then the number field "K" is called "norm-Euclidean" or simply "Euclidean". Strictly speaking it is the ring of integers that is Euclidean since fields are trivially Euclidean domains, but the terminology is standard.
If a field is not norm-Euclidean then that does not mean the ring of integers is not Euclidean, just that the field norm does not satisfy the axioms of a Euclidean function. In fact, the rings of integers of number fields may be divided in several classes:
The norm-Euclidean quadratic fields have been fully classified, they are formula_15 where "d" takes the values
Every Euclidean imaginary quadratic field is norm-Euclidean and is one of the five first fields in the preceding list.

</doc>
<doc id="10377" url="http://en.wikipedia.org/wiki?curid=10377" title="Euclidean algorithm">
Euclidean algorithm

In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in Euclid's "Elements" (c. 300 BC).
It is an example of an "algorithm", a step-by-step procedure for performing a calculation according to well-defined rules,
and is one of the oldest numerical algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 147 = 252 − 105. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until one of the two numbers reaches zero. When that occurs, the other number (the one that is not zero) is the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 × 105 + (−2) × 252. The fact that the GCD can always be expressed in this way is known as Bézout's identity.
The version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two. With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.
The Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it is a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.
Background: greatest common divisor.
The Euclidean algorithm calculates the greatest common divisor (GCD) of two natural numbers "a" and "b". The greatest common divisor "g" is the largest natural number that divides both "a" and "b" without leaving a remainder. Synonyms for the GCD include the "greatest common factor" (GCF), the "highest common factor" (HCF), and the "greatest common measure" (GCM). The greatest common divisor is often written as gcd("a", "b") or, more simply, as ("a", "b"), although the latter notation is also used for other mathematical concepts, such as two-dimensional vectors.
If gcd("a", "b") = 1, then "a" and "b" are said to be coprime (or relatively prime). This property does not imply that "a" or "b" are themselves prime numbers. For example, neither 6 nor 35 is a prime number, since they both have two prime factors: 6 = 2 × 3 and 35 = 5 × 7. Nevertheless, 6 and 35 are coprime. No natural number other than 1 divides both 6 and 35, since they have no prime factors in common.
Let "g" = gcd("a", "b"). Since "a" and "b" are both multiples of "g", they can be written "a" = "mg" and "b" = "ng", and there is no larger number "G" > "g" for which this is true. The natural numbers "m" and "n" must be coprime, since any common factor could be factored out of "m" and "n" to make "g" greater. Thus, any other number "c" that divides both "a" and "b" must also divide "g". The greatest common divisor "g" of "a" and "b" is the unique (positive) common divisor of "a" and "b" that is divisible by any other common divisor "c".
The GCD can be visualized as follows. Consider a rectangular area "a" by "b", and any common divisor "c" that divides both "a" and "b" exactly. The sides of the rectangle can be divided into segments of length "c", which divides the rectangle into a grid of squares of side length "c". The greatest common divisor "g" is the largest value of "c" for which this is possible. For illustration, a 24-by-60 rectangular area can be divided into a grid of: 1-by-1 squares, 2-by-2 squares, 3-by-3 squares, 4-by-4 squares, 6-by-6 squares or 12-by-12 squares. Therefore, 12 is the greatest common divisor of 24 and 60. A 24-by-60 rectangular area can be divided into a grid of 12-by-12 squares, with two squares along one edge (24/12 = 2) and five squares along the other (60/12 = 5).
The GCD of two numbers "a" and "b" is the product of the prime factors shared by the two numbers, where a same prime factor can be used multiple times, but only as long as the product of these factors divides both "a" and "b". For example, since 1386 can be factored into 2 × 3 × 3 × 7 × 11, and 3213 can be factored into 3 × 3 × 3 × 7 × 17, the greatest common divisor of 1386 and 3213 equals 63 = 3 × 3 × 7, the product of their shared prime factors. If two numbers have no prime factors in common, their greatest common divisor is 1 (obtained here as an instance of the empty product), in other words they are coprime. A key advantage of the Euclidean algorithm is that it can find the GCD efficiently without having to compute the prime factors. Factorization of large integers is believed to be a computationally very difficult problem, and the security of many modern cryptography systems is based upon its infeasibility.
Another definition of the GCD is helpful in advanced mathematics, particularly ring theory. The greatest common divisor "g"  of two nonzero numbers "a" and "b" is also their smallest positive integral linear combination, that is, the smallest positive number of the form "ua" + "vb" where "u" and "v" are integers. The set of all integral linear combinations of "a" and "b" is actually the same as the set of all multiples of "g" ("mg", where "m" is an integer). In modern mathematical language, the ideal generated by "a" and "b" is the ideal generated by "g" alone (an ideal generated by a single element is called a principal ideal, and all ideals of the integers are principal ideals). Some properties of the GCD are in fact easier to see with this description, for instance the fact that any common divisor of "a" and "b" also divides the GCD (it divides both terms of "ua" + "vb"). The equivalence of this GCD definition with the other definitions is described below.
The GCD of three or more numbers equals the product of the prime factors common to all the numbers, but it can also be calculated by repeatedly taking the GCDs of pairs of numbers. For example,
Thus, Euclid's algorithm, which computes the GCD of two integers, suffices to calculate the GCD of arbitrarily many integers.
Description.
Procedure.
The Euclidean algorithm proceeds in a series of steps such that the output of each step is used as an input for the next one. Let "k" be an integer that counts the steps of the algorithm, starting with zero. Thus, the initial step corresponds to "k" = 0, the next step corresponds to "k" = 1, and so on.
Each step begins with two nonnegative remainders "r""k"−1 and "r""k"−2. Since the algorithm ensures that the remainders decrease steadily with every step, "r""k"−1 is less than its predecessor "r""k"−2. The goal of the "k"th step is to find a quotient "q""k" and remainder "r""k" that satisfy the equation
and that have "r""k" < "r""k"−1. In other words, multiples of the smaller number "r""k"−1 are subtracted from the larger number "r""k"−2 until the remainder "r""k" is smaller than "r""k"−1.
In the initial step ("k" = 0), the remainders "r"−2 and "r"−1 equal "a" and "b", the numbers for which the GCD is sought. In the next step ("k" = 1), the remainders equal "b" and the remainder "r"0 of the initial step, and so on. Thus, the algorithm can be written as a sequence of equations
If "a" is smaller than "b", the first step of the algorithm swaps the numbers. For example, if "a" < "b", the initial quotient "q"0 equals zero, and the remainder "r"0 is "a". Thus, "r""k" is smaller than its predecessor "r""k"−1 for all "k" ≥ 0.
Since the remainders decrease with every step but can never be negative, a remainder "r""N" must eventually equal zero, at which point the algorithm stops. The final nonzero remainder "r""N"−1 is the greatest common divisor of "a" and "b". The number "N" cannot be infinite because there are only a finite number of nonnegative integers between the initial remainder "r"0 and zero.
Proof of validity.
The validity of the Euclidean algorithm can be proven by a two-step argument. In the first step, the final nonzero remainder "r""N"−1 is shown to divide both "a" and "b". Since it is a common divisor, it must be less than or equal to the greatest common divisor "g". In the second step, it is shown that any common divisor of "a" and "b", including "g", must divide "r""N"−1; therefore, "g" must be less than or equal to "r""N"−1. These two conclusions are inconsistent unless "r""N"−1 = "g".
To demonstrate that "r""N"−1 divides both "a" and "b" (the first step), "r""N"−1 divides its predecessor "r""N"−2
since the final remainder "r""N" is zero. "r""N"−1 also divides its next predecessor "r""N"−3
because it divides both terms on the right-hand side of the equation. Iterating the same argument, "r""N"−1 divides all the preceding remainders, including "a" and "b". None of the preceding remainders "r""N"−2, "r""N"−3, etc. divide "a" and "b", since they leave a remainder. Since "r""N"−1 is a common divisor of "a" and "b", "r""N"−1 ≤ "g".
In the second step, any natural number "c" that divides both "a" and "b" (in other words, any common divisor of "a" and "b") divides the remainders "r""k". By definition, "a" and "b" can be written as multiples of "c": "a" = "mc" and "b" = "nc", where "m" and "n" are natural numbers. Therefore, "c" divides the initial remainder "r"0, since "r"0 = "a" − "q"0"b" = "mc" − "q"0"nc" = ("m" − "q"0"n")"c". An analogous argument shows that "c" also divides the subsequent remainders "r"1, "r"2, etc. Therefore, the greatest common divisor "g" must divide "r""N"−1, which implies that "g" ≤ "r""N"−1. Since the first part of the argument showed the reverse ("r""N"−1 ≤ "g"), it follows that "g" = "r""N"−1. Thus, "g" is the greatest common divisor of all the succeeding pairs:
Worked example.
For illustration, the Euclidean algorithm can be used to find the greatest common divisor of "a" = 1071 and "b" = 462. To begin, multiples of 462 are subtracted from 1071 until the remainder is less than 462. Two such multiples can be subtracted ("q"0 = 2), leaving a remainder of 147:
Then multiples of 147 are subtracted from 462 until the remainder is less than 147. Three multiples can be subtracted ("q"1 = 3), leaving a remainder of 21:
Then multiples of 21 are subtracted from 147 until the remainder is less than 21. Seven multiples can be subtracted ("q"2 = 7), leaving no remainder:
Since the last remainder is zero, the algorithm ends with 21 as the greatest common divisor of 1071 and 462. This agrees with the gcd(1071, 462) found by prime factorization above. In tabular form, the steps are
Visualization.
The Euclidean algorithm can be visualized in terms of the tiling analogy given above for the greatest common divisor. Assume that we wish to cover an "a"-by-"b" rectangle with square tiles exactly, where "a" is the larger of the two numbers. We first attempt to tile the rectangle using "b"-by-"b" square tiles; however, this leaves an "r"0-by-"b" residual rectangle untiled, where "r"0 < "b". We then attempt to tile the residual rectangle with "r"0-by-"r"0 square tiles. This leaves a second residual rectangle "r"1-by-"r"0, which we attempt to tile using "r"1-by-"r"1 square tiles, and so on. The sequence ends when there is no residual rectangle, i.e., when the square tiles cover the previous residual rectangle exactly. The length of the sides of the smallest square tile is the GCD of the dimensions of the original rectangle. For example, the smallest square tile in the adjacent figure is 21-by-21 (shown in red), and 21 is the GCD of 1071 and 462, the dimensions of the original rectangle (shown in green).
Euclidean division.
At every step "k", the Euclidean algorithm computes a quotient "q""k" and remainder "r""k" from two numbers "r""k"−1 and "r""k"−2
where the magnitude of "r""k" is strictly less than that of "r""k"−1. The theorem which underlies the definition of the Euclidean division ensures that such a quotient and remainder always exist and are unique.
In Euclid's original version of the algorithm, the quotient and remainder are found by repeated subtraction; that is, "r""k"−1 is subtracted from "r""k"−2 repeatedly until the remainder "r""k" is smaller than "r""k"−1. After that "r""k" and "r""k"−1 are exchanged and the process is iterated. Euclidean division reduces all the steps between two exchanges into a single step, which is thus more efficient. Moreover, the quotients are not needed, thus one may replace Euclidean division by the modulo operation, which gives only the remainder. Thus the iteration of the Euclidean algorithm becomes simply
Implementations.
Implementations of the algorithm may be expressed in pseudocode. For example, the division-based version may be programmed as
 function gcd(a, b)
 while b ≠ 0
 t := b
 b := a mod b
 a := t
 return a
At the beginning of the "k"th iteration, the variable "b" holds the latest remainder "r""k"−1, whereas the variable "a" holds its predecessor, "r""k"−2. The step "b" := "a" mod "b" is equivalent to the above recursion formula "r""k" ≡ "r""k"−2 mod "r""k"−1. The temporary variable "t" holds the value of "r""k"−1 while the next remainder "r""k" is being calculated. At the end of the loop iteration, the variable "b" holds the remainder "r""k", whereas the variable "a" holds its predecessor, "r""k"−1.
In the subtraction-based version which was Euclid's original version, the remainder calculation ("b" = "a" mod "b") is replaced by repeated subtraction. Contrary to the division-based version, which works with arbitrary integers as input, the subtraction-based version supposes that the input consists of positive integers and stops when "a" = "b":
 function gcd(a, b)
 while a ≠ b
 if a > b
 a := a − b
 else
 b := b − a
 return a
The variables "a" and "b" alternate holding the previous remainders "r""k"−1 and "r""k"−2. Assume that "a" is larger than "b" at the beginning of an iteration; then "a" equals "r""k"−2, since "r""k"−2 > "r""k"−1. During the loop iteration, "a" is reduced by multiples of the previous remainder "b" until "a" is smaller than "b". Then "a" is the next remainder "r""k". Then "b" is reduced by multiples of "a" until it is again smaller than "a", giving the next remainder "r""k"+1, and so on.
The recursive version is based on the equality of the GCDs of successive remainders and the stopping condition gcd("r""N"−1, 0) = "r""N"−1.
 function gcd(a, b)
 if b = 0
 return a
 else
 return gcd(b, a mod b)
For illustration, the gcd(1071, 462) is calculated from the equivalent gcd(462, 1071 mod 462) = gcd(462, 147). The latter GCD is calculated from the gcd(147, 462 mod 147) = gcd(147, 21), which in turn is calculated from the gcd(21, 147 mod 21) = gcd(21, 0) = 21.
Method of least absolute remainders.
In another version of Euclid's algorithm, the quotient at each step is increased by one if the resulting negative remainder is smaller in magnitude than the typical positive remainder. Previously, the equation
assumed that |"r""k"−1| > "r""k" > 0. However, an alternative negative remainder "e""k" can be computed:
if "r""k"−1 > 0 or
if "r""k"−1 < 0.
If "r""k" is replaced by "e""k". when |"e""k"| < |"r""k"|, then one gets a variant of Euclidean algorithm such that
at each step.
Leopold Kronecker has shown that this version requires the least number of steps of any version of Euclid's algorithm. More generally, it has been proven that, for every input numbers "a" and "b", the number of steps is minimal if and only if "q""k" is chosen in order that formula_1 where formula_2 is the golden ratio.
Historical development.
The Euclidean algorithm is one of the oldest algorithms in common use. It appears in Euclid's "Elements" (c. 300 BC), specifically in Book 7 (Propositions 1–2) and Book 10 (Propositions 2–3). In Book 7, the algorithm is formulated for integers, whereas in Book 10, it is formulated for lengths of line segments. (In modern usage, one would say it was formulated there for real numbers. But lengths, areas, and volumes, represented as real numbers in modern usage, are not measured in the same units and there is no natural unit of length, area, or volume; the concept of real numbers was unknown at that time.) The latter algorithm is geometrical. The GCD of two lengths "a" and "b" corresponds to the greatest length "g" that measures "a" and "b" evenly; in other words, the lengths "a" and "b" are both integer multiples of the length "g".
The algorithm was probably not discovered by Euclid, who compiled results from earlier mathematicians in his "Elements". The mathematician and historian B. L. van der Waerden suggests that Book VII derives from a textbook on number theory written by mathematicians in the school of Pythagoras. The algorithm was probably known by Eudoxus of Cnidus (about 375 BC). The algorithm may even pre-date Eudoxus, judging from the use of the technical term ἀνθυφαίρεσις ("anthyphairesis", reciprocal subtraction) in works by Euclid and Aristotle.
Centuries later, Euclid's algorithm was discovered independently both in India and in China, primarily to solve Diophantine equations that arise in astronomy and making accurate calendars. In the late 5th century, the Indian mathematician and astronomer Aryabhata described the algorithm as the "pulverizer", perhaps because of its effectiveness in solving Diophantine equations. Although a special case of the Chinese remainder theorem had already been described by Chinese mathematician and astronomer Sun Tzu, the general solution was published by Qin Jiushao in his 1247 book "Shushu Jiuzhang" (數書九章 "Mathematical Treatise in Nine Sections"). The Euclidean algorithm was first described in Europe in the second edition of Bachet's "Problèmes plaisants et délectables" ("Pleasant and enjoyable problems", 1624). In Europe, it was likewise used to solve Diophantine equations and in developing continued fractions. The extended Euclidean algorithm was published by the English mathematician Nicholas Saunderson, who attributed it to Roger Cotes as a method for computing continued fractions efficiently.
In the 19th century, the Euclidean algorithm led to the development of new number systems, such as Gaussian integers and Eisenstein integers. In 1815, Carl Gauss used the Euclidean algorithm to demonstrate unique factorization of Gaussian integers, although his work was first published in 1832. Gauss mentioned the algorithm in his "Disquisitiones Arithmeticae" (published 1801), but only as a method for continued fractions. Peter Gustav Lejeune Dirichlet seems to have been the first to describe the Euclidean algorithm as the basis for much of number theory. Lejeune Dirichlet noted that many results of number theory, such as unique factorization, would hold true for any other system of numbers to which the Euclidean algorithm could be applied. Lejeune Dirichlet's lectures on number theory were edited and extended by Richard Dedekind, who used Euclid's algorithm to study algebraic integers, a new general type of number. For example, Dedekind was the first to prove Fermat's two-square theorem using the unique factorization of Gaussian integers. Dedekind also defined the concept of a Euclidean domain, a number system in which a generalized version of the Euclidean algorithm can be defined (as described below). In the closing decades of the 19th century, the Euclidean algorithm gradually became eclipsed by Dedekind's more general theory of ideals.
Other applications of Euclid's algorithm were developed in the 19th century. In 1829, Charles Sturm showed that the algorithm was useful in the Sturm chain method for counting the real roots of polynomials in any given interval.
The Euclidean algorithm was the first integer relation algorithm, which is a method for finding integer relations between commensurate real numbers. Several novel integer relation algorithms have been developed, such as the algorithm of Helaman Ferguson and R.W. Forcade (1979) and the LLL algorithm.
In 1969, Cole and Davie developed a two-player game based on the Euclidean algorithm, called "The Game of Euclid", which has an optimal strategy. The players begin with two piles of "a" and "b" stones. The players take turns removing "m" multiples of the smaller pile from the larger. Thus, if the two piles consist of "x" and "y" stones, where "x" is larger than "y", the next player can reduce the larger pile from "x" stones to "x" − "my" stones, as long as the latter is a nonnegative integer. The winner is the first player to reduce one pile to zero stones.
Mathematical applications.
Bézout's identity.
Bézout's identity states that the greatest common divisor "g" of two integers "a" and "b" can be represented as a linear sum of the original two numbers "a" and "b". In other words, it is always possible to find integers "s" and "t" such that "g" = "sa" + "tb".
The integers "s" and "t" can be calculated from the quotients "q"0, "q"1, etc. by reversing the order of equations in Euclid's algorithm. Beginning with the next-to-last equation, "g" can be expressed in terms of the quotient "q""N"−1 and the two preceding remainders, "r""N"−2 and "r""N"−3:
Those two remainders can be likewise expressed in terms of their quotients and preceding remainders,
Substituting these formulae for "r""N"−2 and "r""N"−3 into the first equation yields "g" as a linear sum of the remainders "r""N"−4 and "r""N"−5. The process of substituting remainders by formulae involving their predecessors can be continued until the original numbers "a" and "b" are reached:
After all the remainders "r"0, "r"1, etc. have been substituted, the final equation expresses "g" as a linear sum of "a" and "b": "g" = "sa" + "tb". Bézout's identity, and therefore the previous algorithm, can both be generalized to the context of Euclidean domains.
Principal ideals and related problems.
Bézout's identity provides yet another definition of the greatest common divisor "g" of two numbers "a" and "b". Consider the set of all numbers "ua" + "vb", where "u" and "v" are any two integers. Since "a" and "b" are both divisible by "g", every number in the set is divisible by "g". In other words, every number of the set is an integer multiple of "g". This is true for every common divisor of "a" and "b". However, unlike other common divisors, the greatest common divisor is a member of the set; by Bézout's identity, choosing "u" = "s" and "v" = "t" gives "g". A smaller common divisor cannot be a member of the set, since every member of the set must be divisible by "g". Conversely, any multiple "m" of "g" can be obtained by choosing "u" = "ms" and "v" = "mt", where "s" and "t" are the integers of Bézout's identity. This may be seen by multiplying Bézout's identity by "m",
Therefore, the set of all numbers "ua" + "vb" is equivalent to the set of multiples "m" of "g". In other words, the set of all possible sums of integer multiples of two numbers ("a" and "b") is equivalent to the set of multiples of gcd("a", "b"). The GCD is said to be the generator of the ideal of "a" and "b". This GCD definition led to the modern abstract algebraic concepts of a principal ideal (an ideal generated by a single element) and a principal ideal domain (a domain in which every ideal is a principal ideal).
Certain problems can be solved using this result. For example, consider two measuring cups of volume "a" and "b". By adding/subtracting "u" multiples of the first cup and "v" multiples of the second cup, any volume "ua" + "vb" can be measured out. These volumes are all multiples of "g" = gcd("a", "b").
Extended Euclidean algorithm.
The integers "s" and "t" of Bézout's identity can be computed efficiently using the extended Euclidean algorithm. This extension adds two recursive equations to Euclid's algorithm
with the starting values
Using this recursion, Bézout's integers "s" and "t" are given by "s" = "s""N" and "t" = "t""N", where "N+1" is the step on which the algorithm terminates with "r""N+1" = 0.
The validity of this approach can be shown by induction. Assume that the recursion formula is correct up to step "k" − 1 of the algorithm; in other words, assume that
for all "j" less than "k". The "k"th step of the algorithm gives the equation
Since the recursion formula has been assumed to be correct for "r""k"−2 and "r""k"−1, they may be expressed in terms of the corresponding "s" and "t" variables
Rearranging this equation yields the recursion formula for step "k", as required
Matrix method.
The integers "s" and "t" can also be found using an equivalent matrix method. The sequence of equations of Euclid's algorithm
can be written as a product of 2-by-2 quotient matrices multiplying a two-dimensional remainder vector
Let M represent the product of all the quotient matrices
This simplifies the Euclidean algorithm to the form
To express "g" as a linear sum of "a" and "b", both sides of this equation can be multiplied by the inverse of the matrix M. The determinant of M equals (−1)"N"+1, since it equals the product of the determinants of the quotient matrices, each of which is negative one. Since the determinant of M is never zero, the vector of the final remainders can be solved using the inverse of M
Since the top equation gives
the two integers of Bézout's identity are "s" = (−1)"N"+1"m"22 and "t" = (−1)"N""m"12. The matrix method is as efficient as the equivalent recursion, with two multiplications and two additions per step of the Euclidean algorithm.
Euclid's lemma and unique factorization.
Bézout's identity is essential to many applications of Euclid's algorithm, such as demonstrating the unique factorization of numbers into prime factors. To illustrate this, suppose that a number "L" can be written as a product of two factors "u" and "v", that is, "L" = "uv". If another number "w" also divides "L" but is coprime with "u", then "w" must divide "v", by the following argument: If the greatest common divisor of "u" and "w" is 1, then integers "s" and "t" can be found such that
by Bézout's identity. Multiplying both sides by "v" gives the relation
Since "w" divides both terms on the right-hand side, it must also divide the left-hand side, "v". This result is known as Euclid's lemma. Specifically, if a prime number divides "L", then it must divide at least one factor of "L". Conversely, if a number "w" is coprime to each of a series of numbers "a"1, "a"2, …, "a""n", then "w" is also coprime to their product, "a"1 × "a"2 × … × "a""n".
Euclid's lemma suffices to prove that every number has a unique factorization into prime numbers. To see this, assume the contrary, that there are two independent factorizations of "L" into "m" and "n" prime factors, respectively
Since each prime "p" divides "L" by assumption, it must also divide one of the "q" factors; since each "q" is prime as well, it must be that "p" = "q". Iteratively dividing by the "p" factors shows that each "p" has an equal counterpart "q"; the two prime factorizations are identical except for their order. The unique factorization of numbers into primes has many applications in mathematical proofs, as shown below.
Linear Diophantine equations.
Diophantine equations are equations in which the solutions are restricted to integers; they are named after the 3rd-century Alexandrian mathematician Diophantus. A typical "linear" Diophantine equation seeks integers "x" and "y" such that
where "a", "b" and "c" are given integers. This can be written as an equation for "x" in modular arithmetic:
Let "g" be the greatest common divisor of "a" and "b". Both terms in "ax" + "by" are divisible by "g"; therefore, "c" must also be divisible by "g", or the equation has no solutions. By dividing both sides by "c"/"g", the equation can be reduced to Bezout's identity
where "s" and "t" can be found by the extended Euclidean algorithm. This provides one solution to the Diophantine equation, "x"1 = "s" ("c"/"g") and "y"1 = "t" ("c"/"g").
In general, a linear Diophantine equation has no solutions, or an infinite number of solutions. To find the latter, consider two solutions, ("x"1, "y"1) and ("x"2, "y"2), where
or equivalently
Therefore, the smallest difference between two "x" solutions is "b"/"g", whereas the smallest difference between two "y" solutions is "a"/"g". Thus, the solutions may be expressed as
By allowing "t" to vary over all possible integers, an infinite family of solutions can be generated from a single solution ("x"1, "y"1). If the solutions are required to be "positive" integers ("x" > 0, "y" > 0), only a finite number of solutions may be possible. This restriction on the acceptable solutions allows systems of Diophantine equations to be solved with more unknowns than equations; this is impossible for a system of linear equations when the solutions can be any real number.
Multiplicative inverses and the RSA algorithm.
A finite field is a set of numbers with four generalized operations. The operations are called addition, subtraction, multiplication and division and have their usual properties, such as commutativity, associativity and distributivity. An example of a finite field is the set of 13 numbers {0, 1, 2, …, 12} using modular arithmetic. In this field, the results of any mathematical operation (addition, subtraction, multiplication, or division) is reduced modulo 13; that is, multiples of 13 are added or subtracted until the result is brought within the range 0–12. For example, the result of 5 × 7 = 35 mod 13 = 9. Such finite fields can be defined for any prime "p"; using more sophisticated definitions, they can also be defined for any power "m" of a prime "p" "m". Finite fields are often called Galois fields, and are abbreviated as GF("p") or GF("p" "m").
In such a field with "m" numbers, every nonzero element "a" has a unique modular multiplicative inverse, "a"−1 such that "aa"−1 = "a"−1"a" ≡ 1 mod "m". This inverse can be found by solving the congruence equation "ax" ≡ 1 mod "m", or the equivalent linear Diophantine equation
This equation can be solved by the Euclidean algorithm, as described above. Finding multiplicative inverses is an essential step in the RSA algorithm, which is widely used in electronic commerce; specifically, the equation determines the integer used to decrypt the message. Note that although the RSA algorithm uses rings rather than fields, the Euclidean algorithm can still be used to find a multiplicative inverse where one exists. The Euclidean algorithm also has other applications in error-correcting codes; for example, it can be used as an alternative to the Berlekamp–Massey algorithm for decoding BCH and Reed–Solomon codes, which are based on Galois fields.
Chinese remainder theorem.
Euclid's algorithm can also be used to solve multiple linear Diophantine equations. Such equations arise in the Chinese remainder theorem, which describes a novel method to represent an integer "x". Instead of representing an integer by its digits, it may be represented by its remainders "x""i" modulo a set of "N" coprime numbers "m""i":
The goal is to determine "x" from its "N" remainders "x""i". The solution is to combine the multiple equations into a single linear Diophantine equation with a much larger modulus "M" that is the product of all the individual moduli "m""i", and define "M""i" as
Thus, each "M""i" is the product of all the moduli "except" "m""i". The solution depends on finding "N" new numbers "h""i" such that
With these numbers "h""i", any integer "x" can be reconstructed from its remainders "x""i" by the equation
Since these numbers "h""i" are the multiplicative inverses of the "M""i", they may be found using Euclid's algorithm as described in the previous subsection.
Stern–Brocot tree.
The Euclidean algorithm can be used to arrange the set of all positive rational numbers into an infinite binary search tree, called the Stern–Brocot tree.
The number 1 (expressed as a fraction 1/1) is placed at the root of the tree, and the location of any other number "a"/"b" can be found by computing gcd("a","b") using the original form of the Euclidean algorithm, in which each step replaces the larger of the two given numbers by its difference with the smaller number (not its remainder), stopping when two equal numbers are reached. A step of the Euclidean algorithm that replaces the first of the two numbers corresponds to a step in the tree from a node to its right child, and a step that replaces the second of the two numbers corresponds to a step in the tree from a node to its left child. The sequence of steps constructed in this way does not depend on whether "a"/"b" is given in lowest terms, and forms a path from the root to a node containing the number "a"/"b". This fact can be used to prove that each positive rational number appears exactly once in this tree.
For example, 3/4 can be found by starting at the root, going to the left once, then to the right twice:
The Euclidean algorithm has almost the same relationship to another binary tree on the rational numbers called the Calkin–Wilf tree. The difference is that the path is reversed: instead of producing a path from the root of the tree to a target, it produces a path from the target to the root.
Continued fractions.
The Euclidean algorithm has a close relationship with continued fractions. The sequence of equations can be written in the form
The last term on the right-hand side always equals the inverse of the left-hand side of the next equation. Thus, the first two equations may be combined to form
The third equation may be used to substitute the denominator term "r"1/"r"0, yielding
The final ratio of remainders "r""k"/"r""k"−1 can always be replaced using the next equation in the series, up to the final equation. The result is a continued fraction
In the worked example above, the gcd(1071, 462) was calculated, and the quotients "q""k" were 2, 3 and 7, respectively. Therefore, the fraction 1071/462 may be written
as can be confirmed by calculation.
Factorization algorithms.
Calculating a greatest common divisor is an essential step in several integer factorization algorithms, such as Pollard's rho algorithm, Shor's algorithm, Dixon's factorization method and the Lenstra elliptic curve factorization. The Euclidean algorithm may be used to find this GCD efficiently. Continued fraction factorization uses continued fractions, which are determined using Euclid's algorithm.
Algorithmic efficiency.
The computational efficiency of Euclid's algorithm has been studied thoroughly. This efficiency can be described by the number of division steps the algorithm requires, multiplied by the computational expense of each step. The first known analysis of Euclid's algorithm is due to A.-A.-L. Reynaud in 1811, who showed that the number of division steps on input ("u", "v") is bounded by "v"; later he improved this to "v"/2  + 2. Later, in 1841, P.-J.-E. Finck showed that the number of division steps is at most 2 log2 "v" + 1, and hence Euclid's algorithm runs in time polynomial in the size of the input; also see. His analysis was refined by Gabriel Lamé in 1844, who showed that the number of steps required for completion is never more than five times the number "h" of base-10 digits of the smaller number "b".
In the uniform cost model (suitable for analyzing the complexity of gcd calculation on numbers that fit into a single machine word), each step of the algorithm takes constant time, and Lamé's analysis implies that the total running time is also "O"("h"). However, in a model of computation suitable for computation with larger numbers, the computational expense of a single remainder computation in the algorithm can be as large as "O"("h"2). In this case the total time for all of the steps of the algorithm can be analyzed using a telescoping series, showing that it is also "O"("h"2). Modern algorithmic techniques based on the Schönhage–Strassen algorithm for fast integer multiplication can be used to speed this up, leading to subquadratic algorithms for the GCD.
Number of steps.
The number of steps to calculate the GCD of two natural numbers, "a" and "b", may be denoted by "T"("a", "b"). If "g" is the GCD of "a" and "b", then "a" = "mg" and "b" = "ng" for two coprime numbers "m" and "n". Then
as may be seen by dividing all the steps in the Euclidean algorithm by "g". By the same argument, the number of steps remains the same if "a" and "b" are multiplied by a common factor "w": "T"("a", "b") = "T"("wa", "wb"). Therefore, the number of steps "T" may vary dramatically between neighboring pairs of numbers, such as T("a", "b") and T("a", "b" + 1), depending on the size of the two GCDs.
The recursive nature of the Euclidean algorithm gives another equation
where "T"("x", 0) = 0 by assumption.
Worst-case.
If the Euclidean algorithm requires "N" steps for a pair of natural numbers "a" > "b" > 0, the smallest values of "a" and "b" for which this is true are the Fibonacci numbers "F""N"+2 and "F""N"+1, respectively. This can be shown by induction. If "N" = 1, "b" divides "a" with no remainder; the smallest natural numbers for which this is true is "b" = 1 and "a" = 2, which are "F"2 and "F"3, respectively. Now assume that the result holds for all values of "N" up to "M" − 1. The first step of the "M"-step algorithm is "a" = "q"0"b" + "r"0, and the second step is "b" = "q"1"r"0 + "r"1. Since the algorithm is recursive, it required "M" − 1 steps to find gcd("b", "r"0) and their smallest values are "F""M"+1 and "F""M". The smallest value of "a" is therefore when "q"0 = 1, which gives "a" = "b" + "r"0 = "F""M"+1 + "F""M" = "F""M"+2. This proof, published by Gabriel Lamé in 1844, represents the beginning of computational complexity theory, and also the first practical application of the Fibonacci numbers.
This result suffices to show that the number of steps in Euclid's algorithm can never be more than five times the number of its digits (base 10). For if the algorithm requires "N" steps, then "b" is greater than or equal to "F""N"+1 which in turn is greater than or equal to "φ""N"−1, where "φ" is the golden ratio. Since "b" ≥ "φ""N"−1, then "N" − 1 ≤ log"φ""b". Since log10"φ" > 1/5, ("N" − 1)/5 < log10"φ" log"φ""b" = log10"b". Thus, "N" ≤ 5 log10"b". Thus, the Euclidean algorithm always needs less than "O"("h") divisions, where "h" is the number of digits in the smaller number "b".
Average.
The average number of steps taken by the Euclidean algorithm has been defined in three different ways. The first definition is the average time "T"("a") required to calculate the GCD of a given number "a" and a smaller natural number "b" chosen with equal probability from the integers 0 to "a" − 1
However, since "T"("a", "b") fluctuates dramatically with the GCD of the two numbers, the averaged function "T"("a") is likewise "noisy".
To reduce this noise, a second average "τ"("a") is taken over all numbers coprime with "a"
There are "φ"("a") coprime integers less than "a", where "φ" is Euler's totient function. This tau average grows smoothly with "a"
with the residual error being of order "a"−(1/6) + ε, where ε is infinitesimal. The constant "C" ("Porter's Constant")in this formula equals
where γ is the Euler–Mascheroni constant and ζ' is the derivative of the Riemann zeta function. The leading coefficient (12/π2) ln 2 was determined by two independent methods.
Since the first average can be calculated from the tau average by summing over the divisors "d" of "a"
it can be approximated by the formula
where Λ("d") is the Mangoldt function.
A third average "Y"("n") is defined as the mean number of steps required when both "a" and "b" are chosen randomly (with uniform distribution) from 1 to "n"
Substituting the approximate formula for "T"("a") into this equation yields an estimate for "Y"("n")
Computational expense per step.
In each step "k" of the Euclidean algorithm, the quotient "q""k" and remainder "r""k" are computed for a given pair of integers "r""k"−2 and "r""k"−1
The computational expense per step is associated chiefly with finding "q""k", since the remainder "r""k" can be calculated quickly from "r""k"−2, "r""k"−1, and "q""k"
The computational expense of dividing "h"-bit numbers scales as "O"("h"("ℓ"+1)), where "ℓ" is the length of the quotient.
For comparison, Euclid's original subtraction-based algorithm can be much slower. A single integer division is equivalent to the quotient "q" number of subtractions. If the ratio of "a" and "b" is very large, the quotient is large and many subtractions will be required. On the other hand, it has been shown that the quotients are very likely to be small integers. The probability of a given quotient "q" is approximately ln|"u"/("u" − 1)| where "u" = ("q" + 1)2. For illustration, the probability of a quotient of 1, 2, 3, or 4 is roughly 41.5%, 17.0%, 9.3%, and 5.9%, respectively. Since the operation of subtraction is faster than division, particularly for large numbers, the subtraction-based Euclid's algorithm is competitive with the division-based version. This is exploited in the binary version of Euclid's algorithm.
Combining the estimated number of steps with the estimated computational expense per step shows that the Euclid's algorithm grows quadratically ("h"2) with the average number of digits "h" in the initial two numbers "a" and "b". Let "h"0, "h"1, …, "h""N"−1 represent the number of digits in the successive remainders "r"0, "r"1, …, "r""N"−1. Since the number of steps "N" grows linearly with "h", the running time is bounded by
Alternative methods.
Euclid's algorithm is widely used in practice, especially for small numbers, due to its simplicity. For comparison, the efficiency of alternatives to Euclid's algorithm may be determined.
One inefficient approach to finding the GCD of two natural numbers "a" and "b" is to calculate all their common divisors; the GCD is then the largest common divisor. The common divisors can be found by dividing both numbers by successive integers from 2 to the smaller number "b". The number of steps of this approach grows linearly with "b", or exponentially in the number of digits. Another inefficient approach is to find the prime factors of one or both numbers. As noted above, the GCD equals the product of the prime factors shared by the two numbers "a" and "b". Present methods for prime factorization are also inefficient; many modern cryptography systems even rely on that inefficiency.
The binary GCD algorithm is an efficient alternative that substitutes division with faster operations by exploiting the binary representation used by computers. However, this alternative also scales like "O"("h"²). It is generally faster than the Euclidean algorithm on real computers, even though it scales in the same way. Additional efficiency can be gleaned by examining only the leading digits of the two numbers "a" and "b". The binary algorithm can be extended to other bases ("k"-ary algorithms), with up to fivefold increases in speed. Lehmer's GCD algorithm uses the same general principle as the binary algorithm to speed up GCD computations in arbitrary bases.
A recursive approach for very large integers (with more than 25,000 digits) leads to "subquadratic integer GCD algorithms", such as those of Schönhage, and Stehlé and Zimmermann. These algorithms exploit the 2×2 matrix form of the Euclidean algorithm given above. These subquadratic methods generally scale as "O"("h" (log "h")2 (log log "h")).
Generalizations.
Although the Euclidean algorithm is used to find the greatest common divisor of two natural numbers (positive integers), it may be generalized to the real numbers, and to other mathematical objects, such as polynomials, quadratic integers and Hurwitz quaternions. In the latter cases, the Euclidean algorithm is used to demonstrate the crucial property of unique factorization, i.e., that such numbers can be factored uniquely into irreducible elements, the counterparts of prime numbers. Unique factorization is essential to many proofs of number theory.
Rational and real numbers.
Euclid's algorithm can be applied to real numbers, as described by Euclid in Book 10 of his "Elements". The goal of the algorithm is to identify a real number "g" such that two given real numbers, "a" and "b", are integer multiples of it: "a" = "mg" and "b" = "ng", where "m" and "n" are integers. This identification is equivalent to finding an integer relation among the real numbers "a" and "b"; that is, it determines integers "s" and "t" such that "sa" + "tb" = 0. Euclid uses this algorithm to treat the question of incommensurable lengths.
The real-number Euclidean algorithm differs from its integer counterpart in two respects. First, the remainders "r""k" are real numbers, although the quotients "q""k" are integers as before. Second, the algorithm is not guaranteed to end in a finite number "N" of steps. If it does, the fraction "a"/"b" is a rational number, i.e., the ratio of two integers
and can be written as a finite continued fraction ["q"0; "q"1, "q"2, …, "q""N"]. If the algorithm does not stop, the fraction "a"/"b" is an irrational number and can be described by an infinite continued fraction ["q"0; "q"1, "q"2, …]. Examples of infinite continued fractions are the golden ratio "φ" = [1; 1, 1, …] and the square root of two, √2 = [1; 2, 2, …]. The algorithm is unlikely to stop, since almost all ratios "a"/"b" of two real numbers are irrational.
An infinite continued fraction may be truncated at a step "k" ["q"0; "q"1, "q"2, …, "q""k"] to yield an approximation to "a"/"b" that improves as "k" is increased. The approximation is described by convergents "m""k"/"n""k"; the numerator and denominators are coprime and obey the recurrence relation
where "m"−1 = "n"−2 = 1 and "m"−2 = "n"−1 = 0 are the initial values of the recursion. The convergent "m""k"/"n""k" is the best rational number approximation to "a"/"b" with denominator "n""k":
Polynomials.
Polynomials in a single variable "x" can be added, multiplied and factored into irreducible polynomials, which are the analogs of the prime numbers for integers. The greatest common divisor polynomial "g"("x") of two polynomials "a"("x") and "b"("x") is defined as the product of their shared irreducible polynomials, which can be identified using the Euclidean algorithm. The basic procedure is similar to integers. At each step "k", a quotient polynomial "q""k"("x") and a remainder polynomial "r""k"("x") are identified to satisfy the recursive equation
where "r"−2("x") = "a"("x") and "r"−1("x") = "b"("x"). The quotient polynomial is chosen so that the leading term of "q""k"("x") "r""k"−1("x") equals the leading term of "r""k"−2("x"); this ensures that the degree of each remainder is smaller than the degree of its predecessor deg["r""k"("x")] < deg["r""k"−1("x")]. Since the degree is a nonnegative integer, and since it decreases with every step, the Euclidean algorithm concludes in a finite number of steps. The final nonzero remainder is the greatest common divisor of the original two polynomials, "a"("x") and "b"("x").
For example, consider the following two quartic polynomials, which each factor into two quadratic polynomials
and
Dividing "a"("x") by "b"("x") yields a remainder "r"0("x") = "x"3 + (2/3) "x"2 + (5/3) "x" − (2/3). In the next step, "b"("x") is divided by "r"0("x") yielding a remainder "r"1("x") = "x"2 + "x" + 2. Finally, dividing "r"0("x") by "r"1("x") yields a zero remainder, indicating that "r"1("x") is the greatest common divisor polynomial of "a"("x") and "b"("x"), consistent with their factorization.
Many of the applications described above for integers carry over to polynomials. The Euclidean algorithm can be used to solve linear Diophantine equations and Chinese remainder problems for polynomials; continued fractions of polynomials can also be defined.
The polynomial Euclidean algorithm has other applications, such as Sturm chains, a method for counting the zeros of a polynomial that lie inside a given real interval. This in turn has applications in several areas, such as the Routh–Hurwitz stability criterion in control theory.
Finally, the coefficients of the polynomials need not be drawn from integers, real numbers or even the complex numbers. For example, the coefficients may be drawn from a general field, such as the finite fields GF("p") described above. The corresponding conclusions about the Euclidean algorithm and its applications hold even for such polynomials.
Gaussian integers.
The Gaussian integers are complex numbers of the form α = "u" + "vi", where "u" and "v" are ordinary integers and "i" is the square root of negative one. By defining an analog of the Euclidean algorithm, Gaussian integers can be shown to be uniquely factorizable, by the argument above. This unique factorization is helpful in many applications, such as deriving all Pythagorean triples or proving Fermat's theorem on sums of two squares. In general, the Euclidean algorithm is convenient in such applications, but not essential; for example, the theorems can often be proven by other arguments.
The Euclidean algorithm developed for two Gaussian integers α and β is nearly the same as that for normal integers, but differs in two respects. As before, the task at each step "k" is to identify a quotient "q""k" and a remainder "r""k" such that
where "r""k"−2 = α, "r""k"−1 = β, and every remainder is strictly smaller than its predecessor, |"r""k"| < |"r""k"−1|. The first difference is that the quotients and remainders are themselves Gaussian integers, and thus are complex numbers. The quotients "q""k" are generally found by rounding the real and complex parts of the exact ratio (such as the complex number α/β) to the nearest integers. The second difference lies in the necessity of defining how one complex remainder can be "smaller" than another. To do this, a norm function "f"("u" + "v"i) = "u"2 + "v"2 is defined, which converts every Gaussian integer "u" + "vi" into a normal integer. After each step "k" of the Euclidean algorithm, the norm of the remainder "f"("r""k") is smaller than the norm of the preceding remainder, "f"("r""k"−1). Since the norm is a nonnegative integer and decreases with every step, the Euclidean algorithm for Gaussian integers ends in a finite number of steps. The final nonzero remainder is the gcd(α,β), the Gaussian integer of largest norm that divides both α and β; it is unique up to multiplication by a unit, ±1 or ±"i".
Many of the other applications of the Euclidean algorithm carry over to Gaussian integers. For example, it can be used to solve linear Diophantine equations and Chinese remainder problems for Gaussian integers; continued fractions of Gaussian integers can also be defined.
Euclidean domains.
A set of elements under two binary operations, + and −, is called a Euclidean domain if it forms a commutative ring "R" and, roughly speaking, if a generalized Euclidean algorithm can be performed on them. The two operations of such a ring need not be the addition and multiplication of ordinary arithmetic; rather, they can be more general, such as the operations of a mathematical group or monoid. Nevertheless, these general operations should respect many of the laws governing ordinary arithmetic, such as commutativity, associativity and distributivity.
The generalized Euclidean algorithm requires a "Euclidean function", i.e., a mapping "f" from "R" into the set of nonnegative integers such that, for any two nonzero elements "a" and "b" in "R", there exist "q" and "r" in "R" such that "a" = "qb" + "r" and "f"("r") < "f"("b"). An example of this mapping is the norm function used to order the Gaussian integers above. The function "f" can be the magnitude of the number, or the degree of a polynomial. The basic principle is that each step of the algorithm reduces "f" inexorably; hence, if "f" can be reduced only a finite number of times, the algorithm must stop in a finite number of steps. This principle relies heavily on the natural well-ordering of the non-negative integers; roughly speaking, this requires that every non-empty set of non-negative integers has a smallest member.
The fundamental theorem of arithmetic applies to any Euclidean domain: Any number from a Euclidean domain can be factored uniquely into irreducible elements. Any Euclidean domain is a unique factorization domain (UFD), although the converse is not true. The Euclidean domains and the UFD's are subclasses of the GCD domains, domains in which a greatest common divisor of two numbers always exists. In other words, a greatest common divisor may exist (for all pairs of elements in a domain), although it may not be possible to find it using a Euclidean algorithm. A Euclidean domain is always a principal ideal domain (PID), an integral domain in which every ideal is a principal ideal. Again, the converse is not true: not every PID is a Euclidean domain.
The unique factorization of Euclidean domains is useful in many applications. For example, the unique factorization of the Gaussian integers is convenient in deriving formulae for all Pythagorean triples and in proving Fermat's theorem on sums of two squares. Unique factorization was also a key element in an attempted proof of Fermat's Last Theorem published in 1847 by Gabriel Lamé, the same mathematician who analyzed the efficiency of Euclid's algorithm, based on a suggestion of Joseph Liouville. Lamé's approach required the unique factorization of numbers of the form "x" + ω"y", where "x" and "y" are integers, and ω = "e"2"i"π/"n" is an "n"th root of 1, that is, ω"n" = 1. Although this approach succeeds for some values of "n" (such as "n"=3, the Eisenstein integers), in general such numbers do "not" factor uniquely. This failure of unique factorization in some cyclotomic fields led Ernst Kummer to the concept of ideal numbers and, later, Richard Dedekind to ideals.
Unique factorization of quadratic integers.
The quadratic integer rings are helpful to illustrate Euclidean domains. Quadratic integers are generalizations of the Gaussian integers in which the imaginary unit "i" is replaced by a number ω. Thus, they have the form "u" + "v" ω, where "u" and "v" are integers and ω has one of two forms, depending on a parameter "D". If "D" does not equal a multiple of four plus one, then
If, however, "D" does equal a multiple of four plus one, then
If the function "f" corresponds to a norm function, such as that used to order the Gaussian integers above, then the domain is known as "norm-Euclidean". The norm-Euclidean rings of quadratic integers are exactly those where "D" = −11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57 or 73. The quadratic integers with "D" = −1 and −3 are known as the Gaussian integers and Eisenstein integers, respectively.
If "f" is allowed to be any Euclidean function, then the list of possible "D" values for which the domain is Euclidean is not yet known. The first example of a Euclidean domain that was not norm-Euclidean (with "D" = 69) was published in 1994. In 1973, Weinberger proved that a quadratic integer ring with "D" > 0 is Euclidean if, and only if, it is a principal ideal domain, provided that the generalized Riemann hypothesis holds.
Noncommutative rings.
The Euclidean algorithm may be applied to noncommutative rings such as the set of Hurwitz quaternions. Let α and β represent two elements from such a ring. They have a common right divisor δ if α = ξδ and β = ηδ for some choice of ξ and η in the ring. Similarly, they have a common left divisor if α = δξ and β = δη for some choice of ξ and η in the ring. Since multiplication is not commutative, there are two versions of the Euclidean algorithm, one for right divisors and one for left divisors. Choosing the right divisors, the first step in finding the gcd(α, β) by the Euclidean algorithm can be written
where ψ0 represents the quotient and ρ0 the remainder. This equation shows that any common right divisor of α and β is likewise a common divisor of the remainder ρ0. The analogous equation for the left divisors would be
With either choice, the process is repeated as above until the greatest common right or left divisor is identified. As in the Euclidean domain, the "size" of the remainder ρ0 must be strictly smaller than β, and there must be only a finite number of possible sizes for ρ0, so that the algorithm is guaranteed to terminate.
Most of the results for the GCD carry over to noncommutative numbers. For example, Bézout's identity states that the right gcd(α, β) can be expressed as a linear combination of α and β. In other words, there are numbers σ and τ such that
The analogous identity for the left GCD is nearly the same:
Bézout's identity can be used to solve Diophantine equations. For instance, one of the standard proofs of Lagrange's four-square theorem, that every positive integer can be represented as a sum of four squares, is based on quaternion GCDs in this way.
Notes.
</dl>

</doc>
<doc id="10378" url="http://en.wikipedia.org/wiki?curid=10378" title="European Centre for Medium-Range Weather Forecasts">
European Centre for Medium-Range Weather Forecasts

The European Centre for Medium-Range Weather Forecasts (ECMWF) is an independent intergovernmental organisation supported by 21 European Member States and 13 cooperating States. At its headquarters in Reading, England, one of the largest supercomputer complexes in Europe is linked by high-speed telecommunication lines to the computer systems of the national weather services of its supporting states. The Centre's computer system contains the world's largest archive of numerical weather prediction data.
Objectives.
ECMWF is renowned worldwide as providing the most accurate medium-range global weather forecasts to 15 days and seasonal forecasts to 12 months. Its products are provided to the European National Weather Services, as a complement to the national short-range and climatological activities. The National Meteorological Services of Member States and Co-operating States use ECMWF’s products for their own national duties, in particular to give early warning of potentially damaging severe weather.
ECMWF was established in 1975, in recognition of the need to pool the scientific and technical resources of Europe’s meteorological services and institutions for the production of medium-range weather forecasts and of the economic and social benefits expected from it. The Centre employs about 160 staff members and 70 consultants coming from Member and Co-operating States.
The primary purposes of the Centre are the development of a capability for medium-range weather forecasting and provision of medium-range weather forecasts to the Member States. The objectives of the Centre shall be to develop, and operate on a regular basis, global models and data-assimilation systems for the dynamics, thermodynamics and composition of the Earth's fluid envelope and interacting parts of the Earth-system, with a view to preparing forecasts by means of numerical methods, providing initial conditions for the forecasts, and contributing to monitoring the relevant parts of the Earth-system.
The main objectives of the ECMWF are:
Work and Projects.
ECMWF uses the computer modelling technique of numerical weather prediction (NWP) to forecast the weather from its present measured state. The calculations require a constant input of meteorological data, collected by satellites and earth observation systems such as automatic and manned stations, aircraft, ships and weather balloons.
The data are fed into ECMWF's databases and assimilated into its NWP models to produce:
Over the past three decades ECMWF's activities and wide-ranging programme of research and development have played a pioneering role in the remarkable advancement of weather forecasting and data assimilation systems. ECMWF has dramatically improved the accuracy and reliability of weather forecasting, working in collaboration with Member and Co-operating States, the European Union and partners such as the World Meteorological Organization (WMO), the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT) and the European Space Agency (ESA).
ECMWF, through its partnerships with EUMETSAT, ESA, the EU and the European Science community has established a leading position for Europe in the exploitation of satellite data for operational numerical weather prediction, and in the exploitation of satellite data for operational seasonal forecasting with coupled atmosphere-ocean-land models. The increasing amount of satellite data and the development of more sophisticated ways of extraction information from that data have made a major contribution to improving the accuracy and utility of NWP forecasts. ECMWF continuously endeavours to improve the use of satellite observations for NWP.
Reanalysis.
ECMWF makes significant contributions to support research on climate variability. ECMWF has pioneered an approach known as reanalysis, which involves feeding weather observations collected over decades into a NWP system to recreate past atmospheric, sea- and land-surface conditions over specific time periods to obtain a clearer picture of how the climate has changed. Reanalysis provides a four-dimensional picture of the atmosphere and effectively allows monitoring of the variability and change of global climate, thereby contributing also to the understanding and attribution of climate change.
To date, and with support from Europe's National Meteorological Services and the European Commission, ECMWF has conducted two major reanalyses of the global atmosphere: the first ECMWF re-analysis (ERA-15) project generated reanalyses from December 1978 to February 1994; the ERA-40 project generated reanalyses from September 1957 to August 2002.
Operational forecast model.
The ECMWF is best known in the United States for its global operational forecast model, known officially as the "Integrated Forecast System" but usually known informally as the "ECMWF" or "Euro". The model runs both in a "deterministic forecast" mode and as an ensemble. The ECMWF model runs every 12 hours and forecasts out to 10 days. The ECMWF model is primarily used as comparison against the Global Forecast System, which is based in the United States and is run by the National Centers for Environmental Prediction. However, unlike the GFS, which is in the public domain under provisions of United States law, the ECMWF model is proprietary and copyrighted. Nonetheless, a limited amount of output from the model has been publicly released through both the ECMWF and various sites.
Early warning of severe weather events.
ECMWF's strategy underlines its commitment to maintaining the current rapid rate of improvement of its global medium-range forecasts and products. The ECMWF strategy puts the early warning of severe weather as its principal goal. ECMWF can contribute to the development of strategies to mitigate and adapt to climate change. In particular, ECMWF’s emphasis on the provision of reliable predictions of severe weather can be seen as a key contribution to help society adapt to the dangers and threats associated with global warming. Also scientists and researchers around the world use ECMWF’s forecast products to monitor the environment and analyse climate change.
Forecasts of severe weather events are vital to warn authorities and the public, and to allow appropriate mitigating action to be taken. Early warnings, made a few days ahead of potential events, are of significant benefit, giving additional time to allow contingency plans to be put into place. The increased time gained by issuing accurate warnings of severe weather will be crucial to save lives, for instance by evacuating a large number of people from endangered areas (e.g. storm surges in the North Sea), or taking cautionary actions to avoid major threats to goods and services (e.g. extreme winds).
One example was the model's prediction of Hurricane Sandy in October 2012 making landfall on the East Coast of the United States seven days before it actually happened.
It also predicted the intensity and track of the November 2012 nor'easter, which impacted the east coast a week after Sandy.
ECMWF's Extreme Forecast Index (EFI) was developed as a tool to identify where the EPS forecast distribution differs substantially from that of the model climate. It is an integral measure referenced to the model climate that contains all the information regarding variability of weather parameters, in location and time. Thus users can recognise the abnormality of a weather situation without having to define specific space- and time-dependent thresholds.
ECMWF's monthly and seasonal forecasts provide early predictions of events such as heat waves, cold spells and droughts, as well as their impacts on sectors such as agriculture, energy and health. Since ECMWF runs a wave model, there are also predictions of coastal waves and storm surges in European waters which can be used to provide warnings.
Members.
The ECMWF is made up of 21 European countries:
The ECMWF has concluded co-operation agreements with other states: Bulgaria, Croatia, Czech Republic, Estonia, Hungary, Israel, Latvia, Lithuania, Macedonia, Montenegro, Morocco, Romania and Slovakia.
Site.
ECMWF is based at Shinfield Park, Reading, United Kingdom. It shared grounds with the UK Met. Office College until Summer 2002, before the college was relocated to Exeter ahead of the move of the Met. Office Headquarters from Bracknell in 2003. The land the Met Office College was on is now used by residential housing.

</doc>
<doc id="10380" url="http://en.wikipedia.org/wiki?curid=10380" title="European Broadcasting Union">
European Broadcasting Union

The European Broadcasting Union (EBU; French: "Union européenne de radio-télévision (UER)") is an alliance of public service media entities, established on the 12 February 1950. As of 2014, the organisation comprises 72 Active Members in 56 countries, and 37 Associate Members from a further 22 countries. Most EU states are part of this organisation and therefore EBU has been subject to supranational legislation and regulation. It also hosted debates between candidates for the European Commission presidency for the 2014 parliamentary elections but is unrelated to the institution itself. It is best known for producing the Eurovision Song Contest.
General description.
Members of the EBU are radio and television companies, most of which are government-owned public service broadcasters or privately owned stations with public service missions. Active Members come from as far north as Iceland and as far south as Egypt, from Ireland in the west and Azerbaijan in the east, and almost every nation from geographical Europe in between. Associate Members are from countries and territories beyond Europe, such as Canada, Japan, Mexico, India and Hong Kong. Associate Members from the United States include ABC, CBS, NBC, the Corporation for Public Broadcasting, Time Warner, and the only individual station, Chicago-based classical music station WFMT.
Active Members are those paying EBU members meeting all technical criteria for full membership, whose states are either within the European Broadcasting Area (EBA) or members of the Council of Europe. Syria is an example of a country within the EBA not complying with all technical criteria for full membership, and thus it is currently only granted Associated Membership.
The EBU's highest profile production is the Eurovision Song Contest, organised by its Eurovision Network. The Eurovision Network also organises the Eurovision Dance Contest, the Junior Eurovision Song Contest, the Eurovision Young Dancers competition, and other competitions for young musicians and screenwriters, which are modelled along similar lines. The countries represented in the EBU also cooperate to create documentaries and (animated) children's programming.
Radio collaborations include Euroclassic Notturno – an overnight classical music stream, produced by BBC Radio 3 and broadcast in the United Kingdom as "Through the Night" – and special theme days, such as the annual Christmas music relays from around Europe.
Most EBU broadcasters have a group deal to carry the Olympics and FIFA World Cup (particularly, the games of their country and the Final). Another annually recurring event which is broadcast across Europe through the EBU is the Vienna New Year's Concert.
The theme music played before EBU broadcasts is Marc-Antoine Charpentier's "Prelude to Te Deum". It is well known to Europeans as it is played before and after the Eurovision Song Contest and other important events.
History.
It was formed on 12 February 1950 by 23 broadcasting organisations from Europe and the Mediterranean at a conference in the coastal resort of Torquay in Devon, England. In 1993, the International Radio and Television Organisation ("OIRT"), an equivalent organisation of broadcasters from Central and Eastern Europe, was merged with the EBU.
The first co-production was the animated series "The Animals of Farthing Wood" from 1993 based on the books of the same title by Colin Dann. The second animated collaboration was "Noah's Island" from 1997 and more recently, Pitt and Kantrop. Another important EBU programme is "Jeux Sans Frontières".
Technical activities.
The objective of the is simply to assist EBU Members (see below) in this period of unprecedented technological changes. This includes provision of technical information to Members via conferences and workshops, as well as in written form (such as the , and the magazine).
The EBU also encourages active collaboration between its Members on the basis that they can freely share their knowledge and experience, thus achieving considerably more than individual Members could achieve by themselves. Much of this collaboration is achieved through Project Groups which study specific technical issues of common interest: for example, EBU Members have long been preparing for the revision of the 1961 Stockholm Plan.
The EBU places great emphasis on the use of open standards. Widespread use of open standards (such as MPEG-2, DAB, DVB, etc.) ensures interoperability between products from different vendors, as well as facilitating the exchange of programme material between EBU Members and promoting "horizontal markets" for the benefit of all consumers.
EBU Members and the EBU Technical Department have long played an important role in the development of many systems used in radio and television broadcasting, such as:
The EBU has also actively encouraged the development and implementation of:
Greek state broadcaster controversy of 2013.
The Greek government shut down the state broadcaster ERT on 11 June, at short notice, citing government spending concerns related to the Euro crisis. In response, the European Broadcasting Union set up a makeshift studio on the same day, near the former ERT offices in Athens, in order to continue providing EBU members with news-gathering and broadcast relay services that had formerly been provided by ERT.
The EBU put out a statement expressing its "profound dismay" at the shutdown, urged the Greek Prime Minister "to use all his powers to immediately reverse this decision" and offered the "advice, assistance and expertise necessary for ERT to be preserved". As of 4 May 2014, the New Hellenic Radio, Internet and Television (NERIT) broadcaster started nation wide transmissions, and overtook ERT's vacant Active Membership slot in EBU.
Members.
The active member list as of May 2014, comprised the following 73 broadcasting companies from 56 countries.
EBU membership applications.
Below is a table of broadcasting networks who have submitted applications for Active EBU Membership and are either still under review, or have had their applications rejected.
Associate members.
Any group or organisation member of the International Telecommunication Union (ITU), which provide a radio and/or television service outside of the European Broadcasting Area, are permitted to submit applications to the EBU for Associate Membership. Countries which have this status also pay an annual fee to maintain this status, if a fee is not paid, then their Associate Membership is revoked. It was also noted by the EBU that any country that is granted Associate Member status does not include any access into the Eurovision system.
The list of Associate Members of EBU, comprised the following 37 broadcasting companies from 22 countries as of January 2014.
Approved participant members.
Any groups or organisations from a country with International Telecommunication Union (ITU) membership, which do not qualify for either the EBU's Active or Associate memberships, but still provide a broadcasting activity for the EBU, are granted a unique Approved Participants membership, which lasts approximately five years. An application for this status may be submitted to the EBU at any given time, providing an annual fee is paid.
The following seven EBU broadcast members had status as Approved Participants in December 2014.
Organised events.
The EBU in cooperation with the respective host broadcaster, organises competitions and events in which its Members can participate, if they wish to do so. These include:
Eurovision Song Contest.
The Eurovision Song Contest (French: "Concours Eurovision de la Chanson") is an annual international song competition, that was first held in Lugano, Switzerland, on 24 May 1956. Seven countries participated – each submitting two songs, for a total of 14. This was the only contest in which more than one song per country was performed: since 1957 all contests have allowed one entry per country. The 1956 contest was won by the host nation, Switzerland. In this competition, only countries that are members of the EBU can participate. The first winner was Switzerland, and the most recent is Austria.
Junior Eurovision Song Contest.
Junior Eurovision Song Contest (French: "Concours Eurovision de la Chanson Junior"), is an annual international song competition, that was first held in Copenhagen, Denmark, on 15 November 2003. Sixteen countries participated – each submitting one song, for a total of 16 entries. The 2003 Contest was won by Croatia and the current winner is Italy.
Eurovision Young Musicians.
Eurovision Young Musicians is a competition for European musicians that are younger than 19 years old. It is organised by the EBU and is a member of EMCY.
The televised competition is held every two years, with some countries holding national heats. Since its foundation in 1982, the Eurovision Young Musicians competition has become one of the most important music competitions on an international level.
The first competition was held in Manchester, United Kingdom on 11 May 1982.
Eurovision Dance Contest.
The Eurovision Dance Contest (not to be confused with the "Eurovision Young Dancers Competition") was an international dancing competition that was held for the first time in London, United Kingdom on 1 September 2007. The competition was repeated in 2008, but has not been held since.
Eurovision Young Dancers.
The Eurovision Young Dancers is a biennial dance showcase broadcast on television throughout Europe. The first competition was held in Reggio Emilia, Italy on 16 June 1985.
It uses a format similar to the Eurovision Song Contest, every country that is a member of the EBU has had the opportunity to send a dance act to compete for the title of "Eurovision Young Dancer". The act can be either a solo act or a dance couple, and all contestants must be between the ages of 16 and 21 years and not professionally engaged. The winner is chosen by television viewers across the EBU through a real-time, electronic and onscreen voting mechanism.
Let the Peoples Sing.
Let the Peoples Sing is a biennial choir competition, the participants of which are chosen from radio recordings entered by EBU radio members. The final, encompassing three categories and around ten choirs, is offered as a live broadcast to all EBU members. The overall winner is awarded the "Silver Rose Bowl".
Jeux Sans Frontières.
Jeux Sans Frontières (English: Games Without Frontiers, or Games Without Borders) was a Europe-wide television game show. In its original conception, it was broadcast from 1965 to 1999 under the auspices of the EBU. The original series run ended in 1982 but was revived in 1988 with a different complexion of nations and was hosted by smaller broadcasters.

</doc>
<doc id="10382" url="http://en.wikipedia.org/wiki?curid=10382" title="Electrothermal-chemical technology">
Electrothermal-chemical technology

Electrothermal-chemical (ETC) technology is an attempt to increase accuracy and muzzle energy of future tank, artillery, and close-in weapon system guns by improving the predictability and rate of expansion of propellants inside the barrel.
An electrothermal-chemical gun uses a plasma cartridge to ignite and control the ammunition's propellant, using electrical energy to trigger the process. ETC increases the performance of conventional solid propellants, reduces the effect of temperature on propellant expansion and allows for more advanced, higher density propellants to be used.
The technology has been under development since the mid-1980s and at present is actively being researched in the United States by the Army Research Laboratory and private organizations. It is possible that electrothermal-chemical gun propulsion will be an integral part of US Army's future combat system and those of other countries such as Germany and the United Kingdom. 
Electrothermal-chemical technology is part of a broad research and development program that encompasses all electric gun technology, such as railguns and coil guns.
Background.
The constant battle between armour and armor-piercing round has caused a near constant development of the main battle tank design. The evolution of American anti-tank weapons can be traced back to requirements to combat Soviet tanks. In the late eighties, it was thought that the protection level of the Future Soviet Tank (FST) could exceed 700 mm of rolled homogeneous armour equivalence at its maximum thickness, which was effectively immune against the contemporary M-829 armour piercing fin stabilized discarding sabot. Today it is estimated that a tank gun will have to achieve muzzle energies on the level of 18 MJ—which is double the muzzle energy of current tank gun systems — to be able to successfully penetrate such armour plating. In the eighties the most immediate method available to NATO to counter Soviet advances in armour technology was the adoption of a 140 mm main gun. This, however, required a redesigned turret that could incorporate the inherently larger breech and ammunition, and it also required some sort of automatic loader. Although the 140 mm gun was considered a real interim solution it was decided after the fall of the Soviet Union that the increase in muzzle energy it provided was not worth the increase in weight. Resources were therefore spent on research into other programs that could provide the needed muzzle energy. One of the most successful alternative technologies remains electrothermal-chemical ignition.
Most proposed advances in gun technology are based on the assumption that the solid propellant as a stand-alone propulsion system is no longer capable of delivering the required muzzle energy. This requirement has been underscored by the appearance of the Russian T-90 main battle tank. Even the elongation of current gun tubes, such as the new German 120 mm L/55, which was introduced by Rheinmetall is considered only an interim solution as it doesn't offer the required increase in muzzle velocity. Even advanced kinetic energy ammunition such as the United States' M-829A3 is considered only an interim solution against future threats. To that extent the solid propellant is considered to have reached the end of its usefulness, although it will remain the principal propulsion method for at least the next decade until newer technologies mature. To improve on the capabilities of a solid propellant weapon the electrothermal-chemical gun may see production as early as 2016.
ETC technology offers a medium-risk upgrade and is developed to the point that further improvements are so minor that it can be considered mature. The lightweight American 120 mm XM-291 came close to achieving 17 MJ of muzzle energy, which is the lower-end muzzle energy spectrum for a 140 mm gun. However, the success of the XM-291 doesn't imply the success of ETC technology as there are key parts of the propulsion system that are not yet understood or fully developed, such as the plasma ignition process. Nevertheless, there is substantial existing evidence that ETC technology is viable and something worth the money to continue development. Furthermore, it can be integrated into current gun systems.
How it works.
An electrothermal-chemical gun uses a plasma cartridge to ignite and control the ammunition's propellant, using electrical energy as a catalyst to begin the process. Originally researched by Dr. Jon Parmentola for the U.S. Army, it has grown into a very plausible successor to a standard solid propellant tank gun. Since the beginning of research the United States has funded the XM-291 gun project with USD 4,000,000, basic research with USD 300,000, and applied research with USD 600,000. Since then it has been proven to work, although efficiency to the level required has not yet been accomplished. ETC increases the performance of conventional solid propellants, reduces the effect of temperature on propellant expansion and allows for more advanced, higher density propellants to be used. It will also reduce pressure placed on the barrel in comparison to alternative technologies that offer the same muzzle energy given the fact that it helps spread the propellant's gas much more smoothly during ignition. Currently, there are two principal methods of plasma initiation: the flashboard large area emitter (FLARE) and the triple coaxial plasma igniter (TCPI).
Flashboard large area emitter.
Flashboards run in several parallel strings to provide a large area of plasma or ultraviolet radiation and uses the breakdown and vaporization of gaps of diamonds to produce the required plasma. These parallel strings are mounted in tubes and oriented to have their gaps azimuthal to the tube's axis. It discharges by using high pressure air to move air out of the way. FLARE initiators can ignite propellants through the release of plasma, or even through the use of ultraviolet heat radiation. The absorption length of a solid propellant is sufficient enough to be ignited by radiation from a plasma source. However, FLARE has most likely not reached optimal design requirements and further understanding of FLARE and how it works is completely necessary to ensure the evolution of the technology. If FLARE provided the XM-291 gun project with the sufficient radiative heat to ignite the propellant to achieve a muzzle energy of 17 MJ one could only imagine the possibilities with a fully developed FLARE plasma igniter. Current areas of study include how plasma will affect the propellant through radiation, the deliverance of mechanical energy and heat directly and by driving gas flow. Despite these daunting tasks FLARE has been seen as the most plausible igniter for future application on ETC guns.
Triple coaxial plasma igniter.
A coaxial igniter consists of a fully insulated conductor, covered by four strips of aluminum foil. All of this is further insulated in a tube about 1.6 cm in diameter that is perforated with small holes. The idea is to use an electrical flow through the conductor and then exploding the flow into vapor and then breaking it down into plasma. Consequently, the plasma escapes through the constant perforations throughout the insulating tube and initiates the surrounding propellant. A TCPI igniter is fitted in individual propellant cases for each piece of ammunition. However, TCPI is no longer considered a viable method of propellant ignition because it may damage the fins and does not deliver energy as efficiently as a FLARE igniter.
Feasibility.
The XM-291 is the best existing example of a working electrothermal-chemical gun. It was an alternate technology to the heavier caliber 140 mm gun by using the dual-caliber approach. It uses a breech that is large enough to accept 140 mm ammunition and be mounted with both a 120 mm barrel and a 135 mm or 140 mm barrel. The XM-291 also mounts a larger gun tube and a larger ignition chamber than the existing M256 L/44 main gun. Through the application of electrothermal-chemical technology the XM-291 has been able to achieve muzzle energy outputs that equate that to a low-level 140 mm gun, while achieving muzzle velocities greater than those of the larger 140 mm gun. Although the XM-291 does not mean that ETC technology is viable it does offer an example that it is possible.
ETC is also a more viable option than other alternatives by definition. ETC requires much less energy input from outside sources, like a battery, than a railgun or a coilgun would. Tests have shown that energy output by the propellant is higher than energy input from outside sources on ETC guns. In comparison, a railgun currently cannot achieve a higher muzzle velocity than the amount of energy input. Even at 50% efficiency a rail gun launching a projectile with a kinetic energy of 20 MJ would require an energy input into the rails of 40 MJ, and 50% efficiency has not yet been achieved. To put this into perspective, a rail gun launching at 9 MJ of energy would need roughly 32 MJ worth of energy from capacitors. Current advances in energy storage allow for energy densities as high as 2.5 MJ/m³, which means that a battery delivering 32 MJ of energy would require a volume of 12.8 m³; this is not a viable volume for use in a modern main battle tank, especially one designed to be lighter than existing models. There has even been discussion about eliminating the necessity for an outside electrical source in ETC ignition by initiating the plasma cartridge through a small explosive force.
Furthermore, ETC technology is not only applicable to solid propellants. To increase muzzle velocity even further electrothermal-chemical ignition can work with liquid propellants, although this would require further research into plasma ignition. ETC technology is also compatible with existing projects to reduce the amount of recoil delivered to the vehicle while firing. Understandably, recoil of a gun firing a projectile at 17 MJ or more will increase directly with the increase in muzzle energy in accordance to Newton's third law of motion and successful implementation of recoil reduction mechanisms will be vital to the installation of an ETC powered gun in an existing vehicle design. For example, OTO Melara's new lightweight 120 mm L/45 gun has achieved a recoil force of 25 t by using a longer recoil mechanism (550 mm) and a pepperpot muzzle brake. Reduction in recoil can also be achieved through mass attenuation of the thermal sleeve. The ability of ETC technology to be applied to existing gun designs means that for future gun upgrades there's no longer the necessity to redesign the turret to include a larger breech or caliber gun barrel.
Several countries have already determined that ETC technology is viable for the future and have funded indigenous projects considerably. These include the United States, Germany and the United Kingdom, amongst others. The United States' XM360, which is planned to equip the Future Combat Systems Mounted Combat System light tank and may be the M1 Abrams' next gun upgrade, is reportedly based on the XM291 and may include ETC technology, or portions of ETC technology. Tests of this gun have been performed using "precision ignition" technology, which may refer to ETC ignition.
Bibliography.
</dl>

</doc>
<doc id="10384" url="http://en.wikipedia.org/wiki?curid=10384" title="Boeing E-3 Sentry">
Boeing E-3 Sentry

The Boeing E-3 Sentry, commonly known as AWACS, is an airborne early warning and control (AEW&C) aircraft developed by Boeing as the prime contractor. Derived from the Boeing 707, it provides all-weather surveillance, command, control and communications, and is used by the United States Air Force (USAF), NATO, Royal Air Force (RAF), French Air Force and Royal Saudi Air Force. The E-3 is distinguished by the distinctive rotating radar dome above the fuselage. Production ended in 1992 after 68 aircraft had been built.
In the mid-1960s, the USAF was seeking an aircraft to replace its piston-engined Lockheed EC-121 Warning Star, which had seen service for over a decade. After issuing preliminary development contracts to three companies, the USAF picked Boeing to construct two airframes to test Westinghouse Electric's and Hughes's competing radars. Both radars used Pulse-Doppler technology, with Westinghouse's design emerging as the contract winner. Col. Emmett Virgil Conkling, who was an early participant in radar development in England prior to the official U.S. entry into WWII, retired from his position with the Air Force in the Pentagon and took the position of head of development in Seattle. Testing on the first production E-3 began in October 1975.
The first USAF E-3 was delivered in March 1977, and during the next seven years, a total of 34 aircraft were manufactured. NATO, as a single identity, also had eighteen aircraft manufactured, basing them in Germany. The E-3 was also sold to the United Kingdom (seven) and France (four) and Saudi Arabia (five, plus eight E-3 derived tanker aircraft). In 1991, by which time the last aircraft had been delivered, E-3s participated in Operation Desert Storm, playing a crucial role of directing Coalition aircraft against the enemy. Throughout the aircraft's service life, numerous upgrades were performed to enhance its capabilities. In 1996, Westinghouse Electric was acquired by Northrop before being renamed "Northrop Grumman Electronic Systems", which currently supports the E-3's radar.
Development.
Background.
In 1963, the USAF asked for proposals for an Airborne Warning and Control System (AWACS) to replace its EC-121 Warning Stars, which had served in the airborne early warning role for over a decade. The new aircraft would take advantage of improvements in radar technology which allowed airborne radars to "look down" and detect low-flying aircraft (see Look-down/shoot-down), even over land, which was previously impractical due to ground clutter (see terrain mask). Contracts were issued to Boeing, Douglas and Lockheed, the latter being eliminated in July 1966. In 1967, a parallel program was put into place to develop the radar, with Westinghouse Electric and the Hughes Aircraft being asked to compete in producing the radar system. In 1968 it was referred to as Overland Radar Technology (ORT) during development tests on the modified EC-121Q. The Westinghouse's radar antenna was going to be used by whichever company won the radar competition, since Westinghouse had pioneered in the design of high-power RF phase-shifters.
Boeing initially proposed a purpose-built aircraft, but tests indicated that it would not outperform the already-operational 707, so the latter was chosen instead. To increase endurance, this design was to be powered by eight General Electric TF34s, or carrying its radar in a rotating dome mounted at the top of a forward-swept tail, above the fuselage. Boeing was selected ahead of McDonnell Douglas's DC-8-based proposal in July 1970. Initial orders were placed for two aircraft, designated "EC-137D" as test beds to evaluate the two competing radars. As the test-beds did not need the same 14-hour endurance demanded of the production aircraft, the EC-137s retained the Pratt & Whitney JT3D commercial engines, and a later reduction in endurance requirement led to retaining the normal engines in production.
The first EC-137 made its maiden flight on 9 February 1972, with the fly-off between the two radars taking place during March–July that year. Favorable test results saw the selection of Westinghouse's radar for the production aircraft. Hughes's radar was initially thought to be a certain winner, simply because much of its design was also going into the new F-15 Eagle's radar program. The Westinghouse radar used a pipelined Fast Fourier Transform (FFT) to digitally resolve 128 Doppler frequencies, while Hughes's radars used analog filters based on the design for the F-15 fighter. Westinghouse's engineering team won this competition by using a programmable 18-bit computer whose software could be modified before each mission. This computer was the AN/AYK-8 design from the B-57G program, and designated AYK-8-EP1 for its much expanded memory. This radar also multiplexed a Beyond The Horizon (BTH) pulse mode that could complement the pulse-Doppler radar mode. This proved to be beneficial especially when the BTH mode is used to detect ships at sea when the radar beam is directed below the horizon.
Full-scale development.
Approval was given on 26 January 1973 for full-scale development of the AWACS system. To allow further development of the aircraft's systems, orders were placed for three pre-production aircraft, the first of which performed its maiden flight in February 1975. To save costs, the endurance requirements were relaxed allowing the new aircraft to retain the four JT3D (US Military designation TF33) engines. IBM and Hazeltine were selected to develop the mission computer and display system. The IBM computer receiving the designation 4PI, and the software is written in JOVIAL. A Semi-Automatic Ground Environment (SAGE) or BUIC operator would immediately be at home with the track displays and tabular displays, but differences in symbology would create compatibility problems in tactical ground radar systems in Iceland, Europe and Korea over Link-11 (TADIL-A).
Modifications to the Boeing 707 for the E-3 Sentry included a rotating radar dome, single-point ground refueling, air refueling, and a bail-out chute. The original design called for two bail-out chutes (one forward, and one aft) but the aft bail-out chute was deleted as a way to cut mounting costs. Engineering, test and evaluation began on the first E-3 Sentry in October 1975. During 1977–1992, a total of 68 E-3s were built.
Future status.
Because the Boeing 707 is no longer in production, the E-3 mission package has been fitted into the Boeing E-767 for the Japan Air Self Defense Forces. The E-10 MC2A was intended to replace USAF E-3s—along with the RC-135 and the E-8—but the E-10 program was canceled by the Department of Defense. The USAF is now performing a series of incremental improvements, mainly to avionics, to bring the E-3 up to current standards of performance. Boeing is flight testing its Block 40/45 E-3s. This modified E-3 contains upgrades of the mission crew and air battle management sections, as well as significantly upgraded electronic equipment.
Another program that the Air Force is considering is the "Avionics Modernization Program" (AMP). AMP would equip the E-3s with glass cockpits. The Air Force also wants modified E-3s with jet engines that are more reliable than the original ones, and also with at least 19% higher fuel efficiencies. New turbofan engines would give these E-3s longer ranges, longer time-on-station, and a shorter critical runway length. If the modification is carried out, the E-3s could take off with full fuel loads using runways only 10000 ft long, and also at higher ambient temperatures and lower barometric pressures, such as from bases in mountainous areas. Now that the E-8 Joint STARS are being fitted with the new Pratt & Whitney JT8D-219 turbofans, which are stated as having one-half the cost of the competing engine, the CFM56, the Air Force is again studying the possibility of replacing the E-3's original turbofan engines with more-efficient ones.
Upgrading NATO's E-3 fleet is complicated by the heterogeneity of the fleet's equipment. Each NATO member's E-3 aircraft are configured differently, and NATO has not finalized upgrade or replacement plans. The airplanes themselves can be flown to 2050 with appropriate maintenance, but as the world-wide fleet of 707 aircraft dwindles, supporting the E-3 becomes more difficult.
Design.
Overview.
The E-3 Sentry's airframe is a modified Boeing 707-320B Advanced model. USAF and NATO E-3s have an unrefueled range of some 4000 mi or eight hours of flying. The newer E-3 versions bought by France, Saudi Arabia and the UK are equipped with newer CFM56-2 turbofan engines, and these can fly for about 11 hours or about 5000 mi. The Sentry's range and on-station time can be increased through air-to-air refueling and the crews can work in shifts by the use of an on-board crew rest and meals area.
When deployed, the E-3 monitors an assigned area of the battlefield and provides information for commanders of air operations to gain and maintain control of the battle; whilst as an air defense asset, E-3s can detect, identify and track airborne enemy forces far from the boundaries of the U.S. or NATO countries and can direct fighter-interceptor aircraft to these targets. In support of air-to-ground operations, the E-3 can provide direct information needed for interdiction, reconnaissance, airlift and close-air support for friendly ground forces.
Avionics.
The unpressurized rotodome is 30 feet (9.1 m) in diameter, six feet (1.8 m) thick at the center, and is held 11 feet (3.4 m) above the fuselage by two struts. It is tilted down at the front to reduce its air drag during take-offs, and while flying endurance speed (which is corrected electronically by both the radar and SSR antenna phase shifters). The dome uses both bleed air and cooling doors to remove the heat generated by electronic and mechanical equipment. The hydraulically rotated antenna system permits the Westinghouse Corporation's AN/APY-1 and AN/APY-2 passive electronically scanned array radar system to provide surveillance from the Earth's surface up into the stratosphere, over land or water.
Other major subsystems in the E-3 Sentry are navigation, communications, and computers. Consoles display computer-processed data in graphic and tabular format on video screens. Console operators perform surveillance, identification, weapons control, battle management and communications functions. The radar and computer subsystems on the E-3 can gather and present broad and detailed battlefield information. This includes position and tracking information on enemy aircraft and ships, and location and status of friendly aircraft and naval vessels. The information can be sent to major command and control centers in rear areas or aboard ships. In times of crisis, data can be forwarded to the National Command Authority in the U.S. via RC-135 or naval aircraft carrier task forces.
Electrical generators mounted on each of the E-3's four engines provide the one megawatt of electrical power that is required by the E-3's radars and other electronics. Its pulse-Doppler radar has a range of more than 250 mi (400 km) for low-flying targets at its operating altitude, and the pulse "over-the-horizon radar" radar has a range of approximately 400 mi (650 km) for aircraft flying at medium to high altitudes. The radar combined with a secondary surveillance radar to provide a look down to detect, identify and track enemy and friendly low-flying aircraft while eliminating ground clutter (radar) returns.
Upgrades.
Starting in 1987, USAF E-3s were upgraded under the "Block 30/35 Modification Program" to enhance the E-3's capabilities. On 30 October 2001, final airframe to be upgraded under this program was rolled out. Several major enhancements were made, firstly the installation of electronic support measures (ESM) and an electronic surveillance capability, for both active and passive means of detection. The Joint Tactical Information Distribution System (JTIDS) provides rapid and secure communication for transmitting information, including target positions and identification data, to other friendly platforms. Global Positioning System (GPS) capability was also added. Onboard computers were also overhauled to accommodate JTIDS, Link-16, the new ESM systems and to provide for future enhancements.
The Radar System Improvement Program (RSIP) was a joint US/NATO development program. RSIP enhances the operational capability of the E-3 radars' electronic countermeasures, and dramatically improve the system's reliability, maintainability, and availability. Essentially, this program replaced the older transistor-transistor logic (TTL) and emitter-coupled logic (MECL) electronic components, long-since out of production, with off-the-shelf digital computers that utilised a High-level programming language instead of assembly language. Significant improvement came from replacing the old 8-bit FFT with 24-bit FFTs, and the 12-bit A/D (Sign + 12-bits) with a 15-bit A/D (Sign + 15-bits). These hardware and software modifications improve the E-3 radars' performance, providing enhanced detection with an emphasis towards low radar cross-section (RCS) targets.
The RAF had also joined the USAF in adding RSIP to upgrade the E-3's radars. The retrofitting of the E-3 squadrons were completed in December 2000. Along with the RSIP upgrade was installation of the Global Positioning System/Inertial Navigation Systems which dramatically improve positioning accuracy. In 2002, Boeing was awarded a contract to add RSIP to the small French AWACS squadron. Installation was completed in 2006.
Operational history.
In March 1977 the 552nd Airborne Warning and Control Wing (now the 552d Air Control Wing) at Tinker AFB, Oklahoma received the first E-3 aircraft. The 34th and last USAF Sentry was delivered in June 1984. In March 1996, the USAF activated the 513th Air Control Group (513 ACG), an ACC-gained Air Force Reserve Command (AFRC) AWACS unit under the Reserve Associate Program. Collocated with the 552 ACW at Tinker AFB, the 513 ACG which performs similar duties on active duty E-3 aircraft shared with the 552 ACW.
The USAF have a total of thirty-one E-3s in active service. Twenty-seven are stationed at Tinker AFB and belong to the Air Combat Command (ACC). Four are assigned to the Pacific Air Forces (PACAF) and stationed at Kadena AB, Okinawa and Elmendorf AFB, Alaska. One aircraft (TS-3) was assigned to Boeing for testing and development (retired/scrapped June 2012).
In 1977 Iran placed an order for ten E-3's, however this order was cancelled following the 1979 revolution.
NATO acquired 18 E-3As and support equipment for a NATO air defense force. Since all aircraft must be registered with a certain country, the decision was made to register the 18 NATO Sentries with Luxembourg, a NATO member that previously did not have any air force. The first NATO E-3 was delivered in January 1982. The eighteen E-3s were operated by Number 1, 2 and 3 Squadrons of NATO's E-3 Component, based at NATO Air Base Geilenkirchen. Presently 17 NATO E-3As are in the inventory, since one E-3 was lost in a crash.
The United Kingdom and France are not part of the NATO E-3A Component, instead procuring E-3 aircraft through a joint project. The UK and France operate their E-3 aircraft independently of each other and of NATO. The UK operates six aircraft (with a seventh now retired) and France operates four aircraft, all fitted with the newer CFM56-2 engines. The British requirement came about following the cancellation of the British Aerospace Nimrod AEW3 project to replace the Avro Shackleton AEW2 during the 1980s. The UK E-3 order was placed in February 1987, with deliveries starting in 1990. The other operator of the type, delivered between June 1986 and September 1987, is Saudi Arabia which operates five aircraft, all fitted with CFM56-2 engines, This particular sale was hotly contested between the Reagan administration and opponents of the sale 
E-3 Sentry aircraft were among the first to deploy during Operation Desert Shield, where they immediately established as an around-the-clock radar screen to defend against Iraqi forces. During Operation Desert Storm, E-3s flew 379 missions and logged 5,052 hours of on-station time. The data collection capability of the E-3 radar and computer subsystems allowed an entire air war to be recorded for the first time in history. In addition to providing senior leadership with time-critical information on the actions of enemy forces, E-3 controllers assisted in 38 of the 41 air-to-air kills recorded during the conflict. NATO and RAF E-3s participated in the international military operation in Libya.
On 27 January 2015, the RAF deployed an E-3D Sentry to Cyprus in support of U.S.-led coalition airstrikes against Islamic State militants in Iraq and Syria. The Sentry joins RAF Panavia Tornado, MQ-9 Reaper, and AirTanker Voyager aircraft performing or supporting almost daily strikes against militants.
Incidents and accidents.
The E-3 has been involved in three hull-loss accidents.
Specifications (USAF/NATO).
"Data from" Globalsecurity.org
General characteristics* Crew: Flight crew: 4Mission crew: 13–19
Performance
References.
</dl>

</doc>
<doc id="10385" url="http://en.wikipedia.org/wiki?curid=10385" title="Northrop Grumman E-8 Joint STARS">
Northrop Grumman E-8 Joint STARS

The Northrop Grumman E-8 Joint Surveillance Target Attack Radar System (Joint STARS) is a United States Air Force Airborne ground surveillance, battle management and command and control aircraft. It tracks ground vehicles and some aircraft, collects imagery, and relays tactical pictures to ground and air theater commanders. The aircraft is operated by both active duty Air Force and Air National Guard units and also carries specially trained U.S. Army personnel as additional flight crew.
Development.
Joint STARS evolved from separate United States Army and Air Force programs to develop, detect, locate and attack enemy armor at ranges beyond the forward area of troops. In 1982, the programs were merged and the U.S. Air Force became the lead agent. The concept and sensor technology for the E-8 was developed and tested on the Tacit Blue experimental aircraft. The prime contract was awarded to Grumman Aerospace Corporation in September 1985 for two E-8A development systems.
Upgrades.
In late 2005, Northrop Grumman was awarded a contract for upgrading engines and other systems. Pratt & Whitney, in a joint venture with Seven Q Seven (SQS), will produce and deliver JT8D-219 engines for the E-8s. Their greater efficiency will allow the Joint STARS to spend more time on station, take off from a wider range of runways, climb faster, fly higher all with a much reduced cost per flying hour. 
In December 2008, an E-8C test aircraft took its first flight with the new engines. In 2009, the company began engine replacement and additional upgrade efforts. However, the re-engining funding was temporarily halted in 2009 as the Air Force began to consider other options for performing the JSTARS mission.
Future.
The Air Force began an analysis of alternatives (AOA) in March 2010 for its next generation ground moving target indication (GMTI) radar aircraft fleet. The study was completed in March 2012 and recommended buying a new business jet-based ISR aircraft, such as an Air Force version of the Navy P-8 Poseidon, and the RQ-4B Global Hawk Block 40. However, at a Senate Armed Services Committee meeting on 20 March 2012, the Air Force said it can not afford a new ISR platform. The E-8 is considered viable in the near and medium terms. As of October 2012, a test plane has had a Hewlett-Packard central computer installed, with work to begin on the rest in 2013. Before the AOA started, Northrop Grumman received funds for two batches of new engines. One set of engines has flown on a JSTARS test plane, and the other set is in storage. The Air Force does not plan to replace the engines of the 16-plane fleet due to the fiscal environment. The company wants to replace the aircraft's data link, but the Air Force will not, due to cost and because they can still receive data through satellite links. Northrop also wants to upgrade its communications with Force XXI Battle Command Brigade and Below because of the Army's shift towards the system. The Air Force says JSTARS is in a phase of the capability improvements and is expected to remain in operation through 2030.
On 23 January 2014, the USAF revealed a plan for the acquisition of a new business jet-class replacement for the E-8C JSTARS. The program is called JSTARS Recap and plans for the aircraft to reach initial operating capability (IOC) by 2022. The airframe must be more efficient, and separate contracts will be awarded for developing the aircraft, airborne sensor, battle management command and control (BMC2) system, and communications subsystem. JSTARS Recap is currently unfunded and the Air Force FY 2014 budget did not include requests. The program may be launched in FY 2015.
On 8 April 2014, the Air Force held an industry day for companies interested in competing for JSTARS Recap; attendees included Boeing, Bombardier Aerospace, and Gulfstream Aerospace. Air Force procurement documents call for a replacement for the Boeing 707-based E-8C as a "business jet class" airframe that is "significantly smaller and more efficient." Current pre-decisional requirements are for an aircraft with a 10-13 person crew with a 3.96 - radar array. Though smaller than the crew and radar size of the E-8C, it could be challenging to meet those demands in a typical business jet and could require a relatively large platform. The staffing and sensor requirements are comparable to the cancelled Northrop Grumman E-10 MC2A, which was originally planned as the E-8's replacement. The Air Force plans to award a contract at the end of FY 2016, a relatively quick pace partly to avoid budget redistributions to other programs. Replacing the E-8C avoids nearly $11 billion in operations and sustainment costs needed to keep the fleet relevant and airworthy. The aircraft is to fly at 38,000 ft for eight hours. Program managers are interested in integrating an FAA-certified flight deck, aerial refueling capability, and potentially full motion video and the joint range extension applications protocol to transmit data to joint agencies at further distances. Another potential feature could be beyond-line-of-sight communications with unmanned aerial vehicles like the RQ-4 Global Hawk.
Gulfstream confirmed in late May 2014 that they would offer their Gulfstream G650 for the Air Force's JSTARS replacement. Their bidding strategy is to team with a defense contractor to serve as the integrator. Bombardier is considering offering the Global 6000, in use with the Royal Air Force as the Raytheon Sentinel and the USAF as the E-11A airborne communications relay. Aircraft selection may be based on whether the service wants a large airframe to carry heavy payloads, or a smaller aircraft that would be more nimble and operate from shorter runways. Companies that attended the industry week in April that are contenders for providing electrical systems include Harris Corporation, Rockwell Collins, Lockheed Martin, L-3 Communications, Raytheon, DRS Technologies, and BAE Systems. Boeing plans to offer a solution based on their Boeing 737-700 commercial jetliner airframe; the 737-800 configuration is already in military service with the U.S. Navy as the P-8 Poseidon for maritime surveillance, and would be favored if the Air Force chooses a larger platform. The decision on airframe size may be based on whether the Air Force thinks it can have processing capabilities off-board or if it wants to keep everything on the physical platform.
Northrop Grumman has also announced their intention to compete for the future of Joint STARS, although they have not confirmed what airframe they will use. The company has a Gulfstream G550 test aircraft that has been integrated with Joint STARS capabilities and has flown for more than 500 hours. The test aircraft's existence was announced in 2014. Lockheed Martin has teamed with Raytheon and L-3 Communications to offer a JSTARS replacement, but will not decide which platform to use until the Air Force decides if it wants a converted airliner or business jet-sized class aircraft.
Design.
The E-8C is an aircraft modified from the Boeing 707-300 series commercial airliner. The E-8 carries specialized radar, communications, operations and control subsystems. The most prominent external feature is the 40 ft (12 m) canoe-shaped radome under the forward fuselage that houses the 24 ft (7.3 m) side-looking APY-7 passive electronically scanned array antenna.
The E-8C can respond quickly and effectively to support worldwide military contingency operations. It is a jam-resistant system capable of operating while experiencing heavy electronic countermeasures. The E-8C can fly a mission profile for 9 hours without refueling. Its range and on-station time can be substantially increased through in-flight refueling.
Radar and systems.
The AN/APY-7 radar can operate in wide area surveillance, ground moving target indicator (GMTI), fixed target indicator (FTI) target classification, and synthetic aperture radar (SAR) modes.
To pick up moving targets, the radar looks at the Doppler frequency shift of the returned signal. It can look from a long range, which the military refers to as a high standoff capability. The antenna can be tilted to either side of the aircraft for a 120-degree field of view covering nearly 50,000 km² (19,305 mile²) and can simultaneously track 600 targets at more than 250 km (152 miles). The GMTI modes cannot pick up objects that are too small, insufficiently dense, or stationary. Data processing allows the APY-7 to differentiate between armored vehicles (tracked tanks) and trucks, allowing targeting personnel to better select the appropriate ordnance for various targets.
The system's SAR modes can produce images of stationary objects. Objects with many angles (for example, the interior of a pick-up bed) will give a much better radar signature, or specular return. In addition to being able to detect, locate and track large numbers of ground vehicles, the radar has a limited capability to detect helicopters, rotating antennas and low, slow-moving fixed-wing aircraft.
The radar and computer subsystems on the E-8C can gather and display broad and detailed battlefield information. Data is collected as events occur. This includes position and tracking information on enemy and friendly ground forces. The information is relayed in near-real time to the US Army's common ground stations via the secure jam-resistant surveillance and control data link (SCDL) and to other ground C4I nodes beyond line-of-sight via ultra high frequency satellite communications.
Other major E-8C prime mission equipment are the communications/datalink (COMM/DLX) and operations and control (O&C)subsystems. Eighteen operator workstations display computer-processed data in graphic and tabular format on video screens. Operators and technicians perform battle management, surveillance, weapons, intelligence, communications and maintenance functions.
Northrop Grumman has tested the installation of a MS-177 camera on an E-8C to provide real time visual target confirmation.
Battle management.
In missions from peacekeeping operations to major theater war, the E-8C can provide targeting data and intelligence for attack aviation, naval surface fire, field artillery and friendly maneuver forces. The information helps air and land commanders to control the battlespace.
The E-8's ground-moving radar can tell approximate number of vehicles, location, speed, and direction of travel. It cannot identify exactly what type of vehicle a target is, tell what equipment it has, or discern whether it is friendly, hostile, or a bystander, so commanders often crosscheck the JSTARS data against other sources. In the Army, JSTARS data is analyzed in and disseminated from a Ground Station Module (GSM).
Operational history.
The two E-8A development aircraft were deployed in 1991 to participate in Operation Desert Storm under the direction of Albert J. Verderosa, even though they were still in development. The joint program accurately tracked mobile Iraqi forces, including tanks and Scud missiles. Crews flew developmental aircraft on 49 combat sorties, accumulating more than 500 combat hours and a 100% mission effectiveness rate.
These Joint STARS developmental aircraft also participated in Operation Joint Endeavor, a NATO peacekeeping mission, in December 1995. While flying in friendly air space, the test-bed E-8A and pre-production E-8C aircraft monitored ground movements to confirm compliance with the Dayton Peace Accords agreements. Crews flew 95 consecutive operational sorties and more than 1,000 flight hours with a 98% mission effectiveness rate.
The 93d Air Control Wing, which activated 29 January 1996, accepted its first aircraft, 11 June 1996, and deployed in support of Operation Joint Endeavor in October. The provisional 93d Air Expeditionary Group monitored treaty compliance while NATO rotated troops through Bosnia and Herzegovina. The first production E-8C and a pre-production E-8C flew 36 operational sorties and more than 470 flight hours with a 100% effectiveness rate. The wing declared initial operational capability 18 December 1997 after receiving the second production aircraft. Operation Allied Force saw Joint STARS in action again from February to June 1999 accumulating more than 1,000 flight hours and a 94.5% mission-effectiveness rate in support of the U.S. lead Kosovo War.
On 1 October 2002, the 93d Air Control Wing (93 ACW) was "blended" with the 116th Bomb Wing in a ceremony at Robins Air Force Base, Georgia. The 116 BW was an Air National Guard wing equipped with the B-1B Lancer bomber at Robins AFB. As a result of a USAF reorganization of the B-1B force, all B-1Bs were assigned to active duty wings, resulting in the 116 BW lacking a current mission. Extensive efforts by the Georgia's governor and congressional delegation led to the resulting "blending", with the newly created wing designated as the 116th Air Control Wing (116 ACW). The 93 ACW was inactivated the same day. The 116 ACW constituted the first fully blended wing of active duty and Air National Guard airmen.
The 116 ACW has been heavily involved in both Operation Enduring Freedom and Operation Iraqi Freedom, earning high marks for operational effectiveness and recently completing 10,000 combat hours. The wing took delivery of the 17th and final E-8C on 23 March 2005. The E-8C Joint STARS routinely supports various taskings of the Combined Force Command Korea during the North Korean winter exercise cycle and for the United Nations enforcing resolutions on Iraq. The twelfth production aircraft, outfitted with an upgraded operations and control subsystem, was delivered to the USAF on 5 November 2001.
On 13 March 2009, a Joint STARS aircraft was damaged beyond economical repair when a test plug was left on a fuel tank vent, subsequently causing the fuel tank to rupture during in-flight refueling. There were no casualties but the aircraft sustained $25 million in damage.
On 3 September 2009, Loren B. Thompson of the Lexington Institute raised the question of why most of the JSTARS fleet was sitting idle instead of being used to track insurgents in Afghanistan. Thompson states that the JSTARS' radar has an inherent capacity to find what the Army calls 'dismounted' targets—insurgents walking around or placing roadside bombs. Thompson's neutrality has been questioned by some since Lexington Institute is heavily funded by defense contractors, including Northrop.
Recent trials of JSTARS in Afghanistan are destined to develop tactics, techniques and procedures in tracking dismounted, moving groups of Taliban. 
On 28 November 2010, amidst escalating danger of war breaking out between North and South Korea, the South Korean government requested the U.S. to implement JSTARS in order to monitor and track North Korean military movements near the DMZ.
On 17 January 2011, Northrop Grumman's E-8C Joint Surveillance Target Attack Radar System (Joint STARS) test bed aircraft recently completed the second of two deployments to Naval Air Station Point Mugu, California, in support of the U.S. Navy Joint Surface Warfare Joint Capability Technology Demonstration to test its Network-Enabled Weapon architecture.
The Joint STARS aircraft executed three Operational Utility Assessment flights and demonstrated its ability to guide anti-ship weapons against surface combatants at a variety of standoff distances in the NEW architecture. The Joint STARS aircraft served as the network command-and-control node, as well as a node for transmitting in-flight target message updates to an AGM-154 C-1 Joint Standoff Weapon carried by U.S. Navy F/A-18 Hornets using its advanced long range tracking and targeting capability.
From 2001 to January 2011 the Joint STARS fleet has flown over 63,000 hours in 5,200 combat missions in support of Operations Iraqi Freedom, Enduring Freedom and New Dawn.
On 1 October 2011, the "blended" wing construct of the 116th Air Control Wing (116 ACW), combining Air National Guard and Regular Air Force personnel in a single unit was discontinued. On this date, the 461st Air Control Wing (461 ACW) was established at Robins AFB as the Air Force's sole active duty E-8 JSTARS wing while the 116 ACW reverted to a traditional Air National Guard wing within the Georgia Air National Guard. Both units share the same E-8 aircraft and will often fly with mixed crews, but now function as separate units.
Operators.
United States Air Force
Specifications (E-8C).
"Data from" USAF Factsheet</ul>
Avionics
AN/APY-7 synthetic aperture radar
References.
 This article incorporates  from websites or documents of the .

</doc>
<doc id="10388" url="http://en.wikipedia.org/wiki?curid=10388" title="Eric Cheney">
Eric Cheney

Eric Cheney (5 January 1924 – 30 December 2001) was an independent British motorcycle designer and constructor. He was known as one of the best motorcycle frame designers of his era.
Early life.
Cheney attended the Lancasterian School in Winchester before joining the Royal Navy at the age of 18, where he served on wartime Arctic convoys and in motor torpedo boats and gained experience of engineering and working on high performance engines. Cheney also worked on the development of remote controlled submarines for the Royal Navy.
Career.
After World War II, Cheney joined the motorcycle dealers Archers of Aldershot as a mechanic. Cheney began racing motocross and became one Britain's best riders, along with his travelling companion Les Archer, who went on to become European champion. He had ten successful years on the Continental circuit but a prolonged illness due to an infection contracted while racing in Algeria ended his riding career. He moved into bike preparation and designs for motorcycle chassis and suspension systems.
Cheney had no formal training as a motorcycle designer yet was able to create original and high performance motorcycle chassis designs working in a simple workshop that was essentially a domestic garage. His approach has been described as "like a medieval engineer" as in an age of computer aided design and significant resources for research and development teams, he worked entirely by intuition. Eric relied on his long personal experience of international off road competition riding and would prepare his initial designs for a new motorcycle frame in chalk on the wall of his workshop. Experimenting with different lines until he was satisfied, Eric would then form the steel tubing using his chalk drawings as a guide. Only when he had built a working prototype motorcycle would he start work on a final jig for mass production. He was once quoted as saying "I know when it's right and it screams at me when it's wrong."
In the late 1960s, the British motorcycle industry was unable to support a national team to compete in the International Six Days Trial so, Cheney hand built a limited number of ISDT Cheney-Triumphs using his own design of twin down-tube frame with a specially tuned Triumph 5TA engine. Fitted with tapered conical hubs, special motocross forks and large alloy fuel tanks, a Cheney Triumph was first used in the 1968 British Trophy Team. In 1970 and 1971 three 504cc Cheney Triumphs were used by the British team in the ISDT, in which Cheney won a manufacturer's prize. Replicas were built, but production was short-lived due to a shortage of engines.
Cheney's most noted successes were in the Grand Prix road racing championships, with Phil Read using his chassis in tandem with a Yamaha engine to win the 1971 250cc world championship. His designs were the last British ones to win a Grand Prix. He never worked for any of the major manufacturers but maintained a productive relationship with BSA in its heyday. After the demise of BSA in 1972, Cheney joined with former BSA factory rider John Banks to develop and campaign a highly successful BSA powered motocross bike.
Some of Cheney's motorcycle designs are now famous in their own right, such as the competition BSA Gold Stars of Jerry Scott and Keith Hickman and the John Banks replica which used a BSA B50 engine specially tuned by Cheney. He also built some racing frames for Suzuki Grand Prix motorcycles in 1968 and, it has been suggested that Suzuki engineers incorporated features of Cheney's designs, such as magnesium hubs and lower fork legs into production road going motorcycles. Cheney's company was originally known as Eric Cheney Designs, then changed to Inter-Moto, now known as Cheney Racing.
Steve McQueen.
American actor Steve McQueen, an experienced off road rider who represented the United States in the ISDT bought a number of Cheney's motorcycles at full price because he considered them better than other makes.

</doc>
<doc id="10390" url="http://en.wikipedia.org/wiki?curid=10390" title="Econometrics">
Econometrics

Econometrics is the application of mathematics, statistical methods, and computer science, to economic data and is described as the branch of economics that aims to give empirical content to economic relations. More precisely, it is "the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference." An introductory economics textbook describes econometrics as allowing economists "to sift through mountains of data to extract simple relationships." The first known use of the term "econometrics" (in cognate form) was by Polish economist Paweł Ciompa in 1910. Ragnar Frisch is credited with coining the term in the sense in which it is used today.
Basic econometric models: linear regression.
The basic tool for econometrics is the linear regression model. In modern econometrics, other statistical tools are frequently used, but linear regression is still the most frequently used starting point for an analysis. Estimating a linear regression on two variables can be visualized as fitting a line through data points representing paired values of the independent and dependent variables.
For example, consider Okun's law, which relates GDP growth to the unemployment rate. This relationship is represented in a linear regression where the change in unemployment rate (formula_1) is a function of an intercept (formula_2), a given value of GDP growth multiplied by a slope coefficient formula_3 and an error term, formula_4:
The unknown parameters formula_2 and formula_3 can be estimated. Here formula_3 is estimated to be −1.77 and formula_2 is estimated to be 0.83. This means that if GDP growth increased by one percentage point, the unemployment rate would be predicted to drop by 1.77 points. The model could then be tested for statistical significance as to whether an increase in growth is associated with a decrease in the unemployment, as hypothesized. If the estimate of formula_3 were not significantly different from 0, the test would fail to find evidence that changes in the growth rate and unemployment rate were related. The variance in a prediction of the dependent variable (unemployment) as a function of the independent variable (GDP growth) is given in polynomial least squares.
Theory.
Econometric theory uses statistical theory to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. An estimator is unbiased if its expected value is the true value of the parameter; It is consistent if it converges to the true value as sample size gets larger, and it is efficient if the estimator has lower standard error than other unbiased estimators for a given sample size. Ordinary least squares (OLS) is often used for estimation since it provides the BLUE or "best linear unbiased estimator" (where "best" means most efficient, unbiased estimator) given the Gauss-Markov assumptions. When these assumptions are violated or other statistical properties are desired, other estimation techniques such as maximum likelihood estimation, generalized method of moments, or generalized least squares are used. Estimators that incorporate prior beliefs are advocated by those who favor Bayesian statistics over traditional, classical or "frequentist" approaches.
Methods.
"Applied econometrics" uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analyzing economic history, and forecasting.
Econometrics may use standard statistical models to study economic questions, but most often they are with observational data, rather than in controlled experiments. In this, the design of observational studies in econometrics is similar to the design of studies in other observational disciplines, such as astronomy, epidemiology, sociology and political science. Analysis of data from an observational study is guided by the study protocol, although exploratory data analysis may by useful for generating new hypotheses. Economics often analyzes systems of equations and inequalities, such as supply and demand hypothesized to be in equilibrium. Consequently, the field of econometrics has developed methods for identification and estimation of simultaneous-equation models. These methods are analogous to methods used in other areas of science, such as the field of system identification in systems analysis and control theory. Such methods may allow researchers to estimate models and investigate their empirical consequences, without directly manipulating the system.
One of the fundamental statistical methods used by econometricians is regression analysis. Regression methods are important in econometrics because economists typically cannot use controlled experiments. Econometricians often seek illuminating natural experiments in the absence of evidence from controlled experiments. Observational data may be subject to omitted-variable bias and a list of other problems that must be addressed using causal analysis of simultaneous-equation models.
Example.
A simple example of a relationship in econometrics from the field of labor economics is:
This example assumes that the natural logarithm of a person's wage is a linear function of the number of years of education that person has acquired. The parameter formula_12 measures the increase in the natural log of the wage attributable to one more year of education. The term formula_13 is a random variable representing all other factors that may have direct influence on wage. The econometric goal is to estimate the parameters, formula_14 under specific assumptions about the random variable formula_13. For example, if formula_13 is uncorrelated with years of education, then the equation can be estimated with ordinary least squares.
If the researcher could randomly assign people to different levels of education, the data set thus generated would allow estimation of the effect of changes in years of education on wages. In reality, those experiments cannot be conducted. Instead, the econometrician observes the years of education of and the wages paid to people who differ along many dimensions. Given this kind of data, the estimated coefficient on Years of Education in the equation above reflects both the effect of education on wages and the effect of other variables on wages, if those other variables were correlated with education. For example, people born in certain places may have higher wages and higher levels of education. Unless the econometrician controls for place of birth in the above equation, the effect of birthplace on wages may be falsely attributed to the effect of education on wages.
The most obvious way to control for birthplace is to include a measure of the effect of birthplace in the equation above. Exclusion of birthplace, together with the assumption that formula_4 is uncorrelated with education produces a misspecified model. Another technique is to include in the equation additional set of measured covariates which are not instrumental variables, yet render formula_12 identifiable. An overview of econometric methods used to study this problem were provided by Card (1999).
Journals.
The main journals which publish work in econometrics are "Econometrica", the "Journal of Econometrics", the "Review of Economics and Statistics", "Econometric Theory", the "Journal of Applied Econometrics", "Econometric Reviews", the "Econometrics Journal", "Applied Econometrics and International Development", the "Journal of Business & Economic Statistics", and the "".
Limitations and criticisms.
Like other forms of statistical analysis, badly specified econometric models may show a spurious relationship where two variables are correlated but causally unrelated. In a study of the use of econometrics in major economics journals, McCloskey concluded that economists report p values (following the Fisherian tradition of tests of significance of point null-hypotheses), neglecting concerns of type II errors; economists fail to report estimates of the size of effects (apart from statistical significance) and to discuss their economic importance. Economists also fail to use economic reasoning for model selection, especially for deciding which variables to include in a regression.
In some cases, economic variables cannot be experimentally manipulated as treatments randomly assigned to subjects. In such cases, economists rely on observational studies, often using data sets with many strongly associated covariates, resulting in enormous numbers of models with similar explanatory ability but different covariates and regression estimates. Regarding the plurality of models compatible with observational data-sets, Edward Leamer urged that "professionals ... properly withhold belief until an inference can be shown to be adequately insensitive to the choice of assumptions".
Economists from the Austrian School argue that aggregate economic models are not well suited to describe economic reality because they waste a large part of specific knowledge. Friedrich Hayek in his "The Use of Knowledge in Society" argued that "knowledge of the particular circumstances of time and place" is not easily aggregated and is often ignored by professional economists.

</doc>
<doc id="10391" url="http://en.wikipedia.org/wiki?curid=10391" title="Ellen van Langen">
Ellen van Langen

Ellen Gezina Maria van Langen (born February 9, 1966) is a Dutch former middle distance runner, who specialised in the 800 metres. She is the 1992 Olympic Champion.
Career.
Van Langen was born in Oldenzaal, Overijssel. She has a degree in economics from the University of Amsterdam. Before she started running she played football. In 1989, she won a silver medal behind Ana Quirot at the World Student Games (Universiade), running 1:59.82. In 1990, she finished fourth in the final of the European Championships in Split, with 1:57.57.
At the 1992 Olympic Games in Barcelona, Van Langen won the Olympic title in the 800 m in a time of 1:55.54, a time which remained her personal best. As of 2014, the time ranks her 14th on the world all-time list. After her Olympic triumph, she was plagued by various injuries. Her best result after 1992 was a 6th-place finish at the 1995 World Championships. She retired from the sport in 1998.
After her gold medal win in 1992 the Amsterdam unemployment benefits office terminated her dole on grounds that she might earn money from her victory in the future.
External links.
<br>
<br>
<br>

</doc>
<doc id="10392" url="http://en.wikipedia.org/wiki?curid=10392" title="Emacs Lisp">
Emacs Lisp

Emacs Lisp is a dialect of the Lisp programming language used by the GNU Emacs and XEmacs text editors (which this article will refer to collectively as "Emacs"). It is used for implementing most of the editing functionality built into Emacs, the remainder being written in C (as is the Lisp interpreter itself). Emacs Lisp is also referred to as Elisp, although there is also an older, unrelated Lisp dialect with that name.
Users of Emacs commonly write Emacs Lisp code to customize and extend Emacs. Other options include the "Customize" feature that's in GNU Emacs since version 20. This provides a set of preference pages and when the user saves their preferences, Customize writes the necessary Emacs Lisp code to the user's config file.
Emacs Lisp can also function as a scripting language, much like the Unix Bourne shell or Python, by calling Emacs in "batch mode". In this way it may be called from the command line or via an executable file, and its editing functions, such as buffers and movement commands are available to the program just as in the normal mode.
Compared to other Lisp dialects.
In terms of features, it is closely related to the Maclisp dialect, with some later influence from Common Lisp. It supports imperative and functional programming methods. Richard Stallman chose Lisp as the extension language for his rewrite of Emacs (the original used TECO as its extension language) because of its powerful features, including the ability to treat functions as data. Unlike Common Lisp, Scheme existed at the time Stallman was rewriting Gosling Emacs into GNU Emacs, but he chose not to use it because of its comparatively poor performance on workstations, and he wanted to develop a dialect which he thought would be more easily optimized.
The Lisp dialect used in Emacs differs substantially from the more modern Common Lisp and Scheme dialects commonly used for applications programming. For example: Emacs Lisp uses dynamic rather than lexical scope by default (see below). That is, a function can reference local variables in the scope it's called from, but not in the scope where it was defined.
Example.
To understand the logic behind Emacs Lisp, it's important to remember that there is an emphasis on providing data structures and features specific to making a versatile text editor.
Here follows a simple example of an Emacs extension written in Emacs Lisp. In Emacs, the editing area can be split into separate areas called "windows", each displaying a different "buffer". A buffer is a region of text loaded into Emacs' memory (possibly from a file) which can be saved into a text document.
Users issue the "C-x 2" command to open a new window. This runs the Emacs Lisp function split-window-vertically. Normally, when the new window appears, it displays the same buffer as the previous one. Suppose we wish to make it display the next available buffer. In order to do this, the user writes the following Emacs Lisp code, in either an existing Emacs Lisp source file or an empty Emacs buffer:
The first statement, (defun ...), defines a new function, my-split-window-func, which calls split-window-vertically (the old window-splitting function), then tells the new window to display another (new) buffer. The second statement, (global-set-key ...) re-binds the key sequence "C-x 2" to the new function.
This can also be written using the feature called "advice", which allows the user to create wrappers around existing functions instead of defining their own. This has the advantage of being simpler to write but the disadvantage of making debugging more complicated. For this reason, "advice" is not allowed in the source code of GNU Emacs itself, but if a user wishes, the advice feature can be used in their own code to reimplement the above code as follows:
This instructs split-window-vertically to execute the user-supplied code whenever it is called, before executing the rest of the function.
These changes take effect at code "evaluation" time, using (for instance) the command "M-x eval-buffer" flag. It is not necessary to recompile or even restart Emacs. If the code is saved into the Emacs "init file" (usually a file named ".emacs" in the user's home directory), then Emacs will load the extension the next time it starts. Otherwise, the changes will be lost when the user exits Emacs.
Source code.
In file-systems, Emacs Lisp code exists as plain text files, by convention with the filename suffix ".el". The user's init file is an exception, often appearing as ".emacs" despite being evaluated as any Emacs Lisp code. When the files are loaded, an interpreter component of the Emacs program reads and parses the functions and variables, storing them in memory. They are then available to other editing functions, and to user commands. Functions and variables can be freely modified and re-loaded.
In order to save time and memory space, much of the functionality of Emacs loads only when required. Each set of optional features is implemented by a collection of Emacs code called a "library". For example, there is a library for highlighting keywords in program source code, and a library for playing the game of Tetris. Each library is implemented using one or more Emacs Lisp source files.
Emacs developers write certain functions in C. These are "primitives", also known as "built-in functions" or "subrs". Although primitives can be called from Lisp code, they can only be modified by editing the C source files and recompiling. In GNU Emacs, primitives are not available as external libraries; they are part of the Emacs executable. In XEmacs, runtime loading of such primitives is possible, using the operating system's support for dynamic linking. Functions may be written as primitives because they need access to external data and libraries not otherwise available from Emacs Lisp, or because they are called often enough that the comparative speed of C versus Emacs Lisp makes a worthwhile difference.
However, because errors in C code can easily lead to segmentation violations or to more subtle bugs, which crash the editor, and because writing C code that interacts correctly with the Emacs Lisp garbage collector is error-prone, relatively few functions are implemented as primitives.
Byte code.
"Byte-compilation" can make Emacs Lisp code faster. Emacs contains a compiler which can translate Emacs Lisp source files into a special representation known as bytecode. Emacs Lisp bytecode files have the filename suffix ".elc". Compared to source files, bytecode files load faster, occupy less space on the disk, use less memory when loaded, and run faster.
Bytecode still runs more slowly than primitives, but functions loaded as bytecode can be easily modified and re-loaded. In addition, bytecode files are platform-independent. The standard Emacs Lisp code distributed with Emacs is loaded as bytecode, although the matching source files are usually provided for the user's reference as well. User-supplied extensions are typically not byte-compiled, as they are neither as large nor as computationally intensive.
Language features.
Notably, the "cl" package implements a fairly large subset of Common Lisp.
Emacs Lisp (unlike some other Lisp implementations) does not do tail-call optimization. Without this, tail recursions can eventually lead to stack overflow.
The apel library aids in writing portable Emacs Lisp code, with the help of the polysylabi platform bridge.
From dynamic to lexical scoping.
Emacs Lisp uses dynamic scope, offering static (or lexical) as an option starting from version 24. It can be activated by setting the file local variable codice_1.
In dynamic scoping, if a programmer declares a variable within the scope of a function, it is available to subroutines called from within that function. Originally, this was intended as an optimization; lexical scoping was still uncommon and of uncertain performance. "I asked RMS when he was implementing emacs lisp why it was dynamically scoped and his exact reply was that lexical scope was too inefficient." Dynamic scoping was also meant to provide greater flexibility for user customizations. However, dynamic scoping has several disadvantages. Firstly, it can easily lead to bugs in large programs, due to unintended interactions between variables in different functions. Secondly, accessing variables under dynamic scoping is generally slower than under lexical scoping.
Also, the codice_2 macro in the "cl" package does provide effective lexical scope to Emacs Lisp programmers, but while `cl' is widely used, codice_2 is rarely used.
Discouraged features.
After decades of evolution, some features have become deprecated or replaced, and other features are now supported for Emacs users but their use is not allowed in the Emacs source code itself. codice_4 and codice_5 are two examples of the latter, of which Richard Stallman says "it makes for confusion in debugging. (...) Users can use these features -- the only people they might confuse are themselves, (...) However, in our code, we should handle these situations in other ways." Adding new hooks can sometimes provide what's necessary to replace "advice".

</doc>
<doc id="10393" url="http://en.wikipedia.org/wiki?curid=10393" title="Edward Bulwer-Lytton">
Edward Bulwer-Lytton

Edward George Earle Lytton Bulwer-Lytton, 1st Baron Lytton PC (25 May 1803 – 18 January 1873), was an English novelist, poet, playwright, and politician. He was immensely popular with the reading public and wrote a stream of bestselling novels which earned him a considerable fortune. He coined the phrases "the great unwashed", "pursuit of the almighty dollar", "the pen is mightier than the sword", "dweller on the threshold", as well as the infamous opening line "It was a dark and stormy night".
Life.
Bulwer-Lytton was born on 25 May 1803 to General William Earle Bulwer of Heydon Hall and Wood Dalling, Norfolk and Elizabeth Barbara Lytton, daughter of Richard Warburton Lytton of Knebworth, Hertfordshire. He had two elder brothers, William Earle Lytton Bulwer (1799–1877) and Henry (1801–1872), later Lord Dalling and Bulwer.
When Edward was four his father died and his mother moved to London. He was a delicate, neurotic child and was discontented at a number of boarding schools. But he was precocious and Mr Wallington at Baling encouraged him to publish, at the age of fifteen, an immature work, "Ishmael and Other Poems".
In 1822 he entered Trinity College, Cambridge, where he met John Auldjo, but shortly afterwards moved to Trinity Hall. In 1825 he won the Chancellor's Gold Medal for English verse. In the following year he took his B.A. degree and printed, for private circulation, a small volume of poems, "Weeds and Wild Flowers".
He purchased a commission in the army, but sold it without serving.
In August 1827, against his mother's wishes, he married Rosina Doyle Wheeler (1802–1882), a famous Irish beauty. When they married his mother withdrew his allowance and he was forced to work for a living. They had two children, Lady Emily Elizabeth Bulwer-Lytton (1828–1848), and (Edward) Robert Lytton Bulwer-Lytton, 1st Earl of Lytton (1831–1891) who became Governor-General and Viceroy of British India (1876–1880).
His writing and political work strained their marriage while his infidelity embittered Rosina; in 1833 they separated acrimoniously and in 1836 the separation became legal. Three years later, Rosina published "Cheveley, or the Man of Honour" (1839), a near-libellous fiction bitterly satirising her husband's alleged hypocrisy.
In June 1858, when her husband was standing as parliamentary candidate for Hertfordshire, she indignantly denounced him at the hustings. He retaliated by threatening her publishers, withholding her allowance, and denying access to the children. Finally he had her committed to a mental asylum. But, after a public outcry she was released a few weeks later. This incident was chronicled in her memoir, "A Blighted Life" (1880). For years she continued her attacks upon her husband’s character. 
The death of Bulwer-Lytton's mother in 1843 greatly saddened him. His own "exhaustion of toil and study had been completed by great anxiety and grief", and by "about the January of 1844, I was thoroughly shattered". In his mother's room, Bulwer-Lytton "had inscribed above the mantelpiece a request that future generations preserve the room as his beloved mother had used it"; it remains essentially unchanged to this day.
On 20 February 1844, in accordance with his mother's will, he changed his surname from 'Bulwer' to 'Bulwer-Lytton' and assumed the arms of Lytton by royal licence. His widowed mother had done the same in 1811. But, his brothers remained plain 'Bulwer'.
By chance he encountered a copy of "Captain Claridge's work on the "Water Cure", as practised by Priessnitz, at Graefenberg", and "making allowances for certain exaggerations therein", pondered the option of travelling to Graefenberg, but preferred to find something closer to home, with access to his own doctors in case of failure: "I who scarcely lived through a day without leech or potion!".
After reading a pamphlet by Doctor James Wilson, who operated a hydropathic establishment with James Manby Gully at Malvern, he stayed there for "some nine or ten weeks", after which he "continued the system some seven weeks longer under Doctor Weiss, at Petersham", then again at "Doctor Schmidt's magnificent hydropathic establishment at Boppart" (at the former Marienberg Convent at Boppard), after developing a cold and fever upon his return home.
When King Otto of Greece abdicated in 1862, he was offered the crown of Greece, which he declined.
In 1866 Bulwer-Lytton was raised to the peerage as Baron Lytton.
The English Rosicrucian society, founded in 1867 by Robert Wentworth Little, claimed Bulwer-Lytton as their 'Grand Patron', but he wrote to the society complaining that he was 'extremely surprised' by their use of the title, as he had 'never sanctioned such'. Nevertheless, a number of esoteric groups have continued to claim Bulwer-Lytton as their own, chiefly because some of his writings—such as the 1842 book "Zanoni"—have included Rosicrucian and other esoteric notions. According to the Fulham Football Club, he once resided in the original Craven Cottage, today the site of their stadium.
Bulwer-Lytton had long suffered with a disease of the ear and for the last two or three years of his life he lived in Torquay nursing his health. Following an operation to cure deafness, an abscess formed in his ear and burst; he endured intense pain for a week and died at 2am on 18 January 1873 just short of his 70th birthday. The cause of death was not clear but it was thought that the infection had affected his brain and caused a fit. Rosina outlived him by nine years. Against his wishes, Bulwer-Lytton was honoured with a burial in Westminster Abbey.
His unfinished history "Athens: Its Rise and Fall" was published posthumously.
Career.
Bulwer-Lytton began his career as a follower of Jeremy Bentham. In 1831 he was elected member for St Ives in Cornwall, after which he was returned for Lincoln in 1832, and sat in Parliament for that city for nine years. He spoke in favour of the Reform Bill, and took the leading part in securing the reduction, after vainly essaying the repeal, of the newspaper stamp duties. His influence was perhaps most keenly felt when, on the Whigs’ dismissal from office in 1834, he issued a pamphlet entitled "A Letter to a Late Cabinet Minister on the Crisis". Lord Melbourne, then Prime Minister, offered him a lordship of the admiralty, which he declined as likely to interfere with his activity as an author.
In 1841, he left Parliament and didn't return to politics until 1852; this time, having differed from the policy of Lord John Russell over the Corn Laws, he stood for Hertfordshire as a Conservative. Lord Lytton held that seat until 1866, when he was raised to the peerage as "Baron Lytton of Knebworth" in the County of Hertford. In 1858 he entered Lord Derby's government as Secretary of State for the Colonies, thus serving alongside his old friend Disraeli. In the House of Lords he was comparatively inactive. He took a proprietary interest in the development of the Crown Colony of British Columbia and wrote with great passion to the Royal Engineers upon assigning them their duties there. The former HBC Fort Dallas at Camchin, the confluence of the Thompson and Fraser Rivers, was renamed in his honour by Governor Sir James Douglas in 1858 as Lytton, British Columbia.
Literary works.
Bulwer-Lytton's literary career began in 1820—with the publication of a book of poems—and spanned much of the nineteenth century. He wrote in a variety of genres, including historical fiction, mystery, romance, the occult, and science fiction. He financed his extravagant life with a varied and prolific literary output, sometimes publishing anonymously. 
In 1828 "Pelham" brought him public acclaim and established his reputation as a wit and dandy. The book also made a significant contribution in the changing of men's fashion. Prior to the novel, evening wear for men could be of any colour, but the upper-class quickly adopted the habit of using black evening wear only, a habit that is still dominant, just as the characters in "Pelham". Its intricate plot and humorous, intimate portrayal of pre-Victorian dandyism kept gossips busy trying to associate public figures with characters in the book. "Pelham" resembled Benjamin Disraeli's recent first novel "Vivian Grey" (1827).
Bulwer-Lytton admired Benjamin’s father, Isaac D'Israeli, himself a noted author. They began corresponding in the late 1820s and met for the first time in March 1830, when Isaac D'Israeli dined at Bulwer-Lytton’s house. (Also present that evening were Charles Pelham Villiers and Alexander Cockburn. The young Villiers was to have a long parliamentary career, while Cockburn became Lord Chief Justice of England in 1859.) 
Bulwer-Lytton reached the height of his popularity with the publication of "Godolphin" (1833). This was followed by "The Pilgrims of the Rhine" (1834), "The Last Days of Pompeii" (1834), "Rienzi, Last of the Roman Tribunes" (1835), and "Harold, the Last of the Saxons" (1848). "The Last Days of Pompeii" was inspired by Karl Briullov's painting, "The Last Day of Pompeii", which Bulwer-Lytton saw in Milan.
He also wrote the horror story "The Haunted and the Haunters" or "The House and the Brain" (1859). Another novel dealing with a supernatural theme was "A Strange Story" (1862), which was an influence on Bram Stoker's "Dracula".
Bulwer-Lyton penned many other works, including "The Coming Race" or "Vril: The Power of the Coming Race" (1871), which drew heavily on his interest in the occult and contributed to the birth of the science fiction genre. Its story of a subterranean race waiting to reclaim the surface of the Earth is an early science fiction theme. The book popularised the Hollow Earth theory and may have inspired Nazi mysticism. His term "vril" lent its name to Bovril meat extract. Adopted by theosophists and occultists since the 1870s, "vril" would develop into a major esoteric topic, and eventually become closely associated with the ideas of an esoteric neo-Nazism after 1945.
His play, "Money" (1840), was first produced at the Theatre Royal, Haymarket, London, on 8 December 1840. The first American production was at the Old Park Theater in New York on 1 February 1841. Subsequent productions include the Prince of Wales's Theatre's in 1872 and it was also the inaugural play at the new California Theatre in San Francisco in 1869.
Among Bulwer-Lytton's lesser-known contributions to literature is that he convinced Charles Dickens to revise the ending of "Great Expectations" to make it more palatable to the reading public, as in the original version of the novel, Pip and Estella do not get together.
Legacy.
Quotations.
Bulwer-Lytton's most famous quotation, "the pen is mightier than the sword", is from his play "Richelieu" where it appears in the line "beneath the rule of men entirely great, the pen is mightier than the sword"
In addition, he gave the world the memorable phrase "pursuit of the almighty dollar" from his novel "The Coming Race".
He is also credited with "the great unwashed". He used this rather disparaging term in his 1830 novel "Paul Clifford": "He is certainly a man who bathes and ‘lives cleanly’, (two especial charges preferred against him by Messrs. the Great Unwashed)." 
"The Last Days of Pompeii" has been cited as the first source, but inspection of the original text shows this to be wrong. However, the term "the Unwashed" with the same meaning, appears in "The Parisians": "He says that Paris has grown so dirty since 4 September, that it is only fit for the feet of the Unwashed." "The Parisians", though, was not published until 1872, while William Makepeace Thackeray's novel "Pendennis" (1850) uses the phrase ironically, implying it was already established. The Oxford English Dictionary refers to "Messrs. the Great Unwashed" in Lytton's "Paul Clifford" (1830), as the earliest instance.
Bulwer-Lytton is also credited with the appellation for the Germans "Das Volk der Dichter und Denker", that is, the people of poets and thinkers.
Theosophy.
Also the writers of theosophy were influenced by his work. Annie Besant and especially Helena Blavatsky incorporated his thoughts and ideas from particularly "The Last Days of Pompeii", "Vril, the Power of the Coming Race" and "Zanoni" in her own books.
Contest.
Bulwer-Lytton's name lives on in the annual Bulwer-Lytton Fiction Contest, in which contestants think-up terrible openings for imaginary novels, inspired by the first line of his 1830 novel "Paul Clifford":"It was a dark and stormy night; the rain fell in torrents—except at occasional intervals, when it was checked by a violent gust of wind which swept up the streets (for it is in London that our scene lies), rattling along the housetops, and fiercely agitating the scanty flame of the lamps that struggled against the darkness." Entrants in the contest seek to capture the rapid changes in point of view, the florid language, and the atmosphere of the full sentence. The opening was popularized by the "Peanuts" comic strip, in which Snoopy's sessions on the typewriter usually began with It was a dark and stormy night. The same words also form the first sentence of Madeleine L'Engle’s Newbery Medal-winning novel "A Wrinkle in Time." Similar wording appears in Edgar Allan Poe's 1831 short story, "The Bargain Lost", although not at the very beginning. It reads:"It was a dark and stormy night. The rain fell in cataracts."
Operas.
Several of Bulwer-Lytton's novels were made into operas, one of which, "Rienzi, der Letzte der Tribunen" (1842) by Richard Wagner, eventually became more famous than the novel. "Leonora" (1846) by William Henry Fry, the first European-styled "grand" opera composed in the United States of America, is based on Bulwer-Lytton's play "The Lady of Lyons", as is Frederic Cowen's first opera "Pauline" (1876). Verdi rival Errico Petrella's most successful opera, "Jone" (1858), was based upon Bulwer-Lytton's "The Last Days of Pompeii", and was performed all over the world until the First World War. "Harold, the Last of the Saxons" (1848) was the source for Verdi's opera "Aroldo" in 1857.
Magazines.
In 1831 Bulwer-Lytton became the editor of the "New Monthly" but he resigned the following year. In 1841, he started the "Monthly Chronicle", a semi-scientific magazine. During his career he wrote poetry, prose, and stage plays; his last novel was "Kenelm Chillingly", which was in course of publication in "Blackwood’s Magazine" at the time of his death in 1873.
Translations.
Bulwer-Lytton's works of fiction and non-fiction were translated in his day and since then into many languages, including Serbian (by Laza Kostic), German, Russian, Norwegian, Swedish, French, Finnish, and Spanish. In 1879, his "Ernest Maltravers" was the first complete novel from the West to be translated into Japanese.
Place names.
The township of Lytton, Quebec (today part of Montcerf-Lytton) was named after him.
Lytton, British Columbia, and Lytton, Iowa are also named after him.
Portrayal on television.
Bulwer-Lytton was portrayed by the actor Brett Usher in the 1978 television serial "Disraeli".
External links.
 Bulwer-Lytton ebooks
Other links

</doc>
<doc id="10400" url="http://en.wikipedia.org/wiki?curid=10400" title="History of Esperanto">
History of Esperanto

The constructed international auxiliary language Esperanto was developed in the 1870s and 80s by L. L. Zamenhof, and first published in 1887. The number of speakers has grown gradually over time, although it has not had much support from governments and international bodies, and has sometimes been outlawed or otherwise suppressed.
Standardized Yiddish and relexified Esperanto.
Around 1880, while in Moscow and approximately simultaneously with working on Esperanto, Zamenhof made an aborted attempt to standardize Yiddish, based on his native Bialystok (Northeastern) dialect, as a unifying language for the Jews of the Russian Empire. He even used a Latin alphabet, with the letters "ć, h́, ś, ź" (the same as in early drafts of Esperanto, later "ĉ, ĥ, ŝ, ĵ") and "ě" for schwa. However, he concluded there was no future for such a project, and abandoned it, dedicating himself to Esperanto as a unifying language for all humankind. It has been suggested that Esperanto was not an arbitrary pastiche of major European languages but a Latinate relexification of Yiddish, and thus both genealogically a Slavic language, and a close parallel to Modern Hebrew, a Hebraic relexification of Yiddish.
Development of the language before publication.
Zamenhof would later say that he had dreamed of a world language since he was a child. At first he considered a revival of Latin, but after learning it in school he decided it was too complicated to be a common means of international communication. When he learned English, he realised that verb conjugations were unnecessary, and that grammatical systems could be much simpler than he had expected. He still had the problem of memorising a large vocabulary, until he noticed two Russian signs labelled Швейцарская ("švejtsarskaja," a porter's lodge – from швейцар "švejtsar," a porter) and Кондитерская ("konditerskaja," a confectioner's shop – from кондитер "konditer," a confectioner). He then realised that a judicious use of affixes could greatly decrease the number of root words needed for communication. He chose to take his vocabulary from Romance and Germanic, the languages that were most widely taught in schools around the world and would therefore be recognisable to the largest number of people.
Zamenhof taught an early version of the language to his high-school classmates. Then, for several years, he worked on translations and poetry to refine his creation. In 1895 he wrote, "I worked for six years perfecting and testing the language, even though it had seemed to me in 1878 that it was already completely ready." When he was ready to publish, the Czarist censors would not allow it. Stymied, he spent his time in translating works such as the Bible and Shakespeare. This enforced delay led to continued improvement. In July 1887 he published his "Unua Libro" (First Book), a basic introduction to the language. This was essentially the language spoken today.
Esperanto history from publication until the first world congress.
At first the movement grew most in the Russian empire and eastern Europe, but soon spread to western Europe and beyond: to Argentina in 1889; to Canada in 1901; to Algeria, Chile, Japan, Mexico, and Peru in 1903; to Tunisia in 1904; and to Australia, the United States, Guinea, Indochina, New Zealand, Tonkin, and Uruguay in 1905.
In its first years Esperanto was used mainly in publications by Zamenhof and early adopters like Antoni Grabowski, in extensive correspondence (mostly now lost), in the magazine "La Esperantisto", published from 1889 to 1895 and only occasionally in personal encounters.
In 1894 under pressure from Wilhelm Trompeter, the publisher of the magazine "La Esperantisto", and some other leading users, Zamenhof reluctantly put forward a radical reform to be voted on by readers. He proposed the reduction of the alphabet to 22 letters (by eliminating the accented letters and most of their sounds), the change of the plural to "-i", the use of a positional accusative instead of the ending "-n", the removal of the distinction between adjectives and adverbs, the reduction of the number of participles from six to two, and the replacement of the table of correlatives with more Latinate words or phrases. These reforms were overwhelmingly rejected, but some were picked up in subsequent reforms (such as Ido) and criticisms of the language. In the following decade Esperanto spread into western Europe, especially France. By 1905 there were already 27 magazines being published (Auld 1988).
A small international conference was held in 1904, leading to the first world congress in August 1905 in Boulogne-sur-Mer, France. There were 688 Esperanto speakers present from 20 nationalities. At this congress, Zamenhof officially resigned his leadership of the Esperanto movement, as he did not want personal prejudice against himself (or anti-Semitism) to hinder the progress of the language. He proposed a declaration on founding principles of the Esperanto movement, which the attendees of the congress endorsed.
Esperanto history since the first congress.
World congresses have been held every year since 1905, except during the two World Wars.
The autonomous territory of Neutral Moresnet, between Belgium and Germany, had a sizable proportion of Esperanto-speakers among its small and multiethnic population. There was a proposal to make Esperanto its official language.
In the early 1920s, there was a proposal for the League of Nations to accept Esperanto as their working language. Ten delegates accepted the proposal with only one voice against, the French delegate, Gabriel Hanotaux. Hanotaux did not like how the French language was losing its position as the international language and saw Esperanto as a threat. However, two years later the League recommended that its member states include Esperanto in their educational curricula. Many people see the 1920s as the heyday of the Esperanto movement.
Starting in the 1930s, Adolf Hitler and Joseph Stalin murdered many Esperanto speakers because of their anti-nationalistic tendencies.
Hitler wrote in "Mein Kampf" that it was created as a universal language to unite the Jewish diaspora.
The creation of a Jew-free National German Esperanto League was not enough to placate the Nazis.
The teaching of Esperanto was not allowed in German prisoner-of-war camps during World War II. Esperantists sometimes were able to get around the ban by convincing guards that they were teaching Italian, the language of Germany's closest ally.
In the early years of the Soviet Union, Esperanto was given a measure of government support, and an officially recognized Soviet Esperanto Association came into being. However, in 1937, Stalin reversed this policy. He denounced Esperanto as "the language of spies" and had Esperantists exiled or executed. The use of Esperanto was effectively banned until 1956.
While Esperanto itself was not enough cause for execution, its use was extended among Jews or trade unionists and encouraged contacts with foreigners.
Fascist Italy, on the other hand, made some efforts of promoting tourism in Italy through Esperanto leaflets and appreciated the similarities of Italian and Esperanto.
Portugal's right-wing governments persecuted the language from 1936 until the Carnation Revolution of 1974. After the Spanish Civil War, Francoist Spain persecuted the Anarchists and Catalan nationalists among whom the speaking of Esperanto had been quite widespread; but in the 1950s, the Esperanto movement was tolerated again, with Francisco Franco accepting the honorary patronage of the Madrid World Esperanto Congress.
The Cold War, especially in the 1950s and 1960s, put a damper on the Esperanto movement as well, as there were fears on both sides that Esperanto could be used for enemy propaganda. However, the language experienced something of a renaissance in the 1970s and spread to new parts of the world, such as its veritable explosion in popularity in Iran in 1975. By 1991 there were enough African Esperantists to warrant a pan-African congress. The language continues to spread, although it is not officially recognised by any country, and is part of the state educational curriculum of only a few.
Evolution of the language.
The Declaration of Boulogne of 1905 limited changes to Esperanto. That declaration stated, among other things, that the basis of the language should remain the "Fundamento de Esperanto" ("Foundation of Esperanto", a group of early works by Zamenhof), which is to be binding forever: nobody has the right to make changes to it. The declaration also permits new concepts to be expressed as the speaker sees fit, but it recommends doing so in accordance with the original style.
Many Esperantists believe this declaration stabilising the language is a major reason why the Esperanto speaker community grew beyond the levels attained by other constructed languages and has developed a flourishing culture. Other constructed languages have been hindered from developing a stable speaking community by continual tinkering. Also, many developers of constructed languages have been possessive of their creation and have worked to prevent others from contributing to the language. One such ultimately disastrous case was Schleyer's Volapük. In contrast, Zamenhof declared that "Esperanto belongs to the Esperantists", and moved to the background once the language was published, allowing others to share in the early development of the language.
The grammatical description in the earliest books was somewhat vague, so a consensus on usage (influenced by Zamenhof's answers to some questions) developed over time within boundaries set by the initial outline (Auld 1988). Even before the Declaration of Boulogne, the language was remarkably stable; only one set of lexical changes were made in the first year after publication, namely changing "when", "then", "never", "sometimes", "always" from "kian", "tian", "nenian", "ian", "ĉian" to "kiam", "tiam", "neniam" etc., to avoid confusion with the accusative forms of "kia" "what sort of", "tia" "that sort of", etc. Thus Esperanto achieved a stability of structure and grammar similar to that which natural languages enjoy by virtue of their native speakers and established bodies of literature. One could learn Esperanto without having it move from underfoot. Changes could and did occur in the language, but only by acquiring widespread popular support; there was no central authority making arbitrary changes, as happened with Volapük and some other languages.
Modern Esperanto usage may in fact depart from that originally described in the "Fundamento", though the differences are largely semantic (involving changed meaning of words) rather than grammatical or phonological. The translation given for "I like this one", in the sample phrases in the main Esperanto article, offers a significant example. According to the "Fundamento", "Mi ŝatas ĉi tiun" would in fact have meant "I esteem this one". The traditional usage is "Tiu ĉi plaĉas al mi" (literally, "this one is pleasing to me"), which reflects the phrasing of most European languages (French "celui-ci me plaît", Spanish "éste me gusta", Russian "это мне нравится" [eto mnye nravitsya], German "Das gefällt mir", Italian "mi piace"). However, the original "Ĉi tiu plaĉas al mi" continues to be commonly used.
For later changes to the language, see Modern evolution of Esperanto.
Dialects, reform projects and derived languages.
Esperanto has not fragmented into regional dialects through natural language use. This may be because it is the language of daily communication for only a small minority of its speakers. However at least three other factors work against dialects, namely the centripetal force of the Fundamento, the unifying influence of the Plena Vortaro and its successors, which exemplified usage from the works of Zamenhof and leading writers, and the transnational ambitions of the speech community itself. Slang and jargon have developed to some extent, but such features interfere with universal communication – the whole point of Esperanto – and so have generally been avoided.
However, in the early twentieth century numerous reform projects were proposed. Almost all of these "esperantido"s were stillborn, but the very first, Ido ("Offspring"), had significant success for several years. Ido was proposed by the "Delegation for the Adoption of an International Auxiliary Language" in Paris in October 1907. Its main reforms were in bringing the alphabet, semantics, and some grammatical features into closer alignment with the Romance languages, as well as removal of adjectival agreement and the accusative case except when necessary. At first a number of leading Esperantists put their support behind the Ido project, but the movement stagnated and declined, first with the accidental death of one of its main proponents and later as people proposed further changes, and the number of current speakers is estimated at between 250 and 5000. However, Ido has proven to be a rich source of Esperanto vocabulary.
Some more focused reform projects, affecting only a particular feature of the language, have gained a few adherents. One of these is "riism", which modifies the language to incorporate non-sexist language and gender-neutral pronouns. However, most of these projects are specific to individual nationalities (riism from English speakers, for example), and the only changes that have gained acceptance in the Esperanto community have been the minor and gradual bottom-up reforms discussed in the last section.
Esperanto is credited with influencing or inspiring several later competing language projects, such as Occidental (1922) and Novial (1928). These always lagged far behind Esperanto in their popularity. By contrast, Interlingua (1951) has greatly surpassed Ido in terms of popularity. It shows little or no Esperanto influence, however.

</doc>
<doc id="10402" url="http://en.wikipedia.org/wiki?curid=10402" title="Esperanto grammar">
Esperanto grammar

Esperanto is a constructed auxiliary language. A highly regular grammar makes Esperanto much easier to learn than most other languages of the world, though particular features may be more or less advantageous or difficult depending on the language background of the learner. Parts of speech are immediately obvious, for example: Τhe suffix "-o" indicates a noun, "-a" an adjective, "-as" a present-tense verb, and so on for other grammatical functions. An extensive system of affixes may be freely combined with roots to generate vocabulary; and the rules of word formation are straightforward, allowing speakers to communicate with a much smaller root vocabulary than in most other languages. It is possible to communicate effectively with a vocabulary built upon 400 to 500 roots, though there are numerous specialized vocabularies for sciences, professions, and other activities. 
Reference grammars of the language include the "Plena Analiza Gramatiko" (English: "Complete Analytical Grammar") by Kálmán Kalocsay and Gaston Waringhien, and the "Plena Manlibro de Esperanta Gramatiko" (English: "Complete Handbook of Esperanto Grammar") by Bertilo Wennergren.
Grammatical summary.
Esperanto has an agglutinative morphology, no grammatical gender, and simple verbal and nominal inflections. Verbal suffixes indicate four moods, of which the indicative has three tenses, and are derived for several aspects, but do not agree with the grammatical person or number of their subjects. Nouns and adjectives have two cases, nominative/oblique and accusative/allative, and two numbers, singular and plural; the adjectival form of personal pronouns behaves like a genitive case. Adjectives generally agree with nouns in case and number. In addition to indicating direct objects, the accusative/allative case is used with nouns, adjectives and adverbs to show the destination of a motion, or to replace certain prepositions; the nominative/oblique is used in all other situations. The case system allows for a flexible word order that reflects information flow and other pragmatic concerns, as in Russian, Greek, and Latin. 
These concepts are illustrated below.
Script and pronunciation.
Esperanto uses the Latin alphabet. The orthography utilizes diacritics, which make digraphs such as English "ch" and "sh" unnecessary. (Alternatively, Esperanto may be written with English-like digraphs in "h" or, unofficially, "x", rather than with diacritics, but this is seldom seen outside email.) Overall, the Esperanto alphabet resembles the Czech alphabet, but with circumflexes rather than háčeks on the letters "ĉ, ŝ;" Western-based "ĝ, ĵ" in place of Czech "dž, ž;" and "ĥ" for Czech "ch." These letters are unique to Esperanto, though it also has a letter "ŭ" that is shared with the Belarusian Łacinka alphabet and has been dropped from the Romanian alphabet. 
Zamenhof suggested Italian as a model for Esperanto pronunciation.
The article.
Esperanto has a single definite article, "la," which is invariable. It is similar to English "the." 
"La" is used:
The article is also used for inalienable possession of body parts and kin terms, where English would use a possessive adjective:
The article "la," like the demonstrative adjective "tiu" (this, that), nearly always occurs at the beginning of the noun phrase, but this is not required by the grammar, and exceptions occur in poetry.
There is no grammatically required indefinite article: "homo" means either "human being" or "a human being", depending on the context, and similarly the plural "homoj" means "human beings" or "some human beings". The words "iu" and "unu" (or their plurals "iuj" and "unuj") may be used somewhat like indefinite articles, but they're closer in meaning to "some" and "a certain" than to English "a". This use of "unu" corresponds to English "a" when the "a" indicates a specific individual. Consider, for example, 
This use of "unu" plays the role of "there was" in the introduction of fairy tales ("There was an old woman who ...") and in introducing new participants ("A man came up to me and said ...").
Parts of speech.
The suffixes "-o, -a, -e," and "-i" indicate that a word is a noun, adjective, adverb, and infinitive verb, respectively. Many new words can be derived simply by changing these suffixes, just as "-ly" derives adverbs from adjectives in English: From "vidi" (to see), we get "vida" (visual), "vide" (visually), and "vido" (sight). 
Each root word has an inherent part of speech: nominal, adjectival, verbal, or adverbial. These must be memorized explicitly and affect the use of the part-of-speech suffixes. With an adjectival or verbal root, the nominal suffix -o indicates an abstraction: "parolo" (an act of speech, one's word) from the verbal root "paroli" (to speak); "belo" (beauty) from the adjectival root "bela" (beautiful); whereas with a noun, the nominal suffix simply indicates the noun. Nominal or verbal roots may likewise be modified with the adjectival suffix -a: "reĝa" (royal), from the nominal root "reĝo" (a king); "parola" (spoken). The various verbal endings mean "to be" when added to an adjectival root: "beli" (to be beautiful); and with a nominal root they mean "to act as" the noun, "to use" the noun, "etc.," depending on the semantics of the root: "reĝi" (to reign). There are relatively few adverbial roots, so most words ending in "-e" are derived: "bele" (beautifully). Often with a nominal or verbal root, the English equivalent is a prepositional phrase: "parole" (by speech, orally); "vide" (by sight, visually); "reĝe" (like a king, royally). 
The meanings of part-of-speech affixes depend on the inherent part of speech of the root they are applied to. For example, "brosi" (to brush) is based on a nominal root (and therefore listed in modern dictionaries under the entry "broso)," whereas "kombi" (to comb) is based on a verbal root (and therefore listed under "kombi)." Change the suffix to "-o," and the similar meanings of "brosi" and "kombi" diverge: "broso" is a brush, the name of an instrument, whereas "kombo" is a combing, the name of an action. That is, changing verbal "kombi" (to comb) to a noun simply creates the name for the action; for the name of the tool, the suffix "-ilo" is used, which derives words for instruments from verbal roots: "kombilo" (a comb). On the other hand, changing the nominal root "broso" (a brush) to a verb gives the action associated with that noun, "brosi" (to brush). For the name of the action, the suffix "-ado" will change a derived verb back to a noun: "brosado" (a brushing). Similarly, an abstraction of a nominal root (changing it to an adjective and then back to a noun) requires the suffix "-eco," as in "infaneco" (childhood), but an abstraction of an adjectival or verbal root merely requires the nominal "-o: belo" (beauty). Nevertheless, redundantly affixed forms such as "beleco" are acceptable and widely used. 
A limited number of basic adverbs do not end with "-e," but with an undefined part-of-speech ending "-aŭ". Not all words ending in "-aŭ" are adverbs, and most of the adverbs that end in "-aŭ" have other functions, such as "hodiaŭ" "today" [noun or adverb] or "ankoraŭ" "yet, still" [conjunction or adverb]. About a dozen other adverbs are bare roots, such as "nun" "now", "tro" "too, too much", not counting the adverbs among the correlatives. (See special Esperanto adverbs.)
Other parts of speech occur as bare roots, without special suffixes. These are the prepositions "(al" "to"), conjunctions "(kaj" "and"), interjections "(ho" "oh"), numerals "(du" "two"), and pronouns "(mi" "I"—The final "-i" found on pronouns is not a suffix, but part of the root). There are also several grammatical "particles" which don't fit neatly into any category, and which must generally precede the words they modify, such as "ne" (not), "ankaŭ" (also), "nur" (only), "eĉ" (even).
The part-of-speech endings may be iterated. With the "-aŭ" suffix, this is nearly universal, and the "-aŭ" is rarely dropped: "anstataŭ" 'instead of', "anstataŭe" 'instead', "anstantaŭa" 'substitute', "anstataŭo" 'a substitute', "anstataŭi" 'to replace', etc. (Rarely "anstate, anstata, anstato, anstati.") In the case of prepositions and particles, there is nothing to drop: "nea" 'negative', "nei" 'to deny'. However, occasionally other endings double up. For example, "vivu!" "viva!" (the volitive of "vivi" 'to live') has a nominal form "vivuo" (a cry of 'viva!') and a doubly verbal form "vivui" (to cry 'viva!').
Nouns and adjectives.
A suffix "-j" following the noun or adjective suffixes "-o" or "-a" makes a word plural. Without this suffix, a countable noun is understood to be singular. Direct objects take an accusative case suffix "-n," which goes after any plural suffix. (The resulting sequence "-ojn" rhymes with English "coin," and "-ajn" rhymes with "fine.)"
Adjectives agree with nouns. That is, they are plural if the nouns they modify are plural, and accusative if the nouns they modify are accusative. Compare "bona tago; bonaj tagoj; bonan tagon; bonajn tagojn" (good day/days). This requirement allows for free word orders of adjective-noun and noun-adjective, even when two noun phrases are adjacent in subject–object–verb or verb–subject–object clauses:
Agreement clarifies the syntax in other ways as well. Adjectives take the plural suffix when they modify more than one noun, even if those nouns are all singular:
A predicative adjective does not take the accusative case suffix even when the noun it modifies does:
Pronouns.
There are three types of pronouns in Esperanto: personal "(vi" "you"), demonstrative "(tio" "that", "iu" "someone"), and relative/interrogative "(kio" "what").
Personal pronouns.
The Esperanto personal pronoun system is similar to that of English, but with the addition of a reflexive pronoun. 
Personal pronouns take the accusative suffix "-n" as nouns do: "min" (me), "lin" (him), "ŝin" (her). Possessive adjectives are formed with the adjectival suffix "-a: mia" (my), "ĝia" (its), "nia" (our). These agree with their noun like any other adjective: "ni salutis liajn amikojn" (we greeted his friends). Esperanto does not have separate forms for the possessive pronouns; this sense is generally (though not always) indicated with the definite article: "la mia" (mine).
The reflexive pronoun is used, in non-subject phrases only, to refer to back to the subject, usually only in the third and indefinite persons:
The indefinite pronoun is used when making general statements, and is often used where English would have the subject "it" with a passive verb, 
With impersonal verbs such as verbs of weather, however, no pronoun is used:
Zamenhof created an informal second-person singular pronoun "ci" (thou), and capitalized the formal singular pronoun "Vi," following usage in most European languages, but these forms are rarely seen today. 
"Ĝi" is used principally with animals and objects. Zamenhof also prescribed it to be the (gender-neutral) third-person singular pronoun, for use when the gender of an individual is unknown, or to refer to an epicene noun such as "persono" (person). However, this use is generally only found with children:
When speaking of adults or people in general, it is much more common for the demonstrative adjective and pronoun "tiu" (that one) to be used in such situations. However, this remedy is not always available. For example, the sentence,
the pronoun "tiu" is understood to refer only to someone other than the person speaking, and so cannot be used in place of "li" or "ŝi."
Other pronouns.
The demonstrative and relative pronouns form part of the correlative system, and are described in that article. The pronouns are the forms ending in "-o" (simple pronouns) and "-u" (adjectival pronouns). Their accusative case is formed in "-n," but the genitive case ends in "-es," which is the same for singular and plural and does not take accusative marking. Compare the nominative phases "lia domo" (his house) and "ties domo" (that one's house, those ones' house) with the plural "liaj domoj (his houses) and "ties domoj (that one's houses, those ones' houses), and with the accusative genitive "lian domon" and "ties domon."
Prepositions.
Although Esperanto word order is fairly free, prepositions must come at the beginning of a noun phrase. Whereas in languages such as German, prepositions may require a noun to be in various cases (accusative, dative, "etc.)," in Esperanto all prepositions govern the nominative: "por Johano" (for John). The only exception is when there are two or more prepositions and one is "replaced" by the accusative. 
Prepositions should be used with a definite meaning. When no one preposition is clearly correct, the indefinite preposition "je" should be used:
Alternatively, the accusative may be used without a preposition:
Note that although "la trian" (the third) is in the accusative, "de majo" (of May) is still a prepositional phrase, and so the noun "majo" remains in the nominative case.
A frequent use of the accusative is in place of "al" (to) to indicate the direction or goal of motion (allative construction). It is especially common when there would otherwise be a double preposition:
The accusative/allative may stand in for other prepositions as well, especially when they have vague meanings that don't add much to the clause. Adverbs, with or without the case suffix, are frequently used in place of prepositional phrases:
Both "por" and "pro" often translate English 'for'. However, they distinguish "for a goal" (looking forward in time, or causing: "por") and "for a cause" (looking back in time, or being caused by: "pro"): To vote "por" your friend means to cast a ballot with their name on it, whereas to vote "pro" your friend would mean to vote in their place or as they asked you to. 
The preposition most distinct from English usage is perhaps "de", which corresponds to English "of, from, off," and "(done) by":
However, English "of" corresponds to several Esperanto prepositions as well: "de, el" (out of, made of), and "da" (quantity of, unity of form and contents): 
The last of these, "da", is semantically Slavic and is difficult for Western Europeans, to the extent that even many Esperanto dictionaries and grammars define it incorrectly.
Occasionally a new preposition is coined. Because a bare root may indicate a preposition or interjection, removing the grammatical suffix from another part of speech can be used to derive a preposition or interjection. For example, from "fari" (to do, to make) we get the preposition "far" (done by).
Verbs.
All verbal inflection is regular. Three tenses together form what is called the indicative mood. The other moods are the infinitive, conditional, and jussive. No aspectual distinctions are required by the grammar, but derivational expressions of Aktionsart are common. 
Verbs do not change form according to their subject. "I am, we are," and "he is" are simply "mi estas, ni estas," and "li estas," respectively. Impersonal subjects are not used: "pluvas" (it is raining); "estas muso en la domo" (there's a mouse in the house).
Most verbs are inherently transitive or intransitive. As with the inherent part of speech of a root, this is not apparent from the shape of the verb and must simply be memorized. Transitivity is changed with the suffixes "-igi" (the transitivizer/causative) and "-iĝi" (the intransitivizer/middle voice): 
The verbal paradigm.
The tenses have characteristic vowels. "A" indicates the present tense, "i" the past, and "o" the future. 
The verbal forms may be illustrated with the root "esper-" (hope):
A verb can be made emphatic with the particle "ja" (indeed): "mi ja esperas" (I do hope), "mi ja esperis" (I did hope).
Tense.
As in English, Esperanto present tense may be used for generic statements such as "birds fly" ("la birdoj flugas"). 
The Esperanto future is a true tense, used whenever future time is meant. For example, in English "when I see you" the verb is in the present tense despite the time being in the future; in Esperanto, future tense is required: "kiam mi vidos vin".
Esperanto tense is relative. This differs from English absolute tense, where the tense is past, present, or future of the moment of speaking: In Esperanto, the tense of a subordinate verb is instead anterior or posterior to the time of the main verb. For example, "John said that he would go" is in Esperanto "Johano diris, ke li iros" (lit., "John said that he will go"); this does not mean that he will go at some point in the future from now (as "John said that he will go" means in English), but that at the time he said this his going was still in the future.
Mood.
The conditional mood is used for such expressions as "se mi povus, mi irus" (if I could, I would go) and "se mi estus vi, mi irus" (if I were you, I'd go). 
The jussive mood, called the "volitive" in Esperanto, is used for wishing and requesting, and serves as the imperative. It covers some of the uses of the subjunctive in European languages:
Aspect.
Verbal aspect is not grammatically required in Esperanto. However, aspectual distinctions may be expressed via participles (see below), and the Slavic aspectual system survives in two aktionsart affixes, perfective (often inceptive) "ek-" and imperfective "-adi." Compare,
and,
Various prepositions may also be used as aktionsart prefixes, such as "el" (out of), used to indicate that an action is performed to completion or at least to a considerable degree, also as in Slavic languages. In,
the verb "el-lern-is" is past tense "(-is)," and performed to significant completion "(el-)."
The copula.
The verb "esti" (to be) is both the copula and the existential ("there is") verb. As a copula linking two noun phrases, it does not cause either to take the accusative case. Therefore, unlike the situation with other verbs, word order with "esti" can be semantically important: compare "hundoj estas personoj" (dogs are people) and "personoj estas hundoj" (people are dogs). 
It is becoming increasingly common to replace "esti"-plus-adjective with a verb: "la ĉielo estas blua" or "la ĉielo bluas" (the sky is blue). This is a stylistic rather than grammatical change in the language, as the more economical verbal forms were always found in poetry.
Participles.
Participles are verbal derivatives. In Esperanto, there are half a dozen forms, which retain the vowel of the related verbal tense. In addition to carrying aspect, participles are the principal means of conveying voice, with two paradigms, active (performing an action) and passive (receiving an action).
Adjectival participles.
The basic principle of the participles may be illustrated with the verb "fali" (to fall). Picture Wile E. Coyote running off a cliff. Before gravity kicks in (after all, this is a cartoon), he is "falonta" (about to fall). As he drops, he is "falanta" (falling). After he impacts the desert floor, he is "falinta" (fallen). 
Active and passive pairs can be illustrated with the transitive verb "haki" (to chop). Picture a woodsman approaching a tree with an axe, intending to chop it down. He is "hakonta" (about to chop) and the tree is "hakota" (about to be chopped). While swinging the axe, he is "hakanta" (chopping) and the tree "hakata" (being chopped). After the tree has fallen, he is "hakinta" (having chopped) and the tree "hakita" (chopped).
Adjectival participles agree with nouns in number and case, just as other adjectives do:
Compound tense.
Compound tenses are formed with the adjectival participles plus "esti" (to be) as the auxiliary verb. The participle reflects aspect and voice, while the verb carries tense: 
These are not used as often as their English equivalents. For "I "am" go"ing" to the store", you would normally use the simple present "mi iras" in Esperanto.
The tense and mood of "esti" can be changed in these compound tenses:
Although such periphrastic constructions are familiar to speakers of most European languages, the option of contracting ["esti" + adjective] into a verb is often seen for adjectival participles: 
The most common of these synthetic forms are:
Infinitive and jussive forms are also found. There is a parallel passive paradigm.
Nominal participles.
Participles may be turned into adverbs or nouns by replacing the adjectival suffix "-a" with "-e" or "-o." This means that, in Esperanto, some nouns may be inflected for tense. 
A nominal participle indicates "one who participates" in the action specified by the verbal root. For example, "esperinto" is a "hoper" (past tense), or "one who had been hoping."
Adverbial participles.
Adverbial participles are used with subjectless clauses:
Conditional and tenseless participles (unofficial).
Occasionally, the participle paradigm will be extended to include conditional participles, with the vowel "u (-unt-, -ut-)." If, for example, in our tree-chopping example, the woodsman found that the tree had been spiked and so couldn't be cut down after all, he would be "hakunta" and the tree "hakuta" (he, the one "who would chop", and the tree, the one that "would be chopped"). 
This can also be illustrated with the verb "prezidi" (to preside). Just after the recount of the 2000 United States presidential election:
The conditional forms are infrequent, but their regular derivation ensures that they can be readily understood, even if rarely needed. No European language has conditional participles; in English, words like "prezidunto" must be expressed periphrastically, as in the title of Kipling's "The Man Who Would Be King."
The tense-neutral word "prezidento" is officially a separate root, not a derivative of the verb "prezidi." The element "-ento" frequently occurs in words for occupations where one would not wish to specify tense, such as "prezidento" or "studento" (student). Because there is often a verb derived from the same Latin root, in these cases "prezidi" (to preside) and "studi" (to study), this "-ento" has occasionally been proposed as a tense-neutral active participle by analogy with the temporal participles "-anto, -into, -onto." However, there is no analogous passive *"-eto", because that already exists as the diminutive suffix. The nearest equivalent is the middle voice suffix "-iĝi," which is commonly used as a generic passive. Unlike the active case, where a few new nouns like "prezidento" were sufficient to avoid making the language overly specific, a need for a neutral passive participle was felt early on. For example, there was heated debate for several decades as to whether "I was born in 19xx" should be "mi estis naskita" (I had been born) or "mi estis naskata" (literally 'I was being born'), with the French and Germans generally holding opposite opinions deriving from usage in their native languages. Today, people sidestep the issue with the temporally neutral "mi naskiĝis" (I was born).
Negatives.
A statement is made negative by using "ne" or one of the negative "(neni-)" correlatives. Ordinarily, only one negative word is allowed per clause:
Two negatives within a clause cancel each other out, with the result being a positive sentence.
The word "ne" comes before the word it negates, with the default position being before the verb:
The latter will frequently be reordered as "ne tion mi skribis" depending on the flow of information.
Questions.
"Wh" questions are asked with one of the interrogative/relative "(ki-") correlatives. They are commonly placed at the beginning of the sentence, but different word orders are allowed for stress:
Yes/no questions are marked with the conjunction "ĉu" (whether): 
Such questions can be answered "jes" (yes) or "ne" (no) in the European fashion of aligning with the polarity of the answer, or "ĝuste" (correct) or "malĝuste" (incorrect) in the Japanese fashion of aligning with the polarity of the question: 
Note that Esperanto questions may have the same word order as statements.
Conjunctions.
Basic Esperanto conjunctions are "kaj" (both/and), "aŭ" (either/or), "nek" (neither/nor), "se" (if), "ĉu" (whether/or), "sed" (but), "anstataŭ" (instead of), "krom" (besides, in addition to), "kiel" (like, as), "ke" (that). Like prepositions, they precede the phrase or clause they modify: 
However, unlike prepositions, they allow the accusative case, as in the following example from Don Harlow: 
Interjections.
Interjections may be derived from bare affixes or roots: "ek!" (get going!), from the perfective prefix; "um" (um, er), from the indefinite/undefined suffix; "fek!" (shit!), from "feki" (to defecate).
Word formation.
Esperanto derivational morphology uses a large number of lexical and grammatical affixes (prefixes and suffixes). These, along with compounding, decrease the memory load of the language, as they allow for the expansion of a relatively small number of basic roots into a large vocabulary. For example, the Esperanto root "vid-" (see) regularly corresponds to several dozen English words: "see (saw, seen), sight, blind, vision, visual, visible, nonvisual, invisible, unsightly, glance, view, vista, panorama, observant" etc., though there are also separate Esperanto roots for a couple of these concepts.
Numbers.
Numerals.
The cardinal numerals are:
These are grammatically numerals, not nouns, and as such do not take the accusative case suffix. However, "unu" (and only "unu") is sometimes used adjectivally or demonstratively, meaning "a certain", and in such cases it may take the plural affix "-j," just as the demonstrative pronoun "tiu" does:
In such use "unu" is irregular in that it doesn't take the accusative affix "-n" in the singular, but does in the plural: 
but 
Additionally, when counting off, the final "u" of "unu" may be dropped, as if it were a part-of-speech suffix: 
Higher numbers.
At numbers beyond the thousands, the international roots "miliono" (million) and "miliardo" (milliard) are used. Beyond this there are two systems: A "billion" in most English-speaking countries is different from a "billion" in most other countries (109 "vs." 1012 respectively; that is, a thousand million "vs." a million million). The international root "biliono" is likewise ambiguous in Esperanto, and is deprecated for this reason. An unambiguous system based on adding the Esperanto suffix "-iliono" to numerals is generally used instead, sometimes supplemented by a second suffix "-iliardo:"
Note that these forms are grammatically nouns, not numerals, and therefore cannot modify a noun directly: "mil homojn" (a thousand people [accusative]) but "milionon da homoj" (a million people [accusative]). An unambiguous international system is also provided by the metric prefixes, and the nonce numerals "meg" (miliono) and "gig" (miliardo) are occasionally derived from them: "meg homojn" (a million people).
Compound numerals and derivatives.
Numerals are written together as one word when their values are multiplied, and separately when their values are added "(dudek" 20, "dek du" 12, "dudek du" 22). Ordinals are formed with the adjectival suffix "-a," quantities with the nominal suffix "-o," multiples with "-obl-," fractions with "-on-," collectives with "-op-," and repetitions with the root "-foj-." 
The particle "po" is used to mark distributive numbers, that is, the idea of distributing a certain number of items to each member of a group. Consequently the logogram @ is not used (except in email addresses, of course): 
Note that particle "po" forms a phrase with the numeral "tri" and is not a preposition for the noun phrase "tri pomojn," so it does not prevent a grammatical object from taking the accusative case.
Comparisons.
Comparisons are made with the adverbial correlatives "tiel ... kiel" (as ... as), the adverbial roots "pli" (more) and "plej" (most), the antonym prefix "mal-," and the preposition "ol" (than): 
Implied comparisons are made with "tre" (very) and "tro" (too [much]). 
Phrases like "The more people, the smaller the portions" and "All the better!" are translated using "ju" and "des" in place of "the": 
Word order.
Esperanto has a fairly flexible word order. However, word order does play a role in Esperanto grammar, even if a much lesser role than it does in English. For example, the negative particle "ne" generally comes before the element being negated; negating the verb has the effect of negating the entire clause (or rather, there is ambiguity between negating the verb alone and negating the clause): 
However, when the entire clause is negated, the "ne" may be left till last:
The last order reflects a typical topic–comment (or theme–rheme) order: Known information, the topic under discussion, is introduced first, and what one has to say about it follows. (I went not: As for my going, there was none.) For example, yet another order, "ne iris mi", would suggest that the possibility of not having gone was under discussion, and "mi" is given as an example of one who did not go. 
Compare: 
The noun phrase.
Within a noun phrase, either the order "adjective–noun" or "noun–adjective" may occur, though the former is somewhat more common. Less flexibility occurs with numerals and demonstratives, with "numeral–noun" and "demonstrative–noun" being the norm, as in English. 
The last (numeral after noun) is practically unheard of outside poetry. Demonstratives such as "tiu" are rather uncommon after a noun as well, used there primarily for emphasis ("plumo tiu" 'that pen'). Even possessive pronouns strongly favor initial position, though the opposite is well known from "Patro nia" 'Our Father' in the Paternoster. 
Adjective–noun order is much freer. With simple adjectives, adjective–noun order predominates, especially if the noun is long or complex. However, a long or complex adjective typically comes after the noun, in some cases parallel to structures in English, as in the second example below:
Adjectives also normally occur after correlative nouns. Again, this is one of the situations where adjectives come after nouns in English:
Changing the word order here can change the meaning, at least with the correlative "nenio" 'nothing':
With multiple words in a phrase, the order is typically demonstrative/pronoun–numeral–(adjective/noun): 
However, the article "la" comes almost exclusively at the front of the noun phrase except, rarely, in poetry: 
Because of adjectival agreement, an adjective may be separated from the rest of the noun phrase without confusion. For example, in 
the subject and verb, "vi havos", interrupt the noun phrase "brilan sukceson". However, though occasionally found in poetry, such constructions are generally foreign to the language. 
In prepositional phrases, the preposition is "required" to come at the front of the noun phrase (that is, even before the article "la"), though it is commonly replaced by turning the noun into an adverb:
Constituent order.
Constituent order "within" a clause is generally free, apart from copular clauses. 
The default order is subject–verb–object, though any order may occur, with subject and object distinguished by case, and other constituents distinguished by prepositions:
The expectation of a topic–comment (theme–rheme) order apply here, so the context will influence word order: in "la katon ĉasis la hundo", the cat is the topic of the conversation, and the dog is the news; in "la hundo la katon ĉasis", the dog is the topic of the conversation, and it is the action of chasing that is the news; and in "ĉasis la hundo la katon", the action of chasing is already the topic of discussion.
Context is required to tell whether 
means the dog chased a cat which was in the garden, or there, in the garden, the dog chased the cat. These may be disambiguated with 
and 
Of course, if it chases the cat in"to" the garden, the case of 'garden' would change: 
Within copulative clauses, however, there are restrictions. Copulas are words such as "esti" 'be', "iĝi" 'become', "resti" 'remain', and "ŝajni" 'seem', for which neither noun phrase takes the accusative case. In such cases only two orders are generally found: noun-copula-predicate and, much less commonly, predicate-copula-noun. 
(That is, the copula intervenes between the two noun phrases, unless context or punctuation/intonation disambiguate:
Generally, if a characteristic of the noun is being described, the choice between the two orders is not important: 
However, "la vento sovaĝa estas" is unclear, at least in writing, as it could be interpreted as 'the wild wind is', leaving the reader to ask, 'is what?'. 
With two nouns, complications can arise. Sometimes context makes clear; demonstratives and articles, for example, usually occur only in the subject:
However, as noted above, there is a huge difference between saying generically 'dogs are people' and 'people are dogs'. In such cases the first noun is read as the subject, as in English. Similarly, 
is the opposite sentiment of
Attributive phrases and clauses.
In the sentence above, "la hundo ĉasis la katon, kiu estis en la ĝardeno" 'the dog chased the cat, which was in the garden', the relative pronoun "kiu" 'which' is restricted to a position "after" the noun 'cat'. In general, relative clauses and attributive prepositional phrases follow the noun they modify.
Attributive prepositional phrases, which are dependent on nouns, include genitives ("la libro de Johano" 'John's book') as well as "la kato en la ĝardeno" 'the cat in the garden' in the example above. Their order cannot be reversed: neither "*la de Johano libro" nor "*la en la ĝardeno kato" is possible. This behavior is more restrictive than prepositional phrases which are dependent on verbs, and which can be moved around: both "ĉasis en la ĝardeno" and "en la ĝardeno ĉasis" are acceptable for 'chased in the garden'. 
Relative clauses are similar, in that they are attributive and are subject to the same word-order constraint, except that rather than being linked by a preposition, the two elements are linked by a relative pronoun such as "kiu" 'which': 
Note that the noun and its adjacent relative pronoun do not agree in case. Rather, their cases depend on their relationships with their respective verbs. However, they do agree in number:
Other word orders are possible, as long as the relative pronoun remains adjacent to the noun it depends on:
Because of this word-order constraint, Esperanto is restricted to an SVO order in relative clauses when the linking noun is the subject of the verb, but to an OVS order when it is the object. Compare
and
In the latter, English requires passive verb constructions, but this is not necessary in Esperanto.
Clause order.
Coordinate clauses allow flexible word order, but tend to be iconic. For example, in
the inference is that the cat fled after the dog started to chase it, not that the dog chased a cat which was already fleeing. For the latter reading, the clause order would be reversed:
This distinction is lost in subordinate clauses such as the relative clauses in the previous section: 
In written English, a comma disambiguates the two readings, but both require a comma in Esperanto. 
Non-relative subordinate clauses are similarly restricted. They follow the conjunction "ke" 'that', as in,
Non-Indo-European aspects.
There is very little about Esperanto that is not Indo-European in origin. Although it is billed as a neutral international language, its vocabulary, syntax, and semantics derive predominantly from Indo-European national languages. Roots are typically Romance or Germanic in origin. The semantics shows a significant Slavic influence. 
It is often claimed that there are elements of the grammar which are not found in these language families. Frequently mentioned is Esperanto's agglutinative morphology based on invariant morphemes, and the subsequent lack of ablaut (internal inflection of its roots), which Zamenhof himself thought would prove alien to Indo-European language speakers. Ablaut is an element of all the source languages; an English example is "song sing sang sung." However, the majority of words in all Indo-European languages inflect without ablaut, as "cat, cats" and "walk, walked" do in English. (This is the so-called strong–weak dichotomy.) Historically, many Indo-European languages have expanded the range of their 'weak' inflections, and Esperanto has merely taken this development closer to its logical conclusion, with the only remaining ablaut being frozen in a few sets of semantically related roots such as "pli, plej, plu" (more, most, further), "tre, tro" (very, too much), and in the verbal morphemes "-as, -anta, -ata; -is, -inta, -ita; -os, -onta, -ota;" and "-us." 
Other features often cited as being nonstandard for an Indo-European language, such as the dedicated suffixes for different parts of speech, or the "-o" suffix for singular nouns, actually do occur in Indo-European languages such as Russian. More pertinent is the accusative plural in "-jn," which is derived through leveling of standard Indo-European grammatical structures. The Esperanto nominal–adjectival paradigm as a whole is taken from Greek: Esperanto nominative singular "muzo (muse) vs. Greek "mousa, nominative plural "muzoj vs. Greek "mousai," and accusative singular "muzon vs. Greek "mousan." (Latin and Lithuanian had very similar setups, with [j] in the plural and a nasal in the accusative.) However, Esperanto does not have a discrete accusative plural suffix analogous with Greek "mous-ās;" rather, it compounds the simple accusative and plural suffixes: "muz-o-j-n." This morphology does not occur as more than a marginal element in any of Esperanto's source language families, and is "formally" similar to European but not Indo-European Hungarian and Turkish—that is, it is similar in its mechanics, but not in use. None of these proposed "non-European" elements of the original Esperanto proposal were actually taken from non-European or non-Indo-European languages, and any similarities with those languages are accidental.
East Asian languages may have had some influence on the development of Esperanto grammar after its creation. The principally cited candidate is the replacement of predicate adjectives with verbs, such as "la ĉielo bluas" (the sky is blue) for "la ĉielo estas blua" and "mia filino belu!" (may my daughter be beautiful!) for the "mia filino estu bela!" mentioned above. This is a regularization of existing grammatical forms and was always found in poetry; if there has been an influence of an East Asian language, it has only been in the spread of such forms, not in their origin. Such usage is not entirely unknown in Europe: Latin has an analogous "folium viret" for "folium viride est" (the leaf is green) and "avis rubet" for "avis rubra est" (the bird is red).
Perhaps the best candidate for a "non-Indo-European" feature is the blurred distinction between roots and affixes. Esperanto derivational affixes may be used as independent root words and inflect for part of speech like other roots. This occurs only sporadically in other languages of the world, Indo-European or not. For example, "ismo" has an English equivalent in "an ism", but English has no adjectival form ("ismic"?) equivalent to Esperanto "isma." For most such affixes, natural languages familiar to Europeans must use a separate root, such as English "member" for Esperanto "ano," "quality" for "eco," "tendency" for "emo," etc.
Sample text.
The Pater noster, from the first Esperanto publication in 1887, illustrates many of the grammatical points presented above, and should be readable to those familiar with it without translation:
The morphologically complex words (see Esperanto word formation) are:
External links.
A fairly good overview of Esperanto's grammar and word-building system can be gained by viewing:

</doc>
<doc id="10404" url="http://en.wikipedia.org/wiki?curid=10404" title="Esperanto culture">
Esperanto culture

The language Esperanto is often used to access an international culture, including a large body of original as well as translated literature. There are over 25,000 Esperanto books (originals and translations) as well as over a hundred regularly distributed Esperanto magazines. Many Esperanto speakers use the language for free travel throughout the world using the Pasporta Servo. Others like the idea of having pen pals in many countries around the world using services like the Esperanto Pen Pal Service. Every year, Esperanto speakers meet for the World Congress of Esperanto (Esperanto: "Universala Kongreso de Esperanto"). These attract around 1500–3000 speakers, and the best-attended conferences are regularly those held in Central or Eastern Europe, close to the birthplace of Esperanto (see statistics at World Congress of Esperanto).
Literature, music and film.
Every year, hundreds of new titles are published in Esperanto along with music. Also, many Esperanto newspapers and magazines exist. 
"Monato" is a general news magazine "like a genuinely international "Time" or "Newsweek"", but written by local correspondents. A magazine for the blind, "Aŭroro", has been published since 1920. 
Esperanto can be heard in television and radio broadcasts and on the internet. There are currently radio broadcasts from China Radio International, Melbourne Ethnic Community Radio, Radio Habana Cuba, Radio Audizioni Italiane (Rai), Radio Polonia, Radio F.R.E.I. and Radio Vatican. Internacia Televido, an internet television channel, began broadcasting in November 2005.
Historically most of the music published in Esperanto has been in various folk traditions; in recent decades more rock and other modern genres has appeared.
In 1964, Jacques-Louis Mahé produced the first full-length feature film in Esperanto, entitled "Angoroj". This was followed in 1965 by the first American Esperanto-production: "Incubus", starring William Shatner. Several shorter films have been produced since. As of July 2003[ [update]], the Esperanto-language Wikipedia 14 films and 3 short films.
In 2011, Academy Award-nominated director Sam Green ("The Weather Underground"), released a new documentary about Esperanto titled (La Universala Lingvo.) This 30-minute film traces the history of Esperanto.
Cultural community.
There are cultural commonalities between Esperanto speakers, which is a distinctive feature of a cultural community. Esperanto was created to foster universal understanding, solidarity and peace. A large proportion of the Esperanto movement continue to hold such goals, and most are at least sympathetic to them. Additionally, many Esperantists use the language as a window to the larger world, to meet people from other countries on an equal footing, and for travel. The Esperanto-community has a certain set of shared background knowledge. 
To some extent there are also shared traditions, like the Zamenhof Day, and shared behaviour patterns, like avoiding the usage of one's national language at Esperanto meetings unless there are good reasons for its use (Esperanto culture has a special word, "krokodili" ("to crocodile"), to describe this avoided behaviour). On the other hand, it has been said that some aspects of shared traditions normally found in cultural communities, like clothing and cooking, aren't found in the Esperanto community. Not everyone would agree. Like other people, Esperanto speakers who are interested in cookery exchange and share recipes, both their own creations and their national and regional dishes. Various collections of such recipes have been published in book form in Esperanto, e.g. "Internacie kuiri" (“Cooking Internationally”) by Maria Becker-Meisberger, published by FEL (Flemish Esperanto League), Antwerp 1989, ISBN 90-71205-34-7, "Manĝoj el sanigaj plantoj" (“Healthy Vegetable Dishes”) by Zlata Nanić, published by BIO-ZRNO, Zagreb 2002, ISBN 953-97664-5-1. Sometimes at Esperanto gatherings, such original dishes as those devised by Zlatka Nanić can be sampled. Some Esperanto periodicals, such as "MONATO" include cookery items from time to time. As regards clothing, at every "Universala Kongreso", held every year in a different country, many of those attending can been seen wearing their national or regional costumes.
On December 15 (L. L. Zamenhof's birthday), Esperanto speakers around the world celebrate Zamenhof Day, sometimes relabelled Esperanto Book Day.
The poem "La Espero" is generally considered to be the Esperanto anthem. It speaks of the achievement of world peace, "sacred harmony" and "eternal blessing" on the basis of a neutral language. Nonetheless Esperanto speakers may or may not agree whether the stated benefits could in fact be achieved in this way. At the first Esperanto congress, in Boulogne-sur-Mer in 1905, a declaration was made which defined an "Esperantist" merely as one who knows and uses the language "regardless of what kind of aims he uses it for", and which also specifically declared any ideal beyond the spread of the language itself to be a private matter for the individual speaker.

</doc>
<doc id="10406" url="http://en.wikipedia.org/wiki?curid=10406" title="Emotion">
Emotion

Emotion is, in everyday speech, a person's state of mind and instinctive responses, but scientific discourse has drifted to other meanings and there is no consensus on a definition. Emotion is often intertwined with mood, temperament, personality, disposition, and motivation. On some theories, cognition is an important aspect of emotion. Those acting primarily on emotion may seem as if they are not thinking, but mental processes are still essential, particularly in the interpretation of events. For example, the realization of danger and subsequent arousal of the nervous system (e.g. rapid heartbeat and breathing, sweating, muscle tension) is integral to the experience of fear. Other theories, however, claim that emotion is separate from and can precede cognition.
Emotions are complex. According to some theories, they are a state of feeling that results in physical and psychological changes that influence our behavior. The physiology of emotion is closely linked to arousal of the nervous system with various states and strengths of arousal relating, apparently, to particular emotions. Emotion is also linked to behavioral tendency. Extroverted people are more likely to be social and express their emotions, while introverted people are more likely to be more socially withdrawn and conceal their emotions. Emotion is often the driving force behind motivation, positive or negative. An alternative definition of emotion is a "positive or negative experience that is associated with a particular pattern of physiological activity." According to other theories, emotions are not causal forces but simply syndromes of components, which might include motivation, feeling, behavior, and physiological changes, but no one of these components is the emotion. Nor is the emotion an entity that causes these components
Emotions involve different components, such as subjective experience, cognitive processes, expressive behavior, psychophysiological changes, and instrumental behavior. At one time, academics attempted to identify the emotion with one of the components: William James with a subjective experience, behaviorists with instrumental behavior, psychophysiologists with physiological changes, and so on. More recently, emotion is said to consist of all the components. The different components of emotion are categorized somewhat differently depending on the academic discipline. In psychology and philosophy, emotion typically includes a subjective, conscious experience characterized primarily by psychophysiological expressions, biological reactions, and mental states. A similar multicomponential description of emotion is found in sociology. For example, Peggy Thoits described emotions as involving physiological components, cultural or emotional labels (e.g., anger, surprise etc.), expressive body actions, and the appraisal of situations and contexts.
Research on emotion has increased significantly over the past two decades with many fields contributing including psychology, neuroscience, endocrinology, medicine, history, sociology, and even computer science. The numerous theories that attempt to explain the origin, neurobiology, experience, and function of emotions have only fostered more intense research on this topic. Current areas of research in the concept of emotion include the development of materials that stimulate and elicit emotion. In addition PET scans and fMRI scans help study the affective processes in the brain. It also is influenced by hormones and neurotransmitters such as dopamine, noradrenaline, serotonin, oxytocin, cortisol and GABA.
Etymology, definitions, and differentiation.
The word "emotion" dates back to 1579, when it was adapted from the French word "émouvoir", which means "to stir up". The term emotion was introduced into academic discussion to replace passion. One dictionary said that the earliest precursors of the word likely dates back to the very origins of language. The modern word emotion is heterogeneous In some uses of the word, emotions are intense feelings that are directed at someone or something. On the other hand, emotion can be used to refer to states that are mild (as in annoyed or content) and to states that are not directed at anything (as in anxiety and depression). One line of research thus looks at the meaning of the word emotion in everyday language and this usage is rather different from that in academic dscourse. Another line of research asks about languages other than English, and one interesting finding is that many languages have a similar but not identical term 
Emotions have been described by some theorists as discrete and consistent responses to internal or external events which have a particular significance for the organism. Emotions are brief in duration and consist of a coordinated set of responses, which may include verbal, physiological, behavioural, and neural mechanisms. Psychotherapist Michael C. Graham describes all emotions as existing on a continuum of intensity. Thus fear might range from mild concern to terror or shame might range from simple embarrassment to toxic shame. Emotions have also been described as biologically given and a result of evolution because they provided good solutions to ancient and recurring problems that faced our ancestors. Moods are feelings that tend to be less intense than emotions and that often lack a contextual stimulus.
Emotion can be differentiated from a number of similar constructs within the field of affective neuroscience:
In addition, relationships exist between emotions, such as having positive or negative influences, with direct opposites existing. These concepts are described in contrasting and categorization of emotions. Graham differentiates emotions as functional or dysfunctional and argues all functional emotions have benefits.
Components.
In Scherer's components processing model of emotion, five crucial elements of emotion are said to exist. From the component processing perspective, emotion experience is said to require that all of these processes become coordinated and synchronized for a short period of time, driven by appraisal processes. Although the inclusion of cognitive appraisal as one of the elements is slightly controversial, since some theorists make the assumption that emotion and cognition are separate but interacting systems, the component processing model provides a sequence of events that effectively describes the coordination involved during an emotional episode.
Classification.
A distinction can be made between emotional episodes and emotional dispositions. Emotional dispositions are also comparable to character traits, where someone may be said to be generally disposed to experience certain emotions. For example, an irritable person is generally disposed to feel irritation more easily or quickly than others do. Finally, some theorists place emotions within a more general category of "affective states" where affective states can also include emotion-related phenomena such as pleasure and pain, motivational states (for example, hunger or curiosity), moods, dispositions and traits.
The classification of emotions has mainly been researched from two fundamental viewpoints. The first viewpoint is that emotions are discrete and fundamentally different constructs while the second viewpoint asserts that emotions can be characterized on a dimensional basis in groupings.
Basic emotions.
For more than 40 years, Paul Ekman has supported the view that emotions are discrete, measurable, and physiologically distinct. Ekman's most influential work revolved around the finding that certain emotions appeared to be universally recognized, even in cultures that were preliterate and could not have learned associations for facial expressions through media. Another classic study found that when participants contorted their facial muscles into distinct facial expressions (e.g. disgust), they reported subjective and physiological experiences that matched the distinct facial expressions. His research findings led him to classify six emotions as basic: anger, disgust, fear, happiness, sadness and surprise.
Robert Plutchik agreed with Ekman's biologically driven perspective but developed the "wheel of emotions", suggesting eight primary emotions grouped on a positive or negative basis: joy versus sadness; anger versus fear; trust versus distrust; and surprise versus anticipation. Some basic emotions can be modified to form complex emotions. The complex emotions could arise from cultural conditioning or association combined with the basic emotions. Alternatively, similar to the way primary colors combine, "primary emotions" could blend to form the full spectrum of human emotional experience. For example, interpersonal anger and disgust could blend to form contempt. Relationships exist between basic emotions, resulting in positive or negative influences.
Multi-dimensional analysis.
Through the use of multidimensional scaling, psychologists can map out similar emotional experiences, which allows a visual depiction of the "emotional distance" between experiences. A further step can be taken by looking at the map's dimensions of the emotional experiences. The emotional experiences are divided into two dimensions known as valence (how negative or positive the experience feels) and arousal (how energized or enervated the experience feels). These two dimensions can be depicted on a 2D coordinate map. This two-dimensional map was theorized to capture one important component of emotion called core affect. Core affect is not the only component to emotion, but gives the emotion its hedonic and felt energy.
The idea that core affect is but one component of the emotion led to a theory called “psychological construction.” According to this theory, an emotional episode consists of a set of components, each of which is an on-going process and none of which is necessary or sufficient for the emotion to be instantiated. The set of components is not fixed, either by human evolutionary history or by social norms and roles. Instead, the emotional episode is assembled at the moment of its occurrence to suit its specific circumstances. One implication is that all cases of, for example, fear are not identical but instead bear a family resemblance to one another.
Theories on the experience.
Ancient Greece and Middle Ages.
Theories about emotions stretch back to at least as far as the stoics of Ancient Greece and Ancient China. In the latter it was believed that excess emotion caused damage to "qi", which in turn, damages the vital organs. The four humours theory made popular by Hippocrates contributed to the study of emotion in the same way that it did for medicine.
Western philosophy regarded emotion in varying ways. In stoic theories it was seen as a hindrance to reason and therefore a hindrance to virtue. Aristotle believed that emotions were an essential component to virtue. In the Aristotelian view all emotions (called passions) corresponded to an appetite or capacity. During the Middle Ages, the Aristotelian view was adopted and further developed by scholasticism and Thomas Aquinas in particular. There are also theories in the works of philosophers such as René Descartes, Niccolò Machiavelli, Baruch Spinoza and David Hume. In the 19th century emotions were considered adaptive and were studied more frequently from an empiricist psychiatric perspective.
Evolutionary theories.
Perspectives on emotions from evolutionary theory were initiated in the late 19th century with Charles Darwin's book "The Expression of the Emotions in Man and Animals". Darwin argued that emotions actually served a purpose for humans, in communication and also in aiding their survival. Darwin, therefore, argued that emotions evolved via natural selection and therefore have universal cross-cultural counterparts. Darwin also detailed the virtues of experiencing emotions and the parallel experiences that occur in animals (see emotion in animals). This led the way for animal research on emotions and the eventual determination of the neural underpinnings of emotion.
More contemporary views along the evolutionary psychology spectrum posit that both basic emotions and social emotions evolved to motivate (social) behaviors that were adaptive in the ancestral environment. Current research suggests that emotion is an essential part of any human decision-making and planning, and the famous distinction made between reason and emotion is not as clear as it seems. Paul D. MacLean claims that emotion competes with even more instinctive responses, on one hand, and the more abstract reasoning, on the other hand. The increased potential in neuroimaging has also allowed investigation into evolutionarily ancient parts of the brain. Important neurological advances were derived from these perspectives in the 1990s by Joseph E. LeDoux and António Damásio.
Research on social emotion also focuses on the physical displays of emotion including body language of animals and humans (see affect display). For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared. Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.
Somatic theories.
Somatic theories of emotion claim that bodily responses, rather than cognitive interpretations, are essential to emotions. The first modern version of such theories came from William James in the 1880s. The theory lost favor in the 20th century, but has regained popularity more recently due largely to theorists such as John Cacioppo, António Damásio, Joseph E. LeDoux and Robert Zajonc who are able to appeal to neurological evidence.
James–Lange theory.
In his 1884 article William James argued that feelings and emotions were "secondary" to physiological phenomena. In his theory, James proposed that the perception of what he called an "exciting fact" directly led to a physiological response, known as "emotion." To account for different types of emotional experiences, James proposed that stimuli trigger activity in the autonomic nervous system, which in turn produces an emotional experience in the brain. The Danish psychologist Carl Lange also proposed a similar theory at around the same time, and therefore this theory became known as the James–Lange theory. As James wrote, "the perception of bodily changes, as they occur, "is" the emotion." James further claims that "we feel sad because we cry, angry because we strike, afraid because we tremble, and neither we cry, strike, nor tremble because we are sorry, angry, or fearful, as the case may be."
An example of this theory in action would be as follows: An emotion-evoking stimulus (snake) triggers a pattern of physiological response (increased heart rate, faster breathing, etc.), which is interpreted as a particular emotion (fear). This theory is supported by experiments in which by manipulating the bodily state induces a desired emotional state. Some people may believe that emotions give rise to emotion-specific actions: e.g. "I'm crying because I'm sad," or "I ran away because I was scared." The issue with the James–Lange theory is that of causation (bodily states causing emotions and being "a priori"), not that of the bodily influences on emotional experience (which can be argued and is still quite prevalent today in biofeedback studies and embodiment theory).
Although mostly abandoned in its original form, Tim Dalgleish argues that most contemporary neuroscientists have embraced the components of the James-Lange theory of emotions.
The James–Lange theory has remained influential. Its main contribution is the emphasis it places on the embodiment of emotions, especially the argument that changes in the bodily concomitants of emotions can alter their experienced intensity. Most contemporary neuroscientists would endorse a modified James–Lange view in which bodily feedback modulates the experience of emotion." (p. 583)
Cannon–Bard theory.
Walter Bradford Cannon agreed that physiological responses played a crucial role in emotions, but did not believe that physiological responses alone could explain subjective emotional experiences. He argued that physiological responses were too slow and often imperceptible and this could not account for the relatively rapid and intense subjective awareness of emotion. He also believed that the richness, variety, and temporal course of emotional experiences could not stem from physiological reactions, that reflected fairly undifferentiated fight or flight responses. An example of this theory in action is as follows: An emotion-evoking event (snake) triggers simultaneously both a physiological response and a conscious experience of an emotion.
Phillip Bard contributed to the theory with his work on animals. Bard found that sensory, motor, and physiological information all had to pass through the diencephalon (particularly the thalamus), before being subjected to any further processing. Therefore, Cannon also argued that it was not anatomically possible for sensory events to trigger a physiological response prior to triggering conscious awareness and emotional stimuli had to trigger both physiological and experiential aspects of emotion simultaneously.
Two-factor theory.
Stanley Schachter formulated his theory on the earlier work of a Spanish physician, Gregorio Maranon, who injected patients with epinephrine and subsequently asked them how they felt. Interestingly, Maranon found that most of these patients felt something but in the absence of an actual emotion-evoking stimulus, the patients were unable to interpret their physiological arousal as an experienced emotion. Schachter did agree that physiological reactions played a big role in emotions. He suggested that physiological reactions contributed to emotional experience by facilitating a focused cognitive appraisal of a given physiologically arousing event and that this appraisal was what defined the subjective emotional experience. Emotions were thus a result of two-stage process: general physiological arousal, and experience of emotion. For example, the physiological arousal, heart pounding, in a response to an evoking stimulus, the sight of a bear in the kitchen. The brain then quickly scans the area, to explain the pounding, and notices the bear. Consequently, the brain interprets the pounding heart as being the result of fearing the bear. With his student, Jerome Singer, Schachter demonstrated that subjects can have different emotional reactions despite being placed into the same physiological state with an injection of epinephrine. Subjects were observed to express either anger or amusement depending on whether another person in the situation (a confederate) displayed that emotion. Hence, the combination of the appraisal of the situation (cognitive) and the participants' reception of adrenaline or a placebo together determined the response. This experiment has been criticized in Jesse Prinz's (2004) "Gut Reactions".
Cognitive theories.
With the two-factor theory now incorporating cognition, several theories began to argue that cognitive activity in the form of judgments, evaluations, or thoughts were entirely necessary for an emotion to occur. One of the main proponents of this view was Richard Lazarus who argued that emotions must have some cognitive intentionality. The cognitive activity involved in the interpretation of an emotional context may be conscious or unconscious and may or may not take the form of conceptual processing.
Lazarus' theory is very influential; emotion is a disturbance that occurs in the following order:
For example: Jenny sees a snake.
Lazarus stressed that the quality and intensity of emotions are controlled through cognitive processes. These processes underline coping strategies that form the emotional reaction by altering the relationship between the person and the environment.
George Mandler provided an extensive theoretical and empirical discussion of emotion as influenced by cognition, consciousness, and the autonomic nervous system in two books (Mind and Emotion, 1975, and Mind and Body: Psychology of Emotion and Stress, 1984)
There are some theories on emotions arguing that cognitive activity in the form of judgements, evaluations, or thoughts are necessary in order for an emotion to occur.
A prominent philosophical exponent is Robert C. Solomon (for example, "The Passions, Emotions and the Meaning of Life", 1993). Solomon claims that emotions are judgements. He has put forward a more nuanced view which responds to what he has called the ‘standard objection’ to cognitivism, the idea that a judgement that something is fearsome can occur with or without emotion, so judgement cannot be identified with emotion.
The theory proposed by Nico Frijda where appraisal leads to action tendencies is another example.
It has also been suggested that emotions (affect heuristics, feelings and gut-feeling reactions) are often used as shortcuts to process information and influence behavior. The affect infusion model (AIM) is a theoretical model developed by Joseph Forgas in the early 1990s that attempts to explain how emotion and mood interact with one's ability to process information.
Theories dealing with perception either use one or multiples perceptions in order to find an emotion (Goldie, 2007).A recent hybrid of the somatic and cognitive theories of emotion is the perceptual theory. This theory is neo-Jamesian in arguing that bodily responses are central to emotions, yet it emphasizes the meaningfulness of emotions or the idea that emotions are about something, as is recognized by cognitive theories. The novel claim of this theory is that conceptually-based cognition is unnecessary for such meaning. Rather the bodily changes themselves "perceive" the meaningful content of the emotion because of being causally triggered by certain situations. In this respect, emotions are held to be analogous to faculties such as vision or touch, which provide information about the relation between the subject and the world in various ways. A sophisticated defense of this view is found in philosopher Jesse Prinz's book "Gut Reactions", and psychologist James Laird's book "Feelings".
This is a communication-based theory developed by Howard M. Weiss and Russell Cropanzano (1996), that looks at the causes, structures, and consequences of emotional experience (especially in work contexts). This theory suggests that emotions are influenced and caused by events which in turn influence attitudes and behaviors. This theoretical frame also emphasizes "time" in that human beings experience what they call emotion episodes— a "series of emotional states extended over time and organized around an underlying theme." This theory has been utilized by numerous researchers to better understand emotion from a communicative lens, and was reviewed further by Howard M. Weiss and Daniel J. Beal in their article, "Reflections on Affective Events Theory", published in "Research on Emotion in Organizations" in 2005.
Situated perspective on emotion.
A situated perspective on emotion, developed by Paul E. Griffiths and Andrea Scarantino , emphasizes the importance of external factors in the development and communication of emotion, drawing upon the situationism approach in psychology. This theory is markedly different from both cognitivist and neo-Jamesian theories of emotion, both of which see emotion as a purely internal process, with the environment only acting as a stimulus to the emotion. In contrast, a situationist perspective on emotion views emotion as the product of an organism investigating its environment, and observing the responses of other organisms. Emotion stimulates the evolution of social relationships, acting as a signal to mediate the behavior of other organisms. In some contexts, the expression of emotion (both voluntary and involuntary) could be seen as strategic moves in the transactions between different organisms. The situated perspective on emotion states that conceptual thought is not an inherent part of emotion, since emotion is an action-oriented form of skillful engagement with the world. Griffiths and Scarantino suggested that this perspective on emotion could be helpful in understanding phobias, as well as the emotions of infants and animals.
Genetics.
Emotions can motivate social interactions and relationships and therefore are directly related with basic physiology, particularly with the stress systems. This is important because emotions are related to the anti-stress complex, with an oxytocin-attachment system, which plays a major role in bonding. Emotional phenotype temperaments affect social connectedness and fitness in complex social systems (Kurt Kortschal 2013). These characteristics are shared with other species and taxa and are due to the effects of genes and their continuous transmission. Information that is encoded in the DNA sequences provides the blueprint for assembling proteins that make up our cells. Zygotes require genetic information from their parental germ cells, and at every speciation event, heritable traits that have enabled its ancestor to survive and reproduce successfully are passed down along with new traits that could be potentially beneficial to the offspring. 
In the five million years since the linages leading to modern humans and chimpanzees split, only about 1.2% of their genetic material has been modified. This suggests that everything that separates us from chimpanzees must be encoded in that very small amount of DNA, including our behaviors. Students that study animal behaviors have only identified intraspecific examples of gene-dependent behavioral phenotypes. In voles (Microtus spp.) minor genetic differences have been identified in a vasopressin receptor gene that corresponds to major species differences in social organization and the mating system (Hammock & Young 2005). 
Another potential example with behavioral differences is the FOCP2 gene, which is involved in neural circuitry handling speech and language (Vargha-Khadem et al. 2005). Its present form in humans differed from that of the chimpanzees by only a few mutations and has been present for about 200,000 years, coinciding with the beginning of modern humans (Enard et al. 2002). Speech, language, and social organization are all part of the basis for emotions.
Neurocircuitry.
Based on discoveries made through neural mapping of the limbic system, the neurobiological explanation of human emotion is that emotion is a pleasant or unpleasant mental state organized in the limbic system of the mammalian brain. If distinguished from reactive responses of reptiles, emotions would then be mammalian elaborations of general vertebrate arousal patterns, in which neurochemicals (for example, dopamine, noradrenaline, and serotonin) step-up or step-down the brain's activity level, as visible in body movements, gestures, and postures. Emotions can likely be mediated by pheromones (see fear).
For example, the emotion of love is proposed to be the expression of paleocircuits of the mammalian brain (specifically, modules of the cingulate gyrus) which facilitate the care, feeding, and grooming of offspring. Paleocircuits are neural platforms for bodily expression configured before the advent of cortical circuits for speech. They consist of pre-configured pathways or networks of nerve cells in the forebrain, brain stem and spinal cord.
The motor centers of reptiles react to sensory cues of vision, sound, touch, chemical, gravity, and motion with pre-set body movements and programmed postures. With the arrival of night-active mammals, smell replaced vision as the dominant sense, and a different way of responding arose from the olfactory sense, which is proposed to have developed into mammalian emotion and emotional memory. The mammalian brain invested heavily in olfaction to succeed at night as reptiles slept—one explanation for why olfactory lobes in mammalian brains are proportionally larger than in the reptiles. These odor pathways gradually formed the neural blueprint for what was later to become our limbic brain.
Emotions are thought to be related to certain activities in brain areas that direct our attention, motivate our behavior, and determine the significance of what is going on around us. Pioneering work by Broca (1878), Papez (1937), and MacLean (1952) suggested that emotion is related to a group of structures in the center of the brain called the limbic system, which includes the hypothalamus, cingulate cortex, hippocampi, and other structures. More recent research has shown that some of these limbic structures are not as directly related to emotion as others are, while some non-limbic structures have been found to be of greater emotional relevance.
In 2011, Lövheim proposed a direct relation between specific combinations of the levels of the signal substances dopamine, noradrenaline and serotonin and eight basic emotions. A model was presented where the signal substances form the axes of a coordinate system, and the eight basic emotions according to Silvan Tomkins are placed in the eight corners. Anger is, according to the model, for example produced by the combination of low serotonin, high dopamine and high noradrenaline.
Prefrontal cortex.
There is ample evidence that the left prefrontal cortex is activated by stimuli that cause positive approach. If attractive stimuli can selectively activate a region of the brain, then logically the converse should hold, that selective activation of that region of the brain should cause a stimulus to be judged more positively. This was demonstrated for moderately attractive visual stimuli and replicated and extended to include negative stimuli.
Two neurobiological models of emotion in the prefrontal cortex made opposing predictions. The Valence Model predicted that anger, a negative emotion, would activate the right prefrontal cortex. The Direction Model predicted that anger, an approach emotion, would activate the left prefrontal cortex. The second model was supported.
This still left open the question of whether the opposite of approach in the prefrontal cortex is better described as moving away (Direction Model), as unmoving but with strength and resistance (Movement Model), or as unmoving with passive yielding (Action Tendency Model). Support for the Action Tendency Model (passivity related to right prefrontal activity) comes from research on shyness and research on behavioral inhibition. Research that tested the competing hypotheses generated by all four models also supported the Action Tendency Model.
Homeostatic/primordial emotion.
Another neurological approach distinguishes two classes of emotion: "classical" emotions such as love, anger and fear that are evoked by environmental stimuli, and "primordial" or "homeostatic emotions" – attention-demanding feelings evoked by body states, such as pain, hunger and fatigue, that motivate behavior (withdrawal, eating or resting in these examples) aimed at maintaining the body's internal milieu at its ideal state.
Derek Denton defines the latter as "the subjective element of the instincts, which are the genetically programmed behaviour patterns which contrive homeostasis. They include thirst, hunger for air, hunger for food, pain and hunger for specific minerals etc. There are two constituents of a primordial emotion--the specific sensation which when severe may be imperious, and the compelling intention for gratification by a consummatory act."
Disciplinary approaches.
Many different disciplines have produced work on the emotions. Human sciences study the role of emotions in mental processes, disorders, and neural mechanisms. In psychiatry, emotions are examined as part of the discipline's study and treatment of mental disorders in humans. Nursing studies emotions as part of its approach to the provision of holistic health care to humans. Psychology examines emotions from a scientific perspective by treating them as mental processes and behavior and they explore the underlying physiological and neurological processes. In neuroscience sub-fields such as social neuroscience and affective neuroscience, scientists study the neural mechanisms of emotion by combining neuroscience with the psychological study of personality, emotion, and mood. In linguistics, the expression of emotion may change to the meaning of sounds. In education, the role of emotions in relation to learning is examined.
Social sciences often examine emotion for the role that it plays in human culture and social interactions. In sociology, emotions are examined for the role they play in human society, social patterns and interactions, and culture. In anthropology, the study of humanity, scholars use ethnography to undertake contextual analyses and cross-cultural comparisons of a range of human activities. Some anthropology studies examine the role of emotions in human activities. In the field of communication sciences, critical organizational scholars have examined the role of emotions in organizations, from the perspectives of managers, employees, and even customers. A focus on emotions in organizations can be credited to Arlie Russell Hochschild's concept of emotional labor. The University of Queensland hosts EmoNet, an e-mail distribution list representing a network of academics that facilitates scholarly discussion of all matters relating to the study of emotion in organizational settings. The list was established in January 1997 and has over 700 members from across the globe.
In economics, the social science that studies the production, distribution, and consumption of goods and services, emotions are analyzed in some sub-fields of microeconomics, in order to assess the role of emotions on purchase decision-making and risk perception. In criminology, a social science approach to the study of crime, scholars often draw on behavioral sciences, sociology, and psychology; emotions are examined in criminology issues such as anomie theory and studies of "toughness," aggressive behavior, and hooliganism. In law, which underpins civil obedience, politics, economics and society, evidence about people's emotions is often raised in tort law claims for compensation and in criminal law prosecutions against alleged lawbreakers (as evidence of the defendant's state of mind during trials, sentencing, and parole hearings). In political science, emotions are examined in a number of sub-fields, such as the analysis of voter decision-making.
In philosophy, emotions are studied in sub-fields such as ethics, the philosophy of art (for example, sensory–emotional values, and matters of taste and sentimentality), and the philosophy of music (see also Music and emotion). In history, scholars examine documents and other sources to interpret and analyze past activities; speculation on the emotional state of the authors of historical documents is one of the tools of interpretation. In literature and film-making, the expression of emotion is the cornerstone of genres such as drama, melodrama, and romance. In communication studies, scholars study the role that emotion plays in the dissemination of ideas and messages. Emotion is also studied in non-human animals in ethology, a branch of zoology which focuses on the scientific study of animal behavior. Ethology is a combination of laboratory and field science, with strong ties to ecology and evolution. Ethologists often study one type of behavior (for example, aggression) in a number of unrelated animals.
History.
The history of emotions has become an increasingly popular topic recently, with some scholars arguing that it is an essential category of analysis, not unlike class, race, or gender. Historians, like other social scientists, assume that emotions, feelings and their expressions are regulated in different ways by both different cultures and different historical times, and constructivist school of history claims even that some sentiments and meta-emotions, for example Schadenfreude, are learnt and not only regulated by culture. Historians of emotion trace and analyse the changing norms and rules of feeling, while examining emotional regimes, codes, and lexicons from social, cultural or political history perspectives. Others focus on the history of medicine, science or psychology. What somebody can and may feel (and show) in a given situation, towards certain people or things, depends on social norms and rules. It is thus historically variable and open to change. Several research centers have opened in the past few years in Germany, England, Spain, Sweden and Australia.
Furthermore, research in historical trauma suggests that some traumatic emotions can be passed on from parents to offspring to second and even third generation, presented as examples of transgenerational trauma.
Sociology.
A common way in which emotions are conceptualized in sociology is in terms of the multidimensional characteristics including cultural or emotional labels (e.g., anger, pride, fear, happiness), physiological changes (e.g., increased perspiration, changes in pulse rate), expressive facial and body movements (e.g., smiling, frowning, baring teeth), and appraisals of situational cues. One comprehensive theory of emotional arousal in humans has been developed by Jonathan Turner (2007: 2009). Two of the key eliciting factors for the arousal of emotions within this theory are expectations states and sanctions. When people enter a situation or encounter with certain expectations for how the encounter should unfold, they will experience different emotions depending on the extent to which expectations for Self, other and situation are met or not met. People can also provide positive or negative sanctions directed at Self or other which also trigger different emotional experiences in individuals. Turner analyzed a wide range of emotion theories across different fields of research including sociology, psychology, evolutionary science, and neuroscience. Based on this analysis, he identified four emotion that all researchers consider to founded on human neurology including assertive-anger, aversion-fear, satisfaction-happiness, and disappointment-sadness. These four categories are called primary emotions and there is some agreement amongst researchers that these primary emotions become combined to produce more elaborate and complex emotional experiences. These more elaborate emotions are called first-order elaborations in Turner's theory and they include sentiments such as pride, triumph, and awe. Emotions can also be experienced at different levels of intensity so that feelings of concern are a low-intensity variation of the primary emotion aversion-fear whereas depression is a higher intensity variant.
Attempts are frequently made to regulate emotion according to the conventions of the society and the situation based on many (sometimes conflicting) demands and expectations which originate from various entities. The emotion of anger is in many cultures discouraged in girls and women, while fear is discouraged in boys and men. Expectations attached to social roles, such as "acting as man" and not as a woman, and the accompanying "feeling rules" contribute to the differences in expression of certain emotions. Some cultures encourage or discourage happiness, sadness, or jealousy, and the free expression of the emotion of disgust is considered socially unacceptable in most cultures. Some social institutions are seen as based on certain emotion, such as love in the case of contemporary institution of marriage. In advertising, such as health campaigns and political messages, emotional appeals are commonly found. Recent examples include no-smoking health campaigns and political campaign advertising emphasizing the fear of terrorism.
Sociological attention to emotion has varied over time. Emilé Durkheim (1915/1965) wrote about the collective effervescence or emotional energy that was experienced by members of totemic rituals in Australian aborigine society. He explained how the heightened state of emotional energy achieved during totemic rituals transported individuals above themselves giving them the sense that they were in the presence of a higher power, a force, that was embedded in the sacred objects that were worshipped. These feelings of exaltation, he argued, ultimately lead people to believe that there were forces that governed sacred objects.
In the 1990s, sociologists focused on different aspects of specific emotions and how these emotions were socially relevant. For Cooley (1992), pride and shame were the most important emotions that drive people to take various social actions. During every encounter, he proposed that we monitor ourselves through the "looking glass" that the gestures and reactions of others provide. Depending on these reactions, we either experience pride or shame and this results in particular paths of action. Retzinger (1991) conducted studies of married couples who experienced cycles of rage and shame. Drawing predominantly on Goffman and Cooley's work, Scheff (1990) developed a microsociological theory of the social bond. The formation or disruption of social bonds is dependant on the emotions that people experience during interactions.
Subsequent to these developments, Randall Collins (2004) formulated his interaction ritual theory by drawing on Durkheim's work on totemic rituals that was extended by Goffman (1964/2013; 1967) into everyday focused encounters. Based on interaction ritual theory, we experience different levels or intensities of emotional energy during face-to-face interactions. Emotional energy is considered to be a feeling of confidence to take action and a boldness that one experiences when they are charged up from the collective effervescence generated during group gatherings that reach high levels of intensity.
There is a growing body of research applying the sociology of emotion to understanding the learning experiences of school students during classroom interactions with teachers and other students (e.g., Milne & Otieno, 2007; Olitsky, 2007
Apart from interaction ritual traditions of the sociology of emotion, other approaches have been classed into one of 6 other categories (Turner, 2009) including:
This list provides a general overview of different traditions in the sociology of emotion that sometimes conceptualise emotion in different ways and at other times in complementary ways. Many of these different approaches were synthesized by Turner (2007) in his sociological theory of human emotions in an attempt to produce one comprehensive sociological account that draws on developments from many of the above traditions.
Psychotherapy and regulation.
Emotion regulation refers to the cognitive and behavioral strategies people use to influence their own emotional experience. For example, a behavioral strategy in which one avoids a situation to avoid unwanted emotions (e.g., trying not to think about the situation, doing distracting activities, etc.). Depending on the particular school's general emphasis on either cognitive components of emotion, physical energy discharging, or on symbolic movement and facial expression components of emotion, different schools of psychotherapy approach the regulation of emotion differently. Cognitively oriented schools approach them via their cognitive components, such as rational emotive behavior therapy. Yet others approach emotions via symbolic movement and facial expression components (like in contemporary Gestalt therapy).
Computer science.
In the 2000s, research in computer science, engineering, psychology and neuroscience has been aimed at developing devices that recognize human affect display and model emotions. In computer science, affective computing is a branch of the study and development of artificial intelligence that deals with the design of systems and devices that can recognize, interpret, and process human emotions. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as to early philosophical enquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing. Detecting emotional information begins with passive sensors which capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. Emotional speech processing recognizes the user's emotional state by analyzing speech patterns. The detection and processing of facial expression or body gestures is achieved through detectors and sensors.
Notable theorists.
In the late 19th century, the most influential theorists were William James (1842–1910) and Carl Lange (1834–1900). James was an American psychologist and philosopher who wrote about educational psychology, psychology of religious experience/mysticism, and the philosophy of pragmatism. Lange was a Danish physician and psychologist. Working independently, they developed the James–Lange theory, a hypothesis on the origin and nature of emotions. The theory states that within human beings, as a response to experiences in the world, the autonomic nervous system creates physiological events such as muscular tension, a rise in heart rate, perspiration, and dryness of the mouth. Emotions, then, are feelings which come about as a result of these physiological changes, rather than being their cause.
Silvan Tomkins (1911–1991) developed the Affect theory and Script theory. The Affect theory introduced the concept of basic emotions, and was based on the idea that the dominance of the emotion , which he called the affect system, was the motivating force in human life.
Some of the most influential theorists on emotion from the 20th century have died in the last decade. They include Magda B. Arnold (1903–2002), an American psychologist who developed the appraisal theory of emotions; Richard Lazarus (1922–2002), an American psychologist who specialized in emotion and stress, especially in relation to cognition; Herbert A. Simon (1916–2001), who included emotions into decision making and artificial intelligence; Robert Plutchik (1928–2006), an American psychologist who developed a psychoevolutionary theory of emotion; Robert Zajonc (1923–2008) a Polish–American social psychologist who specialized in social and cognitive processes such as social facilitation; Robert C. Solomon (1942–2007), an American philosopher who contributed to the theories on the philosophy of emotions with books such as "What Is An Emotion?: Classic and Contemporary Readings" (Oxford, 2003); Peter Goldie (1946–2011), a British philosopher who specialized in ethics, aesthetics, emotion, mood and character; Nico Frijda (1927–2015), a Dutch psychologist who advanced the theory that human emotions serve to promote a tendency to undertake actions that are appropriate in the circumstances, detailed in his book "The Emotions" (1986).
Influential theorists who are still active include the following psychologists, neurologists, philosophers, and sociologists:
References.
Notes
Bibliography
</dl>

</doc>
<doc id="10407" url="http://en.wikipedia.org/wiki?curid=10407" title="Epictetus">
Epictetus

Epictetus (; Greek: Ἐπίκτητος; A.D. c. 55 – 135) was a Greek Stoic philosopher. He was born a slave at Hierapolis, Phrygia (present day Pamukkale, Turkey), and lived in Rome until his banishment, when he went to Nicopolis in north-western Greece for the rest of his life. His teachings were written down and published by his pupil Arrian in his "Discourses".
Epictetus taught that philosophy is a way of life and not just a theoretical discipline. To Epictetus, all external events are determined by fate, and are thus beyond our control; we should accept whatever happens calmly and dispassionately. However, individuals are responsible for their own actions, which they can examine and control through rigorous self-discipline.
Life.
Epictetus was born c. 55 A.D., presumably at Hierapolis, Phrygia. The name his parents gave him is unknown; the word "epíktetos" (ἐπίκτητος) in Greek simply means "acquired." He spent his youth as a slave in Rome to Epaphroditos, a wealthy freedman and secretary to Nero.
Early in life, Epictetus acquired a passion for philosophy, and with the permission of his wealthy owner, he studied Stoic philosophy under Musonius Rufus, which allowed him to rise in respectability as he grew more educated. He somehow became crippled, with Origen stating that his leg was deliberately broken by his master, and Simplicius stating that he had been lame from childhood.
Epictetus obtained his freedom some time after Nero's death in 68 A.D., and began to teach philosophy in Rome. About 93 A.D. Emperor Domitian banished all philosophers from the city, and Epictetus fled to Nicopolis in Epirus, Greece, where he founded a philosophical school.
His most famous pupil, Arrian, studied under him when a young man (c. 108 A.D.) and claimed to have written the famous "Discourses" from his lecture notes, which he argued should be considered comparable to the Socratic literature. Arrian describes Epictetus as being a powerful speaker who could "induce his listener to feel just what Epictetus wanted him to feel." Many eminent figures sought conversations with him, and the Emperor Hadrian was friendly with him and may have listened to him speak at his school in Nicopolis.
He lived a life of great simplicity, with few possessions and lived alone for a long time, but in his old age he adopted a friend's child who would otherwise have been left to die, and raised him with the aid of a woman. Epictetus was never married. He died sometime around 135 A.D. After his death, his lamp was purchased by an admirer for 3,000 drachmae.
Thought.
No writings of Epictetus himself are really known. His discourses were transcribed and compiled by his pupil Arrian (author of the "Anabasis Alexandri"). The main work is "The Discourses," four books of which have been preserved (out of an original eight). Arrian also compiled a popular digest, entitled the "Enchiridion," or "Handbook." In a preface to the "Discourses," addressed to Lucius Gellius, Arrian states that "whatever I heard him say I used to write down, word for word, as best I could, endeavouring to preserve it as a memorial, for my own future use, of his way of thinking and the frankness of his speech."
Epictetus maintains that the foundation of all philosophy is self-knowledge, that is, the conviction of our ignorance and gullibility ought to be the first subject of our study. Logic provides valid reasoning and certainty in judgment, but it is subordinate to practical needs. The first and most necessary part of philosophy concerns the application of doctrine, for example, that people should not lie; the second concerns reasons, e.g. why people should not lie; while the third, lastly, examines and establishes the reasons. This is the logical part, which finds reasons, shows what is a reason, and that a given reason is a right one. This last part is necessary, but only on account of the second, which again is rendered necessary by the first.
Both the "Discourses" and the "Enchiridion" begin by distinguishing between those things in our power ("prohairetic" things) and those things not in our power ("aprohairetic" things). That alone is in our power, which is our own work; and in this class are our opinions, impulses, desires, and aversions. What, on the contrary, is not in our power, are our bodies, possessions, glory, and power. Any delusion on this point leads to the greatest errors, misfortunes, and troubles, and to the slavery of the soul.
We have no power over external things, and the good that ought to be the object of our earnest pursuit, is to be found only within ourselves. The determination between what is good and what is not good is made by the capacity for choice ("prohairesis"). Prohairesis allows us to act, and gives us the kind of freedom that only rational animals have. It is determined by our reason, which of all our faculties sees and tests itself and everything else. It is the right use of the impressions ("phantasia") that bombard the mind that is in our power:
Practise then from the start to say to every harsh impression, "You are an impression, and not at all the thing you appear to be." Then examine it and test it by these rules you have, and firstly, and chiefly, by this: whether the impression has to do with the things that are up to us, or those that are not; and if it has to do with the things that are not up to us, be ready to reply, "It is nothing to me."
We will not be troubled at any loss, but will say to ourselves on such an occasion: "I have lost nothing that belongs to me; it was not something of mine that was torn from me, but something that was not in my power has left me." Nothing beyond the use of our opinion is properly ours. Every possession rests on opinion. What is to cry and to weep? An opinion. What is misfortune, or a quarrel, or a complaint? All these things are opinions; opinions founded on the delusion that what is not subject to our own choice can be either good or evil, which it cannot. By rejecting these opinions, and seeking good and evil in the power of choice alone, we may confidently achieve peace of mind in every condition of life.
Reason alone is good, and the irrational is evil, and the irrational is intolerable to the rational. The good person should labour chiefly on their own reason; to perfect this is in our power. To repel evil opinions by the good is the noble contest in which humans should engage; it is not an easy task, but it promises true freedom, peace of mind ("ataraxia"), and a divine command over the emotions ("apatheia"). We should especially be on our guard against the opinion of pleasure because of its apparent sweetness and charms. The first object of philosophy, therefore, is to purify the mind.
Epictetus teaches that the preconceptions ("prolepsis") of good and evil are common to all. Good alone is profitable and to be desired, and evil is hurtful and to be avoided. Different opinions arise only from the application of these preconceptions to particular cases, and it is then that the darkness of ignorance, which blindly maintains the correctness of its own opinion, must be dispelled. People entertain different and conflicting opinions of good, and in their judgment of a particular good, people frequently contradict themselves. Philosophy should provide a standard for good and evil. This process is greatly facilitated because the mind and the works of the mind are alone in our power, whereas all external things that aid life are beyond our control.
The essence of God is goodness; we have all good that could be given to us. The gods too gave us the soul and reason, which is not measured by breadth or depth, but by knowledge and sentiments, and by which we attain to greatness, and may equal even with the gods. We should, therefore, cultivate the mind with special care. If we wish for nothing but what God wills, we shall be truly free, and all will come to pass with us according to our desire; and we shall be as little subject to restraint as Zeus himself.
Every individual is connected with the rest of the world, and the universe is fashioned for universal harmony. Wise people, therefore, will pursue, not merely their own will, but will also be subject to the rightful order of the world. We should conduct ourselves through life fulfilling all our duties as children, siblings, parents, and citizens.
For our country or friends we ought to be ready to undergo or perform the greatest difficulties. The good person, if able to foresee the future, would peacefully and contentedly help to bring about their own sickness, maiming, and even death, knowing that this is the right order of the universe. We have all a certain part to play in the world, and we have done enough when we have performed what our nature allows. In the exercise of our powers, we may become aware of the destiny we are intended to fulfill.
We are like travellers at an inn, or guests at a stranger's table; whatever is offered we take with thankfulness, and sometimes, when the turn comes, we may refuse; in the former case we are a worthy guest of the gods, and in the latter we appear as a sharer in their power. Anyone who finds life intolerable is free to quit it, but we should not abandon our appointed role without sufficient reason. The Stoic sage will never find life intolerable and will complain of no one, either God or human. Those who go wrong we should pardon and treat with compassion, since it is from ignorance that they err, being as it were blind.
It is only our opinions and principles that can render us unhappy, and it is only the ignorant person that finds fault with another. Every desire degrades us, and renders us slaves of what we desire. We ought not to forget the transitory character of all external advantages, even in the midst of our enjoyment of them; but always to bear in mind that they are not our own, and that therefore they do not properly belong to us. Thus prepared, we shall never be carried away by opinions.
The final entry of the "Enchiridion," or "Handbook," begins: "Upon all occasions we ought to have these maxims ready at hand":
Conduct me, Zeus, and thou, O Destiny,
Wherever thy decree has fixed my lot.
I follow willingly; and, did I not,
Wicked and wretched would I follow still.
(Diogenes Laertius quoting Cleanthes; quoted also by Seneca, Epistle 107.)"
Whoe'er yields properly to Fate is deemed
Wise among men, and knows the laws of Heaven.
O Crito, if it thus pleases the gods, thus let it be.
Anytus and Meletus may indeed kill me, but they cannot harm me.
Influence.
Military.
The philosophy of Epictetus is well known in the U.S. military through the writings and example of James Stockdale, an American fighter pilot who was shot down over North Viet Nam, became a prisoner of war during the Vietnam War, and later a vice presidential candidate. In "Courage under Fire: Testing Epictetus's Doctrines in a Laboratory of Human Behavior" (1993), Stockdale credits Epictetus with helping him endure seven and a half years in a North Viet Namese military prison—including torture—and four years in solitary confinement.
In his conclusion, Stockdale quoted Epictetus as saying, "The emotions of grief, pity, and even affection are well-known disturbers of the soul. Grief is the most offensive; Epictetus considered the suffering of grief an act of evil. It is a willful act, going against the will of God to have all men share happiness" (p. 235).
Philosophy.
The philosophy of Epictetus was an influence on the Roman Emperor Marcus Aurelius (121 to 180 A.D.) whose reign was marked by wars with the resurgent Parthia in southern Asia and against the Germanic tribes in Europe. Aurelius quotes from Epictetus repeatedly in his own work, "Meditations", written during his campaigns in central Europe.
In the 6th century, the Neoplatonist philosopher Simplicius, who was persecuted for his pagan beliefs during the reign of Justinian, wrote an extant commentary on the "Enchiridion". At the end of the commentary Simplicius wrote: "Nor does my writing this commentary prove beneficial to others only, for I myself have already found great advantage from it, by the agreeable diversion it has given me, in a season of trouble and public calamity." George Long considered the commentary "worth reading", but then opined, "But how many will read it? Perhaps one in a million."
When Bernard Stiegler was imprisoned for five years for armed robbery in France, he assembled an "ensemble of disciplines," which he called (in reference to Epictetus) his "melete". This ensemble amounted to a practice of reading and writing that Stiegler derived from the writings of Epictetus. This led to his transformation, and upon being released from incarceration he became a professional philosopher. Stiegler tells the story of this transformation in his book, "Acting Out".
Literature.
The philosophy of Epictetus plays a key role in the 1998 novel by Tom Wolfe, "A Man in Full". This was in part the outcome of discussions Wolfe had with James Stockdale (see above). The importance of Epictetus' Stoicism for Stockdale, its role in "A Man in Full", and its significance in "Gladiator (2000 film)" is discussed by William O. Stephens in "The Rebirth of Stoicism?"
Mohun Biswas, in the novel "A House for Mr Biswas" (1961), by V.S. Naipaul, is pleased to think himself a follower of Epictetus and Marcus Aurelius; the irony is that he never actually behaves as a Stoic.
“Everything has two handles, the one by which it may be carried, the other by which it cannot” is the theme of "Disturbances in the Field" (1983), by Lynne Sharon Schwartz. Lydia, the central character, turns often to "The Golden Sayings of Epictetus".
A line from the "Enchiridion" is used as a title quotation in "The Life and Opinions of Tristram Shandy, Gentleman" by Laurence Sterne, which translates to, "Not things, but opinions about things, trouble men." The quotation alludes to a theme of the novel about how the suffering of many of its characters (above all Walter Shandy) is the result of the opinions and assumptions they make about reality.
Epictetus is mentioned in "A Portrait of the Artist as a Young Man" by James Joyce: in the fifth chapter of the novel the protagonist Stephen Daedalus discusses Epictetus's famous lamp with a Dean of his college. Epictetus is also mentioned briefly in "Franny and Zooey" by J. D. Salinger, and is referred to by Theodore Dreiser in his novel "Sister Carrie". Both the longevity of Epictetus's life and his philosophy are alluded to in John Berryman's poem, "Of Suicide."
Epictetus is referred to, but not mentioned by name, in Matthew Arnold's sonnet "To a Friend." Arnold provides three historical personalities as his inspiration and support in difficult times (Epictetus is preceded by Homer and succeeded by Sophocles):
Much he, whose friendship I not long since won,<br>
That halting slave, who in Nicopolis<br>
Taught Arrian, when Vespasian's brutal son<br>
Cleared Rome of what most shamed him.
Psychology.
Psychologist Albert Ellis, the founder of Rational Emotive Behavior Therapy, credited Epictetus with providing a foundation for his system of psychotherapy.
Religion.
Kiyozawa Manshi, a controversial reformer within the Higashi Honganji branch of Jodo Shinshu Buddhism cited Epictetus as one of the three major influences on his spiritual development and thought.
Acting.
Epictetus' philosophy is an influence on the acting method introduced by David Mamet and William H. Macy, known as Practical Aesthetics. The main book that describes the method, "The Practical Handbook for the Actor", lists the "Enchiridion" in the bibliography.

</doc>
<doc id="10408" url="http://en.wikipedia.org/wiki?curid=10408" title="Edward Lear">
Edward Lear

Edward Lear (12 or 13 May 1812 – 29 January 1888) was an English artist, illustrator, musician, author and poet, and is known now mostly for his literary nonsense in poetry and prose and especially his limericks, a form he popularised. His principal areas of work as an artist were threefold: as a draughtsman employed to illustrate birds and animals; making coloured drawings during his journeys, which he reworked later, sometimes as plates for his travel books; as a (minor) illustrator of Alfred Tennyson's poems. As an author, he is known principally for his popular nonsense collections of poems, songs, short stories, botanical drawings, recipes, and alphabets. He also composed and published twelve musical settings of Tennyson's poetry.
Biography.
Early years.
Lear was born into a middle-class family in the village of Holloway near London, the penultimate of twenty-one children (and youngest to survive) of Ann Clark Skerrett and Jeremiah Lear. He was raised by his eldest sister, also named Ann, 21 years his senior. Owing to the family's limited finances, Lear and his sister were required to leave the family home and live together when he was aged four. Ann doted on Edward and continued to act as a mother for him until her death, when he was almost 50 years of age.
Lear suffered from lifelong health afflictions. From the age of six he suffered frequent "grand mal" epileptic seizures, and bronchitis, asthma, and during later life, partial blindness. Lear experienced his first seizure at a fair near Highgate with his father. The event scared and embarrassed him. Lear felt lifelong guilt and shame for his epileptic condition. His adult diaries indicate that he always sensed the onset of a seizure in time to remove himself from public view. When Lear was about seven years old he began to show signs of depression, possibly due to the instability of his childhood. He suffered from periods of severe melancholia which he referred to as "the Morbids."
Artist.
Lear was already drawing "for bread and cheese" by the time he was aged 16 and soon developed into a serious "ornithological draughtsman" employed by the Zoological Society and then from 1832 to 1836 by the Earl of Derby, who kept a private menagerie at his estate Knowsley Hall. Lear's first publication, published when he was 19 years old, was "Illustrations of the Family of Psittacidae, or Parrots" in 1830.
His paintings were well received and he was compared favourably with the naturalist John James Audubon.
Among other travels, he visited Greece and Egypt during 1848–49, and toured India and Ceylon during 1873–75. While travelling he produced large quantities of coloured wash drawings in a distinctive style, which he converted later in his studio into oil and watercolour paintings, as well as prints for his books. His landscape style often shows views with strong sunlight, with intense contrasts of colour.
Between 1878 to 1883 Lear spent his summers on Monte Generoso, a mountain on the border between the Swiss canton of Ticino and the Italian region of Lombardy. His oil painting "The Plains of Lombardy from Monte Generoso" is in the Ashmolean Museum in the English city of Oxford.
Throughout his life he continued to paint seriously. He had a lifelong ambition to illustrate Tennyson's poems; near the end of his life a volume with a small number of illustrations was published.
Composer and Musician.
Lear played the accordion, flute, and guitar, but primarily the piano. He composed music for many Romantic and Victorian poems, but was known mostly for his many musical settings of Tennyson's poetry. He published four settings in 1853, five in 1859, and three in 1860. Lear's were the only musical settings that Tennyson approved of. Lear also composed music for many of his nonsense songs, including "The Owl and the Pussy-cat," but only two of the scores have survived, the music for "The Courtship of the Yonghy-Bonghy-Bò" and "The Pelican Chorus." While he never played professionally, he did perform his own nonsense songs and his settings of others' poetry at countless social gatherings, sometimes adding his own lyrics (as with the song "The Nervous Family"), and sometimes replacing serious lyrics with nursery rhymes.
Relationships.
Lear's most fervent and painful friendship involved Franklin Lushington. He met the young barrister in Malta in 1849 and then toured southern Greece with him. Lear developed an undoubtedly homosexual passion for him that Lushington did not reciprocate. Although they remained friends for almost forty years, until Lear's death, the disparity of their feelings for one another constantly tormented Lear. Indeed, none of Lear's attempts at male companionship were successful; the very intensity of Lear's affections seemingly doomed the relationships.
The closest he came to marriage with a woman was two proposals, both to the same person 46 years his junior, which were not accepted. For companions he relied instead on friends and correspondents, and especially, during later life, on his Albanian Souliote chef, Giorgis, a faithful friend and, as Lear complained, a thoroughly unsatisfactory chef. Another trusted companion in Sanremo was his cat, Foss, who died in 1886 and was buried with some ceremony in a garden at Villa Tennyson.
San Remo and death.
Lear travelled widely throughout his life and eventually settled in San Remo, on his beloved Mediterranean coast, in the 1870s, at a villa he named "Villa Tennyson."
Lear was known to introduce himself with a long pseudonym: "Mr Abebika kratoponoko Prizzikalo Kattefello Ablegorabalus Ableborinto phashyph" or "Chakonoton the Cozovex Dossi Fossi Sini Tomentilla Coronilla Polentilla
Battledore & Shuttlecock Derry down Derry Dumps" which he based on "Aldiborontiphoskyphorniostikos".
After a long decline in his health, Lear died at his villa in 1888, of the heart disease from which he had suffered since at least 1870. Lear's funeral was said to be a sad, lonely affair by the wife of Dr. Hassall, Lear's physician, none of Lear's many lifelong friends being able to attend.
Lear is buried in the Cemetery Foce in San Remo. On his headstone are inscribed these lines about Mount Tomohrit (in Albania) from Tennyson's poem "To E.L. [Edward Lear], On His Travels in Greece":
The centenary of his death was marked in Britain with a set of Royal Mail stamps in 1988 and an exhibition at the Royal Academy. Lear's birthplace area is now marked with a plaque at Bowman's Mews, Islington, in London, and his bicentenary during 2012 was celebrated with a variety of events, exhibitions and lectures in venues across the world including an International Owl and Pussycat Day on his birthday.
Author.
In 1846 Lear published "," a volume of limericks that went through three editions and helped popularize the form and the genre of literary nonsense. In 1871 he published "Nonsense Songs, Stories, Botany and Alphabets", which included his most famous nonsense song, "The Owl and the Pussycat," which he wrote for the children of his patron Edward Stanley, 13th Earl of Derby. Many other works followed.
Lear's nonsense books were quite popular during his lifetime, but a rumor developed that "Edward Lear" was merely a pseudonym, and the books' true author was the man to whom Lear had dedicated the works, his patron the Earl of Derby. Promoters of this rumor offered as evidence the facts that both men were named Edward, and that "Lear" is an anagram of "Earl."
Lear's nonsense works are distinguished by a facility of verbal invention and a poet's delight in the sounds of words, both real and imaginary. A stuffed rhinoceros becomes a "diaphanous doorscraper." A "blue Boss-Woss" plunges into "a perpendicular, spicular, orbicular, quadrangular, circular depth of soft mud." His heroes are Quangle-Wangles, Pobbles, and Jumblies. One of his most famous verbal inventions, the phrase "runcible spoon," occurs in the closing lines of "The Owl and the Pussycat," and is now found in many English dictionaries:
Though famous for his neologisms, Lear used a number of other devices in his works in order to defy reader expectations. For example, "Cold Are The Crabs, conforms to the sonnet tradition until the dramatically foreshortened last line.
Limericks are invariably typeset as four plus one lines presently, but Lear's limericks were published in a variety of formats. It appears that Lear wrote them in manuscript in as many lines as there was room for beneath the picture. For the first three editions most are typeset as, respectively, two, five, and three lines. The cover of one edition bears an entire limerick typeset in two lines:
In Lear's limericks the first and last lines usually end with the same word rather than rhyming. For the most part they are truly nonsensical and devoid of any punch line or point. They are completely free of the bawdyness with which the verse form is now associated. A typical thematic element is the presence of a callous and critical "they." An example of a typical Lear limerick:
Lear's self-description in verse, "How Pleasant to know Mr. Lear," ends with this stanza, a reference to his own mortality:
Five of Lear's limericks from the Book of Nonsense, in the 1946 Italian translation by Carlo Izzo, were set to music for choir a cappella by Goffredo Petrassi in 1952.
Portrayals.
Edward Lear has been played in radio dramas by Andrew Sachs in "The Need for Nonsense" by Julia Blackburn (BBC Radio 4, 9 February 2009) and by Derek Jacobi in "By the Coast of Coromandel" by Lavinia Murray (BBC Radio 4, 21 December 2011).

</doc>
<doc id="10409" url="http://en.wikipedia.org/wiki?curid=10409" title="Eve Arden">
Eve Arden

Eve Arden (April 30, 1908 – November 12, 1990) was an American actress. Her almost 60-year career crossed most media frontiers with both supporting and leading roles, but she may be best-remembered for playing the sardonic but engaging title character, a high school teacher, on "Our Miss Brooks", and as the Rydell High School principal in both "Grease" and "Grease 2".
Early life.
Eve Arden was born Eunice M. Quedens (pronounced qwi-DENZ) on April 30, 1908, in Mill Valley, California, to Lucille and Charles Peter Quedens. Her parents divorced when she was a child. Arden claimed to have been an insecure child, declaring later in life that she needed therapy because her mother was so much more beautiful than she. 
Some sources indicate that Arden was Catholic based solely on the fact that she had, at one point, attended a Roman Catholic convent school. However, at age 16, she left Tamalpais High School, a public high school, and joined a stock theater company. She made her film debut, under her real name, in the backstage musical "Song of Love" (1929). She played a wisecracking showgirl who becomes a rival to the film's star, singer Belle Baker. The film was one of Columbia Pictures' earliest successes.
Arden's Broadway debut came in 1934, when she was cast in that year's "Ziegfeld Follies" revue. This role was the first in which she was credited as Eve Arden. She chose that name after being told by producer Lee Shubert to drop her real name and claimed she was inspired by two cosmetics bottles in her dressing room, one labeled "Evening in Paris" and the other by "Elizabeth Arden".
Career.
Film.
Arden's film career began in earnest in 1937 when she appeared in the films "Oh Doctor" and "Stage Door". Her "Stage Door" portrayal of a fast-talking, witty supporting character, gained Arden considerable notice and was to be a template for many of Arden's future roles.
Her many memorable screen roles include a supporting role as Joan Crawford's wise-cracking friend in "Mildred Pierce" (1945) for which she received an Academy Award nomination as Best Supporting Actress, and James Stewart's wistful secretary in Otto Preminger's murder mystery, "Anatomy of a Murder" (1959). (One of her co-stars in that film was husband Brooks West.) She also performed some acrobatics while trying to steal a wallet from Groucho Marx in the Marx Brothers film "At the Circus" (1939). In 1946 exhibitors voted her the sixth-most promising "star of tomorrow".
Radio and television.
Arden's ability with witty scripts made her a natural talent for radio; she became a regular on Danny Kaye's short-lived but memorably zany comedy-variety show in 1946, which also featured swing bandleader Harry James and gravel-voiced character actor-comedian Lionel Stander.
Kaye's show lasted one season, but Arden's display of comic talent and timing set the stage for her to be cast in her best-known role, Madison High School English teacher Connie Brooks in "Our Miss Brooks". Arden portrayed the character on radio from 1948 to 1957, in a television version of the program from 1952 to 1956, and in a 1956 feature film. Arden's character clashed with the school's principal, Osgood Conklin (played by Gale Gordon), and nursed an unrequited crush on fellow teacher Philip Boynton (played originally by future film star Jeff Chandler and, later on radio, then on television, by Robert Rockwell). Except for Chandler, the entire radio cast of Arden, Gordon, Richard Crenna (Walter Denton), Robert Rockwell (Mr. Philip Boynton), Gloria McMillan (Harriet Conklin), and Jane Morgan (landlady Margaret Davis) played the same roles on television.
Arden's portrayal of the character was so popular that she was made an honorary member of the National Education Association, received a 1952 award from the Teachers College of Connecticut's Alumni Association "for humanizing the American teacher", and even received teaching job offers. She won a listeners' poll by "Radio Mirror" magazine as the top ranking comedienne of 1948–1949, receiving her award at the end of an "Our Miss Brooks" broadcast that March. "I'm certainly going to try in the coming months to merit the honor you've bestowed upon me, because I understand that if I win this (award) two years in a row, I get to keep Mr. Boynton," she joked. But she was also a hit with the critics; a winter 1949 poll of newspaper and magazine radio editors taken by "Motion Picture Daily" named her the year's best radio comedienne.
Arden had a very brief guest appearance in a 1955 "I Love Lucy" episode entitled "L.A. at Last" in which she played herself. While awaiting their food at The Brown Derby, a Hollywood restaurant, Lucy Ricardo (Lucille Ball) and Ethel Mertz (Vivian Vance) argue over whether a certain portrait on a nearby wall is Shelley Winters or Judy Holliday. Lucy urges Ethel to ask a lady occupying the next booth, who turns and replies, "Neither. That's Eve Arden." Ethel suddenly realizes she was just talking to Arden herself, who soon passes Lucy and Ethel's table to leave the restaurant while the pair gawk. 
Desilu Productions, jointly owned by Desi Arnaz and Ball during their marriage, was the production company for the "Our Miss Brooks" television show, which filmed during the same years as "I Love Lucy". Ball and Arden became acquainted when they co-starred together in the film "Stage Door" in 1937. It was Ball, according to numerous radio historians, who suggested Arden for "Our Miss Brooks" after Shirley Booth auditioned for but failed to land the role and Ball – committed at the time to "My Favorite Husband" – could not.
Arden tried another series in the fall of 1957, "The Eve Arden Show", but it was canceled in spring of 1958 after 26 episodes. In 1966, Arden played Nurse Kelton in the episode of "Bewitched". She later co-starred with Kaye Ballard as her neighbor and in-law, Eve Hubbard, in the 1967–69 situation comedy "The Mothers-in-Law", which was produced by Desi Arnaz after the dissolution of Desilu Productions. She also played the title character's aunt on the TV series "Maude".
Other credits.
Arden was one of many actresses to take on the title roles in "Hello, Dolly!" and "Auntie Mame" in the 1960s; in 1967, she won the Sarah Siddons Award for her work in Chicago theatre. Arden was cast in 1983 as the leading lady in what was to be her Broadway comeback in "Moose Murders". But she wisely withdrew (and was replaced with the much younger Holland Taylor) after one preview performance, citing "artistic differences", and the show went on to open and close on the same night, becoming known as one of the most legendary flops in Broadway history.
She became familiar to a new generation of film-goers when she played Principal McGee in both 1978's "Grease" and 1982's "Grease 2", as well as making appearances on such television shows as "Bewitched", "Alice", "Maude" and "Falcon Crest". In 1985, she appeared as the wicked stepmother in the "Faerie Tale Theatre" production of "Cinderella".
Arden published an autobiography, "The Three Phases of Eve", in 1985. In addition to her Academy Award nomination, Arden has two stars on the Hollywood Walk of Fame: Radio and Television (see List of stars on the Hollywood Walk of Fame for addresses). She was inducted into the National Radio Hall of Fame in 1995.
Personal life.
Arden was married to Ned Bergen from 1939 to 1947, and to actor Brooks West from 1952 until his death in 1984 from a heart ailment. She and West had four children. She was also involved with Danny Kaye.
Death.
On November 12, 1990, Arden died from colorectal cancer and heart disease at her home in Los Angeles at the age of 82. She is interred in the Westwood Village Memorial Park Cemetery, Westwood, Los Angeles, California. 

</doc>
<doc id="10412" url="http://en.wikipedia.org/wiki?curid=10412" title="Elementary function">
Elementary function

In mathematics, an elementary function is a function of one variable built from a finite number of exponentials, logarithms, constants, and "n"th roots through composition and combinations using the four elementary operations (+ – × ÷). By considering these functions (and constants) to be complex, the elementary function concept is enlarged to include trigonometric functions and their inverses (see trigonometric functions and complex exponentials).
The roots of a polynomial equation are the solutions of a polynomial equation with constant coefficients. For polynomials of degree four and less there are explicit formulae for determining the roots (the formulae are elementary functions), but elementary functions are not sufficient for finding the roots of general higher-degree polynomials.
Note that some elementary functions, such as roots, logarithms, or inverse trigonometric functions, are not entire functions and their definition may be ambiguous, especially for non-real numbers.
Elementary functions were introduced by Joseph Liouville in a series of papers from 1833 to 1841. An algebraic treatment of elementary functions was started by Joseph Fels Ritt in the 1930s.
Examples.
Examples of elementary functions include:
and
The last function is equal to the inverse cosine trigonometric function formula_3 in the entire complex domain. Hence, formula_3 is an elementary function. An example of a function that is "not" elementary is the error function
a fact that may not be immediately obvious, but can be proven using the Risch algorithm.
Differential algebra.
The mathematical definition of an elementary function, or a function in elementary form, is considered in the context of differential algebra. A differential algebra is an algebra with the extra operation of derivation (algebraic version of differentiation). Using the derivation operation new equations can be written and their solutions used in extensions of the algebra. By starting with the field of rational functions, two special types of transcendental extensions (the logarithm and the exponential) can be added to the field building a tower containing elementary functions.
A differential field "F" is a field "F"0 (rational functions over the rationals Q for example) together with a derivation map "u" → ∂"u". (Here ∂"u" is a new function. Sometimes the notation "u"′ is used.) The derivation captures the properties of differentiation, so that for any two elements of the base field, the derivation is linear
and satisfies the Leibniz product rule
An element "h" is a constant if "∂h = 0". If the base field is over the rationals, care must be taken when extending the field to add the needed transcendental constants.
A function "u" of a differential extension "F"["u"] of a differential field "F" is an elementary function over "F" if the function "u"
(this is Liouville's theorem).

</doc>
<doc id="10413" url="http://en.wikipedia.org/wiki?curid=10413" title="Enchiridion of Epictetus">
Enchiridion of Epictetus

The Enchiridion or Manual of Epictetus (Ancient Greek: Ἐγχειρίδιον Ἐπικτήτου, "Enkheirídion Epiktḗtou") (enchiridion is Greek for "that which is held in the hand" and by extension, to dagger) is a short manual of Stoic ethical advice compiled by Arrian, a 2nd-century disciple of the Greek philosopher Epictetus.
Although the content is similar to the "Discourses of Epictetus", it is not a summary of the "Discourses" but rather a compilation of practical precepts. Eschewing metaphysics, Arrian focused his attention on Epictetus's work applying philosophy in daily life. The primary theme is that one should accept what happens:
What upsets people is not things themselves but their judgments about the things. For example, "death is nothing dreadful (or else it would have appeared dreadful to Socrates)..." <br>
However, "some things are up to us and some are not up to us" and we must act accordingly, taking responsibility for planning and enacting what we can with virtue without becoming upset or disheartened by obstacles and reverses beyond our control.
For many centuries, the "Enchiridion" maintained its authority both with Christians and Pagans. Two Christian writers – Nilus and an anonymous contemporary – wrote paraphrases of it in the early 5th century and Simplicius of Cilicia wrote a commentary upon it in the 6th. The work was first published in Latin translation by Poliziano in Rome in 1493; Beroaldus published another edition in Bologna in 1496. The original Greek was first published in Venice with the Simplicius's commentary in 1528 and an English translation appeared as early as 1567. The book was a common school text in Scotland during the Scottish Enlightenment. Adam Smith had a 1670 edition in his library, acquired as a schoolboy.
English translations.
There have been many English translations of the "Enchiridion". Translations of the "Discourses" (e.g. Elizabeth Carter's and George Long's) have included the "Enchiridion" and it has often been included with other moral writings from the ancient world, such as the "Tablet of Cebes". Some notable translations of the "Enchiridion" include:

</doc>
<doc id="10415" url="http://en.wikipedia.org/wiki?curid=10415" title="Emperor Kinmei">
Emperor Kinmei

Emperor Kinmei (欽明天皇, Kinmei-tennō, 509–571) was the 29th emperor of Japan, according to the traditional order of succession.
His reign is said to have spanned the years from 539 through 571. Kinmei is the first for which contemporary historiography is able to assign verifiable dates.
Traditional narrative.
Kinmei's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great king who rules all under heaven." Alternatively, Kinmei might have been referred to as (ヤマト大王/大君) or the "Great King of Yamato."
Events of Kinmei's life.
Because of several chronological discrepancies in the account of Emperor Kinmei in the Nihon Shoki, some believe that his was actually a rival court to that of Emperors Ankan and Senka. Nevertheless, according to the traditional account, it was not until the death of Emperor Kinmei's older brother Emperor Senka that he gained the throne.
According to this account, Emperor Senka died in 539 at the age of 73; and succession passed to the third son of Emperor Keitai. This Imperial Prince was the next youngest brother of Emperor Senka. He would come to be known as Emperor Kinmei. He established his court at Shikishima no Kanazashi Palace (磯城嶋金刺宮) in Yamato.
The emperor's chief counselors were:
Although the imperial court was not moved to the Asuka region of Japan until 592, Emperor Kinmei's rule is considered by some to be the beginning of the Asuka period of Yamato Japan, particularly by those who associate the Asuka period primarily with the introduction of Buddhism to Japan from Korea.
According to the Nihon Shoki, Emperor Kinmei received a bronze statue of Buddha as a gift from the king of Paekche King Song Myong (聖明王, "Seimei Ō") along with a significant envoy of artisans, monks, and other artifacts in 552. (However, according to the Jōgū Shōtoku Hōō Teisetsu, Buddhism was introduced in 538.) This episode is widely regarded as the official introduction of Buddhism to the country.
With the introduction of a new religion to the court, a deep rift developed between the Mononobe clan, who supported the worship of Japan's traditional deities, and the Soga clan, who supported the adoption of Buddhism.
According to the Nihon Shoki, Emperor Kinmei ruled until his death in 571 and was buried in the Hinokuma no Sakai Burial Mound (桧隈坂合陵). An alternate stronger theory holds that he was actually buried in the Misemaruyama Tumulus (見瀬丸山古墳) located in Kashihara City (橿原市).
This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara. The Imperial Household Agency designates the Nara location as Kinmei's mausoleum. It is formally named "Hinokuma no saki Ai no misasagi"; however, the actual sites of the graves of the early emperors remain problematic, according to some historians and archaeologists.
Genealogy.
Emperor Kinmei's father was Emperor Keitai and his mother was Emperor Ninken's daughter, Princess Tashiraka (手白香皇女, Tashiraka Ōjo). In his lifetime, he was known by the name Amekuni Oshiharaki Hironiwa (天国排開広庭).
Kinmei had six Empresses and 25 Imperial children (16 sons and 9 daughters). According to Nihongi, he had six wives; but Kojiki only gives five wives, identifying the third consort to the sixth one. The first three were his nieces, daughters of his half brother Senka; two others were sisters, daughters of the Omi Soga no Iname

</doc>
<doc id="10416" url="http://en.wikipedia.org/wiki?curid=10416" title="Emperor Bidatsu">
Emperor Bidatsu

Emperor Bidatsu (敏達天皇, Bidatsu-tennō, 538 – 14 September 585) was the 30th emperor of Japan, according to the traditional order of succession.
The years of reign of Bidatsu start in 572 and end in 585; however, there are no certain dates for this emperor's life or reign. The names and sequence of the early emperors were not confirmed as "traditional" until the reign of Emperor Kammu, who was the 50th monarch of the Yamato dynasty.
Traditional narrative.
Historians consider details about the life of Emperor Bidatsu to be possibly legendary, but probable. The name Bidatsu"-tennō" was created for him posthumously by later generations.
In the "Nihonshoki", he is called Nunakura no Futotamashiki (渟中倉太珠敷).
His palace in Yamato Province was called Osada no Miya of Iware.
Events of Bidatsu's life.
In the 15th year of Kimmei's reign, Bidatsu was named Crown Prince.
In the 32nd year of Kimmei"-tennō" 's reign (欽明天皇32年, 572), the old emperor died, and the succession (‘‘senso’’) was received by his second son. Soon after, Emperor Bidatsu is said to have acceded to the throne (‘‘sokui’’).
Bidatsu's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great king who rules all under heaven." Alternatively, Bidatsu might have been referred to as (ヤマト大王/大君) or the "Great King of Yamato."
Bidatsu's reign was marked by power struggles about Buddhism. The two most important men in the court of Bidatsu were Soga no Umako and Mononobe no Moriya. Soga supported the growth of Buddhism, and Moriya wanted to stop it.
Bidatsu sought to re-establish relations with Korean Kingdoms and, according to "Nihonshoki", his court successfully established relations with Baekje and Silla, two of the Three Kingdoms of Korea.
The emperor died from a disease which afflicted him with sores, apparently the first royal victim of smallpox in Japan.
The actual site of Bidatsu's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Osaka.
The Imperial Household Agency designates this location as Bidatsu's mausoleum. It is formally named "Kawachi no Shinaga no naka no o no misasagi".
Genealogy.
He was the second son of Emperor Kimmei. His mother, Ishi-hime, was a daughter of Emperor Senka.
Although he had many children, none of them would ever become emperor. According to "Gukanshō", Bidatsu had four empresses and 16 Imperial children (6 sons and 10 daughters).
Bidatsu's first empress, Hirohime, died in the fifth year of his reign. To replace her, he elevated one of his consorts, Princess Nukatabe, to the rank of empress. Nukatabe was his half-sister by their father Kimmei. Later she ascended to the throne in her own right and is today known as Empress Suiko.
He was succeeded first by one of his brothers, Emperor Yōmei, then by another, Emperor Sushun, and then Empress Suiko, his sister and wife, before his grandson, Emperor Jomei, eventually took the throne.

</doc>
<doc id="10417" url="http://en.wikipedia.org/wiki?curid=10417" title="Emperor Yōmei">
Emperor Yōmei

Emperor Yōmei (用明天皇, Yōmei-tennō, 518–587) was the 31st Emperor of Japan, according to the traditional order of succession.
Yōmei's reign spanned the years from 585 until his death in 587.
Traditional narrative.
He was called Tachibana no Toyohi no Mikoto (橘豊日尊) in the "Nihonshoki". He was also referred to as Prince Ōe (大兄皇子, Ōe no Miko, literally "crown prince") and Prince Ikebe (池辺皇子, Ikebe no Miko) after the palace in which he lived. He acceded to the throne after the death of his half brother, Emperor Bidatsu.
The influential courtiers from Emperor Bidatsu's reign, Mononobe no Moriya, also known as Mononobe Yuge no Moriya no Muraji or as Ō-muraji Yuge no Moriya, and Soga no Umako no Sukune, both remained in their positions during the reign of Emperor Yōmei. Umako was the son of Sogo Iname no Sukune, and therefore, he would have been one of Emperor Yōmei's cousins.
Yōmei's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great king who rules all under heaven." Alternatively, Yōmei might have been referred to as (ヤマト大王/大君) or the "Great King of Yamato."
Emperor Yōmei's reign lasted only two years; and he died at the age of 69.
Because of the brevity of his reign, Emperor Yōmei was not responsible for any radical changes in policy, but his support of Buddhism created tension with supporters of Shintoism who opposed its introduction. According to Nihon Shoki, Emperor Yomei believed both in Buddhism and Shintoism. Moriya, the most influential supporter of Shintoism, conspired with Emperor Yōmei's brother, Prince Anahobe, and after Emperor Yomei's death they made an abortive attempt to seize the throne. Although Emperor Yōmei is reported to have died from illness, this incident and the brevity of his reign have led some to speculate that he was actually assassinated by Moriya and Prince Anahobe.
The actual site of Yōmei's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Osaka.
The Imperial Household Agency designates this location as Yōmei's mausoleum. It is formally named "Kawachi no Shinaga no hara no misasagi".
Genealogy.
Emperor Yōmei was the fourth son of Emperor Kimmei and his mother was Soga no Kitashihime, a daughter of Soga no Iname.
Yomei had three Empresses and seven Imperial sons and daughters.
Yōmei's son, Prince Umayado, is also known as Prince Shōtoku.

</doc>
<doc id="10418" url="http://en.wikipedia.org/wiki?curid=10418" title="Emperor Sushun">
Emperor Sushun

Emperor Sushun (崇峻天皇, Sushun-tennō, died 592) was the 32nd emperor of Japan, according to the traditional order of succession.
Sushun's reign spanned the years from 587 through 592.
Traditional narrative.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was Hatsusebe"-shinnō", also known as Hatsusebe no Waka-sazaki.
His name at birth was Hatsusebe no Miko (長谷部皇子). He was the twelfth son of Emperor Kimmei. His mother was Oane-no-kimi (小姉君), a daughter of Soga no Iname, who was the chief, or Ō-omi, of the Soga clan.
He succeeded his half brother, Emperor Yōmei in 587, and lived in the Kurahashi Palace (Kurahashi no Miya) in Yamato.
Sushun's contemporary title would not have been "Tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably Sumeramikoto (皇尊) or Amenoshita Shiroshimesu Ōkimi (治天下大王), meaning "the great king who rules all under heaven." Alternatively, Sushun might have been referred to as (ヤマト大王/大君) or the "Great King of Yamato."
He came to the throne with the support of the Soga clan and Empress Suiko, his half sister and the widow of Emperor Bidatsu. Initially, the Mononobe clan, a rival clan of the Sogas, allied with Prince Anahobe, another son of Kimmei, and attempted to have him installed as emperor. Soga no Umako, who succeeded his father as Ōomi of the Soga clan, eventually killed Mononobe no Moriya at the Battle of Shigisan, the head of the Mononobe clan, which led to its decline. Umako then installed Emperor Sushun on the throne.
As time went on, Sushun eventually became resentful of Umako's power, and wanted him deposed. It is said that one day, he saw a wild boar and proclaimed, "I want to kill Soga Umako like this wild boar." This angered Soga no Umako and, perhaps out of fear of being struck first, Umako had Sushun assassinated by Yamato no Aya no Ataikoma (東漢直駒) in 592.
Emperor Sushun's reign lasted for five years before his death at the age of 72.
The actual site of Sushun's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara.
The Imperial Household Agency designates this location as Yōmei's mausoleum. It is formally named "Kurahashi no oka no e no misasagi".
Genealogy.
Sushun had one Empress and two Imperial children.

</doc>
<doc id="10419" url="http://en.wikipedia.org/wiki?curid=10419" title="Empress Suiko">
Empress Suiko

Empress Suiko (推古皇后, Suiko-kogo) (554 – 15 April 628) was the 33rd monarch of Japan, according to the traditional order of succession.
Suiko's reign spanned the years from 593 until her death in 628.
In the history of Japan, Suiko was the first of eight women to take on the role of empress regnant. The seven women sovereigns reigning after Suiko were Kōgyoku/Saimei, Jitō, Gemmei, Genshō, Kōken/Shōtoku, Meishō and Go-Sakuramachi.
Traditional narrative.
Before her ascension to the Chrysanthemum Throne, her personal name (her "imina") was Mikekashiya-hime-no-mikoto, also called Toyomike Kashikiya hime no Mikoto.
Empress Suiko had several names including Princess Nukatabe and (possibly posthumous) Toyomike Kashikiya. She was the third daughter of Emperor Kimmei. Her mother was Soga no Iname's daughter, Soga no Kitashihime. Suiko was the younger sister of Emperor Yōmei. They had the same mother.
Events of Suiko's life.
Empress Suiko was a consort to her half-brother, Emperor Bidatsu, but after Bidatsu's first wife died she became his official consort and was given the title Ōkisaki (official consort of the emperor). She bore seven sons.
After Bidatsu's death, Suiko's brother, Emperor Yōmei, came to power for about two years before dying of illness. Upon Yōmei's death, another power struggle arose between the Soga clan and the Mononobe clan, with the Sogas supporting Prince Hatsusebe and the Mononobes supporting Prince Anahobe. The Sogas prevailed once again and Prince Hatsusebe acceded to the throne as Emperor Sushun in 587. However, Sushun began to resent the power of Soga no Umako, the head of the Soga clan, and Umako, perhaps out of fear that Sushun might strike first, had him assassinated by Yamatoaya no Ataikoma (東漢直駒) in 592. When asked to accede to the throne to fill the power vacuum that subsequently developed, Suiko became the first of what would be several examples in Japanese history where a woman was chosen to accede to the throne to avert a power struggle.
Suiko's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great Queen who rules all under heaven." Alternatively, Suiko might have been referred to as (ヤマト大王/大君) or the "Great Queen of Yamato."
Prince Shōtoku was appointed regent the following year. Although political power during Suiko's reign is widely viewed as having been wielded by Prince Shōtoku and Soga no Umako, Suiko was far from powerless. The mere fact that she survived and her reign endured suggests she had significant political skills.
In 599, an earthquake destroyed buildings throughout Yamato province in what is now Nara Prefecture.
Suiko's refusal to grant Soga no Umako's request that he be granted the imperial territory known as Kazuraki no Agata in 624 is cited as evidence of her independence from his influence. Some of the many achievements under Empress Suiko's reign include the official recognition of Buddhism by the issuance of the Flourishing Three Treasures Edict in 594. Suiko was also one of the first Buddhist monarchs in Japan and had taken the vows of a nun shortly before becoming empress.
The reign of this empress was marked by the opening of relations with the Sui court in 600, the adoption of the Twelve Level Cap and Rank System in 603 and the adoption of the Seventeen-article constitution in 604.
The adoption of the Sexagenary cycle calendar ("Jikkan Jūnishi") in Japan is attributed to Empress Suiko in 604. 
At a time when imperial succession was generally determined by clan leaders, rather than the emperor, Suiko left only vague indications of succession to two candidates while on her deathbed. One, Prince Tamura, was a grandson of Emperor Bidatsu and was supported by the main line of Sogas, including Soga no Emishi. The other, Prince Yamashiro, was a son of Prince Shōtoku and had the support of some lesser members of the Soga clan. After a brief struggle within the Soga clan in which one of Prince Yamashiro's main supporters was killed, Prince Tamura was chosen and he acceded to the throne as Emperor Jomei in 629.
Empress Suiko ruled for 35 years. Although there were seven other reigning empresses, their successors were most often selected from amongst the males of the paternal Imperial bloodline, which is why some conservative scholars argue that the women's reigns were temporary and that male-only succession tradition must be maintained in the 21st century. Empress Gemmei, who was followed on the throne by her daughter, Empress Genshō, remains the sole exception to this conventional argument.
The actual site of Suiko's grave is known. This empress is traditionally venerated at a memorial Shinto shrine ("misasagi") at Osaka.
The Imperial Household Agency designates this location as Suiko's mausoleum. It is formally named "Shinaga no Yamada no misasagi".

</doc>
<doc id="10421" url="http://en.wikipedia.org/wiki?curid=10421" title="Empress Kōgyoku">
Empress Kōgyoku

Empress Kōgyoku (皇極天皇, Kōgyoku-tennō, 594–661), also known as Empress Saimei (斉明天皇, Saimei-tennō), was the 35th and 37th monarch of Japan, according to the traditional order of succession.
Kōgyoku's reign spanned the years from 642–645. Her reign as Saimei encompassed 655–661. In other words, 
The two reigns of this one woman spanned the years from 642 through 661.
In the history of Japan, Kōgyoku/Saimei was the second of eight women to take on the role of empress regnant. The sole female monarch before Kōgyoku/Saimei was (a) Suiko"-tennō". The six women sovereigns reigning after Kōgyoku/Saimei were (b) Jitō, (c) Gemmei, (d) Genshō, (e) Kōken/Shōtoku, (f) Meishō, and (g) Go-Sakuramachi.
Traditional narrative.
Before her ascension to the Chrysanthemum Throne, her personal name ("imina") was Takara (宝). As empress, her name would have been "Ametoyo Takara Ikashi Hitarashi hime.
Princess Takara ("Takara no miko") was a great-granddaughter of Emperor Bidatsu. She became the wife and Empress consort of her uncle Emperor Jomei. The Imperial marriage produced three children: 
Events in Kōgyoku's reign.
During her first reign the Soga clan seized power. Her son Naka no Ōe planned a coup d'état and slew Soga no Iruka at the court in front of her throne. The Empress, shocked by this incident, abdicated the throne.
Kōgyoku's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great queen who rules all under heaven." Alternatively, Kōgyoku might have been referred to as (ヤマト大王/大君) or the "Great Queen of Yamato."
Empress Kōgyoku reigned for four years. The years of Kōgyoku's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – was yet to be initiated during her son's too-brief reign.
In this context, Brown and Ishida's translation of "Gukanshō" offers an explanation about the years of Empress Jitō's reign which muddies a sense of easy clarity in the pre-Taiho time-frame: 
The years of Kōgyoku's reign are not more specifically identified by more than one era name or "nengō" which was an innovation of Kōtoku's brief reign.
Events in Saimei's reign.
When Kōtoku died, his designated heir was Naka no Ōe. When Naka no Ōe's mother re-ascended, he continued in the role of her heir and crown prince. In this role, he could and did remain active in the political life of Japan.
In the fifth year of Saimei's reign, Paekche in Korea was destroyed in 660. Japan assisted Paekche loyals in an attempt to aid the revival of Paekche dynasty. Early in 661, Saimei responded to the situation by leaving her capital in Yamato province. Her plan was to lead a military expedition to Korea. The empress stayed in Ishiyu Temporary Palace in Iyo province, today Dōgo Onsen. In May she arrived at Asakura Palace in the north part of Tsukushi province in Kyūshū, today a part of Fukuoka prefecture. The allied army of Japan and Paekche was preparing for war against Silla, but the death of the empress thwarted those plans. In 661, Saimei died in the Asakura Palace before the army departed to Korea. In October her body was brought from Kyūshū by sea to Port Naniwa-zu (today Osaka city); and her state funeral was held in early November.
Empress Saimei ruled for seven years. The years of Saimei's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – languished until Mommu reasserted an imperial right by proclaiming the commencement of Taihō in 701.
The actual site of Kōgyoku/Saimei's grave is known, having been identified as the Kengoshizuka tomb in the village of Asuka, Nara Prefecture. This empress is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara.
The Imperial Household Agency designates this location as Kōgyoku/Seimei's mausoleum. It is formally named "Ochi-no-Okanoe no misasagi".
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Kōgyoku's reign, this apex of the "Daijō-kan" included:
The "kugyō" during Saimei's reign included:

</doc>
<doc id="10422" url="http://en.wikipedia.org/wiki?curid=10422" title="Emperor Kōtoku">
Emperor Kōtoku

Emperor Kōtoku (孝徳天皇, Kōtoku-tennō, 596 – November 24, 654) was the 36th emperor of Japan, according to the traditional order of succession.
The years of his reign lasted from 645 through 654.
Traditional narrative.
Before Kōtoku ascension to the Chrysanthemum Throne, his personal name ("imina") was Karu (軽) or Prince Karu (軽皇子, Karu-no-Ōji).
He enacted the Taika Reform Edicts.
He was a descendant of Emperor Bidatsu. He was a son of Chinu no ōkimi (Prince Chinu, 茅渟王) by Kibitsuhime no ōkimi (Princess Kibitsuhime, 吉備姫王). Empress Kōgyoku was his elder sister from the same parents. Chinu was a son of Prince Oshisaka hikohito no ōe, whose father was the Emperor Bidatsu. He had at least three consorts including his Empress, Hashihito no Himemiko (Princess Hashihito), the daughter of Emperor Jomei and his sister Empress Kōgyoku.
He ruled from July 12, 645 until his death in 654.
In 645 he ascended to the throne two days after Prince Naka no Ōe (Emperor Tenji) assassinated Soga no Iruka in the court of Kōgyoku. Kōgyoku abdicated in favor of her son and crown prince, Naka no Ōe, but Naka no Ōe insisted Kōtoku should ascend to the throne instead.
Kōtoku's contemporary title would not have been "tennō", as most historians believe this title was not introduced until the reigns of Emperor Tenmu and Empress Jitō. Rather, it was presumably "Sumeramikoto" or "Amenoshita Shiroshimesu Ōkimi" (治天下大王), meaning "the great king who rules all under heaven." Alternatively, Kōtoku might have been referred to as (ヤマト大王/大君) or the "Great King of Yamato."
According to the "Nihonshoki", he was of gentle personality and was in favor of Buddhism.
In 645 he created a new city in the area called Naniwa, and moved the capital from Yamato province to this new city (see Nara). The new capital had a sea port and was good for foreign trade and diplomatic activities.
In 653 Kōtoku sent an embassy to the court of the Tang Dynasty in China, but some of the ships were lost "en route".
Naka no Ōe held the rank of crown prince and was the de facto leader of the government. In 653 Naka no Ōe proposed to move the capital again to Yamato province. Kōtoku denied. Naka no Ōe ignored the emperor's policy and moved to the former province. Many courtiers of the court including, Empress Hashihito, followed him. Kōtoku was left in the palace. In the next year he died because of illness. After his death, Naka no Ōe would not ascend to the throne. Instead, his mother and the sister of Kōtoku, the former Empress Kogyoku ascended to the throne under another name, Empress Saimei.
The system of "hasshō kyakkan" (eight ministries and a hundred offices) was first established during the reign of Emperor Kōtoku.
The actual site of Kōtoku's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Osaka.
The Imperial Household Agency designates this location as Kōtoku's mausoleum. It is formally named "Ōsaka-no-shinaga no misasagi".
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Kōtoko's reign, this apex of the "Daijō-kan" included: 
Eras of Kōtoku's reign.
The years of Kōtoku's reign are more specifically identified by more than one era name or "nengō".
Consorts and Children.
Empress: Princess Hashihito (間人皇女) (?–665), daughter of Emperor Jomei
"Hi": Abe no Otarashi-hime (阿部小足媛), daughter of Abe no Kurahashi-maro
"Hi": Saga no Chi-no-iratsume (蘇我乳娘), daughter of Soga no Kura-no-Yamada no Ishikawa-no-maro

</doc>
<doc id="10424" url="http://en.wikipedia.org/wiki?curid=10424" title="Emperor Tenji">
Emperor Tenji

Emperor Tenji (天智天皇, Tenji-tennō, 626 – January 7, 672), also known as Emperor Tenchi, was the 38th emperor of Japan, according to the traditional order of succession.
Tenji's reign spanned the years from 661 through 671.
Traditional narrative.
He was the son of Emperor Jomei, but was preceded as ruler by his mother Empress Saimei.
Prior to his accession, he was known as Prince Naka-no-Ōe (中大兄皇子, Naka-no-Ōe no Ōji).
Events of Tenji's life.
As prince, Naka no Ōe played a crucial role in ending the near-total control the Soga clan had over the imperial family. In 644, seeing the Soga continue to gain power, he conspired with Nakatomi no Kamatari and Soga no Kurayamada no Ishikawa no Maro to assassinate Soga no Iruka in what has come to be known as the Isshi Incident. Although the assassination did not go exactly as planned, Iruka was killed, and his father and predecessor, Soga no Emishi, committed suicide soon after. Following the Isshi Incident, Iruka's adherents dispersed largely without a fight, and Naka no Ōe was named heir apparent. He also married the daughter of his ally Soga no Kurayamada, thus ensuring that a significant portion of the Soga clan's power was on his side.
Events of Tenji's reign.
Naka no Ōe reigned as Emperor Tenji from 661 to 672.
Tenji was particularly active in improving the military institutions which had been established during the Taika reforms.
Death of the emperor.
Following his death in 672, there ensued a succession dispute between his fourteen children (many by different mothers). In the end, he was succeeded by his son, Prince Ōtomo, also known as Emperor Kōbun, then by Tenji's brother Prince Ōama, also known as Emperor Temmu. Almost one hundred years after Tenji's death, the throne passed to his grandson Emperor Kōnin.
The actual site of Tenji's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Yamashina-ku, Kyoto.
The Imperial Household Agency designates this location as Tenji's mausoleum. It is formally named "Yamashina no misasagi".
Poetry.
The "Man'yōshū" includes poems attributed to emperors and empresses; and according to Donald Keene, evolving "Man'yōshū" studies have affected the interpretation of even simple narratives like "The Three Hills." The poem was long considered to be about two male hills in a quarrel over a female hill, but scholars now consider that Kagu and Mimihashi might be female hills in love with the same male hill, Unebi. This still-unresolved enigma in poetic form is said to have been composed by Emperor Tenji while he was still Crown Prince during the reign of Empress Saimei:
One of his 31-letters poems was chosen by Fujiwara no Teika as the first in the very popular anthology "Hyakunin Isshu."
After his death, his wife, Empress Yamato wrote a song of longing about her husband.
"Kugyo".
The top court officials (公卿, Kugyō) during Emperor Tenji's reign included:
Prince Ōtomo (Ōtomo"-shinnō") was the favorite son of Emperor Tenji; and he was also the first to be accorded the title of "Daijō-daijin."
Non"-nengō" period.
The years of Tenji's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – languished until Mommu reasserted an imperial right by proclaiming the commencement of Taihō in 701.
In this context, Brown and Ishida's translation of "Gukanshō" offers an explanation about the years of Empress Jitō's reign which muddies a sense of easy clarity in the pre-Taiho time-frame:
Consorts and Children.
Empress: Empress Yamato (倭姫王) (?–?), daughter of Prince Furuhito-no-Ōe (son of Emperor Jomei).
"Hin": Soga no Ochi-no-iratsume (蘇我遠智娘) (?–651?), daughter of Soga no Kura-no-yamada no Ishikawa-no-maro
"Hin": Soga no Mei-no-iratsume (蘇我姪娘), daughter of Soga no Kura-no-yamada no Ishikawa-no-maro
"Hin": Soga no Hitachi-no-iratsume (蘇我常陸娘), daughter of Soga no Akae
"Hin": Abe no Tachibana-no-iratsume (阿部橘娘) (?–681), daughter of Abe no Kurahashi-maro
Court lady: Oshinumi no Shikibuko-no-iratsume (忍海色夫古娘)
Court lady: Koshi-no-michi no Iratsume (越道伊羅都売)
Court lady: Kurikuma no Kurohime-no-iratsume (栗隈黒媛娘)
Court lady ("Uneme"): Yakako-no-iratsume, a lower court lady from Iga (伊賀采女宅子娘) ("Iga no Uneme")

</doc>
<doc id="10425" url="http://en.wikipedia.org/wiki?curid=10425" title="Emperor Kōbun">
Emperor Kōbun

Emperor Kōbun (弘文天皇, Kōbun-tennō, 648 – August 21, 672) was the 39th emperor of Japan, according to the traditional order of succession.
Kōbun's reign lasted only a few months in 671–672.
Traditional narrative.
Emperor Kōbun was named the 39th emperor by the Meiji government in 1870; and since the late 19th century, he is known by the posthumous name accorded to him by Meiji scholars.
In his lifetime, he was known as Prince Ōtomo (大友皇子, "Ōtomo no ōji"). He was the favorite son of Emperor Tenji; and he was also the first to have been accorded the title of "Daijō-daijin."
Contemporary historians now place the reign of Emperor Kōbun between the reigns of Emperor Tenji and Emperor Temmu; but the "Nihongi," the "Gukanshō," and the "Jinnō Shōtōki" do not recognize this reign. Prince Ōtomo was only given his posthumous title and name in 1870.
The actual site of Kōbun's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Shiga.
The Imperial Household Agency designates this location as Kōbun's mausoleum. It is formally named "Nagara no Yamasaki no misasagi."
Non"-nengō" period.
The years of Kōbun's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – languished until Mommu reasserted an imperial right by proclaiming the commencement of Taihō in 701. 
In this context, Brown and Ishida's translation of "Gukanshō" offers an explanation about the years of Empress Jitō's reign which muddies a sense of easy clarity in the pre-Taiho time-frame: 
"Kugyo".
The top court officials (公卿, Kugyō) during Emperor Kōbun's reign included:
Consorts and Children.
Empress Consort: Princess Tōchi (十市皇女) (648?–678), a daughter of Emperor Temmu
Empress: Fujiwara no Mimimotoji (藤原耳面刀自), a daughter of Fujiwara no Kamatari
Emperor Kōbun had another son named Prince Yota (興多王), whose mother is unknown.

</doc>
<doc id="10426" url="http://en.wikipedia.org/wiki?curid=10426" title="Emperor Tenmu">
Emperor Tenmu

Emperor Tenmu (天武天皇, Tenmu-tennō, c. 631 – October 1, 686) was the 40th emperor of Japan, according to the traditional order of succession.
Tenmu's reign lasted from 672 until his death in 686.
Traditional narrative.
Tenmu was the youngest son of Emperor Jomei and Empress Saimei, and the younger brother of the Emperor Tenji. His name at birth was Prince Ōama (大海人皇子:Ōama no ōji). He was succeeded by Empress Jitō, who was both his niece and his wife.
During the reign of his elder brother, Emperor Tenji, Tenmu was forced to marry several of Tenji's daughters because Tenji thought those marriages would help to strengthen political ties between the two brothers. The nieces he married included Princess Unonosarara, today known as the Empress Jitō, and Princess Ōta. Tenmu also had other consorts whose fathers were influential courtiers.
Tenmu had many children, including his crown prince Kusakabe by Princess Unonosarara; Princess Tōchi; Prince Ōtsu and Princess Ōku by Princess Ōta (whose father also was Tenji); and Prince Toneri, the editor of "Nihonshoki" and father of Emperor Junnin. Through Prince Kusakabe, Tenmu had two emperors and two empresses among his descendents. Empress Shōtoku was the last of these imperial rulers from his lineage.
Events of Tenmu's life.
Emperor Tenmu is the first monarch of Japan, to whom the title "tennō" was assigned contemporaneously—not only by later generations.
The only document on his life was "Nihonshoki". However, it was edited by his son, Prince Toneri, and the work was written during the reigns of his wife and children, causing one to suspect its accuracy and impartiality.
Tenmu's father died while he was young, and he grew up mainly under the guidance of Empress Saimei. He was not expected to gain the throne, because his brother Tenji was the crown prince, being the older son of their mother, the reigning empress.
After Tenji ascended to the throne, Tenmu was appointed crown prince. This was because Tenji had no appropriate heir among his sons at that time, as none of their mothers was of a rank high enough to give the necessary political support. Tenji was suspicious that Tenmu might be so ambitious as to attempt to take the throne, and felt the necessity to strengthen his position through politically advantageous marriages.
Tenji was particularly active in improving the military institutions which had been established during the Taika reforms.
In his old age, Tenji had a son, Prince Ōtomo, by a low-ranking consort. Since Ōtomo had weak political support from his maternal relatives, the general wisdom of the time held that it was not a good idea for him to ascend to the throne, yet Tenji was obsessed with the idea.
In 671 Tenmu felt himself to be in danger and volunteered to resign the office of crown prince to become a monk. He moved to the mountains in Yoshino, Yamato province (now in Yoshino, Nara), officially for reasons of seclusion. He took with him his sons and one of his wives, Princess Unonosarara, a daughter of Tenji. However, he left all his other consorts at the capital, Omikyō in Ōmi Province (today in Otsu, Shiga).
A year later, (in 672) Tenji died and Prince Ōtomo ascended to the throne as Emperor Kōbun. Tenmu assembled an army and marched from Yoshino to the east, to attack the capital of Omikyō in a counterclockwise movement. They marched through Yamato, Iga and Mino provinces to threaten Omikyō in the adjacent province. The army of Tenmu and the army of the young Emperor Kōbun fought in the northwestern part of Mino (nowadays Sekigahara, Gifu). Tenmu's army won and Kōbun committed suicide (Jinshin incident).
As might be expected, Emperor Tenmu was no less active than former-Emperor Tenji in improving the Taika military institutions. Tenmu's reign brought many changes, such as: (1) a centralized war department was organized; (2) the defenses of the Inner Country near the Capital were strengthened; (3) forts and castles were built near Capital and in the western parts of Honshū—and in Kyushu; (4) troops were reviewed; and all provincial governors were ordered to complete the collection of arms and to study tactics.
In 673 Tenmu moved the capital back to Yamato province on the Kiymihara plain, naming his new capital Asuka. The Man'yōshū includes a poem written after the Jinshin conflict ended:
At Asuka, Emperor Tenmu was enthroned. He elevated Unonosarara to be his empress. Events of his reign include:
Tenmu reigned from this capital until his death in 686. The actual site of his grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara. The Imperial Household Agency designates this location as Tenmu's mausoleum. It is formally named "Hinokuma no Ōuchi no misasagi."
Buddhism.
In 675 Emperor Tenmu banned the consumption of animal meat (horse, cattle, dogs, monkeys, birds), due to the influence of Buddhism.
Politics.
In "Nihonshoki" Tenmu is described as a great innovator, but the neutrality of this description is doubtful, since the work was written under the control of his descendants. It seems clear, however, that Tenmu strengthened the power of the emperor and appointed his sons to the highest offices of his government, reducing the traditional influence of powerful clans such as the Ōtomo and Soga. He renewed the system of "kabane", the hereditary titles of duty and rank, but with alterations, including the abolition of some titles. Omi and Muraji, the highest kabane in the earlier period, were reduced in value in the new hierarchy, which consisted of eight kinds of kabane. Each clan received a new kabane according to its closeness to the imperial bloodline and its loyalty to Tenmu.
Tenmu attempted to keep a balance of power among his sons. Once he traveled to Yoshino together with his sons, and there had them swear to cooperate and not to make war on each other. This turned out to be ineffective: one of his sons, Prince Ōtsu, was later executed for treason after the death of Tenmu.
Tenmu's foreign policy favored the Korean kingdom Silla, which took over the entire Korean peninsula in 676. After the unification of Korea by Silla, Tenmu decided to break diplomatic relations with the Tang dynasty of China, evidently in order to keep on good terms with Silla.
Tenmu used religious structures to increase the authority of the imperial throne. During his reign there was increased emphasis on the tie between the imperial household and the Grand Shrine of Ise (dedicated to the ancestor goddess of the emperors, Amaterasu) by sending his daughter Princess Oku as the newly established Saiō of the shrine, and several festivals were financed from the national budget. He also showed favor to Buddhism, and built several large temples and monasteries. It is said that Tenmu asked that each household was encouraged to build an altar with a dais where a Buddha-image and a sutra could be placed so that family worshiping could be held, thus inventing the butsudan. On the other hand, all Buddhist priests, monks and nuns were controlled by the state, and no one was allowed to become a monk without the state's permission. This was aimed at preventing cults and stopping farmers from turning into priests.
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Tenmu's reign, this apex of the "Daijō-kan" included:
Era of Tenmu's reign.
The years of Tenmu's reign were marked by only one era name or "nengō" which was proclaimed in the final months of the emperor's life; and "Shuchō" ended with Tenmu's death.
Non"-nengō" period.
The early years of Tenmu's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – was discontinued during these years, but it was reestablished briefly in 686. The use of "nengō" languished yet again after Tenmu's death until Emperor Mommu reasserted an imperial right by proclaiming the commencement of Taihō in 701.
In this context, Brown and Ishida's translation of "Gukanshō" offers an explanation about the years of Empress Jitō's reign which muddies a sense of easy clarity in the pre-Taihō time-frame:
Wives and Children.
Empress: Princess Uno-no-sarara (鸕野讃良皇女) (Empress Jitō) (645–703)
"Hi": Princess Ōta (大田皇女) (644–667), daughter of Emperor Tenji
"Hi": Princess Ōe (大江皇女) (?–699), daughter of Emperor Tenji
"Hi": Princess Niitabe (新田部皇女) (?–699), daughter of Emperor Tenji
"Bunin": Fujiwara no Hikami-no-iratsume (藤原氷上娘) (?–682), daughter of Fujiwara no Kamatari
"Bunin": Soga no Ōnu-no-iratsume (蘇我大蕤娘) (?–724), daughter of Soga no Akae
"Bunin": Fujiwara no Ioe-no-iratsume (藤原五百重娘), daughter of Fujiwara no Kamatari
Court lady: Nukata no Ōkimi (額田王)
Court lady: Munakata no Amako-no-iratsume (胸形尼子娘), daughter of Munakata-no-Kimi Tokuzen
Court lady: Shishihito no Kajihime-no-iratsume (宍人梶媛娘), daughter of Shishihito-no-Omi Ōmaro

</doc>
<doc id="10427" url="http://en.wikipedia.org/wiki?curid=10427" title="Empress Jitō">
Empress Jitō

Empress Jitō (持統天皇, Jitō-tennō, 645 – 13 January 703) was the 41st monarch of Japan, according to the traditional order of succession.
Jitō's reign spanned the years from 686 through 697.
In the history of Japan, Jitō was the third of eight women to take on the role of empress regnant. The two female monarchs before Jitō were (a) Suiko and (b) Kōgyoku/Saimei. The five women sovereigns reigning after Jitō were (c) Gemmei, (d) Genshō, (e) Kōken/Shōtoku, (f) Meishō, and (g) Go-Sakuramachi.
Traditional narrative.
Empress Jitō was the daughter of Emperor Tenji. Her mother was Ochi-no-Iratsume, the daughter of Minister Ō-omi Soga no Yamada-no Ishikawa Maro. She was the wife of Emperor Temmu, who was Tenji's half brother by another woman, and she succeeded him on the throne.
Empress Jitō's given name was Unonosarara or Unonosasara (鸕野讚良), or alternately Uno.
Events of Jitō's reign.
Jitō took responsibility for court administration after the death of her husband, Emperor Temmu, who was also her uncle. She acceded to the throne in 687 in order to ensure the eventual succession of her son, Kusakabe-shinnō. Throughout this period, Empress Jitō ruled from the Fujiwara Palace in Yamato.
Prince Kusabake was named as crown prince to succeed Jitō, but he died at a young age. Kusabake's son, Karu-no-o, was then named as Jitō's successor. He eventually would become known as Emperor Mommu.
Empress Jitō reigned for eleven years. Although there were seven other reigning empresses, their successors were most often selected from amongst the males of the paternal Imperial bloodline, which is why some conservative scholars argue that the women's reigns were temporary and that male-only succession tradition must be maintained in the 21st century. Empress Gemmei, who was followed on the throne by her daughter, Empress Genshō, remains the sole exception to this conventional argument.
In 697, Jitō abdicated in Mommu's favor; and as a retired sovereign, she took the post-reign title "daijō-tennō." After this, her imperial successors who retired took the same title after abdication.
Jitō continued to hold power as a cloistered ruler, which became a persistent trend in Japanese politics.
The actual site of Jitō's grave is known. This empress is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara.
The Imperial Household Agency designates this location as Jitō's mausoleum. It is formally named "Ochi-no-Okanoe no misasagi".
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Jitō's reign, this apex of the "Daijō-kan" included:
Non"-nengō" period.
Jitō's reign is not linked by scholars to any era or nengō. The Taika era innovation of naming time periods – "nengō" – languished until Mommu reasserted an imperial right by proclaiming the commencement of Taihō in 701.
However, Brown and Ishida's translation of "Gukanshō" offers an explanation which muddies a sense of easy clarity:
Poetry.
Man'yōshū poetry.
The Man'yōshū includes a poem said to have been composed by Jitō
Hyakunin Isshu poetry.
One of the poems attributed to Empress Jitō was selected by Fujiwara no Teika for inclusion in the very popular anthology "Hyakunin Isshu."

</doc>
<doc id="10428" url="http://en.wikipedia.org/wiki?curid=10428" title="Emperor Monmu">
Emperor Monmu

Emperor Monmu (文武天皇, Monmu-tennō, 683–707) was the 42nd emperor of Japan, according to the traditional order of succession.
Monmu's reign spanned the years from 697 through 707.
Traditional narrative.
Before his ascension to the Chrysanthemum Throne, his personal name ("imina") was Karu"-shinnō".
He was a grandson of Emperor Temmu and Empress Jitō. He was the second son of Prince Kusakabe. Monmu's mother was Princess Abe, a daughter of Emperor Tenji. Monmu's mother would later accede to the throne herself, and she would be known as Empress Gemmei.
Events of Monmu's life.
Karu"-shinnō" was only six years old when his father, Crown Prince Kusakabe, died.
Emperor Monmu ruled until his death in 707, at which point he was succeeded by his mother, Empress Gemmei, who was also his first cousin once removed and his first cousin twice removed. He left a young son by Fujiwara no Miyako, a daughter of Fujiwara no Fuhito: Obito no miko (Prince Obito), who eventually became Emperor Shōmu.
Emperor Monmu's reign lasted 10 years. He died at the age of 25.
The actual site of Monmu's grave is known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") at Nara.
The Imperial Household Agency designates this location as Monmu's mausoleum. It is formally named "Hinokuma no Ako no oka no e no misasagi".
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Monmu's reign, this apex of the "Daijō-kan" included:
Eras of Monmu's reign.
Conventional modern scholarship seems to have determined that the years of Monmu's reign are encompassed within more than one era name or "nengō".
Non"-nengō" period.
The initial years of Monmu's reign are not linked by scholars to any era or "nengō". The Taika era innovation of naming time periods – "nengō" – languished until Monmu reasserted an imperial right by proclaiming the commencement of Taihō in 701.
In this context, Brown and Ishida's translation of "Gukanshō" offers an explanation about the years of Empress Jitō's reign which muddies a sense of easy clarity in the pre-Taiho time-frame:
Consorts and Children.
"Bunin": Fujiwara no Miyako (藤原宮子) (?–754), daughter of Fujiwara no Fuhito
"Hin": Ki no Kamado-no-iratsume (紀竃門娘)
"Hin": Ishikawa no Tone-no-iratsume (石川刀子娘)

</doc>
<doc id="10430" url="http://en.wikipedia.org/wiki?curid=10430" title="Empress Genshō">
Empress Genshō

Empress Genshō (元正天皇, Genshō-tennō, 683 – May 22, 748) was the 44th monarch of Japan, according to the traditional order of succession.
Genshō's reign spanned the years 715 through 724.
In the history of Japan, Genshō was the fifth of eight women to take on the role of empress regnant. The four female monarchs before Genshō were: (a) Suiko, (b) Kōgyoku, (c) Jitō and (d) Gemmei. The three women sovereigns reigning after Genshō were (e) Kōken, (f) Meishō, and (g) Go-Sakuramachi.
Genshō was the only Japanese empress regnant to inherit her title from another empress regnant, rather than a male predecessor.
Traditional narrative.
Before her ascension to the Chrysanthemum Throne, her personal name ("imina") was Hidaka"-hime".
Genshō was an elder sister of Emperor Mommu and daughter of Prince Kusakabe and his wife who later became Empress Gemmei. Therefore she was a granddaughter of Emperor Temmu and Empress Jitō by her father and a granddaughter of Emperor Tenji through her mother.
Events of Genshō's life.
Empress Gensho's succession to the throne was intended as a regency until Prince Obito, the son of her deceased younger brother Mommu, was mature enough to ascend the throne. Obito would later become the Emperor Shōmu.
Obito was appointed Crown Prince in 714 by Empress Gemmei. In the next year, 715, Empress Gemmei, then in her fifties, abdicated in favor of her daughter Gensho. Obito was then 14 years old.
Obito remained the crown prince, heir to the new empress. Fujiwara no Fuhito, the most powerful courtier in Gemmei's court, remained at his post until his death in 720. After his death, Prince Nagaya, a grandson of Temmu and the Empress Gensho's cousin, seized power. This power shift was a background for later conflicts between Nagaya and Fuhito's four sons during the reign of Emperor Shōmu (formerly Prince Obito).
Under Gensho's reign, the "Nihonshoki" was finished in 720. This was the first Japanese history book. Organization of the law system known as the "ritsuryo" was continued under the initiatives of Fuhito until his death. These laws and codes were edited and enacted by Fujiwara no Nakamaro, a grandson of Fuhito, and published as "Yoro ritsuryo" under the name of Fuhito. The taxation system which had been introduced by Empress Jitō in the late 7th century began to malfunction. To compensate for the decreased tax revenue, the "Act of possession in three generations", an initiative of Prince Nagaya, was enacted in 723. Under this act, people were allowed to possess a newly cultivated field once every three generations. In the fourth generation, the right of possession would revert to the national government. This act was intended to motivate new cultivation, but it only remained in effect for about 20 years.
Empress Genshō reigned for nine years. Although there were seven other reigning empresses, their successors were most often selected from amongst the males of the paternal Imperial bloodline, which is why some conservative scholars argue that the women's reigns were temporary and that male-only succession tradition must be maintained in the 21st century. Empress Gemmei, who was succeeded by her daughter, remains the sole exception to this conventional argument.
In 724, Gensho abdicated in favor of her nephew, who would be known as Emperor Shōmu. Genshō lived for 25 years after she stepped down from the throne. She never married and had no children. She died at age 65.
Empress Genshō's grave is located in Nara. This empress is traditionally venerated at a memorial Shinto shrine ("misasagi"), also in Nara. The Imperial Household Agency has designated this location as Mommu's mausoleum, and has been formally named "Nahoyama no nishi no misasagi". The Imperial tomb can be visited today in Narazaka-cho, Nara City.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Genshō's reign, this apex of the "Daijō-kan" included:
Eras of Genshō's reign.
The years of Genshō's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="10431" url="http://en.wikipedia.org/wiki?curid=10431" title="Empress Kōken">
Empress Kōken

Empress Kōken (孝謙天皇, Kōken-tennō, 718 – August 28, 770), also known as Empress Shōtoku (称徳天皇, Shōtoku-tennō), was the 46th (with Empress Kōken name) and the 48th monarch of Japan (with Empress Shōtoku name), according to the traditional order of succession. Empress Kōken first reigned from 749 to 758, then, following the Fujiwara no Nakamaro Rebellion, she reascended the throne as Empress Shōtoku from 765 until her death in 770.
Empress Kōken was involved in the Rasputin-like affair with priest Dōkyō and appointed him Grand Minister in 764. In 766 he was promoted to Hōō (priestly emperor) and in 770 had tried to ascend to throne by himself. Death of Empress and resistance of the aristocracy destroyed his plans. 
This incident was a reason for the later move of the Japanese capital from Nara (Heijō).
In the history of Japan, Kōken/Shōtoku was the sixth of eight women to take on the role of empress regnant. The five female monarchs before Kōken/Shōtoku were (a) Suiko, (b) Kōgyoku/Saimei, (c) Jitō, (d) Gemmei, and (e) Genshō; and the two women sovereigns reigning after Kōken/Shōtoku were (f) Meishō, and (g) Go-Sakuramachi.
Traditional narrative.
Empress Kōken's personal name ("imina") was Abe (阿倍). Her father was Emperor Shōmu, and her mother was Empress Kōmyō.
Kōken is traditionally venerated at her tomb; the Imperial Household Agency designates Takano no Misasagi (高野陵, Takano Imperial Mausoleum)
, in Nara, Nara, as the location of Kōken's mausoleum. The site is publicly accessible.
Eras of her reigns.
The years of Kōken's reign are more specifically identified by more than one era name.
The years of Shōtoku's reign are more specifically identified by more than one era name.
Legacy.
Koken's reign was turbulent, and she survived coup attempts by both Tachibana Naramaro and Fujiwara no Nakamaro. Today, she is remembered chiefly for her alleged affair with a Buddhist monk named Dōkyō (道鏡), a man she honored with titles and power. An oracle from Usa Shrine, the shrine of the kami Hachiman (八幡) in Usa, is said to have proclaimed that the monk should be made emperor; but when the empress sent Wake no Kiyomaro (和気清麻呂) to verify the pronouncement, Hachiman decreed that only one of imperial blood should ascend to the throne.
As with the seven other reigning empresses whose successors were most often selected from amongst the males of the paternal imperial bloodline, she was followed on the throne by a male cousin, which is why some conservative scholars argue that the women's reigns were temporary and that male-only succession tradition must be maintained in the 21st century. Empress Gemmei, who was followed on the throne by her daughter, Empress Genshō, remains the sole exception to this conventional argument.
She is also known for sponsoring the Hyakumantō Darani, one of the largest productions of printed works in early Japan.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Kōken's reign, this apex of the "Daijō-kan" included:
The "kugyō" during Shōtoku's reign included:

</doc>
<doc id="10432" url="http://en.wikipedia.org/wiki?curid=10432" title="Emperor Junnin">
Emperor Junnin

Emperor Junnin (淳仁天皇, Junnin-tennō, 733 – November 10, 765) was the 47th emperor of Japan, according to the traditional order of succession. The seventh son of Prince Toneri and a grandson of Emperor Temmu, his reign spanned the years 758 to 764.
Traditional narrative.
Before his ascension to the throne, his name ("imina") was Ōi"-shinnō" (Ōi-no-ō). He was the seventh son of Prince Toneri, a son of Emperor Temmu. And although his father died when he was three, he was not given any rank or office at the court. In the older Japanese documents, he is usually referred to as Haitei (廃帝), the unthroned emperor. The posthumous name of Emperor Junnin was given by Emperor Meiji a thousand years later.
Ascension and reign.
In 757 the Empress Kōken, his third cousin appointed him to be her crown prince instead of Prince Funado, who had been appointed to this position in the will of the Emperor Shōmu. In the tenth year of Kōken"-tennō" 's reign (称徳天皇10年), on December 7, 758 ("Tenpyō-shōhō 2, 1st day of the 8th month"), the empress abdicated and the succession ("senso") passed to her adopted son. Shortly afterwards, Emperor Jimmu is said to have ascended to the throne ("sokui"). In 760 ("Tenpyō-hōji 4"), additional coins were put into circulation—copper coins bearing the words "Mannen Ten-hō", silver coins bearing the words "Teihei Genhō", and gold coins bearing the words "Kaiki Shōhō".
The years of Junnin's reign, 758–765, are more specifically encompassed within a single era name or "nengō"."Tenpyō-hōji" Junnin seemingly had very little power and was a mere figurehead. In 764, six years after Empress Kōken had stepped down, the former empress reclaimed the throne during the Fujiwara no Nakamaro Rebellion, forcing Junnin to abdicate.
Death and mausoleum.
On November 10, 765 ("Tenpyō-jingo 1, 23rd day of the 10th month") the former emperor died while in exile. The site of Junnin's actual grave is unknown, and he is traditionally venerated at a memorial Shinto shrine ("misasagi") at Awaji. The Imperial Household Agency designates this location as Junnin's mausoleum: It is formally named "Awaji no misasagi".
Though Junnin had, technically, been emperor, he was not featured on the official List of Japanese Emperors until the late nineteenth century. In 1870, Emperor Meiji conferred the posthumous name and title by which Emperor Junnin is now known. His place in the traditional order of succession was confirmed at the same time as announcements about Emperor Kōbun and Emperor Chukyo were made public.
Kugyō.
"Kugyō" (公卿) is a collective term for the few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. In general, this elite group included only three or four men at a time, and they were hereditary courtiers whose experience and background would have brought them to the pinnacle of their careers. During Junnin's reign, this group of "Daijō-kan" included:

</doc>
<doc id="10434" url="http://en.wikipedia.org/wiki?curid=10434" title="Emperor Heizei">
Emperor Heizei

Emperor Heizei (平城天皇, Heizei-tennō, 773 – August 5, 824), also known as Heijō-tennō. was the 51st emperor of Japan, according to the traditional order of succession. Heizei's reign lasted from 806 to 809.
Traditional narrative.
He was the eldest son of the Emperor Kammu and empress Fujiwara no Otomuro. Heizei had three Empresses and seven sons and daughters.
Heizei is traditionally venerated at his tomb; the Imperial Household Agency designates Yamamomo no Misasagi (楊梅陵, Yamamomo Imperial Mausoleum)
, in Nara, as the location of Heizei's mausoleum. The site is publicly accessible. Although one of the largest kofun monuments in Japan, archaeological investigations in 1962–1963 indicate that it was constructed in the early 5th century, and that portions of it were destroyed during the construction of Heijo-kyō, calling into question the designation by the Imperial Household Agency.
Events of Heizei's life.
Before he ascended to the throne, his liaison with Fujiwara no Kusuko, the mother of his one consort, caused a scandal. Because of this scandal his father considered depriving him of the rank of crown prince.
His title Heizei was derived from the official name of the capital in Nara, Heizei Kyō.
During Heizei's reign, the bodyguards were reorganized; the existing Imperial Bodyguards became the Left Imperial Bodyguards, while the Middle Bodyguards became the Right Imperial Bodyguards. Both sides were given a new Senior Commander; at this time Heizei appointed Sakanoue no Tamuramaro (758–811) as Senior Commander of the Imperial Bodyguards of the Right. Under Emperor Kammu, Tamuramaro had been appointed as shogun of a military expedition against the Emishi.
Era of Heizei's reign.
The years of Heizei's reign are encompassed within one era name ("nengō").
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Heizei's reign, this apex of the "Daijō-kan" included:
When the daughter of a "chūnagon" became the favored consort of the Crown Prince Ate (later known as Heizei"-tennō"), her father's power and position in court was affected. Kammu disapproved of Fujiwara no Kusuko (藤原薬子, ?-810), former wife of Fujiwara no Tadanushi; and Kammu had her removed from his son's household. After Kammu died, Heizei restored this one-time favorite as part of his household; and this distinction had consequences.
Consorts and children.
Empress (posthumously elevated "Kōgō"): Fujiwara no Tarashiko (藤原帯子) (?–794), also known as Taishi, daughter of Fujiwara no Momokawa
"Hi": Imperial Princess Asahara (朝原内親王) (779–817), daughter of Emperor Kammu
"Hi": Imperial Princess Ōyake (大宅内親王) (?–849), daughter of Emperor Kammu
"Shōshi" Court lady ("Naishi-no-kami"): Fujiwara no Kusuko (藤原薬子, ?–810), former wife of "Chūnagon" Fujiwara no Tadanushi
Court lady: Fujii no Fujiko/"Tōshi" (葛井藤子), daughter of Fujii no Michiyori
Court lady: Ise no Tsugiko (伊勢継子) (772–812), daughter of Ise no Ōna
Court lady: Ki no Iokazu (紀魚員), daughter of Ki no Kotsuo

</doc>
<doc id="10435" url="http://en.wikipedia.org/wiki?curid=10435" title="Emperor Saga">
Emperor Saga

Emperor Saga (嵯峨天皇, Saga-tennō, (October 10, 786 – August 24, 842) was the 52nd emperor of Japan, according to the traditional order of succession. Saga's reign spanned the years from 809 through 823.
Traditional narrative.
Saga was the second son of Emperor Kanmu and Fujiwara no Otomuro. His personal name was Kamino (神野). Saga was an "accomplished calligrapher" able to compose in Chinese who held the first imperial poetry competitions ("naien"). According to legend, he was the first Japanese emperor to drink tea.
Saga is traditionally venerated at his tomb; the Imperial Household Agency designates Saganoyamanoe no Misasagi (嵯峨山上陵, Saganoyamanoe Imperial Mausoleum)
, in Ukyō-ku, Kyoto, as the location of Saga's mausoleum.
Events of Saga's life.
Soon after his enthronement, Saga himself took ill. Saga's untimely health problems provided former-Emperor Heizei with a unique opportunity to foment a rebellion; however, forces loyal to Emperor Saga, led by "taishōgun" Sakanoue no Tamuramaro, quickly defeated the Heizei rebels which thus limited the adverse consequences which would have followed any broader conflict. This same Tamuramaro is remembered in Aomori's annual "nebuta" or "neputa" matsuri which feature a number of gigantic, specially-constructed, illuminated paper floats. These great lantern-structures are colorfully painted with mythical figures; and teams of men carry them through the streets as crowds shout encouragement. This early ninth century military leader is commemorated in this way because he is said to have ordered huge illuminated lanterns to be placed at the top of hills; and when the curious Emishi approached these bright lights to investigate, they were captured and subdued by Tamuramaro's men.
Eras of Saga's reign.
The years of Saga's reign are more specifically identified by more than one era name ("nengō").
Legacy.
In ancient Japan, there were four noble clans, the "Gempeitōkitsu" (源平藤橘). One of these clans, the Minamoto clan are also known as Genji (源氏), and of these, the Saga Genji (嵯峨源氏) are descended from 52nd emperor Saga. Saga's grandson, Minamoto no Tōru, is thought to be an inspiration for the protagonist of the novel "The Tale of Genji".
In the 9th century, Emperor Saga made a decree prohibiting meat consumption except fish and birds. This remained the dietary habit of Japanese until the introduction of European dietary customs in the 19th century. 
Emperor Saga played an important role as a stalwart supporter of the Buddhist monk Kūkai. The emperor helped Kūkai to establish the Shingon School of Buddhism by granting him the Toji temple in the capital Heian-kyō (present day Kyoto).
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Saga's reign (809–823), this "kugyō" included:
Consorts and children.
Saga had 49 children by at least 30 different women. Many of the children received the surname Minamoto, thereby removing them from royal succession.
Empress: Tachibana no Kachiko (橘嘉智子) (786–850), also known as Empress Danrin (檀林皇后, Danrin-kōgō), daughter of Tachibana no Kiyotomo (橘清友).
"Hi"(deposed): Princess Takatsu (高津内親王) (?–841), daughter of Emperor Kammu
"Hi": Tajihi no Takako (多治比高子) (787–825), daughter of Tajihi no Ujimori (多治比氏守)
"Bunin": Fujiwara no Onatsu (藤原緒夏) (?–855), daughter of Fujiwara no Uchimaro (藤原内麻呂)
"Nyōgo": Ōhara no Kiyoko (大原浄子) (?–841), daughter of Ōhara no Yakatsugu (大原家継)
"Nyōgo": Princess Katano (交野女王), daughter of Prince Yamaguchi (山口王)
"Nyōgo": Kudara no Kimyō (百済貴命) (?–851), daughter of Kudara no Shuntetsu (百済俊哲)
"Koui": Iidaka no Yakatoji (飯高宅刀自)
"Koui": Akishino no Koko (秋篠高子/康子), daughter of Akishino no Yasuhito (秋篠安人)
"Koui": Yamada no Chikako (山田近子)
Court lady ("Naishi-no-kami"): Kudara no Kyomyō (百済慶命) (?–849), daughter of Kudara no Kyōshun (百済教俊)
Court lady: Takashina no Kawako (高階河子), daughter of Takashina no Kiyoshina (高階浄階)
Court lady: Fun'ya no Fumiko (文屋文子), daughter of Fun'ya no Kugamaro (文屋久賀麻呂)
Court lady: A daughter of Hiroi no Otona (広井弟名の娘)
Court lady: Fuse no Musashiko (布勢武蔵子)
Court lady: A daughter of Kamitsukeno clan (上毛野氏の娘)
Court lady("Nyoju"): A daughter of Taima no Osadamaro (当麻治田麻呂の娘)
Court lady: A daughter of Abe no Yanatsu (安部楊津の娘)
Court lady: Kasa no Tsugiko (笠継子), daughter of Kasa no Nakamori (笠仲守)
Court lady: A daughter of Tanaka clan (田中氏の娘)
Court lady: A daughter of Awata clan (粟田氏の娘)
Court lady: Ōhara no Matako (大原全子), daughter of Ōhara no Mamuro (大原真室)
Court lady: A daughter of Koreyoshi no Sadamichi (惟良貞道の娘)
Court lady: A daughter of Nagaoka no Okanari (長岡岡成の娘)
Court lady: A daughter of Ki clan (紀氏の娘)
Court lady: Kura no Kageko (内蔵影子)
Court lady: Kannabi no Iseko (甘南備伊勢子)
Court lady: Tachibana no Haruko (橘春子)
Court lady: Ōnakatomi no Mineko (大中臣峯子)
References.
嵯峨山上

</doc>
<doc id="10436" url="http://en.wikipedia.org/wiki?curid=10436" title="Emperor Junna">
Emperor Junna

Emperor Junna (淳和天皇, Junna-tennō, c. 785 – 11 June 840) was the 53rd emperor of Japan, according to the traditional order of succession. Junna reigned from 823 to 833.
Traditional narrative.
Junna had six Empresses and Imperial consorts and 13 Imperial sons and daughters. His personal name ("imina") was Ōtomo (大伴).
Junna is traditionally venerated at his tomb; the Imperial Household Agency designates Ōharano no Nishi no Minenoe no Misasagi (大原野西嶺上陵, Ōharano no Nishi no Minenoe Imperial Mausoleum)
, in Nishikyō-ku, Kyoto, as the location of Junna's mausoleum.
Eras of Junna's reign.
The years of Junna's reign are more specifically identified by more than one era name ("nengō").
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Junna's reign, this apex of the "Daijō-kan" included:
Consorts and children.
Empress: Imperial Princess "Shōshi"/Masako (正子内親王) (810–879), daughter of Emperor Saga
"Hi"(Empress as posthumous honors): Imperial Princess Koshi (高志内親王) (789–809), daughter of Emperor Kammu
"Nyogō": Nagahara no Motohime (永原原姫)
"Nyogō": Tachibana no Ujiko (橘氏子), daughter of Tachibana no Nagana
"Koui": Fujiwara no Kiyoko (藤原潔子), daughter of Fujiwara no Nagaoka
Court lady: Princess Otsugu (緒継女王) (787–847)
Court lady: Ōnakatomi no Yasuko (大中臣安子), daughter of Ōnakatomi no Fuchiio
Court lady: Ōno no Takako (大野鷹子), daughter of Ōno no Masao
Court lady: Tachibana no Funeko (橘船子), daughter of Tachibana no Kiyono
Court lady: Tajihi no Ikeko (丹犀池子), daughter of Tajihi no Kadonari
Court lady: Kiyohara no Haruko (清原春子), daughter of Kiyohara no Natsuno
Unknown lady

</doc>
<doc id="10438" url="http://en.wikipedia.org/wiki?curid=10438" title="Kōmyō">
Kōmyō

Komyo can refer to:

</doc>
<doc id="10439" url="http://en.wikipedia.org/wiki?curid=10439" title="Emperor Sukō">
Emperor Sukō

Emperor Sukō ((崇光天皇, "Sukō Tennō")) (May 25, 1334 – January 31, 1398) was the third of Ashikaga Pretenders during the Period of the Northern and Southern Courts in Japan. According to pre-Meiji scholars, his reign spanned the years from 1348 through 1351.
Genealogy.
His personal name was originally Masuhito (益仁;), but was later changed to Okihito (興仁).
His father was the Northern Pretender Emperor Kōgon. His predecessor, Emperor Kōmyō was his uncle, the younger brother of Emperor Kōgon.
Events of Sukō's life.
In his own lifetime, Sukō and those around him believed that he occupied the Chrysanthemum Throne from November 18, 1348 until November 26, 1351.
In 1348, he became Crown Prince. In the same year, he became Northern Emperor upon the abdication of Emperor Kōmyō. Although Emperor Kōgon ruled as cloistered Emperor, the rivalry between Ashikaga Takauji and Ashikaga Tadayoshi began, and in 1351, Takauji returned to the allegiance of the Southern Court, forcing Emperor Sukō to abdicate. This was intended to reunify the Imperial Line. However, the peace soon fell apart, and in 1352, the Southern Dynasty evacuated Kyoto, abducting with them Retired (Northern) Emperors Emperor Kōgon and Emperor Kōmyō as well as Sukō and the Crown Prince, Imperial Prince Naohito, the son of Emperor Kōgon. Because of this, Takauji made Emperor Kōgon's second son Imperial Prince Iyahito emperor (First Fushimi-no-miya).
Returning to Kyoto in 1357, Emperor Sukō's son Imperial Prince Yoshihito began to work with the Bakufu to be named Crown Prince, but the Bakufu instead decided to make Emperor Go-Kōgon's son (the future Emperor Go-En'yū) Crown Prince instead.
In 1398, Emperor Sukō died. But, 30 years after his death, in 1428, his great-grandson Hikohito (彦仁), as the adopted son of Emperor Shōkō, became Emperor Go-Hanazono, fulfilling Sukō's dearest wish. Sukō is enshrined at the "Daikōmyōji no misasagi" (大光明寺陵) in Fushimi-ku, Kyoto.
References.
</dl>

</doc>
<doc id="10440" url="http://en.wikipedia.org/wiki?curid=10440" title="Emperor Go-Kōgon">
Emperor Go-Kōgon

Emperor Go-Kōgon (後光厳天皇 "Go-Kōgon-tennō") (23 March 1338 – 12 March 1374) was the 4th of the Ashikaga Pretenders during the Period of the Northern and Southern Courts. According to pre-Meiji scholars, his reign spanned the years from 1352 through 1371.
This Nanboku-chō "sovereign" was named after his father Emperor Kōgon and "go-" (後), translates literally as "later;" and thus, he may be called the "Later Emperor Kōgon". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this would-be emperor may be identified as "Kōgon, the second", or as "Kōgon II."
Genealogy.
His personal name was Iyahito (弥仁).
He was the second son of the Northern Pretender Emperor Kōgon, and brother of his predecessor, Emperor Sukō. His mother was Hideko (秀子), daughter of Sanjō Kinhide (三条公秀)
Events of Go-Kōgon's life.
In his own lifetime, Go-Kōgon and those around him believed that he occupied the Chrysanthemum Throne from 25 September 1352 to 9 April 1371.
In 1351, Ashikaga Takauji briefly returned to the allegiance of the Southern Dynasty, causing the Southern Court to briefly consolidate control of the Imperial Line. However, this peace fell apart in 1352. On this occasion, the Southern Court abducted Retired (Northern) Emperors Emperor Kōgon and Emperor Kōmyō as well as Emperor Sukō and the Crown Prince, Imperial Prince Naohito, the son of Emperor Kōgon, from Kyoto to Yoshino, producing a state of affairs in which there was no Emperor in Kyoto.
Because of this, Imperial Prince Iyahito became emperor in 1352 with the support of Ashikaga Yoshiakira.
During this period, the Era of the Northern and Southern Courts, because of the antagonism between the two competing dynasties, public order in Kyoto was disturbed. The Southern Court repeatedly recaptured Kyoto. Emperor Go-Kōgon was forced to repeatedly flee from Kyoto to Ōmi Province and other places. Around the time that Ashikaga Yoshimitsu was named Shōgun (1368), the Southern Courts power weakened, and order was restored to Kyoto. Also around this time, the Emperor's authority began to show its weakness.
On 9 April 1371, he abdicated in favor of his son, who became the Northern Pretender Emperor Go-En'yū. He continued to rule as Cloistered Emperor until he died of illness on 12 March 1374. He is enshrined with other emperors at the imperial tomb called "Fukakusa no kita no misasagi" (深草北陵) in Fushimi-ku, Kyoto.
Eras of Go-Kōgon's reign.
The years of Go-Kōgon's reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="10441" url="http://en.wikipedia.org/wiki?curid=10441" title="Emperor Go-En'yū">
Emperor Go-En'yū

Emperor Go-En'yū (後円融天皇, "Go-En'yū-tennō") (11 January 1359 – 6 June 1393) was the "5th" of the Ashikaga Pretenders during the period of two courts. According to pre-Meiji scholars, his reign spanned the years from 1371 through 1382.
This Nanboku-chō "sovereign" was named after the 10th century Emperor En'yū and "go-" (後), translates literally as "later;" and thus, he may be called the "Later Emperor En'yū". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this would-be emperor may be identified as "En'yū, the second", or as "En'yū II."
Genealogy.
His personal name was Ohito (緒仁).
He was the second son of the fourth Northern Pretender Emperor Go-Kōgon. His mother was Fujiwara no Nakako (藤原仲子), daughter of Hirohashi Kanetsuna (広橋兼綱).
Events of Go-En'yū's life.
In his own lifetime, Go-En'yū and those around him believed that he occupied the Chrysanthemum Throne from 9 April 1371 to 24 May 1382.
In 1371, by Imperial Proclamation, he received the rank of "shinnō" (親王), or Imperial Prince (and potential heir). Immediately afterwards, he became emperor upon the abdication of his father, Emperor Go-Kōgon. There was said to be a disagreement between Go-Kōgon and the retired Northern Emperor Emperor Sukō over the Crown Prince. With the support of Hosokawa Yoriyuki, who controlled the Bakufu, Go-Kōgon's son became the Northern Emperor.
Until 1374, Go-Kōgon ruled as cloistered emperor. In 1368, Ashikaga Yoshimitsu was named Shōgun, and with his guardianship, the Imperial Court was stabilized. In 1382, upon abdicating to Emperor Go-Komatsu, his cloistered rule began. Having no actual power, he rebelled, attempting suicide and accusing Ashikaga Yoshimitsu and his consort Itsuko of adultery.
In 1392, peace with the Southern Court being concluded, the Period of the Northern and Southern Courts came to an end. On 6 June 1393, Go-En'yū died. He is enshrined with other emperors at the imperial tomb called "Fukakusa no kita no misasagi" (深草北陵) in Fushimi-ku, Kyoto.
Eras of Go-En'yū's reign.
The years of Go-En'yū's "Nanboku-chō" reign are more specifically identified by more than one era name or "nengō".
References.
</dl>

</doc>
<doc id="10442" url="http://en.wikipedia.org/wiki?curid=10442" title="Emperor Suizei">
Emperor Suizei

Emperor Suizei (綏靖天皇, Suizei-tennō), sometimes romanized as Suisei and known as "Kamu-nuna-kaha-mimi no mikoto"; was the second Emperor of Japan, according to the traditional order of succession.
No firm dates can be assigned to this emperor's life, but he is conventionally considered to have reigned from 581 to 549 B.C.
Legendary narrative.
Modern scholars have come to question the existence of at least the first nine emperors; Suizei's descendant, Emperor Sujin is the first that many agree might have actually existed. The name Suizei"-tennō" was assigned to him posthumously by later generations.
Suizei is regarded by historians as a "legendary emperor" and there is a paucity of information about him. There is insufficient material available for further verification and study. The reign of Emperor Kimmei (509?–571 AD), the 29th emperor, is the first for which contemporary historiography is able to assign verifiable dates; however, the conventionally accepted names and dates of the early emperors were not to be confirmed as "traditional" until the reign of Emperor Kammu (737–806), the 50th sovereign of the Yamato dynasty.
In the "Kojiki" little more than his name and genealogy are recorded. The "Nihonshoki" is more expansive, though the section is mythical, and almost wholly cut from the cloth of Chinese legends. An Imperial "misasagi" or tomb for Suizei is currently maintained, despite the lack of any reliable early records attesting to his historical existence. He is ranked as the first of eight emperors without specific legends associated with them, also known as the "eight undocumented monarchs" (欠史八代, "Kesshi-hachidai").
The Kojiki does, however, record his ascent to the throne. According to its account Suizei was the younger son of Jimmu's chief wife, Isukeyorihime. His older brother, Kamuyawimimi was originally crown-prince. On Jimmu's death "Tagishimimi," a son of Jimmu by a lesser wife, Ahiratsuhime, attempted to seize the throne. Suizei encouraged Kamuyawimimi to slay Tagishimimi, but since he was overcome by fright at the prospect, Suizei accomplished the deed. On this, Kamuyawimimi ceded his rights and declared that Suizei, being braver, should be emperor. The story may simply reflect an attempt to explain the ancient practice of ultimogeniture, whereby the last-born exercised superior rights of inheritance, a practice later replaced by primogeniture.
Jien records that Suizei was one of the sons of Emperor Jimmu, and that he ruled from the palace of "Takaoka-no-miya" at Katsuragi in what would come to be known as Yamato province.
This emperor's posthumous name literally means "joyfully healthy peace". It is undisputed that this identification is Chinese in form and Buddhist in implication, which suggests that the name must have been regularized centuries after the lifetime ascribed to Suizei, possibly during the time in which legends about the origins of the Yamato dynasty were compiled as the chronicles known today as the "Kojiki".
The actual site of his grave is not known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") in Nara.
The Imperial Household Agency designates this location as his mausoleum. It is formally named "Tsukida no oka no e no misasagi".

</doc>
<doc id="10443" url="http://en.wikipedia.org/wiki?curid=10443" title="Emperor Annei">
Emperor Annei

Emperor Annei (安寧天皇, Annei-tennō); also known as "shikitsuhikotamatemi no Mikoto"; was the third emperor of Japan, according to the traditional order of succession.
No firm dates can be assigned to this emperor's life, but he is conventionally considered to have reigned from 549 to 511 B.C., near the end of the Jōmon period.
Legendary narrative.
Modern scholars have come to question the existence of at least the first nine emperors; Annei's descendant, Emperor Sujin is the first that many agree might have actually existed. The name Annei"-tennō" was assigned to him posthumously by later generations.
Annei is regarded by historians as a "legendary emperor" and there is a paucity of information about him. There is insufficient material available for further verification and study. The reign of Emperor Kimmei (509?–571 AD), the 29th emperor, is the first for which contemporary historiography is able to assign verifiable dates; however, the conventionally accepted names and dates of the early emperors were not to be confirmed as "traditional" until the reign of Emperor Kammu (737–806), the 50th sovereign of the Yamato dynasty.
In "Kojiki" and "Nihonshoki" only his name and genealogy were recorded. The Japanese have traditionally accepted this sovereign's historical existence, and an Imperial "misasagi" or tomb for Annei is currently maintained; however, no extant contemporary records have been discovered that confirm a view that this historical figure actually reigned. He is considered to have been the second of eight emperors without specific legends associated with them, also known as the "eight undocumented monarchs" (欠史八代, "Kesshi-hachidai").
Emperor Annei was either the eldest son or the only son of Emperor Suizei. Before his ascension to the throne, he was known as Prince Shikitsu-hiko Tamatemi.
Jien records that he ruled from the palace of "Ukena-no-miya" at Katashiro in Kawachi in what would come to be known as Yamato province.
This emperor's posthumous name literally means "steady tranquillity". It is undisputed that this identification is Chinese in form and Buddhist in implication, which suggests that the name must have been regularized centuries after the lifetime ascribed to Annei, possibly during the time in which legends about the origins of the Yamato dynasty were compiled as the chronicles known today as the "Kojiki".
The actual site of Annei's grave is not known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") in Nara.
The Imperial Household Agency designates this location as Annei's mausoleum. It is formally named "Unebi-yama no hitsujisaru Mihodo no i no e no no misasagi".

</doc>
<doc id="10445" url="http://en.wikipedia.org/wiki?curid=10445" title="Emperor Kōshō">
Emperor Kōshō

Emperor Kōshō (孝昭天皇, Kōshō-tennō); also known as "Mimatsuhikokaeshine no Mikoto"; was the fifth emperor of Japan, according to the traditional order of succession.
No firm dates can be assigned to this emperor's life or reign, but he is conventionally considered to have reigned from 475 to 393 BC, but he may have lived in the early 1st century.
Legendary narrative.
Modern scholars have come to question the existence of at least the first nine emperors; Suizei's descendant, Emperor Sujin is the first that many agree might have actually existed. The name Kōshō"-tennō" was assigned to him posthumously by later generations.
Kōshō is regarded by historians as a "legendary emperor". There is insufficient material available for further verification and study. The reign of Emperor Kimmei (509?–571 AD), the 29th emperor, is the first for which contemporary historiography is able to assign verifiable dates; However, the conventionally accepted names and dates of the early emperors were not to be confirmed as "traditional" until the reign of Emperor Kammu (737–806), the 50th sovereign of the Yamato dynasty.
In the "Kojiki" and "Nihonshoki" only his name and genealogy were recorded. He is believed to be the oldest son of Emperor Itoku; and his mother is believed to have been Amanotoyototsu-hime, who was the daughter of Okishimimi-no-kami. The Japanese have traditionally accepted this sovereign's historical existence, and an Imperial "misasagi" or tomb for Itoku is currently maintained; however, no extant contemporary records have been discovered that confirm a view that this historical figure actually reigned. He is considered to have been the fourth of eight emperors without specific legends associated with them, also known as the "eight undocumented monarchs" (欠史八代, "Kesshi-hachidai").
Emperor Kōshō was the eldest son of Emperor Itoku. Jien records that he ruled from the palace of "Ikekokoro-no-miya" at Waki-no-kami in what would come to be known as Yamato province.
This posthumous name literally means "filial manifestation". It is undisputed that this identification is Chinese in form and Buddhist in implication, which suggests that the name must have been regularized centuries after the lifetime ascribed to Kōshō, possibly during the time in which legends about the origins of the Yamato dynasty were compiled as the chronicles known today as the "Kojiki".
The actual site of Kōshō's grave is not known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") in Nara.
The Imperial Household Agency designates this location as Kōshō's mausoleum. It is formally named "Wakigami no Hakata no yama no e no misasagi".

</doc>
<doc id="10446" url="http://en.wikipedia.org/wiki?curid=10446" title="Emperor Kōan">
Emperor Kōan

Emperor Kōan (孝安天皇, Kōan-tennō); also known as "Yamatotarashihikokunioshihito no Mikoto"; was the sixth emperor of Japan, according to the traditional order of succession.
No firm dates can be assigned to this emperor's life or reign; he is conventionally considered to have reigned from 392 BC through 291 BC.
Legendary narrative.
Modern scholars have come to question the existence of at least the first nine emperors; Suizei's descendant, Emperor Sujin is the first that many agree might have actually existed. The name Kōan"-tennō" was assigned to him posthumously by later generations.
Kōan is regarded by historians as a "legendary emperor" and there is a paucity of information about him. There is insufficient material available for further verification and study. The reign of Emperor Kimmei (509?–571 AD), the 29th emperor, is the first for which contemporary historiography is able to assign verifiable dates; However, the conventionally accepted names and dates of the early emperors were not to be confirmed as "traditional" until the reign of Emperor Kammu (737–806), the 50th sovereign of the Yamato dynasty.
In the "Kojiki" and "Nihonshoki" only his name and genealogy were recorded. He is believed to be son of Emperor Kōshō; and his mother is believed to have been Yosotarashi-no-hime, who was the daughter of Okitsuyoso, and ancestress of the Owari. The Japanese have traditionally accepted this sovereign's historical existence, and an Imperial "misasagi" or tomb for Kōan is currently maintained; however, no extant contemporary records have been discovered that confirm a view that this historical figure actually reigned. He is considered to have been the fifth of eight emperors without specific legends associated with them, also known as the "eight undocumented monarchs" (欠史八代, "Kesshi-hachidai").
Jien records that Kōan was the second son of Emperor Kōshō, and that he ruled from the palace of "Akitsushima-no-miya" at Muro in what would come to be known as Yamato province.
Kōan is a posthumous name. It is undisputed that this identification is Chinese in form and Buddhist in implication, which suggests that the name must have been regularized centuries after the lifetime ascribed to Kōan, possibly during the time in which legends about the origins of the Yamato dynasty were compiled as the chronicles known today as the "Kojiki".
The actual site of Kōan's grave is not known. This emperor is traditionally venerated at a memorial Shinto shrine ("misasagi") in Nara.
The Imperial Household Agency designates this location as Kōan's mausoleum. It is formally named "Tamate no oka no e no misasagi".
Consorts and Children.
Empress:Oshihime (押媛), daughter of Amatarashikunioshihito (天足彦国押人命)

</doc>
